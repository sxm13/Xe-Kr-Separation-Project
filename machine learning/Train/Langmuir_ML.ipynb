{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-22T02:03:17.640541Z",
     "start_time": "2021-10-22T02:03:12.245960Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from os.path import expanduser\n",
    "home = expanduser(\"~\")\n",
    "import sys\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor,AdaBoostRegressor\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from xgboost import XGBRegressor\n",
    "from skopt.utils import use_named_args\n",
    "from skopt import gp_minimize\n",
    "import joblib\n",
    "import pickle\n",
    "import csv\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR, NuSVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-22T02:04:43.353413Z",
     "start_time": "2021-10-22T02:04:43.344437Z"
    }
   },
   "outputs": [],
   "source": [
    "filename='featuresXeM'\n",
    "data_file_name=filename+\".csv\" # diversity selection filename\n",
    "\n",
    " \n",
    "diverse_ratio=0.8 # training set ratio, \n",
    "remaining_ratio=1-diverse_ratio # test set ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2.2 Load divided data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-22T02:05:12.780748Z",
     "start_time": "2021-10-22T02:05:12.758807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Load file name : divided_set_0.8_0.2_Xe-M.txt\n",
      "# of diverse set, remaining set : 994 249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-62-dd663689e4d9>:31: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  number[i] = np.asarray(d[0],dtype=np.int)\n"
     ]
    }
   ],
   "source": [
    "diverse_set=[]\n",
    "remaining_set=[]\n",
    "\n",
    "txt = open(\"divided_set_\"+str(diverse_ratio)+\"_\"+str(\"%.1f\"%remaining_ratio)+\"_\"+\"Xe-M.txt\",'r').read()\n",
    "print(\" Load file name : divided_set_\"+str(diverse_ratio)+\"_\"+str(\"%.1f\"%remaining_ratio)+\"_\"+\"Xe-M.txt\")\n",
    "s1=txt.find(\"[\",0)\n",
    "s2=txt.find(\"]\",s1)\n",
    "diverse_set=txt[s1+1:s2].split(\", \")\n",
    "diverse_set=[int(i) for i in diverse_set]\n",
    "s3=txt.find(\"[\",s2)\n",
    "s4=txt.find(\"]\",s3)\n",
    "remaining_set=txt[s3+1:s4].split(\", \")\n",
    "remaining_set=[int(i) for i in remaining_set]\n",
    "print(\"# of diverse set, remaining set :\",len(diverse_set),len(remaining_set))\n",
    "\n",
    "\n",
    "with open(data_file_name) as f:\n",
    "        data_file = csv.reader(f)\n",
    "        temp = next(data_file)\n",
    "        N_samples = int(temp[0])\n",
    "        N_features = int(temp[1])\n",
    "        N_targets = int(temp[2])\n",
    "        number = np.empty((N_samples,))\n",
    "        structure = np.empty((N_samples,))\n",
    "        data = np.empty((N_samples, N_features))\n",
    "        target = np.empty((N_samples,N_targets))\n",
    "        temp = next(data_file)  # names of features\n",
    "        structure = []\n",
    "        feature_names=temp[2:2+N_features]\n",
    "        for i, d in enumerate(data_file):\n",
    "            number[i] = np.asarray(d[0],dtype=np.int)\n",
    "            structure.append(d[1])\n",
    "            data[i] = np.asarray(d[2:2+N_features], dtype=np.float64)\n",
    "            target[i] = np.asarray(d[-N_targets:], dtype=np.float64)\n",
    "N_materials = data.shape[0]\n",
    "\n",
    "diverse_set_total=[]\n",
    "remaining_set_total=[]\n",
    "for i,diverse in enumerate(diverse_set):\n",
    "    arridx = np.where(number == diverse)\n",
    "    for j,element_div in enumerate(arridx[0]):\n",
    "        diverse_set_total.append(element_div)\n",
    "for i,remaining in enumerate(remaining_set):\n",
    "    arridx = np.where(number == remaining)\n",
    "    for j,element_rem in enumerate(arridx[0]):\n",
    "        remaining_set_total.append(element_rem)        \n",
    "\n",
    "X_train = data[diverse_set_total]\n",
    "y_train = target[diverse_set_total]\n",
    "X_test = data[remaining_set_total]\n",
    "y_test = target[remaining_set_total]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Plot feature distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T11:22:00.994297Z",
     "start_time": "2021-10-18T11:22:00.229834Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\seaborn\\distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\seaborn\\distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\seaborn\\distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\seaborn\\distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\seaborn\\distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\seaborn\\distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\seaborn\\distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\seaborn\\distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\seaborn\\distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\seaborn\\distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\seaborn\\distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\seaborn\\distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x936 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig=plt.figure(figsize=(12,2*len(feature_names)+1))\n",
    "for i in range(len(feature_names)):\n",
    "    ax=plt.subplot(math.ceil(len(feature_names)/2),2,i+1)\n",
    "    ax.set_title(feature_names[i],fontsize=20,position=(.5,1.01))\n",
    "    ax.tick_params(axis='both', which='both', direction='in', labelsize=15)\n",
    "    sns.distplot(list(X_train.T[i]),ax=ax, color='b',label=\"Training\",hist=False,bins='scott',norm_hist=True, kde_kws={\"shade\": True})\n",
    "    sns.distplot(list(X_test.T[i]),ax=ax, color='r',label=\"Test\",hist=False,bins='scott',norm_hist=True, kde_kws={\"shade\": True})\n",
    "    plt.legend(frameon=True,fontsize=13,edgecolor='k')\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"feature_distribution\"+\"_\"+str(N_features)+\"_\"+str(diverse_ratio)+\"_\"+str(\"%.1f\"%remaining_ratio)+\"_\"+filename+\".png\",dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Train model and Save training model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-22T02:10:50.223782Z",
     "start_time": "2021-10-22T02:08:20.513946Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.48939183938481\n",
      "105.6487605846128\n",
      "104.89291288659997\n",
      "90.31184052204313\n",
      "106.55562956362118\n",
      "115.20750197164355\n",
      "116.49410940437585\n",
      "114.38129012030386\n",
      "120.65261901360262\n",
      "109.54565852833557\n",
      "Best score=90.3118\n",
      "Best parameters:\n",
      "    - n_estimator=118\n",
      "    - max_depth=8\n",
      "    - min_samples_split=7\n",
      "    - min_samples_leaf=12\n",
      "R^2 Training Score: 0.665 \n",
      "R^2 Testing Score: 0.927\n",
      "RMSE Training Score: 8.013 \n",
      "RMSE Testing Score: 1.181\n",
      "MAE Training Score: 1.683 \n",
      "MAE Testing Score: 0.659\n"
     ]
    }
   ],
   "source": [
    "reg = RandomForestRegressor()\n",
    "space  = [Integer(1, 500, name='n_estimators'),\n",
    "          Integer(1, 50, name='max_depth'),\n",
    "         Integer(1, 50, name='min_samples_split'),\n",
    "         Integer(1, 50, name='min_samples_leaf')]\n",
    "@use_named_args(space)\n",
    "\n",
    "def objective(**params):\n",
    "    reg.set_params(**params)\n",
    "    result=-np.mean(cross_val_score(reg, X_train, y_train, cv=5, n_jobs=-1,\n",
    "                                        scoring=\"neg_mean_squared_error\"))\n",
    "    \n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "res_gp = gp_minimize(objective, space, n_calls=10)\n",
    "\n",
    "print(\"Best score=%.4f\" % res_gp.fun)\n",
    "print(\"\"\"Best parameters:\n",
    "    - n_estimator=%d\n",
    "    - max_depth=%d\n",
    "    - min_samples_split=%d\n",
    "    - min_samples_leaf=%d\"\"\" % (res_gp.x[0], res_gp.x[1],\n",
    "                                res_gp.x[2], res_gp.x[3]))\n",
    "reg_opt = RandomForestRegressor(n_estimators=res_gp.x[0],\n",
    "                                        max_depth=res_gp.x[1],\n",
    "                                       min_samples_split=res_gp.x[2],\n",
    "                                       min_samples_leaf=res_gp.x[3],\n",
    "                                       random_state=445,\n",
    "                                       max_features='sqrt')\n",
    "reg_opt.fit(X_train, y_train.ravel())\n",
    "\n",
    "print('R^2 Training Score: {:.3f} \\nR^2 Testing Score: {:.3f}'.format(reg_opt.score(X_train, y_train),reg_opt.score(X_test, y_test)))\n",
    "print('RMSE Training Score: {:.3f} \\nRMSE Testing Score: {:.3f}'.format(np.sqrt(mean_squared_error(y_train,reg_opt.predict(X_train))),np.sqrt(mean_squared_error(y_test,reg_opt.predict(X_test)))))\n",
    "print('MAE Training Score: {:.3f} \\nMAE Testing Score: {:.3f}'.format(mean_absolute_error(y_train,reg_opt.predict(X_train)),mean_absolute_error(y_test,reg_opt.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PLD : 0.048019024967866974', 'LCD : 0.08997190272211383', 'Density : 0.32815566153098946', 'ASA_m2_cm3 : 0.020955392697177302', 'AV_VF : 0.31533969861550853', 'Kr_heat : 0.19755831946634395']\n"
     ]
    }
   ],
   "source": [
    "save_model = joblib.dump(reg_opt,\"RF\"+\"_Kr_M_Langmuir.pkl\")\n",
    "print([feature+\" : \"+str(reg_opt.feature_importances_[i]) for i,feature in enumerate(feature_names)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106.23738418786186\n",
      "78.96255456337252\n",
      "188.5252826618554\n",
      "133.78332466989292\n",
      "95.29991287347141\n",
      "133.44497458020692\n",
      "256.51248032420665\n",
      "77.27066510260285\n",
      "81.69139078419673\n",
      "120.91225333229663\n",
      "69.52230876378646\n",
      "68.03464548668954\n",
      "197.73948351389402\n",
      "67.90164544360543\n",
      "68.3968719892264\n",
      "68.51237819283719\n",
      "86.71799161675638\n",
      "67.81945755496784\n",
      "68.30887264951107\n",
      "66.48446246906295\n",
      "69.57899501454571\n",
      "250.7229975718521\n",
      "66.47139902422299\n",
      "65.67731146705488\n",
      "66.44767878278653\n",
      "65.32077025635243\n",
      "108.3820623305965\n",
      "70.54919406850863\n",
      "65.39561416922488\n",
      "65.13807519084966\n",
      "66.39164099013648\n",
      "65.40448739181197\n",
      "66.34457652415871\n",
      "66.6328476751325\n",
      "77.95366675624766\n",
      "64.75877961753493\n",
      "72.47111970943935\n",
      "65.40813006844213\n",
      "64.54098988771827\n",
      "67.6620594200504\n",
      "69.4180144440596\n",
      "70.20173105656089\n",
      "67.95258199983112\n",
      "66.07876012951905\n",
      "66.19664413556532\n",
      "66.59159304473275\n",
      "128.03233960310217\n",
      "66.97626798108305\n",
      "67.60390656607335\n",
      "65.572791499705\n",
      "66.27416233544983\n",
      "64.43413582829588\n",
      "64.91402620994043\n",
      "65.55600020895616\n",
      "65.73597689010555\n",
      "66.51218459175382\n",
      "65.04956338337347\n",
      "65.31768063780702\n",
      "67.5565474368533\n",
      "64.2507942779387\n",
      "64.74276295411832\n",
      "65.79791293481877\n",
      "180.6687946203168\n",
      "65.76383918910717\n",
      "67.87008883498166\n",
      "67.07389288844391\n",
      "64.09381041900508\n",
      "63.421536677548644\n",
      "64.2844570139396\n",
      "65.91820906462941\n",
      "62.93741714131643\n",
      "72.37773851954208\n",
      "67.32994246169939\n",
      "62.17304506112798\n",
      "66.41253187686952\n",
      "73.50302169073191\n",
      "66.3347303077719\n",
      "66.7235252763059\n",
      "61.91533744895487\n",
      "64.50695614387591\n",
      "62.274810825754216\n",
      "65.93518248086772\n",
      "79.98671148869145\n",
      "66.75359643607356\n",
      "65.90905979466683\n",
      "67.35707210971965\n",
      "85.81730654550822\n",
      "67.46437133982391\n",
      "83.57100164007626\n",
      "67.55081814055482\n",
      "128.54145505020963\n",
      "65.11382637949097\n",
      "67.91839397357168\n",
      "65.13036888296642\n",
      "70.57060778447415\n",
      "230.4994972746528\n",
      "68.49349783597691\n",
      "80.877419765437\n",
      "63.91814233393959\n",
      "70.67971054051249\n",
      "Best score=61.9153\n",
      "Best parameters:\n",
      "    - n_estimator=319\n",
      "    - max_depth=6\n",
      "    - num_parallel_tree=50\n",
      "    - min_child_weight=1\n",
      "    - learning_rate=0.646307\n",
      "    - subsample=0.984521\n",
      "    - gamma=0.001002\n",
      "    - alpha=0.434279\n",
      "R^2 Training Score: 1.000 \n",
      "R^2 Testing Score: 0.942\n",
      "RMSE Training Score: 0.046 \n",
      "RMSE Testing Score: 1.057\n",
      "MAE Training Score: 0.033 \n",
      "MAE Testing Score: 0.653\n"
     ]
    }
   ],
   "source": [
    "reg = XGBRegressor()\n",
    "space  = [Integer(1,500, name='n_estimators'),\n",
    "            Integer(1, 50, name='max_depth'),\n",
    "            Integer(1, 50, name='num_parallel_tree'),\n",
    "            Integer(1, 50, name='min_child_weight'),\n",
    "            Real(0.001,1,\"log-uniform\",name='learning_rate'),\n",
    "            Real(0.01,1,name='subsample'),\n",
    "            Real(0.001,50,\"log-uniform\",name='gamma'),\n",
    "            Real(0, 1, name='alpha')\n",
    "         ]\n",
    "@use_named_args(space)\n",
    "\n",
    "def objective(**params):\n",
    "    reg.set_params(**params)\n",
    "    result=-np.mean(cross_val_score(reg, X_train, y_train, cv=5, n_jobs=8,\n",
    "                                        scoring=\"neg_mean_squared_error\"))\n",
    "    \n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "res_gp = gp_minimize(objective, space, n_calls=100)\n",
    "\n",
    "print(\"Best score=%.4f\" % res_gp.fun)\n",
    "print(\"\"\"Best parameters:\n",
    "    - n_estimator=%d\n",
    "    - max_depth=%d\n",
    "    - num_parallel_tree=%d\n",
    "    - min_child_weight=%d\n",
    "    - learning_rate=%f\n",
    "    - subsample=%f\n",
    "    - gamma=%f\n",
    "    - alpha=%f\"\"\" % (res_gp.x[0],res_gp.x[1],\n",
    "                        res_gp.x[2],res_gp.x[3],\n",
    "                        res_gp.x[4],res_gp.x[5],\n",
    "                        res_gp.x[6],res_gp.x[7]\n",
    "                         ))\n",
    "reg_opt = XGBRegressor(n_estimators=res_gp.x[0],\n",
    "                                    max_depth=res_gp.x[1],\n",
    "                                    num_parallel_tree=res_gp.x[2],\n",
    "                                    min_child_weight=res_gp.x[3],\n",
    "                                    learning_rate=res_gp.x[4],\n",
    "                                    subsample=res_gp.x[5],\n",
    "                                    gamma=res_gp.x[6],\n",
    "                                   alpha=res_gp.x[7]\n",
    "                                  )\n",
    "reg_opt.fit(X_train, y_train)\n",
    "\n",
    "print('R^2 Training Score: {:.3f} \\nR^2 Testing Score: {:.3f}'.format(reg_opt.score(X_train, y_train),reg_opt.score(X_test, y_test)))\n",
    "print('RMSE Training Score: {:.3f} \\nRMSE Testing Score: {:.3f}'.format(np.sqrt(mean_squared_error(y_train,reg_opt.predict(X_train))),np.sqrt(mean_squared_error(y_test,reg_opt.predict(X_test)))))\n",
    "print('MAE Training Score: {:.3f} \\nMAE Testing Score: {:.3f}'.format(mean_absolute_error(y_train,reg_opt.predict(X_train)),mean_absolute_error(y_test,reg_opt.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PLD : 0.014369392', 'LCD : 0.006216315', 'Density : 0.3924986', 'ASA_m2_cm3 : 0.004258436', 'AV_VF : 0.4315661', 'Kr_heat : 0.15109113']\n"
     ]
    }
   ],
   "source": [
    "save_model = joblib.dump(reg_opt,\"GBR\"+\"_Kr_M_Langmuir.pkl\")\n",
    "print([feature+\" : \"+str(reg_opt.feature_importances_[i]) for i,feature in enumerate(feature_names)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2592 candidates, totalling 12960 fits\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "xgb = XGBRegressor(use_rmm=True,validate_parameters=True,disable_default_eval_metric=False)\n",
    "\n",
    "parameters = {\n",
    "             'booster':['dart'],#'gbtree','gblinear',\n",
    "            'nthread':[4], \n",
    "               'objective':['reg:squarederror'],\n",
    "                'eta': [0.1,0.3,0.5], \n",
    "                'max_depth': [1,5,10],# 0\n",
    "                'min_child_weight': [0.5,1],\n",
    "                 'subsample': [0.5,1],\n",
    "# #               'colsample_bytree': [1],\n",
    "# #               'colsample_bylevel': [1],\n",
    "# #               'colsample_bynode': [1],\n",
    "              'n_estimators': [100,200,300],\n",
    "              'gamma': [0,0.5],\n",
    "               'lambda':[1,2],\n",
    "               'alpha':[0,0.5],\n",
    "# #              'max_delta_step': [0],\n",
    "           'tree_method':['exact'],# ,'approx','auto','hist'\n",
    "# #           'predictor':['auto'],# ,'cpu_predictor','gpu_predictor'\n",
    "# #             'sampling_method':['uniform'],#,'gradient_based'\n",
    "# #              'process_type':['default'],#,'update'\n",
    "               'num_parallel_tree':[1,5,10],\n",
    "#              'scale_pos_weight': [0.1,1,2,3],\n",
    "#                'updater':['grow_quantile_histmaker'],#,'grow_colmaker','grow_histmaker','grow_local_histmaker','grow_gpu_hist','sync','refresh','prune'\n",
    "              \n",
    "#                'refresh_leaf': [0],\n",
    "# # #             'grow_policy':['depthwise','lossguide'],\n",
    "#                 'max_bin':[128],\n",
    "#                  'max_leaves':[32],\n",
    "#                 'sketch_eps': [0.001],\n",
    "#               'monotone_constraints':[],\n",
    "#               interaction_constraints:[]\n",
    "             }\n",
    "\n",
    "reg_opt = GridSearchCV(xgb,\n",
    "                        parameters,\n",
    "                        cv = 5,\n",
    "                        n_jobs = 5,\n",
    "                       scoring='r2',\n",
    "                        verbose=True)\n",
    "\n",
    "reg_opt.fit(X_train,y_train)\n",
    "# reg_opt.fit(X_train, np.ravel(y_train))\n",
    "print(reg_opt.best_params_)\n",
    "# save_model = joblib.dump(reg_opt,'jolsul'+\"_\"+str(N_features)+\"_\"+str(diverse_ratio)+\"_\"+str(\"%.1f\"%remaining_ratio)+\"_\"+ML_algorithm+\"_Xe_K1_langmuir.pkl\")\n",
    "print('R^2 Training Score: {:.3f} \\nR^2 Testing Score: {:.3f}'.format(reg_opt.score(X_train, y_train),reg_opt.score(X_test, y_test)))\n",
    "# print('RMSE Training Score: {:.3f} \\nRMSE Testing Score: {:.3f}'.format(np.sqrt(mean_squared_error(y_train,reg_opt.predict(X_train))),np.sqrt(mean_squared_error(y_test,reg_opt.predict(X_test)))))\n",
    "# print('MAE Training Score: {:.3f} \\nMAE Testing Score: {:.3f}'.format(mean_absolute_error(y_train,reg_opt.predict(X_train)),mean_absolute_error(y_test,reg_opt.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      " {'activation': 'relu', 'hidden_layer_sizes': (60,), 'learning_rate': 'constant', 'max_iter': 80000, 'solver': 'adam'}\n",
      "0.272 (+/-0.268) for {'activation': 'relu', 'hidden_layer_sizes': (60, 60), 'learning_rate': 'constant', 'max_iter': 1000, 'solver': 'adam'}\n",
      "0.560 (+/-0.115) for {'activation': 'relu', 'hidden_layer_sizes': (60, 60), 'learning_rate': 'constant', 'max_iter': 1000, 'solver': 'lbfgs'}\n",
      "0.792 (+/-0.395) for {'activation': 'relu', 'hidden_layer_sizes': (60, 60), 'learning_rate': 'constant', 'max_iter': 10000, 'solver': 'adam'}\n",
      "0.560 (+/-0.115) for {'activation': 'relu', 'hidden_layer_sizes': (60, 60), 'learning_rate': 'constant', 'max_iter': 10000, 'solver': 'lbfgs'}\n",
      "0.435 (+/-1.384) for {'activation': 'relu', 'hidden_layer_sizes': (60, 60), 'learning_rate': 'constant', 'max_iter': 20000, 'solver': 'adam'}\n",
      "0.560 (+/-0.115) for {'activation': 'relu', 'hidden_layer_sizes': (60, 60), 'learning_rate': 'constant', 'max_iter': 20000, 'solver': 'lbfgs'}\n",
      "0.323 (+/-1.767) for {'activation': 'relu', 'hidden_layer_sizes': (60, 60), 'learning_rate': 'constant', 'max_iter': 30000, 'solver': 'adam'}\n",
      "0.560 (+/-0.115) for {'activation': 'relu', 'hidden_layer_sizes': (60, 60), 'learning_rate': 'constant', 'max_iter': 30000, 'solver': 'lbfgs'}\n",
      "0.620 (+/-0.939) for {'activation': 'relu', 'hidden_layer_sizes': (60, 60), 'learning_rate': 'constant', 'max_iter': 40000, 'solver': 'adam'}\n",
      "0.560 (+/-0.115) for {'activation': 'relu', 'hidden_layer_sizes': (60, 60), 'learning_rate': 'constant', 'max_iter': 40000, 'solver': 'lbfgs'}\n",
      "0.684 (+/-0.727) for {'activation': 'relu', 'hidden_layer_sizes': (60, 60), 'learning_rate': 'constant', 'max_iter': 80000, 'solver': 'adam'}\n",
      "0.560 (+/-0.115) for {'activation': 'relu', 'hidden_layer_sizes': (60, 60), 'learning_rate': 'constant', 'max_iter': 80000, 'solver': 'lbfgs'}\n",
      "0.272 (+/-0.268) for {'activation': 'relu', 'hidden_layer_sizes': (60, 60), 'learning_rate': 'adaptive', 'max_iter': 1000, 'solver': 'adam'}\n",
      "0.560 (+/-0.115) for {'activation': 'relu', 'hidden_layer_sizes': (60, 60), 'learning_rate': 'adaptive', 'max_iter': 1000, 'solver': 'lbfgs'}\n",
      "0.792 (+/-0.395) for {'activation': 'relu', 'hidden_layer_sizes': (60, 60), 'learning_rate': 'adaptive', 'max_iter': 10000, 'solver': 'adam'}\n",
      "0.560 (+/-0.115) for {'activation': 'relu', 'hidden_layer_sizes': (60, 60), 'learning_rate': 'adaptive', 'max_iter': 10000, 'solver': 'lbfgs'}\n",
      "0.435 (+/-1.384) for {'activation': 'relu', 'hidden_layer_sizes': (60, 60), 'learning_rate': 'adaptive', 'max_iter': 20000, 'solver': 'adam'}\n",
      "0.560 (+/-0.115) for {'activation': 'relu', 'hidden_layer_sizes': (60, 60), 'learning_rate': 'adaptive', 'max_iter': 20000, 'solver': 'lbfgs'}\n",
      "0.323 (+/-1.767) for {'activation': 'relu', 'hidden_layer_sizes': (60, 60), 'learning_rate': 'adaptive', 'max_iter': 30000, 'solver': 'adam'}\n",
      "0.560 (+/-0.115) for {'activation': 'relu', 'hidden_layer_sizes': (60, 60), 'learning_rate': 'adaptive', 'max_iter': 30000, 'solver': 'lbfgs'}\n",
      "0.620 (+/-0.939) for {'activation': 'relu', 'hidden_layer_sizes': (60, 60), 'learning_rate': 'adaptive', 'max_iter': 40000, 'solver': 'adam'}\n",
      "0.560 (+/-0.115) for {'activation': 'relu', 'hidden_layer_sizes': (60, 60), 'learning_rate': 'adaptive', 'max_iter': 40000, 'solver': 'lbfgs'}\n",
      "0.684 (+/-0.727) for {'activation': 'relu', 'hidden_layer_sizes': (60, 60), 'learning_rate': 'adaptive', 'max_iter': 80000, 'solver': 'adam'}\n",
      "0.560 (+/-0.115) for {'activation': 'relu', 'hidden_layer_sizes': (60, 60), 'learning_rate': 'adaptive', 'max_iter': 80000, 'solver': 'lbfgs'}\n",
      "0.658 (+/-0.378) for {'activation': 'relu', 'hidden_layer_sizes': (60,), 'learning_rate': 'constant', 'max_iter': 1000, 'solver': 'adam'}\n",
      "0.584 (+/-0.379) for {'activation': 'relu', 'hidden_layer_sizes': (60,), 'learning_rate': 'constant', 'max_iter': 1000, 'solver': 'lbfgs'}\n",
      "0.688 (+/-0.304) for {'activation': 'relu', 'hidden_layer_sizes': (60,), 'learning_rate': 'constant', 'max_iter': 10000, 'solver': 'adam'}\n",
      "0.584 (+/-0.379) for {'activation': 'relu', 'hidden_layer_sizes': (60,), 'learning_rate': 'constant', 'max_iter': 10000, 'solver': 'lbfgs'}\n",
      "0.753 (+/-0.324) for {'activation': 'relu', 'hidden_layer_sizes': (60,), 'learning_rate': 'constant', 'max_iter': 20000, 'solver': 'adam'}\n",
      "0.584 (+/-0.379) for {'activation': 'relu', 'hidden_layer_sizes': (60,), 'learning_rate': 'constant', 'max_iter': 20000, 'solver': 'lbfgs'}\n",
      "0.782 (+/-0.298) for {'activation': 'relu', 'hidden_layer_sizes': (60,), 'learning_rate': 'constant', 'max_iter': 30000, 'solver': 'adam'}\n",
      "0.584 (+/-0.379) for {'activation': 'relu', 'hidden_layer_sizes': (60,), 'learning_rate': 'constant', 'max_iter': 30000, 'solver': 'lbfgs'}\n",
      "0.784 (+/-0.313) for {'activation': 'relu', 'hidden_layer_sizes': (60,), 'learning_rate': 'constant', 'max_iter': 40000, 'solver': 'adam'}\n",
      "0.584 (+/-0.379) for {'activation': 'relu', 'hidden_layer_sizes': (60,), 'learning_rate': 'constant', 'max_iter': 40000, 'solver': 'lbfgs'}\n",
      "0.824 (+/-0.339) for {'activation': 'relu', 'hidden_layer_sizes': (60,), 'learning_rate': 'constant', 'max_iter': 80000, 'solver': 'adam'}\n",
      "0.584 (+/-0.379) for {'activation': 'relu', 'hidden_layer_sizes': (60,), 'learning_rate': 'constant', 'max_iter': 80000, 'solver': 'lbfgs'}\n",
      "0.658 (+/-0.378) for {'activation': 'relu', 'hidden_layer_sizes': (60,), 'learning_rate': 'adaptive', 'max_iter': 1000, 'solver': 'adam'}\n",
      "0.584 (+/-0.379) for {'activation': 'relu', 'hidden_layer_sizes': (60,), 'learning_rate': 'adaptive', 'max_iter': 1000, 'solver': 'lbfgs'}\n",
      "0.688 (+/-0.304) for {'activation': 'relu', 'hidden_layer_sizes': (60,), 'learning_rate': 'adaptive', 'max_iter': 10000, 'solver': 'adam'}\n",
      "0.584 (+/-0.379) for {'activation': 'relu', 'hidden_layer_sizes': (60,), 'learning_rate': 'adaptive', 'max_iter': 10000, 'solver': 'lbfgs'}\n",
      "0.753 (+/-0.324) for {'activation': 'relu', 'hidden_layer_sizes': (60,), 'learning_rate': 'adaptive', 'max_iter': 20000, 'solver': 'adam'}\n",
      "0.584 (+/-0.379) for {'activation': 'relu', 'hidden_layer_sizes': (60,), 'learning_rate': 'adaptive', 'max_iter': 20000, 'solver': 'lbfgs'}\n",
      "0.782 (+/-0.298) for {'activation': 'relu', 'hidden_layer_sizes': (60,), 'learning_rate': 'adaptive', 'max_iter': 30000, 'solver': 'adam'}\n",
      "0.584 (+/-0.379) for {'activation': 'relu', 'hidden_layer_sizes': (60,), 'learning_rate': 'adaptive', 'max_iter': 30000, 'solver': 'lbfgs'}\n",
      "0.784 (+/-0.313) for {'activation': 'relu', 'hidden_layer_sizes': (60,), 'learning_rate': 'adaptive', 'max_iter': 40000, 'solver': 'adam'}\n",
      "0.584 (+/-0.379) for {'activation': 'relu', 'hidden_layer_sizes': (60,), 'learning_rate': 'adaptive', 'max_iter': 40000, 'solver': 'lbfgs'}\n",
      "0.824 (+/-0.339) for {'activation': 'relu', 'hidden_layer_sizes': (60,), 'learning_rate': 'adaptive', 'max_iter': 80000, 'solver': 'adam'}\n",
      "0.584 (+/-0.379) for {'activation': 'relu', 'hidden_layer_sizes': (60,), 'learning_rate': 'adaptive', 'max_iter': 80000, 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (80000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPRegressor(batch_size='auto',\n",
    "    verbose=False, warm_start=False, \n",
    "    early_stopping=False,\n",
    "    alpha=0.0001,\n",
    "    learning_rate_init=0.001, shuffle=True,\n",
    "    random_state=1, tol=0.00001,\n",
    "    beta_1=0.9, beta_2=0.999, epsilon=1e-08,n_iter_no_change=10000)\n",
    "# space  = [Integer(100,1000, name='hidden_neurons'),\n",
    "#         Integer(5, 10, name='layers'),\n",
    "#          Integer(5, 10, name='max_iter')]\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(60,60),(60,)],\n",
    "    'activation': ['relu'],\n",
    "    'solver': [ 'adam','lbfgs'],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "    'max_iter': [1000,10000,20000,30000,40000,80000],\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(mlp, parameter_space, n_jobs=4, cv=3)\n",
    "clf.fit(X_train, y_train.ravel())\n",
    "\n",
    "print('Best parameters found:\\n', clf.best_params_)\n",
    "\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 115818.04782963\n",
      "Iteration 2, loss = 78082.96873483\n",
      "Iteration 3, loss = 47985.99651525\n",
      "Iteration 4, loss = 26939.90790045\n",
      "Iteration 5, loss = 12726.35626829\n",
      "Iteration 6, loss = 4780.63692433\n",
      "Iteration 7, loss = 1163.38493834\n",
      "Iteration 8, loss = 158.71794514\n",
      "Iteration 9, loss = 244.03697640\n",
      "Iteration 10, loss = 522.92037547\n",
      "Iteration 11, loss = 619.51859987\n",
      "Iteration 12, loss = 517.20563353\n",
      "Iteration 13, loss = 337.47493410\n",
      "Iteration 14, loss = 189.45360260\n",
      "Iteration 15, loss = 111.80064033\n",
      "Iteration 16, loss = 90.36336552\n",
      "Iteration 17, loss = 95.44008195\n",
      "Iteration 18, loss = 101.78910991\n",
      "Iteration 19, loss = 101.68086111\n",
      "Iteration 20, loss = 97.40070101\n",
      "Iteration 21, loss = 93.02573352\n",
      "Iteration 22, loss = 90.81444491\n",
      "Iteration 23, loss = 90.19386232\n",
      "Iteration 24, loss = 90.29042520\n",
      "Iteration 25, loss = 90.37393268\n",
      "Iteration 26, loss = 90.20633907\n",
      "Iteration 27, loss = 89.96330632\n",
      "Iteration 28, loss = 89.75343899\n",
      "Iteration 29, loss = 89.66416624\n",
      "Iteration 30, loss = 89.57277503\n",
      "Iteration 31, loss = 89.52026383\n",
      "Iteration 32, loss = 89.44172492\n",
      "Iteration 33, loss = 89.31274783\n",
      "Iteration 34, loss = 89.24252996\n",
      "Iteration 35, loss = 89.14359143\n",
      "Iteration 36, loss = 89.05842286\n",
      "Iteration 37, loss = 88.95269111\n",
      "Iteration 38, loss = 88.86592856\n",
      "Iteration 39, loss = 88.76799435\n",
      "Iteration 40, loss = 88.68129003\n",
      "Iteration 41, loss = 88.58263617\n",
      "Iteration 42, loss = 88.48895640\n",
      "Iteration 43, loss = 88.38887683\n",
      "Iteration 44, loss = 88.30206533\n",
      "Iteration 45, loss = 88.18392634\n",
      "Iteration 46, loss = 88.09809935\n",
      "Iteration 47, loss = 87.98526728\n",
      "Iteration 48, loss = 87.89957670\n",
      "Iteration 49, loss = 87.79018242\n",
      "Iteration 50, loss = 87.59137026\n",
      "Iteration 51, loss = 87.39782511\n",
      "Iteration 52, loss = 87.19381995\n",
      "Iteration 53, loss = 87.09116478\n",
      "Iteration 54, loss = 86.53488910\n",
      "Iteration 55, loss = 86.10863183\n",
      "Iteration 56, loss = 85.67196954"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1599: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 57, loss = 85.35975093\n",
      "Iteration 58, loss = 84.84530045\n",
      "Iteration 59, loss = 84.58543937\n",
      "Iteration 60, loss = 83.91437577\n",
      "Iteration 61, loss = 83.75581311\n",
      "Iteration 62, loss = 83.06881843\n",
      "Iteration 63, loss = 82.76932029\n",
      "Iteration 64, loss = 82.35368830\n",
      "Iteration 65, loss = 81.75697010\n",
      "Iteration 66, loss = 81.12706105\n",
      "Iteration 67, loss = 80.62519801\n",
      "Iteration 68, loss = 80.23357135\n",
      "Iteration 69, loss = 79.51190769\n",
      "Iteration 70, loss = 78.65971702\n",
      "Iteration 71, loss = 78.33419279\n",
      "Iteration 72, loss = 77.43140756\n",
      "Iteration 73, loss = 76.81421478\n",
      "Iteration 74, loss = 76.32049093\n",
      "Iteration 75, loss = 75.66340578\n",
      "Iteration 76, loss = 74.73899585\n",
      "Iteration 77, loss = 74.26591838\n",
      "Iteration 78, loss = 73.51905254\n",
      "Iteration 79, loss = 72.83872985\n",
      "Iteration 80, loss = 72.43660775\n",
      "Iteration 81, loss = 72.25994730\n",
      "Iteration 82, loss = 71.69916730\n",
      "Iteration 83, loss = 71.41620825\n",
      "Iteration 84, loss = 70.67671913\n",
      "Iteration 85, loss = 69.46731143\n",
      "Iteration 86, loss = 69.07620605\n",
      "Iteration 87, loss = 68.46194179\n",
      "Iteration 88, loss = 67.88682105\n",
      "Iteration 89, loss = 67.69092139\n",
      "Iteration 90, loss = 67.11347524\n",
      "Iteration 91, loss = 66.64605536\n",
      "Iteration 92, loss = 66.16979694\n",
      "Iteration 93, loss = 65.95712996\n",
      "Iteration 94, loss = 65.40561893\n",
      "Iteration 95, loss = 65.30350711\n",
      "Iteration 96, loss = 64.60010005\n",
      "Iteration 97, loss = 63.97413317\n",
      "Iteration 98, loss = 63.86731603\n",
      "Iteration 99, loss = 63.60127567\n",
      "Iteration 100, loss = 63.43529658\n",
      "Iteration 101, loss = 63.18442095\n",
      "Iteration 102, loss = 62.81351395\n",
      "Iteration 103, loss = 62.81941231\n",
      "Iteration 104, loss = 62.00667074\n",
      "Iteration 105, loss = 61.66040581\n",
      "Iteration 106, loss = 61.56515658\n",
      "Iteration 107, loss = 61.23547985\n",
      "Iteration 108, loss = 61.10961383\n",
      "Iteration 109, loss = 60.90287800\n",
      "Iteration 110, loss = 60.74066945\n",
      "Iteration 111, loss = 60.69035104\n",
      "Iteration 112, loss = 60.35734537\n",
      "Iteration 113, loss = 60.16793264\n",
      "Iteration 114, loss = 59.82301514\n",
      "Iteration 115, loss = 59.82824890\n",
      "Iteration 116, loss = 59.81606858\n",
      "Iteration 117, loss = 59.85491334\n",
      "Iteration 118, loss = 59.55603673\n",
      "Iteration 119, loss = 59.20104180\n",
      "Iteration 120, loss = 59.19866103\n",
      "Iteration 121, loss = 59.12519478\n",
      "Iteration 122, loss = 58.90360545\n",
      "Iteration 123, loss = 59.03974053\n",
      "Iteration 124, loss = 59.43285367\n",
      "Iteration 125, loss = 59.26488035\n",
      "Iteration 126, loss = 58.70980428\n",
      "Iteration 127, loss = 58.38317216\n",
      "Iteration 128, loss = 58.47016472\n",
      "Iteration 129, loss = 58.34408049\n",
      "Iteration 130, loss = 58.33663876\n",
      "Iteration 131, loss = 58.49272579\n",
      "Iteration 132, loss = 58.69080086\n",
      "Iteration 133, loss = 58.70511814\n",
      "Iteration 134, loss = 58.23291420\n",
      "Iteration 135, loss = 58.07614582\n",
      "Iteration 136, loss = 57.89992616\n",
      "Iteration 137, loss = 58.12651488\n",
      "Iteration 138, loss = 58.13675291\n",
      "Iteration 139, loss = 57.92267729\n",
      "Iteration 140, loss = 57.95410405\n",
      "Iteration 141, loss = 57.74468286\n",
      "Iteration 142, loss = 58.01304012\n",
      "Iteration 143, loss = 58.08276030\n",
      "Iteration 144, loss = 58.05195111\n",
      "Iteration 145, loss = 57.85594750\n",
      "Iteration 146, loss = 57.65662711\n",
      "Iteration 147, loss = 57.60084606\n",
      "Iteration 148, loss = 57.61628646\n",
      "Iteration 149, loss = 57.65902407\n",
      "Iteration 150, loss = 57.77594997\n",
      "Iteration 151, loss = 57.63267070\n",
      "Iteration 152, loss = 57.47241322\n",
      "Iteration 153, loss = 57.55559055\n",
      "Iteration 154, loss = 57.44298462\n",
      "Iteration 155, loss = 57.51781322\n",
      "Iteration 156, loss = 57.46420161\n",
      "Iteration 157, loss = 57.58001934\n",
      "Iteration 158, loss = 57.89585151\n",
      "Iteration 159, loss = 57.55144821\n",
      "Iteration 160, loss = 57.52948824\n",
      "Iteration 161, loss = 57.50553579\n",
      "Iteration 162, loss = 57.51137034\n",
      "Iteration 163, loss = 57.32857052\n",
      "Iteration 164, loss = 57.40186806\n",
      "Iteration 165, loss = 57.49238717\n",
      "Iteration 166, loss = 57.53426460\n",
      "Iteration 167, loss = 57.35750607\n",
      "Iteration 168, loss = 57.26552600\n",
      "Iteration 169, loss = 57.36954377\n",
      "Iteration 170, loss = 57.43234937\n",
      "Iteration 171, loss = 57.73452812\n",
      "Iteration 172, loss = 57.38186514\n",
      "Iteration 173, loss = 57.33938938\n",
      "Iteration 174, loss = 57.21110070\n",
      "Iteration 175, loss = 57.23570100\n",
      "Iteration 176, loss = 57.38633100\n",
      "Iteration 177, loss = 57.75273654\n",
      "Iteration 178, loss = 58.22740541\n",
      "Iteration 179, loss = 58.12856695\n",
      "Iteration 180, loss = 57.56940114\n",
      "Iteration 181, loss = 57.93522951\n",
      "Iteration 182, loss = 57.72835494\n",
      "Iteration 183, loss = 57.40350189\n",
      "Iteration 184, loss = 57.39058322\n",
      "Iteration 185, loss = 57.50202214\n",
      "Iteration 186, loss = 57.38700424\n",
      "Iteration 187, loss = 57.33712745\n",
      "Iteration 188, loss = 57.48500530\n",
      "Iteration 189, loss = 57.34138887\n",
      "Iteration 190, loss = 57.94387094\n",
      "Iteration 191, loss = 57.57478677\n",
      "Iteration 192, loss = 57.43171049\n",
      "Iteration 193, loss = 57.12942319\n",
      "Iteration 194, loss = 57.78320569\n",
      "Iteration 195, loss = 57.26474258\n",
      "Iteration 196, loss = 57.09755835\n",
      "Iteration 197, loss = 57.05111282\n",
      "Iteration 198, loss = 57.17899205\n",
      "Iteration 199, loss = 57.15530823\n",
      "Iteration 200, loss = 57.10876585\n",
      "Iteration 201, loss = 56.99432190\n",
      "Iteration 202, loss = 57.04067837\n",
      "Iteration 203, loss = 57.14686134\n",
      "Iteration 204, loss = 57.00963961\n",
      "Iteration 205, loss = 57.07594242\n",
      "Iteration 206, loss = 57.28698784\n",
      "Iteration 207, loss = 57.73144858\n",
      "Iteration 208, loss = 57.58455813\n",
      "Iteration 209, loss = 57.79004051\n",
      "Iteration 210, loss = 57.18853499\n",
      "Iteration 211, loss = 56.89900673\n",
      "Iteration 212, loss = 57.13028557\n",
      "Iteration 213, loss = 57.07644234\n",
      "Iteration 214, loss = 56.95505737\n",
      "Iteration 215, loss = 56.96026785\n",
      "Iteration 216, loss = 56.90945402\n",
      "Iteration 217, loss = 56.82035052\n",
      "Iteration 218, loss = 56.91828109\n",
      "Iteration 219, loss = 56.89201298\n",
      "Iteration 220, loss = 56.86561508\n",
      "Iteration 221, loss = 57.07308815\n",
      "Iteration 222, loss = 57.34834482\n",
      "Iteration 223, loss = 57.90578564\n",
      "Iteration 224, loss = 57.74786856\n",
      "Iteration 225, loss = 57.21137640\n",
      "Iteration 226, loss = 57.17137593\n",
      "Iteration 227, loss = 56.87900005\n",
      "Iteration 228, loss = 56.93838559\n",
      "Iteration 229, loss = 56.74382309\n",
      "Iteration 230, loss = 56.62523816\n",
      "Iteration 231, loss = 56.71970710\n",
      "Iteration 232, loss = 56.62793813\n",
      "Iteration 233, loss = 56.89315278\n",
      "Iteration 234, loss = 56.70963823\n",
      "Iteration 235, loss = 56.42858122\n",
      "Iteration 236, loss = 56.70496499\n",
      "Iteration 237, loss = 56.59389892\n",
      "Iteration 238, loss = 56.67171768\n",
      "Iteration 239, loss = 56.46296804\n",
      "Iteration 240, loss = 56.55525184\n",
      "Iteration 241, loss = 56.71395782\n",
      "Iteration 242, loss = 56.89942801\n",
      "Iteration 243, loss = 56.14551227\n",
      "Iteration 244, loss = 56.43241248\n",
      "Iteration 245, loss = 56.46464996\n",
      "Iteration 246, loss = 56.29420684\n",
      "Iteration 247, loss = 56.16149916\n",
      "Iteration 248, loss = 56.15602948\n",
      "Iteration 249, loss = 56.39848159\n",
      "Iteration 250, loss = 56.06483200\n",
      "Iteration 251, loss = 56.24852506\n",
      "Iteration 252, loss = 56.14756506\n",
      "Iteration 253, loss = 56.20152468\n",
      "Iteration 254, loss = 56.05186233\n",
      "Iteration 255, loss = 55.91486660\n",
      "Iteration 256, loss = 55.89467123\n",
      "Iteration 257, loss = 55.81347847\n",
      "Iteration 258, loss = 55.77105347\n",
      "Iteration 259, loss = 55.75288770\n",
      "Iteration 260, loss = 55.67128874\n",
      "Iteration 261, loss = 55.61089306\n",
      "Iteration 262, loss = 55.64546401\n",
      "Iteration 263, loss = 55.54568929\n",
      "Iteration 264, loss = 55.50821068\n",
      "Iteration 265, loss = 55.70193264\n",
      "Iteration 266, loss = 55.40997044\n",
      "Iteration 267, loss = 55.30610884\n",
      "Iteration 268, loss = 55.70602415\n",
      "Iteration 269, loss = 55.39057184\n",
      "Iteration 270, loss = 55.42965591\n",
      "Iteration 271, loss = 55.56466234\n",
      "Iteration 272, loss = 55.56836692\n",
      "Iteration 273, loss = 55.13001077\n",
      "Iteration 274, loss = 55.16252999\n",
      "Iteration 275, loss = 55.15808509\n",
      "Iteration 276, loss = 55.09800424\n",
      "Iteration 277, loss = 55.01282064\n",
      "Iteration 278, loss = 54.91000838\n",
      "Iteration 279, loss = 54.97306520\n",
      "Iteration 280, loss = 54.81851778\n",
      "Iteration 281, loss = 54.82503126\n",
      "Iteration 282, loss = 54.80973103\n",
      "Iteration 283, loss = 54.79295002\n",
      "Iteration 284, loss = 54.69743268\n",
      "Iteration 285, loss = 54.62300099\n",
      "Iteration 286, loss = 54.68217011\n",
      "Iteration 287, loss = 54.51769366\n",
      "Iteration 288, loss = 54.64352549\n",
      "Iteration 289, loss = 54.53274519\n",
      "Iteration 290, loss = 54.37447749\n",
      "Iteration 291, loss = 54.40050350\n",
      "Iteration 292, loss = 54.65234239\n",
      "Iteration 293, loss = 54.56566323\n",
      "Iteration 294, loss = 54.48697374\n",
      "Iteration 295, loss = 54.43309372\n",
      "Iteration 296, loss = 54.37073277\n",
      "Iteration 297, loss = 54.37640437\n",
      "Iteration 298, loss = 54.52703658\n",
      "Iteration 299, loss = 54.04105824\n",
      "Iteration 300, loss = 54.14716761\n",
      "Iteration 301, loss = 54.04386429\n",
      "Iteration 302, loss = 54.06278798\n",
      "Iteration 303, loss = 54.14587774\n",
      "Iteration 304, loss = 54.09871993\n",
      "Iteration 305, loss = 53.78998193\n",
      "Iteration 306, loss = 53.91198877\n",
      "Iteration 307, loss = 53.78886223\n",
      "Iteration 308, loss = 53.77334729\n",
      "Iteration 309, loss = 53.67571398\n",
      "Iteration 310, loss = 53.70301911\n",
      "Iteration 311, loss = 53.64641580\n",
      "Iteration 312, loss = 53.83869113\n",
      "Iteration 313, loss = 54.09667863\n",
      "Iteration 314, loss = 53.63969814\n",
      "Iteration 315, loss = 53.68331511\n",
      "Iteration 316, loss = 53.70542471\n",
      "Iteration 317, loss = 53.61390169\n",
      "Iteration 318, loss = 53.35137730\n",
      "Iteration 319, loss = 53.41379266\n",
      "Iteration 320, loss = 53.55903078\n",
      "Iteration 321, loss = 53.64923306\n",
      "Iteration 322, loss = 53.54749684\n",
      "Iteration 323, loss = 53.42912674\n",
      "Iteration 324, loss = 53.31246429\n",
      "Iteration 325, loss = 53.42309481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 326, loss = 53.32136264\n",
      "Iteration 327, loss = 53.35251752\n",
      "Iteration 328, loss = 53.13808097\n",
      "Iteration 329, loss = 53.11257048\n",
      "Iteration 330, loss = 53.10562472\n",
      "Iteration 331, loss = 53.05879659\n",
      "Iteration 332, loss = 53.00785844\n",
      "Iteration 333, loss = 52.99297264\n",
      "Iteration 334, loss = 52.97108860\n",
      "Iteration 335, loss = 52.98129396\n",
      "Iteration 336, loss = 52.98361641\n",
      "Iteration 337, loss = 52.84846371\n",
      "Iteration 338, loss = 52.93751004\n",
      "Iteration 339, loss = 52.81799310\n",
      "Iteration 340, loss = 52.87000759\n",
      "Iteration 341, loss = 52.83124236\n",
      "Iteration 342, loss = 52.85108632\n",
      "Iteration 343, loss = 52.80848590\n",
      "Iteration 344, loss = 52.96234102\n",
      "Iteration 345, loss = 52.68952006\n",
      "Iteration 346, loss = 52.72981083\n",
      "Iteration 347, loss = 52.78467284\n",
      "Iteration 348, loss = 52.65842647\n",
      "Iteration 349, loss = 52.71722805\n",
      "Iteration 350, loss = 52.68773205\n",
      "Iteration 351, loss = 52.67013407\n",
      "Iteration 352, loss = 52.73721602\n",
      "Iteration 353, loss = 52.66364315\n",
      "Iteration 354, loss = 52.58787331\n",
      "Iteration 355, loss = 52.75119243\n",
      "Iteration 356, loss = 53.01858857\n",
      "Iteration 357, loss = 53.25301260\n",
      "Iteration 358, loss = 52.65446990\n",
      "Iteration 359, loss = 52.61601381\n",
      "Iteration 360, loss = 52.48108661\n",
      "Iteration 361, loss = 52.62650842\n",
      "Iteration 362, loss = 52.60419628\n",
      "Iteration 363, loss = 52.35922840\n",
      "Iteration 364, loss = 52.40300889\n",
      "Iteration 365, loss = 52.56029296\n",
      "Iteration 366, loss = 52.30906955\n",
      "Iteration 367, loss = 52.37791541\n",
      "Iteration 368, loss = 52.53198737\n",
      "Iteration 369, loss = 52.67800382\n",
      "Iteration 370, loss = 52.53764556\n",
      "Iteration 371, loss = 52.09726913\n",
      "Iteration 372, loss = 52.48066969\n",
      "Iteration 373, loss = 52.38715494\n",
      "Iteration 374, loss = 52.21572375\n",
      "Iteration 375, loss = 52.13142852\n",
      "Iteration 376, loss = 52.23816030\n",
      "Iteration 377, loss = 52.11692730\n",
      "Iteration 378, loss = 52.18994248\n",
      "Iteration 379, loss = 52.10223758\n",
      "Iteration 380, loss = 52.28088640\n",
      "Iteration 381, loss = 52.07172658\n",
      "Iteration 382, loss = 52.24482235\n",
      "Iteration 383, loss = 52.03542124\n",
      "Iteration 384, loss = 51.94984681\n",
      "Iteration 385, loss = 52.14473279\n",
      "Iteration 386, loss = 52.00723822\n",
      "Iteration 387, loss = 51.86719454\n",
      "Iteration 388, loss = 52.00417991\n",
      "Iteration 389, loss = 51.97860038\n",
      "Iteration 390, loss = 51.90882180\n",
      "Iteration 391, loss = 52.04943851\n",
      "Iteration 392, loss = 51.86999559\n",
      "Iteration 393, loss = 52.31626294\n",
      "Iteration 394, loss = 52.63442181\n",
      "Iteration 395, loss = 52.73236953\n",
      "Iteration 396, loss = 51.84970402\n",
      "Iteration 397, loss = 51.80293659\n",
      "Iteration 398, loss = 51.98076436\n",
      "Iteration 399, loss = 51.75474249\n",
      "Iteration 400, loss = 51.75009095\n",
      "Iteration 401, loss = 51.72519879\n",
      "Iteration 402, loss = 51.76459235\n",
      "Iteration 403, loss = 51.70525669\n",
      "Iteration 404, loss = 51.70361480\n",
      "Iteration 405, loss = 51.61894777\n",
      "Iteration 406, loss = 51.62402849\n",
      "Iteration 407, loss = 51.63818921\n",
      "Iteration 408, loss = 51.73203552\n",
      "Iteration 409, loss = 51.72559556\n",
      "Iteration 410, loss = 51.61273231\n",
      "Iteration 411, loss = 51.50828014\n",
      "Iteration 412, loss = 51.55746245\n",
      "Iteration 413, loss = 51.60039365\n",
      "Iteration 414, loss = 51.49560983\n",
      "Iteration 415, loss = 51.50927773\n",
      "Iteration 416, loss = 51.45680858\n",
      "Iteration 417, loss = 51.52336663\n",
      "Iteration 418, loss = 51.51005445\n",
      "Iteration 419, loss = 51.50331212\n",
      "Iteration 420, loss = 51.46777357\n",
      "Iteration 421, loss = 51.39798653\n",
      "Iteration 422, loss = 51.67574023\n",
      "Iteration 423, loss = 51.91651264\n",
      "Iteration 424, loss = 51.33385689\n",
      "Iteration 425, loss = 51.35080494\n",
      "Iteration 426, loss = 51.34187390\n",
      "Iteration 427, loss = 51.28429093\n",
      "Iteration 428, loss = 51.36246521\n",
      "Iteration 429, loss = 51.34572128\n",
      "Iteration 430, loss = 51.64650850\n",
      "Iteration 431, loss = 51.96994878\n",
      "Iteration 432, loss = 51.99658667\n",
      "Iteration 433, loss = 51.85465066\n",
      "Iteration 434, loss = 51.49276112\n",
      "Iteration 435, loss = 51.45064723\n",
      "Iteration 436, loss = 51.24556726\n",
      "Iteration 437, loss = 51.43806834\n",
      "Iteration 438, loss = 51.17841950\n",
      "Iteration 439, loss = 51.25351340\n",
      "Iteration 440, loss = 51.44282419\n",
      "Iteration 441, loss = 50.99322970\n",
      "Iteration 442, loss = 51.14070223\n",
      "Iteration 443, loss = 51.16790121\n",
      "Iteration 444, loss = 51.01485528\n",
      "Iteration 445, loss = 51.04628684\n",
      "Iteration 446, loss = 50.98849609\n",
      "Iteration 447, loss = 50.97839673\n",
      "Iteration 448, loss = 51.05637226\n",
      "Iteration 449, loss = 51.06223595\n",
      "Iteration 450, loss = 50.92222240\n",
      "Iteration 451, loss = 51.07784080\n",
      "Iteration 452, loss = 51.14628897\n",
      "Iteration 453, loss = 50.96586053\n",
      "Iteration 454, loss = 50.90704486\n",
      "Iteration 455, loss = 50.92621824\n",
      "Iteration 456, loss = 51.06231574\n",
      "Iteration 457, loss = 51.34189580\n",
      "Iteration 458, loss = 50.92398281\n",
      "Iteration 459, loss = 50.77525985\n",
      "Iteration 460, loss = 50.83372560\n",
      "Iteration 461, loss = 50.99547800\n",
      "Iteration 462, loss = 50.82862006\n",
      "Iteration 463, loss = 50.76923341\n",
      "Iteration 464, loss = 50.83617851\n",
      "Iteration 465, loss = 51.02468243\n",
      "Iteration 466, loss = 51.23968592\n",
      "Iteration 467, loss = 51.07802495\n",
      "Iteration 468, loss = 51.02048392\n",
      "Iteration 469, loss = 51.15064427\n",
      "Iteration 470, loss = 50.70688128\n",
      "Iteration 471, loss = 50.87815474\n",
      "Iteration 472, loss = 50.94251949\n",
      "Iteration 473, loss = 50.55751145\n",
      "Iteration 474, loss = 50.62984344\n",
      "Iteration 475, loss = 50.58442325\n",
      "Iteration 476, loss = 50.60364346\n",
      "Iteration 477, loss = 50.72475921\n",
      "Iteration 478, loss = 50.82071511\n",
      "Iteration 479, loss = 50.91573891\n",
      "Iteration 480, loss = 50.58504779\n",
      "Iteration 481, loss = 50.82986876\n",
      "Iteration 482, loss = 50.49956598\n",
      "Iteration 483, loss = 50.45180261\n",
      "Iteration 484, loss = 50.41698388\n",
      "Iteration 485, loss = 50.41783369\n",
      "Iteration 486, loss = 50.38201194\n",
      "Iteration 487, loss = 50.33611858\n",
      "Iteration 488, loss = 50.36124081\n",
      "Iteration 489, loss = 50.33160067\n",
      "Iteration 490, loss = 50.75819116\n",
      "Iteration 491, loss = 50.70251503\n",
      "Iteration 492, loss = 50.96452421\n",
      "Iteration 493, loss = 50.65637779\n",
      "Iteration 494, loss = 50.55342224\n",
      "Iteration 495, loss = 50.32931529\n",
      "Iteration 496, loss = 50.48254085\n",
      "Iteration 497, loss = 50.38240336\n",
      "Iteration 498, loss = 50.29784533\n",
      "Iteration 499, loss = 50.35483554\n",
      "Iteration 500, loss = 50.18220681\n",
      "Iteration 501, loss = 50.21287274\n",
      "Iteration 502, loss = 50.14620846\n",
      "Iteration 503, loss = 50.19946774\n",
      "Iteration 504, loss = 50.06120160\n",
      "Iteration 505, loss = 50.32258552\n",
      "Iteration 506, loss = 50.09164026\n",
      "Iteration 507, loss = 50.03172896\n",
      "Iteration 508, loss = 50.14066360\n",
      "Iteration 509, loss = 50.04672641\n",
      "Iteration 510, loss = 50.01691398\n",
      "Iteration 511, loss = 50.08438181\n",
      "Iteration 512, loss = 50.00444494\n",
      "Iteration 513, loss = 50.00792033\n",
      "Iteration 514, loss = 49.92052518\n",
      "Iteration 515, loss = 50.07266308\n",
      "Iteration 516, loss = 49.94217358\n",
      "Iteration 517, loss = 50.18586449\n",
      "Iteration 518, loss = 49.98820491\n",
      "Iteration 519, loss = 49.86021152\n",
      "Iteration 520, loss = 49.88216403\n",
      "Iteration 521, loss = 50.08970696\n",
      "Iteration 522, loss = 49.76668968\n",
      "Iteration 523, loss = 49.90271167\n",
      "Iteration 524, loss = 50.16430280\n",
      "Iteration 525, loss = 50.31455110\n",
      "Iteration 526, loss = 49.84449621\n",
      "Iteration 527, loss = 50.02267221\n",
      "Iteration 528, loss = 50.02678888\n",
      "Iteration 529, loss = 49.89594196\n",
      "Iteration 530, loss = 49.86518374\n",
      "Iteration 531, loss = 49.75279471\n",
      "Iteration 532, loss = 49.92725311\n",
      "Iteration 533, loss = 49.79875263\n",
      "Iteration 534, loss = 49.76434783\n",
      "Iteration 535, loss = 49.75420867\n",
      "Iteration 536, loss = 49.86541667\n",
      "Iteration 537, loss = 50.28007005\n",
      "Iteration 538, loss = 49.74761637\n",
      "Iteration 539, loss = 49.56744794\n",
      "Iteration 540, loss = 49.59682813\n",
      "Iteration 541, loss = 49.73364159\n",
      "Iteration 542, loss = 49.57071130\n",
      "Iteration 543, loss = 49.76156043\n",
      "Iteration 544, loss = 49.99698758\n",
      "Iteration 545, loss = 50.31367604\n",
      "Iteration 546, loss = 51.07367829\n",
      "Iteration 547, loss = 49.80177779\n",
      "Iteration 548, loss = 49.51876524\n",
      "Iteration 549, loss = 49.48449506\n",
      "Iteration 550, loss = 49.43199554\n",
      "Iteration 551, loss = 49.37343250\n",
      "Iteration 552, loss = 49.69725308\n",
      "Iteration 553, loss = 49.63663581\n",
      "Iteration 554, loss = 49.46334206\n",
      "Iteration 555, loss = 49.29511805\n",
      "Iteration 556, loss = 49.61197764\n",
      "Iteration 557, loss = 49.25864537\n",
      "Iteration 558, loss = 49.32862790\n",
      "Iteration 559, loss = 49.42988303\n",
      "Iteration 560, loss = 49.57305906\n",
      "Iteration 561, loss = 49.64676266\n",
      "Iteration 562, loss = 49.36415924\n",
      "Iteration 563, loss = 49.17773301\n",
      "Iteration 564, loss = 49.32203987\n",
      "Iteration 565, loss = 49.38528512\n",
      "Iteration 566, loss = 49.72585317\n",
      "Iteration 567, loss = 49.67768426\n",
      "Iteration 568, loss = 49.34658415\n",
      "Iteration 569, loss = 49.23210412\n",
      "Iteration 570, loss = 49.33580586\n",
      "Iteration 571, loss = 49.23851290\n",
      "Iteration 572, loss = 49.17209984\n",
      "Iteration 573, loss = 49.13937120\n",
      "Iteration 574, loss = 49.04803948\n",
      "Iteration 575, loss = 49.08157618\n",
      "Iteration 576, loss = 49.06387881\n",
      "Iteration 577, loss = 48.99244741\n",
      "Iteration 578, loss = 49.00092507\n",
      "Iteration 579, loss = 49.09154666\n",
      "Iteration 580, loss = 49.12642571\n",
      "Iteration 581, loss = 49.00378595\n",
      "Iteration 582, loss = 48.96145297\n",
      "Iteration 583, loss = 48.93725468\n",
      "Iteration 584, loss = 48.87188630\n",
      "Iteration 585, loss = 48.89568898\n",
      "Iteration 586, loss = 49.18281884\n",
      "Iteration 587, loss = 49.17955083\n",
      "Iteration 588, loss = 49.04774403\n",
      "Iteration 589, loss = 48.80730168\n",
      "Iteration 590, loss = 48.97350474\n",
      "Iteration 591, loss = 48.82202678\n",
      "Iteration 592, loss = 48.84897167\n",
      "Iteration 593, loss = 49.37447200\n",
      "Iteration 594, loss = 48.96278068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 595, loss = 48.78324345\n",
      "Iteration 596, loss = 49.23815348\n",
      "Iteration 597, loss = 48.75925630\n",
      "Iteration 598, loss = 48.78530837\n",
      "Iteration 599, loss = 48.74861936\n",
      "Iteration 600, loss = 48.72186949\n",
      "Iteration 601, loss = 48.61828415\n",
      "Iteration 602, loss = 48.62346406\n",
      "Iteration 603, loss = 48.57871969\n",
      "Iteration 604, loss = 48.63922448\n",
      "Iteration 605, loss = 48.58671973\n",
      "Iteration 606, loss = 48.62596795\n",
      "Iteration 607, loss = 48.69664203\n",
      "Iteration 608, loss = 48.63259914\n",
      "Iteration 609, loss = 48.44947699\n",
      "Iteration 610, loss = 48.57220197\n",
      "Iteration 611, loss = 48.48169979\n",
      "Iteration 612, loss = 48.50284377\n",
      "Iteration 613, loss = 48.47591589\n",
      "Iteration 614, loss = 48.42775168\n",
      "Iteration 615, loss = 48.45840850\n",
      "Iteration 616, loss = 48.39805208\n",
      "Iteration 617, loss = 48.44905066\n",
      "Iteration 618, loss = 48.45935136\n",
      "Iteration 619, loss = 48.40844281\n",
      "Iteration 620, loss = 48.39015206\n",
      "Iteration 621, loss = 48.37539444\n",
      "Iteration 622, loss = 48.29994843\n",
      "Iteration 623, loss = 48.55442551\n",
      "Iteration 624, loss = 48.53759098\n",
      "Iteration 625, loss = 48.58804264\n",
      "Iteration 626, loss = 48.25707093\n",
      "Iteration 627, loss = 48.21061040\n",
      "Iteration 628, loss = 48.25194511\n",
      "Iteration 629, loss = 48.20328045\n",
      "Iteration 630, loss = 48.25111488\n",
      "Iteration 631, loss = 48.22597869\n",
      "Iteration 632, loss = 48.21121146\n",
      "Iteration 633, loss = 48.24920095\n",
      "Iteration 634, loss = 48.21021272\n",
      "Iteration 635, loss = 48.15976866\n",
      "Iteration 636, loss = 48.28743788\n",
      "Iteration 637, loss = 48.13231764\n",
      "Iteration 638, loss = 48.15583041\n",
      "Iteration 639, loss = 48.08443246\n",
      "Iteration 640, loss = 48.05036865\n",
      "Iteration 641, loss = 48.19047553\n",
      "Iteration 642, loss = 48.06446646\n",
      "Iteration 643, loss = 48.10602954\n",
      "Iteration 644, loss = 48.01352478\n",
      "Iteration 645, loss = 48.21067363\n",
      "Iteration 646, loss = 48.20820768\n",
      "Iteration 647, loss = 48.15848503\n",
      "Iteration 648, loss = 47.92777802\n",
      "Iteration 649, loss = 47.89947426\n",
      "Iteration 650, loss = 48.27252761\n",
      "Iteration 651, loss = 48.45540146\n",
      "Iteration 652, loss = 48.81663055\n",
      "Iteration 653, loss = 49.55840603\n",
      "Iteration 654, loss = 49.69091591\n",
      "Iteration 655, loss = 48.81271956\n",
      "Iteration 656, loss = 48.39370851\n",
      "Iteration 657, loss = 48.45954101\n",
      "Iteration 658, loss = 49.30787230\n",
      "Iteration 659, loss = 48.79523035\n",
      "Iteration 660, loss = 48.07384271\n",
      "Iteration 661, loss = 47.78472692\n",
      "Iteration 662, loss = 47.97484606\n",
      "Iteration 663, loss = 48.23921117\n",
      "Iteration 664, loss = 48.33521785\n",
      "Iteration 665, loss = 48.25821866\n",
      "Iteration 666, loss = 47.87913565\n",
      "Iteration 667, loss = 48.27890519\n",
      "Iteration 668, loss = 48.17172357\n",
      "Iteration 669, loss = 47.86632960\n",
      "Iteration 670, loss = 47.94650695\n",
      "Iteration 671, loss = 47.77021137\n",
      "Iteration 672, loss = 47.59209568\n",
      "Iteration 673, loss = 47.64560180\n",
      "Iteration 674, loss = 47.74538822\n",
      "Iteration 675, loss = 47.52417975\n",
      "Iteration 676, loss = 47.66457738\n",
      "Iteration 677, loss = 47.59450944\n",
      "Iteration 678, loss = 47.57136206\n",
      "Iteration 679, loss = 47.96041961\n",
      "Iteration 680, loss = 47.66474259\n",
      "Iteration 681, loss = 47.43534413\n",
      "Iteration 682, loss = 47.49717921\n",
      "Iteration 683, loss = 47.54481444\n",
      "Iteration 684, loss = 47.39158277\n",
      "Iteration 685, loss = 47.42458275\n",
      "Iteration 686, loss = 47.41603759\n",
      "Iteration 687, loss = 47.42442851\n",
      "Iteration 688, loss = 47.40473746\n",
      "Iteration 689, loss = 47.49552112\n",
      "Iteration 690, loss = 47.45820193\n",
      "Iteration 691, loss = 47.42584593\n",
      "Iteration 692, loss = 47.41067792\n",
      "Iteration 693, loss = 47.37555689\n",
      "Iteration 694, loss = 47.39383081\n",
      "Iteration 695, loss = 47.48732066\n",
      "Iteration 696, loss = 47.24662722\n",
      "Iteration 697, loss = 47.23394438\n",
      "Iteration 698, loss = 47.26750821\n",
      "Iteration 699, loss = 47.55583060\n",
      "Iteration 700, loss = 48.16890314\n",
      "Iteration 701, loss = 48.17931591\n",
      "Iteration 702, loss = 48.09454300\n",
      "Iteration 703, loss = 47.63021876\n",
      "Iteration 704, loss = 47.63568975\n",
      "Iteration 705, loss = 47.43580598\n",
      "Iteration 706, loss = 47.20594460\n",
      "Iteration 707, loss = 47.09608499\n",
      "Iteration 708, loss = 47.14995004\n",
      "Iteration 709, loss = 47.46257496\n",
      "Iteration 710, loss = 47.17013521\n",
      "Iteration 711, loss = 47.07533242\n",
      "Iteration 712, loss = 47.34331723\n",
      "Iteration 713, loss = 47.43730164\n",
      "Iteration 714, loss = 47.88790110\n",
      "Iteration 715, loss = 48.03781793\n",
      "Iteration 716, loss = 47.80124877\n",
      "Iteration 717, loss = 47.69968839\n",
      "Iteration 718, loss = 47.60546255\n",
      "Iteration 719, loss = 47.58155518\n",
      "Iteration 720, loss = 47.40646645\n",
      "Iteration 721, loss = 47.03348093\n",
      "Iteration 722, loss = 47.26242624\n",
      "Iteration 723, loss = 46.99460769\n",
      "Iteration 724, loss = 46.83753367\n",
      "Iteration 725, loss = 46.80960617\n",
      "Iteration 726, loss = 47.07810688\n",
      "Iteration 727, loss = 47.05493143\n",
      "Iteration 728, loss = 47.10666727\n",
      "Iteration 729, loss = 47.14701654\n",
      "Iteration 730, loss = 46.82785770\n",
      "Iteration 731, loss = 46.76301095\n",
      "Iteration 732, loss = 46.91889354\n",
      "Iteration 733, loss = 46.76781596\n",
      "Iteration 734, loss = 46.92037254\n",
      "Iteration 735, loss = 47.00180112\n",
      "Iteration 736, loss = 47.26045326\n",
      "Iteration 737, loss = 47.40429901\n",
      "Iteration 738, loss = 47.20145469\n",
      "Iteration 739, loss = 46.98253151\n",
      "Iteration 740, loss = 46.76454053\n",
      "Iteration 741, loss = 46.71265444\n",
      "Iteration 742, loss = 46.83866675\n",
      "Iteration 743, loss = 46.64041208\n",
      "Iteration 744, loss = 46.59107131\n",
      "Iteration 745, loss = 46.75229978\n",
      "Iteration 746, loss = 46.59410190\n",
      "Iteration 747, loss = 46.52422538\n",
      "Iteration 748, loss = 46.62770749\n",
      "Iteration 749, loss = 46.57870371\n",
      "Iteration 750, loss = 46.64251859\n",
      "Iteration 751, loss = 46.58995161\n",
      "Iteration 752, loss = 46.58764749\n",
      "Iteration 753, loss = 46.45910913\n",
      "Iteration 754, loss = 46.67170236\n",
      "Iteration 755, loss = 46.83741922\n",
      "Iteration 756, loss = 47.00303265\n",
      "Iteration 757, loss = 46.97614763\n",
      "Iteration 758, loss = 46.84416844\n",
      "Iteration 759, loss = 46.71815326\n",
      "Iteration 760, loss = 46.64789187\n",
      "Iteration 761, loss = 46.61450957\n",
      "Iteration 762, loss = 46.80749858\n",
      "Iteration 763, loss = 46.97735832\n",
      "Iteration 764, loss = 46.34229835\n",
      "Iteration 765, loss = 46.68549171\n",
      "Iteration 766, loss = 46.79309825\n",
      "Iteration 767, loss = 46.28094960\n",
      "Iteration 768, loss = 46.45661029\n",
      "Iteration 769, loss = 46.33443689\n",
      "Iteration 770, loss = 46.54250361\n",
      "Iteration 771, loss = 46.66615605\n",
      "Iteration 772, loss = 46.70534920\n",
      "Iteration 773, loss = 46.73740202\n",
      "Iteration 774, loss = 46.32826610\n",
      "Iteration 775, loss = 46.42541329\n",
      "Iteration 776, loss = 46.28563921\n",
      "Iteration 777, loss = 46.23011559\n",
      "Iteration 778, loss = 46.24447055\n",
      "Iteration 779, loss = 46.16930880\n",
      "Iteration 780, loss = 46.14397235\n",
      "Iteration 781, loss = 46.23072568\n",
      "Iteration 782, loss = 46.34292290\n",
      "Iteration 783, loss = 46.25443495\n",
      "Iteration 784, loss = 46.68910704\n",
      "Iteration 785, loss = 46.23217876\n",
      "Iteration 786, loss = 46.12148965\n",
      "Iteration 787, loss = 46.09375429\n",
      "Iteration 788, loss = 46.06459687\n",
      "Iteration 789, loss = 46.08765500\n",
      "Iteration 790, loss = 46.20695052\n",
      "Iteration 791, loss = 45.94752081\n",
      "Iteration 792, loss = 46.08155195\n",
      "Iteration 793, loss = 45.93065361\n",
      "Iteration 794, loss = 46.45886107\n",
      "Iteration 795, loss = 46.75314178\n",
      "Iteration 796, loss = 46.51549230\n",
      "Iteration 797, loss = 46.44266425\n",
      "Iteration 798, loss = 46.06196664\n",
      "Iteration 799, loss = 45.87356459\n",
      "Iteration 800, loss = 45.86958013\n",
      "Iteration 801, loss = 45.86273893\n",
      "Iteration 802, loss = 45.92770913\n",
      "Iteration 803, loss = 45.90586702\n",
      "Iteration 804, loss = 45.99811069\n",
      "Iteration 805, loss = 46.27644402\n",
      "Iteration 806, loss = 46.18140490\n",
      "Iteration 807, loss = 46.09454279\n",
      "Iteration 808, loss = 45.93319115\n",
      "Iteration 809, loss = 46.00856855\n",
      "Iteration 810, loss = 46.05100281\n",
      "Iteration 811, loss = 45.81347884\n",
      "Iteration 812, loss = 45.77983615\n",
      "Iteration 813, loss = 45.78610688\n",
      "Iteration 814, loss = 46.00579261\n",
      "Iteration 815, loss = 45.77261009\n",
      "Iteration 816, loss = 45.57185039\n",
      "Iteration 817, loss = 45.77253279\n",
      "Iteration 818, loss = 46.08370020\n",
      "Iteration 819, loss = 46.08873866\n",
      "Iteration 820, loss = 46.33520412\n",
      "Iteration 821, loss = 46.41028157\n",
      "Iteration 822, loss = 45.57392081\n",
      "Iteration 823, loss = 45.62523177\n",
      "Iteration 824, loss = 45.85886570\n",
      "Iteration 825, loss = 45.75415544\n",
      "Iteration 826, loss = 45.79018302\n",
      "Iteration 827, loss = 45.65397352\n",
      "Iteration 828, loss = 45.84596346\n",
      "Iteration 829, loss = 45.78907931\n",
      "Iteration 830, loss = 45.82888089\n",
      "Iteration 831, loss = 45.77221635\n",
      "Iteration 832, loss = 45.82882037\n",
      "Iteration 833, loss = 45.55055986\n",
      "Iteration 834, loss = 45.56908060\n",
      "Iteration 835, loss = 45.93942275\n",
      "Iteration 836, loss = 45.76610909\n",
      "Iteration 837, loss = 46.05943005\n",
      "Iteration 838, loss = 46.94746553\n",
      "Iteration 839, loss = 45.25509660\n",
      "Iteration 840, loss = 45.84435874\n",
      "Iteration 841, loss = 45.77013166\n",
      "Iteration 842, loss = 45.34622934\n",
      "Iteration 843, loss = 45.44655927\n",
      "Iteration 844, loss = 45.82663534\n",
      "Iteration 845, loss = 45.33227652\n",
      "Iteration 846, loss = 45.29545761\n",
      "Iteration 847, loss = 45.75374851\n",
      "Iteration 848, loss = 45.99768376\n",
      "Iteration 849, loss = 45.19313277\n",
      "Iteration 850, loss = 45.61717927\n",
      "Iteration 851, loss = 46.04691812\n",
      "Iteration 852, loss = 46.04597932\n",
      "Iteration 853, loss = 45.27213796\n",
      "Iteration 854, loss = 45.18315285\n",
      "Iteration 855, loss = 45.56586132\n",
      "Iteration 856, loss = 45.85777871\n",
      "Iteration 857, loss = 46.24132926\n",
      "Iteration 858, loss = 45.78776905\n",
      "Iteration 859, loss = 45.94086244\n",
      "Iteration 860, loss = 46.78664102\n",
      "Iteration 861, loss = 47.11588786\n",
      "Iteration 862, loss = 46.11624564\n",
      "Iteration 863, loss = 45.98402787\n",
      "Iteration 864, loss = 45.76289163\n",
      "Iteration 865, loss = 45.34430999\n",
      "Iteration 866, loss = 45.43522842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 867, loss = 45.15009846\n",
      "Iteration 868, loss = 45.28212014\n",
      "Iteration 869, loss = 45.49696234\n",
      "Iteration 870, loss = 45.19837532\n",
      "Iteration 871, loss = 45.07960906\n",
      "Iteration 872, loss = 45.16047799\n",
      "Iteration 873, loss = 44.92077455\n",
      "Iteration 874, loss = 45.11694312\n",
      "Iteration 875, loss = 45.20259827\n",
      "Iteration 876, loss = 45.25255638\n",
      "Iteration 877, loss = 45.17532141\n",
      "Iteration 878, loss = 45.54691102\n",
      "Iteration 879, loss = 46.00681676\n",
      "Iteration 880, loss = 45.55137824\n",
      "Iteration 881, loss = 44.73320911\n",
      "Iteration 882, loss = 45.32334446\n",
      "Iteration 883, loss = 45.21829067\n",
      "Iteration 884, loss = 45.03462940\n",
      "Iteration 885, loss = 45.39247474\n",
      "Iteration 886, loss = 45.20518911\n",
      "Iteration 887, loss = 45.40086327\n",
      "Iteration 888, loss = 45.17953145\n",
      "Iteration 889, loss = 45.28870311\n",
      "Iteration 890, loss = 45.64926990\n",
      "Iteration 891, loss = 44.82220018\n",
      "Iteration 892, loss = 44.75500950\n",
      "Iteration 893, loss = 45.09032427\n",
      "Iteration 894, loss = 44.84773409\n",
      "Iteration 895, loss = 44.70089265\n",
      "Iteration 896, loss = 44.80326974\n",
      "Iteration 897, loss = 44.91174052\n",
      "Iteration 898, loss = 44.83349480\n",
      "Iteration 899, loss = 44.81333360\n",
      "Iteration 900, loss = 45.35475165\n",
      "Iteration 901, loss = 45.27830052\n",
      "Iteration 902, loss = 44.71175288\n",
      "Iteration 903, loss = 45.11041186\n",
      "Iteration 904, loss = 44.59235530\n",
      "Iteration 905, loss = 44.46438799\n",
      "Iteration 906, loss = 44.51596836\n",
      "Iteration 907, loss = 44.62532308\n",
      "Iteration 908, loss = 45.00796230\n",
      "Iteration 909, loss = 44.60204084\n",
      "Iteration 910, loss = 44.71551144\n",
      "Iteration 911, loss = 44.54746309\n",
      "Iteration 912, loss = 44.52404125\n",
      "Iteration 913, loss = 44.56035941\n",
      "Iteration 914, loss = 44.41911101\n",
      "Iteration 915, loss = 44.44642393\n",
      "Iteration 916, loss = 44.44298356\n",
      "Iteration 917, loss = 44.52474097\n",
      "Iteration 918, loss = 44.85062700\n",
      "Iteration 919, loss = 44.35330876\n",
      "Iteration 920, loss = 44.81665581\n",
      "Iteration 921, loss = 44.80830623\n",
      "Iteration 922, loss = 44.50659607\n",
      "Iteration 923, loss = 44.53093830\n",
      "Iteration 924, loss = 44.44124791\n",
      "Iteration 925, loss = 44.38789789\n",
      "Iteration 926, loss = 44.99684476\n",
      "Iteration 927, loss = 44.59795575\n",
      "Iteration 928, loss = 44.50318234\n",
      "Iteration 929, loss = 44.22334427\n",
      "Iteration 930, loss = 44.33660196\n",
      "Iteration 931, loss = 44.42697514\n",
      "Iteration 932, loss = 44.44123196\n",
      "Iteration 933, loss = 44.22279485\n",
      "Iteration 934, loss = 44.76163502\n",
      "Iteration 935, loss = 44.75814848\n",
      "Iteration 936, loss = 44.61039614\n",
      "Iteration 937, loss = 44.28926926\n",
      "Iteration 938, loss = 44.78611667\n",
      "Iteration 939, loss = 44.48782856\n",
      "Iteration 940, loss = 44.46155468\n",
      "Iteration 941, loss = 44.87078409\n",
      "Iteration 942, loss = 45.51812772\n",
      "Iteration 943, loss = 44.55488584\n",
      "Iteration 944, loss = 44.44714735\n",
      "Iteration 945, loss = 44.43583555\n",
      "Iteration 946, loss = 44.80777234\n",
      "Iteration 947, loss = 44.21775757\n",
      "Iteration 948, loss = 44.28288976\n",
      "Iteration 949, loss = 44.17328421\n",
      "Iteration 950, loss = 44.09492143\n",
      "Iteration 951, loss = 44.76695629\n",
      "Iteration 952, loss = 44.19186693\n",
      "Iteration 953, loss = 44.25564337\n",
      "Iteration 954, loss = 43.97353404\n",
      "Iteration 955, loss = 44.25982733\n",
      "Iteration 956, loss = 43.90562837\n",
      "Iteration 957, loss = 44.52751581\n",
      "Iteration 958, loss = 44.19410371\n",
      "Iteration 959, loss = 44.21168065\n",
      "Iteration 960, loss = 44.21593428\n",
      "Iteration 961, loss = 44.60973977\n",
      "Iteration 962, loss = 44.03074284\n",
      "Iteration 963, loss = 43.97246078\n",
      "Iteration 964, loss = 43.96616239\n",
      "Iteration 965, loss = 43.86202696\n",
      "Iteration 966, loss = 43.91789524\n",
      "Iteration 967, loss = 43.95422234\n",
      "Iteration 968, loss = 44.16983393\n",
      "Iteration 969, loss = 43.97862521\n",
      "Iteration 970, loss = 43.98936014\n",
      "Iteration 971, loss = 43.97639134\n",
      "Iteration 972, loss = 44.20226483\n",
      "Iteration 973, loss = 43.98499024\n",
      "Iteration 974, loss = 44.06329840\n",
      "Iteration 975, loss = 43.93916839\n",
      "Iteration 976, loss = 44.11143937\n",
      "Iteration 977, loss = 43.85272298\n",
      "Iteration 978, loss = 43.75423097\n",
      "Iteration 979, loss = 44.56584407\n",
      "Iteration 980, loss = 44.73203566\n",
      "Iteration 981, loss = 44.58047547\n",
      "Iteration 982, loss = 44.69782116\n",
      "Iteration 983, loss = 44.06434548\n",
      "Iteration 984, loss = 43.61164382\n",
      "Iteration 985, loss = 43.64425608\n",
      "Iteration 986, loss = 43.86953413\n",
      "Iteration 987, loss = 43.72010598\n",
      "Iteration 988, loss = 44.07817070\n",
      "Iteration 989, loss = 44.20583547\n",
      "Iteration 990, loss = 44.11003479\n",
      "Iteration 991, loss = 44.46984369\n",
      "Iteration 992, loss = 44.85381287\n",
      "Iteration 993, loss = 43.93619125\n",
      "Iteration 994, loss = 43.80184272\n",
      "Iteration 995, loss = 43.75052584\n",
      "Iteration 996, loss = 44.84393555\n",
      "Iteration 997, loss = 44.89335009\n",
      "Iteration 998, loss = 43.90966628\n",
      "Iteration 999, loss = 44.66138992\n",
      "Iteration 1000, loss = 44.55814041\n",
      "Iteration 1001, loss = 43.99292266\n",
      "Iteration 1002, loss = 43.56902957\n",
      "Iteration 1003, loss = 43.77925221\n",
      "Iteration 1004, loss = 44.53730042\n",
      "Iteration 1005, loss = 43.82195678\n",
      "Iteration 1006, loss = 43.57613225\n",
      "Iteration 1007, loss = 43.37800754\n",
      "Iteration 1008, loss = 43.30861582\n",
      "Iteration 1009, loss = 43.24569218\n",
      "Iteration 1010, loss = 43.62131942\n",
      "Iteration 1011, loss = 44.45592292\n",
      "Iteration 1012, loss = 43.51337381\n",
      "Iteration 1013, loss = 43.36106016\n",
      "Iteration 1014, loss = 43.25049774\n",
      "Iteration 1015, loss = 43.27179978\n",
      "Iteration 1016, loss = 43.17170361\n",
      "Iteration 1017, loss = 43.21618401\n",
      "Iteration 1018, loss = 43.74315508\n",
      "Iteration 1019, loss = 43.69005804\n",
      "Iteration 1020, loss = 43.31101638\n",
      "Iteration 1021, loss = 43.29658255\n",
      "Iteration 1022, loss = 43.45116387\n",
      "Iteration 1023, loss = 43.32101995\n",
      "Iteration 1024, loss = 43.19675958\n",
      "Iteration 1025, loss = 43.05274872\n",
      "Iteration 1026, loss = 43.19223791\n",
      "Iteration 1027, loss = 43.13181868\n",
      "Iteration 1028, loss = 43.10406066\n",
      "Iteration 1029, loss = 43.15120020\n",
      "Iteration 1030, loss = 43.17017304\n",
      "Iteration 1031, loss = 43.89041173\n",
      "Iteration 1032, loss = 44.69148226\n",
      "Iteration 1033, loss = 45.29928074\n",
      "Iteration 1034, loss = 45.24551279\n",
      "Iteration 1035, loss = 42.93165877\n",
      "Iteration 1036, loss = 43.23166462\n",
      "Iteration 1037, loss = 43.08188727\n",
      "Iteration 1038, loss = 43.02226198\n",
      "Iteration 1039, loss = 43.00326022\n",
      "Iteration 1040, loss = 43.03598904\n",
      "Iteration 1041, loss = 42.93590935\n",
      "Iteration 1042, loss = 43.16995213\n",
      "Iteration 1043, loss = 43.03160494\n",
      "Iteration 1044, loss = 43.04989833\n",
      "Iteration 1045, loss = 43.11093124\n",
      "Iteration 1046, loss = 42.95269482\n",
      "Iteration 1047, loss = 43.15734002\n",
      "Iteration 1048, loss = 44.47070456\n",
      "Iteration 1049, loss = 43.34188001\n",
      "Iteration 1050, loss = 43.07864994\n",
      "Iteration 1051, loss = 43.12598705\n",
      "Iteration 1052, loss = 43.05259382\n",
      "Iteration 1053, loss = 43.03578747\n",
      "Iteration 1054, loss = 43.26048861\n",
      "Iteration 1055, loss = 42.80306641\n",
      "Iteration 1056, loss = 43.06679719\n",
      "Iteration 1057, loss = 43.50016222\n",
      "Iteration 1058, loss = 43.40199317\n",
      "Iteration 1059, loss = 43.47276489\n",
      "Iteration 1060, loss = 44.12966374\n",
      "Iteration 1061, loss = 44.55268183\n",
      "Iteration 1062, loss = 43.61347127\n",
      "Iteration 1063, loss = 44.32869310\n",
      "Iteration 1064, loss = 43.22614833\n",
      "Iteration 1065, loss = 43.84291518\n",
      "Iteration 1066, loss = 43.67951687\n",
      "Iteration 1067, loss = 42.55315537\n",
      "Iteration 1068, loss = 43.31170383\n",
      "Iteration 1069, loss = 44.25812240\n",
      "Iteration 1070, loss = 46.09689170\n",
      "Iteration 1071, loss = 45.90944512\n",
      "Iteration 1072, loss = 44.12501925\n",
      "Iteration 1073, loss = 42.93752278\n",
      "Iteration 1074, loss = 42.93744874\n",
      "Iteration 1075, loss = 44.36926449\n",
      "Iteration 1076, loss = 42.86183691\n",
      "Iteration 1077, loss = 42.45492598\n",
      "Iteration 1078, loss = 42.73549623\n",
      "Iteration 1079, loss = 43.01880745\n",
      "Iteration 1080, loss = 43.15385981\n",
      "Iteration 1081, loss = 42.59203078\n",
      "Iteration 1082, loss = 42.73591850\n",
      "Iteration 1083, loss = 42.86268149\n",
      "Iteration 1084, loss = 43.84328528\n",
      "Iteration 1085, loss = 43.54592089\n",
      "Iteration 1086, loss = 43.41912787\n",
      "Iteration 1087, loss = 43.67402320\n",
      "Iteration 1088, loss = 43.72700507\n",
      "Iteration 1089, loss = 44.43722369\n",
      "Iteration 1090, loss = 43.04710190\n",
      "Iteration 1091, loss = 42.67120799\n",
      "Iteration 1092, loss = 42.55915368\n",
      "Iteration 1093, loss = 42.46215278\n",
      "Iteration 1094, loss = 42.45407810\n",
      "Iteration 1095, loss = 42.67109937\n",
      "Iteration 1096, loss = 42.41437354\n",
      "Iteration 1097, loss = 42.50384023\n",
      "Iteration 1098, loss = 42.54515266\n",
      "Iteration 1099, loss = 42.93241363\n",
      "Iteration 1100, loss = 42.77139666\n",
      "Iteration 1101, loss = 43.49917470\n",
      "Iteration 1102, loss = 44.38890777\n",
      "Iteration 1103, loss = 42.44964797\n",
      "Iteration 1104, loss = 42.68720035\n",
      "Iteration 1105, loss = 42.53438707\n",
      "Iteration 1106, loss = 42.28161686\n",
      "Iteration 1107, loss = 42.89187332\n",
      "Iteration 1108, loss = 42.35090480\n",
      "Iteration 1109, loss = 42.53381983\n",
      "Iteration 1110, loss = 42.29366074\n",
      "Iteration 1111, loss = 42.37375352\n",
      "Iteration 1112, loss = 42.21445336\n",
      "Iteration 1113, loss = 42.23984551\n",
      "Iteration 1114, loss = 42.32087070\n",
      "Iteration 1115, loss = 42.23597117\n",
      "Iteration 1116, loss = 42.51658586\n",
      "Iteration 1117, loss = 42.13488334\n",
      "Iteration 1118, loss = 42.26732841\n",
      "Iteration 1119, loss = 42.38164672\n",
      "Iteration 1120, loss = 42.20312580\n",
      "Iteration 1121, loss = 42.30486666\n",
      "Iteration 1122, loss = 42.08197483\n",
      "Iteration 1123, loss = 42.23882639\n",
      "Iteration 1124, loss = 42.25765192\n",
      "Iteration 1125, loss = 42.06259249\n",
      "Iteration 1126, loss = 42.47487182\n",
      "Iteration 1127, loss = 43.04598576\n",
      "Iteration 1128, loss = 43.05391566\n",
      "Iteration 1129, loss = 42.43015590\n",
      "Iteration 1130, loss = 42.45044379\n",
      "Iteration 1131, loss = 42.62730425\n",
      "Iteration 1132, loss = 42.04328564\n",
      "Iteration 1133, loss = 42.41310239\n",
      "Iteration 1134, loss = 42.21071318\n",
      "Iteration 1135, loss = 42.06866907\n",
      "Iteration 1136, loss = 43.13323766\n",
      "Iteration 1137, loss = 44.22853712\n",
      "Iteration 1138, loss = 42.80166651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1139, loss = 43.53551684\n",
      "Iteration 1140, loss = 42.61201429\n",
      "Iteration 1141, loss = 42.05356584\n",
      "Iteration 1142, loss = 41.95302722\n",
      "Iteration 1143, loss = 41.87946953\n",
      "Iteration 1144, loss = 43.09168453\n",
      "Iteration 1145, loss = 42.58918713\n",
      "Iteration 1146, loss = 42.75753215\n",
      "Iteration 1147, loss = 42.40943558\n",
      "Iteration 1148, loss = 42.03689448\n",
      "Iteration 1149, loss = 41.82680373\n",
      "Iteration 1150, loss = 42.24262814\n",
      "Iteration 1151, loss = 42.47306879\n",
      "Iteration 1152, loss = 41.83300279\n",
      "Iteration 1153, loss = 42.94507139\n",
      "Iteration 1154, loss = 43.17981855\n",
      "Iteration 1155, loss = 42.54914914\n",
      "Iteration 1156, loss = 42.18268528\n",
      "Iteration 1157, loss = 42.24258469\n",
      "Iteration 1158, loss = 41.82278761\n",
      "Iteration 1159, loss = 42.13342030\n",
      "Iteration 1160, loss = 41.85716588\n",
      "Iteration 1161, loss = 42.35509379\n",
      "Iteration 1162, loss = 43.49827004\n",
      "Iteration 1163, loss = 42.21169031\n",
      "Iteration 1164, loss = 42.54938101\n",
      "Iteration 1165, loss = 41.75555684\n",
      "Iteration 1166, loss = 41.79579943\n",
      "Iteration 1167, loss = 41.81467309\n",
      "Iteration 1168, loss = 42.23331179\n",
      "Iteration 1169, loss = 42.08564857\n",
      "Iteration 1170, loss = 42.45996197\n",
      "Iteration 1171, loss = 43.21950358\n",
      "Iteration 1172, loss = 42.59268502\n",
      "Iteration 1173, loss = 43.89668201\n",
      "Iteration 1174, loss = 43.35563870\n",
      "Iteration 1175, loss = 42.06526812\n",
      "Iteration 1176, loss = 42.59505539\n",
      "Iteration 1177, loss = 43.86473522\n",
      "Iteration 1178, loss = 41.72573617\n",
      "Iteration 1179, loss = 41.82979838\n",
      "Iteration 1180, loss = 42.41776944\n",
      "Iteration 1181, loss = 42.12185724\n",
      "Iteration 1182, loss = 41.71860460\n",
      "Iteration 1183, loss = 41.89913585\n",
      "Iteration 1184, loss = 41.86324632\n",
      "Iteration 1185, loss = 42.52754365\n",
      "Iteration 1186, loss = 42.85127699\n",
      "Iteration 1187, loss = 41.43620277\n",
      "Iteration 1188, loss = 41.63541057\n",
      "Iteration 1189, loss = 41.42155815\n",
      "Iteration 1190, loss = 41.52766404\n",
      "Iteration 1191, loss = 41.46782842\n",
      "Iteration 1192, loss = 41.57140786\n",
      "Iteration 1193, loss = 41.33763821\n",
      "Iteration 1194, loss = 41.27220479\n",
      "Iteration 1195, loss = 41.35045037\n",
      "Iteration 1196, loss = 41.23374213\n",
      "Iteration 1197, loss = 41.26055667\n",
      "Iteration 1198, loss = 41.49888785\n",
      "Iteration 1199, loss = 41.34370438\n",
      "Iteration 1200, loss = 41.24370043\n",
      "Iteration 1201, loss = 41.37285082\n",
      "Iteration 1202, loss = 41.34475904\n",
      "Iteration 1203, loss = 41.25852155\n",
      "Iteration 1204, loss = 41.64150310\n",
      "Iteration 1205, loss = 41.61892711\n",
      "Iteration 1206, loss = 42.07840335\n",
      "Iteration 1207, loss = 41.66666878\n",
      "Iteration 1208, loss = 42.26740901\n",
      "Iteration 1209, loss = 41.63074142\n",
      "Iteration 1210, loss = 41.57524270\n",
      "Iteration 1211, loss = 41.24593969\n",
      "Iteration 1212, loss = 41.20498265\n",
      "Iteration 1213, loss = 42.02020192\n",
      "Iteration 1214, loss = 42.01570009\n",
      "Iteration 1215, loss = 43.19431159\n",
      "Iteration 1216, loss = 41.58741802\n",
      "Iteration 1217, loss = 41.33863823\n",
      "Iteration 1218, loss = 41.36945705\n",
      "Iteration 1219, loss = 41.32539353\n",
      "Iteration 1220, loss = 41.13870268\n",
      "Iteration 1221, loss = 41.42636876\n",
      "Iteration 1222, loss = 41.17589818\n",
      "Iteration 1223, loss = 41.03678688\n",
      "Iteration 1224, loss = 41.55101019\n",
      "Iteration 1225, loss = 41.18424498\n",
      "Iteration 1226, loss = 41.08248437\n",
      "Iteration 1227, loss = 41.41302644\n",
      "Iteration 1228, loss = 41.59621533\n",
      "Iteration 1229, loss = 41.40153545\n",
      "Iteration 1230, loss = 41.10667447\n",
      "Iteration 1231, loss = 41.14103936\n",
      "Iteration 1232, loss = 41.60726070\n",
      "Iteration 1233, loss = 41.24927252\n",
      "Iteration 1234, loss = 41.40814857\n",
      "Iteration 1235, loss = 41.05873662\n",
      "Iteration 1236, loss = 41.57561402\n",
      "Iteration 1237, loss = 41.64966955\n",
      "Iteration 1238, loss = 41.46543699\n",
      "Iteration 1239, loss = 41.29204426\n",
      "Iteration 1240, loss = 42.43894485\n",
      "Iteration 1241, loss = 42.92210555\n",
      "Iteration 1242, loss = 41.15013482\n",
      "Iteration 1243, loss = 40.99328960\n",
      "Iteration 1244, loss = 41.37571509\n",
      "Iteration 1245, loss = 42.02073151\n",
      "Iteration 1246, loss = 41.02528849\n",
      "Iteration 1247, loss = 41.56489439\n",
      "Iteration 1248, loss = 41.02706874\n",
      "Iteration 1249, loss = 42.14595318\n",
      "Iteration 1250, loss = 41.65015349\n",
      "Iteration 1251, loss = 41.81273020\n",
      "Iteration 1252, loss = 42.28863790\n",
      "Iteration 1253, loss = 42.00666552\n",
      "Iteration 1254, loss = 41.60843944\n",
      "Iteration 1255, loss = 41.22456368\n",
      "Iteration 1256, loss = 41.41708767\n",
      "Iteration 1257, loss = 41.07489465\n",
      "Iteration 1258, loss = 41.06292985\n",
      "Iteration 1259, loss = 41.42236638\n",
      "Iteration 1260, loss = 41.17535779\n",
      "Iteration 1261, loss = 40.88766414\n",
      "Iteration 1262, loss = 41.28899901\n",
      "Iteration 1263, loss = 41.98237534\n",
      "Iteration 1264, loss = 42.18966973\n",
      "Iteration 1265, loss = 42.33454292\n",
      "Iteration 1266, loss = 41.50541108\n",
      "Iteration 1267, loss = 41.54981249\n",
      "Iteration 1268, loss = 40.84692921\n",
      "Iteration 1269, loss = 40.64447690\n",
      "Iteration 1270, loss = 40.59185736\n",
      "Iteration 1271, loss = 40.63395692\n",
      "Iteration 1272, loss = 40.50870787\n",
      "Iteration 1273, loss = 40.92233828\n",
      "Iteration 1274, loss = 41.29442526\n",
      "Iteration 1275, loss = 40.98531994\n",
      "Iteration 1276, loss = 41.18217060\n",
      "Iteration 1277, loss = 41.06061963\n",
      "Iteration 1278, loss = 42.02047071\n",
      "Iteration 1279, loss = 41.69862549\n",
      "Iteration 1280, loss = 41.06603185\n",
      "Iteration 1281, loss = 41.25791139\n",
      "Iteration 1282, loss = 41.55143480\n",
      "Iteration 1283, loss = 40.75635844\n",
      "Iteration 1284, loss = 40.55585462\n",
      "Iteration 1285, loss = 40.74686753\n",
      "Iteration 1286, loss = 41.01351081\n",
      "Iteration 1287, loss = 40.57979848\n",
      "Iteration 1288, loss = 40.57407936\n",
      "Iteration 1289, loss = 40.65436047\n",
      "Iteration 1290, loss = 40.64494019\n",
      "Iteration 1291, loss = 40.86776907\n",
      "Iteration 1292, loss = 40.72572547\n",
      "Iteration 1293, loss = 41.13253306\n",
      "Iteration 1294, loss = 40.39272894\n",
      "Iteration 1295, loss = 40.71230788\n",
      "Iteration 1296, loss = 41.02642057\n",
      "Iteration 1297, loss = 41.08427944\n",
      "Iteration 1298, loss = 41.36658783\n",
      "Iteration 1299, loss = 40.82206026\n",
      "Iteration 1300, loss = 42.34757322\n",
      "Iteration 1301, loss = 43.50907937\n",
      "Iteration 1302, loss = 41.07606239\n",
      "Iteration 1303, loss = 40.92260827\n",
      "Iteration 1304, loss = 40.45745315\n",
      "Iteration 1305, loss = 40.62040240\n",
      "Iteration 1306, loss = 41.01260907\n",
      "Iteration 1307, loss = 41.21031942\n",
      "Iteration 1308, loss = 40.28767972\n",
      "Iteration 1309, loss = 40.20954073\n",
      "Iteration 1310, loss = 40.85334321\n",
      "Iteration 1311, loss = 41.11949671\n",
      "Iteration 1312, loss = 40.95591806\n",
      "Iteration 1313, loss = 40.76874359\n",
      "Iteration 1314, loss = 40.36114371\n",
      "Iteration 1315, loss = 41.00193419\n",
      "Iteration 1316, loss = 40.37329325\n",
      "Iteration 1317, loss = 40.41611135\n",
      "Iteration 1318, loss = 40.32147138\n",
      "Iteration 1319, loss = 40.56341223\n",
      "Iteration 1320, loss = 40.63794883\n",
      "Iteration 1321, loss = 40.58181712\n",
      "Iteration 1322, loss = 40.68932016\n",
      "Iteration 1323, loss = 41.51248200\n",
      "Iteration 1324, loss = 41.04598252\n",
      "Iteration 1325, loss = 42.28287384\n",
      "Iteration 1326, loss = 41.39902624\n",
      "Iteration 1327, loss = 41.39004678\n",
      "Iteration 1328, loss = 40.64000100\n",
      "Iteration 1329, loss = 41.28974675\n",
      "Iteration 1330, loss = 40.11004009\n",
      "Iteration 1331, loss = 40.76352503\n",
      "Iteration 1332, loss = 40.14179845\n",
      "Iteration 1333, loss = 40.30155227\n",
      "Iteration 1334, loss = 40.78684880\n",
      "Iteration 1335, loss = 40.35430439\n",
      "Iteration 1336, loss = 40.08301843\n",
      "Iteration 1337, loss = 40.51090895\n",
      "Iteration 1338, loss = 40.07654298\n",
      "Iteration 1339, loss = 40.41209875\n",
      "Iteration 1340, loss = 40.64130376\n",
      "Iteration 1341, loss = 42.00033339\n",
      "Iteration 1342, loss = 42.11648281\n",
      "Iteration 1343, loss = 40.73922693\n",
      "Iteration 1344, loss = 40.42650103\n",
      "Iteration 1345, loss = 39.98338043\n",
      "Iteration 1346, loss = 40.08159163\n",
      "Iteration 1347, loss = 40.31372613\n",
      "Iteration 1348, loss = 39.81756049\n",
      "Iteration 1349, loss = 39.83921780\n",
      "Iteration 1350, loss = 39.93966013\n",
      "Iteration 1351, loss = 40.11661775\n",
      "Iteration 1352, loss = 40.47758766\n",
      "Iteration 1353, loss = 40.03228229\n",
      "Iteration 1354, loss = 40.67515947\n",
      "Iteration 1355, loss = 39.82019681\n",
      "Iteration 1356, loss = 40.79058529\n",
      "Iteration 1357, loss = 40.36924372\n",
      "Iteration 1358, loss = 41.16215566\n",
      "Iteration 1359, loss = 41.28393154\n",
      "Iteration 1360, loss = 40.06011343\n",
      "Iteration 1361, loss = 40.16985558\n",
      "Iteration 1362, loss = 39.84305478\n",
      "Iteration 1363, loss = 40.12927151\n",
      "Iteration 1364, loss = 40.64013020\n",
      "Iteration 1365, loss = 39.88840350\n",
      "Iteration 1366, loss = 40.10512135\n",
      "Iteration 1367, loss = 40.51905328\n",
      "Iteration 1368, loss = 40.53097162\n",
      "Iteration 1369, loss = 40.54002552\n",
      "Iteration 1370, loss = 39.99495095\n",
      "Iteration 1371, loss = 40.47040658\n",
      "Iteration 1372, loss = 40.01401873\n",
      "Iteration 1373, loss = 40.10130199\n",
      "Iteration 1374, loss = 41.22584679\n",
      "Iteration 1375, loss = 40.54064336\n",
      "Iteration 1376, loss = 40.50001385\n",
      "Iteration 1377, loss = 40.33049580\n",
      "Iteration 1378, loss = 39.93959495\n",
      "Iteration 1379, loss = 39.90683077\n",
      "Iteration 1380, loss = 40.38656830\n",
      "Iteration 1381, loss = 41.30230213\n",
      "Iteration 1382, loss = 40.94927916\n",
      "Iteration 1383, loss = 39.82591364\n",
      "Iteration 1384, loss = 39.38429408\n",
      "Iteration 1385, loss = 40.44693395\n",
      "Iteration 1386, loss = 40.39293578\n",
      "Iteration 1387, loss = 40.18857857\n",
      "Iteration 1388, loss = 40.69397894\n",
      "Iteration 1389, loss = 40.70711117\n",
      "Iteration 1390, loss = 41.11579199\n",
      "Iteration 1391, loss = 40.19220041\n",
      "Iteration 1392, loss = 40.94944743\n",
      "Iteration 1393, loss = 40.81782431\n",
      "Iteration 1394, loss = 40.15788048\n",
      "Iteration 1395, loss = 39.80652613\n",
      "Iteration 1396, loss = 39.90374481\n",
      "Iteration 1397, loss = 40.04449223\n",
      "Iteration 1398, loss = 39.73142254\n",
      "Iteration 1399, loss = 39.95683247\n",
      "Iteration 1400, loss = 40.54853957\n",
      "Iteration 1401, loss = 39.67015183\n",
      "Iteration 1402, loss = 39.61237187\n",
      "Iteration 1403, loss = 39.78510660\n",
      "Iteration 1404, loss = 39.80210886\n",
      "Iteration 1405, loss = 39.66011000\n",
      "Iteration 1406, loss = 39.75741328\n",
      "Iteration 1407, loss = 40.11360080\n",
      "Iteration 1408, loss = 40.62388860\n",
      "Iteration 1409, loss = 40.05246524\n",
      "Iteration 1410, loss = 39.60959066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1411, loss = 39.76782618\n",
      "Iteration 1412, loss = 39.86920538\n",
      "Iteration 1413, loss = 39.39592316\n",
      "Iteration 1414, loss = 39.90962169\n",
      "Iteration 1415, loss = 40.07772469\n",
      "Iteration 1416, loss = 39.99868579\n",
      "Iteration 1417, loss = 40.44025593\n",
      "Iteration 1418, loss = 40.59875904\n",
      "Iteration 1419, loss = 41.61694599\n",
      "Iteration 1420, loss = 40.83503571\n",
      "Iteration 1421, loss = 40.56149718\n",
      "Iteration 1422, loss = 39.25079811\n",
      "Iteration 1423, loss = 40.07185907\n",
      "Iteration 1424, loss = 39.72299874\n",
      "Iteration 1425, loss = 40.65784605\n",
      "Iteration 1426, loss = 39.89303544\n",
      "Iteration 1427, loss = 39.43147557\n",
      "Iteration 1428, loss = 39.30815823\n",
      "Iteration 1429, loss = 39.53763153\n",
      "Iteration 1430, loss = 39.23055479\n",
      "Iteration 1431, loss = 39.07067800\n",
      "Iteration 1432, loss = 39.18123280\n",
      "Iteration 1433, loss = 39.58469725\n",
      "Iteration 1434, loss = 39.57574726\n",
      "Iteration 1435, loss = 39.65401946\n",
      "Iteration 1436, loss = 40.23310071\n",
      "Iteration 1437, loss = 39.06222917\n",
      "Iteration 1438, loss = 39.67863684\n",
      "Iteration 1439, loss = 40.38293995\n",
      "Iteration 1440, loss = 42.29981706\n",
      "Iteration 1441, loss = 44.29669100\n",
      "Iteration 1442, loss = 41.27332316\n",
      "Iteration 1443, loss = 41.26308298\n",
      "Iteration 1444, loss = 40.84575603\n",
      "Iteration 1445, loss = 39.75514953\n",
      "Iteration 1446, loss = 39.81254538\n",
      "Iteration 1447, loss = 39.67934061\n",
      "Iteration 1448, loss = 39.45498052\n",
      "Iteration 1449, loss = 39.48604626\n",
      "Iteration 1450, loss = 38.99136944\n",
      "Iteration 1451, loss = 39.15096333\n",
      "Iteration 1452, loss = 39.44516654\n",
      "Iteration 1453, loss = 39.17489961\n",
      "Iteration 1454, loss = 39.00483653\n",
      "Iteration 1455, loss = 39.21940212\n",
      "Iteration 1456, loss = 39.14891215\n",
      "Iteration 1457, loss = 39.58819556\n",
      "Iteration 1458, loss = 39.70339661\n",
      "Iteration 1459, loss = 38.96103118\n",
      "Iteration 1460, loss = 39.03070589\n",
      "Iteration 1461, loss = 39.01289620\n",
      "Iteration 1462, loss = 38.91230069\n",
      "Iteration 1463, loss = 39.36383171\n",
      "Iteration 1464, loss = 39.11889097\n",
      "Iteration 1465, loss = 39.46866959\n",
      "Iteration 1466, loss = 39.06481214\n",
      "Iteration 1467, loss = 38.82310292\n",
      "Iteration 1468, loss = 39.03814666\n",
      "Iteration 1469, loss = 39.40898243\n",
      "Iteration 1470, loss = 39.34924975\n",
      "Iteration 1471, loss = 39.92543293\n",
      "Iteration 1472, loss = 39.84515828\n",
      "Iteration 1473, loss = 39.87578640\n",
      "Iteration 1474, loss = 41.57795634\n",
      "Iteration 1475, loss = 39.82076203\n",
      "Iteration 1476, loss = 39.41341133\n",
      "Iteration 1477, loss = 38.88393779\n",
      "Iteration 1478, loss = 39.14884723\n",
      "Iteration 1479, loss = 38.70457129\n",
      "Iteration 1480, loss = 38.67819524\n",
      "Iteration 1481, loss = 39.10642334\n",
      "Iteration 1482, loss = 38.76974319\n",
      "Iteration 1483, loss = 40.12210958\n",
      "Iteration 1484, loss = 40.05022775\n",
      "Iteration 1485, loss = 40.25567278\n",
      "Iteration 1486, loss = 40.58372029\n",
      "Iteration 1487, loss = 38.83130211\n",
      "Iteration 1488, loss = 39.10889353\n",
      "Iteration 1489, loss = 38.67000569\n",
      "Iteration 1490, loss = 39.39782543\n",
      "Iteration 1491, loss = 41.66119310\n",
      "Iteration 1492, loss = 41.84943795\n",
      "Iteration 1493, loss = 40.99435704\n",
      "Iteration 1494, loss = 42.83459815\n",
      "Iteration 1495, loss = 40.29992814\n",
      "Iteration 1496, loss = 38.94007579\n",
      "Iteration 1497, loss = 39.08192967\n",
      "Iteration 1498, loss = 39.13636128\n",
      "Iteration 1499, loss = 40.48807704\n",
      "Iteration 1500, loss = 38.92754291\n",
      "Iteration 1501, loss = 39.88001249\n",
      "Iteration 1502, loss = 38.65009590\n",
      "Iteration 1503, loss = 38.59702293\n",
      "Iteration 1504, loss = 38.80267527\n",
      "Iteration 1505, loss = 38.72505123\n",
      "Iteration 1506, loss = 39.57799240\n",
      "Iteration 1507, loss = 39.05295973\n",
      "Iteration 1508, loss = 39.04645227\n",
      "Iteration 1509, loss = 39.11854549\n",
      "Iteration 1510, loss = 39.20493905\n",
      "Iteration 1511, loss = 39.47639537\n",
      "Iteration 1512, loss = 42.81274019\n",
      "Iteration 1513, loss = 41.08002541\n",
      "Iteration 1514, loss = 40.19268314\n",
      "Iteration 1515, loss = 41.37866371\n",
      "Iteration 1516, loss = 40.60159428\n",
      "Iteration 1517, loss = 40.79056602\n",
      "Iteration 1518, loss = 39.21564101\n",
      "Iteration 1519, loss = 40.36565030\n",
      "Iteration 1520, loss = 39.70667128\n",
      "Iteration 1521, loss = 39.69615791\n",
      "Iteration 1522, loss = 40.85830746\n",
      "Iteration 1523, loss = 40.68793929\n",
      "Iteration 1524, loss = 38.93953742\n",
      "Iteration 1525, loss = 38.53341319\n",
      "Iteration 1526, loss = 38.67373322\n",
      "Iteration 1527, loss = 38.99932078\n",
      "Iteration 1528, loss = 39.27390465\n",
      "Iteration 1529, loss = 39.01886533\n",
      "Iteration 1530, loss = 38.85227110\n",
      "Iteration 1531, loss = 38.36428191\n",
      "Iteration 1532, loss = 38.57679073\n",
      "Iteration 1533, loss = 38.42102974\n",
      "Iteration 1534, loss = 38.53008744\n",
      "Iteration 1535, loss = 39.23338102\n",
      "Iteration 1536, loss = 38.68372216\n",
      "Iteration 1537, loss = 38.64420327\n",
      "Iteration 1538, loss = 38.51215179\n",
      "Iteration 1539, loss = 38.17313632\n",
      "Iteration 1540, loss = 38.32628054\n",
      "Iteration 1541, loss = 38.32627072\n",
      "Iteration 1542, loss = 38.57634470\n",
      "Iteration 1543, loss = 38.91902600\n",
      "Iteration 1544, loss = 38.69149358\n",
      "Iteration 1545, loss = 38.22155830\n",
      "Iteration 1546, loss = 38.09014672\n",
      "Iteration 1547, loss = 38.59440355\n",
      "Iteration 1548, loss = 38.45939469\n",
      "Iteration 1549, loss = 38.08569579\n",
      "Iteration 1550, loss = 39.07448749\n",
      "Iteration 1551, loss = 38.84165674\n",
      "Iteration 1552, loss = 39.09803771\n",
      "Iteration 1553, loss = 38.87138236\n",
      "Iteration 1554, loss = 39.75590847\n",
      "Iteration 1555, loss = 39.30887747\n",
      "Iteration 1556, loss = 40.22436262\n",
      "Iteration 1557, loss = 41.17019734\n",
      "Iteration 1558, loss = 40.13974265\n",
      "Iteration 1559, loss = 38.48665536\n",
      "Iteration 1560, loss = 38.32318126\n",
      "Iteration 1561, loss = 38.70603881\n",
      "Iteration 1562, loss = 38.62070635\n",
      "Iteration 1563, loss = 38.48930458\n",
      "Iteration 1564, loss = 38.13361540\n",
      "Iteration 1565, loss = 38.13802235\n",
      "Iteration 1566, loss = 38.12175714\n",
      "Iteration 1567, loss = 38.07679684\n",
      "Iteration 1568, loss = 38.23495547\n",
      "Iteration 1569, loss = 37.99618199\n",
      "Iteration 1570, loss = 38.17566990\n",
      "Iteration 1571, loss = 38.01255650\n",
      "Iteration 1572, loss = 38.02037595\n",
      "Iteration 1573, loss = 38.85715494\n",
      "Iteration 1574, loss = 40.65118635\n",
      "Iteration 1575, loss = 40.77380940\n",
      "Iteration 1576, loss = 41.03558623\n",
      "Iteration 1577, loss = 40.11696269\n",
      "Iteration 1578, loss = 40.41929647\n",
      "Iteration 1579, loss = 40.69188055\n",
      "Iteration 1580, loss = 39.62693314\n",
      "Iteration 1581, loss = 40.63870812\n",
      "Iteration 1582, loss = 38.81892795\n",
      "Iteration 1583, loss = 39.42533318\n",
      "Iteration 1584, loss = 39.94383797\n",
      "Iteration 1585, loss = 39.20705166\n",
      "Iteration 1586, loss = 39.35933427\n",
      "Iteration 1587, loss = 39.92866674\n",
      "Iteration 1588, loss = 39.80049622\n",
      "Iteration 1589, loss = 39.05719197\n",
      "Iteration 1590, loss = 41.35027054\n",
      "Iteration 1591, loss = 39.90181258\n",
      "Iteration 1592, loss = 37.87007288\n",
      "Iteration 1593, loss = 37.65986063\n",
      "Iteration 1594, loss = 37.87016570\n",
      "Iteration 1595, loss = 37.81526744\n",
      "Iteration 1596, loss = 37.74216212\n",
      "Iteration 1597, loss = 38.42787488\n",
      "Iteration 1598, loss = 38.54516428\n",
      "Iteration 1599, loss = 37.57124712\n",
      "Iteration 1600, loss = 37.53788508\n",
      "Iteration 1601, loss = 37.80090465\n",
      "Iteration 1602, loss = 37.63154309\n",
      "Iteration 1603, loss = 38.30517759\n",
      "Iteration 1604, loss = 38.98504369\n",
      "Iteration 1605, loss = 39.04193278\n",
      "Iteration 1606, loss = 38.21737876\n",
      "Iteration 1607, loss = 38.99466304\n",
      "Iteration 1608, loss = 38.42038311\n",
      "Iteration 1609, loss = 38.69932840\n",
      "Iteration 1610, loss = 38.91154135\n",
      "Iteration 1611, loss = 40.76248965\n",
      "Iteration 1612, loss = 38.64153266\n",
      "Iteration 1613, loss = 38.23788017\n",
      "Iteration 1614, loss = 40.19636621\n",
      "Iteration 1615, loss = 38.41888799\n",
      "Iteration 1616, loss = 39.60085939\n",
      "Iteration 1617, loss = 38.22099279\n",
      "Iteration 1618, loss = 38.45313639\n",
      "Iteration 1619, loss = 38.37074995\n",
      "Iteration 1620, loss = 37.58391460\n",
      "Iteration 1621, loss = 37.50207999\n",
      "Iteration 1622, loss = 37.39876809\n",
      "Iteration 1623, loss = 37.47519355\n",
      "Iteration 1624, loss = 37.93776453\n",
      "Iteration 1625, loss = 38.82799675\n",
      "Iteration 1626, loss = 38.12331640\n",
      "Iteration 1627, loss = 37.80111358\n",
      "Iteration 1628, loss = 38.43873176\n",
      "Iteration 1629, loss = 39.16075468\n",
      "Iteration 1630, loss = 38.86209931\n",
      "Iteration 1631, loss = 38.48632621\n",
      "Iteration 1632, loss = 38.73885811\n",
      "Iteration 1633, loss = 38.06072515\n",
      "Iteration 1634, loss = 37.65166115\n",
      "Iteration 1635, loss = 39.53826263\n",
      "Iteration 1636, loss = 39.00288338\n",
      "Iteration 1637, loss = 38.73381162\n",
      "Iteration 1638, loss = 38.74569667\n",
      "Iteration 1639, loss = 37.67898311\n",
      "Iteration 1640, loss = 37.59957382\n",
      "Iteration 1641, loss = 37.54378043\n",
      "Iteration 1642, loss = 37.26286222\n",
      "Iteration 1643, loss = 38.33790064\n",
      "Iteration 1644, loss = 39.63016174\n",
      "Iteration 1645, loss = 37.62794833\n",
      "Iteration 1646, loss = 38.39226713\n",
      "Iteration 1647, loss = 37.50058522\n",
      "Iteration 1648, loss = 38.33786631\n",
      "Iteration 1649, loss = 38.49012165\n",
      "Iteration 1650, loss = 38.25340244\n",
      "Iteration 1651, loss = 39.67180231\n",
      "Iteration 1652, loss = 38.62774885\n",
      "Iteration 1653, loss = 38.01465234\n",
      "Iteration 1654, loss = 39.03260688\n",
      "Iteration 1655, loss = 38.34280580\n",
      "Iteration 1656, loss = 42.39391645\n",
      "Iteration 1657, loss = 45.01737114\n",
      "Iteration 1658, loss = 41.61483075\n",
      "Iteration 1659, loss = 39.48737345\n",
      "Iteration 1660, loss = 39.27470164\n",
      "Iteration 1661, loss = 38.50804717\n",
      "Iteration 1662, loss = 37.78175611\n",
      "Iteration 1663, loss = 36.89123769\n",
      "Iteration 1664, loss = 37.61027093\n",
      "Iteration 1665, loss = 37.11989920\n",
      "Iteration 1666, loss = 37.05609785\n",
      "Iteration 1667, loss = 37.38926496\n",
      "Iteration 1668, loss = 38.26132047\n",
      "Iteration 1669, loss = 37.86179872\n",
      "Iteration 1670, loss = 39.42423408\n",
      "Iteration 1671, loss = 38.33242439\n",
      "Iteration 1672, loss = 38.50354009\n",
      "Iteration 1673, loss = 38.12709472\n",
      "Iteration 1674, loss = 37.69532229\n",
      "Iteration 1675, loss = 36.85080688\n",
      "Iteration 1676, loss = 37.06058839\n",
      "Iteration 1677, loss = 37.02217153\n",
      "Iteration 1678, loss = 37.56745438\n",
      "Iteration 1679, loss = 38.50543844\n",
      "Iteration 1680, loss = 38.55694452\n",
      "Iteration 1681, loss = 37.75728100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1682, loss = 37.88239814\n",
      "Iteration 1683, loss = 36.86185447\n",
      "Iteration 1684, loss = 37.06765125\n",
      "Iteration 1685, loss = 36.84051455\n",
      "Iteration 1686, loss = 36.87803293\n",
      "Iteration 1687, loss = 36.77862953\n",
      "Iteration 1688, loss = 37.62636086\n",
      "Iteration 1689, loss = 39.74998507\n",
      "Iteration 1690, loss = 38.92579226\n",
      "Iteration 1691, loss = 36.93436502\n",
      "Iteration 1692, loss = 36.57929520\n",
      "Iteration 1693, loss = 37.15059182\n",
      "Iteration 1694, loss = 36.72853242\n",
      "Iteration 1695, loss = 38.68777599\n",
      "Iteration 1696, loss = 39.36420019\n",
      "Iteration 1697, loss = 37.23331042\n",
      "Iteration 1698, loss = 37.37628334\n",
      "Iteration 1699, loss = 37.18575957\n",
      "Iteration 1700, loss = 36.92518256\n",
      "Iteration 1701, loss = 36.94337683\n",
      "Iteration 1702, loss = 38.12580249\n",
      "Iteration 1703, loss = 39.43053088\n",
      "Iteration 1704, loss = 37.45682048\n",
      "Iteration 1705, loss = 36.99919124\n",
      "Iteration 1706, loss = 37.07982558\n",
      "Iteration 1707, loss = 36.85588812\n",
      "Iteration 1708, loss = 36.69275998\n",
      "Iteration 1709, loss = 37.74105964\n",
      "Iteration 1710, loss = 37.03799163\n",
      "Iteration 1711, loss = 36.62706064\n",
      "Iteration 1712, loss = 36.49683247\n",
      "Iteration 1713, loss = 36.50103839\n",
      "Iteration 1714, loss = 36.87108036\n",
      "Iteration 1715, loss = 36.73211590\n",
      "Iteration 1716, loss = 36.31967259\n",
      "Iteration 1717, loss = 37.57566088\n",
      "Iteration 1718, loss = 37.59082744\n",
      "Iteration 1719, loss = 37.13370891\n",
      "Iteration 1720, loss = 37.34595562\n",
      "Iteration 1721, loss = 37.06114757\n",
      "Iteration 1722, loss = 37.70527596\n",
      "Iteration 1723, loss = 37.57132347\n",
      "Iteration 1724, loss = 36.94959111\n",
      "Iteration 1725, loss = 37.40533532\n",
      "Iteration 1726, loss = 37.71660801\n",
      "Iteration 1727, loss = 37.90613406\n",
      "Iteration 1728, loss = 36.50439441\n",
      "Iteration 1729, loss = 36.20675685\n",
      "Iteration 1730, loss = 36.40735523\n",
      "Iteration 1731, loss = 36.79221048\n",
      "Iteration 1732, loss = 39.98700327\n",
      "Iteration 1733, loss = 39.73492960\n",
      "Iteration 1734, loss = 38.16818211\n",
      "Iteration 1735, loss = 37.38502801\n",
      "Iteration 1736, loss = 37.43775817\n",
      "Iteration 1737, loss = 36.52802750\n",
      "Iteration 1738, loss = 36.54143257\n",
      "Iteration 1739, loss = 37.70253736\n",
      "Iteration 1740, loss = 38.08737871\n",
      "Iteration 1741, loss = 38.17008081\n",
      "Iteration 1742, loss = 37.29868491\n",
      "Iteration 1743, loss = 36.91718260\n",
      "Iteration 1744, loss = 37.25425552\n",
      "Iteration 1745, loss = 36.78502365\n",
      "Iteration 1746, loss = 37.11856524\n",
      "Iteration 1747, loss = 37.39265768\n",
      "Iteration 1748, loss = 36.29099515\n",
      "Iteration 1749, loss = 36.07889723\n",
      "Iteration 1750, loss = 36.28530103\n",
      "Iteration 1751, loss = 36.62231784\n",
      "Iteration 1752, loss = 36.68238937\n",
      "Iteration 1753, loss = 36.17962703\n",
      "Iteration 1754, loss = 36.40603714\n",
      "Iteration 1755, loss = 38.06483670\n",
      "Iteration 1756, loss = 37.68824424\n",
      "Iteration 1757, loss = 37.80395294\n",
      "Iteration 1758, loss = 38.27804616\n",
      "Iteration 1759, loss = 36.93243036\n",
      "Iteration 1760, loss = 36.90412137\n",
      "Iteration 1761, loss = 36.17237511\n",
      "Iteration 1762, loss = 37.09058818\n",
      "Iteration 1763, loss = 37.56529412\n",
      "Iteration 1764, loss = 38.27091436\n",
      "Iteration 1765, loss = 37.75860515\n",
      "Iteration 1766, loss = 39.47392480\n",
      "Iteration 1767, loss = 38.28665142\n",
      "Iteration 1768, loss = 37.76727160\n",
      "Iteration 1769, loss = 40.89803493\n",
      "Iteration 1770, loss = 40.80670966\n",
      "Iteration 1771, loss = 42.92980926\n",
      "Iteration 1772, loss = 40.41614943\n",
      "Iteration 1773, loss = 40.16580765\n",
      "Iteration 1774, loss = 37.77366852\n",
      "Iteration 1775, loss = 37.87112945\n",
      "Iteration 1776, loss = 36.55461800\n",
      "Iteration 1777, loss = 36.90014997\n",
      "Iteration 1778, loss = 36.34286989\n",
      "Iteration 1779, loss = 36.76589551\n",
      "Iteration 1780, loss = 36.98963951\n",
      "Iteration 1781, loss = 37.36646277\n",
      "Iteration 1782, loss = 36.93345790\n",
      "Iteration 1783, loss = 36.80765491\n",
      "Iteration 1784, loss = 36.20771579\n",
      "Iteration 1785, loss = 35.88929888\n",
      "Iteration 1786, loss = 36.53098389\n",
      "Iteration 1787, loss = 36.66539115\n",
      "Iteration 1788, loss = 36.72358061\n",
      "Iteration 1789, loss = 36.74451015\n",
      "Iteration 1790, loss = 37.49925838\n",
      "Iteration 1791, loss = 36.57034558\n",
      "Iteration 1792, loss = 38.42546563\n",
      "Iteration 1793, loss = 38.34566039\n",
      "Iteration 1794, loss = 36.92234769\n",
      "Iteration 1795, loss = 36.41668943\n",
      "Iteration 1796, loss = 37.13672929\n",
      "Iteration 1797, loss = 38.12337324\n",
      "Iteration 1798, loss = 37.80657300\n",
      "Iteration 1799, loss = 37.14347571\n",
      "Iteration 1800, loss = 36.45777941\n",
      "Iteration 1801, loss = 35.77775817\n",
      "Iteration 1802, loss = 35.61510316\n",
      "Iteration 1803, loss = 36.64577465\n",
      "Iteration 1804, loss = 36.15543035\n",
      "Iteration 1805, loss = 35.77176719\n",
      "Iteration 1806, loss = 35.93255569\n",
      "Iteration 1807, loss = 36.10849533\n",
      "Iteration 1808, loss = 35.87301612\n",
      "Iteration 1809, loss = 36.24516832\n",
      "Iteration 1810, loss = 35.80283826\n",
      "Iteration 1811, loss = 35.75570926\n",
      "Iteration 1812, loss = 35.60851482\n",
      "Iteration 1813, loss = 36.29301416\n",
      "Iteration 1814, loss = 35.64340847\n",
      "Iteration 1815, loss = 35.98477724\n",
      "Iteration 1816, loss = 36.03688228\n",
      "Iteration 1817, loss = 36.03592017\n",
      "Iteration 1818, loss = 35.90970369\n",
      "Iteration 1819, loss = 36.06707161\n",
      "Iteration 1820, loss = 35.82267271\n",
      "Iteration 1821, loss = 36.01993206\n",
      "Iteration 1822, loss = 36.23411620\n",
      "Iteration 1823, loss = 35.97521483\n",
      "Iteration 1824, loss = 35.51050192\n",
      "Iteration 1825, loss = 35.48964698\n",
      "Iteration 1826, loss = 35.54717814\n",
      "Iteration 1827, loss = 35.25115584\n",
      "Iteration 1828, loss = 35.92239407\n",
      "Iteration 1829, loss = 35.81093386\n",
      "Iteration 1830, loss = 35.96591108\n",
      "Iteration 1831, loss = 35.57879745\n",
      "Iteration 1832, loss = 36.59289775\n",
      "Iteration 1833, loss = 36.29330805\n",
      "Iteration 1834, loss = 35.79569711\n",
      "Iteration 1835, loss = 37.41968048\n",
      "Iteration 1836, loss = 37.11482658\n",
      "Iteration 1837, loss = 36.69663052\n",
      "Iteration 1838, loss = 35.64721530\n",
      "Iteration 1839, loss = 35.31198230\n",
      "Iteration 1840, loss = 35.12269609\n",
      "Iteration 1841, loss = 35.98165972\n",
      "Iteration 1842, loss = 37.07744548\n",
      "Iteration 1843, loss = 35.13457207\n",
      "Iteration 1844, loss = 36.14098548\n",
      "Iteration 1845, loss = 36.46814003\n",
      "Iteration 1846, loss = 35.83818854\n",
      "Iteration 1847, loss = 36.44403652\n",
      "Iteration 1848, loss = 36.78964887\n",
      "Iteration 1849, loss = 37.89003030\n",
      "Iteration 1850, loss = 38.73804906\n",
      "Iteration 1851, loss = 35.88038705\n",
      "Iteration 1852, loss = 36.56384682\n",
      "Iteration 1853, loss = 36.08899732\n",
      "Iteration 1854, loss = 35.17062988\n",
      "Iteration 1855, loss = 35.36830642\n",
      "Iteration 1856, loss = 35.42838244\n",
      "Iteration 1857, loss = 35.41380128\n",
      "Iteration 1858, loss = 35.40540099\n",
      "Iteration 1859, loss = 35.86735538\n",
      "Iteration 1860, loss = 35.48977223\n",
      "Iteration 1861, loss = 35.29144362\n",
      "Iteration 1862, loss = 36.02390415\n",
      "Iteration 1863, loss = 35.28232134\n",
      "Iteration 1864, loss = 35.33164892\n",
      "Iteration 1865, loss = 35.20568886\n",
      "Iteration 1866, loss = 35.94302639\n",
      "Iteration 1867, loss = 36.92698113\n",
      "Iteration 1868, loss = 36.44274182\n",
      "Iteration 1869, loss = 36.06478984\n",
      "Iteration 1870, loss = 36.58207641\n",
      "Iteration 1871, loss = 34.99463511\n",
      "Iteration 1872, loss = 35.11078040\n",
      "Iteration 1873, loss = 35.55724312\n",
      "Iteration 1874, loss = 35.47048151\n",
      "Iteration 1875, loss = 34.91245835\n",
      "Iteration 1876, loss = 35.32629169\n",
      "Iteration 1877, loss = 35.37847089\n",
      "Iteration 1878, loss = 35.64682356\n",
      "Iteration 1879, loss = 35.76898373\n",
      "Iteration 1880, loss = 35.37690009\n",
      "Iteration 1881, loss = 35.34406016\n",
      "Iteration 1882, loss = 35.61989963\n",
      "Iteration 1883, loss = 35.84869778\n",
      "Iteration 1884, loss = 36.35314047\n",
      "Iteration 1885, loss = 36.00991912\n",
      "Iteration 1886, loss = 34.93220364\n",
      "Iteration 1887, loss = 35.13294481\n",
      "Iteration 1888, loss = 34.97944545\n",
      "Iteration 1889, loss = 34.92117548\n",
      "Iteration 1890, loss = 35.81826016\n",
      "Iteration 1891, loss = 37.94461188\n",
      "Iteration 1892, loss = 38.51369010\n",
      "Iteration 1893, loss = 35.82769049\n",
      "Iteration 1894, loss = 35.27001601\n",
      "Iteration 1895, loss = 35.13415960\n",
      "Iteration 1896, loss = 34.83461443\n",
      "Iteration 1897, loss = 36.00636940\n",
      "Iteration 1898, loss = 36.17392254\n",
      "Iteration 1899, loss = 36.78205266\n",
      "Iteration 1900, loss = 36.05249283\n",
      "Iteration 1901, loss = 35.34219077\n",
      "Iteration 1902, loss = 35.26273937\n",
      "Iteration 1903, loss = 34.91370221\n",
      "Iteration 1904, loss = 34.73584234\n",
      "Iteration 1905, loss = 35.05172071\n",
      "Iteration 1906, loss = 35.70100662\n",
      "Iteration 1907, loss = 35.11914999\n",
      "Iteration 1908, loss = 35.09875537\n",
      "Iteration 1909, loss = 34.78973174\n",
      "Iteration 1910, loss = 34.55278615\n",
      "Iteration 1911, loss = 35.34117697\n",
      "Iteration 1912, loss = 35.42182931\n",
      "Iteration 1913, loss = 34.97115700\n",
      "Iteration 1914, loss = 34.94160948\n",
      "Iteration 1915, loss = 34.75781237\n",
      "Iteration 1916, loss = 35.18982918\n",
      "Iteration 1917, loss = 36.41394695\n",
      "Iteration 1918, loss = 35.22039654\n",
      "Iteration 1919, loss = 35.38660786\n",
      "Iteration 1920, loss = 34.68419795\n",
      "Iteration 1921, loss = 35.43616591\n",
      "Iteration 1922, loss = 35.27820765\n",
      "Iteration 1923, loss = 34.42406123\n",
      "Iteration 1924, loss = 34.93987867\n",
      "Iteration 1925, loss = 34.32808669\n",
      "Iteration 1926, loss = 35.13500145\n",
      "Iteration 1927, loss = 35.33566517\n",
      "Iteration 1928, loss = 35.07983499\n",
      "Iteration 1929, loss = 36.41479393\n",
      "Iteration 1930, loss = 38.04723947\n",
      "Iteration 1931, loss = 35.50078435\n",
      "Iteration 1932, loss = 34.88621900\n",
      "Iteration 1933, loss = 34.74980426\n",
      "Iteration 1934, loss = 34.61455195\n",
      "Iteration 1935, loss = 35.19589586\n",
      "Iteration 1936, loss = 34.81298410\n",
      "Iteration 1937, loss = 35.49997863\n",
      "Iteration 1938, loss = 34.71395285\n",
      "Iteration 1939, loss = 34.95234386\n",
      "Iteration 1940, loss = 36.03350491\n",
      "Iteration 1941, loss = 35.60370878\n",
      "Iteration 1942, loss = 34.81628871\n",
      "Iteration 1943, loss = 35.17678547\n",
      "Iteration 1944, loss = 35.23719149\n",
      "Iteration 1945, loss = 35.25340477\n",
      "Iteration 1946, loss = 35.32363105\n",
      "Iteration 1947, loss = 34.53355060\n",
      "Iteration 1948, loss = 34.26328583\n",
      "Iteration 1949, loss = 34.99834911\n",
      "Iteration 1950, loss = 35.21849195\n",
      "Iteration 1951, loss = 34.78362381\n",
      "Iteration 1952, loss = 34.83850920\n",
      "Iteration 1953, loss = 34.76097557\n",
      "Iteration 1954, loss = 34.69786572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1955, loss = 34.46234293\n",
      "Iteration 1956, loss = 35.30080449\n",
      "Iteration 1957, loss = 34.53874231\n",
      "Iteration 1958, loss = 34.32154947\n",
      "Iteration 1959, loss = 34.79228618\n",
      "Iteration 1960, loss = 34.24074527\n",
      "Iteration 1961, loss = 34.14988342\n",
      "Iteration 1962, loss = 35.37654698\n",
      "Iteration 1963, loss = 35.38283192\n",
      "Iteration 1964, loss = 35.68347651\n",
      "Iteration 1965, loss = 35.92681498\n",
      "Iteration 1966, loss = 34.86509285\n",
      "Iteration 1967, loss = 34.14480656\n",
      "Iteration 1968, loss = 34.42884354\n",
      "Iteration 1969, loss = 34.07221046\n",
      "Iteration 1970, loss = 34.03613857\n",
      "Iteration 1971, loss = 34.68171596\n",
      "Iteration 1972, loss = 34.66616689\n",
      "Iteration 1973, loss = 34.31011288\n",
      "Iteration 1974, loss = 33.78037063\n",
      "Iteration 1975, loss = 35.91193219\n",
      "Iteration 1976, loss = 34.40430059\n",
      "Iteration 1977, loss = 34.49479787\n",
      "Iteration 1978, loss = 34.33882490\n",
      "Iteration 1979, loss = 35.16597752\n",
      "Iteration 1980, loss = 36.68694423\n",
      "Iteration 1981, loss = 34.65501371\n",
      "Iteration 1982, loss = 34.74543746\n",
      "Iteration 1983, loss = 35.10858287\n",
      "Iteration 1984, loss = 34.73956643\n",
      "Iteration 1985, loss = 34.13422999\n",
      "Iteration 1986, loss = 34.67775223\n",
      "Iteration 1987, loss = 34.09773845\n",
      "Iteration 1988, loss = 35.03981193\n",
      "Iteration 1989, loss = 34.91520710\n",
      "Iteration 1990, loss = 34.19959456\n",
      "Iteration 1991, loss = 34.07658952\n",
      "Iteration 1992, loss = 34.15002933\n",
      "Iteration 1993, loss = 33.90521295\n",
      "Iteration 1994, loss = 35.88641489\n",
      "Iteration 1995, loss = 35.16516313\n",
      "Iteration 1996, loss = 34.46903519\n",
      "Iteration 1997, loss = 34.08561198\n",
      "Iteration 1998, loss = 33.98653777\n",
      "Iteration 1999, loss = 34.84838701\n",
      "Iteration 2000, loss = 34.81725253\n",
      "Iteration 2001, loss = 34.07212469\n",
      "Iteration 2002, loss = 34.18811153\n",
      "Iteration 2003, loss = 33.78064407\n",
      "Iteration 2004, loss = 33.81392324\n",
      "Iteration 2005, loss = 34.04695181\n",
      "Iteration 2006, loss = 34.42879333\n",
      "Iteration 2007, loss = 34.48498501\n",
      "Iteration 2008, loss = 34.39847540\n",
      "Iteration 2009, loss = 34.04583525\n",
      "Iteration 2010, loss = 33.69862964\n",
      "Iteration 2011, loss = 34.32157225\n",
      "Iteration 2012, loss = 34.42172077\n",
      "Iteration 2013, loss = 34.19326249\n",
      "Iteration 2014, loss = 33.84581556\n",
      "Iteration 2015, loss = 34.61843333\n",
      "Iteration 2016, loss = 34.34306759\n",
      "Iteration 2017, loss = 34.49107942\n",
      "Iteration 2018, loss = 36.54136622\n",
      "Iteration 2019, loss = 37.39718035\n",
      "Iteration 2020, loss = 36.22232135\n",
      "Iteration 2021, loss = 38.49909048\n",
      "Iteration 2022, loss = 36.66993363\n",
      "Iteration 2023, loss = 38.42778501\n",
      "Iteration 2024, loss = 36.08659179\n",
      "Iteration 2025, loss = 34.87906981\n",
      "Iteration 2026, loss = 34.23195032\n",
      "Iteration 2027, loss = 34.50972018\n",
      "Iteration 2028, loss = 34.26292868\n",
      "Iteration 2029, loss = 34.11642657\n",
      "Iteration 2030, loss = 33.84352902\n",
      "Iteration 2031, loss = 34.25858975\n",
      "Iteration 2032, loss = 34.18741179\n",
      "Iteration 2033, loss = 35.14179822\n",
      "Iteration 2034, loss = 34.15952151\n",
      "Iteration 2035, loss = 34.51092057\n",
      "Iteration 2036, loss = 34.80037841\n",
      "Iteration 2037, loss = 33.88652478\n",
      "Iteration 2038, loss = 33.79417762\n",
      "Iteration 2039, loss = 34.48742598\n",
      "Iteration 2040, loss = 35.28773090\n",
      "Iteration 2041, loss = 35.19036167\n",
      "Iteration 2042, loss = 34.36809184\n",
      "Iteration 2043, loss = 35.17748294\n",
      "Iteration 2044, loss = 34.46368353\n",
      "Iteration 2045, loss = 33.53193646\n",
      "Iteration 2046, loss = 33.38017243\n",
      "Iteration 2047, loss = 33.35938304\n",
      "Iteration 2048, loss = 33.57221507\n",
      "Iteration 2049, loss = 34.55566314\n",
      "Iteration 2050, loss = 34.28706197\n",
      "Iteration 2051, loss = 34.73788127\n",
      "Iteration 2052, loss = 34.48012307\n",
      "Iteration 2053, loss = 34.67256513\n",
      "Iteration 2054, loss = 34.18040019\n",
      "Iteration 2055, loss = 33.17902806\n",
      "Iteration 2056, loss = 33.68899021\n",
      "Iteration 2057, loss = 34.25027267\n",
      "Iteration 2058, loss = 33.74131994\n",
      "Iteration 2059, loss = 34.78619993\n",
      "Iteration 2060, loss = 33.29297231\n",
      "Iteration 2061, loss = 34.22052094\n",
      "Iteration 2062, loss = 34.14680856\n",
      "Iteration 2063, loss = 34.69598872\n",
      "Iteration 2064, loss = 33.21020908\n",
      "Iteration 2065, loss = 33.37486770\n",
      "Iteration 2066, loss = 35.12247368\n",
      "Iteration 2067, loss = 36.41461999\n",
      "Iteration 2068, loss = 35.21581672\n",
      "Iteration 2069, loss = 38.19883744\n",
      "Iteration 2070, loss = 38.17095446\n",
      "Iteration 2071, loss = 36.28041116\n",
      "Iteration 2072, loss = 34.18028011\n",
      "Iteration 2073, loss = 34.42522842\n",
      "Iteration 2074, loss = 34.03387451\n",
      "Iteration 2075, loss = 34.05529959\n",
      "Iteration 2076, loss = 35.93379969\n",
      "Iteration 2077, loss = 36.27509093\n",
      "Iteration 2078, loss = 35.57085820\n",
      "Iteration 2079, loss = 35.47855027\n",
      "Iteration 2080, loss = 33.05868920\n",
      "Iteration 2081, loss = 33.27898944\n",
      "Iteration 2082, loss = 34.52039384\n",
      "Iteration 2083, loss = 34.62174591\n",
      "Iteration 2084, loss = 34.76200519\n",
      "Iteration 2085, loss = 38.87394373\n",
      "Iteration 2086, loss = 39.75790769\n",
      "Iteration 2087, loss = 36.04042541\n",
      "Iteration 2088, loss = 34.46235818\n",
      "Iteration 2089, loss = 33.20604956\n",
      "Iteration 2090, loss = 33.29066668\n",
      "Iteration 2091, loss = 32.97398359\n",
      "Iteration 2092, loss = 33.05285961\n",
      "Iteration 2093, loss = 32.94368630\n",
      "Iteration 2094, loss = 32.74084035\n",
      "Iteration 2095, loss = 33.18559815\n",
      "Iteration 2096, loss = 34.33271823\n",
      "Iteration 2097, loss = 33.16668348\n",
      "Iteration 2098, loss = 33.47534288\n",
      "Iteration 2099, loss = 33.91510872\n",
      "Iteration 2100, loss = 34.79278284\n",
      "Iteration 2101, loss = 33.72650991\n",
      "Iteration 2102, loss = 33.74155106\n",
      "Iteration 2103, loss = 33.16319166\n",
      "Iteration 2104, loss = 32.75971458\n",
      "Iteration 2105, loss = 32.89911497\n",
      "Iteration 2106, loss = 34.05878495\n",
      "Iteration 2107, loss = 33.60306986\n",
      "Iteration 2108, loss = 34.40849879\n",
      "Iteration 2109, loss = 34.13072125\n",
      "Iteration 2110, loss = 33.75818986\n",
      "Iteration 2111, loss = 33.97291729\n",
      "Iteration 2112, loss = 33.34041248\n",
      "Iteration 2113, loss = 33.71060571\n",
      "Iteration 2114, loss = 34.43676523\n",
      "Iteration 2115, loss = 34.22279069\n",
      "Iteration 2116, loss = 32.92622961\n",
      "Iteration 2117, loss = 32.96808093\n",
      "Iteration 2118, loss = 33.14836497\n",
      "Iteration 2119, loss = 32.69350600\n",
      "Iteration 2120, loss = 33.86599521\n",
      "Iteration 2121, loss = 34.38235109\n",
      "Iteration 2122, loss = 35.51800286\n",
      "Iteration 2123, loss = 33.07613966\n",
      "Iteration 2124, loss = 32.76006363\n",
      "Iteration 2125, loss = 32.51689411\n",
      "Iteration 2126, loss = 32.64206954\n",
      "Iteration 2127, loss = 32.90333453\n",
      "Iteration 2128, loss = 33.99120144\n",
      "Iteration 2129, loss = 34.57796595\n",
      "Iteration 2130, loss = 32.93546703\n",
      "Iteration 2131, loss = 32.59718460\n",
      "Iteration 2132, loss = 32.51276260\n",
      "Iteration 2133, loss = 34.57875272\n",
      "Iteration 2134, loss = 36.58499590\n",
      "Iteration 2135, loss = 35.93889370\n",
      "Iteration 2136, loss = 36.88443514\n",
      "Iteration 2137, loss = 37.14804884\n",
      "Iteration 2138, loss = 33.38324859\n",
      "Iteration 2139, loss = 33.68203447\n",
      "Iteration 2140, loss = 33.26947996\n",
      "Iteration 2141, loss = 33.31314718\n",
      "Iteration 2142, loss = 35.91684789\n",
      "Iteration 2143, loss = 36.92947125\n",
      "Iteration 2144, loss = 36.34770141\n",
      "Iteration 2145, loss = 37.17945750\n",
      "Iteration 2146, loss = 35.51670275\n",
      "Iteration 2147, loss = 35.95422741\n",
      "Iteration 2148, loss = 35.07049222\n",
      "Iteration 2149, loss = 34.87752320\n",
      "Iteration 2150, loss = 33.08010423\n",
      "Iteration 2151, loss = 32.73810733\n",
      "Iteration 2152, loss = 33.22055404\n",
      "Iteration 2153, loss = 33.05097902\n",
      "Iteration 2154, loss = 32.18761495\n",
      "Iteration 2155, loss = 32.31681329\n",
      "Iteration 2156, loss = 32.52509034\n",
      "Iteration 2157, loss = 32.35372183\n",
      "Iteration 2158, loss = 32.75692641\n",
      "Iteration 2159, loss = 32.46738407\n",
      "Iteration 2160, loss = 32.78635578\n",
      "Iteration 2161, loss = 32.72439662\n",
      "Iteration 2162, loss = 32.90428092\n",
      "Iteration 2163, loss = 33.73163130\n",
      "Iteration 2164, loss = 32.96160857\n",
      "Iteration 2165, loss = 32.41194822\n",
      "Iteration 2166, loss = 32.63714217\n",
      "Iteration 2167, loss = 34.97071026\n",
      "Iteration 2168, loss = 34.74012545\n",
      "Iteration 2169, loss = 33.34412419\n",
      "Iteration 2170, loss = 33.67622826\n",
      "Iteration 2171, loss = 35.13277267\n",
      "Iteration 2172, loss = 32.75558929\n",
      "Iteration 2173, loss = 32.13891573\n",
      "Iteration 2174, loss = 32.34092741\n",
      "Iteration 2175, loss = 33.34407530\n",
      "Iteration 2176, loss = 32.76708471\n",
      "Iteration 2177, loss = 33.68619992\n",
      "Iteration 2178, loss = 33.46489062\n",
      "Iteration 2179, loss = 32.20536644\n",
      "Iteration 2180, loss = 32.52960306\n",
      "Iteration 2181, loss = 32.11104388\n",
      "Iteration 2182, loss = 32.81840880\n",
      "Iteration 2183, loss = 32.32030089\n",
      "Iteration 2184, loss = 31.98485595\n",
      "Iteration 2185, loss = 32.34427356\n",
      "Iteration 2186, loss = 33.69236600\n",
      "Iteration 2187, loss = 33.67474297\n",
      "Iteration 2188, loss = 32.85023914\n",
      "Iteration 2189, loss = 33.07238142\n",
      "Iteration 2190, loss = 33.33572846\n",
      "Iteration 2191, loss = 33.56816036\n",
      "Iteration 2192, loss = 33.39493366\n",
      "Iteration 2193, loss = 32.05422512\n",
      "Iteration 2194, loss = 31.77904776\n",
      "Iteration 2195, loss = 32.46729481\n",
      "Iteration 2196, loss = 32.51042294\n",
      "Iteration 2197, loss = 32.88724904\n",
      "Iteration 2198, loss = 33.01702949\n",
      "Iteration 2199, loss = 31.85450133\n",
      "Iteration 2200, loss = 32.33916619\n",
      "Iteration 2201, loss = 33.05485356\n",
      "Iteration 2202, loss = 33.48678571\n",
      "Iteration 2203, loss = 32.36325458\n",
      "Iteration 2204, loss = 32.89462246\n",
      "Iteration 2205, loss = 34.63118466\n",
      "Iteration 2206, loss = 33.18549995\n",
      "Iteration 2207, loss = 33.21398269\n",
      "Iteration 2208, loss = 33.35734198\n",
      "Iteration 2209, loss = 33.69811566\n",
      "Iteration 2210, loss = 33.46225117\n",
      "Iteration 2211, loss = 32.02508057\n",
      "Iteration 2212, loss = 32.23176579\n",
      "Iteration 2213, loss = 32.79759577\n",
      "Iteration 2214, loss = 34.06001402\n",
      "Iteration 2215, loss = 35.73482800\n",
      "Iteration 2216, loss = 36.52597050\n",
      "Iteration 2217, loss = 36.62825963\n",
      "Iteration 2218, loss = 34.12615588\n",
      "Iteration 2219, loss = 34.02340161\n",
      "Iteration 2220, loss = 32.97493100\n",
      "Iteration 2221, loss = 33.88584269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2222, loss = 36.27969049\n",
      "Iteration 2223, loss = 36.37981186\n",
      "Iteration 2224, loss = 35.99091275\n",
      "Iteration 2225, loss = 34.37006662\n",
      "Iteration 2226, loss = 34.37958294\n",
      "Iteration 2227, loss = 35.19620706\n",
      "Iteration 2228, loss = 33.74934369\n",
      "Iteration 2229, loss = 34.59758968\n",
      "Iteration 2230, loss = 33.74057071\n",
      "Iteration 2231, loss = 32.74194892\n",
      "Iteration 2232, loss = 32.06669857\n",
      "Iteration 2233, loss = 31.84348455\n",
      "Iteration 2234, loss = 31.67359389\n",
      "Iteration 2235, loss = 31.75620160\n",
      "Iteration 2236, loss = 31.59071084\n",
      "Iteration 2237, loss = 31.85396823\n",
      "Iteration 2238, loss = 31.67664386\n",
      "Iteration 2239, loss = 32.63459693\n",
      "Iteration 2240, loss = 32.63063181\n",
      "Iteration 2241, loss = 31.71227640\n",
      "Iteration 2242, loss = 31.41377506\n",
      "Iteration 2243, loss = 31.61552089\n",
      "Iteration 2244, loss = 31.88647001\n",
      "Iteration 2245, loss = 31.53227602\n",
      "Iteration 2246, loss = 31.51917597\n",
      "Iteration 2247, loss = 32.00785535\n",
      "Iteration 2248, loss = 31.39366754\n",
      "Iteration 2249, loss = 31.71608699\n",
      "Iteration 2250, loss = 32.91599246\n",
      "Iteration 2251, loss = 32.59289767\n",
      "Iteration 2252, loss = 31.38509148\n",
      "Iteration 2253, loss = 32.01190962\n",
      "Iteration 2254, loss = 32.02106415\n",
      "Iteration 2255, loss = 31.40340603\n",
      "Iteration 2256, loss = 31.48667199\n",
      "Iteration 2257, loss = 31.33220761\n",
      "Iteration 2258, loss = 31.71862166\n",
      "Iteration 2259, loss = 31.75635805\n",
      "Iteration 2260, loss = 31.79890172\n",
      "Iteration 2261, loss = 31.84726094\n",
      "Iteration 2262, loss = 31.26353898\n",
      "Iteration 2263, loss = 31.33886136\n",
      "Iteration 2264, loss = 31.90267017\n",
      "Iteration 2265, loss = 32.64386533\n",
      "Iteration 2266, loss = 31.62556726\n",
      "Iteration 2267, loss = 32.14651789\n",
      "Iteration 2268, loss = 31.50075484\n",
      "Iteration 2269, loss = 31.83504949\n",
      "Iteration 2270, loss = 31.59476574\n",
      "Iteration 2271, loss = 31.83790431\n",
      "Iteration 2272, loss = 33.38237849\n",
      "Iteration 2273, loss = 31.81120037\n",
      "Iteration 2274, loss = 31.46064866\n",
      "Iteration 2275, loss = 31.71507581\n",
      "Iteration 2276, loss = 32.83372583\n",
      "Iteration 2277, loss = 32.71634041\n",
      "Iteration 2278, loss = 33.43719889\n",
      "Iteration 2279, loss = 32.66925664\n",
      "Iteration 2280, loss = 33.41125473\n",
      "Iteration 2281, loss = 31.99999049\n",
      "Iteration 2282, loss = 32.38085154\n",
      "Iteration 2283, loss = 32.07890445\n",
      "Iteration 2284, loss = 31.77842302\n",
      "Iteration 2285, loss = 31.54022472\n",
      "Iteration 2286, loss = 31.44395724\n",
      "Iteration 2287, loss = 31.94752827\n",
      "Iteration 2288, loss = 31.49658246\n",
      "Iteration 2289, loss = 31.33240045\n",
      "Iteration 2290, loss = 31.28995347\n",
      "Iteration 2291, loss = 33.16296543\n",
      "Iteration 2292, loss = 31.35074308\n",
      "Iteration 2293, loss = 32.25796113\n",
      "Iteration 2294, loss = 31.69039207\n",
      "Iteration 2295, loss = 31.19736120\n",
      "Iteration 2296, loss = 31.80944411\n",
      "Iteration 2297, loss = 32.11917979\n",
      "Iteration 2298, loss = 31.54887272\n",
      "Iteration 2299, loss = 31.39096542\n",
      "Iteration 2300, loss = 32.82727107\n",
      "Iteration 2301, loss = 31.57195760\n",
      "Iteration 2302, loss = 31.05851645\n",
      "Iteration 2303, loss = 31.59384232\n",
      "Iteration 2304, loss = 32.68047938\n",
      "Iteration 2305, loss = 32.00144150\n",
      "Iteration 2306, loss = 31.00794474\n",
      "Iteration 2307, loss = 31.46572711\n",
      "Iteration 2308, loss = 32.14448105\n",
      "Iteration 2309, loss = 32.33002388\n",
      "Iteration 2310, loss = 32.00319238\n",
      "Iteration 2311, loss = 32.49204528\n",
      "Iteration 2312, loss = 31.45705717\n",
      "Iteration 2313, loss = 31.07259055\n",
      "Iteration 2314, loss = 30.84940731\n",
      "Iteration 2315, loss = 30.85060996\n",
      "Iteration 2316, loss = 31.16651863\n",
      "Iteration 2317, loss = 30.93801061\n",
      "Iteration 2318, loss = 31.91976221\n",
      "Iteration 2319, loss = 31.98595578\n",
      "Iteration 2320, loss = 32.73177415\n",
      "Iteration 2321, loss = 32.65643203\n",
      "Iteration 2322, loss = 32.12157954\n",
      "Iteration 2323, loss = 32.08823880\n",
      "Iteration 2324, loss = 32.63811696\n",
      "Iteration 2325, loss = 32.34071271\n",
      "Iteration 2326, loss = 32.26993261\n",
      "Iteration 2327, loss = 30.97834443\n",
      "Iteration 2328, loss = 31.45241937\n",
      "Iteration 2329, loss = 32.36202106\n",
      "Iteration 2330, loss = 34.27354281\n",
      "Iteration 2331, loss = 33.35692080\n",
      "Iteration 2332, loss = 33.22177554\n",
      "Iteration 2333, loss = 32.42234556\n",
      "Iteration 2334, loss = 31.02570404\n",
      "Iteration 2335, loss = 31.31122397\n",
      "Iteration 2336, loss = 31.35573925\n",
      "Iteration 2337, loss = 31.39432755\n",
      "Iteration 2338, loss = 30.74363153\n",
      "Iteration 2339, loss = 30.65929934\n",
      "Iteration 2340, loss = 30.76428760\n",
      "Iteration 2341, loss = 31.53838764\n",
      "Iteration 2342, loss = 32.09747963\n",
      "Iteration 2343, loss = 30.83742795\n",
      "Iteration 2344, loss = 30.71178159\n",
      "Iteration 2345, loss = 30.53254266\n",
      "Iteration 2346, loss = 30.50276583\n",
      "Iteration 2347, loss = 31.63287126\n",
      "Iteration 2348, loss = 31.48784493\n",
      "Iteration 2349, loss = 30.77630937\n",
      "Iteration 2350, loss = 30.43526063\n",
      "Iteration 2351, loss = 30.83333994\n",
      "Iteration 2352, loss = 30.64312083\n",
      "Iteration 2353, loss = 30.90055888\n",
      "Iteration 2354, loss = 30.58320880\n",
      "Iteration 2355, loss = 31.90900857\n",
      "Iteration 2356, loss = 33.70432635\n",
      "Iteration 2357, loss = 33.88582366\n",
      "Iteration 2358, loss = 35.70257754\n",
      "Iteration 2359, loss = 37.12138314\n",
      "Iteration 2360, loss = 35.49664004\n",
      "Iteration 2361, loss = 33.13490780\n",
      "Iteration 2362, loss = 31.78024537\n",
      "Iteration 2363, loss = 34.18864302\n",
      "Iteration 2364, loss = 37.02234377\n",
      "Iteration 2365, loss = 37.66691160\n",
      "Iteration 2366, loss = 34.48343888\n",
      "Iteration 2367, loss = 34.16268573\n",
      "Iteration 2368, loss = 31.95701229\n",
      "Iteration 2369, loss = 30.62782134\n",
      "Iteration 2370, loss = 31.30540638\n",
      "Iteration 2371, loss = 31.71823177\n",
      "Iteration 2372, loss = 33.11626517\n",
      "Iteration 2373, loss = 31.32291985\n",
      "Iteration 2374, loss = 30.58669625\n",
      "Iteration 2375, loss = 30.93251510\n",
      "Iteration 2376, loss = 30.59262299\n",
      "Iteration 2377, loss = 30.66957284\n",
      "Iteration 2378, loss = 30.30735374\n",
      "Iteration 2379, loss = 30.68595248\n",
      "Iteration 2380, loss = 30.17293809\n",
      "Iteration 2381, loss = 30.92519857\n",
      "Iteration 2382, loss = 31.21399730\n",
      "Iteration 2383, loss = 31.21123343\n",
      "Iteration 2384, loss = 30.43064111\n",
      "Iteration 2385, loss = 32.28373548\n",
      "Iteration 2386, loss = 31.23859042\n",
      "Iteration 2387, loss = 30.17533961\n",
      "Iteration 2388, loss = 30.23285016\n",
      "Iteration 2389, loss = 30.32515416\n",
      "Iteration 2390, loss = 30.62632755\n",
      "Iteration 2391, loss = 30.89297972\n",
      "Iteration 2392, loss = 30.23274677\n",
      "Iteration 2393, loss = 30.03930850\n",
      "Iteration 2394, loss = 30.41252704\n",
      "Iteration 2395, loss = 30.23643152\n",
      "Iteration 2396, loss = 30.24240678\n",
      "Iteration 2397, loss = 30.13900815\n",
      "Iteration 2398, loss = 30.39910613\n",
      "Iteration 2399, loss = 30.25549674\n",
      "Iteration 2400, loss = 31.49777506\n",
      "Iteration 2401, loss = 31.94832472\n",
      "Iteration 2402, loss = 31.62025016\n",
      "Iteration 2403, loss = 31.22420322\n",
      "Iteration 2404, loss = 30.42865032\n",
      "Iteration 2405, loss = 30.02391339\n",
      "Iteration 2406, loss = 30.17744367\n",
      "Iteration 2407, loss = 30.65816855\n",
      "Iteration 2408, loss = 31.08396696\n",
      "Iteration 2409, loss = 30.64028263\n",
      "Iteration 2410, loss = 30.68655031\n",
      "Iteration 2411, loss = 30.23212410\n",
      "Iteration 2412, loss = 30.09463222\n",
      "Iteration 2413, loss = 32.24321205\n",
      "Iteration 2414, loss = 30.71790230\n",
      "Iteration 2415, loss = 31.04718604\n",
      "Iteration 2416, loss = 31.49762908\n",
      "Iteration 2417, loss = 30.32768725\n",
      "Iteration 2418, loss = 29.84120948\n",
      "Iteration 2419, loss = 29.88426911\n",
      "Iteration 2420, loss = 29.80693580\n",
      "Iteration 2421, loss = 30.08815203\n",
      "Iteration 2422, loss = 29.82945634\n",
      "Iteration 2423, loss = 29.98169034\n",
      "Iteration 2424, loss = 30.28493534\n",
      "Iteration 2425, loss = 30.07003124\n",
      "Iteration 2426, loss = 30.57022597\n",
      "Iteration 2427, loss = 32.15672272\n",
      "Iteration 2428, loss = 31.06707562\n",
      "Iteration 2429, loss = 30.45276283\n",
      "Iteration 2430, loss = 31.03216903\n",
      "Iteration 2431, loss = 31.61224428\n",
      "Iteration 2432, loss = 30.97553729\n",
      "Iteration 2433, loss = 30.19233378\n",
      "Iteration 2434, loss = 31.31657015\n",
      "Iteration 2435, loss = 30.71116080\n",
      "Iteration 2436, loss = 30.28065454\n",
      "Iteration 2437, loss = 30.67286207\n",
      "Iteration 2438, loss = 30.69046647\n",
      "Iteration 2439, loss = 30.27891152\n",
      "Iteration 2440, loss = 30.22084397\n",
      "Iteration 2441, loss = 30.53714048\n",
      "Iteration 2442, loss = 30.34683061\n",
      "Iteration 2443, loss = 31.55835823\n",
      "Iteration 2444, loss = 31.49621777\n",
      "Iteration 2445, loss = 30.86503955\n",
      "Iteration 2446, loss = 30.53877702\n",
      "Iteration 2447, loss = 30.14573419\n",
      "Iteration 2448, loss = 29.64886486\n",
      "Iteration 2449, loss = 29.90320457\n",
      "Iteration 2450, loss = 31.28085996\n",
      "Iteration 2451, loss = 30.22560014\n",
      "Iteration 2452, loss = 30.06569095\n",
      "Iteration 2453, loss = 30.66917423\n",
      "Iteration 2454, loss = 32.72148431\n",
      "Iteration 2455, loss = 31.19708172\n",
      "Iteration 2456, loss = 30.34121253\n",
      "Iteration 2457, loss = 30.19002413\n",
      "Iteration 2458, loss = 29.53511363\n",
      "Iteration 2459, loss = 29.71690256\n",
      "Iteration 2460, loss = 30.66335554\n",
      "Iteration 2461, loss = 30.63857993\n",
      "Iteration 2462, loss = 31.47658000\n",
      "Iteration 2463, loss = 30.97560642\n",
      "Iteration 2464, loss = 31.44162009\n",
      "Iteration 2465, loss = 30.80472154\n",
      "Iteration 2466, loss = 30.19966593\n",
      "Iteration 2467, loss = 29.60797169\n",
      "Iteration 2468, loss = 29.75588071\n",
      "Iteration 2469, loss = 30.14926138\n",
      "Iteration 2470, loss = 31.29561757\n",
      "Iteration 2471, loss = 30.63437816\n",
      "Iteration 2472, loss = 32.45011634\n",
      "Iteration 2473, loss = 31.18441488\n",
      "Iteration 2474, loss = 30.71846883\n",
      "Iteration 2475, loss = 29.78867493\n",
      "Iteration 2476, loss = 29.44360643\n",
      "Iteration 2477, loss = 29.75442112\n",
      "Iteration 2478, loss = 30.47192410\n",
      "Iteration 2479, loss = 31.37096535\n",
      "Iteration 2480, loss = 31.37677054\n",
      "Iteration 2481, loss = 31.10015634\n",
      "Iteration 2482, loss = 30.35201066\n",
      "Iteration 2483, loss = 30.73883710\n",
      "Iteration 2484, loss = 29.93069015\n",
      "Iteration 2485, loss = 29.67771737\n",
      "Iteration 2486, loss = 30.61423063\n",
      "Iteration 2487, loss = 29.66553842\n",
      "Iteration 2488, loss = 29.85284886\n",
      "Iteration 2489, loss = 30.19702638\n",
      "Iteration 2490, loss = 30.15320849\n",
      "Iteration 2491, loss = 29.18660258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2492, loss = 29.55126340\n",
      "Iteration 2493, loss = 29.95410944\n",
      "Iteration 2494, loss = 30.13478283\n",
      "Iteration 2495, loss = 29.62040637\n",
      "Iteration 2496, loss = 29.94766587\n",
      "Iteration 2497, loss = 30.35115259\n",
      "Iteration 2498, loss = 31.53425722\n",
      "Iteration 2499, loss = 31.26885506\n",
      "Iteration 2500, loss = 31.10777482\n",
      "Iteration 2501, loss = 32.31215313\n",
      "Iteration 2502, loss = 29.76247546\n",
      "Iteration 2503, loss = 29.58554949\n",
      "Iteration 2504, loss = 29.61521661\n",
      "Iteration 2505, loss = 31.19524401\n",
      "Iteration 2506, loss = 32.18628952\n",
      "Iteration 2507, loss = 31.29169983\n",
      "Iteration 2508, loss = 30.97855859\n",
      "Iteration 2509, loss = 32.72780620\n",
      "Iteration 2510, loss = 31.30942500\n",
      "Iteration 2511, loss = 30.30835761\n",
      "Iteration 2512, loss = 30.49368448\n",
      "Iteration 2513, loss = 29.04680953\n",
      "Iteration 2514, loss = 29.11078873\n",
      "Iteration 2515, loss = 29.01657244\n",
      "Iteration 2516, loss = 29.01853808\n",
      "Iteration 2517, loss = 29.13098096\n",
      "Iteration 2518, loss = 29.15067690\n",
      "Iteration 2519, loss = 29.39052054\n",
      "Iteration 2520, loss = 29.41076120\n",
      "Iteration 2521, loss = 29.65238130\n",
      "Iteration 2522, loss = 29.31616149\n",
      "Iteration 2523, loss = 29.41950114\n",
      "Iteration 2524, loss = 29.53846234\n",
      "Iteration 2525, loss = 29.94427267\n",
      "Iteration 2526, loss = 30.85930129\n",
      "Iteration 2527, loss = 29.77654410\n",
      "Iteration 2528, loss = 29.88681504\n",
      "Iteration 2529, loss = 30.02778919\n",
      "Iteration 2530, loss = 30.12077991\n",
      "Iteration 2531, loss = 32.46620594\n",
      "Iteration 2532, loss = 31.29528520\n",
      "Iteration 2533, loss = 30.80164241\n",
      "Iteration 2534, loss = 31.04327404\n",
      "Iteration 2535, loss = 30.36543805\n",
      "Iteration 2536, loss = 32.94120213\n",
      "Iteration 2537, loss = 30.67692773\n",
      "Iteration 2538, loss = 30.12339546\n",
      "Iteration 2539, loss = 29.10328840\n",
      "Iteration 2540, loss = 29.36752047\n",
      "Iteration 2541, loss = 30.28643265\n",
      "Iteration 2542, loss = 30.00796518\n",
      "Iteration 2543, loss = 30.82671548\n",
      "Iteration 2544, loss = 30.12121442\n",
      "Iteration 2545, loss = 29.22980203\n",
      "Iteration 2546, loss = 28.96702281\n",
      "Iteration 2547, loss = 29.42818351\n",
      "Iteration 2548, loss = 29.55552251\n",
      "Iteration 2549, loss = 29.22278934\n",
      "Iteration 2550, loss = 29.11389508\n",
      "Iteration 2551, loss = 28.72797321\n",
      "Iteration 2552, loss = 28.80283456\n",
      "Iteration 2553, loss = 30.87138582\n",
      "Iteration 2554, loss = 30.03061759\n",
      "Iteration 2555, loss = 29.19773438\n",
      "Iteration 2556, loss = 29.03606963\n",
      "Iteration 2557, loss = 29.74436812\n",
      "Iteration 2558, loss = 31.66252913\n",
      "Iteration 2559, loss = 31.12153657\n",
      "Iteration 2560, loss = 30.61234703\n",
      "Iteration 2561, loss = 30.02425024\n",
      "Iteration 2562, loss = 32.04556787\n",
      "Iteration 2563, loss = 31.78215094\n",
      "Iteration 2564, loss = 29.98368172\n",
      "Iteration 2565, loss = 31.14776298\n",
      "Iteration 2566, loss = 31.75838504\n",
      "Iteration 2567, loss = 29.30315465\n",
      "Iteration 2568, loss = 29.50228648\n",
      "Iteration 2569, loss = 28.75353011\n",
      "Iteration 2570, loss = 29.12825842\n",
      "Iteration 2571, loss = 29.17263683\n",
      "Iteration 2572, loss = 28.84623217\n",
      "Iteration 2573, loss = 29.10041727\n",
      "Iteration 2574, loss = 28.91882161\n",
      "Iteration 2575, loss = 29.25798780\n",
      "Iteration 2576, loss = 29.53948386\n",
      "Iteration 2577, loss = 29.86882976\n",
      "Iteration 2578, loss = 29.50072903\n",
      "Iteration 2579, loss = 29.50461822\n",
      "Iteration 2580, loss = 29.00668760\n",
      "Iteration 2581, loss = 28.60322475\n",
      "Iteration 2582, loss = 29.37638003\n",
      "Iteration 2583, loss = 28.94162578\n",
      "Iteration 2584, loss = 29.07534546\n",
      "Iteration 2585, loss = 28.59061946\n",
      "Iteration 2586, loss = 28.59201468\n",
      "Iteration 2587, loss = 28.57869107\n",
      "Iteration 2588, loss = 28.48969243\n",
      "Iteration 2589, loss = 28.59869539\n",
      "Iteration 2590, loss = 28.63967732\n",
      "Iteration 2591, loss = 29.01008961\n",
      "Iteration 2592, loss = 28.49602594\n",
      "Iteration 2593, loss = 29.43326967\n",
      "Iteration 2594, loss = 29.47417553\n",
      "Iteration 2595, loss = 29.05764203\n",
      "Iteration 2596, loss = 29.57936036\n",
      "Iteration 2597, loss = 28.44184738\n",
      "Iteration 2598, loss = 28.58871177\n",
      "Iteration 2599, loss = 28.47705673\n",
      "Iteration 2600, loss = 28.37542717\n",
      "Iteration 2601, loss = 29.14671258\n",
      "Iteration 2602, loss = 29.12600059\n",
      "Iteration 2603, loss = 28.34948252\n",
      "Iteration 2604, loss = 28.72984105\n",
      "Iteration 2605, loss = 29.66101067\n",
      "Iteration 2606, loss = 31.65993669\n",
      "Iteration 2607, loss = 30.28603932\n",
      "Iteration 2608, loss = 30.41980665\n",
      "Iteration 2609, loss = 29.78839362\n",
      "Iteration 2610, loss = 29.72449706\n",
      "Iteration 2611, loss = 28.92673906\n",
      "Iteration 2612, loss = 28.32817650\n",
      "Iteration 2613, loss = 28.64633391\n",
      "Iteration 2614, loss = 28.27121307\n",
      "Iteration 2615, loss = 28.20830822\n",
      "Iteration 2616, loss = 28.62710909\n",
      "Iteration 2617, loss = 28.55944179\n",
      "Iteration 2618, loss = 28.72395141\n",
      "Iteration 2619, loss = 29.70468515\n",
      "Iteration 2620, loss = 29.18628994\n",
      "Iteration 2621, loss = 28.75835589\n",
      "Iteration 2622, loss = 28.74261634\n",
      "Iteration 2623, loss = 28.84587868\n",
      "Iteration 2624, loss = 28.88696380\n",
      "Iteration 2625, loss = 30.29621423\n",
      "Iteration 2626, loss = 29.37239376\n",
      "Iteration 2627, loss = 29.08091254\n",
      "Iteration 2628, loss = 29.35557369\n",
      "Iteration 2629, loss = 29.23451261\n",
      "Iteration 2630, loss = 29.42153072\n",
      "Iteration 2631, loss = 29.50171314\n",
      "Iteration 2632, loss = 29.22117337\n",
      "Iteration 2633, loss = 28.98130322\n",
      "Iteration 2634, loss = 28.65627377\n",
      "Iteration 2635, loss = 29.12854370\n",
      "Iteration 2636, loss = 28.34069098\n",
      "Iteration 2637, loss = 28.76856827\n",
      "Iteration 2638, loss = 28.11150921\n",
      "Iteration 2639, loss = 28.46415682\n",
      "Iteration 2640, loss = 28.51883719\n",
      "Iteration 2641, loss = 28.86404373\n",
      "Iteration 2642, loss = 29.90326093\n",
      "Iteration 2643, loss = 28.43528046\n",
      "Iteration 2644, loss = 28.96047953\n",
      "Iteration 2645, loss = 29.11504477\n",
      "Iteration 2646, loss = 28.74043541\n",
      "Iteration 2647, loss = 27.84190660\n",
      "Iteration 2648, loss = 28.64090461\n",
      "Iteration 2649, loss = 28.47850143\n",
      "Iteration 2650, loss = 28.04084170\n",
      "Iteration 2651, loss = 28.72611307\n",
      "Iteration 2652, loss = 29.62039152\n",
      "Iteration 2653, loss = 28.36210833\n",
      "Iteration 2654, loss = 27.82782131\n",
      "Iteration 2655, loss = 27.86886838\n",
      "Iteration 2656, loss = 28.18132441\n",
      "Iteration 2657, loss = 28.56404547\n",
      "Iteration 2658, loss = 29.03136332\n",
      "Iteration 2659, loss = 28.35178450\n",
      "Iteration 2660, loss = 28.50676206\n",
      "Iteration 2661, loss = 30.20812590\n",
      "Iteration 2662, loss = 28.85595426\n",
      "Iteration 2663, loss = 28.78858812\n",
      "Iteration 2664, loss = 29.52596079\n",
      "Iteration 2665, loss = 28.65541062\n",
      "Iteration 2666, loss = 28.25201924\n",
      "Iteration 2667, loss = 28.72312182\n",
      "Iteration 2668, loss = 29.02322214\n",
      "Iteration 2669, loss = 28.56689732\n",
      "Iteration 2670, loss = 29.67492899\n",
      "Iteration 2671, loss = 29.42380546\n",
      "Iteration 2672, loss = 29.11588128\n",
      "Iteration 2673, loss = 27.99308298\n",
      "Iteration 2674, loss = 27.93475292\n",
      "Iteration 2675, loss = 28.30185219\n",
      "Iteration 2676, loss = 28.30611403\n",
      "Iteration 2677, loss = 28.86719806\n",
      "Iteration 2678, loss = 27.96533248\n",
      "Iteration 2679, loss = 27.88479754\n",
      "Iteration 2680, loss = 28.33738221\n",
      "Iteration 2681, loss = 28.25182654\n",
      "Iteration 2682, loss = 27.98401370\n",
      "Iteration 2683, loss = 28.33862229\n",
      "Iteration 2684, loss = 28.03492433\n",
      "Iteration 2685, loss = 27.82921223\n",
      "Iteration 2686, loss = 28.44747702\n",
      "Iteration 2687, loss = 29.12301884\n",
      "Iteration 2688, loss = 29.02931073\n",
      "Iteration 2689, loss = 29.45529360\n",
      "Iteration 2690, loss = 29.47238777\n",
      "Iteration 2691, loss = 29.12210760\n",
      "Iteration 2692, loss = 28.53826448\n",
      "Iteration 2693, loss = 28.56466296\n",
      "Iteration 2694, loss = 28.26192108\n",
      "Iteration 2695, loss = 29.22320312\n",
      "Iteration 2696, loss = 28.84001133\n",
      "Iteration 2697, loss = 28.48780643\n",
      "Iteration 2698, loss = 28.53315627\n",
      "Iteration 2699, loss = 29.14330634\n",
      "Iteration 2700, loss = 28.05066246\n",
      "Iteration 2701, loss = 29.12791147\n",
      "Iteration 2702, loss = 28.78165671\n",
      "Iteration 2703, loss = 29.05520286\n",
      "Iteration 2704, loss = 28.31917090\n",
      "Iteration 2705, loss = 28.49839688\n",
      "Iteration 2706, loss = 27.94361692\n",
      "Iteration 2707, loss = 29.36996379\n",
      "Iteration 2708, loss = 29.84311470\n",
      "Iteration 2709, loss = 28.46632999\n",
      "Iteration 2710, loss = 27.98912665\n",
      "Iteration 2711, loss = 28.29400208\n",
      "Iteration 2712, loss = 28.20709799\n",
      "Iteration 2713, loss = 29.38592081\n",
      "Iteration 2714, loss = 28.56239774\n",
      "Iteration 2715, loss = 28.97900102\n",
      "Iteration 2716, loss = 28.95436927\n",
      "Iteration 2717, loss = 28.53492519\n",
      "Iteration 2718, loss = 29.45474041\n",
      "Iteration 2719, loss = 28.81672760\n",
      "Iteration 2720, loss = 30.15750907\n",
      "Iteration 2721, loss = 29.87684566\n",
      "Iteration 2722, loss = 28.22290643\n",
      "Iteration 2723, loss = 29.20058036\n",
      "Iteration 2724, loss = 28.50494206\n",
      "Iteration 2725, loss = 28.31839721\n",
      "Iteration 2726, loss = 28.15957810\n",
      "Iteration 2727, loss = 28.96874695\n",
      "Iteration 2728, loss = 29.93248973\n",
      "Iteration 2729, loss = 28.55874542\n",
      "Iteration 2730, loss = 28.97755993\n",
      "Iteration 2731, loss = 29.92556234\n",
      "Iteration 2732, loss = 29.34218908\n",
      "Iteration 2733, loss = 27.70404079\n",
      "Iteration 2734, loss = 28.08297818\n",
      "Iteration 2735, loss = 27.63195823\n",
      "Iteration 2736, loss = 28.62787308\n",
      "Iteration 2737, loss = 28.68947388\n",
      "Iteration 2738, loss = 28.22667725\n",
      "Iteration 2739, loss = 27.70729186\n",
      "Iteration 2740, loss = 28.41765488\n",
      "Iteration 2741, loss = 28.58473417\n",
      "Iteration 2742, loss = 27.58935187\n",
      "Iteration 2743, loss = 29.86017873\n",
      "Iteration 2744, loss = 28.61031522\n",
      "Iteration 2745, loss = 31.76340223\n",
      "Iteration 2746, loss = 30.68868091\n",
      "Iteration 2747, loss = 28.37320307\n",
      "Iteration 2748, loss = 28.94879425\n",
      "Iteration 2749, loss = 29.07813115\n",
      "Iteration 2750, loss = 28.83666574\n",
      "Iteration 2751, loss = 28.28261557\n",
      "Iteration 2752, loss = 28.83735518\n",
      "Iteration 2753, loss = 28.21182411\n",
      "Iteration 2754, loss = 28.46123740\n",
      "Iteration 2755, loss = 27.41223598\n",
      "Iteration 2756, loss = 27.99922501\n",
      "Iteration 2757, loss = 28.64157776\n",
      "Iteration 2758, loss = 27.66339635\n",
      "Iteration 2759, loss = 27.28360714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2760, loss = 27.67708719\n",
      "Iteration 2761, loss = 27.58182048\n",
      "Iteration 2762, loss = 28.59416806\n",
      "Iteration 2763, loss = 29.22066451\n",
      "Iteration 2764, loss = 29.90310453\n",
      "Iteration 2765, loss = 29.22098914\n",
      "Iteration 2766, loss = 27.93844439\n",
      "Iteration 2767, loss = 26.92602577\n",
      "Iteration 2768, loss = 27.62779815\n",
      "Iteration 2769, loss = 27.45703274\n",
      "Iteration 2770, loss = 27.99724491\n",
      "Iteration 2771, loss = 28.35699547\n",
      "Iteration 2772, loss = 28.94041989\n",
      "Iteration 2773, loss = 28.35121869\n",
      "Iteration 2774, loss = 28.39970721\n",
      "Iteration 2775, loss = 28.16760187\n",
      "Iteration 2776, loss = 27.96311015\n",
      "Iteration 2777, loss = 27.65251387\n",
      "Iteration 2778, loss = 27.59736243\n",
      "Iteration 2779, loss = 27.90847671\n",
      "Iteration 2780, loss = 28.23456941\n",
      "Iteration 2781, loss = 27.33590184\n",
      "Iteration 2782, loss = 27.27391758\n",
      "Iteration 2783, loss = 27.94175707\n",
      "Iteration 2784, loss = 27.30052354\n",
      "Iteration 2785, loss = 28.40625633\n",
      "Iteration 2786, loss = 29.44845665\n",
      "Iteration 2787, loss = 29.46011280\n",
      "Iteration 2788, loss = 27.98817640\n",
      "Iteration 2789, loss = 27.31522774\n",
      "Iteration 2790, loss = 28.14048639\n",
      "Iteration 2791, loss = 28.50244324\n",
      "Iteration 2792, loss = 28.94835141\n",
      "Iteration 2793, loss = 28.27507387\n",
      "Iteration 2794, loss = 27.67646050\n",
      "Iteration 2795, loss = 27.23456235\n",
      "Iteration 2796, loss = 26.90543324\n",
      "Iteration 2797, loss = 28.43869920\n",
      "Iteration 2798, loss = 29.41901921\n",
      "Iteration 2799, loss = 29.86628602\n",
      "Iteration 2800, loss = 28.90102841\n",
      "Iteration 2801, loss = 27.35785260\n",
      "Iteration 2802, loss = 26.93977348\n",
      "Iteration 2803, loss = 26.93584038\n",
      "Iteration 2804, loss = 27.06803391\n",
      "Iteration 2805, loss = 27.28154953\n",
      "Iteration 2806, loss = 27.40497612\n",
      "Iteration 2807, loss = 26.90059888\n",
      "Iteration 2808, loss = 26.89498620\n",
      "Iteration 2809, loss = 27.09307375\n",
      "Iteration 2810, loss = 26.84180613\n",
      "Iteration 2811, loss = 27.88026740\n",
      "Iteration 2812, loss = 28.02469919\n",
      "Iteration 2813, loss = 29.26326653\n",
      "Iteration 2814, loss = 29.34520262\n",
      "Iteration 2815, loss = 27.18734873\n",
      "Iteration 2816, loss = 27.43416065\n",
      "Iteration 2817, loss = 28.39486234\n",
      "Iteration 2818, loss = 28.92760943\n",
      "Iteration 2819, loss = 28.28829430\n",
      "Iteration 2820, loss = 27.90489256\n",
      "Iteration 2821, loss = 27.10559194\n",
      "Iteration 2822, loss = 27.27421083\n",
      "Iteration 2823, loss = 26.70923037\n",
      "Iteration 2824, loss = 26.83479268\n",
      "Iteration 2825, loss = 26.60616705\n",
      "Iteration 2826, loss = 26.95964526\n",
      "Iteration 2827, loss = 27.15116915\n",
      "Iteration 2828, loss = 27.29620836\n",
      "Iteration 2829, loss = 27.15102159\n",
      "Iteration 2830, loss = 27.05511309\n",
      "Iteration 2831, loss = 27.76874624\n",
      "Iteration 2832, loss = 27.17883287\n",
      "Iteration 2833, loss = 29.08100540\n",
      "Iteration 2834, loss = 29.40375249\n",
      "Iteration 2835, loss = 28.85436504\n",
      "Iteration 2836, loss = 28.88181909\n",
      "Iteration 2837, loss = 26.95912528\n",
      "Iteration 2838, loss = 26.94212775\n",
      "Iteration 2839, loss = 27.11752505\n",
      "Iteration 2840, loss = 26.78142446\n",
      "Iteration 2841, loss = 26.89922462\n",
      "Iteration 2842, loss = 26.72978270\n",
      "Iteration 2843, loss = 26.71870979\n",
      "Iteration 2844, loss = 26.78013322\n",
      "Iteration 2845, loss = 28.59847019\n",
      "Iteration 2846, loss = 27.38169807\n",
      "Iteration 2847, loss = 27.00862024\n",
      "Iteration 2848, loss = 26.53526360\n",
      "Iteration 2849, loss = 26.93301219\n",
      "Iteration 2850, loss = 26.76246066\n",
      "Iteration 2851, loss = 26.70803637\n",
      "Iteration 2852, loss = 27.38864452\n",
      "Iteration 2853, loss = 27.18547956\n",
      "Iteration 2854, loss = 27.46243893\n",
      "Iteration 2855, loss = 27.90531218\n",
      "Iteration 2856, loss = 27.03419647\n",
      "Iteration 2857, loss = 26.73501094\n",
      "Iteration 2858, loss = 26.97687664\n",
      "Iteration 2859, loss = 27.26259887\n",
      "Iteration 2860, loss = 26.39043696\n",
      "Iteration 2861, loss = 26.49433260\n",
      "Iteration 2862, loss = 26.92048294\n",
      "Iteration 2863, loss = 27.48631692\n",
      "Iteration 2864, loss = 26.73316026\n",
      "Iteration 2865, loss = 26.87010905\n",
      "Iteration 2866, loss = 27.45584603\n",
      "Iteration 2867, loss = 27.64390876\n",
      "Iteration 2868, loss = 27.64728482\n",
      "Iteration 2869, loss = 27.45118887\n",
      "Iteration 2870, loss = 27.24887923\n",
      "Iteration 2871, loss = 26.75685981\n",
      "Iteration 2872, loss = 27.52924382\n",
      "Iteration 2873, loss = 27.75138791\n",
      "Iteration 2874, loss = 27.11655914\n",
      "Iteration 2875, loss = 26.80987636\n",
      "Iteration 2876, loss = 27.14555502\n",
      "Iteration 2877, loss = 27.65167612\n",
      "Iteration 2878, loss = 28.98193343\n",
      "Iteration 2879, loss = 28.69756891\n",
      "Iteration 2880, loss = 28.10099164\n",
      "Iteration 2881, loss = 26.59602897\n",
      "Iteration 2882, loss = 27.43484521\n",
      "Iteration 2883, loss = 29.12688847\n",
      "Iteration 2884, loss = 28.95360532\n",
      "Iteration 2885, loss = 29.17585424\n",
      "Iteration 2886, loss = 26.67129711\n",
      "Iteration 2887, loss = 26.26038428\n",
      "Iteration 2888, loss = 26.66849416\n",
      "Iteration 2889, loss = 26.82310005\n",
      "Iteration 2890, loss = 26.98624464\n",
      "Iteration 2891, loss = 25.99441966\n",
      "Iteration 2892, loss = 26.48322872\n",
      "Iteration 2893, loss = 26.78001154\n",
      "Iteration 2894, loss = 27.19850988\n",
      "Iteration 2895, loss = 26.71042701\n",
      "Iteration 2896, loss = 26.32823117\n",
      "Iteration 2897, loss = 26.32308836\n",
      "Iteration 2898, loss = 26.22311539\n",
      "Iteration 2899, loss = 27.52333259\n",
      "Iteration 2900, loss = 27.76794269\n",
      "Iteration 2901, loss = 28.32105629\n",
      "Iteration 2902, loss = 27.03613994\n",
      "Iteration 2903, loss = 27.09273693\n",
      "Iteration 2904, loss = 27.49485028\n",
      "Iteration 2905, loss = 26.22243484\n",
      "Iteration 2906, loss = 26.68465402\n",
      "Iteration 2907, loss = 27.29828745\n",
      "Iteration 2908, loss = 26.88827228\n",
      "Iteration 2909, loss = 26.85552365\n",
      "Iteration 2910, loss = 26.30932518\n",
      "Iteration 2911, loss = 27.00427916\n",
      "Iteration 2912, loss = 27.06652022\n",
      "Iteration 2913, loss = 26.69789059\n",
      "Iteration 2914, loss = 26.38579137\n",
      "Iteration 2915, loss = 26.09496391\n",
      "Iteration 2916, loss = 27.38423201\n",
      "Iteration 2917, loss = 27.00221388\n",
      "Iteration 2918, loss = 26.05553230\n",
      "Iteration 2919, loss = 26.04480060\n",
      "Iteration 2920, loss = 25.97670627\n",
      "Iteration 2921, loss = 26.42720351\n",
      "Iteration 2922, loss = 26.37801530\n",
      "Iteration 2923, loss = 27.40897851\n",
      "Iteration 2924, loss = 26.15539369\n",
      "Iteration 2925, loss = 27.01636681\n",
      "Iteration 2926, loss = 26.38291380\n",
      "Iteration 2927, loss = 26.18714935\n",
      "Iteration 2928, loss = 26.28189961\n",
      "Iteration 2929, loss = 25.76037070\n",
      "Iteration 2930, loss = 25.99381244\n",
      "Iteration 2931, loss = 27.37531406\n",
      "Iteration 2932, loss = 26.62404534\n",
      "Iteration 2933, loss = 27.08145842\n",
      "Iteration 2934, loss = 26.25136428\n",
      "Iteration 2935, loss = 26.59522182\n",
      "Iteration 2936, loss = 26.61889632\n",
      "Iteration 2937, loss = 26.72352683\n",
      "Iteration 2938, loss = 26.42100178\n",
      "Iteration 2939, loss = 26.96732108\n",
      "Iteration 2940, loss = 26.05198789\n",
      "Iteration 2941, loss = 26.26376409\n",
      "Iteration 2942, loss = 26.20334805\n",
      "Iteration 2943, loss = 26.34271161\n",
      "Iteration 2944, loss = 26.81344468\n",
      "Iteration 2945, loss = 26.36212634\n",
      "Iteration 2946, loss = 26.36433980\n",
      "Iteration 2947, loss = 25.80051208\n",
      "Iteration 2948, loss = 25.71490234\n",
      "Iteration 2949, loss = 26.06113281\n",
      "Iteration 2950, loss = 26.58469357\n",
      "Iteration 2951, loss = 26.97459012\n",
      "Iteration 2952, loss = 26.24617509\n",
      "Iteration 2953, loss = 25.94382968\n",
      "Iteration 2954, loss = 25.60569456\n",
      "Iteration 2955, loss = 25.96745346\n",
      "Iteration 2956, loss = 26.39678927\n",
      "Iteration 2957, loss = 26.64250460\n",
      "Iteration 2958, loss = 26.60678820\n",
      "Iteration 2959, loss = 26.61978379\n",
      "Iteration 2960, loss = 26.88352392\n",
      "Iteration 2961, loss = 26.37835899\n",
      "Iteration 2962, loss = 27.82014868\n",
      "Iteration 2963, loss = 27.11382110\n",
      "Iteration 2964, loss = 26.86287345\n",
      "Iteration 2965, loss = 26.58187397\n",
      "Iteration 2966, loss = 27.98776135\n",
      "Iteration 2967, loss = 29.30051602\n",
      "Iteration 2968, loss = 28.55275399\n",
      "Iteration 2969, loss = 27.37081781\n",
      "Iteration 2970, loss = 27.07267718\n",
      "Iteration 2971, loss = 26.92994997\n",
      "Iteration 2972, loss = 25.81455671\n",
      "Iteration 2973, loss = 25.46917866\n",
      "Iteration 2974, loss = 25.44971407\n",
      "Iteration 2975, loss = 25.80831320\n",
      "Iteration 2976, loss = 26.47260480\n",
      "Iteration 2977, loss = 26.63834102\n",
      "Iteration 2978, loss = 26.54581036\n",
      "Iteration 2979, loss = 25.93062992\n",
      "Iteration 2980, loss = 25.87902109\n",
      "Iteration 2981, loss = 25.91134348\n",
      "Iteration 2982, loss = 25.51186053\n",
      "Iteration 2983, loss = 25.60747276\n",
      "Iteration 2984, loss = 26.24082708\n",
      "Iteration 2985, loss = 25.80766325\n",
      "Iteration 2986, loss = 27.40177201\n",
      "Iteration 2987, loss = 28.47937574\n",
      "Iteration 2988, loss = 27.85649270\n",
      "Iteration 2989, loss = 27.12830303\n",
      "Iteration 2990, loss = 26.17217824\n",
      "Iteration 2991, loss = 26.17541848\n",
      "Iteration 2992, loss = 25.73596236\n",
      "Iteration 2993, loss = 25.20649363\n",
      "Iteration 2994, loss = 25.52254245\n",
      "Iteration 2995, loss = 26.50577439\n",
      "Iteration 2996, loss = 26.35243564\n",
      "Iteration 2997, loss = 26.30156886\n",
      "Iteration 2998, loss = 25.66488360\n",
      "Iteration 2999, loss = 26.61485741\n",
      "Iteration 3000, loss = 26.45280486\n",
      "Iteration 3001, loss = 26.85545194\n",
      "Iteration 3002, loss = 25.50556237\n",
      "Iteration 3003, loss = 26.21132561\n",
      "Iteration 3004, loss = 26.96124218\n",
      "Iteration 3005, loss = 25.96961330\n",
      "Iteration 3006, loss = 25.71950125\n",
      "Iteration 3007, loss = 25.50111901\n",
      "Iteration 3008, loss = 25.59712031\n",
      "Iteration 3009, loss = 26.96901154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3010, loss = 27.21507051\n",
      "Iteration 3011, loss = 26.30436585\n",
      "Iteration 3012, loss = 25.37196912\n",
      "Iteration 3013, loss = 25.24757982\n",
      "Iteration 3014, loss = 25.64298190\n",
      "Iteration 3015, loss = 25.79448934\n",
      "Iteration 3016, loss = 26.72015592\n",
      "Iteration 3017, loss = 25.64829118\n",
      "Iteration 3018, loss = 25.58515726\n",
      "Iteration 3019, loss = 25.60586077\n",
      "Iteration 3020, loss = 25.99966187\n",
      "Iteration 3021, loss = 26.31859317\n",
      "Iteration 3022, loss = 25.78277570\n",
      "Iteration 3023, loss = 25.49378620\n",
      "Iteration 3024, loss = 25.38241920\n",
      "Iteration 3025, loss = 25.89126728\n",
      "Iteration 3026, loss = 26.18941190\n",
      "Iteration 3027, loss = 26.93494387\n",
      "Iteration 3028, loss = 26.16122736\n",
      "Iteration 3029, loss = 25.65883364\n",
      "Iteration 3030, loss = 25.39547503\n",
      "Iteration 3031, loss = 25.62020236\n",
      "Iteration 3032, loss = 25.38841868\n",
      "Iteration 3033, loss = 25.52835298\n",
      "Iteration 3034, loss = 25.12870436\n",
      "Iteration 3035, loss = 25.18231905\n",
      "Iteration 3036, loss = 25.43042341\n",
      "Iteration 3037, loss = 25.87157361\n",
      "Iteration 3038, loss = 25.17646340\n",
      "Iteration 3039, loss = 25.26588541\n",
      "Iteration 3040, loss = 25.42948380\n",
      "Iteration 3041, loss = 27.76366919\n",
      "Iteration 3042, loss = 26.99640430\n",
      "Iteration 3043, loss = 25.81361482\n",
      "Iteration 3044, loss = 25.66673041\n",
      "Iteration 3045, loss = 25.24963820\n",
      "Iteration 3046, loss = 24.99202095\n",
      "Iteration 3047, loss = 25.31055761\n",
      "Iteration 3048, loss = 24.94701929\n",
      "Iteration 3049, loss = 25.74775871\n",
      "Iteration 3050, loss = 26.05828798\n",
      "Iteration 3051, loss = 25.74862411\n",
      "Iteration 3052, loss = 24.95675535\n",
      "Iteration 3053, loss = 25.31287063\n",
      "Iteration 3054, loss = 25.21142154\n",
      "Iteration 3055, loss = 24.89776018\n",
      "Iteration 3056, loss = 25.49142929\n",
      "Iteration 3057, loss = 25.34997347\n",
      "Iteration 3058, loss = 25.06163934\n",
      "Iteration 3059, loss = 25.36913703\n",
      "Iteration 3060, loss = 24.93802167\n",
      "Iteration 3061, loss = 25.64476679\n",
      "Iteration 3062, loss = 25.55099930\n",
      "Iteration 3063, loss = 25.03369359\n",
      "Iteration 3064, loss = 24.69735077\n",
      "Iteration 3065, loss = 25.74303059\n",
      "Iteration 3066, loss = 25.03641349\n",
      "Iteration 3067, loss = 25.23774221\n",
      "Iteration 3068, loss = 24.81512609\n",
      "Iteration 3069, loss = 25.34752866\n",
      "Iteration 3070, loss = 25.76643738\n",
      "Iteration 3071, loss = 24.87443055\n",
      "Iteration 3072, loss = 25.54714577\n",
      "Iteration 3073, loss = 25.10846546\n",
      "Iteration 3074, loss = 24.83702449\n",
      "Iteration 3075, loss = 24.91014831\n",
      "Iteration 3076, loss = 24.55857347\n",
      "Iteration 3077, loss = 24.61456361\n",
      "Iteration 3078, loss = 25.68275353\n",
      "Iteration 3079, loss = 24.88334603\n",
      "Iteration 3080, loss = 24.95432365\n",
      "Iteration 3081, loss = 25.13635007\n",
      "Iteration 3082, loss = 25.16174790\n",
      "Iteration 3083, loss = 24.48051794\n",
      "Iteration 3084, loss = 25.03628058\n",
      "Iteration 3085, loss = 25.95037038\n",
      "Iteration 3086, loss = 26.05587584\n",
      "Iteration 3087, loss = 24.73109342\n",
      "Iteration 3088, loss = 24.57341761\n",
      "Iteration 3089, loss = 25.52894428\n",
      "Iteration 3090, loss = 24.95316375\n",
      "Iteration 3091, loss = 25.01019957\n",
      "Iteration 3092, loss = 24.83414979\n",
      "Iteration 3093, loss = 24.61357573\n",
      "Iteration 3094, loss = 24.71283631\n",
      "Iteration 3095, loss = 24.41290631\n",
      "Iteration 3096, loss = 24.92540672\n",
      "Iteration 3097, loss = 25.88301758\n",
      "Iteration 3098, loss = 24.82066666\n",
      "Iteration 3099, loss = 24.56916732\n",
      "Iteration 3100, loss = 25.42913612\n",
      "Iteration 3101, loss = 25.06388920\n",
      "Iteration 3102, loss = 25.15836150\n",
      "Iteration 3103, loss = 24.41469801\n",
      "Iteration 3104, loss = 24.70451099\n",
      "Iteration 3105, loss = 24.65610345\n",
      "Iteration 3106, loss = 24.83123661\n",
      "Iteration 3107, loss = 24.58803357\n",
      "Iteration 3108, loss = 24.65844772\n",
      "Iteration 3109, loss = 24.92746888\n",
      "Iteration 3110, loss = 26.63835656\n",
      "Iteration 3111, loss = 25.73356338\n",
      "Iteration 3112, loss = 24.65138400\n",
      "Iteration 3113, loss = 24.96926072\n",
      "Iteration 3114, loss = 24.33094599\n",
      "Iteration 3115, loss = 24.66973174\n",
      "Iteration 3116, loss = 24.39683858\n",
      "Iteration 3117, loss = 24.38043928\n",
      "Iteration 3118, loss = 24.38515088\n",
      "Iteration 3119, loss = 24.49677932\n",
      "Iteration 3120, loss = 26.02329968\n",
      "Iteration 3121, loss = 26.12694996\n",
      "Iteration 3122, loss = 26.21320882\n",
      "Iteration 3123, loss = 25.02539189\n",
      "Iteration 3124, loss = 25.01340764\n",
      "Iteration 3125, loss = 24.90891593\n",
      "Iteration 3126, loss = 25.20464202\n",
      "Iteration 3127, loss = 26.72449032\n",
      "Iteration 3128, loss = 26.08686125\n",
      "Iteration 3129, loss = 24.52094700\n",
      "Iteration 3130, loss = 24.16171617\n",
      "Iteration 3131, loss = 24.16532817\n",
      "Iteration 3132, loss = 24.21431131\n",
      "Iteration 3133, loss = 26.46584458\n",
      "Iteration 3134, loss = 26.09736759\n",
      "Iteration 3135, loss = 26.35235942\n",
      "Iteration 3136, loss = 24.73686837\n",
      "Iteration 3137, loss = 24.69565976\n",
      "Iteration 3138, loss = 24.94070808\n",
      "Iteration 3139, loss = 25.30855620\n",
      "Iteration 3140, loss = 25.44237208\n",
      "Iteration 3141, loss = 26.98817577\n",
      "Iteration 3142, loss = 25.74350195\n",
      "Iteration 3143, loss = 24.70871491\n",
      "Iteration 3144, loss = 24.41334425\n",
      "Iteration 3145, loss = 24.21834911\n",
      "Iteration 3146, loss = 24.37632542\n",
      "Iteration 3147, loss = 23.89029624\n",
      "Iteration 3148, loss = 24.25542272\n",
      "Iteration 3149, loss = 24.19756912\n",
      "Iteration 3150, loss = 24.29405224\n",
      "Iteration 3151, loss = 24.35471242\n",
      "Iteration 3152, loss = 24.96776504\n",
      "Iteration 3153, loss = 25.65782180\n",
      "Iteration 3154, loss = 25.15368489\n",
      "Iteration 3155, loss = 24.22311250\n",
      "Iteration 3156, loss = 24.00701752\n",
      "Iteration 3157, loss = 23.83569317\n",
      "Iteration 3158, loss = 24.98843374\n",
      "Iteration 3159, loss = 25.96749080\n",
      "Iteration 3160, loss = 25.30254177\n",
      "Iteration 3161, loss = 25.51120720\n",
      "Iteration 3162, loss = 25.73274896\n",
      "Iteration 3163, loss = 26.63928523\n",
      "Iteration 3164, loss = 26.89599915\n",
      "Iteration 3165, loss = 25.89716463\n",
      "Iteration 3166, loss = 23.81807246\n",
      "Iteration 3167, loss = 24.09404655\n",
      "Iteration 3168, loss = 23.96977269\n",
      "Iteration 3169, loss = 24.95943521\n",
      "Iteration 3170, loss = 25.73669380\n",
      "Iteration 3171, loss = 24.21241371\n",
      "Iteration 3172, loss = 24.47596315\n",
      "Iteration 3173, loss = 24.36667045\n",
      "Iteration 3174, loss = 24.13199209\n",
      "Iteration 3175, loss = 24.04845867\n",
      "Iteration 3176, loss = 25.23159943\n",
      "Iteration 3177, loss = 25.74657259\n",
      "Iteration 3178, loss = 24.42508328\n",
      "Iteration 3179, loss = 24.01591552\n",
      "Iteration 3180, loss = 24.86951403\n",
      "Iteration 3181, loss = 24.54382650\n",
      "Iteration 3182, loss = 24.29092010\n",
      "Iteration 3183, loss = 24.67759229\n",
      "Iteration 3184, loss = 24.31975171\n",
      "Iteration 3185, loss = 24.29515014\n",
      "Iteration 3186, loss = 24.02583143\n",
      "Iteration 3187, loss = 24.00408333\n",
      "Iteration 3188, loss = 24.36486098\n",
      "Iteration 3189, loss = 24.42959571\n",
      "Iteration 3190, loss = 23.87283631\n",
      "Iteration 3191, loss = 24.44940118\n",
      "Iteration 3192, loss = 24.45115191\n",
      "Iteration 3193, loss = 23.64564917\n",
      "Iteration 3194, loss = 23.83601307\n",
      "Iteration 3195, loss = 23.70186434\n",
      "Iteration 3196, loss = 24.28108023\n",
      "Iteration 3197, loss = 24.39378399\n",
      "Iteration 3198, loss = 24.03340705\n",
      "Iteration 3199, loss = 24.53662638\n",
      "Iteration 3200, loss = 24.11147990\n",
      "Iteration 3201, loss = 23.98205501\n",
      "Iteration 3202, loss = 24.02584657\n",
      "Iteration 3203, loss = 23.74586012\n",
      "Iteration 3204, loss = 23.81219724\n",
      "Iteration 3205, loss = 24.80386168\n",
      "Iteration 3206, loss = 25.17149452\n",
      "Iteration 3207, loss = 24.95036473\n",
      "Iteration 3208, loss = 26.99787710\n",
      "Iteration 3209, loss = 25.28793623\n",
      "Iteration 3210, loss = 25.94550525\n",
      "Iteration 3211, loss = 25.23790706\n",
      "Iteration 3212, loss = 25.18340621\n",
      "Iteration 3213, loss = 23.95983152\n",
      "Iteration 3214, loss = 24.26643498\n",
      "Iteration 3215, loss = 24.73710676\n",
      "Iteration 3216, loss = 24.14242554\n",
      "Iteration 3217, loss = 24.14334331\n",
      "Iteration 3218, loss = 26.08829638\n",
      "Iteration 3219, loss = 25.42466259\n",
      "Iteration 3220, loss = 26.46475628\n",
      "Iteration 3221, loss = 27.16430919\n",
      "Iteration 3222, loss = 26.52834534\n",
      "Iteration 3223, loss = 24.86549221\n",
      "Iteration 3224, loss = 24.55867900\n",
      "Iteration 3225, loss = 23.89094572\n",
      "Iteration 3226, loss = 24.33820840\n",
      "Iteration 3227, loss = 25.27250742\n",
      "Iteration 3228, loss = 25.43480723\n",
      "Iteration 3229, loss = 25.65565722\n",
      "Iteration 3230, loss = 24.27597860\n",
      "Iteration 3231, loss = 25.31765960\n",
      "Iteration 3232, loss = 24.37565552\n",
      "Iteration 3233, loss = 24.19860132\n",
      "Iteration 3234, loss = 23.69961187\n",
      "Iteration 3235, loss = 24.20348356\n",
      "Iteration 3236, loss = 23.74385674\n",
      "Iteration 3237, loss = 23.95538910\n",
      "Iteration 3238, loss = 23.49834312\n",
      "Iteration 3239, loss = 23.34164422\n",
      "Iteration 3240, loss = 23.35628038\n",
      "Iteration 3241, loss = 23.47699899\n",
      "Iteration 3242, loss = 23.65071970\n",
      "Iteration 3243, loss = 24.10215254\n",
      "Iteration 3244, loss = 23.84837417\n",
      "Iteration 3245, loss = 24.30056720\n",
      "Iteration 3246, loss = 23.69210612\n",
      "Iteration 3247, loss = 23.69165028\n",
      "Iteration 3248, loss = 24.05855286\n",
      "Iteration 3249, loss = 23.64760416\n",
      "Iteration 3250, loss = 23.22350908\n",
      "Iteration 3251, loss = 23.40130465\n",
      "Iteration 3252, loss = 23.72496075\n",
      "Iteration 3253, loss = 23.41956990\n",
      "Iteration 3254, loss = 23.00122714\n",
      "Iteration 3255, loss = 23.83247018\n",
      "Iteration 3256, loss = 24.71574451\n",
      "Iteration 3257, loss = 24.19984789\n",
      "Iteration 3258, loss = 24.16243698\n",
      "Iteration 3259, loss = 23.34792401\n",
      "Iteration 3260, loss = 23.63705294\n",
      "Iteration 3261, loss = 23.28914253\n",
      "Iteration 3262, loss = 23.29754171\n",
      "Iteration 3263, loss = 22.85868700\n",
      "Iteration 3264, loss = 23.05609595\n",
      "Iteration 3265, loss = 22.90014155\n",
      "Iteration 3266, loss = 22.87380839\n",
      "Iteration 3267, loss = 23.06439896\n",
      "Iteration 3268, loss = 22.90143185\n",
      "Iteration 3269, loss = 23.20634690\n",
      "Iteration 3270, loss = 23.53373955\n",
      "Iteration 3271, loss = 23.20674259\n",
      "Iteration 3272, loss = 24.08993953\n",
      "Iteration 3273, loss = 25.33970708\n",
      "Iteration 3274, loss = 24.58682194\n",
      "Iteration 3275, loss = 25.12633784\n",
      "Iteration 3276, loss = 23.54096229\n",
      "Iteration 3277, loss = 23.48617193\n",
      "Iteration 3278, loss = 23.44894852\n",
      "Iteration 3279, loss = 23.36045558\n",
      "Iteration 3280, loss = 22.76959802\n",
      "Iteration 3281, loss = 24.00546716\n",
      "Iteration 3282, loss = 24.54245654\n",
      "Iteration 3283, loss = 25.42536530\n",
      "Iteration 3284, loss = 24.41980399\n",
      "Iteration 3285, loss = 23.53244293\n",
      "Iteration 3286, loss = 24.41864257\n",
      "Iteration 3287, loss = 24.04038241\n",
      "Iteration 3288, loss = 24.04519462\n",
      "Iteration 3289, loss = 23.63640454\n",
      "Iteration 3290, loss = 23.23293933\n",
      "Iteration 3291, loss = 23.86221556\n",
      "Iteration 3292, loss = 23.06614064\n",
      "Iteration 3293, loss = 22.70986795\n",
      "Iteration 3294, loss = 22.71926108\n",
      "Iteration 3295, loss = 23.12415872\n",
      "Iteration 3296, loss = 22.66356123\n",
      "Iteration 3297, loss = 22.59636056\n",
      "Iteration 3298, loss = 22.78463174\n",
      "Iteration 3299, loss = 23.03253318\n",
      "Iteration 3300, loss = 22.67323790\n",
      "Iteration 3301, loss = 22.66365896\n",
      "Iteration 3302, loss = 23.02240615\n",
      "Iteration 3303, loss = 23.13845724\n",
      "Iteration 3304, loss = 22.49413781\n",
      "Iteration 3305, loss = 23.13830134\n",
      "Iteration 3306, loss = 22.79576178\n",
      "Iteration 3307, loss = 23.31394298\n",
      "Iteration 3308, loss = 23.73033890\n",
      "Iteration 3309, loss = 24.15686284\n",
      "Iteration 3310, loss = 23.37376012\n",
      "Iteration 3311, loss = 23.05320894\n",
      "Iteration 3312, loss = 23.76745581\n",
      "Iteration 3313, loss = 23.95613301\n",
      "Iteration 3314, loss = 23.94823606\n",
      "Iteration 3315, loss = 24.01431769\n",
      "Iteration 3316, loss = 23.06043986\n",
      "Iteration 3317, loss = 22.94066806\n",
      "Iteration 3318, loss = 22.96382662\n",
      "Iteration 3319, loss = 23.10043864\n",
      "Iteration 3320, loss = 22.77949336\n",
      "Iteration 3321, loss = 22.42818212\n",
      "Iteration 3322, loss = 22.87480854\n",
      "Iteration 3323, loss = 23.24635954\n",
      "Iteration 3324, loss = 22.76349872\n",
      "Iteration 3325, loss = 22.80231873\n",
      "Iteration 3326, loss = 22.39898358\n",
      "Iteration 3327, loss = 22.80018747\n",
      "Iteration 3328, loss = 23.52429510\n",
      "Iteration 3329, loss = 25.59321199\n",
      "Iteration 3330, loss = 23.29841735\n",
      "Iteration 3331, loss = 22.69002989\n",
      "Iteration 3332, loss = 22.60636662\n",
      "Iteration 3333, loss = 22.37799300\n",
      "Iteration 3334, loss = 22.85417714\n",
      "Iteration 3335, loss = 22.99993321\n",
      "Iteration 3336, loss = 22.55570704\n",
      "Iteration 3337, loss = 23.44645983\n",
      "Iteration 3338, loss = 24.92845530\n",
      "Iteration 3339, loss = 25.80082954\n",
      "Iteration 3340, loss = 24.64664388\n",
      "Iteration 3341, loss = 24.74228009\n",
      "Iteration 3342, loss = 23.66104565\n",
      "Iteration 3343, loss = 22.22033263\n",
      "Iteration 3344, loss = 22.33724601\n",
      "Iteration 3345, loss = 22.19501190\n",
      "Iteration 3346, loss = 22.53983256\n",
      "Iteration 3347, loss = 22.36290709\n",
      "Iteration 3348, loss = 22.05123626\n",
      "Iteration 3349, loss = 22.38779546\n",
      "Iteration 3350, loss = 22.36053996\n",
      "Iteration 3351, loss = 22.31730239\n",
      "Iteration 3352, loss = 22.63530798\n",
      "Iteration 3353, loss = 23.31161307\n",
      "Iteration 3354, loss = 23.60672488\n",
      "Iteration 3355, loss = 23.02566006\n",
      "Iteration 3356, loss = 23.38310578\n",
      "Iteration 3357, loss = 22.82447319\n",
      "Iteration 3358, loss = 23.55952707\n",
      "Iteration 3359, loss = 23.65445001\n",
      "Iteration 3360, loss = 24.84859425\n",
      "Iteration 3361, loss = 26.56650151\n",
      "Iteration 3362, loss = 23.70139888\n",
      "Iteration 3363, loss = 22.59921888\n",
      "Iteration 3364, loss = 22.23074398\n",
      "Iteration 3365, loss = 22.11357707\n",
      "Iteration 3366, loss = 22.26452839\n",
      "Iteration 3367, loss = 22.57876092\n",
      "Iteration 3368, loss = 22.85119054\n",
      "Iteration 3369, loss = 22.47896357\n",
      "Iteration 3370, loss = 22.24948248\n",
      "Iteration 3371, loss = 22.87358935\n",
      "Iteration 3372, loss = 22.41955028\n",
      "Iteration 3373, loss = 22.79323270\n",
      "Iteration 3374, loss = 22.24715212\n",
      "Iteration 3375, loss = 22.28123162\n",
      "Iteration 3376, loss = 22.88944219\n",
      "Iteration 3377, loss = 23.12676120\n",
      "Iteration 3378, loss = 23.33308645\n",
      "Iteration 3379, loss = 22.43070640\n",
      "Iteration 3380, loss = 22.16703712\n",
      "Iteration 3381, loss = 22.07941143\n",
      "Iteration 3382, loss = 22.17394773\n",
      "Iteration 3383, loss = 22.05600135\n",
      "Iteration 3384, loss = 22.82178245\n",
      "Iteration 3385, loss = 22.34909089\n",
      "Iteration 3386, loss = 23.60952464\n",
      "Iteration 3387, loss = 22.95684865\n",
      "Iteration 3388, loss = 22.00733750\n",
      "Iteration 3389, loss = 22.15119763\n",
      "Iteration 3390, loss = 22.10517503\n",
      "Iteration 3391, loss = 23.12313552\n",
      "Iteration 3392, loss = 23.91942443\n",
      "Iteration 3393, loss = 22.84149262\n",
      "Iteration 3394, loss = 22.78573217\n",
      "Iteration 3395, loss = 22.19836244\n",
      "Iteration 3396, loss = 22.29803332\n",
      "Iteration 3397, loss = 21.88906549\n",
      "Iteration 3398, loss = 22.08309160\n",
      "Iteration 3399, loss = 22.32056695\n",
      "Iteration 3400, loss = 22.01471802\n",
      "Iteration 3401, loss = 22.14694762\n",
      "Iteration 3402, loss = 22.27563901\n",
      "Iteration 3403, loss = 22.48219664\n",
      "Iteration 3404, loss = 22.15137034\n",
      "Iteration 3405, loss = 21.88805099\n",
      "Iteration 3406, loss = 22.24285818\n",
      "Iteration 3407, loss = 22.86268088\n",
      "Iteration 3408, loss = 22.32242954\n",
      "Iteration 3409, loss = 22.83270328\n",
      "Iteration 3410, loss = 23.24812073\n",
      "Iteration 3411, loss = 23.20845822\n",
      "Iteration 3412, loss = 22.17208747\n",
      "Iteration 3413, loss = 23.16849758\n",
      "Iteration 3414, loss = 23.67384842\n",
      "Iteration 3415, loss = 23.39939144\n",
      "Iteration 3416, loss = 22.39892904\n",
      "Iteration 3417, loss = 21.60391606\n",
      "Iteration 3418, loss = 22.33487069\n",
      "Iteration 3419, loss = 22.35974059\n",
      "Iteration 3420, loss = 22.07640731\n",
      "Iteration 3421, loss = 22.34732969\n",
      "Iteration 3422, loss = 24.31973793\n",
      "Iteration 3423, loss = 24.43641555\n",
      "Iteration 3424, loss = 24.23126403\n",
      "Iteration 3425, loss = 23.40019632\n",
      "Iteration 3426, loss = 23.17870602\n",
      "Iteration 3427, loss = 22.36045139\n",
      "Iteration 3428, loss = 23.34775372\n",
      "Iteration 3429, loss = 22.53612500\n",
      "Iteration 3430, loss = 21.83270448\n",
      "Iteration 3431, loss = 21.77929767\n",
      "Iteration 3432, loss = 21.96052184\n",
      "Iteration 3433, loss = 21.91222611\n",
      "Iteration 3434, loss = 21.88208876\n",
      "Iteration 3435, loss = 21.64671455\n",
      "Iteration 3436, loss = 21.48764859\n",
      "Iteration 3437, loss = 21.45152866\n",
      "Iteration 3438, loss = 21.63359877\n",
      "Iteration 3439, loss = 21.61061062\n",
      "Iteration 3440, loss = 22.00366887\n",
      "Iteration 3441, loss = 22.02754959\n",
      "Iteration 3442, loss = 21.82577697\n",
      "Iteration 3443, loss = 21.75855687\n",
      "Iteration 3444, loss = 21.60563574\n",
      "Iteration 3445, loss = 22.48003998\n",
      "Iteration 3446, loss = 22.65204274\n",
      "Iteration 3447, loss = 22.45784367\n",
      "Iteration 3448, loss = 22.14696306\n",
      "Iteration 3449, loss = 21.71323051\n",
      "Iteration 3450, loss = 21.83499314\n",
      "Iteration 3451, loss = 21.97342772\n",
      "Iteration 3452, loss = 22.09535630\n",
      "Iteration 3453, loss = 21.84778965\n",
      "Iteration 3454, loss = 21.49323601\n",
      "Iteration 3455, loss = 21.90819594\n",
      "Iteration 3456, loss = 21.20057126\n",
      "Iteration 3457, loss = 22.03109834\n",
      "Iteration 3458, loss = 22.52216544\n",
      "Iteration 3459, loss = 21.52552486\n",
      "Iteration 3460, loss = 21.36967560\n",
      "Iteration 3461, loss = 21.32342295\n",
      "Iteration 3462, loss = 21.68703839\n",
      "Iteration 3463, loss = 22.30475888\n",
      "Iteration 3464, loss = 21.77238137\n",
      "Iteration 3465, loss = 21.36646576\n",
      "Iteration 3466, loss = 21.25148226\n",
      "Iteration 3467, loss = 21.54971317\n",
      "Iteration 3468, loss = 21.59506146\n",
      "Iteration 3469, loss = 21.57938453\n",
      "Iteration 3470, loss = 21.42742931\n",
      "Iteration 3471, loss = 22.95258762\n",
      "Iteration 3472, loss = 21.66598676\n",
      "Iteration 3473, loss = 21.68857701\n",
      "Iteration 3474, loss = 22.01616806\n",
      "Iteration 3475, loss = 21.83971525\n",
      "Iteration 3476, loss = 21.41386800\n",
      "Iteration 3477, loss = 21.86853297\n",
      "Iteration 3478, loss = 21.73054605\n",
      "Iteration 3479, loss = 21.32388388\n",
      "Iteration 3480, loss = 21.96275030\n",
      "Iteration 3481, loss = 21.03736269\n",
      "Iteration 3482, loss = 21.80753947\n",
      "Iteration 3483, loss = 21.52091345\n",
      "Iteration 3484, loss = 21.62149848\n",
      "Iteration 3485, loss = 21.58172287\n",
      "Iteration 3486, loss = 21.46407198\n",
      "Iteration 3487, loss = 21.46293354\n",
      "Iteration 3488, loss = 21.81634079\n",
      "Iteration 3489, loss = 22.04766178\n",
      "Iteration 3490, loss = 22.34900568\n",
      "Iteration 3491, loss = 21.28729790\n",
      "Iteration 3492, loss = 21.22643974\n",
      "Iteration 3493, loss = 21.17618217\n",
      "Iteration 3494, loss = 21.56811674\n",
      "Iteration 3495, loss = 22.12293637\n",
      "Iteration 3496, loss = 21.52519975\n",
      "Iteration 3497, loss = 21.21983394\n",
      "Iteration 3498, loss = 20.90278513\n",
      "Iteration 3499, loss = 21.36472786\n",
      "Iteration 3500, loss = 22.05115949\n",
      "Iteration 3501, loss = 21.89588076\n",
      "Iteration 3502, loss = 21.44440821\n",
      "Iteration 3503, loss = 21.29719952\n",
      "Iteration 3504, loss = 21.23544510\n",
      "Iteration 3505, loss = 21.45138513\n",
      "Iteration 3506, loss = 21.04365810\n",
      "Iteration 3507, loss = 21.28538374\n",
      "Iteration 3508, loss = 21.09086558\n",
      "Iteration 3509, loss = 22.07026859\n",
      "Iteration 3510, loss = 21.87854407\n",
      "Iteration 3511, loss = 22.39708216\n",
      "Iteration 3512, loss = 22.75480291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3513, loss = 22.79173502\n",
      "Iteration 3514, loss = 21.70753098\n",
      "Iteration 3515, loss = 21.28595156\n",
      "Iteration 3516, loss = 21.18494953\n",
      "Iteration 3517, loss = 21.43305589\n",
      "Iteration 3518, loss = 21.01995053\n",
      "Iteration 3519, loss = 21.28049657\n",
      "Iteration 3520, loss = 21.60148054\n",
      "Iteration 3521, loss = 21.57226370\n",
      "Iteration 3522, loss = 21.26823953\n",
      "Iteration 3523, loss = 20.88263985\n",
      "Iteration 3524, loss = 21.11845375\n",
      "Iteration 3525, loss = 20.88638601\n",
      "Iteration 3526, loss = 20.94664478\n",
      "Iteration 3527, loss = 21.16393502\n",
      "Iteration 3528, loss = 22.08118684\n",
      "Iteration 3529, loss = 21.43889335\n",
      "Iteration 3530, loss = 21.38335513\n",
      "Iteration 3531, loss = 21.86532959\n",
      "Iteration 3532, loss = 21.15515973\n",
      "Iteration 3533, loss = 20.61178483\n",
      "Iteration 3534, loss = 20.60560729\n",
      "Iteration 3535, loss = 20.97090140\n",
      "Iteration 3536, loss = 20.80711995\n",
      "Iteration 3537, loss = 21.53200488\n",
      "Iteration 3538, loss = 20.92261141\n",
      "Iteration 3539, loss = 20.92312205\n",
      "Iteration 3540, loss = 20.73137081\n",
      "Iteration 3541, loss = 21.14258611\n",
      "Iteration 3542, loss = 20.84551655\n",
      "Iteration 3543, loss = 21.10397637\n",
      "Iteration 3544, loss = 20.79014131\n",
      "Iteration 3545, loss = 20.70138406\n",
      "Iteration 3546, loss = 22.13175831\n",
      "Iteration 3547, loss = 21.72017578\n",
      "Iteration 3548, loss = 20.81302584\n",
      "Iteration 3549, loss = 21.53022767\n",
      "Iteration 3550, loss = 21.85851889\n",
      "Iteration 3551, loss = 21.65676664\n",
      "Iteration 3552, loss = 20.97222508\n",
      "Iteration 3553, loss = 20.98919819\n",
      "Iteration 3554, loss = 20.73933629\n",
      "Iteration 3555, loss = 20.88395163\n",
      "Iteration 3556, loss = 21.22124602\n",
      "Iteration 3557, loss = 21.30063345\n",
      "Iteration 3558, loss = 21.17494287\n",
      "Iteration 3559, loss = 20.78497241\n",
      "Iteration 3560, loss = 20.53377789\n",
      "Iteration 3561, loss = 21.33975724\n",
      "Iteration 3562, loss = 20.64039148\n",
      "Iteration 3563, loss = 21.41629061\n",
      "Iteration 3564, loss = 22.41143572\n",
      "Iteration 3565, loss = 20.92088801\n",
      "Iteration 3566, loss = 20.84993732\n",
      "Iteration 3567, loss = 20.80728861\n",
      "Iteration 3568, loss = 22.20389132\n",
      "Iteration 3569, loss = 20.61104043\n",
      "Iteration 3570, loss = 21.09471695\n",
      "Iteration 3571, loss = 21.37077582\n",
      "Iteration 3572, loss = 21.47946375\n",
      "Iteration 3573, loss = 22.26170824\n",
      "Iteration 3574, loss = 22.94506703\n",
      "Iteration 3575, loss = 25.47027083\n",
      "Iteration 3576, loss = 23.25766503\n",
      "Iteration 3577, loss = 22.72509235\n",
      "Iteration 3578, loss = 23.22660088\n",
      "Iteration 3579, loss = 23.25186655\n",
      "Iteration 3580, loss = 21.11401556\n",
      "Iteration 3581, loss = 20.54939976\n",
      "Iteration 3582, loss = 20.28520585\n",
      "Iteration 3583, loss = 20.63029062\n",
      "Iteration 3584, loss = 21.16708809\n",
      "Iteration 3585, loss = 20.74534377\n",
      "Iteration 3586, loss = 21.39845034\n",
      "Iteration 3587, loss = 21.06365682\n",
      "Iteration 3588, loss = 20.76821321\n",
      "Iteration 3589, loss = 21.10152308\n",
      "Iteration 3590, loss = 20.93512487\n",
      "Iteration 3591, loss = 20.48416339\n",
      "Iteration 3592, loss = 20.40098046\n",
      "Iteration 3593, loss = 20.26636690\n",
      "Iteration 3594, loss = 20.41767879\n",
      "Iteration 3595, loss = 20.51449078\n",
      "Iteration 3596, loss = 20.32126236\n",
      "Iteration 3597, loss = 20.51283846\n",
      "Iteration 3598, loss = 21.62556963\n",
      "Iteration 3599, loss = 21.13765815\n",
      "Iteration 3600, loss = 21.95507996\n",
      "Iteration 3601, loss = 20.84174122\n",
      "Iteration 3602, loss = 20.94389391\n",
      "Iteration 3603, loss = 20.95166166\n",
      "Iteration 3604, loss = 21.23616817\n",
      "Iteration 3605, loss = 20.97034578\n",
      "Iteration 3606, loss = 20.47379253\n",
      "Iteration 3607, loss = 20.05371101\n",
      "Iteration 3608, loss = 20.30205737\n",
      "Iteration 3609, loss = 20.08662373\n",
      "Iteration 3610, loss = 21.02728533\n",
      "Iteration 3611, loss = 22.97469621\n",
      "Iteration 3612, loss = 22.79139141\n",
      "Iteration 3613, loss = 22.52973430\n",
      "Iteration 3614, loss = 21.97884921\n",
      "Iteration 3615, loss = 22.67143397\n",
      "Iteration 3616, loss = 22.47213982\n",
      "Iteration 3617, loss = 22.22742074\n",
      "Iteration 3618, loss = 20.94160930\n",
      "Iteration 3619, loss = 21.80580575\n",
      "Iteration 3620, loss = 21.54732881\n",
      "Iteration 3621, loss = 22.01177253\n",
      "Iteration 3622, loss = 20.81658565\n",
      "Iteration 3623, loss = 19.83270137\n",
      "Iteration 3624, loss = 20.21037478\n",
      "Iteration 3625, loss = 20.95280455\n",
      "Iteration 3626, loss = 21.72373362\n",
      "Iteration 3627, loss = 20.27762656\n",
      "Iteration 3628, loss = 20.23585321\n",
      "Iteration 3629, loss = 20.06856807\n",
      "Iteration 3630, loss = 21.08584362\n",
      "Iteration 3631, loss = 21.23024714\n",
      "Iteration 3632, loss = 20.35832167\n",
      "Iteration 3633, loss = 19.90635129\n",
      "Iteration 3634, loss = 20.12923770\n",
      "Iteration 3635, loss = 21.19322012\n",
      "Iteration 3636, loss = 20.70919917\n",
      "Iteration 3637, loss = 20.46010919\n",
      "Iteration 3638, loss = 20.69312816\n",
      "Iteration 3639, loss = 20.78137496\n",
      "Iteration 3640, loss = 20.56201443\n",
      "Iteration 3641, loss = 20.55493070\n",
      "Iteration 3642, loss = 19.90978836\n",
      "Iteration 3643, loss = 20.23413039\n",
      "Iteration 3644, loss = 20.22134259\n",
      "Iteration 3645, loss = 20.83046487\n",
      "Iteration 3646, loss = 20.36117531\n",
      "Iteration 3647, loss = 19.86458255\n",
      "Iteration 3648, loss = 20.06964286\n",
      "Iteration 3649, loss = 20.14118974\n",
      "Iteration 3650, loss = 20.23632754\n",
      "Iteration 3651, loss = 21.35320264\n",
      "Iteration 3652, loss = 21.28463102\n",
      "Iteration 3653, loss = 24.55502979\n",
      "Iteration 3654, loss = 25.12228074\n",
      "Iteration 3655, loss = 21.98787217\n",
      "Iteration 3656, loss = 22.03394203\n",
      "Iteration 3657, loss = 20.60784544\n",
      "Iteration 3658, loss = 20.45397665\n",
      "Iteration 3659, loss = 21.89361265\n",
      "Iteration 3660, loss = 20.73205385\n",
      "Iteration 3661, loss = 20.90709272\n",
      "Iteration 3662, loss = 20.59339085\n",
      "Iteration 3663, loss = 20.15363318\n",
      "Iteration 3664, loss = 19.94953113\n",
      "Iteration 3665, loss = 20.15319575\n",
      "Iteration 3666, loss = 20.41902506\n",
      "Iteration 3667, loss = 20.33681439\n",
      "Iteration 3668, loss = 19.97436889\n",
      "Iteration 3669, loss = 20.38905854\n",
      "Iteration 3670, loss = 19.71872610\n",
      "Iteration 3671, loss = 20.42147362\n",
      "Iteration 3672, loss = 20.90332568\n",
      "Iteration 3673, loss = 21.75444800\n",
      "Iteration 3674, loss = 22.46018347\n",
      "Iteration 3675, loss = 21.74868132\n",
      "Iteration 3676, loss = 23.24880343\n",
      "Iteration 3677, loss = 22.30273982\n",
      "Iteration 3678, loss = 21.57497987\n",
      "Iteration 3679, loss = 21.35420459\n",
      "Iteration 3680, loss = 20.47268654\n",
      "Iteration 3681, loss = 20.09211458\n",
      "Iteration 3682, loss = 20.17978888\n",
      "Iteration 3683, loss = 19.84988101\n",
      "Iteration 3684, loss = 19.75582693\n",
      "Iteration 3685, loss = 19.79877374\n",
      "Iteration 3686, loss = 19.76770543\n",
      "Iteration 3687, loss = 19.98161090\n",
      "Iteration 3688, loss = 19.57498096\n",
      "Iteration 3689, loss = 19.98024744\n",
      "Iteration 3690, loss = 20.21119716\n",
      "Iteration 3691, loss = 20.70733499\n",
      "Iteration 3692, loss = 19.98163083\n",
      "Iteration 3693, loss = 19.80297807\n",
      "Iteration 3694, loss = 20.01494718\n",
      "Iteration 3695, loss = 20.05352051\n",
      "Iteration 3696, loss = 20.37359228\n",
      "Iteration 3697, loss = 19.86721584\n",
      "Iteration 3698, loss = 20.13682045\n",
      "Iteration 3699, loss = 20.30734354\n",
      "Iteration 3700, loss = 19.74723783\n",
      "Iteration 3701, loss = 19.79138960\n",
      "Iteration 3702, loss = 19.47055503\n",
      "Iteration 3703, loss = 19.69796291\n",
      "Iteration 3704, loss = 19.88999801\n",
      "Iteration 3705, loss = 19.66636287\n",
      "Iteration 3706, loss = 19.40200073\n",
      "Iteration 3707, loss = 19.44163769\n",
      "Iteration 3708, loss = 19.77566394\n",
      "Iteration 3709, loss = 19.51178546\n",
      "Iteration 3710, loss = 19.90215714\n",
      "Iteration 3711, loss = 20.39023085\n",
      "Iteration 3712, loss = 19.80766738\n",
      "Iteration 3713, loss = 19.35788913\n",
      "Iteration 3714, loss = 19.51023003\n",
      "Iteration 3715, loss = 19.78327348\n",
      "Iteration 3716, loss = 21.11385306\n",
      "Iteration 3717, loss = 20.27114023\n",
      "Iteration 3718, loss = 20.96216177\n",
      "Iteration 3719, loss = 19.90609418\n",
      "Iteration 3720, loss = 19.77095233\n",
      "Iteration 3721, loss = 20.08691578\n",
      "Iteration 3722, loss = 20.52268209\n",
      "Iteration 3723, loss = 21.18092380\n",
      "Iteration 3724, loss = 21.88893947\n",
      "Iteration 3725, loss = 21.08139593\n",
      "Iteration 3726, loss = 19.59486643\n",
      "Iteration 3727, loss = 19.75435981\n",
      "Iteration 3728, loss = 19.53664105\n",
      "Iteration 3729, loss = 20.37048545\n",
      "Iteration 3730, loss = 21.40871608\n",
      "Iteration 3731, loss = 22.00656436\n",
      "Iteration 3732, loss = 21.17122673\n",
      "Iteration 3733, loss = 21.01615684\n",
      "Iteration 3734, loss = 20.80565695\n",
      "Iteration 3735, loss = 19.72137970\n",
      "Iteration 3736, loss = 19.48412415\n",
      "Iteration 3737, loss = 19.37763127\n",
      "Iteration 3738, loss = 20.32029014\n",
      "Iteration 3739, loss = 19.44934698\n",
      "Iteration 3740, loss = 19.78664392\n",
      "Iteration 3741, loss = 19.59618373\n",
      "Iteration 3742, loss = 19.92535810\n",
      "Iteration 3743, loss = 20.56909076\n",
      "Iteration 3744, loss = 20.57226575\n",
      "Iteration 3745, loss = 19.42433758\n",
      "Iteration 3746, loss = 20.02726509\n",
      "Iteration 3747, loss = 20.56245145\n",
      "Iteration 3748, loss = 21.46820100\n",
      "Iteration 3749, loss = 22.21589680\n",
      "Iteration 3750, loss = 20.34945828\n",
      "Iteration 3751, loss = 19.49515501\n",
      "Iteration 3752, loss = 19.56698037\n",
      "Iteration 3753, loss = 19.99656351\n",
      "Iteration 3754, loss = 19.81323140\n",
      "Iteration 3755, loss = 20.86613851\n",
      "Iteration 3756, loss = 20.86235772\n",
      "Iteration 3757, loss = 21.10034285\n",
      "Iteration 3758, loss = 21.42465964\n",
      "Iteration 3759, loss = 21.41718794\n",
      "Iteration 3760, loss = 20.82770726\n",
      "Iteration 3761, loss = 21.82563184\n",
      "Iteration 3762, loss = 22.37000743\n",
      "Iteration 3763, loss = 21.99411809\n",
      "Iteration 3764, loss = 22.99472073\n",
      "Iteration 3765, loss = 24.81890199\n",
      "Iteration 3766, loss = 23.06274584\n",
      "Iteration 3767, loss = 23.84935159\n",
      "Iteration 3768, loss = 21.30206144\n",
      "Iteration 3769, loss = 19.80087273\n",
      "Iteration 3770, loss = 19.13887407\n",
      "Iteration 3771, loss = 19.09729243\n",
      "Iteration 3772, loss = 19.32400166\n",
      "Iteration 3773, loss = 19.09709417\n",
      "Iteration 3774, loss = 19.78462085\n",
      "Iteration 3775, loss = 19.79448738\n",
      "Iteration 3776, loss = 20.09567958\n",
      "Iteration 3777, loss = 19.97899602\n",
      "Iteration 3778, loss = 20.17872086\n",
      "Iteration 3779, loss = 20.47929977\n",
      "Iteration 3780, loss = 20.64882703\n",
      "Iteration 3781, loss = 20.98027095\n",
      "Iteration 3782, loss = 19.69207309\n",
      "Iteration 3783, loss = 19.67696072\n",
      "Iteration 3784, loss = 19.19904629\n",
      "Iteration 3785, loss = 19.67784860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3786, loss = 19.20004574\n",
      "Iteration 3787, loss = 18.94493374\n",
      "Iteration 3788, loss = 18.88304220\n",
      "Iteration 3789, loss = 18.87497714\n",
      "Iteration 3790, loss = 18.96360539\n",
      "Iteration 3791, loss = 19.00208376\n",
      "Iteration 3792, loss = 19.70946076\n",
      "Iteration 3793, loss = 19.40896600\n",
      "Iteration 3794, loss = 19.02272800\n",
      "Iteration 3795, loss = 19.23371520\n",
      "Iteration 3796, loss = 19.56834707\n",
      "Iteration 3797, loss = 20.71283558\n",
      "Iteration 3798, loss = 21.53421840\n",
      "Iteration 3799, loss = 22.02093293\n",
      "Iteration 3800, loss = 23.52050301\n",
      "Iteration 3801, loss = 21.86808182\n",
      "Iteration 3802, loss = 19.22245540\n",
      "Iteration 3803, loss = 19.78588761\n",
      "Iteration 3804, loss = 20.07874167\n",
      "Iteration 3805, loss = 20.29194925\n",
      "Iteration 3806, loss = 20.11827185\n",
      "Iteration 3807, loss = 19.64381236\n",
      "Iteration 3808, loss = 18.76535729\n",
      "Iteration 3809, loss = 18.83096871\n",
      "Iteration 3810, loss = 18.90112561\n",
      "Iteration 3811, loss = 19.03505200\n",
      "Iteration 3812, loss = 19.14068862\n",
      "Iteration 3813, loss = 19.11417055\n",
      "Iteration 3814, loss = 18.96622214\n",
      "Iteration 3815, loss = 19.29367932\n",
      "Iteration 3816, loss = 19.02831084\n",
      "Iteration 3817, loss = 18.95826718\n",
      "Iteration 3818, loss = 19.46510584\n",
      "Iteration 3819, loss = 20.12699031\n",
      "Iteration 3820, loss = 19.28456340\n",
      "Iteration 3821, loss = 18.74151376\n",
      "Iteration 3822, loss = 18.92306671\n",
      "Iteration 3823, loss = 18.64797315\n",
      "Iteration 3824, loss = 19.23368578\n",
      "Iteration 3825, loss = 19.10843399\n",
      "Iteration 3826, loss = 18.81619733\n",
      "Iteration 3827, loss = 19.12382192\n",
      "Iteration 3828, loss = 19.31225561\n",
      "Iteration 3829, loss = 19.02832607\n",
      "Iteration 3830, loss = 19.41751838\n",
      "Iteration 3831, loss = 19.46939668\n",
      "Iteration 3832, loss = 18.97200533\n",
      "Iteration 3833, loss = 18.77252118\n",
      "Iteration 3834, loss = 18.65088958\n",
      "Iteration 3835, loss = 19.25070515\n",
      "Iteration 3836, loss = 19.22030477\n",
      "Iteration 3837, loss = 19.15170132\n",
      "Iteration 3838, loss = 20.17001689\n",
      "Iteration 3839, loss = 20.16473428\n",
      "Iteration 3840, loss = 19.75763476\n",
      "Iteration 3841, loss = 19.98563942\n",
      "Iteration 3842, loss = 19.43058696\n",
      "Iteration 3843, loss = 20.23110547\n",
      "Iteration 3844, loss = 19.77609202\n",
      "Iteration 3845, loss = 20.55780798\n",
      "Iteration 3846, loss = 20.17978504\n",
      "Iteration 3847, loss = 18.77361002\n",
      "Iteration 3848, loss = 19.48121124\n",
      "Iteration 3849, loss = 19.54285473\n",
      "Iteration 3850, loss = 19.86726286\n",
      "Iteration 3851, loss = 19.33032097\n",
      "Iteration 3852, loss = 19.20723189\n",
      "Iteration 3853, loss = 18.57661808\n",
      "Iteration 3854, loss = 19.53402518\n",
      "Iteration 3855, loss = 20.07616917\n",
      "Iteration 3856, loss = 20.86616172\n",
      "Iteration 3857, loss = 22.03973872\n",
      "Iteration 3858, loss = 21.12507813\n",
      "Iteration 3859, loss = 19.56389314\n",
      "Iteration 3860, loss = 19.37525460\n",
      "Iteration 3861, loss = 20.48576842\n",
      "Iteration 3862, loss = 19.96548135\n",
      "Iteration 3863, loss = 20.28618315\n",
      "Iteration 3864, loss = 21.26887414\n",
      "Iteration 3865, loss = 19.82341829\n",
      "Iteration 3866, loss = 19.59371168\n",
      "Iteration 3867, loss = 20.22284011\n",
      "Iteration 3868, loss = 20.09769072\n",
      "Iteration 3869, loss = 18.63996288\n",
      "Iteration 3870, loss = 18.61269512\n",
      "Iteration 3871, loss = 18.69092063\n",
      "Iteration 3872, loss = 18.80280692\n",
      "Iteration 3873, loss = 18.39037131\n",
      "Iteration 3874, loss = 18.39701526\n",
      "Iteration 3875, loss = 18.90889836\n",
      "Iteration 3876, loss = 19.51877085\n",
      "Iteration 3877, loss = 18.85168420\n",
      "Iteration 3878, loss = 18.72212572\n",
      "Iteration 3879, loss = 18.57043635\n",
      "Iteration 3880, loss = 18.35935678\n",
      "Iteration 3881, loss = 18.32397547\n",
      "Iteration 3882, loss = 18.84572281\n",
      "Iteration 3883, loss = 19.28761320\n",
      "Iteration 3884, loss = 18.81868974\n",
      "Iteration 3885, loss = 19.01486981\n",
      "Iteration 3886, loss = 18.85570616\n",
      "Iteration 3887, loss = 19.98648928\n",
      "Iteration 3888, loss = 20.32033149\n",
      "Iteration 3889, loss = 18.68977536\n",
      "Iteration 3890, loss = 18.34386734\n",
      "Iteration 3891, loss = 18.44058809\n",
      "Iteration 3892, loss = 19.21843297\n",
      "Iteration 3893, loss = 18.95195232\n",
      "Iteration 3894, loss = 18.42144678\n",
      "Iteration 3895, loss = 18.45766173\n",
      "Iteration 3896, loss = 18.91124942\n",
      "Iteration 3897, loss = 19.32972525\n",
      "Iteration 3898, loss = 19.43108055\n",
      "Iteration 3899, loss = 18.63920410\n",
      "Iteration 3900, loss = 19.35013703\n",
      "Iteration 3901, loss = 19.47717948\n",
      "Iteration 3902, loss = 19.50936491\n",
      "Iteration 3903, loss = 18.59127992\n",
      "Iteration 3904, loss = 18.53817436\n",
      "Iteration 3905, loss = 18.20662797\n",
      "Iteration 3906, loss = 18.68384749\n",
      "Iteration 3907, loss = 18.54366350\n",
      "Iteration 3908, loss = 18.89634087\n",
      "Iteration 3909, loss = 18.56032981\n",
      "Iteration 3910, loss = 18.32056371\n",
      "Iteration 3911, loss = 18.98974753\n",
      "Iteration 3912, loss = 18.68418330\n",
      "Iteration 3913, loss = 18.97902590\n",
      "Iteration 3914, loss = 19.91551611\n",
      "Iteration 3915, loss = 19.98714003\n",
      "Iteration 3916, loss = 18.89982990\n",
      "Iteration 3917, loss = 18.67267155\n",
      "Iteration 3918, loss = 18.63842285\n",
      "Iteration 3919, loss = 18.01619527\n",
      "Iteration 3920, loss = 18.25604964\n",
      "Iteration 3921, loss = 18.81805752\n",
      "Iteration 3922, loss = 18.77303838\n",
      "Iteration 3923, loss = 18.08807613\n",
      "Iteration 3924, loss = 18.41567849\n",
      "Iteration 3925, loss = 18.16043777\n",
      "Iteration 3926, loss = 18.36510783\n",
      "Iteration 3927, loss = 19.31824456\n",
      "Iteration 3928, loss = 21.29468532\n",
      "Iteration 3929, loss = 19.74737771\n",
      "Iteration 3930, loss = 19.90455943\n",
      "Iteration 3931, loss = 18.92069642\n",
      "Iteration 3932, loss = 20.56081454\n",
      "Iteration 3933, loss = 19.66991156\n",
      "Iteration 3934, loss = 18.95998336\n",
      "Iteration 3935, loss = 20.06898994\n",
      "Iteration 3936, loss = 19.47253024\n",
      "Iteration 3937, loss = 18.92129852\n",
      "Iteration 3938, loss = 20.53950655\n",
      "Iteration 3939, loss = 20.38806308\n",
      "Iteration 3940, loss = 21.32916728\n",
      "Iteration 3941, loss = 22.02606882\n",
      "Iteration 3942, loss = 21.00618873\n",
      "Iteration 3943, loss = 20.02036090\n",
      "Iteration 3944, loss = 20.82505311\n",
      "Iteration 3945, loss = 19.68778729\n",
      "Iteration 3946, loss = 20.20234659\n",
      "Iteration 3947, loss = 20.73477890\n",
      "Iteration 3948, loss = 19.82479490\n",
      "Iteration 3949, loss = 19.63509275\n",
      "Iteration 3950, loss = 19.97206150\n",
      "Iteration 3951, loss = 19.71363816\n",
      "Iteration 3952, loss = 19.59387441\n",
      "Iteration 3953, loss = 20.39185981\n",
      "Iteration 3954, loss = 19.70568649\n",
      "Iteration 3955, loss = 19.05460026\n",
      "Iteration 3956, loss = 18.44508013\n",
      "Iteration 3957, loss = 18.78715591\n",
      "Iteration 3958, loss = 17.97259531\n",
      "Iteration 3959, loss = 18.53317575\n",
      "Iteration 3960, loss = 19.30399914\n",
      "Iteration 3961, loss = 19.12748773\n",
      "Iteration 3962, loss = 18.46201880\n",
      "Iteration 3963, loss = 20.22368501\n",
      "Iteration 3964, loss = 18.89982982\n",
      "Iteration 3965, loss = 18.87540934\n",
      "Iteration 3966, loss = 18.40463613\n",
      "Iteration 3967, loss = 18.64672776\n",
      "Iteration 3968, loss = 18.10283491\n",
      "Iteration 3969, loss = 18.85666397\n",
      "Iteration 3970, loss = 18.44350870\n",
      "Iteration 3971, loss = 18.84404250\n",
      "Iteration 3972, loss = 17.99278740\n",
      "Iteration 3973, loss = 18.22499814\n",
      "Iteration 3974, loss = 18.11739856\n",
      "Iteration 3975, loss = 18.31726420\n",
      "Iteration 3976, loss = 18.64391877\n",
      "Iteration 3977, loss = 18.41465185\n",
      "Iteration 3978, loss = 18.80269694\n",
      "Iteration 3979, loss = 19.63210985\n",
      "Iteration 3980, loss = 18.66913429\n",
      "Iteration 3981, loss = 19.32220391\n",
      "Iteration 3982, loss = 19.90751495\n",
      "Iteration 3983, loss = 18.88184889\n",
      "Iteration 3984, loss = 18.87594525\n",
      "Iteration 3985, loss = 18.48625510\n",
      "Iteration 3986, loss = 20.13602876\n",
      "Iteration 3987, loss = 19.27719860\n",
      "Iteration 3988, loss = 18.56767126\n",
      "Iteration 3989, loss = 18.06435252\n",
      "Iteration 3990, loss = 18.17552227\n",
      "Iteration 3991, loss = 18.25010117\n",
      "Iteration 3992, loss = 19.22102390\n",
      "Iteration 3993, loss = 20.04291067\n",
      "Iteration 3994, loss = 18.76896987\n",
      "Iteration 3995, loss = 19.13733428\n",
      "Iteration 3996, loss = 19.56038919\n",
      "Iteration 3997, loss = 18.70918496\n",
      "Iteration 3998, loss = 18.22924863\n",
      "Iteration 3999, loss = 18.08854328\n",
      "Iteration 4000, loss = 18.00709255\n",
      "Iteration 4001, loss = 17.95739611\n",
      "Iteration 4002, loss = 17.85564940\n",
      "Iteration 4003, loss = 18.10464692\n",
      "Iteration 4004, loss = 17.74373141\n",
      "Iteration 4005, loss = 17.80789992\n",
      "Iteration 4006, loss = 17.75406378\n",
      "Iteration 4007, loss = 18.18597606\n",
      "Iteration 4008, loss = 17.69282463\n",
      "Iteration 4009, loss = 18.16749979\n",
      "Iteration 4010, loss = 18.43147354\n",
      "Iteration 4011, loss = 19.13493574\n",
      "Iteration 4012, loss = 18.45287217\n",
      "Iteration 4013, loss = 17.89415467\n",
      "Iteration 4014, loss = 17.65510528\n",
      "Iteration 4015, loss = 18.05724493\n",
      "Iteration 4016, loss = 17.82520861\n",
      "Iteration 4017, loss = 17.81309729\n",
      "Iteration 4018, loss = 18.39109747\n",
      "Iteration 4019, loss = 18.26629145\n",
      "Iteration 4020, loss = 19.38316129\n",
      "Iteration 4021, loss = 19.92646044\n",
      "Iteration 4022, loss = 18.32658192\n",
      "Iteration 4023, loss = 17.86983557\n",
      "Iteration 4024, loss = 18.07808256\n",
      "Iteration 4025, loss = 17.79327345\n",
      "Iteration 4026, loss = 17.61582129\n",
      "Iteration 4027, loss = 17.85149863\n",
      "Iteration 4028, loss = 17.94478411\n",
      "Iteration 4029, loss = 19.39323405\n",
      "Iteration 4030, loss = 19.10303722\n",
      "Iteration 4031, loss = 18.07400124\n",
      "Iteration 4032, loss = 18.47710453\n",
      "Iteration 4033, loss = 18.26736596\n",
      "Iteration 4034, loss = 17.81767899\n",
      "Iteration 4035, loss = 18.11601322\n",
      "Iteration 4036, loss = 17.82869734\n",
      "Iteration 4037, loss = 18.57208385\n",
      "Iteration 4038, loss = 17.75087610\n",
      "Iteration 4039, loss = 17.55430107\n",
      "Iteration 4040, loss = 18.38019235\n",
      "Iteration 4041, loss = 17.83647354\n",
      "Iteration 4042, loss = 18.14415249\n",
      "Iteration 4043, loss = 18.10313673\n",
      "Iteration 4044, loss = 18.24431467\n",
      "Iteration 4045, loss = 17.86806437\n",
      "Iteration 4046, loss = 18.09355923\n",
      "Iteration 4047, loss = 18.14524997\n",
      "Iteration 4048, loss = 18.57200331\n",
      "Iteration 4049, loss = 18.91750055\n",
      "Iteration 4050, loss = 19.75303732\n",
      "Iteration 4051, loss = 19.36511008\n",
      "Iteration 4052, loss = 20.67530091\n",
      "Iteration 4053, loss = 18.76513392\n",
      "Iteration 4054, loss = 17.73617195\n",
      "Iteration 4055, loss = 17.78169237\n",
      "Iteration 4056, loss = 17.90313860\n",
      "Iteration 4057, loss = 18.37159981\n",
      "Iteration 4058, loss = 18.21629647\n",
      "Iteration 4059, loss = 17.60382871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4060, loss = 17.51458866\n",
      "Iteration 4061, loss = 17.86597505\n",
      "Iteration 4062, loss = 18.04875780\n",
      "Iteration 4063, loss = 18.71313910\n",
      "Iteration 4064, loss = 18.68009554\n",
      "Iteration 4065, loss = 17.79683149\n",
      "Iteration 4066, loss = 17.71925984\n",
      "Iteration 4067, loss = 18.08534752\n",
      "Iteration 4068, loss = 17.72717701\n",
      "Iteration 4069, loss = 17.70514445\n",
      "Iteration 4070, loss = 18.48356265\n",
      "Iteration 4071, loss = 19.77104368\n",
      "Iteration 4072, loss = 17.87351474\n",
      "Iteration 4073, loss = 17.71049894\n",
      "Iteration 4074, loss = 18.08549809\n",
      "Iteration 4075, loss = 17.56323427\n",
      "Iteration 4076, loss = 17.74462298\n",
      "Iteration 4077, loss = 18.16183791\n",
      "Iteration 4078, loss = 18.69228830\n",
      "Iteration 4079, loss = 18.06169507\n",
      "Iteration 4080, loss = 17.78677989\n",
      "Iteration 4081, loss = 18.79706345\n",
      "Iteration 4082, loss = 19.31314677\n",
      "Iteration 4083, loss = 21.77489039\n",
      "Iteration 4084, loss = 21.21439768\n",
      "Iteration 4085, loss = 20.44591363\n",
      "Iteration 4086, loss = 18.83654876\n",
      "Iteration 4087, loss = 18.23534304\n",
      "Iteration 4088, loss = 18.49883551\n",
      "Iteration 4089, loss = 18.32924811\n",
      "Iteration 4090, loss = 18.90821222\n",
      "Iteration 4091, loss = 18.78428197\n",
      "Iteration 4092, loss = 19.70517603\n",
      "Iteration 4093, loss = 20.61524905\n",
      "Iteration 4094, loss = 20.63355249\n",
      "Iteration 4095, loss = 18.97504878\n",
      "Iteration 4096, loss = 17.93555841\n",
      "Iteration 4097, loss = 18.66680875\n",
      "Iteration 4098, loss = 19.02794363\n",
      "Iteration 4099, loss = 18.62223169\n",
      "Iteration 4100, loss = 19.37977612\n",
      "Iteration 4101, loss = 18.39652662\n",
      "Iteration 4102, loss = 18.49784368\n",
      "Iteration 4103, loss = 17.54465424\n",
      "Iteration 4104, loss = 17.60076278\n",
      "Iteration 4105, loss = 17.51592638\n",
      "Iteration 4106, loss = 17.29121334\n",
      "Iteration 4107, loss = 17.50167414\n",
      "Iteration 4108, loss = 17.83656710\n",
      "Iteration 4109, loss = 17.84160175\n",
      "Iteration 4110, loss = 17.83096256\n",
      "Iteration 4111, loss = 19.76147089\n",
      "Iteration 4112, loss = 18.06384791\n",
      "Iteration 4113, loss = 17.77255906\n",
      "Iteration 4114, loss = 17.83784864\n",
      "Iteration 4115, loss = 18.34047406\n",
      "Iteration 4116, loss = 18.12269682\n",
      "Iteration 4117, loss = 17.96512611\n",
      "Iteration 4118, loss = 17.24994659\n",
      "Iteration 4119, loss = 17.69668652\n",
      "Iteration 4120, loss = 17.51017207\n",
      "Iteration 4121, loss = 17.58071593\n",
      "Iteration 4122, loss = 17.86508245\n",
      "Iteration 4123, loss = 17.69248334\n",
      "Iteration 4124, loss = 17.41596895\n",
      "Iteration 4125, loss = 18.27360065\n",
      "Iteration 4126, loss = 17.48819802\n",
      "Iteration 4127, loss = 17.85110561\n",
      "Iteration 4128, loss = 18.05014120\n",
      "Iteration 4129, loss = 18.28033312\n",
      "Iteration 4130, loss = 19.11586801\n",
      "Iteration 4131, loss = 19.08759877\n",
      "Iteration 4132, loss = 17.83275217\n",
      "Iteration 4133, loss = 17.76332445\n",
      "Iteration 4134, loss = 18.09624361\n",
      "Iteration 4135, loss = 18.21270727\n",
      "Iteration 4136, loss = 20.17406468\n",
      "Iteration 4137, loss = 17.80587032\n",
      "Iteration 4138, loss = 17.15891678\n",
      "Iteration 4139, loss = 17.64802816\n",
      "Iteration 4140, loss = 17.22305373\n",
      "Iteration 4141, loss = 18.17530789\n",
      "Iteration 4142, loss = 17.66376843\n",
      "Iteration 4143, loss = 17.76947145\n",
      "Iteration 4144, loss = 17.24711678\n",
      "Iteration 4145, loss = 17.13238196\n",
      "Iteration 4146, loss = 17.91463529\n",
      "Iteration 4147, loss = 18.33503235\n",
      "Iteration 4148, loss = 18.41882161\n",
      "Iteration 4149, loss = 18.17560382\n",
      "Iteration 4150, loss = 17.33433710\n",
      "Iteration 4151, loss = 17.06638450\n",
      "Iteration 4152, loss = 17.24621721\n",
      "Iteration 4153, loss = 17.12368164\n",
      "Iteration 4154, loss = 17.43761122\n",
      "Iteration 4155, loss = 18.31991857\n",
      "Iteration 4156, loss = 17.37994098\n",
      "Iteration 4157, loss = 17.84802945\n",
      "Iteration 4158, loss = 17.13783474\n",
      "Iteration 4159, loss = 17.42282726\n",
      "Iteration 4160, loss = 17.83784102\n",
      "Iteration 4161, loss = 19.33637595\n",
      "Iteration 4162, loss = 20.88170847\n",
      "Iteration 4163, loss = 19.49472866\n",
      "Iteration 4164, loss = 19.58451386\n",
      "Iteration 4165, loss = 18.16596750\n",
      "Iteration 4166, loss = 17.71584924\n",
      "Iteration 4167, loss = 16.85911642\n",
      "Iteration 4168, loss = 17.47149195\n",
      "Iteration 4169, loss = 17.67592186\n",
      "Iteration 4170, loss = 18.11302186\n",
      "Iteration 4171, loss = 17.76379566\n",
      "Iteration 4172, loss = 17.74192948\n",
      "Iteration 4173, loss = 17.24241256\n",
      "Iteration 4174, loss = 18.11395709\n",
      "Iteration 4175, loss = 17.40977166\n",
      "Iteration 4176, loss = 18.07947651\n",
      "Iteration 4177, loss = 17.19670766\n",
      "Iteration 4178, loss = 17.34506694\n",
      "Iteration 4179, loss = 17.28135909\n",
      "Iteration 4180, loss = 17.45489635\n",
      "Iteration 4181, loss = 17.68996559\n",
      "Iteration 4182, loss = 18.43668087\n",
      "Iteration 4183, loss = 17.06711223\n",
      "Iteration 4184, loss = 16.97909321\n",
      "Iteration 4185, loss = 17.16263541\n",
      "Iteration 4186, loss = 17.59450071\n",
      "Iteration 4187, loss = 18.80701467\n",
      "Iteration 4188, loss = 18.72950040\n",
      "Iteration 4189, loss = 18.75554854\n",
      "Iteration 4190, loss = 19.17402650\n",
      "Iteration 4191, loss = 18.75882334\n",
      "Iteration 4192, loss = 17.57799628\n",
      "Iteration 4193, loss = 17.22887968\n",
      "Iteration 4194, loss = 18.84001287\n",
      "Iteration 4195, loss = 19.72885652\n",
      "Iteration 4196, loss = 18.10821154\n",
      "Iteration 4197, loss = 17.73903627\n",
      "Iteration 4198, loss = 16.95404865\n",
      "Iteration 4199, loss = 17.36244054\n",
      "Iteration 4200, loss = 17.58864592\n",
      "Iteration 4201, loss = 18.46932432\n",
      "Iteration 4202, loss = 17.56648084\n",
      "Iteration 4203, loss = 17.22165055\n",
      "Iteration 4204, loss = 17.06628963\n",
      "Iteration 4205, loss = 16.80035860\n",
      "Iteration 4206, loss = 16.71082986\n",
      "Iteration 4207, loss = 17.02647276\n",
      "Iteration 4208, loss = 18.24807882\n",
      "Iteration 4209, loss = 19.42291642\n",
      "Iteration 4210, loss = 18.84547915\n",
      "Iteration 4211, loss = 16.99043863\n",
      "Iteration 4212, loss = 16.81838775\n",
      "Iteration 4213, loss = 16.62405221\n",
      "Iteration 4214, loss = 17.13474322\n",
      "Iteration 4215, loss = 17.09441723\n",
      "Iteration 4216, loss = 16.89939109\n",
      "Iteration 4217, loss = 17.80521674\n",
      "Iteration 4218, loss = 16.97403818\n",
      "Iteration 4219, loss = 17.00416185\n",
      "Iteration 4220, loss = 16.84578443\n",
      "Iteration 4221, loss = 17.03524300\n",
      "Iteration 4222, loss = 16.71960545\n",
      "Iteration 4223, loss = 16.94215629\n",
      "Iteration 4224, loss = 17.34345696\n",
      "Iteration 4225, loss = 17.17129446\n",
      "Iteration 4226, loss = 18.41423318\n",
      "Iteration 4227, loss = 19.31908469\n",
      "Iteration 4228, loss = 18.20075988\n",
      "Iteration 4229, loss = 17.73780382\n",
      "Iteration 4230, loss = 17.31670335\n",
      "Iteration 4231, loss = 17.86165011\n",
      "Iteration 4232, loss = 17.98975391\n",
      "Iteration 4233, loss = 17.06959643\n",
      "Iteration 4234, loss = 17.45880025\n",
      "Iteration 4235, loss = 17.82800055\n",
      "Iteration 4236, loss = 17.76631793\n",
      "Iteration 4237, loss = 17.19092572\n",
      "Iteration 4238, loss = 16.78183082\n",
      "Iteration 4239, loss = 16.50489926\n",
      "Iteration 4240, loss = 16.65847320\n",
      "Iteration 4241, loss = 17.00456742\n",
      "Iteration 4242, loss = 17.21593039\n",
      "Iteration 4243, loss = 17.15510679\n",
      "Iteration 4244, loss = 18.75060938\n",
      "Iteration 4245, loss = 17.88295762\n",
      "Iteration 4246, loss = 17.62171903\n",
      "Iteration 4247, loss = 17.18697115\n",
      "Iteration 4248, loss = 17.09593956\n",
      "Iteration 4249, loss = 16.56061271\n",
      "Iteration 4250, loss = 16.55011869\n",
      "Iteration 4251, loss = 16.88843863\n",
      "Iteration 4252, loss = 17.02202031\n",
      "Iteration 4253, loss = 17.10621683\n",
      "Iteration 4254, loss = 17.71396527\n",
      "Iteration 4255, loss = 16.57027779\n",
      "Iteration 4256, loss = 16.52345891\n",
      "Iteration 4257, loss = 16.38053304\n",
      "Iteration 4258, loss = 17.90385827\n",
      "Iteration 4259, loss = 17.71628739\n",
      "Iteration 4260, loss = 18.03014977\n",
      "Iteration 4261, loss = 16.80854450\n",
      "Iteration 4262, loss = 16.73306124\n",
      "Iteration 4263, loss = 16.56578235\n",
      "Iteration 4264, loss = 16.69993693\n",
      "Iteration 4265, loss = 16.57715018\n",
      "Iteration 4266, loss = 16.83342323\n",
      "Iteration 4267, loss = 17.25327106\n",
      "Iteration 4268, loss = 17.11839980\n",
      "Iteration 4269, loss = 16.64428299\n",
      "Iteration 4270, loss = 16.59594559\n",
      "Iteration 4271, loss = 17.23306821\n",
      "Iteration 4272, loss = 16.41635802\n",
      "Iteration 4273, loss = 17.13562853\n",
      "Iteration 4274, loss = 16.56678285\n",
      "Iteration 4275, loss = 16.72599241\n",
      "Iteration 4276, loss = 16.65740124\n",
      "Iteration 4277, loss = 16.75625617\n",
      "Iteration 4278, loss = 17.01657632\n",
      "Iteration 4279, loss = 17.85591868\n",
      "Iteration 4280, loss = 18.29907532\n",
      "Iteration 4281, loss = 18.93571829\n",
      "Iteration 4282, loss = 19.85524568\n",
      "Iteration 4283, loss = 18.22274077\n",
      "Iteration 4284, loss = 18.09889249\n",
      "Iteration 4285, loss = 17.98585058\n",
      "Iteration 4286, loss = 16.82898646\n",
      "Iteration 4287, loss = 16.63280193\n",
      "Iteration 4288, loss = 16.84275785\n",
      "Iteration 4289, loss = 16.35004800\n",
      "Iteration 4290, loss = 17.01925925\n",
      "Iteration 4291, loss = 17.33378478\n",
      "Iteration 4292, loss = 17.89316448\n",
      "Iteration 4293, loss = 16.45988707\n",
      "Iteration 4294, loss = 16.46145277\n",
      "Iteration 4295, loss = 16.53509164\n",
      "Iteration 4296, loss = 16.76989056\n",
      "Iteration 4297, loss = 17.18291455\n",
      "Iteration 4298, loss = 16.90547836\n",
      "Iteration 4299, loss = 19.08158996\n",
      "Iteration 4300, loss = 17.71452873\n",
      "Iteration 4301, loss = 17.52463969\n",
      "Iteration 4302, loss = 16.76092548\n",
      "Iteration 4303, loss = 16.91721311\n",
      "Iteration 4304, loss = 16.48218679\n",
      "Iteration 4305, loss = 16.21512191\n",
      "Iteration 4306, loss = 16.48303699\n",
      "Iteration 4307, loss = 17.43176186\n",
      "Iteration 4308, loss = 17.32728890\n",
      "Iteration 4309, loss = 16.59740661\n",
      "Iteration 4310, loss = 16.78729510\n",
      "Iteration 4311, loss = 16.27395728\n",
      "Iteration 4312, loss = 16.39595006\n",
      "Iteration 4313, loss = 16.67931679\n",
      "Iteration 4314, loss = 16.44605808\n",
      "Iteration 4315, loss = 16.36491013\n",
      "Iteration 4316, loss = 16.48676603\n",
      "Iteration 4317, loss = 16.61996495\n",
      "Iteration 4318, loss = 16.12340873\n",
      "Iteration 4319, loss = 16.25471337\n",
      "Iteration 4320, loss = 16.43441489\n",
      "Iteration 4321, loss = 16.24142820\n",
      "Iteration 4322, loss = 16.29312571\n",
      "Iteration 4323, loss = 16.24844079\n",
      "Iteration 4324, loss = 17.33736964\n",
      "Iteration 4325, loss = 17.45855831\n",
      "Iteration 4326, loss = 17.07712427\n",
      "Iteration 4327, loss = 16.66287001\n",
      "Iteration 4328, loss = 16.17430185\n",
      "Iteration 4329, loss = 16.26304296\n",
      "Iteration 4330, loss = 16.13391349\n",
      "Iteration 4331, loss = 17.98466657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4332, loss = 17.72483063\n",
      "Iteration 4333, loss = 17.14165188\n",
      "Iteration 4334, loss = 17.50473137\n",
      "Iteration 4335, loss = 19.14374797\n",
      "Iteration 4336, loss = 16.21395275\n",
      "Iteration 4337, loss = 16.15627761\n",
      "Iteration 4338, loss = 16.42863274\n",
      "Iteration 4339, loss = 16.28243582\n",
      "Iteration 4340, loss = 16.16350296\n",
      "Iteration 4341, loss = 16.28184850\n",
      "Iteration 4342, loss = 16.23123306\n",
      "Iteration 4343, loss = 16.22886016\n",
      "Iteration 4344, loss = 16.02879899\n",
      "Iteration 4345, loss = 16.46507989\n",
      "Iteration 4346, loss = 16.19257359\n",
      "Iteration 4347, loss = 16.09871742\n",
      "Iteration 4348, loss = 17.00620401\n",
      "Iteration 4349, loss = 16.76128981\n",
      "Iteration 4350, loss = 17.10396370\n",
      "Iteration 4351, loss = 17.79210767\n",
      "Iteration 4352, loss = 18.15134551\n",
      "Iteration 4353, loss = 16.80216467\n",
      "Iteration 4354, loss = 17.62180905\n",
      "Iteration 4355, loss = 17.36656562\n",
      "Iteration 4356, loss = 16.56121434\n",
      "Iteration 4357, loss = 16.35056259\n",
      "Iteration 4358, loss = 17.17434295\n",
      "Iteration 4359, loss = 16.94768553\n",
      "Iteration 4360, loss = 16.47141759\n",
      "Iteration 4361, loss = 16.07750595\n",
      "Iteration 4362, loss = 16.17420515\n",
      "Iteration 4363, loss = 16.04977685\n",
      "Iteration 4364, loss = 16.26769984\n",
      "Iteration 4365, loss = 16.00490182\n",
      "Iteration 4366, loss = 16.02773959\n",
      "Iteration 4367, loss = 16.43531785\n",
      "Iteration 4368, loss = 16.74176590\n",
      "Iteration 4369, loss = 16.71765467\n",
      "Iteration 4370, loss = 17.16072770\n",
      "Iteration 4371, loss = 16.35289748\n",
      "Iteration 4372, loss = 15.85367363\n",
      "Iteration 4373, loss = 15.94646198\n",
      "Iteration 4374, loss = 16.47414129\n",
      "Iteration 4375, loss = 17.88740339\n",
      "Iteration 4376, loss = 16.86859804\n",
      "Iteration 4377, loss = 17.21970080\n",
      "Iteration 4378, loss = 18.80365196\n",
      "Iteration 4379, loss = 18.07626526\n",
      "Iteration 4380, loss = 18.74012215\n",
      "Iteration 4381, loss = 16.98855319\n",
      "Iteration 4382, loss = 16.79069114\n",
      "Iteration 4383, loss = 15.92352158\n",
      "Iteration 4384, loss = 15.96272770\n",
      "Iteration 4385, loss = 16.17701236\n",
      "Iteration 4386, loss = 15.87333835\n",
      "Iteration 4387, loss = 15.79258287\n",
      "Iteration 4388, loss = 16.14246507\n",
      "Iteration 4389, loss = 16.29520306\n",
      "Iteration 4390, loss = 16.41260646\n",
      "Iteration 4391, loss = 16.67288193\n",
      "Iteration 4392, loss = 15.99589051\n",
      "Iteration 4393, loss = 16.33290978\n",
      "Iteration 4394, loss = 16.10210875\n",
      "Iteration 4395, loss = 16.14280746\n",
      "Iteration 4396, loss = 16.62142314\n",
      "Iteration 4397, loss = 16.12284526\n",
      "Iteration 4398, loss = 16.09718329\n",
      "Iteration 4399, loss = 16.14131942\n",
      "Iteration 4400, loss = 16.43035436\n",
      "Iteration 4401, loss = 17.44357090\n",
      "Iteration 4402, loss = 17.28804957\n",
      "Iteration 4403, loss = 16.27674574\n",
      "Iteration 4404, loss = 15.81028621\n",
      "Iteration 4405, loss = 15.79190708\n",
      "Iteration 4406, loss = 15.88571939\n",
      "Iteration 4407, loss = 16.37368041\n",
      "Iteration 4408, loss = 16.36824526\n",
      "Iteration 4409, loss = 16.87091480\n",
      "Iteration 4410, loss = 16.82354779\n",
      "Iteration 4411, loss = 15.89868643\n",
      "Iteration 4412, loss = 16.49782879\n",
      "Iteration 4413, loss = 17.19477900\n",
      "Iteration 4414, loss = 17.44775021\n",
      "Iteration 4415, loss = 16.70409147\n",
      "Iteration 4416, loss = 16.30326141\n",
      "Iteration 4417, loss = 16.27351441\n",
      "Iteration 4418, loss = 16.31591023\n",
      "Iteration 4419, loss = 15.64428290\n",
      "Iteration 4420, loss = 16.70891135\n",
      "Iteration 4421, loss = 16.68251883\n",
      "Iteration 4422, loss = 16.88104007\n",
      "Iteration 4423, loss = 16.33990776\n",
      "Iteration 4424, loss = 16.18052799\n",
      "Iteration 4425, loss = 16.26424848\n",
      "Iteration 4426, loss = 16.53765604\n",
      "Iteration 4427, loss = 15.74538751\n",
      "Iteration 4428, loss = 15.86539812\n",
      "Iteration 4429, loss = 15.99207859\n",
      "Iteration 4430, loss = 15.75444824\n",
      "Iteration 4431, loss = 16.01433505\n",
      "Iteration 4432, loss = 15.81369273\n",
      "Iteration 4433, loss = 15.69297610\n",
      "Iteration 4434, loss = 15.92993344\n",
      "Iteration 4435, loss = 15.93138759\n",
      "Iteration 4436, loss = 15.89131071\n",
      "Iteration 4437, loss = 15.92352898\n",
      "Iteration 4438, loss = 16.51237931\n",
      "Iteration 4439, loss = 16.12264261\n",
      "Iteration 4440, loss = 16.21350360\n",
      "Iteration 4441, loss = 17.01939886\n",
      "Iteration 4442, loss = 15.83531800\n",
      "Iteration 4443, loss = 15.87722169\n",
      "Iteration 4444, loss = 15.67151125\n",
      "Iteration 4445, loss = 15.62705515\n",
      "Iteration 4446, loss = 15.78134003\n",
      "Iteration 4447, loss = 16.32649287\n",
      "Iteration 4448, loss = 16.27172715\n",
      "Iteration 4449, loss = 15.76005293\n",
      "Iteration 4450, loss = 15.78701819\n",
      "Iteration 4451, loss = 15.67950321\n",
      "Iteration 4452, loss = 17.00927814\n",
      "Iteration 4453, loss = 16.44582125\n",
      "Iteration 4454, loss = 16.65910091\n",
      "Iteration 4455, loss = 16.02285987\n",
      "Iteration 4456, loss = 16.20842261\n",
      "Iteration 4457, loss = 16.36487299\n",
      "Iteration 4458, loss = 16.20157399\n",
      "Iteration 4459, loss = 15.75363666\n",
      "Iteration 4460, loss = 15.77187507\n",
      "Iteration 4461, loss = 16.36613531\n",
      "Iteration 4462, loss = 16.13728161\n",
      "Iteration 4463, loss = 17.21442472\n",
      "Iteration 4464, loss = 16.40978782\n",
      "Iteration 4465, loss = 16.03807715\n",
      "Iteration 4466, loss = 15.69665249\n",
      "Iteration 4467, loss = 16.44844598\n",
      "Iteration 4468, loss = 15.91084313\n",
      "Iteration 4469, loss = 15.92159951\n",
      "Iteration 4470, loss = 16.23837037\n",
      "Iteration 4471, loss = 16.67652530\n",
      "Iteration 4472, loss = 16.64901801\n",
      "Iteration 4473, loss = 17.57213567\n",
      "Iteration 4474, loss = 16.51552735\n",
      "Iteration 4475, loss = 16.10138309\n",
      "Iteration 4476, loss = 16.21496816\n",
      "Iteration 4477, loss = 15.75504715\n",
      "Iteration 4478, loss = 15.48181466\n",
      "Iteration 4479, loss = 16.21820111\n",
      "Iteration 4480, loss = 15.71978132\n",
      "Iteration 4481, loss = 15.72492506\n",
      "Iteration 4482, loss = 15.47192819\n",
      "Iteration 4483, loss = 15.48125462\n",
      "Iteration 4484, loss = 15.44251512\n",
      "Iteration 4485, loss = 15.28718838\n",
      "Iteration 4486, loss = 15.46825458\n",
      "Iteration 4487, loss = 16.14890126\n",
      "Iteration 4488, loss = 15.73016771\n",
      "Iteration 4489, loss = 15.41467300\n",
      "Iteration 4490, loss = 15.64239921\n",
      "Iteration 4491, loss = 15.92877329\n",
      "Iteration 4492, loss = 15.75026233\n",
      "Iteration 4493, loss = 15.58139345\n",
      "Iteration 4494, loss = 15.51512194\n",
      "Iteration 4495, loss = 15.35345543\n",
      "Iteration 4496, loss = 16.34027828\n",
      "Iteration 4497, loss = 15.80760509\n",
      "Iteration 4498, loss = 16.23087813\n",
      "Iteration 4499, loss = 15.64858168\n",
      "Iteration 4500, loss = 15.46398426\n",
      "Iteration 4501, loss = 16.18614204\n",
      "Iteration 4502, loss = 16.26504585\n",
      "Iteration 4503, loss = 16.25162580\n",
      "Iteration 4504, loss = 15.48130932\n",
      "Iteration 4505, loss = 15.47143526\n",
      "Iteration 4506, loss = 15.44571032\n",
      "Iteration 4507, loss = 15.68045296\n",
      "Iteration 4508, loss = 15.32260900\n",
      "Iteration 4509, loss = 15.89261631\n",
      "Iteration 4510, loss = 16.42571763\n",
      "Iteration 4511, loss = 16.76896387\n",
      "Iteration 4512, loss = 15.39524336\n",
      "Iteration 4513, loss = 15.31725726\n",
      "Iteration 4514, loss = 15.49433598\n",
      "Iteration 4515, loss = 15.72259611\n",
      "Iteration 4516, loss = 15.25366448\n",
      "Iteration 4517, loss = 15.73080445\n",
      "Iteration 4518, loss = 15.95694152\n",
      "Iteration 4519, loss = 15.83788376\n",
      "Iteration 4520, loss = 16.12611486\n",
      "Iteration 4521, loss = 16.30780806\n",
      "Iteration 4522, loss = 17.23303328\n",
      "Iteration 4523, loss = 19.63758070\n",
      "Iteration 4524, loss = 18.13922921\n",
      "Iteration 4525, loss = 17.46174266\n",
      "Iteration 4526, loss = 18.52706412\n",
      "Iteration 4527, loss = 18.22437090\n",
      "Iteration 4528, loss = 16.55973888\n",
      "Iteration 4529, loss = 15.78840711\n",
      "Iteration 4530, loss = 17.01705862\n",
      "Iteration 4531, loss = 16.44270770\n",
      "Iteration 4532, loss = 15.96772268\n",
      "Iteration 4533, loss = 15.84435858\n",
      "Iteration 4534, loss = 16.06387153\n",
      "Iteration 4535, loss = 15.82987247\n",
      "Iteration 4536, loss = 15.86656170\n",
      "Iteration 4537, loss = 15.71965548\n",
      "Iteration 4538, loss = 15.57921365\n",
      "Iteration 4539, loss = 15.75934885\n",
      "Iteration 4540, loss = 15.65203344\n",
      "Iteration 4541, loss = 15.42461123\n",
      "Iteration 4542, loss = 15.06646983\n",
      "Iteration 4543, loss = 15.44462249\n",
      "Iteration 4544, loss = 15.76247548\n",
      "Iteration 4545, loss = 16.63179282\n",
      "Iteration 4546, loss = 15.52746139\n",
      "Iteration 4547, loss = 15.24535095\n",
      "Iteration 4548, loss = 15.14601021\n",
      "Iteration 4549, loss = 15.39864066\n",
      "Iteration 4550, loss = 15.73899950\n",
      "Iteration 4551, loss = 15.26867224\n",
      "Iteration 4552, loss = 16.21692828\n",
      "Iteration 4553, loss = 15.98292390\n",
      "Iteration 4554, loss = 17.17357786\n",
      "Iteration 4555, loss = 17.35626934\n",
      "Iteration 4556, loss = 16.66304097\n",
      "Iteration 4557, loss = 15.77581487\n",
      "Iteration 4558, loss = 15.68712493\n",
      "Iteration 4559, loss = 15.32107600\n",
      "Iteration 4560, loss = 15.74463962\n",
      "Iteration 4561, loss = 15.08812135\n",
      "Iteration 4562, loss = 15.28269130\n",
      "Iteration 4563, loss = 16.78667618\n",
      "Iteration 4564, loss = 17.75772086\n",
      "Iteration 4565, loss = 16.38031178\n",
      "Iteration 4566, loss = 16.06382774\n",
      "Iteration 4567, loss = 15.88823999\n",
      "Iteration 4568, loss = 15.21929595\n",
      "Iteration 4569, loss = 15.42859616\n",
      "Iteration 4570, loss = 15.20967168\n",
      "Iteration 4571, loss = 15.32798497\n",
      "Iteration 4572, loss = 15.83833269\n",
      "Iteration 4573, loss = 15.22797838\n",
      "Iteration 4574, loss = 15.12502401\n",
      "Iteration 4575, loss = 15.54517068\n",
      "Iteration 4576, loss = 15.90605731\n",
      "Iteration 4577, loss = 15.06589474\n",
      "Iteration 4578, loss = 15.53275739\n",
      "Iteration 4579, loss = 15.33051051\n",
      "Iteration 4580, loss = 15.72940256\n",
      "Iteration 4581, loss = 15.75277277\n",
      "Iteration 4582, loss = 15.63275233\n",
      "Iteration 4583, loss = 15.69199775\n",
      "Iteration 4584, loss = 15.00312292\n",
      "Iteration 4585, loss = 15.12319611\n",
      "Iteration 4586, loss = 15.21357497\n",
      "Iteration 4587, loss = 14.90258614\n",
      "Iteration 4588, loss = 16.58796661\n",
      "Iteration 4589, loss = 16.77540663\n",
      "Iteration 4590, loss = 16.04639918\n",
      "Iteration 4591, loss = 15.62878789\n",
      "Iteration 4592, loss = 17.51370553\n",
      "Iteration 4593, loss = 16.26983659\n",
      "Iteration 4594, loss = 15.96756867\n",
      "Iteration 4595, loss = 15.86708993\n",
      "Iteration 4596, loss = 15.37295424\n",
      "Iteration 4597, loss = 15.58186447\n",
      "Iteration 4598, loss = 15.14164561\n",
      "Iteration 4599, loss = 15.53632210\n",
      "Iteration 4600, loss = 15.96958950\n",
      "Iteration 4601, loss = 16.78153790\n",
      "Iteration 4602, loss = 16.03507187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4603, loss = 15.47088982\n",
      "Iteration 4604, loss = 16.64559426\n",
      "Iteration 4605, loss = 17.61613606\n",
      "Iteration 4606, loss = 17.50377229\n",
      "Iteration 4607, loss = 19.08475393\n",
      "Iteration 4608, loss = 16.09507525\n",
      "Iteration 4609, loss = 16.43038653\n",
      "Iteration 4610, loss = 16.00223421\n",
      "Iteration 4611, loss = 15.39453904\n",
      "Iteration 4612, loss = 15.55862388\n",
      "Iteration 4613, loss = 15.86997780\n",
      "Iteration 4614, loss = 15.52509187\n",
      "Iteration 4615, loss = 15.26044751\n",
      "Iteration 4616, loss = 14.94848484\n",
      "Iteration 4617, loss = 15.34254666\n",
      "Iteration 4618, loss = 15.44174327\n",
      "Iteration 4619, loss = 15.43902000\n",
      "Iteration 4620, loss = 16.18863589\n",
      "Iteration 4621, loss = 15.01708787\n",
      "Iteration 4622, loss = 16.12087760\n",
      "Iteration 4623, loss = 15.96562489\n",
      "Iteration 4624, loss = 16.43624848\n",
      "Iteration 4625, loss = 15.98782051\n",
      "Iteration 4626, loss = 15.39774584\n",
      "Iteration 4627, loss = 15.25715897\n",
      "Iteration 4628, loss = 15.18233938\n",
      "Iteration 4629, loss = 15.68687755\n",
      "Iteration 4630, loss = 17.16747040\n",
      "Iteration 4631, loss = 15.65304654\n",
      "Iteration 4632, loss = 16.38896666\n",
      "Iteration 4633, loss = 17.08175025\n",
      "Iteration 4634, loss = 16.57700820\n",
      "Iteration 4635, loss = 16.10756611\n",
      "Iteration 4636, loss = 15.15580069\n",
      "Iteration 4637, loss = 15.21943149\n",
      "Iteration 4638, loss = 14.86789312\n",
      "Iteration 4639, loss = 15.05178751\n",
      "Iteration 4640, loss = 15.25250514\n",
      "Iteration 4641, loss = 15.32649953\n",
      "Iteration 4642, loss = 15.40512804\n",
      "Iteration 4643, loss = 15.63117635\n",
      "Iteration 4644, loss = 15.31997777\n",
      "Iteration 4645, loss = 15.47700326\n",
      "Iteration 4646, loss = 15.50073986\n",
      "Iteration 4647, loss = 14.94410270\n",
      "Iteration 4648, loss = 15.81720894\n",
      "Iteration 4649, loss = 14.80828444\n",
      "Iteration 4650, loss = 14.81407720\n",
      "Iteration 4651, loss = 15.85536252\n",
      "Iteration 4652, loss = 14.73713128\n",
      "Iteration 4653, loss = 14.85968439\n",
      "Iteration 4654, loss = 15.21216753\n",
      "Iteration 4655, loss = 15.16245502\n",
      "Iteration 4656, loss = 15.97786622\n",
      "Iteration 4657, loss = 16.25222606\n",
      "Iteration 4658, loss = 15.89711809\n",
      "Iteration 4659, loss = 17.63170458\n",
      "Iteration 4660, loss = 16.24809455\n",
      "Iteration 4661, loss = 14.71387295\n",
      "Iteration 4662, loss = 14.77139775\n",
      "Iteration 4663, loss = 15.38472640\n",
      "Iteration 4664, loss = 15.49836800\n",
      "Iteration 4665, loss = 15.72002536\n",
      "Iteration 4666, loss = 14.83660067\n",
      "Iteration 4667, loss = 15.86838772\n",
      "Iteration 4668, loss = 16.48932836\n",
      "Iteration 4669, loss = 16.08905961\n",
      "Iteration 4670, loss = 16.46776321\n",
      "Iteration 4671, loss = 16.44243684\n",
      "Iteration 4672, loss = 14.99604471\n",
      "Iteration 4673, loss = 15.45418914\n",
      "Iteration 4674, loss = 15.85465535\n",
      "Iteration 4675, loss = 15.16228870\n",
      "Iteration 4676, loss = 14.87300132\n",
      "Iteration 4677, loss = 14.83143638\n",
      "Iteration 4678, loss = 15.20133782\n",
      "Iteration 4679, loss = 15.01785292\n",
      "Iteration 4680, loss = 15.01763954\n",
      "Iteration 4681, loss = 14.71829757\n",
      "Iteration 4682, loss = 14.68218909\n",
      "Iteration 4683, loss = 14.79316912\n",
      "Iteration 4684, loss = 15.34324661\n",
      "Iteration 4685, loss = 14.87929976\n",
      "Iteration 4686, loss = 14.86524676\n",
      "Iteration 4687, loss = 14.77912687\n",
      "Iteration 4688, loss = 14.79370868\n",
      "Iteration 4689, loss = 15.07923485\n",
      "Iteration 4690, loss = 14.81995362\n",
      "Iteration 4691, loss = 14.92227952\n",
      "Iteration 4692, loss = 15.57325264\n",
      "Iteration 4693, loss = 15.59839604\n",
      "Iteration 4694, loss = 15.15361582\n",
      "Iteration 4695, loss = 15.04583222\n",
      "Iteration 4696, loss = 15.33634546\n",
      "Iteration 4697, loss = 14.96763784\n",
      "Iteration 4698, loss = 14.55741813\n",
      "Iteration 4699, loss = 14.75135398\n",
      "Iteration 4700, loss = 15.33682964\n",
      "Iteration 4701, loss = 15.52074601\n",
      "Iteration 4702, loss = 16.28675860\n",
      "Iteration 4703, loss = 15.61363008\n",
      "Iteration 4704, loss = 15.75048122\n",
      "Iteration 4705, loss = 16.68090718\n",
      "Iteration 4706, loss = 15.44561311\n",
      "Iteration 4707, loss = 14.68641245\n",
      "Iteration 4708, loss = 15.24917716\n",
      "Iteration 4709, loss = 15.89480395\n",
      "Iteration 4710, loss = 15.52896438\n",
      "Iteration 4711, loss = 14.71232767\n",
      "Iteration 4712, loss = 14.55504932\n",
      "Iteration 4713, loss = 14.99135266\n",
      "Iteration 4714, loss = 14.99867285\n",
      "Iteration 4715, loss = 14.92846815\n",
      "Iteration 4716, loss = 14.74056298\n",
      "Iteration 4717, loss = 14.90808463\n",
      "Iteration 4718, loss = 15.18094180\n",
      "Iteration 4719, loss = 14.76964532\n",
      "Iteration 4720, loss = 14.71928745\n",
      "Iteration 4721, loss = 15.34371013\n",
      "Iteration 4722, loss = 15.65664656\n",
      "Iteration 4723, loss = 15.54436959\n",
      "Iteration 4724, loss = 15.56342552\n",
      "Iteration 4725, loss = 15.04940664\n",
      "Iteration 4726, loss = 14.51704480\n",
      "Iteration 4727, loss = 14.46375741\n",
      "Iteration 4728, loss = 14.42479886\n",
      "Iteration 4729, loss = 15.61329127\n",
      "Iteration 4730, loss = 15.88196887\n",
      "Iteration 4731, loss = 15.63764130\n",
      "Iteration 4732, loss = 15.20125841\n",
      "Iteration 4733, loss = 15.34228988\n",
      "Iteration 4734, loss = 14.83587038\n",
      "Iteration 4735, loss = 15.12658939\n",
      "Iteration 4736, loss = 16.00000498\n",
      "Iteration 4737, loss = 14.90520349\n",
      "Iteration 4738, loss = 14.85715275\n",
      "Iteration 4739, loss = 15.22203302\n",
      "Iteration 4740, loss = 15.29543437\n",
      "Iteration 4741, loss = 15.70628440\n",
      "Iteration 4742, loss = 14.78188253\n",
      "Iteration 4743, loss = 14.71320413\n",
      "Iteration 4744, loss = 14.93259079\n",
      "Iteration 4745, loss = 15.45390821\n",
      "Iteration 4746, loss = 14.83712065\n",
      "Iteration 4747, loss = 14.74423214\n",
      "Iteration 4748, loss = 15.49171844\n",
      "Iteration 4749, loss = 16.45269804\n",
      "Iteration 4750, loss = 15.12920441\n",
      "Iteration 4751, loss = 14.46115802\n",
      "Iteration 4752, loss = 14.51266714\n",
      "Iteration 4753, loss = 14.35876206\n",
      "Iteration 4754, loss = 14.39029956\n",
      "Iteration 4755, loss = 14.45441733\n",
      "Iteration 4756, loss = 15.02293757\n",
      "Iteration 4757, loss = 15.40556194\n",
      "Iteration 4758, loss = 15.27259376\n",
      "Iteration 4759, loss = 14.86128492\n",
      "Iteration 4760, loss = 14.50966669\n",
      "Iteration 4761, loss = 14.29242167\n",
      "Iteration 4762, loss = 14.53739730\n",
      "Iteration 4763, loss = 15.05385978\n",
      "Iteration 4764, loss = 15.44881933\n",
      "Iteration 4765, loss = 15.57118221\n",
      "Iteration 4766, loss = 16.05479073\n",
      "Iteration 4767, loss = 16.12322456\n",
      "Iteration 4768, loss = 16.85566132\n",
      "Iteration 4769, loss = 15.60080909\n",
      "Iteration 4770, loss = 14.66083467\n",
      "Iteration 4771, loss = 14.47887297\n",
      "Iteration 4772, loss = 14.49281575\n",
      "Iteration 4773, loss = 14.44269601\n",
      "Iteration 4774, loss = 14.50068956\n",
      "Iteration 4775, loss = 14.44789752\n",
      "Iteration 4776, loss = 14.46363198\n",
      "Iteration 4777, loss = 14.35298591\n",
      "Iteration 4778, loss = 14.39590779\n",
      "Iteration 4779, loss = 14.35101473\n",
      "Iteration 4780, loss = 14.23595116\n",
      "Iteration 4781, loss = 14.56293545\n",
      "Iteration 4782, loss = 16.77006063\n",
      "Iteration 4783, loss = 16.28700956\n",
      "Iteration 4784, loss = 15.35785691\n",
      "Iteration 4785, loss = 15.02294210\n",
      "Iteration 4786, loss = 14.71769026\n",
      "Iteration 4787, loss = 14.59578403\n",
      "Iteration 4788, loss = 14.94303996\n",
      "Iteration 4789, loss = 14.44635332\n",
      "Iteration 4790, loss = 14.33242643\n",
      "Iteration 4791, loss = 15.42200972\n",
      "Iteration 4792, loss = 14.64203477\n",
      "Iteration 4793, loss = 14.55858207\n",
      "Iteration 4794, loss = 14.83469392\n",
      "Iteration 4795, loss = 14.34744652\n",
      "Iteration 4796, loss = 14.32452210\n",
      "Iteration 4797, loss = 14.17263841\n",
      "Iteration 4798, loss = 14.84052479\n",
      "Iteration 4799, loss = 14.22658761\n",
      "Iteration 4800, loss = 14.37722239\n",
      "Iteration 4801, loss = 15.02635115\n",
      "Iteration 4802, loss = 14.39371873\n",
      "Iteration 4803, loss = 14.58708824\n",
      "Iteration 4804, loss = 14.57079310\n",
      "Iteration 4805, loss = 14.59191712\n",
      "Iteration 4806, loss = 14.50493750\n",
      "Iteration 4807, loss = 15.06002834\n",
      "Iteration 4808, loss = 14.50295572\n",
      "Iteration 4809, loss = 14.50447226\n",
      "Iteration 4810, loss = 14.67314099\n",
      "Iteration 4811, loss = 15.26351574\n",
      "Iteration 4812, loss = 15.30165120\n",
      "Iteration 4813, loss = 16.08198827\n",
      "Iteration 4814, loss = 15.68089107\n",
      "Iteration 4815, loss = 15.38649178\n",
      "Iteration 4816, loss = 14.53778962\n",
      "Iteration 4817, loss = 14.20203453\n",
      "Iteration 4818, loss = 15.31348030\n",
      "Iteration 4819, loss = 15.40593162\n",
      "Iteration 4820, loss = 14.58570198\n",
      "Iteration 4821, loss = 14.80381541\n",
      "Iteration 4822, loss = 14.44296601\n",
      "Iteration 4823, loss = 14.47335436\n",
      "Iteration 4824, loss = 14.98943164\n",
      "Iteration 4825, loss = 14.83722086\n",
      "Iteration 4826, loss = 14.17488243\n",
      "Iteration 4827, loss = 15.22244841\n",
      "Iteration 4828, loss = 14.36071042\n",
      "Iteration 4829, loss = 14.85706111\n",
      "Iteration 4830, loss = 14.46891039\n",
      "Iteration 4831, loss = 14.31577724\n",
      "Iteration 4832, loss = 14.71870778\n",
      "Iteration 4833, loss = 14.18613938\n",
      "Iteration 4834, loss = 14.56060417\n",
      "Iteration 4835, loss = 14.04956067\n",
      "Iteration 4836, loss = 14.31761977\n",
      "Iteration 4837, loss = 14.29758681\n",
      "Iteration 4838, loss = 14.91579880\n",
      "Iteration 4839, loss = 16.58184809\n",
      "Iteration 4840, loss = 15.18262733\n",
      "Iteration 4841, loss = 15.64117229\n",
      "Iteration 4842, loss = 14.89794295\n",
      "Iteration 4843, loss = 14.30021569\n",
      "Iteration 4844, loss = 14.55401837\n",
      "Iteration 4845, loss = 16.00462394\n",
      "Iteration 4846, loss = 15.10908124\n",
      "Iteration 4847, loss = 14.80341173\n",
      "Iteration 4848, loss = 14.44493117\n",
      "Iteration 4849, loss = 14.89786452\n",
      "Iteration 4850, loss = 14.40202421\n",
      "Iteration 4851, loss = 14.30315071\n",
      "Iteration 4852, loss = 14.51602060\n",
      "Iteration 4853, loss = 14.40650597\n",
      "Iteration 4854, loss = 14.42314933\n",
      "Iteration 4855, loss = 14.42011385\n",
      "Iteration 4856, loss = 14.28518669\n",
      "Iteration 4857, loss = 14.16478439\n",
      "Iteration 4858, loss = 14.03209035\n",
      "Iteration 4859, loss = 14.04138526\n",
      "Iteration 4860, loss = 14.29558070\n",
      "Iteration 4861, loss = 14.25612300\n",
      "Iteration 4862, loss = 14.26656005\n",
      "Iteration 4863, loss = 14.06573173\n",
      "Iteration 4864, loss = 14.24823811\n",
      "Iteration 4865, loss = 14.80439179\n",
      "Iteration 4866, loss = 15.19137584\n",
      "Iteration 4867, loss = 14.71643899\n",
      "Iteration 4868, loss = 14.48504719\n",
      "Iteration 4869, loss = 14.74643013\n",
      "Iteration 4870, loss = 14.65557958\n",
      "Iteration 4871, loss = 14.61079305\n",
      "Iteration 4872, loss = 14.33728544\n",
      "Iteration 4873, loss = 14.03384273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4874, loss = 14.15415761\n",
      "Iteration 4875, loss = 14.25250353\n",
      "Iteration 4876, loss = 14.09921997\n",
      "Iteration 4877, loss = 14.11566951\n",
      "Iteration 4878, loss = 14.28705092\n",
      "Iteration 4879, loss = 14.22603591\n",
      "Iteration 4880, loss = 14.54548842\n",
      "Iteration 4881, loss = 14.04568817\n",
      "Iteration 4882, loss = 14.48319956\n",
      "Iteration 4883, loss = 14.25049079\n",
      "Iteration 4884, loss = 14.13871582\n",
      "Iteration 4885, loss = 14.03968620\n",
      "Iteration 4886, loss = 13.95201150\n",
      "Iteration 4887, loss = 14.09563832\n",
      "Iteration 4888, loss = 14.49114514\n",
      "Iteration 4889, loss = 14.05900181\n",
      "Iteration 4890, loss = 15.11416312\n",
      "Iteration 4891, loss = 14.74845782\n",
      "Iteration 4892, loss = 15.36752664\n",
      "Iteration 4893, loss = 14.98302353\n",
      "Iteration 4894, loss = 16.65612847\n",
      "Iteration 4895, loss = 14.94706678\n",
      "Iteration 4896, loss = 14.07678278\n",
      "Iteration 4897, loss = 14.66297019\n",
      "Iteration 4898, loss = 14.05555241\n",
      "Iteration 4899, loss = 14.58691705\n",
      "Iteration 4900, loss = 14.23243905\n",
      "Iteration 4901, loss = 14.20098692\n",
      "Iteration 4902, loss = 14.52547511\n",
      "Iteration 4903, loss = 14.94710550\n",
      "Iteration 4904, loss = 14.54356352\n",
      "Iteration 4905, loss = 13.78408584\n",
      "Iteration 4906, loss = 14.29436430\n",
      "Iteration 4907, loss = 14.31281716\n",
      "Iteration 4908, loss = 14.25855618\n",
      "Iteration 4909, loss = 14.66167568\n",
      "Iteration 4910, loss = 14.27209457\n",
      "Iteration 4911, loss = 14.21084927\n",
      "Iteration 4912, loss = 13.99073245\n",
      "Iteration 4913, loss = 13.90877276\n",
      "Iteration 4914, loss = 14.43831587\n",
      "Iteration 4915, loss = 14.58432068\n",
      "Iteration 4916, loss = 14.58729615\n",
      "Iteration 4917, loss = 14.82117172\n",
      "Iteration 4918, loss = 14.98622756\n",
      "Iteration 4919, loss = 16.37193693\n",
      "Iteration 4920, loss = 16.18431020\n",
      "Iteration 4921, loss = 15.26757578\n",
      "Iteration 4922, loss = 15.27334349\n",
      "Iteration 4923, loss = 16.00684244\n",
      "Iteration 4924, loss = 14.89477570\n",
      "Iteration 4925, loss = 14.83590628\n",
      "Iteration 4926, loss = 14.86666498\n",
      "Iteration 4927, loss = 15.16333652\n",
      "Iteration 4928, loss = 13.97249255\n",
      "Iteration 4929, loss = 14.12748044\n",
      "Iteration 4930, loss = 14.28125299\n",
      "Iteration 4931, loss = 13.98435562\n",
      "Iteration 4932, loss = 14.39814436\n",
      "Iteration 4933, loss = 14.73358278\n",
      "Iteration 4934, loss = 15.81004068\n",
      "Iteration 4935, loss = 15.63109871\n",
      "Iteration 4936, loss = 16.63682871\n",
      "Iteration 4937, loss = 16.59209842\n",
      "Iteration 4938, loss = 14.68591019\n",
      "Iteration 4939, loss = 14.14363944\n",
      "Iteration 4940, loss = 14.36731158\n",
      "Iteration 4941, loss = 13.98794653\n",
      "Iteration 4942, loss = 14.62700599\n",
      "Iteration 4943, loss = 15.33301223\n",
      "Iteration 4944, loss = 14.30853536\n",
      "Iteration 4945, loss = 14.88663354\n",
      "Iteration 4946, loss = 15.51822341\n",
      "Iteration 4947, loss = 14.25472849\n",
      "Iteration 4948, loss = 14.70165147\n",
      "Iteration 4949, loss = 15.23772897\n",
      "Iteration 4950, loss = 16.45427054\n",
      "Iteration 4951, loss = 14.69803939\n",
      "Iteration 4952, loss = 15.98727212\n",
      "Iteration 4953, loss = 15.41624951\n",
      "Iteration 4954, loss = 15.45945727\n",
      "Iteration 4955, loss = 15.09504645\n",
      "Iteration 4956, loss = 17.27063480\n",
      "Iteration 4957, loss = 17.88225758\n",
      "Iteration 4958, loss = 15.85292443\n",
      "Iteration 4959, loss = 14.31854508\n",
      "Iteration 4960, loss = 14.11536104\n",
      "Iteration 4961, loss = 13.95601784\n",
      "Iteration 4962, loss = 13.95319693\n",
      "Iteration 4963, loss = 13.90316540\n",
      "Iteration 4964, loss = 14.11069814\n",
      "Iteration 4965, loss = 14.59803349\n",
      "Iteration 4966, loss = 15.04467606\n",
      "Iteration 4967, loss = 15.19708318\n",
      "Iteration 4968, loss = 14.47111776\n",
      "Iteration 4969, loss = 14.65445846\n",
      "Iteration 4970, loss = 14.18373177\n",
      "Iteration 4971, loss = 14.13213974\n",
      "Iteration 4972, loss = 14.40345780\n",
      "Iteration 4973, loss = 14.68804625\n",
      "Iteration 4974, loss = 14.53133937\n",
      "Iteration 4975, loss = 15.96737970\n",
      "Iteration 4976, loss = 15.96839241\n",
      "Iteration 4977, loss = 16.31746873\n",
      "Iteration 4978, loss = 16.47780954\n",
      "Iteration 4979, loss = 14.33341215\n",
      "Iteration 4980, loss = 13.83016740\n",
      "Iteration 4981, loss = 13.91780202\n",
      "Iteration 4982, loss = 13.92888217\n",
      "Iteration 4983, loss = 14.61181313\n",
      "Iteration 4984, loss = 14.53544129\n",
      "Iteration 4985, loss = 14.71139162\n",
      "Iteration 4986, loss = 16.15562826\n",
      "Iteration 4987, loss = 15.61752680\n",
      "Iteration 4988, loss = 15.34081423\n",
      "Iteration 4989, loss = 14.69673558\n",
      "Iteration 4990, loss = 14.41818881\n",
      "Iteration 4991, loss = 14.35699773\n",
      "Iteration 4992, loss = 14.94981804\n",
      "Iteration 4993, loss = 14.78394933\n",
      "Iteration 4994, loss = 14.02308203\n",
      "Iteration 4995, loss = 13.70155505\n",
      "Iteration 4996, loss = 14.98183516\n",
      "Iteration 4997, loss = 14.17955034\n",
      "Iteration 4998, loss = 14.19783054\n",
      "Iteration 4999, loss = 13.90837375\n",
      "Iteration 5000, loss = 13.80532054\n",
      "Iteration 5001, loss = 13.83895651\n",
      "Iteration 5002, loss = 14.96881143\n",
      "Iteration 5003, loss = 14.43052287\n",
      "Iteration 5004, loss = 14.01287649\n",
      "Iteration 5005, loss = 14.40129433\n",
      "Iteration 5006, loss = 14.45506823\n",
      "Iteration 5007, loss = 13.98213078\n",
      "Iteration 5008, loss = 14.74534499\n",
      "Iteration 5009, loss = 14.66498952\n",
      "Iteration 5010, loss = 14.85303794\n",
      "Iteration 5011, loss = 14.06372910\n",
      "Iteration 5012, loss = 14.06748501\n",
      "Iteration 5013, loss = 14.06233192\n",
      "Iteration 5014, loss = 13.97193721\n",
      "Iteration 5015, loss = 13.83990432\n",
      "Iteration 5016, loss = 13.92818902\n",
      "Iteration 5017, loss = 13.89111918\n",
      "Iteration 5018, loss = 13.84458610\n",
      "Iteration 5019, loss = 14.18742921\n",
      "Iteration 5020, loss = 14.02063774\n",
      "Iteration 5021, loss = 14.24108429\n",
      "Iteration 5022, loss = 14.38237512\n",
      "Iteration 5023, loss = 13.93951627\n",
      "Iteration 5024, loss = 13.67849834\n",
      "Iteration 5025, loss = 14.08756619\n",
      "Iteration 5026, loss = 13.73559415\n",
      "Iteration 5027, loss = 13.70526153\n",
      "Iteration 5028, loss = 14.43803535\n",
      "Iteration 5029, loss = 15.04728036\n",
      "Iteration 5030, loss = 13.84997048\n",
      "Iteration 5031, loss = 13.78216597\n",
      "Iteration 5032, loss = 13.72888011\n",
      "Iteration 5033, loss = 14.00042097\n",
      "Iteration 5034, loss = 14.50869815\n",
      "Iteration 5035, loss = 13.94015997\n",
      "Iteration 5036, loss = 13.90219820\n",
      "Iteration 5037, loss = 14.59156658\n",
      "Iteration 5038, loss = 13.99723789\n",
      "Iteration 5039, loss = 13.52467015\n",
      "Iteration 5040, loss = 14.16036142\n",
      "Iteration 5041, loss = 14.20074135\n",
      "Iteration 5042, loss = 13.40996473\n",
      "Iteration 5043, loss = 14.05870143\n",
      "Iteration 5044, loss = 13.81135654\n",
      "Iteration 5045, loss = 13.90013262\n",
      "Iteration 5046, loss = 14.99282640\n",
      "Iteration 5047, loss = 13.83127132\n",
      "Iteration 5048, loss = 13.87389077\n",
      "Iteration 5049, loss = 13.66610036\n",
      "Iteration 5050, loss = 14.02882794\n",
      "Iteration 5051, loss = 14.70736873\n",
      "Iteration 5052, loss = 15.61330823\n",
      "Iteration 5053, loss = 14.76896519\n",
      "Iteration 5054, loss = 14.61434499\n",
      "Iteration 5055, loss = 13.69377793\n",
      "Iteration 5056, loss = 13.74620113\n",
      "Iteration 5057, loss = 14.45157320\n",
      "Iteration 5058, loss = 14.13252459\n",
      "Iteration 5059, loss = 14.10490572\n",
      "Iteration 5060, loss = 13.40494814\n",
      "Iteration 5061, loss = 13.69375366\n",
      "Iteration 5062, loss = 13.86625320\n",
      "Iteration 5063, loss = 13.72796812\n",
      "Iteration 5064, loss = 14.08021141\n",
      "Iteration 5065, loss = 13.76375964\n",
      "Iteration 5066, loss = 14.61693295\n",
      "Iteration 5067, loss = 14.16316785\n",
      "Iteration 5068, loss = 13.87239607\n",
      "Iteration 5069, loss = 13.65103191\n",
      "Iteration 5070, loss = 13.97096832\n",
      "Iteration 5071, loss = 14.04667549\n",
      "Iteration 5072, loss = 15.03184057\n",
      "Iteration 5073, loss = 13.90809157\n",
      "Iteration 5074, loss = 13.57870137\n",
      "Iteration 5075, loss = 13.39602717\n",
      "Iteration 5076, loss = 14.47264615\n",
      "Iteration 5077, loss = 14.24712561\n",
      "Iteration 5078, loss = 13.95111951\n",
      "Iteration 5079, loss = 14.56960724\n",
      "Iteration 5080, loss = 15.01064477\n",
      "Iteration 5081, loss = 15.69153580\n",
      "Iteration 5082, loss = 14.13870376\n",
      "Iteration 5083, loss = 13.65675395\n",
      "Iteration 5084, loss = 13.75681113\n",
      "Iteration 5085, loss = 13.55493463\n",
      "Iteration 5086, loss = 14.75234464\n",
      "Iteration 5087, loss = 16.29406295\n",
      "Iteration 5088, loss = 15.99138748\n",
      "Iteration 5089, loss = 17.02668338\n",
      "Iteration 5090, loss = 15.31838634\n",
      "Iteration 5091, loss = 15.18955668\n",
      "Iteration 5092, loss = 16.45179372\n",
      "Iteration 5093, loss = 15.45207181\n",
      "Iteration 5094, loss = 14.82704901\n",
      "Iteration 5095, loss = 15.76649970\n",
      "Iteration 5096, loss = 15.07585280\n",
      "Iteration 5097, loss = 14.12906375\n",
      "Iteration 5098, loss = 13.77619261\n",
      "Iteration 5099, loss = 14.28500370\n",
      "Iteration 5100, loss = 14.06080864\n",
      "Iteration 5101, loss = 14.28908835\n",
      "Iteration 5102, loss = 13.74076187\n",
      "Iteration 5103, loss = 13.73271632\n",
      "Iteration 5104, loss = 13.45337173\n",
      "Iteration 5105, loss = 13.35592899\n",
      "Iteration 5106, loss = 13.41399118\n",
      "Iteration 5107, loss = 13.51953101\n",
      "Iteration 5108, loss = 14.61939967\n",
      "Iteration 5109, loss = 13.95235995\n",
      "Iteration 5110, loss = 13.69610901\n",
      "Iteration 5111, loss = 14.03422202\n",
      "Iteration 5112, loss = 14.36074615\n",
      "Iteration 5113, loss = 13.90505471\n",
      "Iteration 5114, loss = 14.15786850\n",
      "Iteration 5115, loss = 13.77296225\n",
      "Iteration 5116, loss = 13.41653368\n",
      "Iteration 5117, loss = 13.88362963\n",
      "Iteration 5118, loss = 13.89872320\n",
      "Iteration 5119, loss = 13.44464195\n",
      "Iteration 5120, loss = 13.51463496\n",
      "Iteration 5121, loss = 15.52684245\n",
      "Iteration 5122, loss = 15.15173636\n",
      "Iteration 5123, loss = 14.97795333\n",
      "Iteration 5124, loss = 14.15700492\n",
      "Iteration 5125, loss = 13.33827093\n",
      "Iteration 5126, loss = 13.76884878\n",
      "Iteration 5127, loss = 13.57081817\n",
      "Iteration 5128, loss = 14.56589345\n",
      "Iteration 5129, loss = 13.34889846\n",
      "Iteration 5130, loss = 13.51456557\n",
      "Iteration 5131, loss = 14.51862509\n",
      "Iteration 5132, loss = 13.98422632\n",
      "Iteration 5133, loss = 13.94093856\n",
      "Iteration 5134, loss = 13.71338512\n",
      "Iteration 5135, loss = 13.69818522\n",
      "Iteration 5136, loss = 13.73779081\n",
      "Iteration 5137, loss = 13.55070092\n",
      "Iteration 5138, loss = 13.30260634\n",
      "Iteration 5139, loss = 13.62233407\n",
      "Iteration 5140, loss = 13.39962363\n",
      "Iteration 5141, loss = 13.29153757\n",
      "Iteration 5142, loss = 13.82938735\n",
      "Iteration 5143, loss = 14.00442776\n",
      "Iteration 5144, loss = 14.06176933\n",
      "Iteration 5145, loss = 13.82683183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5146, loss = 13.84929394\n",
      "Iteration 5147, loss = 13.45570842\n",
      "Iteration 5148, loss = 14.17731525\n",
      "Iteration 5149, loss = 14.42025264\n",
      "Iteration 5150, loss = 14.18573347\n",
      "Iteration 5151, loss = 13.78148121\n",
      "Iteration 5152, loss = 13.69294593\n",
      "Iteration 5153, loss = 13.55564406\n",
      "Iteration 5154, loss = 13.55142133\n",
      "Iteration 5155, loss = 13.72313936\n",
      "Iteration 5156, loss = 14.27420641\n",
      "Iteration 5157, loss = 14.43517200\n",
      "Iteration 5158, loss = 13.56684655\n",
      "Iteration 5159, loss = 13.70611610\n",
      "Iteration 5160, loss = 13.51229960\n",
      "Iteration 5161, loss = 13.52302630\n",
      "Iteration 5162, loss = 13.22142536\n",
      "Iteration 5163, loss = 13.30568790\n",
      "Iteration 5164, loss = 13.44352955\n",
      "Iteration 5165, loss = 13.69854937\n",
      "Iteration 5166, loss = 13.67104858\n",
      "Iteration 5167, loss = 13.57468849\n",
      "Iteration 5168, loss = 13.36075064\n",
      "Iteration 5169, loss = 13.28664728\n",
      "Iteration 5170, loss = 13.55282250\n",
      "Iteration 5171, loss = 14.32794780\n",
      "Iteration 5172, loss = 14.34728602\n",
      "Iteration 5173, loss = 13.61711877\n",
      "Iteration 5174, loss = 13.85447929\n",
      "Iteration 5175, loss = 13.49605016\n",
      "Iteration 5176, loss = 13.85841817\n",
      "Iteration 5177, loss = 13.55903681\n",
      "Iteration 5178, loss = 13.29395020\n",
      "Iteration 5179, loss = 14.18153429\n",
      "Iteration 5180, loss = 14.02172224\n",
      "Iteration 5181, loss = 13.27335949\n",
      "Iteration 5182, loss = 13.33575979\n",
      "Iteration 5183, loss = 13.94372537\n",
      "Iteration 5184, loss = 14.09437031\n",
      "Iteration 5185, loss = 13.87936324\n",
      "Iteration 5186, loss = 14.35726266\n",
      "Iteration 5187, loss = 15.67828714\n",
      "Iteration 5188, loss = 16.87853296\n",
      "Iteration 5189, loss = 17.50240209\n",
      "Iteration 5190, loss = 16.47438098\n",
      "Iteration 5191, loss = 14.49393105\n",
      "Iteration 5192, loss = 15.04741221\n",
      "Iteration 5193, loss = 14.17994238\n",
      "Iteration 5194, loss = 14.33091357\n",
      "Iteration 5195, loss = 13.58635839\n",
      "Iteration 5196, loss = 13.41129140\n",
      "Iteration 5197, loss = 14.11288772\n",
      "Iteration 5198, loss = 13.93341820\n",
      "Iteration 5199, loss = 14.90892009\n",
      "Iteration 5200, loss = 14.58347920\n",
      "Iteration 5201, loss = 14.12392849\n",
      "Iteration 5202, loss = 15.12759339\n",
      "Iteration 5203, loss = 15.71340294\n",
      "Iteration 5204, loss = 15.39124067\n",
      "Iteration 5205, loss = 14.84036299\n",
      "Iteration 5206, loss = 14.00831951\n",
      "Iteration 5207, loss = 13.51886076\n",
      "Iteration 5208, loss = 13.66009108\n",
      "Iteration 5209, loss = 14.85933381\n",
      "Iteration 5210, loss = 13.49496962\n",
      "Iteration 5211, loss = 13.33676455\n",
      "Iteration 5212, loss = 13.67389185\n",
      "Iteration 5213, loss = 13.27949338\n",
      "Iteration 5214, loss = 13.73443181\n",
      "Iteration 5215, loss = 13.90926832\n",
      "Iteration 5216, loss = 13.17998213\n",
      "Iteration 5217, loss = 13.33808302\n",
      "Iteration 5218, loss = 13.24701056\n",
      "Iteration 5219, loss = 13.51735289\n",
      "Iteration 5220, loss = 13.88053271\n",
      "Iteration 5221, loss = 13.82422125\n",
      "Iteration 5222, loss = 14.01236360\n",
      "Iteration 5223, loss = 13.38926523\n",
      "Iteration 5224, loss = 13.08590888\n",
      "Iteration 5225, loss = 13.15253805\n",
      "Iteration 5226, loss = 13.29154594\n",
      "Iteration 5227, loss = 13.54459942\n",
      "Iteration 5228, loss = 13.41170452\n",
      "Iteration 5229, loss = 13.87967372\n",
      "Iteration 5230, loss = 13.68794732\n",
      "Iteration 5231, loss = 13.83448885\n",
      "Iteration 5232, loss = 14.20855946\n",
      "Iteration 5233, loss = 13.72503394\n",
      "Iteration 5234, loss = 13.34748562\n",
      "Iteration 5235, loss = 13.77185051\n",
      "Iteration 5236, loss = 13.37687412\n",
      "Iteration 5237, loss = 13.45012454\n",
      "Iteration 5238, loss = 13.98046567\n",
      "Iteration 5239, loss = 13.24223784\n",
      "Iteration 5240, loss = 13.42931900\n",
      "Iteration 5241, loss = 13.42738683\n",
      "Iteration 5242, loss = 13.34045311\n",
      "Iteration 5243, loss = 13.70073852\n",
      "Iteration 5244, loss = 13.89682997\n",
      "Iteration 5245, loss = 14.29718567\n",
      "Iteration 5246, loss = 13.66236361\n",
      "Iteration 5247, loss = 13.25278315\n",
      "Iteration 5248, loss = 13.20312309\n",
      "Iteration 5249, loss = 13.31153597\n",
      "Iteration 5250, loss = 13.01246252\n",
      "Iteration 5251, loss = 13.18131933\n",
      "Iteration 5252, loss = 13.30805011\n",
      "Iteration 5253, loss = 13.69428871\n",
      "Iteration 5254, loss = 13.98941152\n",
      "Iteration 5255, loss = 14.96375362\n",
      "Iteration 5256, loss = 14.31907610\n",
      "Iteration 5257, loss = 14.57246391\n",
      "Iteration 5258, loss = 13.85235028\n",
      "Iteration 5259, loss = 14.92987020\n",
      "Iteration 5260, loss = 13.76713243\n",
      "Iteration 5261, loss = 13.56467169\n",
      "Iteration 5262, loss = 13.12432067\n",
      "Iteration 5263, loss = 13.25956660\n",
      "Iteration 5264, loss = 13.10515234\n",
      "Iteration 5265, loss = 13.13710989\n",
      "Iteration 5266, loss = 13.97673698\n",
      "Iteration 5267, loss = 14.68622815\n",
      "Iteration 5268, loss = 13.30525840\n",
      "Iteration 5269, loss = 13.86012974\n",
      "Iteration 5270, loss = 13.29747285\n",
      "Iteration 5271, loss = 13.38263649\n",
      "Iteration 5272, loss = 14.53082556\n",
      "Iteration 5273, loss = 14.26232873\n",
      "Iteration 5274, loss = 15.08084845\n",
      "Iteration 5275, loss = 15.45322913\n",
      "Iteration 5276, loss = 15.41859851\n",
      "Iteration 5277, loss = 13.88429154\n",
      "Iteration 5278, loss = 13.66652255\n",
      "Iteration 5279, loss = 13.51992226\n",
      "Iteration 5280, loss = 13.61807794\n",
      "Iteration 5281, loss = 13.63446931\n",
      "Iteration 5282, loss = 13.23601178\n",
      "Iteration 5283, loss = 13.15661049\n",
      "Iteration 5284, loss = 13.61274923\n",
      "Iteration 5285, loss = 13.27907837\n",
      "Iteration 5286, loss = 13.30241840\n",
      "Iteration 5287, loss = 13.25626280\n",
      "Iteration 5288, loss = 13.17904333\n",
      "Iteration 5289, loss = 14.01447627\n",
      "Iteration 5290, loss = 14.92008494\n",
      "Iteration 5291, loss = 15.94473510\n",
      "Iteration 5292, loss = 15.01362357\n",
      "Iteration 5293, loss = 13.88780359\n",
      "Iteration 5294, loss = 14.22846495\n",
      "Iteration 5295, loss = 14.29835262\n",
      "Iteration 5296, loss = 13.23958891\n",
      "Iteration 5297, loss = 13.35226404\n",
      "Iteration 5298, loss = 13.43807903\n",
      "Iteration 5299, loss = 12.85964349\n",
      "Iteration 5300, loss = 13.12433438\n",
      "Iteration 5301, loss = 13.17021167\n",
      "Iteration 5302, loss = 12.96243965\n",
      "Iteration 5303, loss = 13.23496134\n",
      "Iteration 5304, loss = 13.28770526\n",
      "Iteration 5305, loss = 13.62922345\n",
      "Iteration 5306, loss = 13.39897808\n",
      "Iteration 5307, loss = 13.33828914\n",
      "Iteration 5308, loss = 13.59830703\n",
      "Iteration 5309, loss = 13.39982875\n",
      "Iteration 5310, loss = 14.31944417\n",
      "Iteration 5311, loss = 14.18831732\n",
      "Iteration 5312, loss = 14.49191758\n",
      "Iteration 5313, loss = 14.35086480\n",
      "Iteration 5314, loss = 13.64990826\n",
      "Iteration 5315, loss = 13.50902131\n",
      "Iteration 5316, loss = 14.25677880\n",
      "Iteration 5317, loss = 14.41197392\n",
      "Iteration 5318, loss = 14.41700930\n",
      "Iteration 5319, loss = 13.96776979\n",
      "Iteration 5320, loss = 12.97018729\n",
      "Iteration 5321, loss = 13.05425919\n",
      "Iteration 5322, loss = 13.18559368\n",
      "Iteration 5323, loss = 13.51394829\n",
      "Iteration 5324, loss = 14.66793847\n",
      "Iteration 5325, loss = 15.02839227\n",
      "Iteration 5326, loss = 14.79010693\n",
      "Iteration 5327, loss = 13.74684589\n",
      "Iteration 5328, loss = 16.13581716\n",
      "Iteration 5329, loss = 15.48473030\n",
      "Iteration 5330, loss = 13.89553093\n",
      "Iteration 5331, loss = 15.48699539\n",
      "Iteration 5332, loss = 15.12379713\n",
      "Iteration 5333, loss = 14.35731899\n",
      "Iteration 5334, loss = 14.88614576\n",
      "Iteration 5335, loss = 16.35985479\n",
      "Iteration 5336, loss = 14.62479317\n",
      "Iteration 5337, loss = 13.57682707\n",
      "Iteration 5338, loss = 14.09477523\n",
      "Iteration 5339, loss = 13.47982729\n",
      "Iteration 5340, loss = 14.51262701\n",
      "Iteration 5341, loss = 14.07158506\n",
      "Iteration 5342, loss = 13.89188099\n",
      "Iteration 5343, loss = 12.90298778\n",
      "Iteration 5344, loss = 12.93421075\n",
      "Iteration 5345, loss = 13.26290785\n",
      "Iteration 5346, loss = 13.36038126\n",
      "Iteration 5347, loss = 13.31954646\n",
      "Iteration 5348, loss = 13.09613849\n",
      "Iteration 5349, loss = 13.32289549\n",
      "Iteration 5350, loss = 15.01806670\n",
      "Iteration 5351, loss = 15.95234021\n",
      "Iteration 5352, loss = 13.98279171\n",
      "Iteration 5353, loss = 14.22218880\n",
      "Iteration 5354, loss = 14.69125849\n",
      "Iteration 5355, loss = 14.69655981\n",
      "Iteration 5356, loss = 14.36769273\n",
      "Iteration 5357, loss = 16.10728929\n",
      "Iteration 5358, loss = 13.49422471\n",
      "Iteration 5359, loss = 13.67027560\n",
      "Iteration 5360, loss = 13.95576885\n",
      "Iteration 5361, loss = 13.69492077\n",
      "Iteration 5362, loss = 13.99930416\n",
      "Iteration 5363, loss = 13.58457868\n",
      "Iteration 5364, loss = 13.25308408\n",
      "Iteration 5365, loss = 12.90219210\n",
      "Iteration 5366, loss = 12.94823473\n",
      "Iteration 5367, loss = 13.74790524\n",
      "Iteration 5368, loss = 13.01296912\n",
      "Iteration 5369, loss = 12.92451099\n",
      "Iteration 5370, loss = 13.19554072\n",
      "Iteration 5371, loss = 12.96751574\n",
      "Iteration 5372, loss = 13.01952812\n",
      "Iteration 5373, loss = 12.82521009\n",
      "Iteration 5374, loss = 13.02342453\n",
      "Iteration 5375, loss = 13.09365386\n",
      "Iteration 5376, loss = 13.74502538\n",
      "Iteration 5377, loss = 13.87509115\n",
      "Iteration 5378, loss = 12.99553679\n",
      "Iteration 5379, loss = 13.05715476\n",
      "Iteration 5380, loss = 13.33392744\n",
      "Iteration 5381, loss = 13.99023756\n",
      "Iteration 5382, loss = 13.98067365\n",
      "Iteration 5383, loss = 14.21721857\n",
      "Iteration 5384, loss = 13.61418764\n",
      "Iteration 5385, loss = 13.16556676\n",
      "Iteration 5386, loss = 13.54739183\n",
      "Iteration 5387, loss = 12.93919033\n",
      "Iteration 5388, loss = 13.17307704\n",
      "Iteration 5389, loss = 13.52308409\n",
      "Iteration 5390, loss = 12.96928661\n",
      "Iteration 5391, loss = 12.88110403\n",
      "Iteration 5392, loss = 13.15397057\n",
      "Iteration 5393, loss = 12.97547615\n",
      "Iteration 5394, loss = 13.29236253\n",
      "Iteration 5395, loss = 12.82407232\n",
      "Iteration 5396, loss = 12.89478597\n",
      "Iteration 5397, loss = 13.10830843\n",
      "Iteration 5398, loss = 12.94350776\n",
      "Iteration 5399, loss = 12.94008576\n",
      "Iteration 5400, loss = 12.72700595\n",
      "Iteration 5401, loss = 12.73700521\n",
      "Iteration 5402, loss = 13.06794510\n",
      "Iteration 5403, loss = 12.99187697\n",
      "Iteration 5404, loss = 13.02573444\n",
      "Iteration 5405, loss = 13.01051113\n",
      "Iteration 5406, loss = 13.77977917\n",
      "Iteration 5407, loss = 13.78424040\n",
      "Iteration 5408, loss = 13.13061064\n",
      "Iteration 5409, loss = 13.16426599\n",
      "Iteration 5410, loss = 12.94460826\n",
      "Iteration 5411, loss = 13.19118923\n",
      "Iteration 5412, loss = 13.01886450\n",
      "Iteration 5413, loss = 13.10018582\n",
      "Iteration 5414, loss = 12.94592652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5415, loss = 12.98975356\n",
      "Iteration 5416, loss = 13.11115690\n",
      "Iteration 5417, loss = 13.09502654\n",
      "Iteration 5418, loss = 13.05780406\n",
      "Iteration 5419, loss = 13.04419764\n",
      "Iteration 5420, loss = 13.20028802\n",
      "Iteration 5421, loss = 12.76307700\n",
      "Iteration 5422, loss = 13.63119923\n",
      "Iteration 5423, loss = 13.33315631\n",
      "Iteration 5424, loss = 12.88331821\n",
      "Iteration 5425, loss = 12.74380678\n",
      "Iteration 5426, loss = 13.56458531\n",
      "Iteration 5427, loss = 13.01009078\n",
      "Iteration 5428, loss = 12.88193523\n",
      "Iteration 5429, loss = 12.82265144\n",
      "Iteration 5430, loss = 13.02260417\n",
      "Iteration 5431, loss = 12.94430680\n",
      "Iteration 5432, loss = 13.33374969\n",
      "Iteration 5433, loss = 13.45560356\n",
      "Iteration 5434, loss = 13.16293642\n",
      "Iteration 5435, loss = 12.98628072\n",
      "Iteration 5436, loss = 13.36221023\n",
      "Iteration 5437, loss = 13.03705866\n",
      "Iteration 5438, loss = 12.70069583\n",
      "Iteration 5439, loss = 12.68861606\n",
      "Iteration 5440, loss = 12.73325630\n",
      "Iteration 5441, loss = 13.25995974\n",
      "Iteration 5442, loss = 13.60568203\n",
      "Iteration 5443, loss = 13.28172086\n",
      "Iteration 5444, loss = 13.64255309\n",
      "Iteration 5445, loss = 12.77191041\n",
      "Iteration 5446, loss = 13.47933241\n",
      "Iteration 5447, loss = 12.93297809\n",
      "Iteration 5448, loss = 12.88332869\n",
      "Iteration 5449, loss = 12.99050242\n",
      "Iteration 5450, loss = 13.08880281\n",
      "Iteration 5451, loss = 13.35192478\n",
      "Iteration 5452, loss = 13.43039407\n",
      "Iteration 5453, loss = 12.81832756\n",
      "Iteration 5454, loss = 12.66780279\n",
      "Iteration 5455, loss = 13.00108881\n",
      "Iteration 5456, loss = 12.81653697\n",
      "Iteration 5457, loss = 12.58730551\n",
      "Iteration 5458, loss = 13.20852851\n",
      "Iteration 5459, loss = 14.95149178\n",
      "Iteration 5460, loss = 14.06796739\n",
      "Iteration 5461, loss = 13.05003043\n",
      "Iteration 5462, loss = 12.69992803\n",
      "Iteration 5463, loss = 13.07116417\n",
      "Iteration 5464, loss = 13.57380121\n",
      "Iteration 5465, loss = 13.68466765\n",
      "Iteration 5466, loss = 13.93325830\n",
      "Iteration 5467, loss = 13.09782854\n",
      "Iteration 5468, loss = 13.18948016\n",
      "Iteration 5469, loss = 13.37196336\n",
      "Iteration 5470, loss = 13.42017240\n",
      "Iteration 5471, loss = 14.68855188\n",
      "Iteration 5472, loss = 12.92321866\n",
      "Iteration 5473, loss = 13.33448331\n",
      "Iteration 5474, loss = 13.01645088\n",
      "Iteration 5475, loss = 13.92549156\n",
      "Iteration 5476, loss = 12.88029039\n",
      "Iteration 5477, loss = 13.00028097\n",
      "Iteration 5478, loss = 13.40655437\n",
      "Iteration 5479, loss = 13.53127891\n",
      "Iteration 5480, loss = 13.20355154\n",
      "Iteration 5481, loss = 13.45162081\n",
      "Iteration 5482, loss = 13.01256326\n",
      "Iteration 5483, loss = 12.74483656\n",
      "Iteration 5484, loss = 12.57033445\n",
      "Iteration 5485, loss = 12.71301595\n",
      "Iteration 5486, loss = 12.82041684\n",
      "Iteration 5487, loss = 12.84453889\n",
      "Iteration 5488, loss = 12.53364110\n",
      "Iteration 5489, loss = 12.72782831\n",
      "Iteration 5490, loss = 12.76660565\n",
      "Iteration 5491, loss = 12.57662315\n",
      "Iteration 5492, loss = 12.73882148\n",
      "Iteration 5493, loss = 13.58279745\n",
      "Iteration 5494, loss = 13.06226119\n",
      "Iteration 5495, loss = 12.66440521\n",
      "Iteration 5496, loss = 12.94734900\n",
      "Iteration 5497, loss = 12.89709154\n",
      "Iteration 5498, loss = 13.26143233\n",
      "Iteration 5499, loss = 12.90950266\n",
      "Iteration 5500, loss = 12.84832279\n",
      "Iteration 5501, loss = 12.70686280\n",
      "Iteration 5502, loss = 12.51679231\n",
      "Iteration 5503, loss = 13.11538416\n",
      "Iteration 5504, loss = 13.13693209\n",
      "Iteration 5505, loss = 13.14133171\n",
      "Iteration 5506, loss = 12.61072943\n",
      "Iteration 5507, loss = 12.82540124\n",
      "Iteration 5508, loss = 12.77861702\n",
      "Iteration 5509, loss = 12.76028312\n",
      "Iteration 5510, loss = 13.12743575\n",
      "Iteration 5511, loss = 12.89023335\n",
      "Iteration 5512, loss = 13.02612486\n",
      "Iteration 5513, loss = 12.94904399\n",
      "Iteration 5514, loss = 13.14881548\n",
      "Iteration 5515, loss = 12.56621552\n",
      "Iteration 5516, loss = 13.13966751\n",
      "Iteration 5517, loss = 13.85057876\n",
      "Iteration 5518, loss = 13.98299223\n",
      "Iteration 5519, loss = 13.07520260\n",
      "Iteration 5520, loss = 13.93042692\n",
      "Iteration 5521, loss = 14.81388674\n",
      "Iteration 5522, loss = 14.51354046\n",
      "Iteration 5523, loss = 15.84063230\n",
      "Iteration 5524, loss = 16.36876159\n",
      "Iteration 5525, loss = 15.65953097\n",
      "Iteration 5526, loss = 16.53823321\n",
      "Iteration 5527, loss = 14.51018827\n",
      "Iteration 5528, loss = 13.48171505\n",
      "Iteration 5529, loss = 12.52239618\n",
      "Iteration 5530, loss = 12.78324800\n",
      "Iteration 5531, loss = 12.43160554\n",
      "Iteration 5532, loss = 12.47999728\n",
      "Iteration 5533, loss = 13.24820062\n",
      "Iteration 5534, loss = 13.09814886\n",
      "Iteration 5535, loss = 12.73999830\n",
      "Iteration 5536, loss = 12.74785064\n",
      "Iteration 5537, loss = 12.77657859\n",
      "Iteration 5538, loss = 12.96659922\n",
      "Iteration 5539, loss = 13.01972982\n",
      "Iteration 5540, loss = 13.52271088\n",
      "Iteration 5541, loss = 14.99145114\n",
      "Iteration 5542, loss = 15.72767801\n",
      "Iteration 5543, loss = 14.40496254\n",
      "Iteration 5544, loss = 14.75859910\n",
      "Iteration 5545, loss = 14.30131916\n",
      "Iteration 5546, loss = 13.99814185\n",
      "Iteration 5547, loss = 12.80616065\n",
      "Iteration 5548, loss = 13.02242675\n",
      "Iteration 5549, loss = 12.88338059\n",
      "Iteration 5550, loss = 12.58228734\n",
      "Iteration 5551, loss = 12.73200406\n",
      "Iteration 5552, loss = 12.67151524\n",
      "Iteration 5553, loss = 12.97751048\n",
      "Iteration 5554, loss = 12.85595081\n",
      "Iteration 5555, loss = 12.56929463\n",
      "Iteration 5556, loss = 12.75946734\n",
      "Iteration 5557, loss = 12.61135662\n",
      "Iteration 5558, loss = 12.86948126\n",
      "Iteration 5559, loss = 13.60057233\n",
      "Iteration 5560, loss = 13.84588824\n",
      "Iteration 5561, loss = 13.36867978\n",
      "Iteration 5562, loss = 13.53424160\n",
      "Iteration 5563, loss = 12.93795603\n",
      "Iteration 5564, loss = 12.73622176\n",
      "Iteration 5565, loss = 13.58378358\n",
      "Iteration 5566, loss = 12.80824662\n",
      "Iteration 5567, loss = 12.66025413\n",
      "Iteration 5568, loss = 12.65840923\n",
      "Iteration 5569, loss = 12.51598165\n",
      "Iteration 5570, loss = 12.96260802\n",
      "Iteration 5571, loss = 13.65724401\n",
      "Iteration 5572, loss = 13.39351914\n",
      "Iteration 5573, loss = 13.52788300\n",
      "Iteration 5574, loss = 14.93219748\n",
      "Iteration 5575, loss = 14.61274214\n",
      "Iteration 5576, loss = 13.91740980\n",
      "Iteration 5577, loss = 12.90797306\n",
      "Iteration 5578, loss = 13.10463625\n",
      "Iteration 5579, loss = 12.50735980\n",
      "Iteration 5580, loss = 12.38635728\n",
      "Iteration 5581, loss = 12.59480031\n",
      "Iteration 5582, loss = 12.48462297\n",
      "Iteration 5583, loss = 12.67807013\n",
      "Iteration 5584, loss = 13.36425380\n",
      "Iteration 5585, loss = 12.78413542\n",
      "Iteration 5586, loss = 12.60155084\n",
      "Iteration 5587, loss = 12.80470317\n",
      "Iteration 5588, loss = 12.38801109\n",
      "Iteration 5589, loss = 12.49639360\n",
      "Iteration 5590, loss = 12.53398603\n",
      "Iteration 5591, loss = 13.12801564\n",
      "Iteration 5592, loss = 13.17975174\n",
      "Iteration 5593, loss = 12.80394271\n",
      "Iteration 5594, loss = 12.90123776\n",
      "Iteration 5595, loss = 13.23121990\n",
      "Iteration 5596, loss = 13.37589803\n",
      "Iteration 5597, loss = 13.10412449\n",
      "Iteration 5598, loss = 12.67237647\n",
      "Iteration 5599, loss = 12.83529853\n",
      "Iteration 5600, loss = 12.81304214\n",
      "Iteration 5601, loss = 14.59711005\n",
      "Iteration 5602, loss = 14.18929416\n",
      "Iteration 5603, loss = 13.88259992\n",
      "Iteration 5604, loss = 13.40032282\n",
      "Iteration 5605, loss = 13.71237274\n",
      "Iteration 5606, loss = 13.21357518\n",
      "Iteration 5607, loss = 12.68260362\n",
      "Iteration 5608, loss = 12.70239758\n",
      "Iteration 5609, loss = 13.04118279\n",
      "Iteration 5610, loss = 12.74464451\n",
      "Iteration 5611, loss = 12.63052186\n",
      "Iteration 5612, loss = 13.00743471\n",
      "Iteration 5613, loss = 12.50787715\n",
      "Iteration 5614, loss = 12.51440326\n",
      "Iteration 5615, loss = 12.53177379\n",
      "Iteration 5616, loss = 12.63520515\n",
      "Iteration 5617, loss = 12.66813659\n",
      "Iteration 5618, loss = 12.30139740\n",
      "Iteration 5619, loss = 12.54672146\n",
      "Iteration 5620, loss = 12.84957973\n",
      "Iteration 5621, loss = 13.45574645\n",
      "Iteration 5622, loss = 14.52259732\n",
      "Iteration 5623, loss = 13.90177418\n",
      "Iteration 5624, loss = 13.36411859\n",
      "Iteration 5625, loss = 12.59812006\n",
      "Iteration 5626, loss = 13.21635872\n",
      "Iteration 5627, loss = 14.09702777\n",
      "Iteration 5628, loss = 12.93089461\n",
      "Iteration 5629, loss = 13.45601164\n",
      "Iteration 5630, loss = 13.47020351\n",
      "Iteration 5631, loss = 13.55148244\n",
      "Iteration 5632, loss = 13.56124417\n",
      "Iteration 5633, loss = 14.94849018\n",
      "Iteration 5634, loss = 13.53241074\n",
      "Iteration 5635, loss = 14.92337908\n",
      "Iteration 5636, loss = 14.49824076\n",
      "Iteration 5637, loss = 13.97954822\n",
      "Iteration 5638, loss = 13.39599913\n",
      "Iteration 5639, loss = 13.14111928\n",
      "Iteration 5640, loss = 13.19275993\n",
      "Iteration 5641, loss = 12.30249771\n",
      "Iteration 5642, loss = 12.61999615\n",
      "Iteration 5643, loss = 13.00463379\n",
      "Iteration 5644, loss = 12.94559718\n",
      "Iteration 5645, loss = 12.35788908\n",
      "Iteration 5646, loss = 13.00597082\n",
      "Iteration 5647, loss = 12.47826891\n",
      "Iteration 5648, loss = 12.51385483\n",
      "Iteration 5649, loss = 12.61915598\n",
      "Iteration 5650, loss = 12.42703841\n",
      "Iteration 5651, loss = 12.60973214\n",
      "Iteration 5652, loss = 12.77453658\n",
      "Iteration 5653, loss = 13.39269501\n",
      "Iteration 5654, loss = 13.26221632\n",
      "Iteration 5655, loss = 12.75567978\n",
      "Iteration 5656, loss = 13.20170835\n",
      "Iteration 5657, loss = 12.47045138\n",
      "Iteration 5658, loss = 13.28164153\n",
      "Iteration 5659, loss = 13.47876394\n",
      "Iteration 5660, loss = 12.91868236\n",
      "Iteration 5661, loss = 13.20720345\n",
      "Iteration 5662, loss = 14.00597049\n",
      "Iteration 5663, loss = 12.64450920\n",
      "Iteration 5664, loss = 12.38543516\n",
      "Iteration 5665, loss = 12.25944623\n",
      "Iteration 5666, loss = 12.47498642\n",
      "Iteration 5667, loss = 12.24610198\n",
      "Iteration 5668, loss = 12.39469092\n",
      "Iteration 5669, loss = 12.52017162\n",
      "Iteration 5670, loss = 13.09797891\n",
      "Iteration 5671, loss = 13.23657704\n",
      "Iteration 5672, loss = 12.42016899\n",
      "Iteration 5673, loss = 12.34731791\n",
      "Iteration 5674, loss = 12.42479154\n",
      "Iteration 5675, loss = 12.34039190\n",
      "Iteration 5676, loss = 12.39280996\n",
      "Iteration 5677, loss = 12.51751334\n",
      "Iteration 5678, loss = 13.00001917\n",
      "Iteration 5679, loss = 13.80305891\n",
      "Iteration 5680, loss = 14.73357146\n",
      "Iteration 5681, loss = 14.45175202\n",
      "Iteration 5682, loss = 13.51531497\n",
      "Iteration 5683, loss = 13.29343794\n",
      "Iteration 5684, loss = 13.45834222\n",
      "Iteration 5685, loss = 13.16684494\n",
      "Iteration 5686, loss = 12.64435839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5687, loss = 12.47612217\n",
      "Iteration 5688, loss = 13.42542934\n",
      "Iteration 5689, loss = 12.89018734\n",
      "Iteration 5690, loss = 12.84111060\n",
      "Iteration 5691, loss = 13.09804359\n",
      "Iteration 5692, loss = 12.99268816\n",
      "Iteration 5693, loss = 14.46355599\n",
      "Iteration 5694, loss = 14.15326845\n",
      "Iteration 5695, loss = 14.91507485\n",
      "Iteration 5696, loss = 13.74060657\n",
      "Iteration 5697, loss = 14.59465346\n",
      "Iteration 5698, loss = 14.15436956\n",
      "Iteration 5699, loss = 13.88550933\n",
      "Iteration 5700, loss = 13.02890831\n",
      "Iteration 5701, loss = 12.51921733\n",
      "Iteration 5702, loss = 12.18916585\n",
      "Iteration 5703, loss = 12.22786501\n",
      "Iteration 5704, loss = 12.37129100\n",
      "Iteration 5705, loss = 12.26450782\n",
      "Iteration 5706, loss = 13.01360830\n",
      "Iteration 5707, loss = 14.48324540\n",
      "Iteration 5708, loss = 13.96270903\n",
      "Iteration 5709, loss = 13.61387784\n",
      "Iteration 5710, loss = 13.35032001\n",
      "Iteration 5711, loss = 13.07837599\n",
      "Iteration 5712, loss = 14.50866911\n",
      "Iteration 5713, loss = 14.04291438\n",
      "Iteration 5714, loss = 13.44073181\n",
      "Iteration 5715, loss = 12.68976039\n",
      "Iteration 5716, loss = 13.54657364\n",
      "Iteration 5717, loss = 13.23901988\n",
      "Iteration 5718, loss = 13.65900577\n",
      "Iteration 5719, loss = 14.13805798\n",
      "Iteration 5720, loss = 13.90410789\n",
      "Iteration 5721, loss = 14.33441740\n",
      "Iteration 5722, loss = 13.06737648\n",
      "Iteration 5723, loss = 12.74957796\n",
      "Iteration 5724, loss = 12.67580826\n",
      "Iteration 5725, loss = 12.47347826\n",
      "Iteration 5726, loss = 12.84953235\n",
      "Iteration 5727, loss = 14.08575041\n",
      "Iteration 5728, loss = 13.44525521\n",
      "Iteration 5729, loss = 13.28763757\n",
      "Iteration 5730, loss = 12.83253049\n",
      "Iteration 5731, loss = 12.45897260\n",
      "Iteration 5732, loss = 12.97015377\n",
      "Iteration 5733, loss = 13.26854502\n",
      "Iteration 5734, loss = 12.70721555\n",
      "Iteration 5735, loss = 12.99040670\n",
      "Iteration 5736, loss = 12.51976724\n",
      "Iteration 5737, loss = 12.43657246\n",
      "Iteration 5738, loss = 12.70727725\n",
      "Iteration 5739, loss = 12.28609268\n",
      "Iteration 5740, loss = 12.15972299\n",
      "Iteration 5741, loss = 12.13561116\n",
      "Iteration 5742, loss = 12.52610276\n",
      "Iteration 5743, loss = 12.80014474\n",
      "Iteration 5744, loss = 13.09433838\n",
      "Iteration 5745, loss = 12.96373929\n",
      "Iteration 5746, loss = 12.44481821\n",
      "Iteration 5747, loss = 12.20302478\n",
      "Iteration 5748, loss = 12.51621012\n",
      "Iteration 5749, loss = 12.55286351\n",
      "Iteration 5750, loss = 12.40452095\n",
      "Iteration 5751, loss = 12.66022753\n",
      "Iteration 5752, loss = 13.76453058\n",
      "Iteration 5753, loss = 13.33719962\n",
      "Iteration 5754, loss = 12.93063475\n",
      "Iteration 5755, loss = 12.72746880\n",
      "Iteration 5756, loss = 14.13368529\n",
      "Iteration 5757, loss = 14.59766902\n",
      "Iteration 5758, loss = 15.18221816\n",
      "Iteration 5759, loss = 14.25439093\n",
      "Iteration 5760, loss = 13.27382397\n",
      "Iteration 5761, loss = 13.66537416\n",
      "Iteration 5762, loss = 13.16092927\n",
      "Iteration 5763, loss = 12.34071288\n",
      "Iteration 5764, loss = 12.33854649\n",
      "Iteration 5765, loss = 12.25957121\n",
      "Iteration 5766, loss = 12.42159967\n",
      "Iteration 5767, loss = 12.41541385\n",
      "Iteration 5768, loss = 12.34680602\n",
      "Iteration 5769, loss = 12.44398655\n",
      "Iteration 5770, loss = 12.50342334\n",
      "Iteration 5771, loss = 12.50654742\n",
      "Iteration 5772, loss = 12.63100793\n",
      "Iteration 5773, loss = 12.12991692\n",
      "Iteration 5774, loss = 12.30371684\n",
      "Iteration 5775, loss = 12.85640268\n",
      "Iteration 5776, loss = 13.37328256\n",
      "Iteration 5777, loss = 13.39432095\n",
      "Iteration 5778, loss = 13.78114895\n",
      "Iteration 5779, loss = 12.50400822\n",
      "Iteration 5780, loss = 12.67182164\n",
      "Iteration 5781, loss = 13.27976195\n",
      "Iteration 5782, loss = 13.89583841\n",
      "Iteration 5783, loss = 13.65716941\n",
      "Iteration 5784, loss = 14.20588136\n",
      "Iteration 5785, loss = 14.80437510\n",
      "Iteration 5786, loss = 15.15629461\n",
      "Iteration 5787, loss = 14.98474547\n",
      "Iteration 5788, loss = 15.23099470\n",
      "Iteration 5789, loss = 14.39075738\n",
      "Iteration 5790, loss = 13.83028243\n",
      "Iteration 5791, loss = 13.48067863\n",
      "Iteration 5792, loss = 14.00755084\n",
      "Iteration 5793, loss = 12.92371252\n",
      "Iteration 5794, loss = 13.13331356\n",
      "Iteration 5795, loss = 13.07344331\n",
      "Iteration 5796, loss = 12.70063290\n",
      "Iteration 5797, loss = 13.17424311\n",
      "Iteration 5798, loss = 13.38270844\n",
      "Iteration 5799, loss = 15.81544404\n",
      "Iteration 5800, loss = 14.48370258\n",
      "Iteration 5801, loss = 13.53750575\n",
      "Iteration 5802, loss = 13.86259145\n",
      "Iteration 5803, loss = 14.03568202\n",
      "Iteration 5804, loss = 13.37235824\n",
      "Iteration 5805, loss = 13.51704336\n",
      "Iteration 5806, loss = 13.06074591\n",
      "Iteration 5807, loss = 13.19541368\n",
      "Iteration 5808, loss = 13.91357151\n",
      "Iteration 5809, loss = 13.12175101\n",
      "Iteration 5810, loss = 13.05728002\n",
      "Iteration 5811, loss = 12.58573797\n",
      "Iteration 5812, loss = 12.76662598\n",
      "Iteration 5813, loss = 12.37944947\n",
      "Iteration 5814, loss = 12.21100816\n",
      "Iteration 5815, loss = 12.26327621\n",
      "Iteration 5816, loss = 12.76286378\n",
      "Iteration 5817, loss = 12.53660832\n",
      "Iteration 5818, loss = 12.43351243\n",
      "Iteration 5819, loss = 12.23108869\n",
      "Iteration 5820, loss = 12.01248072\n",
      "Iteration 5821, loss = 13.12477994\n",
      "Iteration 5822, loss = 12.52986397\n",
      "Iteration 5823, loss = 12.73410871\n",
      "Iteration 5824, loss = 12.29796527\n",
      "Iteration 5825, loss = 12.46555814\n",
      "Iteration 5826, loss = 13.53710675\n",
      "Iteration 5827, loss = 13.62791760\n",
      "Iteration 5828, loss = 13.33099562\n",
      "Iteration 5829, loss = 13.91308275\n",
      "Iteration 5830, loss = 13.61577826\n",
      "Iteration 5831, loss = 12.96930476\n",
      "Iteration 5832, loss = 12.63038739\n",
      "Iteration 5833, loss = 12.46184804\n",
      "Iteration 5834, loss = 12.10661122\n",
      "Iteration 5835, loss = 12.24640609\n",
      "Iteration 5836, loss = 12.14127122\n",
      "Iteration 5837, loss = 12.18442654\n",
      "Iteration 5838, loss = 12.30987591\n",
      "Iteration 5839, loss = 12.16265799\n",
      "Iteration 5840, loss = 12.50654830\n",
      "Iteration 5841, loss = 12.53063329\n",
      "Iteration 5842, loss = 13.16419440\n",
      "Iteration 5843, loss = 14.16781306\n",
      "Iteration 5844, loss = 13.13490291\n",
      "Iteration 5845, loss = 13.20809622\n",
      "Iteration 5846, loss = 13.57017032\n",
      "Iteration 5847, loss = 13.42489333\n",
      "Iteration 5848, loss = 12.51813321\n",
      "Iteration 5849, loss = 12.36908330\n",
      "Iteration 5850, loss = 12.56453576\n",
      "Iteration 5851, loss = 14.65945956\n",
      "Iteration 5852, loss = 13.84854630\n",
      "Iteration 5853, loss = 14.07491094\n",
      "Iteration 5854, loss = 13.47221892\n",
      "Iteration 5855, loss = 13.23892839\n",
      "Iteration 5856, loss = 13.13131840\n",
      "Iteration 5857, loss = 14.63980135\n",
      "Iteration 5858, loss = 13.97744601\n",
      "Iteration 5859, loss = 12.80732298\n",
      "Iteration 5860, loss = 12.09932729\n",
      "Iteration 5861, loss = 12.11076438\n",
      "Iteration 5862, loss = 12.17102467\n",
      "Iteration 5863, loss = 12.01949272\n",
      "Iteration 5864, loss = 12.30434827\n",
      "Iteration 5865, loss = 12.20592895\n",
      "Iteration 5866, loss = 12.18101577\n",
      "Iteration 5867, loss = 12.30204324\n",
      "Iteration 5868, loss = 12.28325355\n",
      "Iteration 5869, loss = 12.67118235\n",
      "Iteration 5870, loss = 12.43351263\n",
      "Iteration 5871, loss = 12.16019409\n",
      "Iteration 5872, loss = 12.21531983\n",
      "Iteration 5873, loss = 12.55331486\n",
      "Iteration 5874, loss = 12.60020871\n",
      "Iteration 5875, loss = 13.39585091\n",
      "Iteration 5876, loss = 13.73346305\n",
      "Iteration 5877, loss = 14.33066941\n",
      "Iteration 5878, loss = 13.25338307\n",
      "Iteration 5879, loss = 12.38521250\n",
      "Iteration 5880, loss = 12.75036151\n",
      "Iteration 5881, loss = 12.75727840\n",
      "Iteration 5882, loss = 12.81060154\n",
      "Iteration 5883, loss = 13.32668477\n",
      "Iteration 5884, loss = 13.45313626\n",
      "Iteration 5885, loss = 12.48742528\n",
      "Iteration 5886, loss = 13.21170796\n",
      "Iteration 5887, loss = 12.82393518\n",
      "Iteration 5888, loss = 12.60578238\n",
      "Iteration 5889, loss = 12.40763073\n",
      "Iteration 5890, loss = 12.32241886\n",
      "Iteration 5891, loss = 12.22407620\n",
      "Iteration 5892, loss = 12.25915592\n",
      "Iteration 5893, loss = 12.41120026\n",
      "Iteration 5894, loss = 12.40834536\n",
      "Iteration 5895, loss = 12.31961715\n",
      "Iteration 5896, loss = 12.51471860\n",
      "Iteration 5897, loss = 12.60472002\n",
      "Iteration 5898, loss = 13.14449206\n",
      "Iteration 5899, loss = 12.73113041\n",
      "Iteration 5900, loss = 12.62228580\n",
      "Iteration 5901, loss = 12.46360777\n",
      "Iteration 5902, loss = 12.02780993\n",
      "Iteration 5903, loss = 12.97076743\n",
      "Iteration 5904, loss = 13.21981866\n",
      "Iteration 5905, loss = 12.42632247\n",
      "Iteration 5906, loss = 12.45299805\n",
      "Iteration 5907, loss = 12.60251693\n",
      "Iteration 5908, loss = 13.10904450\n",
      "Iteration 5909, loss = 12.87223091\n",
      "Iteration 5910, loss = 12.81566401\n",
      "Iteration 5911, loss = 13.86525335\n",
      "Iteration 5912, loss = 13.13429950\n",
      "Iteration 5913, loss = 12.76829171\n",
      "Iteration 5914, loss = 12.18703962\n",
      "Iteration 5915, loss = 12.02504664\n",
      "Iteration 5916, loss = 12.01183906\n",
      "Iteration 5917, loss = 12.04207409\n",
      "Iteration 5918, loss = 11.96052911\n",
      "Iteration 5919, loss = 12.03802974\n",
      "Iteration 5920, loss = 12.66744075\n",
      "Iteration 5921, loss = 12.48446296\n",
      "Iteration 5922, loss = 13.88537108\n",
      "Iteration 5923, loss = 12.84324785\n",
      "Iteration 5924, loss = 12.00164928\n",
      "Iteration 5925, loss = 11.92894997\n",
      "Iteration 5926, loss = 13.11460744\n",
      "Iteration 5927, loss = 13.03971669\n",
      "Iteration 5928, loss = 12.64582025\n",
      "Iteration 5929, loss = 11.96967299\n",
      "Iteration 5930, loss = 12.06356325\n",
      "Iteration 5931, loss = 12.11805075\n",
      "Iteration 5932, loss = 12.73314740\n",
      "Iteration 5933, loss = 12.29901125\n",
      "Iteration 5934, loss = 12.63077570\n",
      "Iteration 5935, loss = 11.79432786\n",
      "Iteration 5936, loss = 11.89146052\n",
      "Iteration 5937, loss = 12.57664321\n",
      "Iteration 5938, loss = 12.96522804\n",
      "Iteration 5939, loss = 12.64131109\n",
      "Iteration 5940, loss = 12.19359298\n",
      "Iteration 5941, loss = 12.42504684\n",
      "Iteration 5942, loss = 12.15679461\n",
      "Iteration 5943, loss = 12.43882677\n",
      "Iteration 5944, loss = 12.61309386\n",
      "Iteration 5945, loss = 12.06496235\n",
      "Iteration 5946, loss = 11.93546942\n",
      "Iteration 5947, loss = 12.02816764\n",
      "Iteration 5948, loss = 12.00500878\n",
      "Iteration 5949, loss = 12.49766765\n",
      "Iteration 5950, loss = 12.09235453\n",
      "Iteration 5951, loss = 12.20158890\n",
      "Iteration 5952, loss = 12.48487694\n",
      "Iteration 5953, loss = 12.79741408\n",
      "Iteration 5954, loss = 12.31572653\n",
      "Iteration 5955, loss = 12.31990361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5956, loss = 12.14764829\n",
      "Iteration 5957, loss = 12.19548570\n",
      "Iteration 5958, loss = 12.08577925\n",
      "Iteration 5959, loss = 12.24860352\n",
      "Iteration 5960, loss = 12.71750083\n",
      "Iteration 5961, loss = 13.35556512\n",
      "Iteration 5962, loss = 12.02109373\n",
      "Iteration 5963, loss = 12.52648180\n",
      "Iteration 5964, loss = 12.07170086\n",
      "Iteration 5965, loss = 12.32163154\n",
      "Iteration 5966, loss = 11.85835432\n",
      "Iteration 5967, loss = 11.98203910\n",
      "Iteration 5968, loss = 11.99155263\n",
      "Iteration 5969, loss = 11.92360652\n",
      "Iteration 5970, loss = 11.87225724\n",
      "Iteration 5971, loss = 11.89478413\n",
      "Iteration 5972, loss = 12.15133061\n",
      "Iteration 5973, loss = 12.07205630\n",
      "Iteration 5974, loss = 12.37095882\n",
      "Iteration 5975, loss = 12.04384482\n",
      "Iteration 5976, loss = 12.29409641\n",
      "Iteration 5977, loss = 12.10357452\n",
      "Iteration 5978, loss = 11.84446400\n",
      "Iteration 5979, loss = 12.41144307\n",
      "Iteration 5980, loss = 12.22305982\n",
      "Iteration 5981, loss = 12.16819060\n",
      "Iteration 5982, loss = 12.60224634\n",
      "Iteration 5983, loss = 12.52592316\n",
      "Iteration 5984, loss = 11.97355219\n",
      "Iteration 5985, loss = 11.87374354\n",
      "Iteration 5986, loss = 12.50153294\n",
      "Iteration 5987, loss = 12.41028208\n",
      "Iteration 5988, loss = 12.51672859\n",
      "Iteration 5989, loss = 12.50019160\n",
      "Iteration 5990, loss = 12.44356075\n",
      "Iteration 5991, loss = 13.60898903\n",
      "Iteration 5992, loss = 13.31028256\n",
      "Iteration 5993, loss = 13.42966598\n",
      "Iteration 5994, loss = 12.38166185\n",
      "Iteration 5995, loss = 12.10228570\n",
      "Iteration 5996, loss = 12.76321274\n",
      "Iteration 5997, loss = 11.82893229\n",
      "Iteration 5998, loss = 12.70163668\n",
      "Iteration 5999, loss = 13.19042210\n",
      "Iteration 6000, loss = 13.07470033\n",
      "Iteration 6001, loss = 11.93287004\n",
      "Iteration 6002, loss = 12.05519601\n",
      "Iteration 6003, loss = 12.52316205\n",
      "Iteration 6004, loss = 13.41163572\n",
      "Iteration 6005, loss = 13.75940490\n",
      "Iteration 6006, loss = 14.74594734\n",
      "Iteration 6007, loss = 12.11439866\n",
      "Iteration 6008, loss = 11.93663686\n",
      "Iteration 6009, loss = 12.00785270\n",
      "Iteration 6010, loss = 11.75962292\n",
      "Iteration 6011, loss = 12.47705239\n",
      "Iteration 6012, loss = 12.58896938\n",
      "Iteration 6013, loss = 13.65023143\n",
      "Iteration 6014, loss = 12.83578184\n",
      "Iteration 6015, loss = 12.46152253\n",
      "Iteration 6016, loss = 12.30053148\n",
      "Iteration 6017, loss = 12.10953604\n",
      "Iteration 6018, loss = 12.21421880\n",
      "Iteration 6019, loss = 12.24281718\n",
      "Iteration 6020, loss = 12.26829663\n",
      "Iteration 6021, loss = 12.28717161\n",
      "Iteration 6022, loss = 12.15735760\n",
      "Iteration 6023, loss = 12.55878627\n",
      "Iteration 6024, loss = 13.15902926\n",
      "Iteration 6025, loss = 12.36037122\n",
      "Iteration 6026, loss = 13.39774404\n",
      "Iteration 6027, loss = 12.86027994\n",
      "Iteration 6028, loss = 14.33553220\n",
      "Iteration 6029, loss = 14.69675993\n",
      "Iteration 6030, loss = 14.22345531\n",
      "Iteration 6031, loss = 12.60578465\n",
      "Iteration 6032, loss = 13.24640377\n",
      "Iteration 6033, loss = 13.51064184\n",
      "Iteration 6034, loss = 12.91438127\n",
      "Iteration 6035, loss = 12.15251023\n",
      "Iteration 6036, loss = 13.00397825\n",
      "Iteration 6037, loss = 12.85881787\n",
      "Iteration 6038, loss = 12.55941239\n",
      "Iteration 6039, loss = 12.09522963\n",
      "Iteration 6040, loss = 11.72519586\n",
      "Iteration 6041, loss = 11.81271659\n",
      "Iteration 6042, loss = 11.82816461\n",
      "Iteration 6043, loss = 11.97274203\n",
      "Iteration 6044, loss = 11.88954750\n",
      "Iteration 6045, loss = 12.51298366\n",
      "Iteration 6046, loss = 12.35256146\n",
      "Iteration 6047, loss = 12.65942035\n",
      "Iteration 6048, loss = 12.34722315\n",
      "Iteration 6049, loss = 12.81638933\n",
      "Iteration 6050, loss = 11.79071433\n",
      "Iteration 6051, loss = 11.87197785\n",
      "Iteration 6052, loss = 12.19339345\n",
      "Iteration 6053, loss = 11.82186163\n",
      "Iteration 6054, loss = 11.77367280\n",
      "Iteration 6055, loss = 12.40451966\n",
      "Iteration 6056, loss = 11.95213149\n",
      "Iteration 6057, loss = 11.89448197\n",
      "Iteration 6058, loss = 12.34507333\n",
      "Iteration 6059, loss = 12.00735677\n",
      "Iteration 6060, loss = 12.24982331\n",
      "Iteration 6061, loss = 11.96140006\n",
      "Iteration 6062, loss = 13.83209266\n",
      "Iteration 6063, loss = 12.28187263\n",
      "Iteration 6064, loss = 12.56933304\n",
      "Iteration 6065, loss = 12.31249289\n",
      "Iteration 6066, loss = 12.07682081\n",
      "Iteration 6067, loss = 11.80170018\n",
      "Iteration 6068, loss = 11.71080551\n",
      "Iteration 6069, loss = 11.91599455\n",
      "Iteration 6070, loss = 11.96801295\n",
      "Iteration 6071, loss = 12.05084877\n",
      "Iteration 6072, loss = 11.93049406\n",
      "Iteration 6073, loss = 12.21416339\n",
      "Iteration 6074, loss = 12.63781983\n",
      "Iteration 6075, loss = 11.98810506\n",
      "Iteration 6076, loss = 11.96849530\n",
      "Iteration 6077, loss = 11.86123698\n",
      "Iteration 6078, loss = 11.88076731\n",
      "Iteration 6079, loss = 11.94468977\n",
      "Iteration 6080, loss = 12.16969701\n",
      "Iteration 6081, loss = 12.47304844\n",
      "Iteration 6082, loss = 11.88723538\n",
      "Iteration 6083, loss = 11.87895032\n",
      "Iteration 6084, loss = 12.07441795\n",
      "Iteration 6085, loss = 12.07486497\n",
      "Iteration 6086, loss = 11.98357336\n",
      "Iteration 6087, loss = 12.42078830\n",
      "Iteration 6088, loss = 13.28386324\n",
      "Iteration 6089, loss = 13.60052766\n",
      "Iteration 6090, loss = 13.92531433\n",
      "Iteration 6091, loss = 13.03798960\n",
      "Iteration 6092, loss = 12.66394068\n",
      "Iteration 6093, loss = 12.57522036\n",
      "Iteration 6094, loss = 12.65942877\n",
      "Iteration 6095, loss = 11.75903574\n",
      "Iteration 6096, loss = 11.96305256\n",
      "Iteration 6097, loss = 11.74372804\n",
      "Iteration 6098, loss = 12.14817063\n",
      "Iteration 6099, loss = 11.75552858\n",
      "Iteration 6100, loss = 12.17250016\n",
      "Iteration 6101, loss = 12.86354310\n",
      "Iteration 6102, loss = 12.89992857\n",
      "Iteration 6103, loss = 12.60230738\n",
      "Iteration 6104, loss = 13.43294028\n",
      "Iteration 6105, loss = 12.62851826\n",
      "Iteration 6106, loss = 13.51031334\n",
      "Iteration 6107, loss = 15.22845415\n",
      "Iteration 6108, loss = 12.65234134\n",
      "Iteration 6109, loss = 12.27194934\n",
      "Iteration 6110, loss = 12.01667180\n",
      "Iteration 6111, loss = 12.07441005\n",
      "Iteration 6112, loss = 11.94445532\n",
      "Iteration 6113, loss = 12.89922609\n",
      "Iteration 6114, loss = 11.93173948\n",
      "Iteration 6115, loss = 11.82046840\n",
      "Iteration 6116, loss = 11.81172096\n",
      "Iteration 6117, loss = 12.23173061\n",
      "Iteration 6118, loss = 12.26128819\n",
      "Iteration 6119, loss = 12.18230679\n",
      "Iteration 6120, loss = 12.16947561\n",
      "Iteration 6121, loss = 11.97539894\n",
      "Iteration 6122, loss = 11.97624421\n",
      "Iteration 6123, loss = 12.41462610\n",
      "Iteration 6124, loss = 13.30333290\n",
      "Iteration 6125, loss = 13.86080814\n",
      "Iteration 6126, loss = 13.35218134\n",
      "Iteration 6127, loss = 12.90837632\n",
      "Iteration 6128, loss = 12.87557175\n",
      "Iteration 6129, loss = 12.07112250\n",
      "Iteration 6130, loss = 12.20544504\n",
      "Iteration 6131, loss = 11.84184163\n",
      "Iteration 6132, loss = 11.96006790\n",
      "Iteration 6133, loss = 11.78553104\n",
      "Iteration 6134, loss = 11.88276557\n",
      "Iteration 6135, loss = 11.68625405\n",
      "Iteration 6136, loss = 11.79200416\n",
      "Iteration 6137, loss = 12.51524886\n",
      "Iteration 6138, loss = 12.38929807\n",
      "Iteration 6139, loss = 11.74495530\n",
      "Iteration 6140, loss = 12.02494917\n",
      "Iteration 6141, loss = 12.22866529\n",
      "Iteration 6142, loss = 12.72765259\n",
      "Iteration 6143, loss = 12.20749950\n",
      "Iteration 6144, loss = 12.65160347\n",
      "Iteration 6145, loss = 12.57348425\n",
      "Iteration 6146, loss = 12.65762780\n",
      "Iteration 6147, loss = 13.00048290\n",
      "Iteration 6148, loss = 12.87076791\n",
      "Iteration 6149, loss = 12.38504986\n",
      "Iteration 6150, loss = 11.81242893\n",
      "Iteration 6151, loss = 12.09868454\n",
      "Iteration 6152, loss = 12.00573289\n",
      "Iteration 6153, loss = 12.12585356\n",
      "Iteration 6154, loss = 12.04319299\n",
      "Iteration 6155, loss = 12.45027016\n",
      "Iteration 6156, loss = 12.15600284\n",
      "Iteration 6157, loss = 12.63115436\n",
      "Iteration 6158, loss = 12.08818996\n",
      "Iteration 6159, loss = 12.41922467\n",
      "Iteration 6160, loss = 12.64804576\n",
      "Iteration 6161, loss = 11.96374296\n",
      "Iteration 6162, loss = 11.61766208\n",
      "Iteration 6163, loss = 12.13213432\n",
      "Iteration 6164, loss = 12.05195111\n",
      "Iteration 6165, loss = 12.00678640\n",
      "Iteration 6166, loss = 11.85688754\n",
      "Iteration 6167, loss = 11.54376740\n",
      "Iteration 6168, loss = 11.99741247\n",
      "Iteration 6169, loss = 12.48880575\n",
      "Iteration 6170, loss = 12.98605708\n",
      "Iteration 6171, loss = 13.93605541\n",
      "Iteration 6172, loss = 14.97572611\n",
      "Iteration 6173, loss = 16.36009759\n",
      "Iteration 6174, loss = 16.14910097\n",
      "Iteration 6175, loss = 16.12965215\n",
      "Iteration 6176, loss = 14.75980353\n",
      "Iteration 6177, loss = 13.84984389\n",
      "Iteration 6178, loss = 13.18296075\n",
      "Iteration 6179, loss = 12.15722303\n",
      "Iteration 6180, loss = 12.18516631\n",
      "Iteration 6181, loss = 12.60521855\n",
      "Iteration 6182, loss = 12.16155546\n",
      "Iteration 6183, loss = 12.36100948\n",
      "Iteration 6184, loss = 12.31561791\n",
      "Iteration 6185, loss = 11.87352790\n",
      "Iteration 6186, loss = 12.38801751\n",
      "Iteration 6187, loss = 12.38954619\n",
      "Iteration 6188, loss = 12.44250354\n",
      "Iteration 6189, loss = 11.99652592\n",
      "Iteration 6190, loss = 11.96817494\n",
      "Iteration 6191, loss = 12.19077470\n",
      "Iteration 6192, loss = 11.67886651\n",
      "Iteration 6193, loss = 11.75911789\n",
      "Iteration 6194, loss = 11.84431340\n",
      "Iteration 6195, loss = 12.02965080\n",
      "Iteration 6196, loss = 11.76619243\n",
      "Iteration 6197, loss = 11.93011711\n",
      "Iteration 6198, loss = 11.92834154\n",
      "Iteration 6199, loss = 11.62543341\n",
      "Iteration 6200, loss = 11.91216931\n",
      "Iteration 6201, loss = 11.79433957\n",
      "Iteration 6202, loss = 11.90508182\n",
      "Iteration 6203, loss = 12.03056684\n",
      "Iteration 6204, loss = 12.11793568\n",
      "Iteration 6205, loss = 11.79144966\n",
      "Iteration 6206, loss = 12.62098153\n",
      "Iteration 6207, loss = 11.67925099\n",
      "Iteration 6208, loss = 12.04060308\n",
      "Iteration 6209, loss = 12.17546515\n",
      "Iteration 6210, loss = 11.78887912\n",
      "Iteration 6211, loss = 12.62422596\n",
      "Iteration 6212, loss = 12.37775132\n",
      "Iteration 6213, loss = 11.79865226\n",
      "Iteration 6214, loss = 12.44026409\n",
      "Iteration 6215, loss = 12.99624241\n",
      "Iteration 6216, loss = 12.57331998\n",
      "Iteration 6217, loss = 13.08977955\n",
      "Iteration 6218, loss = 13.22022758\n",
      "Iteration 6219, loss = 12.26267539\n",
      "Iteration 6220, loss = 11.94240666\n",
      "Iteration 6221, loss = 12.47365046\n",
      "Iteration 6222, loss = 12.17656704\n",
      "Iteration 6223, loss = 12.02031368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6224, loss = 11.60813564\n",
      "Iteration 6225, loss = 11.75278385\n",
      "Iteration 6226, loss = 12.21463063\n",
      "Iteration 6227, loss = 12.16786954\n",
      "Iteration 6228, loss = 11.45475424\n",
      "Iteration 6229, loss = 11.88321816\n",
      "Iteration 6230, loss = 11.88844289\n",
      "Iteration 6231, loss = 12.34637991\n",
      "Iteration 6232, loss = 11.83889803\n",
      "Iteration 6233, loss = 12.18588128\n",
      "Iteration 6234, loss = 11.97255926\n",
      "Iteration 6235, loss = 12.23671387\n",
      "Iteration 6236, loss = 12.04928131\n",
      "Iteration 6237, loss = 11.57609768\n",
      "Iteration 6238, loss = 11.92556603\n",
      "Iteration 6239, loss = 11.61322620\n",
      "Iteration 6240, loss = 11.67520668\n",
      "Iteration 6241, loss = 12.59530711\n",
      "Iteration 6242, loss = 13.41862445\n",
      "Iteration 6243, loss = 13.68815094\n",
      "Iteration 6244, loss = 12.19326330\n",
      "Iteration 6245, loss = 12.43544915\n",
      "Iteration 6246, loss = 12.58541707\n",
      "Iteration 6247, loss = 11.91497897\n",
      "Iteration 6248, loss = 11.78050756\n",
      "Iteration 6249, loss = 12.02051938\n",
      "Iteration 6250, loss = 12.14822101\n",
      "Iteration 6251, loss = 11.81662175\n",
      "Iteration 6252, loss = 12.14441062\n",
      "Iteration 6253, loss = 12.55298295\n",
      "Iteration 6254, loss = 12.46692003\n",
      "Iteration 6255, loss = 12.45359936\n",
      "Iteration 6256, loss = 12.10858667\n",
      "Iteration 6257, loss = 11.58435166\n",
      "Iteration 6258, loss = 11.76799633\n",
      "Iteration 6259, loss = 11.69207711\n",
      "Iteration 6260, loss = 11.78329316\n",
      "Iteration 6261, loss = 11.73729290\n",
      "Iteration 6262, loss = 11.58225717\n",
      "Iteration 6263, loss = 12.21810267\n",
      "Iteration 6264, loss = 12.41492288\n",
      "Iteration 6265, loss = 13.14130377\n",
      "Iteration 6266, loss = 12.50175773\n",
      "Iteration 6267, loss = 13.31425014\n",
      "Iteration 6268, loss = 12.88370479\n",
      "Iteration 6269, loss = 12.87448566\n",
      "Iteration 6270, loss = 12.99479688\n",
      "Iteration 6271, loss = 12.64337886\n",
      "Iteration 6272, loss = 11.88873525\n",
      "Iteration 6273, loss = 11.79258952\n",
      "Iteration 6274, loss = 12.57506591\n",
      "Iteration 6275, loss = 12.48068649\n",
      "Iteration 6276, loss = 12.41037841\n",
      "Iteration 6277, loss = 12.77866186\n",
      "Iteration 6278, loss = 12.97031842\n",
      "Iteration 6279, loss = 14.21820974\n",
      "Iteration 6280, loss = 13.26957658\n",
      "Iteration 6281, loss = 13.51118524\n",
      "Iteration 6282, loss = 12.58720195\n",
      "Iteration 6283, loss = 13.08076573\n",
      "Iteration 6284, loss = 13.70517676\n",
      "Iteration 6285, loss = 12.85007931\n",
      "Iteration 6286, loss = 13.19488740\n",
      "Iteration 6287, loss = 13.16023161\n",
      "Iteration 6288, loss = 12.66142970\n",
      "Iteration 6289, loss = 12.35591544\n",
      "Iteration 6290, loss = 13.22233872\n",
      "Iteration 6291, loss = 12.37677634\n",
      "Iteration 6292, loss = 12.25751597\n",
      "Iteration 6293, loss = 11.64329960\n",
      "Iteration 6294, loss = 11.75793566\n",
      "Iteration 6295, loss = 11.42725276\n",
      "Iteration 6296, loss = 11.65792679\n",
      "Iteration 6297, loss = 11.76386152\n",
      "Iteration 6298, loss = 12.09273623\n",
      "Iteration 6299, loss = 11.96181091\n",
      "Iteration 6300, loss = 12.31843853\n",
      "Iteration 6301, loss = 11.93181077\n",
      "Iteration 6302, loss = 12.04329667\n",
      "Iteration 6303, loss = 11.87928343\n",
      "Iteration 6304, loss = 11.35312305\n",
      "Iteration 6305, loss = 11.46728234\n",
      "Iteration 6306, loss = 11.59357292\n",
      "Iteration 6307, loss = 11.52993049\n",
      "Iteration 6308, loss = 11.63443675\n",
      "Iteration 6309, loss = 11.63686003\n",
      "Iteration 6310, loss = 11.44816124\n",
      "Iteration 6311, loss = 11.38862324\n",
      "Iteration 6312, loss = 12.83271029\n",
      "Iteration 6313, loss = 12.47992182\n",
      "Iteration 6314, loss = 11.89686961\n",
      "Iteration 6315, loss = 11.52803139\n",
      "Iteration 6316, loss = 11.51094060\n",
      "Iteration 6317, loss = 11.51006952\n",
      "Iteration 6318, loss = 11.85861515\n",
      "Iteration 6319, loss = 12.44887797\n",
      "Iteration 6320, loss = 12.28670732\n",
      "Iteration 6321, loss = 11.71245525\n",
      "Iteration 6322, loss = 12.06155303\n",
      "Iteration 6323, loss = 11.77206684\n",
      "Iteration 6324, loss = 11.72682714\n",
      "Iteration 6325, loss = 11.66612414\n",
      "Iteration 6326, loss = 11.73929582\n",
      "Iteration 6327, loss = 12.11076025\n",
      "Iteration 6328, loss = 11.47235533\n",
      "Iteration 6329, loss = 12.30608011\n",
      "Iteration 6330, loss = 11.95908765\n",
      "Iteration 6331, loss = 11.92483731\n",
      "Iteration 6332, loss = 12.04418830\n",
      "Iteration 6333, loss = 11.52430750\n",
      "Iteration 6334, loss = 11.45170308\n",
      "Iteration 6335, loss = 11.67125126\n",
      "Iteration 6336, loss = 11.49048407\n",
      "Iteration 6337, loss = 11.55339078\n",
      "Iteration 6338, loss = 12.38075635\n",
      "Iteration 6339, loss = 13.90185879\n",
      "Iteration 6340, loss = 13.57435279\n",
      "Iteration 6341, loss = 12.78966036\n",
      "Iteration 6342, loss = 12.09367187\n",
      "Iteration 6343, loss = 11.72559395\n",
      "Iteration 6344, loss = 11.84164205\n",
      "Iteration 6345, loss = 12.11090581\n",
      "Iteration 6346, loss = 12.12529134\n",
      "Iteration 6347, loss = 11.97877653\n",
      "Iteration 6348, loss = 12.75995107\n",
      "Iteration 6349, loss = 12.80104169\n",
      "Iteration 6350, loss = 12.70125057\n",
      "Iteration 6351, loss = 11.82621257\n",
      "Iteration 6352, loss = 12.30517404\n",
      "Iteration 6353, loss = 12.34508295\n",
      "Iteration 6354, loss = 12.01243808\n",
      "Iteration 6355, loss = 11.61424499\n",
      "Iteration 6356, loss = 11.51148502\n",
      "Iteration 6357, loss = 11.85345201\n",
      "Iteration 6358, loss = 11.88063410\n",
      "Iteration 6359, loss = 12.31711040\n",
      "Iteration 6360, loss = 11.73665730\n",
      "Iteration 6361, loss = 11.48549093\n",
      "Iteration 6362, loss = 11.72834366\n",
      "Iteration 6363, loss = 12.25411720\n",
      "Iteration 6364, loss = 11.93301091\n",
      "Iteration 6365, loss = 13.54853131\n",
      "Iteration 6366, loss = 12.71956050\n",
      "Iteration 6367, loss = 12.06013655\n",
      "Iteration 6368, loss = 11.54800801\n",
      "Iteration 6369, loss = 12.03439322\n",
      "Iteration 6370, loss = 11.39983316\n",
      "Iteration 6371, loss = 11.48040475\n",
      "Iteration 6372, loss = 11.80689153\n",
      "Iteration 6373, loss = 11.59603053\n",
      "Iteration 6374, loss = 11.61672424\n",
      "Iteration 6375, loss = 12.18546883\n",
      "Iteration 6376, loss = 13.71278101\n",
      "Iteration 6377, loss = 13.11938138\n",
      "Iteration 6378, loss = 12.07687514\n",
      "Iteration 6379, loss = 11.71388150\n",
      "Iteration 6380, loss = 11.60109605\n",
      "Iteration 6381, loss = 11.57038830\n",
      "Iteration 6382, loss = 11.58097077\n",
      "Iteration 6383, loss = 12.21983462\n",
      "Iteration 6384, loss = 13.02821877\n",
      "Iteration 6385, loss = 13.86095129\n",
      "Iteration 6386, loss = 13.32172947\n",
      "Iteration 6387, loss = 12.27806817\n",
      "Iteration 6388, loss = 12.00693266\n",
      "Iteration 6389, loss = 11.84749942\n",
      "Iteration 6390, loss = 11.22202678\n",
      "Iteration 6391, loss = 11.77905197\n",
      "Iteration 6392, loss = 12.42565109\n",
      "Iteration 6393, loss = 11.55990928\n",
      "Iteration 6394, loss = 11.60556344\n",
      "Iteration 6395, loss = 11.42014983\n",
      "Iteration 6396, loss = 11.37574395\n",
      "Iteration 6397, loss = 11.39001692\n",
      "Iteration 6398, loss = 11.77553099\n",
      "Iteration 6399, loss = 11.63781390\n",
      "Iteration 6400, loss = 11.48722054\n",
      "Iteration 6401, loss = 11.76953127\n",
      "Iteration 6402, loss = 11.94864118\n",
      "Iteration 6403, loss = 11.65563043\n",
      "Iteration 6404, loss = 11.67773860\n",
      "Iteration 6405, loss = 12.23420670\n",
      "Iteration 6406, loss = 11.53808337\n",
      "Iteration 6407, loss = 11.61195496\n",
      "Iteration 6408, loss = 11.69267485\n",
      "Iteration 6409, loss = 11.62191542\n",
      "Iteration 6410, loss = 11.73521838\n",
      "Iteration 6411, loss = 11.63205241\n",
      "Iteration 6412, loss = 11.80296002\n",
      "Iteration 6413, loss = 11.96544070\n",
      "Iteration 6414, loss = 11.97194095\n",
      "Iteration 6415, loss = 12.27406899\n",
      "Iteration 6416, loss = 11.64837417\n",
      "Iteration 6417, loss = 11.78833533\n",
      "Iteration 6418, loss = 12.74295272\n",
      "Iteration 6419, loss = 13.06317139\n",
      "Iteration 6420, loss = 12.25604552\n",
      "Iteration 6421, loss = 12.90067828\n",
      "Iteration 6422, loss = 13.62067747\n",
      "Iteration 6423, loss = 11.90028966\n",
      "Iteration 6424, loss = 11.85271859\n",
      "Iteration 6425, loss = 12.09436414\n",
      "Iteration 6426, loss = 11.93673326\n",
      "Iteration 6427, loss = 13.94167747\n",
      "Iteration 6428, loss = 12.32901421\n",
      "Iteration 6429, loss = 12.82504986\n",
      "Iteration 6430, loss = 12.46042417\n",
      "Iteration 6431, loss = 12.13088440\n",
      "Iteration 6432, loss = 12.30389988\n",
      "Iteration 6433, loss = 11.79864962\n",
      "Iteration 6434, loss = 11.37903322\n",
      "Iteration 6435, loss = 11.49050826\n",
      "Iteration 6436, loss = 11.77355182\n",
      "Iteration 6437, loss = 11.82366938\n",
      "Iteration 6438, loss = 11.67536554\n",
      "Iteration 6439, loss = 11.64214420\n",
      "Iteration 6440, loss = 11.66394477\n",
      "Iteration 6441, loss = 11.68392114\n",
      "Iteration 6442, loss = 11.61298758\n",
      "Iteration 6443, loss = 11.52015105\n",
      "Iteration 6444, loss = 11.37186941\n",
      "Iteration 6445, loss = 11.63505583\n",
      "Iteration 6446, loss = 11.89872325\n",
      "Iteration 6447, loss = 12.00320632\n",
      "Iteration 6448, loss = 11.97998342\n",
      "Iteration 6449, loss = 12.28853205\n",
      "Iteration 6450, loss = 12.16751052\n",
      "Iteration 6451, loss = 12.98158037\n",
      "Iteration 6452, loss = 11.86081957\n",
      "Iteration 6453, loss = 11.70739764\n",
      "Iteration 6454, loss = 11.61956544\n",
      "Iteration 6455, loss = 11.35710951\n",
      "Iteration 6456, loss = 11.57188750\n",
      "Iteration 6457, loss = 12.36463589\n",
      "Iteration 6458, loss = 11.94777340\n",
      "Iteration 6459, loss = 11.69747529\n",
      "Iteration 6460, loss = 11.83668430\n",
      "Iteration 6461, loss = 11.77106032\n",
      "Iteration 6462, loss = 11.59637740\n",
      "Iteration 6463, loss = 11.73731062\n",
      "Iteration 6464, loss = 11.73436713\n",
      "Iteration 6465, loss = 12.63662257\n",
      "Iteration 6466, loss = 14.89227895\n",
      "Iteration 6467, loss = 13.08471314\n",
      "Iteration 6468, loss = 12.79181433\n",
      "Iteration 6469, loss = 12.76574234\n",
      "Iteration 6470, loss = 13.42885154\n",
      "Iteration 6471, loss = 12.52713913\n",
      "Iteration 6472, loss = 12.66900981\n",
      "Iteration 6473, loss = 11.98676953\n",
      "Iteration 6474, loss = 12.30505953\n",
      "Iteration 6475, loss = 13.06173542\n",
      "Iteration 6476, loss = 14.45820097\n",
      "Iteration 6477, loss = 13.50202968\n",
      "Iteration 6478, loss = 12.38394429\n",
      "Iteration 6479, loss = 12.06027044\n",
      "Iteration 6480, loss = 11.85058346\n",
      "Iteration 6481, loss = 11.41746212\n",
      "Iteration 6482, loss = 11.27310454\n",
      "Iteration 6483, loss = 11.60725863\n",
      "Iteration 6484, loss = 11.25827749\n",
      "Iteration 6485, loss = 11.60015610\n",
      "Iteration 6486, loss = 11.68294110\n",
      "Iteration 6487, loss = 11.73498971\n",
      "Iteration 6488, loss = 11.53570167\n",
      "Iteration 6489, loss = 11.92132169\n",
      "Iteration 6490, loss = 12.54623817\n",
      "Iteration 6491, loss = 11.57682159\n",
      "Iteration 6492, loss = 11.91228942\n",
      "Iteration 6493, loss = 11.64097886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6494, loss = 12.52170593\n",
      "Iteration 6495, loss = 12.08916167\n",
      "Iteration 6496, loss = 11.99849783\n",
      "Iteration 6497, loss = 11.36064380\n",
      "Iteration 6498, loss = 11.37614749\n",
      "Iteration 6499, loss = 11.46057934\n",
      "Iteration 6500, loss = 11.72395506\n",
      "Iteration 6501, loss = 11.36932872\n",
      "Iteration 6502, loss = 11.70443668\n",
      "Iteration 6503, loss = 11.53291196\n",
      "Iteration 6504, loss = 11.42835359\n",
      "Iteration 6505, loss = 11.58168640\n",
      "Iteration 6506, loss = 11.42682787\n",
      "Iteration 6507, loss = 11.24908665\n",
      "Iteration 6508, loss = 11.83843221\n",
      "Iteration 6509, loss = 11.43651790\n",
      "Iteration 6510, loss = 11.45217854\n",
      "Iteration 6511, loss = 11.28504224\n",
      "Iteration 6512, loss = 11.61550110\n",
      "Iteration 6513, loss = 11.72516515\n",
      "Iteration 6514, loss = 11.99635047\n",
      "Iteration 6515, loss = 12.64396535\n",
      "Iteration 6516, loss = 11.54247410\n",
      "Iteration 6517, loss = 12.02895320\n",
      "Iteration 6518, loss = 12.35196404\n",
      "Iteration 6519, loss = 11.92132948\n",
      "Iteration 6520, loss = 12.16264120\n",
      "Iteration 6521, loss = 12.71603435\n",
      "Iteration 6522, loss = 11.84155939\n",
      "Iteration 6523, loss = 11.32248188\n",
      "Iteration 6524, loss = 11.56361108\n",
      "Iteration 6525, loss = 11.62788910\n",
      "Iteration 6526, loss = 11.60551007\n",
      "Iteration 6527, loss = 12.44086358\n",
      "Iteration 6528, loss = 11.96445388\n",
      "Iteration 6529, loss = 11.98299332\n",
      "Iteration 6530, loss = 11.89952236\n",
      "Iteration 6531, loss = 13.88699966\n",
      "Iteration 6532, loss = 13.03432506\n",
      "Iteration 6533, loss = 11.52599146\n",
      "Iteration 6534, loss = 11.54048275\n",
      "Iteration 6535, loss = 11.73912559\n",
      "Iteration 6536, loss = 11.25540848\n",
      "Iteration 6537, loss = 11.96778083\n",
      "Iteration 6538, loss = 11.65410542\n",
      "Iteration 6539, loss = 11.64755183\n",
      "Iteration 6540, loss = 11.99796836\n",
      "Iteration 6541, loss = 11.38021646\n",
      "Iteration 6542, loss = 11.75820104\n",
      "Iteration 6543, loss = 11.83805887\n",
      "Iteration 6544, loss = 11.21243265\n",
      "Iteration 6545, loss = 11.19330943\n",
      "Iteration 6546, loss = 11.66399124\n",
      "Iteration 6547, loss = 12.08411837\n",
      "Iteration 6548, loss = 12.50747157\n",
      "Iteration 6549, loss = 12.52220952\n",
      "Iteration 6550, loss = 11.75781287\n",
      "Iteration 6551, loss = 11.34750736\n",
      "Iteration 6552, loss = 11.39117855\n",
      "Iteration 6553, loss = 11.60620808\n",
      "Iteration 6554, loss = 11.49244751\n",
      "Iteration 6555, loss = 11.45186920\n",
      "Iteration 6556, loss = 11.06732063\n",
      "Iteration 6557, loss = 11.66885870\n",
      "Iteration 6558, loss = 11.40644603\n",
      "Iteration 6559, loss = 11.62192106\n",
      "Iteration 6560, loss = 12.41953500\n",
      "Iteration 6561, loss = 12.08221459\n",
      "Iteration 6562, loss = 11.47981310\n",
      "Iteration 6563, loss = 11.76762205\n",
      "Iteration 6564, loss = 12.53071021\n",
      "Iteration 6565, loss = 12.18877407\n",
      "Iteration 6566, loss = 12.08880662\n",
      "Iteration 6567, loss = 11.90383024\n",
      "Iteration 6568, loss = 12.28083448\n",
      "Iteration 6569, loss = 12.90721525\n",
      "Iteration 6570, loss = 11.70308785\n",
      "Iteration 6571, loss = 12.24949063\n",
      "Iteration 6572, loss = 12.62897570\n",
      "Iteration 6573, loss = 11.65928897\n",
      "Iteration 6574, loss = 12.32749181\n",
      "Iteration 6575, loss = 11.86943808\n",
      "Iteration 6576, loss = 11.59754655\n",
      "Iteration 6577, loss = 12.22747019\n",
      "Iteration 6578, loss = 12.74886452\n",
      "Iteration 6579, loss = 11.90026249\n",
      "Iteration 6580, loss = 11.63821114\n",
      "Iteration 6581, loss = 11.57882154\n",
      "Iteration 6582, loss = 11.49105873\n",
      "Iteration 6583, loss = 12.09088329\n",
      "Iteration 6584, loss = 11.29142474\n",
      "Iteration 6585, loss = 11.67271768\n",
      "Iteration 6586, loss = 12.26165889\n",
      "Iteration 6587, loss = 11.27784447\n",
      "Iteration 6588, loss = 11.65061187\n",
      "Iteration 6589, loss = 11.93712508\n",
      "Iteration 6590, loss = 11.41068845\n",
      "Iteration 6591, loss = 11.52067053\n",
      "Iteration 6592, loss = 11.24000935\n",
      "Iteration 6593, loss = 11.73239317\n",
      "Iteration 6594, loss = 11.60848818\n",
      "Iteration 6595, loss = 11.49264703\n",
      "Iteration 6596, loss = 11.84159052\n",
      "Iteration 6597, loss = 11.64177401\n",
      "Iteration 6598, loss = 12.36529570\n",
      "Iteration 6599, loss = 11.78655310\n",
      "Iteration 6600, loss = 11.46342711\n",
      "Iteration 6601, loss = 11.40242295\n",
      "Iteration 6602, loss = 11.91587376\n",
      "Iteration 6603, loss = 11.47201168\n",
      "Iteration 6604, loss = 11.11245574\n",
      "Iteration 6605, loss = 11.51719608\n",
      "Iteration 6606, loss = 11.24576852\n",
      "Iteration 6607, loss = 11.95830349\n",
      "Iteration 6608, loss = 11.79478324\n",
      "Iteration 6609, loss = 11.28991787\n",
      "Iteration 6610, loss = 11.78466781\n",
      "Iteration 6611, loss = 12.91753807\n",
      "Iteration 6612, loss = 13.92776539\n",
      "Iteration 6613, loss = 13.19958098\n",
      "Iteration 6614, loss = 13.39528138\n",
      "Iteration 6615, loss = 12.57905055\n",
      "Iteration 6616, loss = 12.20625032\n",
      "Iteration 6617, loss = 11.60319890\n",
      "Iteration 6618, loss = 11.26665737\n",
      "Iteration 6619, loss = 12.60265109\n",
      "Iteration 6620, loss = 11.46862210\n",
      "Iteration 6621, loss = 11.72960358\n",
      "Iteration 6622, loss = 11.88031691\n",
      "Iteration 6623, loss = 11.65341519\n",
      "Iteration 6624, loss = 11.35658180\n",
      "Iteration 6625, loss = 11.51320938\n",
      "Iteration 6626, loss = 11.30582501\n",
      "Iteration 6627, loss = 11.83480467\n",
      "Iteration 6628, loss = 11.61679612\n",
      "Iteration 6629, loss = 11.41069959\n",
      "Iteration 6630, loss = 12.09291631\n",
      "Iteration 6631, loss = 11.76697813\n",
      "Iteration 6632, loss = 11.62904070\n",
      "Iteration 6633, loss = 11.35672653\n",
      "Iteration 6634, loss = 11.59052001\n",
      "Iteration 6635, loss = 12.16045837\n",
      "Iteration 6636, loss = 11.88717916\n",
      "Iteration 6637, loss = 11.95096322\n",
      "Iteration 6638, loss = 12.07427333\n",
      "Iteration 6639, loss = 12.27965357\n",
      "Iteration 6640, loss = 14.53326408\n",
      "Iteration 6641, loss = 11.86434992\n",
      "Iteration 6642, loss = 11.33004773\n",
      "Iteration 6643, loss = 12.15871576\n",
      "Iteration 6644, loss = 11.60606992\n",
      "Iteration 6645, loss = 11.58069070\n",
      "Iteration 6646, loss = 11.75320111\n",
      "Iteration 6647, loss = 11.17669609\n",
      "Iteration 6648, loss = 11.97370420\n",
      "Iteration 6649, loss = 11.52556023\n",
      "Iteration 6650, loss = 11.31189843\n",
      "Iteration 6651, loss = 11.53882951\n",
      "Iteration 6652, loss = 12.15259514\n",
      "Iteration 6653, loss = 11.68230377\n",
      "Iteration 6654, loss = 11.92169852\n",
      "Iteration 6655, loss = 11.69464761\n",
      "Iteration 6656, loss = 11.12498006\n",
      "Iteration 6657, loss = 11.14625976\n",
      "Iteration 6658, loss = 11.51352560\n",
      "Iteration 6659, loss = 11.72122176\n",
      "Iteration 6660, loss = 11.02093328\n",
      "Iteration 6661, loss = 11.26659839\n",
      "Iteration 6662, loss = 11.51190903\n",
      "Iteration 6663, loss = 11.58016347\n",
      "Iteration 6664, loss = 11.28407892\n",
      "Iteration 6665, loss = 11.74989440\n",
      "Iteration 6666, loss = 11.36744095\n",
      "Iteration 6667, loss = 11.36693677\n",
      "Iteration 6668, loss = 11.59293021\n",
      "Iteration 6669, loss = 11.62434400\n",
      "Iteration 6670, loss = 11.65284328\n",
      "Iteration 6671, loss = 11.07382715\n",
      "Iteration 6672, loss = 11.38922745\n",
      "Iteration 6673, loss = 11.22418762\n",
      "Iteration 6674, loss = 11.51981623\n",
      "Iteration 6675, loss = 11.59054899\n",
      "Iteration 6676, loss = 11.36804050\n",
      "Iteration 6677, loss = 11.10606376\n",
      "Iteration 6678, loss = 11.35526257\n",
      "Iteration 6679, loss = 11.83066879\n",
      "Iteration 6680, loss = 12.37320485\n",
      "Iteration 6681, loss = 12.32333033\n",
      "Iteration 6682, loss = 12.44645963\n",
      "Iteration 6683, loss = 13.45394297\n",
      "Iteration 6684, loss = 11.73126859\n",
      "Iteration 6685, loss = 12.00119493\n",
      "Iteration 6686, loss = 11.93903277\n",
      "Iteration 6687, loss = 11.35558979\n",
      "Iteration 6688, loss = 11.15320767\n",
      "Iteration 6689, loss = 11.31675778\n",
      "Iteration 6690, loss = 12.44040108\n",
      "Iteration 6691, loss = 13.00665055\n",
      "Iteration 6692, loss = 11.99899994\n",
      "Iteration 6693, loss = 11.99674186\n",
      "Iteration 6694, loss = 11.32289586\n",
      "Iteration 6695, loss = 11.71499606\n",
      "Iteration 6696, loss = 11.79527230\n",
      "Iteration 6697, loss = 11.83868815\n",
      "Iteration 6698, loss = 12.09526411\n",
      "Iteration 6699, loss = 11.47333604\n",
      "Iteration 6700, loss = 11.74566278\n",
      "Iteration 6701, loss = 11.89985088\n",
      "Iteration 6702, loss = 11.98556738\n",
      "Iteration 6703, loss = 11.68974758\n",
      "Iteration 6704, loss = 12.05736713\n",
      "Iteration 6705, loss = 11.59000118\n",
      "Iteration 6706, loss = 11.10340637\n",
      "Iteration 6707, loss = 11.31453112\n",
      "Iteration 6708, loss = 11.40850061\n",
      "Iteration 6709, loss = 11.44301762\n",
      "Iteration 6710, loss = 11.54765708\n",
      "Iteration 6711, loss = 11.51924946\n",
      "Iteration 6712, loss = 11.12043861\n",
      "Iteration 6713, loss = 11.22195133\n",
      "Iteration 6714, loss = 11.50451956\n",
      "Iteration 6715, loss = 11.92634959\n",
      "Iteration 6716, loss = 11.25193270\n",
      "Iteration 6717, loss = 12.09990765\n",
      "Iteration 6718, loss = 11.33083233\n",
      "Iteration 6719, loss = 11.74692499\n",
      "Iteration 6720, loss = 11.01305319\n",
      "Iteration 6721, loss = 11.64222577\n",
      "Iteration 6722, loss = 11.71592559\n",
      "Iteration 6723, loss = 11.53188113\n",
      "Iteration 6724, loss = 11.73889015\n",
      "Iteration 6725, loss = 11.64435303\n",
      "Iteration 6726, loss = 11.80463927\n",
      "Iteration 6727, loss = 13.27923831\n",
      "Iteration 6728, loss = 12.93752993\n",
      "Iteration 6729, loss = 12.88438771\n",
      "Iteration 6730, loss = 11.64874968\n",
      "Iteration 6731, loss = 11.38069339\n",
      "Iteration 6732, loss = 11.43094702\n",
      "Iteration 6733, loss = 11.14575617\n",
      "Iteration 6734, loss = 11.12098286\n",
      "Iteration 6735, loss = 11.26782613\n",
      "Iteration 6736, loss = 12.37488650\n",
      "Iteration 6737, loss = 11.39334836\n",
      "Iteration 6738, loss = 11.24612589\n",
      "Iteration 6739, loss = 11.09087220\n",
      "Iteration 6740, loss = 11.36387571\n",
      "Iteration 6741, loss = 12.09095292\n",
      "Iteration 6742, loss = 11.52360787\n",
      "Iteration 6743, loss = 11.47601500\n",
      "Iteration 6744, loss = 11.10683718\n",
      "Iteration 6745, loss = 11.63132047\n",
      "Iteration 6746, loss = 11.60020691\n",
      "Iteration 6747, loss = 12.05237038\n",
      "Iteration 6748, loss = 11.44789244\n",
      "Iteration 6749, loss = 11.42467914\n",
      "Iteration 6750, loss = 11.56255878\n",
      "Iteration 6751, loss = 12.22197897\n",
      "Iteration 6752, loss = 11.95578606\n",
      "Iteration 6753, loss = 13.14392249\n",
      "Iteration 6754, loss = 12.35312015\n",
      "Iteration 6755, loss = 12.02014976\n",
      "Iteration 6756, loss = 11.96175246\n",
      "Iteration 6757, loss = 11.28022224\n",
      "Iteration 6758, loss = 11.11435542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6759, loss = 11.00755642\n",
      "Iteration 6760, loss = 12.15569377\n",
      "Iteration 6761, loss = 12.54480371\n",
      "Iteration 6762, loss = 12.67800954\n",
      "Iteration 6763, loss = 11.86157292\n",
      "Iteration 6764, loss = 11.44594453\n",
      "Iteration 6765, loss = 11.75674925\n",
      "Iteration 6766, loss = 11.45509124\n",
      "Iteration 6767, loss = 11.77307842\n",
      "Iteration 6768, loss = 11.74934440\n",
      "Iteration 6769, loss = 11.98707163\n",
      "Iteration 6770, loss = 12.05771629\n",
      "Iteration 6771, loss = 12.73006091\n",
      "Iteration 6772, loss = 11.19598247\n",
      "Iteration 6773, loss = 11.51925240\n",
      "Iteration 6774, loss = 10.97491970\n",
      "Iteration 6775, loss = 11.58658773\n",
      "Iteration 6776, loss = 11.99120845\n",
      "Iteration 6777, loss = 11.66735121\n",
      "Iteration 6778, loss = 11.57952752\n",
      "Iteration 6779, loss = 11.48645868\n",
      "Iteration 6780, loss = 11.10659340\n",
      "Iteration 6781, loss = 10.90752292\n",
      "Iteration 6782, loss = 11.66386998\n",
      "Iteration 6783, loss = 11.93172932\n",
      "Iteration 6784, loss = 13.18398459\n",
      "Iteration 6785, loss = 11.99010868\n",
      "Iteration 6786, loss = 11.23015565\n",
      "Iteration 6787, loss = 11.84272463\n",
      "Iteration 6788, loss = 12.44701130\n",
      "Iteration 6789, loss = 11.76523686\n",
      "Iteration 6790, loss = 11.46922120\n",
      "Iteration 6791, loss = 12.24283123\n",
      "Iteration 6792, loss = 12.52590842\n",
      "Iteration 6793, loss = 11.92276463\n",
      "Iteration 6794, loss = 11.39549495\n",
      "Iteration 6795, loss = 11.04363803\n",
      "Iteration 6796, loss = 11.02263002\n",
      "Iteration 6797, loss = 11.37141519\n",
      "Iteration 6798, loss = 12.72377907\n",
      "Iteration 6799, loss = 11.80289912\n",
      "Iteration 6800, loss = 11.17746446\n",
      "Iteration 6801, loss = 11.24607023\n",
      "Iteration 6802, loss = 11.72083058\n",
      "Iteration 6803, loss = 12.10163830\n",
      "Iteration 6804, loss = 11.19342403\n",
      "Iteration 6805, loss = 11.54812986\n",
      "Iteration 6806, loss = 11.12127649\n",
      "Iteration 6807, loss = 11.38430042\n",
      "Iteration 6808, loss = 11.75413904\n",
      "Iteration 6809, loss = 11.50849839\n",
      "Iteration 6810, loss = 11.44389813\n",
      "Iteration 6811, loss = 11.18000517\n",
      "Iteration 6812, loss = 11.50978449\n",
      "Iteration 6813, loss = 11.24772824\n",
      "Iteration 6814, loss = 11.20460126\n",
      "Iteration 6815, loss = 11.92993966\n",
      "Iteration 6816, loss = 11.52001924\n",
      "Iteration 6817, loss = 11.12397802\n",
      "Iteration 6818, loss = 11.08712708\n",
      "Iteration 6819, loss = 11.18689467\n",
      "Iteration 6820, loss = 12.01508121\n",
      "Iteration 6821, loss = 12.48064136\n",
      "Iteration 6822, loss = 12.65092403\n",
      "Iteration 6823, loss = 11.31779082\n",
      "Iteration 6824, loss = 11.53706940\n",
      "Iteration 6825, loss = 11.02408650\n",
      "Iteration 6826, loss = 11.54422309\n",
      "Iteration 6827, loss = 11.35935029\n",
      "Iteration 6828, loss = 12.12154536\n",
      "Iteration 6829, loss = 11.29387951\n",
      "Iteration 6830, loss = 12.61719542\n",
      "Iteration 6831, loss = 12.19115352\n",
      "Iteration 6832, loss = 12.34185323\n",
      "Iteration 6833, loss = 11.92295998\n",
      "Iteration 6834, loss = 11.19116732\n",
      "Iteration 6835, loss = 10.95931326\n",
      "Iteration 6836, loss = 11.72780315\n",
      "Iteration 6837, loss = 11.78679678\n",
      "Iteration 6838, loss = 12.34540866\n",
      "Iteration 6839, loss = 11.21078960\n",
      "Iteration 6840, loss = 11.83225085\n",
      "Iteration 6841, loss = 11.32859965\n",
      "Iteration 6842, loss = 11.20208636\n",
      "Iteration 6843, loss = 11.51826229\n",
      "Iteration 6844, loss = 11.71462693\n",
      "Iteration 6845, loss = 11.63840808\n",
      "Iteration 6846, loss = 11.12284279\n",
      "Iteration 6847, loss = 11.14019120\n",
      "Iteration 6848, loss = 11.54063799\n",
      "Iteration 6849, loss = 11.19603690\n",
      "Iteration 6850, loss = 11.22699971\n",
      "Iteration 6851, loss = 10.97248331\n",
      "Iteration 6852, loss = 11.37342815\n",
      "Iteration 6853, loss = 12.51004621\n",
      "Iteration 6854, loss = 14.11565564\n",
      "Iteration 6855, loss = 14.72209435\n",
      "Iteration 6856, loss = 12.41648438\n",
      "Iteration 6857, loss = 11.95780593\n",
      "Iteration 6858, loss = 12.33728770\n",
      "Iteration 6859, loss = 14.83723733\n",
      "Iteration 6860, loss = 14.25099090\n",
      "Iteration 6861, loss = 12.44662466\n",
      "Iteration 6862, loss = 11.98378946\n",
      "Iteration 6863, loss = 13.40170946\n",
      "Iteration 6864, loss = 11.36220037\n",
      "Iteration 6865, loss = 11.65978415\n",
      "Iteration 6866, loss = 11.62856087\n",
      "Iteration 6867, loss = 11.33441111\n",
      "Iteration 6868, loss = 11.12721048\n",
      "Iteration 6869, loss = 11.65517765\n",
      "Iteration 6870, loss = 11.53934412\n",
      "Iteration 6871, loss = 11.25775124\n",
      "Iteration 6872, loss = 11.57711342\n",
      "Iteration 6873, loss = 11.29147911\n",
      "Iteration 6874, loss = 11.44730055\n",
      "Iteration 6875, loss = 10.77447668\n",
      "Iteration 6876, loss = 11.38669828\n",
      "Iteration 6877, loss = 10.91912124\n",
      "Iteration 6878, loss = 11.84886543\n",
      "Iteration 6879, loss = 11.73250079\n",
      "Iteration 6880, loss = 11.25076724\n",
      "Iteration 6881, loss = 11.22488607\n",
      "Iteration 6882, loss = 11.12321775\n",
      "Iteration 6883, loss = 11.11521374\n",
      "Iteration 6884, loss = 11.48807966\n",
      "Iteration 6885, loss = 11.22964742\n",
      "Iteration 6886, loss = 11.61265928\n",
      "Iteration 6887, loss = 10.93763723\n",
      "Iteration 6888, loss = 10.92019032\n",
      "Iteration 6889, loss = 11.04845023\n",
      "Iteration 6890, loss = 10.94920156\n",
      "Iteration 6891, loss = 11.78279946\n",
      "Iteration 6892, loss = 11.43060588\n",
      "Iteration 6893, loss = 11.65089180\n",
      "Iteration 6894, loss = 11.59251174\n",
      "Iteration 6895, loss = 11.57822734\n",
      "Iteration 6896, loss = 11.05814908\n",
      "Iteration 6897, loss = 12.66142994\n",
      "Iteration 6898, loss = 12.95994276\n",
      "Iteration 6899, loss = 12.15521168\n",
      "Iteration 6900, loss = 12.16473924\n",
      "Iteration 6901, loss = 11.58149367\n",
      "Iteration 6902, loss = 11.11051933\n",
      "Iteration 6903, loss = 11.04984136\n",
      "Iteration 6904, loss = 11.84474178\n",
      "Iteration 6905, loss = 12.19256235\n",
      "Iteration 6906, loss = 11.19429752\n",
      "Iteration 6907, loss = 10.92822707\n",
      "Iteration 6908, loss = 11.06091296\n",
      "Iteration 6909, loss = 11.31968725\n",
      "Iteration 6910, loss = 10.91580475\n",
      "Iteration 6911, loss = 11.05417761\n",
      "Iteration 6912, loss = 11.61774670\n",
      "Iteration 6913, loss = 11.57217934\n",
      "Iteration 6914, loss = 12.07243180\n",
      "Iteration 6915, loss = 12.22478712\n",
      "Iteration 6916, loss = 11.80265930\n",
      "Iteration 6917, loss = 11.57488691\n",
      "Iteration 6918, loss = 12.12389536\n",
      "Iteration 6919, loss = 11.33143577\n",
      "Iteration 6920, loss = 11.18890475\n",
      "Iteration 6921, loss = 11.45576784\n",
      "Iteration 6922, loss = 11.37143470\n",
      "Iteration 6923, loss = 11.16724421\n",
      "Iteration 6924, loss = 11.36888344\n",
      "Iteration 6925, loss = 11.42031383\n",
      "Iteration 6926, loss = 10.83547032\n",
      "Iteration 6927, loss = 11.26448400\n",
      "Iteration 6928, loss = 11.07919382\n",
      "Iteration 6929, loss = 11.38645399\n",
      "Iteration 6930, loss = 11.39821962\n",
      "Iteration 6931, loss = 12.19379656\n",
      "Iteration 6932, loss = 11.30328690\n",
      "Iteration 6933, loss = 11.03338642\n",
      "Iteration 6934, loss = 11.15214971\n",
      "Iteration 6935, loss = 10.97112057\n",
      "Iteration 6936, loss = 11.22575562\n",
      "Iteration 6937, loss = 12.07457436\n",
      "Iteration 6938, loss = 11.19999830\n",
      "Iteration 6939, loss = 11.76753791\n",
      "Iteration 6940, loss = 11.25618585\n",
      "Iteration 6941, loss = 11.27382225\n",
      "Iteration 6942, loss = 11.36800295\n",
      "Iteration 6943, loss = 11.27632299\n",
      "Iteration 6944, loss = 11.98954631\n",
      "Iteration 6945, loss = 12.03198298\n",
      "Iteration 6946, loss = 11.76870964\n",
      "Iteration 6947, loss = 12.67127802\n",
      "Iteration 6948, loss = 11.96630759\n",
      "Iteration 6949, loss = 11.96800202\n",
      "Iteration 6950, loss = 11.79125710\n",
      "Iteration 6951, loss = 12.43780822\n",
      "Iteration 6952, loss = 12.69816576\n",
      "Iteration 6953, loss = 11.66298265\n",
      "Iteration 6954, loss = 12.79592059\n",
      "Iteration 6955, loss = 12.45072196\n",
      "Iteration 6956, loss = 11.49828569\n",
      "Iteration 6957, loss = 11.48249562\n",
      "Iteration 6958, loss = 10.75308277\n",
      "Iteration 6959, loss = 11.19641948\n",
      "Iteration 6960, loss = 11.22412423\n",
      "Iteration 6961, loss = 11.23186771\n",
      "Iteration 6962, loss = 11.14749793\n",
      "Iteration 6963, loss = 11.66710474\n",
      "Iteration 6964, loss = 11.87317567\n",
      "Iteration 6965, loss = 11.18889151\n",
      "Iteration 6966, loss = 10.88435623\n",
      "Iteration 6967, loss = 11.11060923\n",
      "Iteration 6968, loss = 11.74483038\n",
      "Iteration 6969, loss = 11.51677855\n",
      "Iteration 6970, loss = 11.31153016\n",
      "Iteration 6971, loss = 11.14528053\n",
      "Iteration 6972, loss = 10.96005807\n",
      "Iteration 6973, loss = 11.16782664\n",
      "Iteration 6974, loss = 11.23175334\n",
      "Iteration 6975, loss = 11.48812494\n",
      "Iteration 6976, loss = 11.48872444\n",
      "Iteration 6977, loss = 12.27356549\n",
      "Iteration 6978, loss = 11.07361475\n",
      "Iteration 6979, loss = 11.09264934\n",
      "Iteration 6980, loss = 11.80193352\n",
      "Iteration 6981, loss = 11.17057171\n",
      "Iteration 6982, loss = 11.22021299\n",
      "Iteration 6983, loss = 10.88134673\n",
      "Iteration 6984, loss = 11.18194042\n",
      "Iteration 6985, loss = 11.57440253\n",
      "Iteration 6986, loss = 12.33667808\n",
      "Iteration 6987, loss = 11.29668169\n",
      "Iteration 6988, loss = 11.34182927\n",
      "Iteration 6989, loss = 11.48098948\n",
      "Iteration 6990, loss = 11.21306421\n",
      "Iteration 6991, loss = 10.83658509\n",
      "Iteration 6992, loss = 10.89933197\n",
      "Iteration 6993, loss = 10.88561484\n",
      "Iteration 6994, loss = 11.40636987\n",
      "Iteration 6995, loss = 12.07421827\n",
      "Iteration 6996, loss = 11.04687259\n",
      "Iteration 6997, loss = 11.78414995\n",
      "Iteration 6998, loss = 11.07989969\n",
      "Iteration 6999, loss = 11.16265714\n",
      "Iteration 7000, loss = 11.36776522\n",
      "Iteration 7001, loss = 11.05376969\n",
      "Iteration 7002, loss = 11.49161577\n",
      "Iteration 7003, loss = 11.16343936\n",
      "Iteration 7004, loss = 11.11174889\n",
      "Iteration 7005, loss = 10.88821868\n",
      "Iteration 7006, loss = 11.55750205\n",
      "Iteration 7007, loss = 11.49886213\n",
      "Iteration 7008, loss = 11.49075766\n",
      "Iteration 7009, loss = 11.93503684\n",
      "Iteration 7010, loss = 11.23486043\n",
      "Iteration 7011, loss = 11.48928508\n",
      "Iteration 7012, loss = 11.60334306\n",
      "Iteration 7013, loss = 11.81552930\n",
      "Iteration 7014, loss = 10.94736201\n",
      "Iteration 7015, loss = 10.89018388\n",
      "Iteration 7016, loss = 11.10324762\n",
      "Iteration 7017, loss = 12.45741762\n",
      "Iteration 7018, loss = 12.98486332\n",
      "Iteration 7019, loss = 11.09129240\n",
      "Iteration 7020, loss = 10.76834026\n",
      "Iteration 7021, loss = 10.84444142\n",
      "Iteration 7022, loss = 11.33494679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7023, loss = 11.03623500\n",
      "Iteration 7024, loss = 11.26731556\n",
      "Iteration 7025, loss = 10.69264574\n",
      "Iteration 7026, loss = 11.14450393\n",
      "Iteration 7027, loss = 10.90946848\n",
      "Iteration 7028, loss = 10.80422996\n",
      "Iteration 7029, loss = 10.82219144\n",
      "Iteration 7030, loss = 10.87643879\n",
      "Iteration 7031, loss = 11.35147099\n",
      "Iteration 7032, loss = 12.57453784\n",
      "Iteration 7033, loss = 12.86490585\n",
      "Iteration 7034, loss = 12.74341737\n",
      "Iteration 7035, loss = 12.42018475\n",
      "Iteration 7036, loss = 11.39753679\n",
      "Iteration 7037, loss = 11.77475396\n",
      "Iteration 7038, loss = 11.74578602\n",
      "Iteration 7039, loss = 11.14493846\n",
      "Iteration 7040, loss = 11.53440971\n",
      "Iteration 7041, loss = 11.78974819\n",
      "Iteration 7042, loss = 11.00354174\n",
      "Iteration 7043, loss = 11.25291143\n",
      "Iteration 7044, loss = 11.50021034\n",
      "Iteration 7045, loss = 10.92513490\n",
      "Iteration 7046, loss = 11.76420121\n",
      "Iteration 7047, loss = 12.00434154\n",
      "Iteration 7048, loss = 11.44379714\n",
      "Iteration 7049, loss = 11.41789965\n",
      "Iteration 7050, loss = 12.67299951\n",
      "Iteration 7051, loss = 11.51531414\n",
      "Iteration 7052, loss = 10.86462560\n",
      "Iteration 7053, loss = 10.73649316\n",
      "Iteration 7054, loss = 10.73804628\n",
      "Iteration 7055, loss = 11.02142836\n",
      "Iteration 7056, loss = 11.02119692\n",
      "Iteration 7057, loss = 10.97603627\n",
      "Iteration 7058, loss = 10.97522776\n",
      "Iteration 7059, loss = 10.77334122\n",
      "Iteration 7060, loss = 10.88091156\n",
      "Iteration 7061, loss = 11.32936117\n",
      "Iteration 7062, loss = 11.57057581\n",
      "Iteration 7063, loss = 11.58787790\n",
      "Iteration 7064, loss = 10.97010809\n",
      "Iteration 7065, loss = 10.82727070\n",
      "Iteration 7066, loss = 10.91170665\n",
      "Iteration 7067, loss = 10.96374771\n",
      "Iteration 7068, loss = 11.30599833\n",
      "Iteration 7069, loss = 11.91976917\n",
      "Iteration 7070, loss = 12.03673790\n",
      "Iteration 7071, loss = 12.49982037\n",
      "Iteration 7072, loss = 12.32428698\n",
      "Iteration 7073, loss = 12.38190588\n",
      "Iteration 7074, loss = 10.78080475\n",
      "Iteration 7075, loss = 11.44107314\n",
      "Iteration 7076, loss = 12.05781915\n",
      "Iteration 7077, loss = 12.98268991\n",
      "Iteration 7078, loss = 12.22687091\n",
      "Iteration 7079, loss = 11.75733701\n",
      "Iteration 7080, loss = 12.18979892\n",
      "Iteration 7081, loss = 13.03483813\n",
      "Iteration 7082, loss = 11.90882701\n",
      "Iteration 7083, loss = 11.44658860\n",
      "Iteration 7084, loss = 10.98023868\n",
      "Iteration 7085, loss = 11.03495358\n",
      "Iteration 7086, loss = 11.48818497\n",
      "Iteration 7087, loss = 11.49129020\n",
      "Iteration 7088, loss = 10.97993208\n",
      "Iteration 7089, loss = 11.38052789\n",
      "Iteration 7090, loss = 11.68173321\n",
      "Iteration 7091, loss = 11.79149638\n",
      "Iteration 7092, loss = 11.27239618\n",
      "Iteration 7093, loss = 11.05354659\n",
      "Iteration 7094, loss = 11.03339050\n",
      "Iteration 7095, loss = 11.10094375\n",
      "Iteration 7096, loss = 11.15329087\n",
      "Iteration 7097, loss = 11.95105796\n",
      "Iteration 7098, loss = 11.98439543\n",
      "Iteration 7099, loss = 11.13277006\n",
      "Iteration 7100, loss = 11.04982072\n",
      "Iteration 7101, loss = 11.72392305\n",
      "Iteration 7102, loss = 10.99868181\n",
      "Iteration 7103, loss = 11.16810059\n",
      "Iteration 7104, loss = 10.76240345\n",
      "Iteration 7105, loss = 11.04817153\n",
      "Iteration 7106, loss = 11.16896448\n",
      "Iteration 7107, loss = 11.15477258\n",
      "Iteration 7108, loss = 11.60781753\n",
      "Iteration 7109, loss = 11.84615271\n",
      "Iteration 7110, loss = 12.04889300\n",
      "Iteration 7111, loss = 12.67887995\n",
      "Iteration 7112, loss = 11.53572578\n",
      "Iteration 7113, loss = 13.39812568\n",
      "Iteration 7114, loss = 12.09195445\n",
      "Iteration 7115, loss = 12.62190857\n",
      "Iteration 7116, loss = 11.45598631\n",
      "Iteration 7117, loss = 12.31663743\n",
      "Iteration 7118, loss = 14.00765793\n",
      "Iteration 7119, loss = 12.62259007\n",
      "Iteration 7120, loss = 11.76022341\n",
      "Iteration 7121, loss = 11.02453566\n",
      "Iteration 7122, loss = 10.79218409\n",
      "Iteration 7123, loss = 10.73054562\n",
      "Iteration 7124, loss = 10.84725940\n",
      "Iteration 7125, loss = 10.69251704\n",
      "Iteration 7126, loss = 10.84866852\n",
      "Iteration 7127, loss = 10.99826924\n",
      "Iteration 7128, loss = 10.64679946\n",
      "Iteration 7129, loss = 11.12003000\n",
      "Iteration 7130, loss = 11.21223739\n",
      "Iteration 7131, loss = 11.14475691\n",
      "Iteration 7132, loss = 10.98152480\n",
      "Iteration 7133, loss = 10.84010822\n",
      "Iteration 7134, loss = 10.65379951\n",
      "Iteration 7135, loss = 11.27264852\n",
      "Iteration 7136, loss = 11.01768899\n",
      "Iteration 7137, loss = 10.98053729\n",
      "Iteration 7138, loss = 10.77114724\n",
      "Iteration 7139, loss = 10.97696716\n",
      "Iteration 7140, loss = 11.01211349\n",
      "Iteration 7141, loss = 10.96179505\n",
      "Iteration 7142, loss = 10.86226421\n",
      "Iteration 7143, loss = 10.67545966\n",
      "Iteration 7144, loss = 10.65694679\n",
      "Iteration 7145, loss = 10.81400843\n",
      "Iteration 7146, loss = 11.34818670\n",
      "Iteration 7147, loss = 10.70202787\n",
      "Iteration 7148, loss = 10.71336228\n",
      "Iteration 7149, loss = 11.00658940\n",
      "Iteration 7150, loss = 10.93681521\n",
      "Iteration 7151, loss = 11.22612591\n",
      "Iteration 7152, loss = 10.95612887\n",
      "Iteration 7153, loss = 11.19311285\n",
      "Iteration 7154, loss = 10.82280392\n",
      "Iteration 7155, loss = 10.82587549\n",
      "Iteration 7156, loss = 11.02523705\n",
      "Iteration 7157, loss = 10.99540290\n",
      "Iteration 7158, loss = 11.17812680\n",
      "Iteration 7159, loss = 11.50365280\n",
      "Iteration 7160, loss = 11.81922633\n",
      "Iteration 7161, loss = 11.15309079\n",
      "Iteration 7162, loss = 10.84315743\n",
      "Iteration 7163, loss = 11.05680213\n",
      "Iteration 7164, loss = 10.79828692\n",
      "Iteration 7165, loss = 11.35022926\n",
      "Iteration 7166, loss = 11.17479723\n",
      "Iteration 7167, loss = 11.72640479\n",
      "Iteration 7168, loss = 11.38261667\n",
      "Iteration 7169, loss = 11.09950491\n",
      "Iteration 7170, loss = 11.25929387\n",
      "Iteration 7171, loss = 12.19638997\n",
      "Iteration 7172, loss = 11.19071514\n",
      "Iteration 7173, loss = 11.11450948\n",
      "Iteration 7174, loss = 10.63729714\n",
      "Iteration 7175, loss = 10.83168573\n",
      "Iteration 7176, loss = 11.91028724\n",
      "Iteration 7177, loss = 12.60696262\n",
      "Iteration 7178, loss = 11.55497964\n",
      "Iteration 7179, loss = 11.91651835\n",
      "Iteration 7180, loss = 11.38770145\n",
      "Iteration 7181, loss = 11.70461931\n",
      "Iteration 7182, loss = 10.85857446\n",
      "Iteration 7183, loss = 11.25170965\n",
      "Iteration 7184, loss = 11.09560508\n",
      "Iteration 7185, loss = 11.04550295\n",
      "Iteration 7186, loss = 10.67340719\n",
      "Iteration 7187, loss = 10.88118182\n",
      "Iteration 7188, loss = 11.13541004\n",
      "Iteration 7189, loss = 13.06368662\n",
      "Iteration 7190, loss = 11.45203359\n",
      "Iteration 7191, loss = 11.36998387\n",
      "Iteration 7192, loss = 11.24754484\n",
      "Iteration 7193, loss = 11.86616094\n",
      "Iteration 7194, loss = 11.46042266\n",
      "Iteration 7195, loss = 11.65759334\n",
      "Iteration 7196, loss = 10.96892105\n",
      "Iteration 7197, loss = 10.95704121\n",
      "Iteration 7198, loss = 10.99593020\n",
      "Iteration 7199, loss = 10.96511616\n",
      "Iteration 7200, loss = 10.72921354\n",
      "Iteration 7201, loss = 11.26753656\n",
      "Iteration 7202, loss = 11.45240241\n",
      "Iteration 7203, loss = 10.64624584\n",
      "Iteration 7204, loss = 10.72653276\n",
      "Iteration 7205, loss = 10.88953964\n",
      "Iteration 7206, loss = 10.86641202\n",
      "Iteration 7207, loss = 10.60459498\n",
      "Iteration 7208, loss = 10.76026965\n",
      "Iteration 7209, loss = 11.12625716\n",
      "Iteration 7210, loss = 10.97289356\n",
      "Iteration 7211, loss = 10.67169389\n",
      "Iteration 7212, loss = 10.73187452\n",
      "Iteration 7213, loss = 11.51376572\n",
      "Iteration 7214, loss = 11.82209899\n",
      "Iteration 7215, loss = 11.77504609\n",
      "Iteration 7216, loss = 12.03719381\n",
      "Iteration 7217, loss = 11.29339207\n",
      "Iteration 7218, loss = 11.08539608\n",
      "Iteration 7219, loss = 11.25481444\n",
      "Iteration 7220, loss = 11.74176045\n",
      "Iteration 7221, loss = 11.15591628\n",
      "Iteration 7222, loss = 12.10421000\n",
      "Iteration 7223, loss = 11.33113487\n",
      "Iteration 7224, loss = 10.83837556\n",
      "Iteration 7225, loss = 11.42201937\n",
      "Iteration 7226, loss = 12.72789624\n",
      "Iteration 7227, loss = 13.31966456\n",
      "Iteration 7228, loss = 11.66016792\n",
      "Iteration 7229, loss = 11.42152334\n",
      "Iteration 7230, loss = 10.88218230\n",
      "Iteration 7231, loss = 11.13558708\n",
      "Iteration 7232, loss = 11.54073588\n",
      "Iteration 7233, loss = 10.75709812\n",
      "Iteration 7234, loss = 11.22732761\n",
      "Iteration 7235, loss = 12.05943100\n",
      "Iteration 7236, loss = 11.43329932\n",
      "Iteration 7237, loss = 11.43617630\n",
      "Iteration 7238, loss = 11.05552293\n",
      "Iteration 7239, loss = 10.64004944\n",
      "Iteration 7240, loss = 10.64590436\n",
      "Iteration 7241, loss = 10.96433138\n",
      "Iteration 7242, loss = 11.16618699\n",
      "Iteration 7243, loss = 11.12061781\n",
      "Iteration 7244, loss = 10.97127214\n",
      "Iteration 7245, loss = 11.01796527\n",
      "Iteration 7246, loss = 11.09901727\n",
      "Iteration 7247, loss = 12.29656700\n",
      "Iteration 7248, loss = 12.34684117\n",
      "Iteration 7249, loss = 12.71221590\n",
      "Iteration 7250, loss = 12.02171730\n",
      "Iteration 7251, loss = 11.52165429\n",
      "Iteration 7252, loss = 11.02273977\n",
      "Iteration 7253, loss = 11.17136021\n",
      "Iteration 7254, loss = 10.99148041\n",
      "Iteration 7255, loss = 10.69975555\n",
      "Iteration 7256, loss = 10.86203991\n",
      "Iteration 7257, loss = 11.07248203\n",
      "Iteration 7258, loss = 10.91623535\n",
      "Iteration 7259, loss = 12.46616699\n",
      "Iteration 7260, loss = 12.37571225\n",
      "Iteration 7261, loss = 11.99305862\n",
      "Iteration 7262, loss = 11.03081588\n",
      "Iteration 7263, loss = 11.33589514\n",
      "Iteration 7264, loss = 11.31757711\n",
      "Iteration 7265, loss = 11.69164267\n",
      "Iteration 7266, loss = 11.40626945\n",
      "Iteration 7267, loss = 11.51325732\n",
      "Iteration 7268, loss = 11.25515840\n",
      "Iteration 7269, loss = 12.16859744\n",
      "Iteration 7270, loss = 11.29195672\n",
      "Iteration 7271, loss = 11.41704403\n",
      "Iteration 7272, loss = 11.61525775\n",
      "Iteration 7273, loss = 11.66984396\n",
      "Iteration 7274, loss = 10.73414760\n",
      "Iteration 7275, loss = 11.36690983\n",
      "Iteration 7276, loss = 11.19631812\n",
      "Iteration 7277, loss = 11.41925879\n",
      "Iteration 7278, loss = 11.31974178\n",
      "Iteration 7279, loss = 11.25470050\n",
      "Iteration 7280, loss = 11.87125372\n",
      "Iteration 7281, loss = 10.82999201\n",
      "Iteration 7282, loss = 11.39334727\n",
      "Iteration 7283, loss = 11.73047811\n",
      "Iteration 7284, loss = 11.38746039\n",
      "Iteration 7285, loss = 11.17486043\n",
      "Iteration 7286, loss = 11.63448607\n",
      "Iteration 7287, loss = 11.95199677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7288, loss = 11.64474839\n",
      "Iteration 7289, loss = 11.79910910\n",
      "Iteration 7290, loss = 11.67199215\n",
      "Iteration 7291, loss = 11.03899892\n",
      "Iteration 7292, loss = 11.32345210\n",
      "Iteration 7293, loss = 10.80641987\n",
      "Iteration 7294, loss = 10.75459254\n",
      "Iteration 7295, loss = 10.77734892\n",
      "Iteration 7296, loss = 11.33858914\n",
      "Iteration 7297, loss = 10.75293113\n",
      "Iteration 7298, loss = 10.90831343\n",
      "Iteration 7299, loss = 11.18469436\n",
      "Iteration 7300, loss = 11.23466938\n",
      "Iteration 7301, loss = 11.46681006\n",
      "Iteration 7302, loss = 10.65722685\n",
      "Iteration 7303, loss = 11.41831484\n",
      "Iteration 7304, loss = 11.22709985\n",
      "Iteration 7305, loss = 11.13268586\n",
      "Iteration 7306, loss = 10.81338035\n",
      "Iteration 7307, loss = 10.69881680\n",
      "Iteration 7308, loss = 10.51503219\n",
      "Iteration 7309, loss = 10.55112926\n",
      "Iteration 7310, loss = 10.83967425\n",
      "Iteration 7311, loss = 12.52429004\n",
      "Iteration 7312, loss = 11.14863543\n",
      "Iteration 7313, loss = 10.99301681\n",
      "Iteration 7314, loss = 11.15159220\n",
      "Iteration 7315, loss = 10.89274559\n",
      "Iteration 7316, loss = 12.19957576\n",
      "Iteration 7317, loss = 11.20122244\n",
      "Iteration 7318, loss = 11.47484864\n",
      "Iteration 7319, loss = 11.07044660\n",
      "Iteration 7320, loss = 11.06159736\n",
      "Iteration 7321, loss = 10.68637514\n",
      "Iteration 7322, loss = 11.16129646\n",
      "Iteration 7323, loss = 10.88972488\n",
      "Iteration 7324, loss = 12.22908596\n",
      "Iteration 7325, loss = 12.00135394\n",
      "Iteration 7326, loss = 12.35176259\n",
      "Iteration 7327, loss = 11.25631940\n",
      "Iteration 7328, loss = 11.29450064\n",
      "Iteration 7329, loss = 11.99695594\n",
      "Iteration 7330, loss = 11.62734076\n",
      "Iteration 7331, loss = 13.43913613\n",
      "Iteration 7332, loss = 12.94932672\n",
      "Iteration 7333, loss = 15.01914719\n",
      "Iteration 7334, loss = 13.82743779\n",
      "Iteration 7335, loss = 15.34720660\n",
      "Iteration 7336, loss = 12.23658311\n",
      "Iteration 7337, loss = 13.87887366\n",
      "Iteration 7338, loss = 12.25547478\n",
      "Iteration 7339, loss = 10.89142256\n",
      "Iteration 7340, loss = 11.23827695\n",
      "Iteration 7341, loss = 10.74287683\n",
      "Iteration 7342, loss = 11.01677279\n",
      "Iteration 7343, loss = 10.80270365\n",
      "Iteration 7344, loss = 10.46617606\n",
      "Iteration 7345, loss = 10.57528815\n",
      "Iteration 7346, loss = 11.05647211\n",
      "Iteration 7347, loss = 11.78546276\n",
      "Iteration 7348, loss = 11.10473267\n",
      "Iteration 7349, loss = 12.01712858\n",
      "Iteration 7350, loss = 12.22563051\n",
      "Iteration 7351, loss = 11.97699596\n",
      "Iteration 7352, loss = 11.88257824\n",
      "Iteration 7353, loss = 12.01838879\n",
      "Iteration 7354, loss = 11.22155881\n",
      "Iteration 7355, loss = 11.55279966\n",
      "Iteration 7356, loss = 11.95898982\n",
      "Iteration 7357, loss = 10.87428592\n",
      "Iteration 7358, loss = 10.74820039\n",
      "Iteration 7359, loss = 10.68701819\n",
      "Iteration 7360, loss = 11.48778859\n",
      "Iteration 7361, loss = 10.99291671\n",
      "Iteration 7362, loss = 11.87467990\n",
      "Iteration 7363, loss = 11.24863139\n",
      "Iteration 7364, loss = 11.76041388\n",
      "Iteration 7365, loss = 13.01769283\n",
      "Iteration 7366, loss = 12.24788617\n",
      "Iteration 7367, loss = 11.94646371\n",
      "Iteration 7368, loss = 11.54543443\n",
      "Iteration 7369, loss = 11.48609896\n",
      "Iteration 7370, loss = 11.43506873\n",
      "Iteration 7371, loss = 11.16996672\n",
      "Iteration 7372, loss = 10.78329308\n",
      "Iteration 7373, loss = 10.83757350\n",
      "Iteration 7374, loss = 10.73506906\n",
      "Iteration 7375, loss = 11.03117905\n",
      "Iteration 7376, loss = 11.29421197\n",
      "Iteration 7377, loss = 11.98158411\n",
      "Iteration 7378, loss = 11.98760592\n",
      "Iteration 7379, loss = 11.44926155\n",
      "Iteration 7380, loss = 11.40198847\n",
      "Iteration 7381, loss = 10.52936511\n",
      "Iteration 7382, loss = 10.87636841\n",
      "Iteration 7383, loss = 11.39483184\n",
      "Iteration 7384, loss = 11.30794812\n",
      "Iteration 7385, loss = 11.45132610\n",
      "Iteration 7386, loss = 12.13552685\n",
      "Iteration 7387, loss = 11.31881895\n",
      "Iteration 7388, loss = 11.36140610\n",
      "Iteration 7389, loss = 11.91841643\n",
      "Iteration 7390, loss = 14.39934327\n",
      "Iteration 7391, loss = 12.09149706\n",
      "Iteration 7392, loss = 11.33424848\n",
      "Iteration 7393, loss = 11.08657672\n",
      "Iteration 7394, loss = 10.86041787\n",
      "Iteration 7395, loss = 10.97797054\n",
      "Iteration 7396, loss = 11.11039922\n",
      "Iteration 7397, loss = 10.83255292\n",
      "Iteration 7398, loss = 11.25848651\n",
      "Iteration 7399, loss = 11.45472232\n",
      "Iteration 7400, loss = 11.15913840\n",
      "Iteration 7401, loss = 11.22967007\n",
      "Iteration 7402, loss = 11.25413169\n",
      "Iteration 7403, loss = 10.57194055\n",
      "Iteration 7404, loss = 10.87861357\n",
      "Iteration 7405, loss = 10.51811534\n",
      "Iteration 7406, loss = 11.24343308\n",
      "Iteration 7407, loss = 10.91896508\n",
      "Iteration 7408, loss = 11.08049703\n",
      "Iteration 7409, loss = 11.02716589\n",
      "Iteration 7410, loss = 10.71860555\n",
      "Iteration 7411, loss = 10.67209137\n",
      "Iteration 7412, loss = 10.66330327\n",
      "Iteration 7413, loss = 10.44459645\n",
      "Iteration 7414, loss = 10.47414678\n",
      "Iteration 7415, loss = 10.52096244\n",
      "Iteration 7416, loss = 11.62326401\n",
      "Iteration 7417, loss = 10.88490483\n",
      "Iteration 7418, loss = 10.83432284\n",
      "Iteration 7419, loss = 11.31994559\n",
      "Iteration 7420, loss = 11.17874778\n",
      "Iteration 7421, loss = 10.61865491\n",
      "Iteration 7422, loss = 10.87256448\n",
      "Iteration 7423, loss = 10.90689434\n",
      "Iteration 7424, loss = 10.51710288\n",
      "Iteration 7425, loss = 11.16414361\n",
      "Iteration 7426, loss = 10.63106500\n",
      "Iteration 7427, loss = 10.73421752\n",
      "Iteration 7428, loss = 10.77696975\n",
      "Iteration 7429, loss = 11.34971515\n",
      "Iteration 7430, loss = 10.40693986\n",
      "Iteration 7431, loss = 10.53949337\n",
      "Iteration 7432, loss = 10.69183006\n",
      "Iteration 7433, loss = 10.79637864\n",
      "Iteration 7434, loss = 10.95980325\n",
      "Iteration 7435, loss = 10.48614675\n",
      "Iteration 7436, loss = 10.51692609\n",
      "Iteration 7437, loss = 10.96595882\n",
      "Iteration 7438, loss = 11.61218593\n",
      "Iteration 7439, loss = 10.49974434\n",
      "Iteration 7440, loss = 11.02553165\n",
      "Iteration 7441, loss = 11.61516078\n",
      "Iteration 7442, loss = 11.22417732\n",
      "Iteration 7443, loss = 11.91984168\n",
      "Iteration 7444, loss = 11.84817822\n",
      "Iteration 7445, loss = 11.52118490\n",
      "Iteration 7446, loss = 10.77211646\n",
      "Iteration 7447, loss = 10.64928500\n",
      "Iteration 7448, loss = 10.56167443\n",
      "Iteration 7449, loss = 12.23865085\n",
      "Iteration 7450, loss = 11.55053748\n",
      "Iteration 7451, loss = 12.92090155\n",
      "Iteration 7452, loss = 11.75409219\n",
      "Iteration 7453, loss = 11.68583276\n",
      "Iteration 7454, loss = 12.78275707\n",
      "Iteration 7455, loss = 14.46344053\n",
      "Iteration 7456, loss = 13.65820636\n",
      "Iteration 7457, loss = 13.38088143\n",
      "Iteration 7458, loss = 12.31090901\n",
      "Iteration 7459, loss = 12.10260842\n",
      "Iteration 7460, loss = 14.35343660\n",
      "Iteration 7461, loss = 11.17711455\n",
      "Iteration 7462, loss = 10.65422025\n",
      "Iteration 7463, loss = 11.11673684\n",
      "Iteration 7464, loss = 10.94208660\n",
      "Iteration 7465, loss = 11.17084331\n",
      "Iteration 7466, loss = 10.79373978\n",
      "Iteration 7467, loss = 11.26819099\n",
      "Iteration 7468, loss = 11.63118253\n",
      "Iteration 7469, loss = 10.62660659\n",
      "Iteration 7470, loss = 10.67318726\n",
      "Iteration 7471, loss = 11.42779664\n",
      "Iteration 7472, loss = 11.55232035\n",
      "Iteration 7473, loss = 11.88135355\n",
      "Iteration 7474, loss = 12.72689199\n",
      "Iteration 7475, loss = 12.12329131\n",
      "Iteration 7476, loss = 11.69742462\n",
      "Iteration 7477, loss = 11.26834345\n",
      "Iteration 7478, loss = 10.86333757\n",
      "Iteration 7479, loss = 10.72875058\n",
      "Iteration 7480, loss = 10.89029917\n",
      "Iteration 7481, loss = 10.52944782\n",
      "Iteration 7482, loss = 10.50891174\n",
      "Iteration 7483, loss = 11.08860616\n",
      "Iteration 7484, loss = 10.35325322\n",
      "Iteration 7485, loss = 11.21257585\n",
      "Iteration 7486, loss = 10.82962274\n",
      "Iteration 7487, loss = 10.79273558\n",
      "Iteration 7488, loss = 11.23469247\n",
      "Iteration 7489, loss = 10.32427312\n",
      "Iteration 7490, loss = 10.54242062\n",
      "Iteration 7491, loss = 10.57598895\n",
      "Iteration 7492, loss = 10.56809667\n",
      "Iteration 7493, loss = 10.55139364\n",
      "Iteration 7494, loss = 10.51918345\n",
      "Iteration 7495, loss = 10.67703135\n",
      "Iteration 7496, loss = 10.87036746\n",
      "Iteration 7497, loss = 11.21833332\n",
      "Iteration 7498, loss = 11.12859417\n",
      "Iteration 7499, loss = 11.27189766\n",
      "Iteration 7500, loss = 11.73221466\n",
      "Iteration 7501, loss = 10.86306993\n",
      "Iteration 7502, loss = 10.89537988\n",
      "Iteration 7503, loss = 10.66742205\n",
      "Iteration 7504, loss = 10.76349899\n",
      "Iteration 7505, loss = 10.42804074\n",
      "Iteration 7506, loss = 11.08311156\n",
      "Iteration 7507, loss = 11.28093171\n",
      "Iteration 7508, loss = 11.07236432\n",
      "Iteration 7509, loss = 10.81349767\n",
      "Iteration 7510, loss = 10.91369579\n",
      "Iteration 7511, loss = 11.45527145\n",
      "Iteration 7512, loss = 11.23269742\n",
      "Iteration 7513, loss = 10.75127305\n",
      "Iteration 7514, loss = 10.35397641\n",
      "Iteration 7515, loss = 10.91497357\n",
      "Iteration 7516, loss = 10.72296624\n",
      "Iteration 7517, loss = 11.23350966\n",
      "Iteration 7518, loss = 11.83526766\n",
      "Iteration 7519, loss = 11.37440995\n",
      "Iteration 7520, loss = 12.19083435\n",
      "Iteration 7521, loss = 12.44693811\n",
      "Iteration 7522, loss = 12.33232148\n",
      "Iteration 7523, loss = 11.24722427\n",
      "Iteration 7524, loss = 11.31478523\n",
      "Iteration 7525, loss = 10.91837764\n",
      "Iteration 7526, loss = 11.28350889\n",
      "Iteration 7527, loss = 11.00773134\n",
      "Iteration 7528, loss = 10.92340393\n",
      "Iteration 7529, loss = 10.32391614\n",
      "Iteration 7530, loss = 10.73150061\n",
      "Iteration 7531, loss = 10.60016069\n",
      "Iteration 7532, loss = 10.57835502\n",
      "Iteration 7533, loss = 10.53851868\n",
      "Iteration 7534, loss = 10.36822459\n",
      "Iteration 7535, loss = 10.72963122\n",
      "Iteration 7536, loss = 11.14806949\n",
      "Iteration 7537, loss = 10.55284323\n",
      "Iteration 7538, loss = 10.84197645\n",
      "Iteration 7539, loss = 10.72741741\n",
      "Iteration 7540, loss = 10.63062976\n",
      "Iteration 7541, loss = 10.89475521\n",
      "Iteration 7542, loss = 10.72581449\n",
      "Iteration 7543, loss = 11.67894772\n",
      "Iteration 7544, loss = 10.58905975\n",
      "Iteration 7545, loss = 10.61794953\n",
      "Iteration 7546, loss = 11.27610069\n",
      "Iteration 7547, loss = 10.78551917\n",
      "Iteration 7548, loss = 11.95582939\n",
      "Iteration 7549, loss = 12.58304328\n",
      "Iteration 7550, loss = 10.71371648\n",
      "Iteration 7551, loss = 10.95039188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7552, loss = 10.91600256\n",
      "Iteration 7553, loss = 11.15546408\n",
      "Iteration 7554, loss = 10.49305206\n",
      "Iteration 7555, loss = 11.42945839\n",
      "Iteration 7556, loss = 10.73404970\n",
      "Iteration 7557, loss = 10.88097833\n",
      "Iteration 7558, loss = 10.78189683\n",
      "Iteration 7559, loss = 10.70981032\n",
      "Iteration 7560, loss = 11.00099752\n",
      "Iteration 7561, loss = 10.42789434\n",
      "Iteration 7562, loss = 10.74261184\n",
      "Iteration 7563, loss = 11.01954392\n",
      "Iteration 7564, loss = 11.00396357\n",
      "Iteration 7565, loss = 10.64277425\n",
      "Iteration 7566, loss = 10.72360698\n",
      "Iteration 7567, loss = 10.80178469\n",
      "Iteration 7568, loss = 10.83750122\n",
      "Iteration 7569, loss = 11.67306595\n",
      "Iteration 7570, loss = 10.44269355\n",
      "Iteration 7571, loss = 10.67655976\n",
      "Iteration 7572, loss = 10.97707962\n",
      "Iteration 7573, loss = 10.74298572\n",
      "Iteration 7574, loss = 10.74627355\n",
      "Iteration 7575, loss = 10.44625028\n",
      "Iteration 7576, loss = 10.74953899\n",
      "Iteration 7577, loss = 10.77238187\n",
      "Iteration 7578, loss = 11.10630181\n",
      "Iteration 7579, loss = 10.92912336\n",
      "Iteration 7580, loss = 10.46411307\n",
      "Iteration 7581, loss = 10.60134175\n",
      "Iteration 7582, loss = 10.24626119\n",
      "Iteration 7583, loss = 10.74257367\n",
      "Iteration 7584, loss = 10.85469286\n",
      "Iteration 7585, loss = 10.64083296\n",
      "Iteration 7586, loss = 11.07312006\n",
      "Iteration 7587, loss = 10.58248642\n",
      "Iteration 7588, loss = 10.46400823\n",
      "Iteration 7589, loss = 10.49469546\n",
      "Iteration 7590, loss = 11.28253019\n",
      "Iteration 7591, loss = 11.68350023\n",
      "Iteration 7592, loss = 11.65724690\n",
      "Iteration 7593, loss = 12.33921375\n",
      "Iteration 7594, loss = 12.22325089\n",
      "Iteration 7595, loss = 12.64031261\n",
      "Iteration 7596, loss = 11.16635321\n",
      "Iteration 7597, loss = 10.55715097\n",
      "Iteration 7598, loss = 10.95373711\n",
      "Iteration 7599, loss = 11.84070468\n",
      "Iteration 7600, loss = 11.25713047\n",
      "Iteration 7601, loss = 10.58217738\n",
      "Iteration 7602, loss = 11.76133970\n",
      "Iteration 7603, loss = 10.34588606\n",
      "Iteration 7604, loss = 11.66075641\n",
      "Iteration 7605, loss = 10.86044577\n",
      "Iteration 7606, loss = 10.98749372\n",
      "Iteration 7607, loss = 11.50531937\n",
      "Iteration 7608, loss = 11.03998107\n",
      "Iteration 7609, loss = 10.84115423\n",
      "Iteration 7610, loss = 11.29874724\n",
      "Iteration 7611, loss = 11.63993535\n",
      "Iteration 7612, loss = 11.73720683\n",
      "Iteration 7613, loss = 11.56555111\n",
      "Iteration 7614, loss = 11.45891831\n",
      "Iteration 7615, loss = 10.67340604\n",
      "Iteration 7616, loss = 11.23538866\n",
      "Iteration 7617, loss = 12.43057329\n",
      "Iteration 7618, loss = 11.34302767\n",
      "Iteration 7619, loss = 11.10764563\n",
      "Iteration 7620, loss = 11.62518802\n",
      "Iteration 7621, loss = 11.31335100\n",
      "Iteration 7622, loss = 11.03637474\n",
      "Iteration 7623, loss = 10.47385617\n",
      "Iteration 7624, loss = 10.42940445\n",
      "Iteration 7625, loss = 10.90448967\n",
      "Iteration 7626, loss = 11.15677961\n",
      "Iteration 7627, loss = 10.90253380\n",
      "Iteration 7628, loss = 10.68404943\n",
      "Iteration 7629, loss = 10.32443744\n",
      "Iteration 7630, loss = 10.43169150\n",
      "Iteration 7631, loss = 10.34573843\n",
      "Iteration 7632, loss = 10.35657765\n",
      "Iteration 7633, loss = 10.46522096\n",
      "Iteration 7634, loss = 10.43586865\n",
      "Iteration 7635, loss = 10.38632148\n",
      "Iteration 7636, loss = 10.33820974\n",
      "Iteration 7637, loss = 10.80350540\n",
      "Iteration 7638, loss = 10.49572991\n",
      "Iteration 7639, loss = 11.56523080\n",
      "Iteration 7640, loss = 11.02769806\n",
      "Iteration 7641, loss = 11.33386359\n",
      "Iteration 7642, loss = 10.55577964\n",
      "Iteration 7643, loss = 10.71839684\n",
      "Iteration 7644, loss = 10.58306641\n",
      "Iteration 7645, loss = 10.41152618\n",
      "Iteration 7646, loss = 10.61160769\n",
      "Iteration 7647, loss = 10.21000571\n",
      "Iteration 7648, loss = 10.48085123\n",
      "Iteration 7649, loss = 10.49719576\n",
      "Iteration 7650, loss = 10.43878914\n",
      "Iteration 7651, loss = 10.77278756\n",
      "Iteration 7652, loss = 11.63452475\n",
      "Iteration 7653, loss = 11.84036553\n",
      "Iteration 7654, loss = 13.13965080\n",
      "Iteration 7655, loss = 12.25207294\n",
      "Iteration 7656, loss = 11.80787388\n",
      "Iteration 7657, loss = 10.48866503\n",
      "Iteration 7658, loss = 10.71261125\n",
      "Iteration 7659, loss = 10.80263062\n",
      "Iteration 7660, loss = 11.14130497\n",
      "Iteration 7661, loss = 12.27393953\n",
      "Iteration 7662, loss = 11.30125269\n",
      "Iteration 7663, loss = 10.91112564\n",
      "Iteration 7664, loss = 10.43573804\n",
      "Iteration 7665, loss = 10.93433004\n",
      "Iteration 7666, loss = 10.44953454\n",
      "Iteration 7667, loss = 10.36442416\n",
      "Iteration 7668, loss = 10.72057412\n",
      "Iteration 7669, loss = 10.27012155\n",
      "Iteration 7670, loss = 10.70363388\n",
      "Iteration 7671, loss = 10.24881586\n",
      "Iteration 7672, loss = 10.21281539\n",
      "Iteration 7673, loss = 10.56454478\n",
      "Iteration 7674, loss = 10.26704512\n",
      "Iteration 7675, loss = 10.55332603\n",
      "Iteration 7676, loss = 10.50597331\n",
      "Iteration 7677, loss = 11.12196842\n",
      "Iteration 7678, loss = 10.72221774\n",
      "Iteration 7679, loss = 10.50507380\n",
      "Iteration 7680, loss = 10.90762758\n",
      "Iteration 7681, loss = 10.62629390\n",
      "Iteration 7682, loss = 11.12549268\n",
      "Iteration 7683, loss = 10.27010017\n",
      "Iteration 7684, loss = 10.61848541\n",
      "Iteration 7685, loss = 10.33657267\n",
      "Iteration 7686, loss = 10.56151255\n",
      "Iteration 7687, loss = 10.57200546\n",
      "Iteration 7688, loss = 10.35939576\n",
      "Iteration 7689, loss = 11.02989454\n",
      "Iteration 7690, loss = 10.62447880\n",
      "Iteration 7691, loss = 10.68582191\n",
      "Iteration 7692, loss = 10.83901759\n",
      "Iteration 7693, loss = 10.88808081\n",
      "Iteration 7694, loss = 10.37163041\n",
      "Iteration 7695, loss = 10.62352792\n",
      "Iteration 7696, loss = 10.95747339\n",
      "Iteration 7697, loss = 11.41899047\n",
      "Iteration 7698, loss = 10.58031791\n",
      "Iteration 7699, loss = 10.93605083\n",
      "Iteration 7700, loss = 10.56857372\n",
      "Iteration 7701, loss = 10.82630826\n",
      "Iteration 7702, loss = 10.72517349\n",
      "Iteration 7703, loss = 10.19962721\n",
      "Iteration 7704, loss = 10.29669751\n",
      "Iteration 7705, loss = 10.62304704\n",
      "Iteration 7706, loss = 10.24870279\n",
      "Iteration 7707, loss = 10.33622630\n",
      "Iteration 7708, loss = 10.95012893\n",
      "Iteration 7709, loss = 11.17154258\n",
      "Iteration 7710, loss = 10.59118034\n",
      "Iteration 7711, loss = 10.92351692\n",
      "Iteration 7712, loss = 10.73018749\n",
      "Iteration 7713, loss = 11.48243332\n",
      "Iteration 7714, loss = 11.25233057\n",
      "Iteration 7715, loss = 10.50644157\n",
      "Iteration 7716, loss = 10.45069843\n",
      "Iteration 7717, loss = 10.82024448\n",
      "Iteration 7718, loss = 11.07013046\n",
      "Iteration 7719, loss = 10.80421612\n",
      "Iteration 7720, loss = 11.24192769\n",
      "Iteration 7721, loss = 10.76141681\n",
      "Iteration 7722, loss = 10.97577860\n",
      "Iteration 7723, loss = 10.73164433\n",
      "Iteration 7724, loss = 10.31820554\n",
      "Iteration 7725, loss = 10.52017434\n",
      "Iteration 7726, loss = 11.29970277\n",
      "Iteration 7727, loss = 11.48057705\n",
      "Iteration 7728, loss = 10.41547795\n",
      "Iteration 7729, loss = 10.83755106\n",
      "Iteration 7730, loss = 10.47816219\n",
      "Iteration 7731, loss = 10.38555479\n",
      "Iteration 7732, loss = 10.56689944\n",
      "Iteration 7733, loss = 10.93097516\n",
      "Iteration 7734, loss = 11.81078102\n",
      "Iteration 7735, loss = 11.18116466\n",
      "Iteration 7736, loss = 11.40843041\n",
      "Iteration 7737, loss = 11.43088947\n",
      "Iteration 7738, loss = 10.91988437\n",
      "Iteration 7739, loss = 10.59423644\n",
      "Iteration 7740, loss = 10.83016124\n",
      "Iteration 7741, loss = 10.56093162\n",
      "Iteration 7742, loss = 10.52652002\n",
      "Iteration 7743, loss = 10.34665954\n",
      "Iteration 7744, loss = 10.50382164\n",
      "Iteration 7745, loss = 10.37817466\n",
      "Iteration 7746, loss = 10.44896984\n",
      "Iteration 7747, loss = 10.89554080\n",
      "Iteration 7748, loss = 11.21902171\n",
      "Iteration 7749, loss = 10.76563554\n",
      "Iteration 7750, loss = 12.13527038\n",
      "Iteration 7751, loss = 11.13620364\n",
      "Iteration 7752, loss = 10.53789798\n",
      "Iteration 7753, loss = 10.93923959\n",
      "Iteration 7754, loss = 10.99050895\n",
      "Iteration 7755, loss = 11.03381214\n",
      "Iteration 7756, loss = 10.85955045\n",
      "Iteration 7757, loss = 10.22581059\n",
      "Iteration 7758, loss = 10.79608360\n",
      "Iteration 7759, loss = 10.52985465\n",
      "Iteration 7760, loss = 10.83622326\n",
      "Iteration 7761, loss = 10.51934479\n",
      "Iteration 7762, loss = 10.74532095\n",
      "Iteration 7763, loss = 10.76365266\n",
      "Iteration 7764, loss = 10.58705644\n",
      "Iteration 7765, loss = 12.03316365\n",
      "Iteration 7766, loss = 12.22095450\n",
      "Iteration 7767, loss = 10.66963783\n",
      "Iteration 7768, loss = 10.44569224\n",
      "Iteration 7769, loss = 10.38162658\n",
      "Iteration 7770, loss = 11.15189787\n",
      "Iteration 7771, loss = 10.45551834\n",
      "Iteration 7772, loss = 10.77842394\n",
      "Iteration 7773, loss = 11.10496564\n",
      "Iteration 7774, loss = 10.83899205\n",
      "Iteration 7775, loss = 12.48858322\n",
      "Iteration 7776, loss = 11.69157260\n",
      "Iteration 7777, loss = 11.02793146\n",
      "Iteration 7778, loss = 12.64998741\n",
      "Iteration 7779, loss = 12.19893404\n",
      "Iteration 7780, loss = 13.50374427\n",
      "Iteration 7781, loss = 12.80889499\n",
      "Iteration 7782, loss = 13.09744192\n",
      "Iteration 7783, loss = 13.71211693\n",
      "Iteration 7784, loss = 12.25862110\n",
      "Iteration 7785, loss = 11.50462991\n",
      "Iteration 7786, loss = 11.30611267\n",
      "Iteration 7787, loss = 10.94912207\n",
      "Iteration 7788, loss = 10.52849280\n",
      "Iteration 7789, loss = 10.33755259\n",
      "Iteration 7790, loss = 10.34352255\n",
      "Iteration 7791, loss = 10.37049432\n",
      "Iteration 7792, loss = 10.82689472\n",
      "Iteration 7793, loss = 11.14127587\n",
      "Iteration 7794, loss = 10.10071143\n",
      "Iteration 7795, loss = 10.75265199\n",
      "Iteration 7796, loss = 11.64202092\n",
      "Iteration 7797, loss = 10.65021995\n",
      "Iteration 7798, loss = 10.33184991\n",
      "Iteration 7799, loss = 10.29282815\n",
      "Iteration 7800, loss = 10.63820102\n",
      "Iteration 7801, loss = 10.70564151\n",
      "Iteration 7802, loss = 10.57401184\n",
      "Iteration 7803, loss = 10.91219936\n",
      "Iteration 7804, loss = 10.48418222\n",
      "Iteration 7805, loss = 10.39497381\n",
      "Iteration 7806, loss = 10.51807413\n",
      "Iteration 7807, loss = 10.25998498\n",
      "Iteration 7808, loss = 10.46434084\n",
      "Iteration 7809, loss = 10.90371311\n",
      "Iteration 7810, loss = 10.41397892\n",
      "Iteration 7811, loss = 11.49436075\n",
      "Iteration 7812, loss = 10.55427704\n",
      "Iteration 7813, loss = 10.91846687\n",
      "Iteration 7814, loss = 11.21672923\n",
      "Iteration 7815, loss = 10.20746512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7816, loss = 10.44377330\n",
      "Iteration 7817, loss = 10.55266257\n",
      "Iteration 7818, loss = 10.38829891\n",
      "Iteration 7819, loss = 10.77522380\n",
      "Iteration 7820, loss = 10.60610446\n",
      "Iteration 7821, loss = 10.44103826\n",
      "Iteration 7822, loss = 10.29194648\n",
      "Iteration 7823, loss = 10.70898924\n",
      "Iteration 7824, loss = 11.49347347\n",
      "Iteration 7825, loss = 10.28331793\n",
      "Iteration 7826, loss = 10.87550253\n",
      "Iteration 7827, loss = 10.80689490\n",
      "Iteration 7828, loss = 10.57280659\n",
      "Iteration 7829, loss = 10.33556892\n",
      "Iteration 7830, loss = 10.25966055\n",
      "Iteration 7831, loss = 10.26384650\n",
      "Iteration 7832, loss = 10.74894521\n",
      "Iteration 7833, loss = 10.43785562\n",
      "Iteration 7834, loss = 10.54041708\n",
      "Iteration 7835, loss = 10.28023956\n",
      "Iteration 7836, loss = 10.65212228\n",
      "Iteration 7837, loss = 10.19731074\n",
      "Iteration 7838, loss = 10.62798254\n",
      "Iteration 7839, loss = 10.30840927\n",
      "Iteration 7840, loss = 10.59335356\n",
      "Iteration 7841, loss = 10.54368973\n",
      "Iteration 7842, loss = 10.25134594\n",
      "Iteration 7843, loss = 10.33827096\n",
      "Iteration 7844, loss = 10.66449062\n",
      "Iteration 7845, loss = 11.04520402\n",
      "Iteration 7846, loss = 10.69000931\n",
      "Iteration 7847, loss = 10.42158801\n",
      "Iteration 7848, loss = 10.96770411\n",
      "Iteration 7849, loss = 10.20183692\n",
      "Iteration 7850, loss = 10.72768458\n",
      "Iteration 7851, loss = 10.47961665\n",
      "Iteration 7852, loss = 11.00749021\n",
      "Iteration 7853, loss = 11.49175942\n",
      "Iteration 7854, loss = 10.83059862\n",
      "Iteration 7855, loss = 10.77166665\n",
      "Iteration 7856, loss = 11.19968427\n",
      "Iteration 7857, loss = 11.54432548\n",
      "Iteration 7858, loss = 11.23434235\n",
      "Iteration 7859, loss = 12.12358463\n",
      "Iteration 7860, loss = 11.42080091\n",
      "Iteration 7861, loss = 13.61448889\n",
      "Iteration 7862, loss = 12.21836017\n",
      "Iteration 7863, loss = 11.79102842\n",
      "Iteration 7864, loss = 11.27668931\n",
      "Iteration 7865, loss = 11.57491595\n",
      "Iteration 7866, loss = 10.30633990\n",
      "Iteration 7867, loss = 11.17203445\n",
      "Iteration 7868, loss = 11.59090828\n",
      "Iteration 7869, loss = 10.43607276\n",
      "Iteration 7870, loss = 10.72634164\n",
      "Iteration 7871, loss = 10.58322659\n",
      "Iteration 7872, loss = 10.33292163\n",
      "Iteration 7873, loss = 11.09693574\n",
      "Iteration 7874, loss = 10.83988606\n",
      "Iteration 7875, loss = 10.36665121\n",
      "Iteration 7876, loss = 10.82629376\n",
      "Iteration 7877, loss = 10.64797016\n",
      "Iteration 7878, loss = 10.56281354\n",
      "Iteration 7879, loss = 10.96389021\n",
      "Iteration 7880, loss = 10.36170742\n",
      "Iteration 7881, loss = 10.41513366\n",
      "Iteration 7882, loss = 10.45069563\n",
      "Iteration 7883, loss = 10.62775230\n",
      "Iteration 7884, loss = 10.82410642\n",
      "Iteration 7885, loss = 11.16770619\n",
      "Iteration 7886, loss = 11.44981714\n",
      "Iteration 7887, loss = 10.94733365\n",
      "Iteration 7888, loss = 10.58669783\n",
      "Iteration 7889, loss = 11.70759862\n",
      "Iteration 7890, loss = 11.12794539\n",
      "Iteration 7891, loss = 11.21167427\n",
      "Iteration 7892, loss = 12.21803223\n",
      "Iteration 7893, loss = 11.21075744\n",
      "Iteration 7894, loss = 11.39610475\n",
      "Iteration 7895, loss = 10.47085538\n",
      "Iteration 7896, loss = 10.76967374\n",
      "Iteration 7897, loss = 10.69018658\n",
      "Iteration 7898, loss = 10.92667594\n",
      "Iteration 7899, loss = 10.52420985\n",
      "Iteration 7900, loss = 10.45829849\n",
      "Iteration 7901, loss = 11.35724466\n",
      "Iteration 7902, loss = 10.66195960\n",
      "Iteration 7903, loss = 10.51613372\n",
      "Iteration 7904, loss = 10.71809195\n",
      "Iteration 7905, loss = 10.12531052\n",
      "Iteration 7906, loss = 11.48064548\n",
      "Iteration 7907, loss = 10.90654843\n",
      "Iteration 7908, loss = 10.67777597\n",
      "Iteration 7909, loss = 10.27474635\n",
      "Iteration 7910, loss = 10.48376811\n",
      "Iteration 7911, loss = 10.15961086\n",
      "Iteration 7912, loss = 10.29598736\n",
      "Iteration 7913, loss = 10.15579220\n",
      "Iteration 7914, loss = 10.16895838\n",
      "Iteration 7915, loss = 10.24318378\n",
      "Iteration 7916, loss = 10.19573165\n",
      "Iteration 7917, loss = 11.71907250\n",
      "Iteration 7918, loss = 13.32378702\n",
      "Iteration 7919, loss = 12.85697548\n",
      "Iteration 7920, loss = 14.23916553\n",
      "Iteration 7921, loss = 12.95761504\n",
      "Iteration 7922, loss = 11.37401590\n",
      "Iteration 7923, loss = 11.62245860\n",
      "Iteration 7924, loss = 12.78003584\n",
      "Iteration 7925, loss = 13.15748531\n",
      "Iteration 7926, loss = 13.70392275\n",
      "Iteration 7927, loss = 15.80708985\n",
      "Iteration 7928, loss = 14.22048092\n",
      "Iteration 7929, loss = 15.52022296\n",
      "Iteration 7930, loss = 14.56566814\n",
      "Iteration 7931, loss = 11.83610245\n",
      "Iteration 7932, loss = 11.46054495\n",
      "Iteration 7933, loss = 10.69074115\n",
      "Iteration 7934, loss = 10.97324351\n",
      "Iteration 7935, loss = 10.62313972\n",
      "Iteration 7936, loss = 10.40565376\n",
      "Iteration 7937, loss = 10.56856977\n",
      "Iteration 7938, loss = 11.39667924\n",
      "Iteration 7939, loss = 10.60449420\n",
      "Iteration 7940, loss = 11.15165052\n",
      "Iteration 7941, loss = 11.14660664\n",
      "Iteration 7942, loss = 11.07066286\n",
      "Iteration 7943, loss = 11.71191212\n",
      "Iteration 7944, loss = 10.99474717\n",
      "Iteration 7945, loss = 11.39065604\n",
      "Iteration 7946, loss = 11.19607756\n",
      "Iteration 7947, loss = 11.02432679\n",
      "Iteration 7948, loss = 11.89647804\n",
      "Iteration 7949, loss = 11.85180305\n",
      "Iteration 7950, loss = 11.62350353\n",
      "Iteration 7951, loss = 10.78414116\n",
      "Iteration 7952, loss = 10.67185565\n",
      "Iteration 7953, loss = 10.32679583\n",
      "Iteration 7954, loss = 10.46675614\n",
      "Iteration 7955, loss = 10.25558838\n",
      "Iteration 7956, loss = 10.75252753\n",
      "Iteration 7957, loss = 10.47675961\n",
      "Iteration 7958, loss = 10.65818048\n",
      "Iteration 7959, loss = 10.92359871\n",
      "Iteration 7960, loss = 10.73270999\n",
      "Iteration 7961, loss = 10.35519575\n",
      "Iteration 7962, loss = 10.38756965\n",
      "Iteration 7963, loss = 10.06455130\n",
      "Iteration 7964, loss = 10.49570862\n",
      "Iteration 7965, loss = 10.45694657\n",
      "Iteration 7966, loss = 10.31028121\n",
      "Iteration 7967, loss = 10.21990846\n",
      "Iteration 7968, loss = 10.32619311\n",
      "Iteration 7969, loss = 10.04068565\n",
      "Iteration 7970, loss = 10.83407681\n",
      "Iteration 7971, loss = 10.77777532\n",
      "Iteration 7972, loss = 10.17617314\n",
      "Iteration 7973, loss = 10.37170478\n",
      "Iteration 7974, loss = 10.47914542\n",
      "Iteration 7975, loss = 10.83153846\n",
      "Iteration 7976, loss = 11.11130795\n",
      "Iteration 7977, loss = 11.20983509\n",
      "Iteration 7978, loss = 10.57225539\n",
      "Iteration 7979, loss = 10.24445258\n",
      "Iteration 7980, loss = 10.43113164\n",
      "Iteration 7981, loss = 10.32470908\n",
      "Iteration 7982, loss = 10.08140778\n",
      "Iteration 7983, loss = 10.17921006\n",
      "Iteration 7984, loss = 10.10033672\n",
      "Iteration 7985, loss = 10.43893160\n",
      "Iteration 7986, loss = 10.62903374\n",
      "Iteration 7987, loss = 11.30339194\n",
      "Iteration 7988, loss = 12.81112436\n",
      "Iteration 7989, loss = 11.29630229\n",
      "Iteration 7990, loss = 11.05767382\n",
      "Iteration 7991, loss = 11.39512759\n",
      "Iteration 7992, loss = 10.95775160\n",
      "Iteration 7993, loss = 11.47370155\n",
      "Iteration 7994, loss = 10.23872811\n",
      "Iteration 7995, loss = 10.15555006\n",
      "Iteration 7996, loss = 10.49625814\n",
      "Iteration 7997, loss = 10.36518776\n",
      "Iteration 7998, loss = 10.55781561\n",
      "Iteration 7999, loss = 10.89271821\n",
      "Iteration 8000, loss = 10.40785790\n",
      "Iteration 8001, loss = 10.08248613\n",
      "Iteration 8002, loss = 10.83542203\n",
      "Iteration 8003, loss = 12.08200156\n",
      "Iteration 8004, loss = 11.74594095\n",
      "Iteration 8005, loss = 11.62577286\n",
      "Iteration 8006, loss = 11.21528950\n",
      "Iteration 8007, loss = 10.53841084\n",
      "Iteration 8008, loss = 10.73418184\n",
      "Iteration 8009, loss = 10.12913267\n",
      "Iteration 8010, loss = 10.89735342\n",
      "Iteration 8011, loss = 11.22901328\n",
      "Iteration 8012, loss = 11.63078352\n",
      "Iteration 8013, loss = 11.19837751\n",
      "Iteration 8014, loss = 11.13608470\n",
      "Iteration 8015, loss = 10.82383562\n",
      "Iteration 8016, loss = 10.91116116\n",
      "Iteration 8017, loss = 10.74772393\n",
      "Iteration 8018, loss = 11.82032976\n",
      "Iteration 8019, loss = 10.73666095\n",
      "Iteration 8020, loss = 11.58155530\n",
      "Iteration 8021, loss = 12.15853466\n",
      "Iteration 8022, loss = 11.70148455\n",
      "Iteration 8023, loss = 11.14386192\n",
      "Iteration 8024, loss = 11.51489391\n",
      "Iteration 8025, loss = 10.84872161\n",
      "Iteration 8026, loss = 10.87787603\n",
      "Iteration 8027, loss = 10.68979971\n",
      "Iteration 8028, loss = 10.62611691\n",
      "Iteration 8029, loss = 10.33130769\n",
      "Iteration 8030, loss = 10.03699346\n",
      "Iteration 8031, loss = 10.56311368\n",
      "Iteration 8032, loss = 10.50707894\n",
      "Iteration 8033, loss = 10.30659118\n",
      "Iteration 8034, loss = 10.35251021\n",
      "Iteration 8035, loss = 10.73032933\n",
      "Iteration 8036, loss = 10.44380518\n",
      "Iteration 8037, loss = 10.54861593\n",
      "Iteration 8038, loss = 12.12765213\n",
      "Iteration 8039, loss = 10.77247218\n",
      "Iteration 8040, loss = 10.36360306\n",
      "Iteration 8041, loss = 10.51596162\n",
      "Iteration 8042, loss = 11.11815180\n",
      "Iteration 8043, loss = 10.45048310\n",
      "Iteration 8044, loss = 10.51339008\n",
      "Iteration 8045, loss = 10.06883990\n",
      "Iteration 8046, loss = 10.18043513\n",
      "Iteration 8047, loss = 10.62465416\n",
      "Iteration 8048, loss = 10.17646849\n",
      "Iteration 8049, loss = 11.18109612\n",
      "Iteration 8050, loss = 11.67634939\n",
      "Iteration 8051, loss = 11.55236868\n",
      "Iteration 8052, loss = 11.14015849\n",
      "Iteration 8053, loss = 11.18917466\n",
      "Iteration 8054, loss = 11.25435569\n",
      "Iteration 8055, loss = 10.94538979\n",
      "Iteration 8056, loss = 11.75109175\n",
      "Iteration 8057, loss = 12.31841462\n",
      "Iteration 8058, loss = 11.98076924\n",
      "Iteration 8059, loss = 11.09434745\n",
      "Iteration 8060, loss = 10.35579917\n",
      "Iteration 8061, loss = 10.28397041\n",
      "Iteration 8062, loss = 10.50885919\n",
      "Iteration 8063, loss = 10.89093503\n",
      "Iteration 8064, loss = 10.57263886\n",
      "Iteration 8065, loss = 11.37705735\n",
      "Iteration 8066, loss = 10.58517920\n",
      "Iteration 8067, loss = 10.45474566\n",
      "Iteration 8068, loss = 10.77698716\n",
      "Iteration 8069, loss = 10.27460763\n",
      "Iteration 8070, loss = 10.32898922\n",
      "Iteration 8071, loss = 10.36734245\n",
      "Iteration 8072, loss = 10.46322116\n",
      "Iteration 8073, loss = 10.35437171\n",
      "Iteration 8074, loss = 10.21324925\n",
      "Iteration 8075, loss = 9.99344993\n",
      "Iteration 8076, loss = 10.01240871\n",
      "Iteration 8077, loss = 10.05207050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8078, loss = 10.07430703\n",
      "Iteration 8079, loss = 10.05413895\n",
      "Iteration 8080, loss = 10.07418920\n",
      "Iteration 8081, loss = 10.13111712\n",
      "Iteration 8082, loss = 10.10047183\n",
      "Iteration 8083, loss = 10.19808095\n",
      "Iteration 8084, loss = 10.96341277\n",
      "Iteration 8085, loss = 10.50386677\n",
      "Iteration 8086, loss = 10.26839007\n",
      "Iteration 8087, loss = 10.27976922\n",
      "Iteration 8088, loss = 10.16408613\n",
      "Iteration 8089, loss = 10.22306083\n",
      "Iteration 8090, loss = 10.95345343\n",
      "Iteration 8091, loss = 10.41638549\n",
      "Iteration 8092, loss = 10.59897220\n",
      "Iteration 8093, loss = 11.88169558\n",
      "Iteration 8094, loss = 10.74876919\n",
      "Iteration 8095, loss = 11.51689804\n",
      "Iteration 8096, loss = 10.36122462\n",
      "Iteration 8097, loss = 11.59411715\n",
      "Iteration 8098, loss = 10.89747330\n",
      "Iteration 8099, loss = 10.51543823\n",
      "Iteration 8100, loss = 10.29375236\n",
      "Iteration 8101, loss = 10.31191903\n",
      "Iteration 8102, loss = 10.23964869\n",
      "Iteration 8103, loss = 10.92659457\n",
      "Iteration 8104, loss = 10.88012877\n",
      "Iteration 8105, loss = 10.65242236\n",
      "Iteration 8106, loss = 10.30691841\n",
      "Iteration 8107, loss = 10.43787328\n",
      "Iteration 8108, loss = 10.38363108\n",
      "Iteration 8109, loss = 10.67722830\n",
      "Iteration 8110, loss = 10.51920785\n",
      "Iteration 8111, loss = 10.34407139\n",
      "Iteration 8112, loss = 10.78118738\n",
      "Iteration 8113, loss = 11.30108787\n",
      "Iteration 8114, loss = 11.10758734\n",
      "Iteration 8115, loss = 10.70114271\n",
      "Iteration 8116, loss = 10.86034677\n",
      "Iteration 8117, loss = 10.64942977\n",
      "Iteration 8118, loss = 11.40146829\n",
      "Iteration 8119, loss = 10.85608763\n",
      "Iteration 8120, loss = 10.89473505\n",
      "Iteration 8121, loss = 10.93551046\n",
      "Iteration 8122, loss = 10.22894639\n",
      "Iteration 8123, loss = 10.29383350\n",
      "Iteration 8124, loss = 10.28207066\n",
      "Iteration 8125, loss = 10.71826475\n",
      "Iteration 8126, loss = 10.49172949\n",
      "Iteration 8127, loss = 10.44400042\n",
      "Iteration 8128, loss = 10.34795749\n",
      "Iteration 8129, loss = 10.26297695\n",
      "Iteration 8130, loss = 10.84632847\n",
      "Iteration 8131, loss = 10.38514964\n",
      "Iteration 8132, loss = 10.62376198\n",
      "Iteration 8133, loss = 12.11115532\n",
      "Iteration 8134, loss = 10.64817641\n",
      "Iteration 8135, loss = 10.33334845\n",
      "Iteration 8136, loss = 10.97243406\n",
      "Iteration 8137, loss = 10.20393527\n",
      "Iteration 8138, loss = 10.41564539\n",
      "Iteration 8139, loss = 10.89583077\n",
      "Iteration 8140, loss = 11.26549821\n",
      "Iteration 8141, loss = 12.83964263\n",
      "Iteration 8142, loss = 13.72657821\n",
      "Iteration 8143, loss = 13.78090241\n",
      "Iteration 8144, loss = 11.41437581\n",
      "Iteration 8145, loss = 10.91348047\n",
      "Iteration 8146, loss = 10.32854435\n",
      "Iteration 8147, loss = 10.38581674\n",
      "Iteration 8148, loss = 10.42146590\n",
      "Iteration 8149, loss = 10.67918354\n",
      "Iteration 8150, loss = 10.00936816\n",
      "Iteration 8151, loss = 10.29771996\n",
      "Iteration 8152, loss = 10.24357389\n",
      "Iteration 8153, loss = 10.07363704\n",
      "Iteration 8154, loss = 10.79954496\n",
      "Iteration 8155, loss = 10.61903116\n",
      "Iteration 8156, loss = 11.09420684\n",
      "Iteration 8157, loss = 10.69400201\n",
      "Iteration 8158, loss = 10.35152850\n",
      "Iteration 8159, loss = 10.14706094\n",
      "Iteration 8160, loss = 10.01176128\n",
      "Iteration 8161, loss = 10.43465581\n",
      "Iteration 8162, loss = 10.39750486\n",
      "Iteration 8163, loss = 10.75781768\n",
      "Iteration 8164, loss = 10.66074728\n",
      "Iteration 8165, loss = 10.69597033\n",
      "Iteration 8166, loss = 10.58827964\n",
      "Iteration 8167, loss = 10.62142221\n",
      "Iteration 8168, loss = 10.21304644\n",
      "Iteration 8169, loss = 10.39205371\n",
      "Iteration 8170, loss = 10.24517202\n",
      "Iteration 8171, loss = 10.16167678\n",
      "Iteration 8172, loss = 10.22645492\n",
      "Iteration 8173, loss = 10.66382453\n",
      "Iteration 8174, loss = 10.58961822\n",
      "Iteration 8175, loss = 10.71542644\n",
      "Iteration 8176, loss = 10.81656405\n",
      "Iteration 8177, loss = 10.19885119\n",
      "Iteration 8178, loss = 10.33216073\n",
      "Iteration 8179, loss = 10.41269174\n",
      "Iteration 8180, loss = 10.47911838\n",
      "Iteration 8181, loss = 10.06885400\n",
      "Iteration 8182, loss = 10.12895121\n",
      "Iteration 8183, loss = 10.40113194\n",
      "Iteration 8184, loss = 11.08132638\n",
      "Iteration 8185, loss = 10.35212345\n",
      "Iteration 8186, loss = 10.53837146\n",
      "Iteration 8187, loss = 10.63186745\n",
      "Iteration 8188, loss = 10.21403522\n",
      "Iteration 8189, loss = 11.12965606\n",
      "Iteration 8190, loss = 12.20626615\n",
      "Iteration 8191, loss = 11.69774691\n",
      "Iteration 8192, loss = 10.75504858\n",
      "Iteration 8193, loss = 10.27620995\n",
      "Iteration 8194, loss = 10.00843159\n",
      "Iteration 8195, loss = 10.32746173\n",
      "Iteration 8196, loss = 10.39794325\n",
      "Iteration 8197, loss = 11.66695104\n",
      "Iteration 8198, loss = 11.17927626\n",
      "Iteration 8199, loss = 11.78814538\n",
      "Iteration 8200, loss = 11.29970444\n",
      "Iteration 8201, loss = 13.87104652\n",
      "Iteration 8202, loss = 11.72234619\n",
      "Iteration 8203, loss = 11.68957185\n",
      "Iteration 8204, loss = 11.76334629\n",
      "Iteration 8205, loss = 10.82509188\n",
      "Iteration 8206, loss = 10.97706344\n",
      "Iteration 8207, loss = 11.76488971\n",
      "Iteration 8208, loss = 11.73676072\n",
      "Iteration 8209, loss = 10.92572660\n",
      "Iteration 8210, loss = 11.18269464\n",
      "Iteration 8211, loss = 10.44058587\n",
      "Iteration 8212, loss = 11.32580864\n",
      "Iteration 8213, loss = 10.46774524\n",
      "Iteration 8214, loss = 10.88068082\n",
      "Iteration 8215, loss = 10.89538738\n",
      "Iteration 8216, loss = 10.34218528\n",
      "Iteration 8217, loss = 10.32434026\n",
      "Iteration 8218, loss = 10.25147858\n",
      "Iteration 8219, loss = 10.14699818\n",
      "Iteration 8220, loss = 10.16619294\n",
      "Iteration 8221, loss = 10.11960202\n",
      "Iteration 8222, loss = 9.98829467\n",
      "Iteration 8223, loss = 10.09672912\n",
      "Iteration 8224, loss = 10.18504176\n",
      "Iteration 8225, loss = 9.97868735\n",
      "Iteration 8226, loss = 10.33030785\n",
      "Iteration 8227, loss = 11.86061154\n",
      "Iteration 8228, loss = 11.40789498\n",
      "Iteration 8229, loss = 10.53340004\n",
      "Iteration 8230, loss = 11.56461109\n",
      "Iteration 8231, loss = 10.74836455\n",
      "Iteration 8232, loss = 10.10333374\n",
      "Iteration 8233, loss = 10.59919971\n",
      "Iteration 8234, loss = 10.06404627\n",
      "Iteration 8235, loss = 10.18972167\n",
      "Iteration 8236, loss = 10.04097831\n",
      "Iteration 8237, loss = 10.52368525\n",
      "Iteration 8238, loss = 11.17504596\n",
      "Iteration 8239, loss = 12.26846452\n",
      "Iteration 8240, loss = 12.18487208\n",
      "Iteration 8241, loss = 13.16075574\n",
      "Iteration 8242, loss = 11.20601083\n",
      "Iteration 8243, loss = 11.92531361\n",
      "Iteration 8244, loss = 10.64252598\n",
      "Iteration 8245, loss = 11.22558855\n",
      "Iteration 8246, loss = 11.27207062\n",
      "Iteration 8247, loss = 10.60677930\n",
      "Iteration 8248, loss = 10.30708789\n",
      "Iteration 8249, loss = 10.74827522\n",
      "Iteration 8250, loss = 10.96039455\n",
      "Iteration 8251, loss = 11.67467942\n",
      "Iteration 8252, loss = 12.37340155\n",
      "Iteration 8253, loss = 12.16196879\n",
      "Iteration 8254, loss = 12.94554426\n",
      "Iteration 8255, loss = 11.17516161\n",
      "Iteration 8256, loss = 11.25159153\n",
      "Iteration 8257, loss = 9.99452521\n",
      "Iteration 8258, loss = 10.00496789\n",
      "Iteration 8259, loss = 10.02654512\n",
      "Iteration 8260, loss = 10.31710566\n",
      "Iteration 8261, loss = 10.33759955\n",
      "Iteration 8262, loss = 10.12439913\n",
      "Iteration 8263, loss = 10.22569652\n",
      "Iteration 8264, loss = 10.32078185\n",
      "Iteration 8265, loss = 10.28991156\n",
      "Iteration 8266, loss = 10.32353132\n",
      "Iteration 8267, loss = 10.14320770\n",
      "Iteration 8268, loss = 10.08512398\n",
      "Iteration 8269, loss = 10.45518776\n",
      "Iteration 8270, loss = 10.85775999\n",
      "Iteration 8271, loss = 10.69452869\n",
      "Iteration 8272, loss = 10.66935899\n",
      "Iteration 8273, loss = 10.20694180\n",
      "Iteration 8274, loss = 10.31538132\n",
      "Iteration 8275, loss = 10.32442168\n",
      "Iteration 8276, loss = 10.79892925\n",
      "Iteration 8277, loss = 10.76114568\n",
      "Iteration 8278, loss = 11.65745223\n",
      "Iteration 8279, loss = 11.17291205\n",
      "Iteration 8280, loss = 10.81787531\n",
      "Iteration 8281, loss = 11.03276184\n",
      "Iteration 8282, loss = 10.09709151\n",
      "Iteration 8283, loss = 10.14749323\n",
      "Iteration 8284, loss = 10.03009120\n",
      "Iteration 8285, loss = 10.40277985\n",
      "Iteration 8286, loss = 10.18612554\n",
      "Iteration 8287, loss = 9.84135259\n",
      "Iteration 8288, loss = 10.09034165\n",
      "Iteration 8289, loss = 10.70938665\n",
      "Iteration 8290, loss = 10.71207732\n",
      "Iteration 8291, loss = 10.84854679\n",
      "Iteration 8292, loss = 11.13613888\n",
      "Iteration 8293, loss = 10.15128940\n",
      "Iteration 8294, loss = 10.15751741\n",
      "Iteration 8295, loss = 10.44572360\n",
      "Iteration 8296, loss = 10.63403666\n",
      "Iteration 8297, loss = 10.76314231\n",
      "Iteration 8298, loss = 11.78057983\n",
      "Iteration 8299, loss = 11.07039636\n",
      "Iteration 8300, loss = 12.22316265\n",
      "Iteration 8301, loss = 10.56623720\n",
      "Iteration 8302, loss = 10.69079573\n",
      "Iteration 8303, loss = 10.56342530\n",
      "Iteration 8304, loss = 10.68107772\n",
      "Iteration 8305, loss = 10.34846636\n",
      "Iteration 8306, loss = 9.92712336\n",
      "Iteration 8307, loss = 10.21612322\n",
      "Iteration 8308, loss = 10.49125229\n",
      "Iteration 8309, loss = 10.15137262\n",
      "Iteration 8310, loss = 10.26153303\n",
      "Iteration 8311, loss = 11.39746719\n",
      "Iteration 8312, loss = 11.13361772\n",
      "Iteration 8313, loss = 11.69179340\n",
      "Iteration 8314, loss = 11.66179567\n",
      "Iteration 8315, loss = 10.49579468\n",
      "Iteration 8316, loss = 10.63139388\n",
      "Iteration 8317, loss = 10.78359682\n",
      "Iteration 8318, loss = 10.11153200\n",
      "Iteration 8319, loss = 10.34717267\n",
      "Iteration 8320, loss = 9.95477249\n",
      "Iteration 8321, loss = 10.16172623\n",
      "Iteration 8322, loss = 10.53342730\n",
      "Iteration 8323, loss = 10.36780839\n",
      "Iteration 8324, loss = 10.13420430\n",
      "Iteration 8325, loss = 10.44672271\n",
      "Iteration 8326, loss = 10.63900186\n",
      "Iteration 8327, loss = 10.83420319\n",
      "Iteration 8328, loss = 9.98677596\n",
      "Iteration 8329, loss = 10.25988294\n",
      "Iteration 8330, loss = 10.23427599\n",
      "Iteration 8331, loss = 10.22909654\n",
      "Iteration 8332, loss = 10.41641891\n",
      "Iteration 8333, loss = 10.06380667\n",
      "Iteration 8334, loss = 10.29465006\n",
      "Iteration 8335, loss = 10.27442382\n",
      "Iteration 8336, loss = 10.07029491\n",
      "Iteration 8337, loss = 10.63789592\n",
      "Iteration 8338, loss = 11.10895733\n",
      "Iteration 8339, loss = 9.94484070\n",
      "Iteration 8340, loss = 10.29382075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8341, loss = 10.17382423\n",
      "Iteration 8342, loss = 10.18245981\n",
      "Iteration 8343, loss = 10.75902674\n",
      "Iteration 8344, loss = 10.29009867\n",
      "Iteration 8345, loss = 10.34285859\n",
      "Iteration 8346, loss = 9.91005641\n",
      "Iteration 8347, loss = 10.60809177\n",
      "Iteration 8348, loss = 10.18285066\n",
      "Iteration 8349, loss = 10.47424754\n",
      "Iteration 8350, loss = 10.55032922\n",
      "Iteration 8351, loss = 12.21038672\n",
      "Iteration 8352, loss = 12.08260183\n",
      "Iteration 8353, loss = 11.06383314\n",
      "Iteration 8354, loss = 10.40873520\n",
      "Iteration 8355, loss = 11.32592431\n",
      "Iteration 8356, loss = 11.67222895\n",
      "Iteration 8357, loss = 11.17124671\n",
      "Iteration 8358, loss = 10.74700147\n",
      "Iteration 8359, loss = 11.41292113\n",
      "Iteration 8360, loss = 10.90930936\n",
      "Iteration 8361, loss = 10.75007295\n",
      "Iteration 8362, loss = 10.50607766\n",
      "Iteration 8363, loss = 10.49462155\n",
      "Iteration 8364, loss = 10.59556776\n",
      "Iteration 8365, loss = 10.22363278\n",
      "Iteration 8366, loss = 10.03674643\n",
      "Iteration 8367, loss = 10.31678325\n",
      "Iteration 8368, loss = 10.39133764\n",
      "Iteration 8369, loss = 11.53745437\n",
      "Iteration 8370, loss = 10.39579220\n",
      "Iteration 8371, loss = 10.40789145\n",
      "Iteration 8372, loss = 10.31759691\n",
      "Iteration 8373, loss = 10.06183483\n",
      "Iteration 8374, loss = 10.09114673\n",
      "Iteration 8375, loss = 9.95023116\n",
      "Iteration 8376, loss = 10.37218877\n",
      "Iteration 8377, loss = 10.81876242\n",
      "Iteration 8378, loss = 10.47888931\n",
      "Iteration 8379, loss = 10.13217568\n",
      "Iteration 8380, loss = 10.85379739\n",
      "Iteration 8381, loss = 11.32649640\n",
      "Iteration 8382, loss = 11.33794200\n",
      "Iteration 8383, loss = 11.27831385\n",
      "Iteration 8384, loss = 10.69371241\n",
      "Iteration 8385, loss = 10.77374170\n",
      "Iteration 8386, loss = 10.50877500\n",
      "Iteration 8387, loss = 10.51859071\n",
      "Iteration 8388, loss = 9.97670294\n",
      "Iteration 8389, loss = 10.17540433\n",
      "Iteration 8390, loss = 10.35494001\n",
      "Iteration 8391, loss = 11.07840858\n",
      "Iteration 8392, loss = 10.24235947\n",
      "Iteration 8393, loss = 10.76025176\n",
      "Iteration 8394, loss = 10.43404386\n",
      "Iteration 8395, loss = 11.34697418\n",
      "Iteration 8396, loss = 12.57223121\n",
      "Iteration 8397, loss = 12.09794137\n",
      "Iteration 8398, loss = 11.07559264\n",
      "Iteration 8399, loss = 10.40586068\n",
      "Iteration 8400, loss = 10.04359366\n",
      "Iteration 8401, loss = 10.13658979\n",
      "Iteration 8402, loss = 10.00871641\n",
      "Iteration 8403, loss = 9.95036485\n",
      "Iteration 8404, loss = 10.46844415\n",
      "Iteration 8405, loss = 10.38017121\n",
      "Iteration 8406, loss = 11.12659586\n",
      "Iteration 8407, loss = 11.67314444\n",
      "Iteration 8408, loss = 10.78922813\n",
      "Iteration 8409, loss = 10.49773416\n",
      "Iteration 8410, loss = 10.85733004\n",
      "Iteration 8411, loss = 10.35872181\n",
      "Iteration 8412, loss = 9.97236498\n",
      "Iteration 8413, loss = 10.16281472\n",
      "Iteration 8414, loss = 10.25410879\n",
      "Iteration 8415, loss = 10.16574390\n",
      "Iteration 8416, loss = 11.10679021\n",
      "Iteration 8417, loss = 11.03446225\n",
      "Iteration 8418, loss = 11.70164077\n",
      "Iteration 8419, loss = 10.55919524\n",
      "Iteration 8420, loss = 10.38465449\n",
      "Iteration 8421, loss = 10.24188268\n",
      "Iteration 8422, loss = 9.86149161\n",
      "Iteration 8423, loss = 10.25522256\n",
      "Iteration 8424, loss = 10.06400947\n",
      "Iteration 8425, loss = 10.20029479\n",
      "Iteration 8426, loss = 10.26103087\n",
      "Iteration 8427, loss = 10.00871536\n",
      "Iteration 8428, loss = 10.14132458\n",
      "Iteration 8429, loss = 10.02518126\n",
      "Iteration 8430, loss = 10.83785701\n",
      "Iteration 8431, loss = 10.17473995\n",
      "Iteration 8432, loss = 10.07178664\n",
      "Iteration 8433, loss = 10.12935536\n",
      "Iteration 8434, loss = 10.86988162\n",
      "Iteration 8435, loss = 10.89018062\n",
      "Iteration 8436, loss = 10.28027247\n",
      "Iteration 8437, loss = 11.00707832\n",
      "Iteration 8438, loss = 11.24214400\n",
      "Iteration 8439, loss = 11.16595960\n",
      "Iteration 8440, loss = 11.99858633\n",
      "Iteration 8441, loss = 10.25614796\n",
      "Iteration 8442, loss = 11.12412812\n",
      "Iteration 8443, loss = 10.85459870\n",
      "Iteration 8444, loss = 10.24367789\n",
      "Iteration 8445, loss = 10.04273380\n",
      "Iteration 8446, loss = 9.96145057\n",
      "Iteration 8447, loss = 9.91766823\n",
      "Iteration 8448, loss = 10.85406627\n",
      "Iteration 8449, loss = 9.89656232\n",
      "Iteration 8450, loss = 10.73674068\n",
      "Iteration 8451, loss = 10.65868315\n",
      "Iteration 8452, loss = 10.27098957\n",
      "Iteration 8453, loss = 10.32233071\n",
      "Iteration 8454, loss = 10.00221044\n",
      "Iteration 8455, loss = 10.11222745\n",
      "Iteration 8456, loss = 10.24665008\n",
      "Iteration 8457, loss = 9.99365763\n",
      "Iteration 8458, loss = 11.23223327\n",
      "Iteration 8459, loss = 11.57199972\n",
      "Iteration 8460, loss = 11.69492524\n",
      "Iteration 8461, loss = 11.57526558\n",
      "Iteration 8462, loss = 12.46777027\n",
      "Iteration 8463, loss = 12.81439356\n",
      "Iteration 8464, loss = 12.90066844\n",
      "Iteration 8465, loss = 11.56173965\n",
      "Iteration 8466, loss = 10.47048273\n",
      "Iteration 8467, loss = 10.72744868\n",
      "Iteration 8468, loss = 11.22590013\n",
      "Iteration 8469, loss = 10.91778149\n",
      "Iteration 8470, loss = 10.77239009\n",
      "Iteration 8471, loss = 10.27498812\n",
      "Iteration 8472, loss = 10.28773651\n",
      "Iteration 8473, loss = 10.12583438\n",
      "Iteration 8474, loss = 10.05119974\n",
      "Iteration 8475, loss = 10.58747374\n",
      "Iteration 8476, loss = 9.73154800\n",
      "Iteration 8477, loss = 9.99141162\n",
      "Iteration 8478, loss = 10.50144580\n",
      "Iteration 8479, loss = 10.19941909\n",
      "Iteration 8480, loss = 10.04634196\n",
      "Iteration 8481, loss = 10.69130675\n",
      "Iteration 8482, loss = 10.14757396\n",
      "Iteration 8483, loss = 10.57645393\n",
      "Iteration 8484, loss = 9.97576755\n",
      "Iteration 8485, loss = 10.37044650\n",
      "Iteration 8486, loss = 10.13549235\n",
      "Iteration 8487, loss = 9.96117734\n",
      "Iteration 8488, loss = 10.11919190\n",
      "Iteration 8489, loss = 10.31558495\n",
      "Iteration 8490, loss = 10.17122157\n",
      "Iteration 8491, loss = 10.07227078\n",
      "Iteration 8492, loss = 10.44939781\n",
      "Iteration 8493, loss = 10.45311621\n",
      "Iteration 8494, loss = 10.58372788\n",
      "Iteration 8495, loss = 11.60806066\n",
      "Iteration 8496, loss = 11.06098533\n",
      "Iteration 8497, loss = 11.19365430\n",
      "Iteration 8498, loss = 11.73038397\n",
      "Iteration 8499, loss = 12.16820973\n",
      "Iteration 8500, loss = 11.77508397\n",
      "Iteration 8501, loss = 10.82872240\n",
      "Iteration 8502, loss = 9.89155464\n",
      "Iteration 8503, loss = 10.31752339\n",
      "Iteration 8504, loss = 10.58843633\n",
      "Iteration 8505, loss = 10.48077986\n",
      "Iteration 8506, loss = 11.24989461\n",
      "Iteration 8507, loss = 10.33772567\n",
      "Iteration 8508, loss = 10.11553158\n",
      "Iteration 8509, loss = 10.26604980\n",
      "Iteration 8510, loss = 9.81175332\n",
      "Iteration 8511, loss = 10.50172948\n",
      "Iteration 8512, loss = 10.03273177\n",
      "Iteration 8513, loss = 10.79955774\n",
      "Iteration 8514, loss = 10.65633271\n",
      "Iteration 8515, loss = 10.60105552\n",
      "Iteration 8516, loss = 10.20351079\n",
      "Iteration 8517, loss = 10.95076584\n",
      "Iteration 8518, loss = 10.79352470\n",
      "Iteration 8519, loss = 10.14146374\n",
      "Iteration 8520, loss = 10.54898122\n",
      "Iteration 8521, loss = 10.40446654\n",
      "Iteration 8522, loss = 10.29412786\n",
      "Iteration 8523, loss = 11.79612169\n",
      "Iteration 8524, loss = 11.15270732\n",
      "Iteration 8525, loss = 10.32754363\n",
      "Iteration 8526, loss = 9.98751840\n",
      "Iteration 8527, loss = 10.50028607\n",
      "Iteration 8528, loss = 10.21606493\n",
      "Iteration 8529, loss = 10.26771369\n",
      "Iteration 8530, loss = 10.00163052\n",
      "Iteration 8531, loss = 10.18636525\n",
      "Iteration 8532, loss = 9.92014685\n",
      "Iteration 8533, loss = 10.03728307\n",
      "Iteration 8534, loss = 10.00226241\n",
      "Iteration 8535, loss = 10.75839900\n",
      "Iteration 8536, loss = 10.30861155\n",
      "Iteration 8537, loss = 10.34637800\n",
      "Iteration 8538, loss = 9.95558359\n",
      "Iteration 8539, loss = 10.04955576\n",
      "Iteration 8540, loss = 10.33187049\n",
      "Iteration 8541, loss = 10.28201478\n",
      "Iteration 8542, loss = 10.26726825\n",
      "Iteration 8543, loss = 10.48336349\n",
      "Iteration 8544, loss = 10.59736474\n",
      "Iteration 8545, loss = 9.85522774\n",
      "Iteration 8546, loss = 10.05245627\n",
      "Iteration 8547, loss = 9.90880815\n",
      "Iteration 8548, loss = 10.01213817\n",
      "Iteration 8549, loss = 9.71659911\n",
      "Iteration 8550, loss = 10.68023970\n",
      "Iteration 8551, loss = 10.12950082\n",
      "Iteration 8552, loss = 10.52132653\n",
      "Iteration 8553, loss = 10.25642169\n",
      "Iteration 8554, loss = 10.84863140\n",
      "Iteration 8555, loss = 11.01655685\n",
      "Iteration 8556, loss = 10.94563370\n",
      "Iteration 8557, loss = 10.48752305\n",
      "Iteration 8558, loss = 11.04213677\n",
      "Iteration 8559, loss = 10.49077259\n",
      "Iteration 8560, loss = 10.15127026\n",
      "Iteration 8561, loss = 11.07779771\n",
      "Iteration 8562, loss = 10.75229668\n",
      "Iteration 8563, loss = 9.77859241\n",
      "Iteration 8564, loss = 10.02211284\n",
      "Iteration 8565, loss = 10.07864535\n",
      "Iteration 8566, loss = 9.90216873\n",
      "Iteration 8567, loss = 10.15937221\n",
      "Iteration 8568, loss = 10.59951137\n",
      "Iteration 8569, loss = 10.01811116\n",
      "Iteration 8570, loss = 9.87416575\n",
      "Iteration 8571, loss = 9.94175856\n",
      "Iteration 8572, loss = 9.90248050\n",
      "Iteration 8573, loss = 9.89611820\n",
      "Iteration 8574, loss = 10.36126077\n",
      "Iteration 8575, loss = 10.39819026\n",
      "Iteration 8576, loss = 11.85861931\n",
      "Iteration 8577, loss = 11.68820597\n",
      "Iteration 8578, loss = 12.18990273\n",
      "Iteration 8579, loss = 10.59416423\n",
      "Iteration 8580, loss = 11.73517036\n",
      "Iteration 8581, loss = 11.60847317\n",
      "Iteration 8582, loss = 11.76162987\n",
      "Iteration 8583, loss = 10.15714950\n",
      "Iteration 8584, loss = 10.42992103\n",
      "Iteration 8585, loss = 10.40804552\n",
      "Iteration 8586, loss = 10.24133524\n",
      "Iteration 8587, loss = 9.85367744\n",
      "Iteration 8588, loss = 10.08630570\n",
      "Iteration 8589, loss = 9.89725799\n",
      "Iteration 8590, loss = 10.77673611\n",
      "Iteration 8591, loss = 10.11490229\n",
      "Iteration 8592, loss = 9.67742403\n",
      "Iteration 8593, loss = 10.05651374\n",
      "Iteration 8594, loss = 10.63412525\n",
      "Iteration 8595, loss = 10.66973949\n",
      "Iteration 8596, loss = 10.28867159\n",
      "Iteration 8597, loss = 10.62643937\n",
      "Iteration 8598, loss = 11.27884227\n",
      "Iteration 8599, loss = 11.09715674\n",
      "Iteration 8600, loss = 11.32982586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8601, loss = 11.63623506\n",
      "Iteration 8602, loss = 11.05566738\n",
      "Iteration 8603, loss = 10.34060903\n",
      "Iteration 8604, loss = 10.02890089\n",
      "Iteration 8605, loss = 10.30790701\n",
      "Iteration 8606, loss = 10.16865328\n",
      "Iteration 8607, loss = 9.92936063\n",
      "Iteration 8608, loss = 9.96466234\n",
      "Iteration 8609, loss = 9.89412306\n",
      "Iteration 8610, loss = 9.72152941\n",
      "Iteration 8611, loss = 10.70842302\n",
      "Iteration 8612, loss = 10.32428526\n",
      "Iteration 8613, loss = 10.26815028\n",
      "Iteration 8614, loss = 10.19286007\n",
      "Iteration 8615, loss = 10.17632783\n",
      "Iteration 8616, loss = 10.65189624\n",
      "Iteration 8617, loss = 10.56239677\n",
      "Iteration 8618, loss = 11.14871257\n",
      "Iteration 8619, loss = 10.13795298\n",
      "Iteration 8620, loss = 10.26263731\n",
      "Iteration 8621, loss = 10.21217445\n",
      "Iteration 8622, loss = 10.71782438\n",
      "Iteration 8623, loss = 10.77152235\n",
      "Iteration 8624, loss = 10.62559097\n",
      "Iteration 8625, loss = 11.15017337\n",
      "Iteration 8626, loss = 11.88841420\n",
      "Iteration 8627, loss = 13.51125607\n",
      "Iteration 8628, loss = 13.81875365\n",
      "Iteration 8629, loss = 12.44996282\n",
      "Iteration 8630, loss = 12.13482516\n",
      "Iteration 8631, loss = 10.12460592\n",
      "Iteration 8632, loss = 10.03320835\n",
      "Iteration 8633, loss = 10.29775676\n",
      "Iteration 8634, loss = 9.90687544\n",
      "Iteration 8635, loss = 9.93061935\n",
      "Iteration 8636, loss = 9.79902535\n",
      "Iteration 8637, loss = 9.73547912\n",
      "Iteration 8638, loss = 9.64485457\n",
      "Iteration 8639, loss = 9.77625733\n",
      "Iteration 8640, loss = 10.04294597\n",
      "Iteration 8641, loss = 10.44054444\n",
      "Iteration 8642, loss = 10.08781527\n",
      "Iteration 8643, loss = 10.54890464\n",
      "Iteration 8644, loss = 9.87901545\n",
      "Iteration 8645, loss = 10.83439097\n",
      "Iteration 8646, loss = 10.81090666\n",
      "Iteration 8647, loss = 9.94760403\n",
      "Iteration 8648, loss = 9.72670630\n",
      "Iteration 8649, loss = 10.34267304\n",
      "Iteration 8650, loss = 10.24226674\n",
      "Iteration 8651, loss = 10.91861426\n",
      "Iteration 8652, loss = 9.92810746\n",
      "Iteration 8653, loss = 10.15309743\n",
      "Iteration 8654, loss = 9.97235458\n",
      "Iteration 8655, loss = 10.32633442\n",
      "Iteration 8656, loss = 10.39482181\n",
      "Iteration 8657, loss = 9.59764329\n",
      "Iteration 8658, loss = 9.97689558\n",
      "Iteration 8659, loss = 9.94191338\n",
      "Iteration 8660, loss = 10.76067002\n",
      "Iteration 8661, loss = 10.14734899\n",
      "Iteration 8662, loss = 10.03981131\n",
      "Iteration 8663, loss = 10.86526478\n",
      "Iteration 8664, loss = 10.31433623\n",
      "Iteration 8665, loss = 10.09906196\n",
      "Iteration 8666, loss = 10.03202104\n",
      "Iteration 8667, loss = 10.18840179\n",
      "Iteration 8668, loss = 9.77447498\n",
      "Iteration 8669, loss = 10.04852799\n",
      "Iteration 8670, loss = 10.15125225\n",
      "Iteration 8671, loss = 10.72759266\n",
      "Iteration 8672, loss = 10.84199408\n",
      "Iteration 8673, loss = 10.71271809\n",
      "Iteration 8674, loss = 10.11712518\n",
      "Iteration 8675, loss = 9.80051243\n",
      "Iteration 8676, loss = 10.65966098\n",
      "Iteration 8677, loss = 10.63014558\n",
      "Iteration 8678, loss = 10.32129045\n",
      "Iteration 8679, loss = 10.11878203\n",
      "Iteration 8680, loss = 10.71063427\n",
      "Iteration 8681, loss = 10.47345442\n",
      "Iteration 8682, loss = 9.91919820\n",
      "Iteration 8683, loss = 9.95266917\n",
      "Iteration 8684, loss = 11.19645820\n",
      "Iteration 8685, loss = 11.81389572\n",
      "Iteration 8686, loss = 11.28115458\n",
      "Iteration 8687, loss = 10.87174079\n",
      "Iteration 8688, loss = 10.04298562\n",
      "Iteration 8689, loss = 9.93601678\n",
      "Iteration 8690, loss = 10.27222073\n",
      "Iteration 8691, loss = 10.42575353\n",
      "Iteration 8692, loss = 10.68049139\n",
      "Iteration 8693, loss = 11.11567082\n",
      "Iteration 8694, loss = 11.47735119\n",
      "Iteration 8695, loss = 11.78113524\n",
      "Iteration 8696, loss = 11.73936917\n",
      "Iteration 8697, loss = 10.47619482\n",
      "Iteration 8698, loss = 10.28418189\n",
      "Iteration 8699, loss = 9.87892719\n",
      "Iteration 8700, loss = 9.96349442\n",
      "Iteration 8701, loss = 10.27026903\n",
      "Iteration 8702, loss = 10.69761954\n",
      "Iteration 8703, loss = 10.67740979\n",
      "Iteration 8704, loss = 10.07453099\n",
      "Iteration 8705, loss = 10.46159772\n",
      "Iteration 8706, loss = 10.51539560\n",
      "Iteration 8707, loss = 10.17196512\n",
      "Iteration 8708, loss = 9.73883403\n",
      "Iteration 8709, loss = 10.15809004\n",
      "Iteration 8710, loss = 10.33245133\n",
      "Iteration 8711, loss = 9.82797498\n",
      "Iteration 8712, loss = 10.46019342\n",
      "Iteration 8713, loss = 9.74780838\n",
      "Iteration 8714, loss = 9.90917106\n",
      "Iteration 8715, loss = 10.33615896\n",
      "Iteration 8716, loss = 10.36485700\n",
      "Iteration 8717, loss = 10.76896266\n",
      "Iteration 8718, loss = 10.49915628\n",
      "Iteration 8719, loss = 10.38303679\n",
      "Iteration 8720, loss = 10.80114296\n",
      "Iteration 8721, loss = 11.77233201\n",
      "Iteration 8722, loss = 10.12302903\n",
      "Iteration 8723, loss = 11.06110163\n",
      "Iteration 8724, loss = 10.96973144\n",
      "Iteration 8725, loss = 10.18930857\n",
      "Iteration 8726, loss = 9.95164236\n",
      "Iteration 8727, loss = 10.41353254\n",
      "Iteration 8728, loss = 9.94376315\n",
      "Iteration 8729, loss = 9.91827705\n",
      "Iteration 8730, loss = 9.75964474\n",
      "Iteration 8731, loss = 10.00895476\n",
      "Iteration 8732, loss = 9.78864499\n",
      "Iteration 8733, loss = 10.46541144\n",
      "Iteration 8734, loss = 11.34296316\n",
      "Iteration 8735, loss = 10.31812856\n",
      "Iteration 8736, loss = 10.19528137\n",
      "Iteration 8737, loss = 10.67996176\n",
      "Iteration 8738, loss = 10.60060737\n",
      "Iteration 8739, loss = 10.82565004\n",
      "Iteration 8740, loss = 10.89444166\n",
      "Iteration 8741, loss = 9.78833319\n",
      "Iteration 8742, loss = 9.93483684\n",
      "Iteration 8743, loss = 10.46612654\n",
      "Iteration 8744, loss = 9.90801290\n",
      "Iteration 8745, loss = 9.92983384\n",
      "Iteration 8746, loss = 9.94198470\n",
      "Iteration 8747, loss = 9.73543608\n",
      "Iteration 8748, loss = 10.17920896\n",
      "Iteration 8749, loss = 10.59170732\n",
      "Iteration 8750, loss = 10.61204601\n",
      "Iteration 8751, loss = 10.87370767\n",
      "Iteration 8752, loss = 11.17260606\n",
      "Iteration 8753, loss = 11.46684588\n",
      "Iteration 8754, loss = 11.33945546\n",
      "Iteration 8755, loss = 11.50314339\n",
      "Iteration 8756, loss = 11.95703650\n",
      "Iteration 8757, loss = 11.23638229\n",
      "Iteration 8758, loss = 10.87162148\n",
      "Iteration 8759, loss = 10.51350370\n",
      "Iteration 8760, loss = 11.37637476\n",
      "Iteration 8761, loss = 12.74258024\n",
      "Iteration 8762, loss = 13.58442368\n",
      "Iteration 8763, loss = 14.25377857\n",
      "Iteration 8764, loss = 13.95016255\n",
      "Iteration 8765, loss = 12.74397886\n",
      "Iteration 8766, loss = 10.33986807\n",
      "Iteration 8767, loss = 10.30508873\n",
      "Iteration 8768, loss = 10.29033863\n",
      "Iteration 8769, loss = 10.10002404\n",
      "Iteration 8770, loss = 11.11504503\n",
      "Iteration 8771, loss = 11.22510322\n",
      "Iteration 8772, loss = 11.78374029\n",
      "Iteration 8773, loss = 9.75090321\n",
      "Iteration 8774, loss = 10.29755975\n",
      "Iteration 8775, loss = 10.83752986\n",
      "Iteration 8776, loss = 9.98361328\n",
      "Iteration 8777, loss = 10.25000355\n",
      "Iteration 8778, loss = 10.17621193\n",
      "Iteration 8779, loss = 10.24980396\n",
      "Iteration 8780, loss = 11.67872553\n",
      "Iteration 8781, loss = 11.21556835\n",
      "Iteration 8782, loss = 11.57016080\n",
      "Iteration 8783, loss = 10.78392833\n",
      "Iteration 8784, loss = 10.16739219\n",
      "Iteration 8785, loss = 10.20837509\n",
      "Iteration 8786, loss = 10.32411431\n",
      "Iteration 8787, loss = 9.91348827\n",
      "Iteration 8788, loss = 10.19800680\n",
      "Iteration 8789, loss = 9.61704659\n",
      "Iteration 8790, loss = 9.96129390\n",
      "Iteration 8791, loss = 10.77701445\n",
      "Iteration 8792, loss = 10.66638226\n",
      "Iteration 8793, loss = 9.96723384\n",
      "Iteration 8794, loss = 10.62605165\n",
      "Iteration 8795, loss = 11.34974297\n",
      "Iteration 8796, loss = 11.19608202\n",
      "Iteration 8797, loss = 10.98911393\n",
      "Iteration 8798, loss = 10.54772745\n",
      "Iteration 8799, loss = 10.47819948\n",
      "Iteration 8800, loss = 10.27480304\n",
      "Iteration 8801, loss = 9.72027331\n",
      "Iteration 8802, loss = 10.70666927\n",
      "Iteration 8803, loss = 10.68477184\n",
      "Iteration 8804, loss = 11.31173942\n",
      "Iteration 8805, loss = 10.56287669\n",
      "Iteration 8806, loss = 10.89150669\n",
      "Iteration 8807, loss = 10.13890747\n",
      "Iteration 8808, loss = 9.90248296\n",
      "Iteration 8809, loss = 10.51660455\n",
      "Iteration 8810, loss = 10.14527783\n",
      "Iteration 8811, loss = 9.88632283\n",
      "Iteration 8812, loss = 10.52254526\n",
      "Iteration 8813, loss = 9.96989327\n",
      "Iteration 8814, loss = 10.11233216\n",
      "Iteration 8815, loss = 9.84797801\n",
      "Iteration 8816, loss = 10.08641248\n",
      "Iteration 8817, loss = 9.79796260\n",
      "Iteration 8818, loss = 9.79148198\n",
      "Iteration 8819, loss = 10.27123139\n",
      "Iteration 8820, loss = 10.31926468\n",
      "Iteration 8821, loss = 10.67373914\n",
      "Iteration 8822, loss = 10.09878789\n",
      "Iteration 8823, loss = 9.72180089\n",
      "Iteration 8824, loss = 9.70458381\n",
      "Iteration 8825, loss = 9.75180990\n",
      "Iteration 8826, loss = 10.22159507\n",
      "Iteration 8827, loss = 10.39928785\n",
      "Iteration 8828, loss = 10.70418596\n",
      "Iteration 8829, loss = 10.29814415\n",
      "Iteration 8830, loss = 9.99397696\n",
      "Iteration 8831, loss = 10.08807545\n",
      "Iteration 8832, loss = 9.90694011\n",
      "Iteration 8833, loss = 10.36517027\n",
      "Iteration 8834, loss = 10.08929151\n",
      "Iteration 8835, loss = 9.94439897\n",
      "Iteration 8836, loss = 9.88012984\n",
      "Iteration 8837, loss = 9.75479252\n",
      "Iteration 8838, loss = 9.90048745\n",
      "Iteration 8839, loss = 9.62592773\n",
      "Iteration 8840, loss = 9.69921195\n",
      "Iteration 8841, loss = 9.87377735\n",
      "Iteration 8842, loss = 10.62534248\n",
      "Iteration 8843, loss = 10.50157582\n",
      "Iteration 8844, loss = 10.97652112\n",
      "Iteration 8845, loss = 10.92900786\n",
      "Iteration 8846, loss = 10.93362000\n",
      "Iteration 8847, loss = 10.84550436\n",
      "Iteration 8848, loss = 11.39191182\n",
      "Iteration 8849, loss = 11.41711680\n",
      "Iteration 8850, loss = 11.11290488\n",
      "Iteration 8851, loss = 10.26247816\n",
      "Iteration 8852, loss = 9.98879903\n",
      "Iteration 8853, loss = 10.17755499\n",
      "Iteration 8854, loss = 11.10022133\n",
      "Iteration 8855, loss = 10.40540526\n",
      "Iteration 8856, loss = 10.72082919\n",
      "Iteration 8857, loss = 10.49184265\n",
      "Iteration 8858, loss = 10.10733837\n",
      "Iteration 8859, loss = 10.60745977\n",
      "Iteration 8860, loss = 10.27532362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8861, loss = 10.79858171\n",
      "Iteration 8862, loss = 11.91750602\n",
      "Iteration 8863, loss = 10.11657226\n",
      "Iteration 8864, loss = 9.81054335\n",
      "Iteration 8865, loss = 9.78912155\n",
      "Iteration 8866, loss = 9.84437715\n",
      "Iteration 8867, loss = 10.09380872\n",
      "Iteration 8868, loss = 10.01073913\n",
      "Iteration 8869, loss = 9.91001990\n",
      "Iteration 8870, loss = 10.61223818\n",
      "Iteration 8871, loss = 10.56076172\n",
      "Iteration 8872, loss = 10.79245954\n",
      "Iteration 8873, loss = 10.24046239\n",
      "Iteration 8874, loss = 10.13783556\n",
      "Iteration 8875, loss = 10.18032159\n",
      "Iteration 8876, loss = 10.13438981\n",
      "Iteration 8877, loss = 10.20425461\n",
      "Iteration 8878, loss = 11.12621876\n",
      "Iteration 8879, loss = 11.48145238\n",
      "Iteration 8880, loss = 10.62411805\n",
      "Iteration 8881, loss = 10.49844011\n",
      "Iteration 8882, loss = 9.65448517\n",
      "Iteration 8883, loss = 9.73811263\n",
      "Iteration 8884, loss = 10.75952478\n",
      "Iteration 8885, loss = 9.94033884\n",
      "Iteration 8886, loss = 10.45837915\n",
      "Iteration 8887, loss = 10.92996039\n",
      "Iteration 8888, loss = 10.78943406\n",
      "Iteration 8889, loss = 10.26531784\n",
      "Iteration 8890, loss = 9.82628142\n",
      "Iteration 8891, loss = 9.76240705\n",
      "Iteration 8892, loss = 9.85920385\n",
      "Iteration 8893, loss = 9.94355971\n",
      "Iteration 8894, loss = 10.20671613\n",
      "Iteration 8895, loss = 10.42129373\n",
      "Iteration 8896, loss = 10.06451064\n",
      "Iteration 8897, loss = 10.14564137\n",
      "Iteration 8898, loss = 11.41807288\n",
      "Iteration 8899, loss = 11.27996040\n",
      "Iteration 8900, loss = 10.53553760\n",
      "Iteration 8901, loss = 10.49428478\n",
      "Iteration 8902, loss = 10.14547149\n",
      "Iteration 8903, loss = 10.22823038\n",
      "Iteration 8904, loss = 9.60751790\n",
      "Iteration 8905, loss = 10.03606182\n",
      "Iteration 8906, loss = 9.89827439\n",
      "Iteration 8907, loss = 10.09019097\n",
      "Iteration 8908, loss = 10.55040752\n",
      "Iteration 8909, loss = 10.89464568\n",
      "Iteration 8910, loss = 11.08997065\n",
      "Iteration 8911, loss = 9.71805343\n",
      "Iteration 8912, loss = 9.83799848\n",
      "Iteration 8913, loss = 10.69976907\n",
      "Iteration 8914, loss = 10.76423358\n",
      "Iteration 8915, loss = 10.54583705\n",
      "Iteration 8916, loss = 9.88293846\n",
      "Iteration 8917, loss = 9.97917349\n",
      "Iteration 8918, loss = 10.18465034\n",
      "Iteration 8919, loss = 11.19527964\n",
      "Iteration 8920, loss = 10.47832872\n",
      "Iteration 8921, loss = 10.28579457\n",
      "Iteration 8922, loss = 10.73552807\n",
      "Iteration 8923, loss = 11.33715041\n",
      "Iteration 8924, loss = 11.56600382\n",
      "Iteration 8925, loss = 10.79917609\n",
      "Iteration 8926, loss = 10.46524351\n",
      "Iteration 8927, loss = 10.17810123\n",
      "Iteration 8928, loss = 9.70477476\n",
      "Iteration 8929, loss = 10.51576650\n",
      "Iteration 8930, loss = 10.41063945\n",
      "Iteration 8931, loss = 10.51184049\n",
      "Iteration 8932, loss = 10.08307178\n",
      "Iteration 8933, loss = 10.67264712\n",
      "Iteration 8934, loss = 11.15189393\n",
      "Iteration 8935, loss = 10.62203197\n",
      "Iteration 8936, loss = 10.27469633\n",
      "Iteration 8937, loss = 9.97586466\n",
      "Iteration 8938, loss = 10.18337784\n",
      "Iteration 8939, loss = 9.93836208\n",
      "Iteration 8940, loss = 9.80497741\n",
      "Iteration 8941, loss = 9.76057098\n",
      "Iteration 8942, loss = 10.17425193\n",
      "Iteration 8943, loss = 9.85424877\n",
      "Iteration 8944, loss = 9.74759250\n",
      "Iteration 8945, loss = 10.62580757\n",
      "Iteration 8946, loss = 10.43700147\n",
      "Iteration 8947, loss = 10.51600209\n",
      "Iteration 8948, loss = 9.95186248\n",
      "Iteration 8949, loss = 10.09651511\n",
      "Iteration 8950, loss = 10.66280001\n",
      "Iteration 8951, loss = 11.09694195\n",
      "Iteration 8952, loss = 11.43216747\n",
      "Iteration 8953, loss = 11.11744628\n",
      "Iteration 8954, loss = 10.16012359\n",
      "Iteration 8955, loss = 10.12698056\n",
      "Iteration 8956, loss = 10.23626045\n",
      "Iteration 8957, loss = 10.64817717\n",
      "Iteration 8958, loss = 9.93729568\n",
      "Iteration 8959, loss = 10.08046407\n",
      "Iteration 8960, loss = 10.39076396\n",
      "Iteration 8961, loss = 10.06315836\n",
      "Iteration 8962, loss = 10.25149946\n",
      "Iteration 8963, loss = 10.75983621\n",
      "Iteration 8964, loss = 10.08497437\n",
      "Iteration 8965, loss = 10.47802888\n",
      "Iteration 8966, loss = 10.24157356\n",
      "Iteration 8967, loss = 9.70009808\n",
      "Iteration 8968, loss = 10.06203114\n",
      "Iteration 8969, loss = 10.40758447\n",
      "Iteration 8970, loss = 10.62317313\n",
      "Iteration 8971, loss = 11.10491564\n",
      "Iteration 8972, loss = 11.49744766\n",
      "Iteration 8973, loss = 13.17142272\n",
      "Iteration 8974, loss = 10.79945051\n",
      "Iteration 8975, loss = 11.22836260\n",
      "Iteration 8976, loss = 13.80143035\n",
      "Iteration 8977, loss = 12.27855969\n",
      "Iteration 8978, loss = 12.16876589\n",
      "Iteration 8979, loss = 11.47453639\n",
      "Iteration 8980, loss = 11.16063198\n",
      "Iteration 8981, loss = 11.10799955\n",
      "Iteration 8982, loss = 10.29968971\n",
      "Iteration 8983, loss = 10.77198547\n",
      "Iteration 8984, loss = 10.69286700\n",
      "Iteration 8985, loss = 10.81588610\n",
      "Iteration 8986, loss = 9.89903034\n",
      "Iteration 8987, loss = 9.93725360\n",
      "Iteration 8988, loss = 9.66988512\n",
      "Iteration 8989, loss = 10.13690311\n",
      "Iteration 8990, loss = 10.13301485\n",
      "Iteration 8991, loss = 10.09684801\n",
      "Iteration 8992, loss = 10.02513033\n",
      "Iteration 8993, loss = 9.90710801\n",
      "Iteration 8994, loss = 10.15042764\n",
      "Iteration 8995, loss = 10.32745052\n",
      "Iteration 8996, loss = 10.20850917\n",
      "Iteration 8997, loss = 10.24763915\n",
      "Iteration 8998, loss = 9.78570410\n",
      "Iteration 8999, loss = 10.32213755\n",
      "Iteration 9000, loss = 10.00727989\n",
      "Iteration 9001, loss = 10.07206137\n",
      "Iteration 9002, loss = 10.19301514\n",
      "Iteration 9003, loss = 10.81911876\n",
      "Iteration 9004, loss = 10.91071332\n",
      "Iteration 9005, loss = 10.53423354\n",
      "Iteration 9006, loss = 11.05203415\n",
      "Iteration 9007, loss = 10.33063021\n",
      "Iteration 9008, loss = 9.93982043\n",
      "Iteration 9009, loss = 9.84788872\n",
      "Iteration 9010, loss = 10.02997404\n",
      "Iteration 9011, loss = 10.14393936\n",
      "Iteration 9012, loss = 10.44192638\n",
      "Iteration 9013, loss = 10.03797308\n",
      "Iteration 9014, loss = 10.87915813\n",
      "Iteration 9015, loss = 10.90796171\n",
      "Iteration 9016, loss = 11.67404902\n",
      "Iteration 9017, loss = 10.90978234\n",
      "Iteration 9018, loss = 10.76365075\n",
      "Iteration 9019, loss = 11.27500324\n",
      "Iteration 9020, loss = 11.48204786\n",
      "Iteration 9021, loss = 10.72539627\n",
      "Iteration 9022, loss = 10.26774915\n",
      "Iteration 9023, loss = 10.80045273\n",
      "Iteration 9024, loss = 10.26945018\n",
      "Iteration 9025, loss = 10.31848742\n",
      "Iteration 9026, loss = 10.57254349\n",
      "Iteration 9027, loss = 10.20151898\n",
      "Iteration 9028, loss = 9.87189467\n",
      "Iteration 9029, loss = 9.94186199\n",
      "Iteration 9030, loss = 9.99346462\n",
      "Iteration 9031, loss = 9.57787375\n",
      "Iteration 9032, loss = 10.11600620\n",
      "Iteration 9033, loss = 9.52293404\n",
      "Iteration 9034, loss = 9.77941377\n",
      "Iteration 9035, loss = 9.88485461\n",
      "Iteration 9036, loss = 9.68842624\n",
      "Iteration 9037, loss = 9.88591831\n",
      "Iteration 9038, loss = 10.01884612\n",
      "Iteration 9039, loss = 10.54128251\n",
      "Iteration 9040, loss = 12.65328982\n",
      "Iteration 9041, loss = 10.77574952\n",
      "Iteration 9042, loss = 10.56135268\n",
      "Iteration 9043, loss = 10.06377655\n",
      "Iteration 9044, loss = 10.38156862\n",
      "Iteration 9045, loss = 9.73605180\n",
      "Iteration 9046, loss = 9.50571903\n",
      "Iteration 9047, loss = 9.69058955\n",
      "Iteration 9048, loss = 9.70310449\n",
      "Iteration 9049, loss = 10.05150863\n",
      "Iteration 9050, loss = 10.01922281\n",
      "Iteration 9051, loss = 9.98378879\n",
      "Iteration 9052, loss = 10.33995152\n",
      "Iteration 9053, loss = 10.16804304\n",
      "Iteration 9054, loss = 10.94054709\n",
      "Iteration 9055, loss = 10.13838346\n",
      "Iteration 9056, loss = 9.89048804\n",
      "Iteration 9057, loss = 9.70266463\n",
      "Iteration 9058, loss = 10.28710693\n",
      "Iteration 9059, loss = 9.84561871\n",
      "Iteration 9060, loss = 10.55326931\n",
      "Iteration 9061, loss = 11.29482770\n",
      "Iteration 9062, loss = 10.97946476\n",
      "Iteration 9063, loss = 10.85795055\n",
      "Iteration 9064, loss = 11.76247889\n",
      "Iteration 9065, loss = 10.95383968\n",
      "Iteration 9066, loss = 10.23874569\n",
      "Iteration 9067, loss = 9.78394341\n",
      "Iteration 9068, loss = 9.78990030\n",
      "Iteration 9069, loss = 9.74661357\n",
      "Iteration 9070, loss = 9.66081721\n",
      "Iteration 9071, loss = 9.54119743\n",
      "Iteration 9072, loss = 9.94545044\n",
      "Iteration 9073, loss = 9.56109586\n",
      "Iteration 9074, loss = 9.80731909\n",
      "Iteration 9075, loss = 10.14802093\n",
      "Iteration 9076, loss = 10.24861762\n",
      "Iteration 9077, loss = 9.55423847\n",
      "Iteration 9078, loss = 10.01780316\n",
      "Iteration 9079, loss = 10.00259461\n",
      "Iteration 9080, loss = 9.89924820\n",
      "Iteration 9081, loss = 9.90728877\n",
      "Iteration 9082, loss = 9.47186625\n",
      "Iteration 9083, loss = 10.65701871\n",
      "Iteration 9084, loss = 10.78405872\n",
      "Iteration 9085, loss = 10.46927587\n",
      "Iteration 9086, loss = 10.38326911\n",
      "Iteration 9087, loss = 10.02635420\n",
      "Iteration 9088, loss = 10.17196996\n",
      "Iteration 9089, loss = 9.95643442\n",
      "Iteration 9090, loss = 10.32823159\n",
      "Iteration 9091, loss = 9.84833845\n",
      "Iteration 9092, loss = 10.34118739\n",
      "Iteration 9093, loss = 11.09082696\n",
      "Iteration 9094, loss = 10.34551469\n",
      "Iteration 9095, loss = 10.95405756\n",
      "Iteration 9096, loss = 10.71261083\n",
      "Iteration 9097, loss = 10.75654729\n",
      "Iteration 9098, loss = 10.83787332\n",
      "Iteration 9099, loss = 10.81241712\n",
      "Iteration 9100, loss = 10.03147593\n",
      "Iteration 9101, loss = 9.78917823\n",
      "Iteration 9102, loss = 9.82861763\n",
      "Iteration 9103, loss = 9.97856654\n",
      "Iteration 9104, loss = 10.68858710\n",
      "Iteration 9105, loss = 11.10076890\n",
      "Iteration 9106, loss = 10.47306305\n",
      "Iteration 9107, loss = 10.13188061\n",
      "Iteration 9108, loss = 9.97006264\n",
      "Iteration 9109, loss = 9.73341082\n",
      "Iteration 9110, loss = 9.68322240\n",
      "Iteration 9111, loss = 9.91090469\n",
      "Iteration 9112, loss = 9.69480734\n",
      "Iteration 9113, loss = 9.97089222\n",
      "Iteration 9114, loss = 10.28278683\n",
      "Iteration 9115, loss = 10.11221600\n",
      "Iteration 9116, loss = 9.85214769\n",
      "Iteration 9117, loss = 10.60476746\n",
      "Iteration 9118, loss = 10.23633457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9119, loss = 10.24898118\n",
      "Iteration 9120, loss = 9.81810813\n",
      "Iteration 9121, loss = 10.66351351\n",
      "Iteration 9122, loss = 10.43321035\n",
      "Iteration 9123, loss = 10.12378877\n",
      "Iteration 9124, loss = 11.36902229\n",
      "Iteration 9125, loss = 9.77949070\n",
      "Iteration 9126, loss = 9.55016739\n",
      "Iteration 9127, loss = 9.67252417\n",
      "Iteration 9128, loss = 9.66970673\n",
      "Iteration 9129, loss = 9.67906865\n",
      "Iteration 9130, loss = 9.48373600\n",
      "Iteration 9131, loss = 9.52715507\n",
      "Iteration 9132, loss = 9.78379763\n",
      "Iteration 9133, loss = 9.48896764\n",
      "Iteration 9134, loss = 9.78489645\n",
      "Iteration 9135, loss = 9.66442060\n",
      "Iteration 9136, loss = 9.59564603\n",
      "Iteration 9137, loss = 9.76711072\n",
      "Iteration 9138, loss = 10.21301302\n",
      "Iteration 9139, loss = 10.53761995\n",
      "Iteration 9140, loss = 10.79771723\n",
      "Iteration 9141, loss = 11.14402540\n",
      "Iteration 9142, loss = 10.51423999\n",
      "Iteration 9143, loss = 11.87345727\n",
      "Iteration 9144, loss = 10.16926864\n",
      "Iteration 9145, loss = 11.22417972\n",
      "Iteration 9146, loss = 11.03507491\n",
      "Iteration 9147, loss = 11.44816758\n",
      "Iteration 9148, loss = 10.09068501\n",
      "Iteration 9149, loss = 10.45491849\n",
      "Iteration 9150, loss = 10.73740220\n",
      "Iteration 9151, loss = 10.61562878\n",
      "Iteration 9152, loss = 10.82470946\n",
      "Iteration 9153, loss = 9.97008339\n",
      "Iteration 9154, loss = 9.95699967\n",
      "Iteration 9155, loss = 9.71973773\n",
      "Iteration 9156, loss = 10.45574989\n",
      "Iteration 9157, loss = 10.04683589\n",
      "Iteration 9158, loss = 11.29186262\n",
      "Iteration 9159, loss = 11.22047078\n",
      "Iteration 9160, loss = 10.26077625\n",
      "Iteration 9161, loss = 9.82066835\n",
      "Iteration 9162, loss = 9.70611575\n",
      "Iteration 9163, loss = 9.50710542\n",
      "Iteration 9164, loss = 9.63577887\n",
      "Iteration 9165, loss = 9.64870346\n",
      "Iteration 9166, loss = 9.90967001\n",
      "Iteration 9167, loss = 10.98664739\n",
      "Iteration 9168, loss = 10.03615987\n",
      "Iteration 9169, loss = 10.22209167\n",
      "Iteration 9170, loss = 10.45722171\n",
      "Iteration 9171, loss = 10.11077048\n",
      "Iteration 9172, loss = 10.21321095\n",
      "Iteration 9173, loss = 10.76183493\n",
      "Iteration 9174, loss = 10.20791176\n",
      "Iteration 9175, loss = 10.50588100\n",
      "Iteration 9176, loss = 10.07586448\n",
      "Iteration 9177, loss = 9.72719537\n",
      "Iteration 9178, loss = 10.95512455\n",
      "Iteration 9179, loss = 11.93317170\n",
      "Iteration 9180, loss = 11.16760977\n",
      "Iteration 9181, loss = 9.98235226\n",
      "Iteration 9182, loss = 10.38208153\n",
      "Iteration 9183, loss = 9.67899655\n",
      "Iteration 9184, loss = 10.01744782\n",
      "Iteration 9185, loss = 9.59804530\n",
      "Iteration 9186, loss = 9.37051061\n",
      "Iteration 9187, loss = 9.82964221\n",
      "Iteration 9188, loss = 9.67554131\n",
      "Iteration 9189, loss = 9.53837051\n",
      "Iteration 9190, loss = 9.91606610\n",
      "Iteration 9191, loss = 10.76742852\n",
      "Iteration 9192, loss = 10.25255750\n",
      "Iteration 9193, loss = 11.51303013\n",
      "Iteration 9194, loss = 10.83701817\n",
      "Iteration 9195, loss = 10.32672199\n",
      "Iteration 9196, loss = 9.76639367\n",
      "Iteration 9197, loss = 9.51962946\n",
      "Iteration 9198, loss = 10.00177152\n",
      "Iteration 9199, loss = 9.72490938\n",
      "Iteration 9200, loss = 9.95681915\n",
      "Iteration 9201, loss = 9.95698905\n",
      "Iteration 9202, loss = 11.58675178\n",
      "Iteration 9203, loss = 10.73000393\n",
      "Iteration 9204, loss = 10.37917992\n",
      "Iteration 9205, loss = 9.85724832\n",
      "Iteration 9206, loss = 9.70603015\n",
      "Iteration 9207, loss = 9.73449415\n",
      "Iteration 9208, loss = 9.53655615\n",
      "Iteration 9209, loss = 9.94685419\n",
      "Iteration 9210, loss = 9.98790145\n",
      "Iteration 9211, loss = 10.01103829\n",
      "Iteration 9212, loss = 10.12391271\n",
      "Iteration 9213, loss = 10.16456802\n",
      "Iteration 9214, loss = 10.24000505\n",
      "Iteration 9215, loss = 10.06464323\n",
      "Iteration 9216, loss = 11.01086839\n",
      "Iteration 9217, loss = 10.17724514\n",
      "Iteration 9218, loss = 10.11846668\n",
      "Iteration 9219, loss = 9.76185660\n",
      "Iteration 9220, loss = 9.84787320\n",
      "Iteration 9221, loss = 11.13832242\n",
      "Iteration 9222, loss = 10.72459299\n",
      "Iteration 9223, loss = 10.06970634\n",
      "Iteration 9224, loss = 9.72114883\n",
      "Iteration 9225, loss = 10.00573264\n",
      "Iteration 9226, loss = 10.65834703\n",
      "Iteration 9227, loss = 10.12907663\n",
      "Iteration 9228, loss = 10.40593447\n",
      "Iteration 9229, loss = 9.85345544\n",
      "Iteration 9230, loss = 10.37940409\n",
      "Iteration 9231, loss = 10.17191881\n",
      "Iteration 9232, loss = 10.55434821\n",
      "Iteration 9233, loss = 10.62743789\n",
      "Iteration 9234, loss = 11.91488585\n",
      "Iteration 9235, loss = 10.71411278\n",
      "Iteration 9236, loss = 11.74341979\n",
      "Iteration 9237, loss = 10.46061033\n",
      "Iteration 9238, loss = 10.57319342\n",
      "Iteration 9239, loss = 10.93290716\n",
      "Iteration 9240, loss = 11.71822144\n",
      "Iteration 9241, loss = 9.92871977\n",
      "Iteration 9242, loss = 9.88776974\n",
      "Iteration 9243, loss = 9.58897855\n",
      "Iteration 9244, loss = 9.70388691\n",
      "Iteration 9245, loss = 10.20056426\n",
      "Iteration 9246, loss = 10.03085466\n",
      "Iteration 9247, loss = 10.55174630\n",
      "Iteration 9248, loss = 11.68595939\n",
      "Iteration 9249, loss = 10.25049828\n",
      "Iteration 9250, loss = 10.44487620\n",
      "Iteration 9251, loss = 10.83311854\n",
      "Iteration 9252, loss = 10.06531777\n",
      "Iteration 9253, loss = 10.85857507\n",
      "Iteration 9254, loss = 10.59796016\n",
      "Iteration 9255, loss = 10.48145951\n",
      "Iteration 9256, loss = 11.47662871\n",
      "Iteration 9257, loss = 10.71355922\n",
      "Iteration 9258, loss = 10.76616879\n",
      "Iteration 9259, loss = 10.58730640\n",
      "Iteration 9260, loss = 10.28801482\n",
      "Iteration 9261, loss = 11.98775510\n",
      "Iteration 9262, loss = 10.68331931\n",
      "Iteration 9263, loss = 10.48048671\n",
      "Iteration 9264, loss = 9.88419066\n",
      "Iteration 9265, loss = 10.08041350\n",
      "Iteration 9266, loss = 9.52914846\n",
      "Iteration 9267, loss = 10.13262189\n",
      "Iteration 9268, loss = 9.68479642\n",
      "Iteration 9269, loss = 9.65294732\n",
      "Iteration 9270, loss = 9.70937009\n",
      "Iteration 9271, loss = 10.57683692\n",
      "Iteration 9272, loss = 10.13605857\n",
      "Iteration 9273, loss = 9.61449984\n",
      "Iteration 9274, loss = 9.88772288\n",
      "Iteration 9275, loss = 9.60099364\n",
      "Iteration 9276, loss = 9.31246486\n",
      "Iteration 9277, loss = 9.95507966\n",
      "Iteration 9278, loss = 9.86112220\n",
      "Iteration 9279, loss = 9.56158003\n",
      "Iteration 9280, loss = 9.55796458\n",
      "Iteration 9281, loss = 9.64129151\n",
      "Iteration 9282, loss = 9.77666483\n",
      "Iteration 9283, loss = 9.96478316\n",
      "Iteration 9284, loss = 9.90493625\n",
      "Iteration 9285, loss = 10.23650702\n",
      "Iteration 9286, loss = 10.26385853\n",
      "Iteration 9287, loss = 10.55646361\n",
      "Iteration 9288, loss = 9.67761336\n",
      "Iteration 9289, loss = 10.22740316\n",
      "Iteration 9290, loss = 10.40249404\n",
      "Iteration 9291, loss = 10.84877670\n",
      "Iteration 9292, loss = 9.62099733\n",
      "Iteration 9293, loss = 9.46967485\n",
      "Iteration 9294, loss = 9.48569129\n",
      "Iteration 9295, loss = 9.71788191\n",
      "Iteration 9296, loss = 9.96579987\n",
      "Iteration 9297, loss = 9.83046565\n",
      "Iteration 9298, loss = 10.67100077\n",
      "Iteration 9299, loss = 10.45346062\n",
      "Iteration 9300, loss = 11.72933180\n",
      "Iteration 9301, loss = 10.16689936\n",
      "Iteration 9302, loss = 10.00753654\n",
      "Iteration 9303, loss = 10.60122822\n",
      "Iteration 9304, loss = 10.27214158\n",
      "Iteration 9305, loss = 10.71386518\n",
      "Iteration 9306, loss = 9.99126476\n",
      "Iteration 9307, loss = 10.55909190\n",
      "Iteration 9308, loss = 11.24165304\n",
      "Iteration 9309, loss = 10.47447019\n",
      "Iteration 9310, loss = 10.11213497\n",
      "Iteration 9311, loss = 10.61263545\n",
      "Iteration 9312, loss = 10.35789800\n",
      "Iteration 9313, loss = 9.73096265\n",
      "Iteration 9314, loss = 9.43554884\n",
      "Iteration 9315, loss = 9.88673501\n",
      "Iteration 9316, loss = 9.55084079\n",
      "Iteration 9317, loss = 9.60495420\n",
      "Iteration 9318, loss = 9.73803226\n",
      "Iteration 9319, loss = 10.13943899\n",
      "Iteration 9320, loss = 10.55389699\n",
      "Iteration 9321, loss = 10.80494613\n",
      "Iteration 9322, loss = 10.11928501\n",
      "Iteration 9323, loss = 11.03492445\n",
      "Iteration 9324, loss = 12.53753318\n",
      "Iteration 9325, loss = 10.45220775\n",
      "Iteration 9326, loss = 10.02519122\n",
      "Iteration 9327, loss = 9.98316369\n",
      "Iteration 9328, loss = 9.89482213\n",
      "Iteration 9329, loss = 9.77373083\n",
      "Iteration 9330, loss = 9.61179497\n",
      "Iteration 9331, loss = 9.53256792\n",
      "Iteration 9332, loss = 9.49974922\n",
      "Iteration 9333, loss = 9.90606633\n",
      "Iteration 9334, loss = 9.66993559\n",
      "Iteration 9335, loss = 9.58367938\n",
      "Iteration 9336, loss = 10.06408802\n",
      "Iteration 9337, loss = 10.07552649\n",
      "Iteration 9338, loss = 10.68207930\n",
      "Iteration 9339, loss = 10.57280949\n",
      "Iteration 9340, loss = 10.22894943\n",
      "Iteration 9341, loss = 10.99665897\n",
      "Iteration 9342, loss = 10.91590888\n",
      "Iteration 9343, loss = 10.08604075\n",
      "Iteration 9344, loss = 10.37141694\n",
      "Iteration 9345, loss = 9.85865711\n",
      "Iteration 9346, loss = 10.06376642\n",
      "Iteration 9347, loss = 10.15152017\n",
      "Iteration 9348, loss = 10.14885636\n",
      "Iteration 9349, loss = 10.23198977\n",
      "Iteration 9350, loss = 10.22406500\n",
      "Iteration 9351, loss = 10.07010217\n",
      "Iteration 9352, loss = 9.70173461\n",
      "Iteration 9353, loss = 9.58043352\n",
      "Iteration 9354, loss = 9.80687529\n",
      "Iteration 9355, loss = 9.34759071\n",
      "Iteration 9356, loss = 10.75655768\n",
      "Iteration 9357, loss = 10.47422821\n",
      "Iteration 9358, loss = 9.65742139\n",
      "Iteration 9359, loss = 10.14419216\n",
      "Iteration 9360, loss = 9.83122851\n",
      "Iteration 9361, loss = 9.99390170\n",
      "Iteration 9362, loss = 10.41644808\n",
      "Iteration 9363, loss = 10.75862057\n",
      "Iteration 9364, loss = 10.25337120\n",
      "Iteration 9365, loss = 9.99276419\n",
      "Iteration 9366, loss = 9.53594843\n",
      "Iteration 9367, loss = 9.72894859\n",
      "Iteration 9368, loss = 10.19600398\n",
      "Iteration 9369, loss = 11.14943809\n",
      "Iteration 9370, loss = 10.46365279\n",
      "Iteration 9371, loss = 9.73232273\n",
      "Iteration 9372, loss = 9.58089389\n",
      "Iteration 9373, loss = 9.83840573\n",
      "Iteration 9374, loss = 10.76003682\n",
      "Iteration 9375, loss = 10.96724789\n",
      "Iteration 9376, loss = 10.20438957\n",
      "Iteration 9377, loss = 9.98191888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9378, loss = 10.04703815\n",
      "Iteration 9379, loss = 9.68034132\n",
      "Iteration 9380, loss = 9.63266602\n",
      "Iteration 9381, loss = 9.89061279\n",
      "Iteration 9382, loss = 9.70156434\n",
      "Iteration 9383, loss = 9.49505309\n",
      "Iteration 9384, loss = 9.57017786\n",
      "Iteration 9385, loss = 9.40345731\n",
      "Iteration 9386, loss = 10.14067272\n",
      "Iteration 9387, loss = 10.32795576\n",
      "Iteration 9388, loss = 9.99989480\n",
      "Iteration 9389, loss = 10.01351406\n",
      "Iteration 9390, loss = 9.91982022\n",
      "Iteration 9391, loss = 9.68512515\n",
      "Iteration 9392, loss = 9.60255240\n",
      "Iteration 9393, loss = 10.02839437\n",
      "Iteration 9394, loss = 9.47190901\n",
      "Iteration 9395, loss = 9.59607236\n",
      "Iteration 9396, loss = 9.80335727\n",
      "Iteration 9397, loss = 9.56886032\n",
      "Iteration 9398, loss = 9.41465574\n",
      "Iteration 9399, loss = 9.44768282\n",
      "Iteration 9400, loss = 9.73092578\n",
      "Iteration 9401, loss = 9.78735624\n",
      "Iteration 9402, loss = 10.59522940\n",
      "Iteration 9403, loss = 10.22663327\n",
      "Iteration 9404, loss = 9.81556266\n",
      "Iteration 9405, loss = 9.63865021\n",
      "Iteration 9406, loss = 10.67320400\n",
      "Iteration 9407, loss = 10.06843418\n",
      "Iteration 9408, loss = 11.16305465\n",
      "Iteration 9409, loss = 10.58025820\n",
      "Iteration 9410, loss = 10.90401084\n",
      "Iteration 9411, loss = 10.76459619\n",
      "Iteration 9412, loss = 10.81141596\n",
      "Iteration 9413, loss = 10.57705426\n",
      "Iteration 9414, loss = 9.89178500\n",
      "Iteration 9415, loss = 9.82094660\n",
      "Iteration 9416, loss = 9.45262239\n",
      "Iteration 9417, loss = 9.94197258\n",
      "Iteration 9418, loss = 9.74048352\n",
      "Iteration 9419, loss = 9.42069339\n",
      "Iteration 9420, loss = 9.62458489\n",
      "Iteration 9421, loss = 9.59336729\n",
      "Iteration 9422, loss = 10.63945720\n",
      "Iteration 9423, loss = 11.49757871\n",
      "Iteration 9424, loss = 11.48492109\n",
      "Iteration 9425, loss = 11.72557083\n",
      "Iteration 9426, loss = 10.73251267\n",
      "Iteration 9427, loss = 9.69180805\n",
      "Iteration 9428, loss = 9.45443832\n",
      "Iteration 9429, loss = 9.62228464\n",
      "Iteration 9430, loss = 9.56188082\n",
      "Iteration 9431, loss = 9.67809517\n",
      "Iteration 9432, loss = 9.88060415\n",
      "Iteration 9433, loss = 10.15593019\n",
      "Iteration 9434, loss = 10.27514877\n",
      "Iteration 9435, loss = 9.73028787\n",
      "Iteration 9436, loss = 9.92359027\n",
      "Iteration 9437, loss = 9.91856712\n",
      "Iteration 9438, loss = 9.88951098\n",
      "Iteration 9439, loss = 9.89301056\n",
      "Iteration 9440, loss = 10.38161482\n",
      "Iteration 9441, loss = 10.67654268\n",
      "Iteration 9442, loss = 12.28067637\n",
      "Iteration 9443, loss = 11.67397453\n",
      "Iteration 9444, loss = 11.56754393\n",
      "Iteration 9445, loss = 10.34129724\n",
      "Iteration 9446, loss = 12.29843020\n",
      "Iteration 9447, loss = 10.92032729\n",
      "Iteration 9448, loss = 10.53495938\n",
      "Iteration 9449, loss = 9.51323971\n",
      "Iteration 9450, loss = 9.47332319\n",
      "Iteration 9451, loss = 10.09637387\n",
      "Iteration 9452, loss = 10.32216381\n",
      "Iteration 9453, loss = 10.41224283\n",
      "Iteration 9454, loss = 9.67830293\n",
      "Iteration 9455, loss = 9.83332528\n",
      "Iteration 9456, loss = 10.26599078\n",
      "Iteration 9457, loss = 9.88782026\n",
      "Iteration 9458, loss = 9.76688230\n",
      "Iteration 9459, loss = 9.54585170\n",
      "Iteration 9460, loss = 9.69835125\n",
      "Iteration 9461, loss = 9.63551180\n",
      "Iteration 9462, loss = 10.17640444\n",
      "Iteration 9463, loss = 9.78587410\n",
      "Iteration 9464, loss = 10.66678978\n",
      "Iteration 9465, loss = 9.71655502\n",
      "Iteration 9466, loss = 9.90712623\n",
      "Iteration 9467, loss = 10.07022108\n",
      "Iteration 9468, loss = 9.85194168\n",
      "Iteration 9469, loss = 10.02126715\n",
      "Iteration 9470, loss = 10.03499212\n",
      "Iteration 9471, loss = 9.94520935\n",
      "Iteration 9472, loss = 9.54112486\n",
      "Iteration 9473, loss = 9.36889266\n",
      "Iteration 9474, loss = 9.74241533\n",
      "Iteration 9475, loss = 9.66031028\n",
      "Iteration 9476, loss = 9.85567322\n",
      "Iteration 9477, loss = 9.99277833\n",
      "Iteration 9478, loss = 10.27613697\n",
      "Iteration 9479, loss = 10.84112996\n",
      "Iteration 9480, loss = 10.38930682\n",
      "Iteration 9481, loss = 9.86488139\n",
      "Iteration 9482, loss = 10.24239611\n",
      "Iteration 9483, loss = 10.97167793\n",
      "Iteration 9484, loss = 12.61414238\n",
      "Iteration 9485, loss = 10.65723978\n",
      "Iteration 9486, loss = 9.74810915\n",
      "Iteration 9487, loss = 9.47899226\n",
      "Iteration 9488, loss = 9.93751910\n",
      "Iteration 9489, loss = 9.54639027\n",
      "Iteration 9490, loss = 9.73287228\n",
      "Iteration 9491, loss = 10.58501249\n",
      "Iteration 9492, loss = 11.15452595\n",
      "Iteration 9493, loss = 10.43261986\n",
      "Iteration 9494, loss = 9.52211984\n",
      "Iteration 9495, loss = 11.03507004\n",
      "Iteration 9496, loss = 10.92114129\n",
      "Iteration 9497, loss = 10.12804290\n",
      "Iteration 9498, loss = 9.76667512\n",
      "Iteration 9499, loss = 9.50476138\n",
      "Iteration 9500, loss = 9.64278169\n",
      "Iteration 9501, loss = 9.25954233\n",
      "Iteration 9502, loss = 9.66477642\n",
      "Iteration 9503, loss = 9.80138492\n",
      "Iteration 9504, loss = 10.19833437\n",
      "Iteration 9505, loss = 11.34736134\n",
      "Iteration 9506, loss = 9.70245746\n",
      "Iteration 9507, loss = 9.80381935\n",
      "Iteration 9508, loss = 9.51551735\n",
      "Iteration 9509, loss = 9.40866422\n",
      "Iteration 9510, loss = 10.06532079\n",
      "Iteration 9511, loss = 9.71152173\n",
      "Iteration 9512, loss = 10.84576541\n",
      "Iteration 9513, loss = 10.20580024\n",
      "Iteration 9514, loss = 10.19548974\n",
      "Iteration 9515, loss = 10.26369087\n",
      "Iteration 9516, loss = 10.79975949\n",
      "Iteration 9517, loss = 11.51214485\n",
      "Iteration 9518, loss = 10.82944361\n",
      "Iteration 9519, loss = 10.19640700\n",
      "Iteration 9520, loss = 10.66565087\n",
      "Iteration 9521, loss = 9.60696472\n",
      "Iteration 9522, loss = 10.80955628\n",
      "Iteration 9523, loss = 9.80152418\n",
      "Iteration 9524, loss = 9.74425773\n",
      "Iteration 9525, loss = 9.99629673\n",
      "Iteration 9526, loss = 9.77565186\n",
      "Iteration 9527, loss = 9.33422755\n",
      "Iteration 9528, loss = 9.43382040\n",
      "Iteration 9529, loss = 9.53886317\n",
      "Iteration 9530, loss = 9.43386026\n",
      "Iteration 9531, loss = 9.60483038\n",
      "Iteration 9532, loss = 9.63094024\n",
      "Iteration 9533, loss = 10.37325641\n",
      "Iteration 9534, loss = 10.32335240\n",
      "Iteration 9535, loss = 10.30374558\n",
      "Iteration 9536, loss = 10.35416475\n",
      "Iteration 9537, loss = 11.06985813\n",
      "Iteration 9538, loss = 10.24147618\n",
      "Iteration 9539, loss = 9.96080984\n",
      "Iteration 9540, loss = 10.06848192\n",
      "Iteration 9541, loss = 10.32412144\n",
      "Iteration 9542, loss = 10.55248155\n",
      "Iteration 9543, loss = 11.31307626\n",
      "Iteration 9544, loss = 9.80566274\n",
      "Iteration 9545, loss = 10.23092519\n",
      "Iteration 9546, loss = 9.75065214\n",
      "Iteration 9547, loss = 9.90637298\n",
      "Iteration 9548, loss = 9.64784672\n",
      "Iteration 9549, loss = 9.62856915\n",
      "Iteration 9550, loss = 9.29969779\n",
      "Iteration 9551, loss = 9.45894646\n",
      "Iteration 9552, loss = 9.49878926\n",
      "Iteration 9553, loss = 9.36587926\n",
      "Iteration 9554, loss = 9.34808306\n",
      "Iteration 9555, loss = 9.57356118\n",
      "Iteration 9556, loss = 9.49493350\n",
      "Iteration 9557, loss = 9.50166188\n",
      "Iteration 9558, loss = 9.99213808\n",
      "Iteration 9559, loss = 9.50156309\n",
      "Iteration 9560, loss = 10.08536191\n",
      "Iteration 9561, loss = 9.53244686\n",
      "Iteration 9562, loss = 10.25509333\n",
      "Iteration 9563, loss = 10.09266853\n",
      "Iteration 9564, loss = 11.48312714\n",
      "Iteration 9565, loss = 10.65996288\n",
      "Iteration 9566, loss = 12.64343716\n",
      "Iteration 9567, loss = 12.13269044\n",
      "Iteration 9568, loss = 11.28184028\n",
      "Iteration 9569, loss = 9.92314101\n",
      "Iteration 9570, loss = 9.49372522\n",
      "Iteration 9571, loss = 10.03345773\n",
      "Iteration 9572, loss = 9.38722658\n",
      "Iteration 9573, loss = 9.97943904\n",
      "Iteration 9574, loss = 9.68969829\n",
      "Iteration 9575, loss = 9.58105129\n",
      "Iteration 9576, loss = 9.89959630\n",
      "Iteration 9577, loss = 9.96004818\n",
      "Iteration 9578, loss = 10.40572652\n",
      "Iteration 9579, loss = 9.28230520\n",
      "Iteration 9580, loss = 10.96702941\n",
      "Iteration 9581, loss = 10.12622780\n",
      "Iteration 9582, loss = 10.85087316\n",
      "Iteration 9583, loss = 11.30045027\n",
      "Iteration 9584, loss = 11.22316514\n",
      "Iteration 9585, loss = 11.19515692\n",
      "Iteration 9586, loss = 11.07599676\n",
      "Iteration 9587, loss = 9.74349293\n",
      "Iteration 9588, loss = 10.01403667\n",
      "Iteration 9589, loss = 9.79388034\n",
      "Iteration 9590, loss = 9.77955835\n",
      "Iteration 9591, loss = 9.60797509\n",
      "Iteration 9592, loss = 10.45680251\n",
      "Iteration 9593, loss = 9.50869260\n",
      "Iteration 9594, loss = 9.58196644\n",
      "Iteration 9595, loss = 10.04617312\n",
      "Iteration 9596, loss = 9.86695006\n",
      "Iteration 9597, loss = 10.09710372\n",
      "Iteration 9598, loss = 10.53653702\n",
      "Iteration 9599, loss = 10.27608459\n",
      "Iteration 9600, loss = 9.55674263\n",
      "Iteration 9601, loss = 9.55362943\n",
      "Iteration 9602, loss = 9.79645574\n",
      "Iteration 9603, loss = 10.46309083\n",
      "Iteration 9604, loss = 10.85429612\n",
      "Iteration 9605, loss = 9.87980127\n",
      "Iteration 9606, loss = 9.87418654\n",
      "Iteration 9607, loss = 10.17991821\n",
      "Iteration 9608, loss = 9.56619430\n",
      "Iteration 9609, loss = 9.71625390\n",
      "Iteration 9610, loss = 10.35614191\n",
      "Iteration 9611, loss = 9.31296012\n",
      "Iteration 9612, loss = 9.90851076\n",
      "Iteration 9613, loss = 9.88766518\n",
      "Iteration 9614, loss = 10.45640619\n",
      "Iteration 9615, loss = 10.09951773\n",
      "Iteration 9616, loss = 10.43791193\n",
      "Iteration 9617, loss = 12.05582826\n",
      "Iteration 9618, loss = 10.99172227\n",
      "Iteration 9619, loss = 9.58599518\n",
      "Iteration 9620, loss = 10.02333889\n",
      "Iteration 9621, loss = 11.06053952\n",
      "Iteration 9622, loss = 10.55982708\n",
      "Iteration 9623, loss = 10.67021554\n",
      "Iteration 9624, loss = 9.84677155\n",
      "Iteration 9625, loss = 10.67515388\n",
      "Iteration 9626, loss = 9.77226882\n",
      "Iteration 9627, loss = 9.68720198\n",
      "Iteration 9628, loss = 9.53433648\n",
      "Iteration 9629, loss = 9.61228121\n",
      "Iteration 9630, loss = 9.27249167\n",
      "Iteration 9631, loss = 9.94026776\n",
      "Iteration 9632, loss = 9.35011659\n",
      "Iteration 9633, loss = 9.57243801\n",
      "Iteration 9634, loss = 10.09427475\n",
      "Iteration 9635, loss = 9.58477758\n",
      "Iteration 9636, loss = 9.29326322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9637, loss = 9.67180010\n",
      "Iteration 9638, loss = 9.39694157\n",
      "Iteration 9639, loss = 9.49179266\n",
      "Iteration 9640, loss = 9.44431111\n",
      "Iteration 9641, loss = 9.85592980\n",
      "Iteration 9642, loss = 9.55609179\n",
      "Iteration 9643, loss = 10.44929092\n",
      "Iteration 9644, loss = 10.75439207\n",
      "Iteration 9645, loss = 10.47762827\n",
      "Iteration 9646, loss = 11.28355705\n",
      "Iteration 9647, loss = 13.83140621\n",
      "Iteration 9648, loss = 11.40385448\n",
      "Iteration 9649, loss = 10.60332028\n",
      "Iteration 9650, loss = 9.47774633\n",
      "Iteration 9651, loss = 9.54382361\n",
      "Iteration 9652, loss = 9.88011680\n",
      "Iteration 9653, loss = 9.96858408\n",
      "Iteration 9654, loss = 10.38863618\n",
      "Iteration 9655, loss = 10.00392054\n",
      "Iteration 9656, loss = 10.26865568\n",
      "Iteration 9657, loss = 9.99283833\n",
      "Iteration 9658, loss = 9.33287016\n",
      "Iteration 9659, loss = 10.17043479\n",
      "Iteration 9660, loss = 9.83314119\n",
      "Iteration 9661, loss = 9.48229918\n",
      "Iteration 9662, loss = 9.51894551\n",
      "Iteration 9663, loss = 9.46656434\n",
      "Iteration 9664, loss = 9.95068975\n",
      "Iteration 9665, loss = 10.19807527\n",
      "Iteration 9666, loss = 11.34293070\n",
      "Iteration 9667, loss = 10.37507272\n",
      "Iteration 9668, loss = 10.18451107\n",
      "Iteration 9669, loss = 9.50660423\n",
      "Iteration 9670, loss = 9.85810188\n",
      "Iteration 9671, loss = 9.98615415\n",
      "Iteration 9672, loss = 10.41651909\n",
      "Iteration 9673, loss = 9.42554837\n",
      "Iteration 9674, loss = 9.51343561\n",
      "Iteration 9675, loss = 9.46041667\n",
      "Iteration 9676, loss = 9.45397895\n",
      "Iteration 9677, loss = 9.48522685\n",
      "Iteration 9678, loss = 10.08609054\n",
      "Iteration 9679, loss = 9.48361612\n",
      "Iteration 9680, loss = 9.39909636\n",
      "Iteration 9681, loss = 9.39519494\n",
      "Iteration 9682, loss = 9.80276815\n",
      "Iteration 9683, loss = 9.78704588\n",
      "Iteration 9684, loss = 10.51270289\n",
      "Iteration 9685, loss = 10.24625640\n",
      "Iteration 9686, loss = 10.10148964\n",
      "Iteration 9687, loss = 10.68013725\n",
      "Iteration 9688, loss = 10.43826033\n",
      "Iteration 9689, loss = 9.87207804\n",
      "Iteration 9690, loss = 10.57128462\n",
      "Iteration 9691, loss = 9.92646473\n",
      "Iteration 9692, loss = 9.97452367\n",
      "Iteration 9693, loss = 11.02034367\n",
      "Iteration 9694, loss = 10.01762085\n",
      "Iteration 9695, loss = 9.76316303\n",
      "Iteration 9696, loss = 9.55505619\n",
      "Iteration 9697, loss = 9.50985514\n",
      "Iteration 9698, loss = 10.04480204\n",
      "Iteration 9699, loss = 9.82323982\n",
      "Iteration 9700, loss = 10.04448723\n",
      "Iteration 9701, loss = 9.75317962\n",
      "Iteration 9702, loss = 9.68553393\n",
      "Iteration 9703, loss = 9.85104250\n",
      "Iteration 9704, loss = 10.14070143\n",
      "Iteration 9705, loss = 10.71371514\n",
      "Iteration 9706, loss = 10.87250541\n",
      "Iteration 9707, loss = 10.62598982\n",
      "Iteration 9708, loss = 9.64656865\n",
      "Iteration 9709, loss = 9.38952521\n",
      "Iteration 9710, loss = 9.55064320\n",
      "Iteration 9711, loss = 9.41563271\n",
      "Iteration 9712, loss = 9.73121688\n",
      "Iteration 9713, loss = 9.86819325\n",
      "Iteration 9714, loss = 10.67283381\n",
      "Iteration 9715, loss = 10.86543001\n",
      "Iteration 9716, loss = 11.03697601\n",
      "Iteration 9717, loss = 10.51195485\n",
      "Iteration 9718, loss = 10.99886695\n",
      "Iteration 9719, loss = 9.45560571\n",
      "Iteration 9720, loss = 9.30948316\n",
      "Iteration 9721, loss = 9.32048541\n",
      "Iteration 9722, loss = 9.62739823\n",
      "Iteration 9723, loss = 9.70319558\n",
      "Iteration 9724, loss = 10.55324977\n",
      "Iteration 9725, loss = 9.88271680\n",
      "Iteration 9726, loss = 10.45415399\n",
      "Iteration 9727, loss = 10.15088278\n",
      "Iteration 9728, loss = 10.07202085\n",
      "Iteration 9729, loss = 9.27696055\n",
      "Iteration 9730, loss = 9.43073271\n",
      "Iteration 9731, loss = 9.53677831\n",
      "Iteration 9732, loss = 9.43041475\n",
      "Iteration 9733, loss = 9.34484543\n",
      "Iteration 9734, loss = 9.37890319\n",
      "Iteration 9735, loss = 9.76109021\n",
      "Iteration 9736, loss = 9.29704163\n",
      "Iteration 9737, loss = 9.79508527\n",
      "Iteration 9738, loss = 9.80488713\n",
      "Iteration 9739, loss = 10.25812590\n",
      "Iteration 9740, loss = 11.28237518\n",
      "Iteration 9741, loss = 10.29666442\n",
      "Iteration 9742, loss = 10.76824393\n",
      "Iteration 9743, loss = 9.67215409\n",
      "Iteration 9744, loss = 9.46433179\n",
      "Iteration 9745, loss = 9.98742443\n",
      "Iteration 9746, loss = 9.77218175\n",
      "Iteration 9747, loss = 10.22776588\n",
      "Iteration 9748, loss = 9.87473318\n",
      "Iteration 9749, loss = 10.43705458\n",
      "Iteration 9750, loss = 10.58036526\n",
      "Iteration 9751, loss = 10.63898989\n",
      "Iteration 9752, loss = 11.22478355\n",
      "Iteration 9753, loss = 10.01187376\n",
      "Iteration 9754, loss = 9.51619969\n",
      "Iteration 9755, loss = 9.47966976\n",
      "Iteration 9756, loss = 9.34895418\n",
      "Iteration 9757, loss = 9.18517264\n",
      "Iteration 9758, loss = 9.37296138\n",
      "Iteration 9759, loss = 9.93292003\n",
      "Iteration 9760, loss = 9.51920350\n",
      "Iteration 9761, loss = 10.03661285\n",
      "Iteration 9762, loss = 10.06617088\n",
      "Iteration 9763, loss = 9.30163346\n",
      "Iteration 9764, loss = 9.21226812\n",
      "Iteration 9765, loss = 9.40731248\n",
      "Iteration 9766, loss = 9.29436395\n",
      "Iteration 9767, loss = 9.77804717\n",
      "Iteration 9768, loss = 9.76837924\n",
      "Iteration 9769, loss = 9.65054945\n",
      "Iteration 9770, loss = 9.85169866\n",
      "Iteration 9771, loss = 10.15371239\n",
      "Iteration 9772, loss = 9.62858383\n",
      "Iteration 9773, loss = 10.08184016\n",
      "Iteration 9774, loss = 11.34999513\n",
      "Iteration 9775, loss = 10.58147915\n",
      "Iteration 9776, loss = 11.88457130\n",
      "Iteration 9777, loss = 11.76112739\n",
      "Iteration 9778, loss = 10.70443351\n",
      "Iteration 9779, loss = 10.50879333\n",
      "Iteration 9780, loss = 10.77919497\n",
      "Iteration 9781, loss = 9.96097592\n",
      "Iteration 9782, loss = 11.72541495\n",
      "Iteration 9783, loss = 10.54440523\n",
      "Iteration 9784, loss = 9.50674081\n",
      "Iteration 9785, loss = 9.51935006\n",
      "Iteration 9786, loss = 9.71678865\n",
      "Iteration 9787, loss = 10.44209302\n",
      "Iteration 9788, loss = 10.03250793\n",
      "Iteration 9789, loss = 11.26508043\n",
      "Iteration 9790, loss = 10.63365412\n",
      "Iteration 9791, loss = 10.73887427\n",
      "Iteration 9792, loss = 10.29391571\n",
      "Iteration 9793, loss = 10.68804014\n",
      "Iteration 9794, loss = 11.55150266\n",
      "Iteration 9795, loss = 10.82610649\n",
      "Iteration 9796, loss = 11.57480390\n",
      "Iteration 9797, loss = 10.87757445\n",
      "Iteration 9798, loss = 11.11791780\n",
      "Iteration 9799, loss = 11.51786345\n",
      "Iteration 9800, loss = 10.02811305\n",
      "Iteration 9801, loss = 9.92544156\n",
      "Iteration 9802, loss = 9.61798392\n",
      "Iteration 9803, loss = 9.62631856\n",
      "Iteration 9804, loss = 9.66723726\n",
      "Iteration 9805, loss = 9.58953628\n",
      "Iteration 9806, loss = 9.54808790\n",
      "Iteration 9807, loss = 10.32963477\n",
      "Iteration 9808, loss = 10.16019341\n",
      "Iteration 9809, loss = 9.72659086\n",
      "Iteration 9810, loss = 10.14891322\n",
      "Iteration 9811, loss = 10.01483489\n",
      "Iteration 9812, loss = 9.90775066\n",
      "Iteration 9813, loss = 9.90667642\n",
      "Iteration 9814, loss = 9.96376609\n",
      "Iteration 9815, loss = 9.98843046\n",
      "Iteration 9816, loss = 10.25967751\n",
      "Iteration 9817, loss = 9.52896540\n",
      "Iteration 9818, loss = 9.85778862\n",
      "Iteration 9819, loss = 9.32196793\n",
      "Iteration 9820, loss = 9.81616026\n",
      "Iteration 9821, loss = 9.68841527\n",
      "Iteration 9822, loss = 9.53711151\n",
      "Iteration 9823, loss = 9.56591867\n",
      "Iteration 9824, loss = 9.46520154\n",
      "Iteration 9825, loss = 9.86279189\n",
      "Iteration 9826, loss = 10.10208381\n",
      "Iteration 9827, loss = 10.51534692\n",
      "Iteration 9828, loss = 9.77853042\n",
      "Iteration 9829, loss = 9.83162409\n",
      "Iteration 9830, loss = 9.62646447\n",
      "Iteration 9831, loss = 9.87915174\n",
      "Iteration 9832, loss = 9.85793141\n",
      "Iteration 9833, loss = 9.30277691\n",
      "Iteration 9834, loss = 9.58926671\n",
      "Iteration 9835, loss = 10.14335094\n",
      "Iteration 9836, loss = 11.20493081\n",
      "Iteration 9837, loss = 10.92514200\n",
      "Iteration 9838, loss = 9.88545104\n",
      "Iteration 9839, loss = 10.08039041\n",
      "Iteration 9840, loss = 10.04086920\n",
      "Iteration 9841, loss = 9.91699794\n",
      "Iteration 9842, loss = 10.09794071\n",
      "Iteration 9843, loss = 11.14103179\n",
      "Iteration 9844, loss = 9.97569631\n",
      "Iteration 9845, loss = 9.59054164\n",
      "Iteration 9846, loss = 9.93650241\n",
      "Iteration 9847, loss = 10.82899171\n",
      "Iteration 9848, loss = 11.68212595\n",
      "Iteration 9849, loss = 9.82064102\n",
      "Iteration 9850, loss = 10.89013983\n",
      "Iteration 9851, loss = 10.16317448\n",
      "Iteration 9852, loss = 9.99141104\n",
      "Iteration 9853, loss = 9.79913413\n",
      "Iteration 9854, loss = 9.60577678\n",
      "Iteration 9855, loss = 10.18086878\n",
      "Iteration 9856, loss = 10.46763288\n",
      "Iteration 9857, loss = 10.64477593\n",
      "Iteration 9858, loss = 9.69815160\n",
      "Iteration 9859, loss = 9.92629835\n",
      "Iteration 9860, loss = 10.12107702\n",
      "Iteration 9861, loss = 9.50437382\n",
      "Iteration 9862, loss = 9.28312973\n",
      "Iteration 9863, loss = 9.53700623\n",
      "Iteration 9864, loss = 9.51377817\n",
      "Iteration 9865, loss = 9.49150364\n",
      "Iteration 9866, loss = 9.92087202\n",
      "Iteration 9867, loss = 10.39132111\n",
      "Iteration 9868, loss = 9.85241387\n",
      "Iteration 9869, loss = 9.90216556\n",
      "Iteration 9870, loss = 10.49561225\n",
      "Iteration 9871, loss = 9.77398792\n",
      "Iteration 9872, loss = 10.45722870\n",
      "Iteration 9873, loss = 10.68057135\n",
      "Iteration 9874, loss = 10.44724708\n",
      "Iteration 9875, loss = 9.83019321\n",
      "Iteration 9876, loss = 9.43580823\n",
      "Iteration 9877, loss = 9.56218620\n",
      "Iteration 9878, loss = 9.36154842\n",
      "Iteration 9879, loss = 9.80682815\n",
      "Iteration 9880, loss = 9.90971280\n",
      "Iteration 9881, loss = 11.20493940\n",
      "Iteration 9882, loss = 9.73540599\n",
      "Iteration 9883, loss = 9.53824251\n",
      "Iteration 9884, loss = 9.29798058\n",
      "Iteration 9885, loss = 9.87119443\n",
      "Iteration 9886, loss = 10.19537498\n",
      "Iteration 9887, loss = 9.94871724\n",
      "Iteration 9888, loss = 11.26051494\n",
      "Iteration 9889, loss = 9.77788035\n",
      "Iteration 9890, loss = 9.64423222\n",
      "Iteration 9891, loss = 9.36405795\n",
      "Iteration 9892, loss = 9.77554645\n",
      "Iteration 9893, loss = 9.85631470\n",
      "Iteration 9894, loss = 9.70662066\n",
      "Iteration 9895, loss = 9.43283012\n",
      "Iteration 9896, loss = 9.38399073\n",
      "Iteration 9897, loss = 9.22117217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9898, loss = 9.55357826\n",
      "Iteration 9899, loss = 9.21398384\n",
      "Iteration 9900, loss = 9.16459988\n",
      "Iteration 9901, loss = 9.39147052\n",
      "Iteration 9902, loss = 10.54111492\n",
      "Iteration 9903, loss = 9.39571804\n",
      "Iteration 9904, loss = 10.39748364\n",
      "Iteration 9905, loss = 10.54155881\n",
      "Iteration 9906, loss = 10.51309857\n",
      "Iteration 9907, loss = 9.98164629\n",
      "Iteration 9908, loss = 10.44770119\n",
      "Iteration 9909, loss = 10.93114123\n",
      "Iteration 9910, loss = 10.09759020\n",
      "Iteration 9911, loss = 9.33478037\n",
      "Iteration 9912, loss = 9.77212167\n",
      "Iteration 9913, loss = 9.78352973\n",
      "Iteration 9914, loss = 9.88256123\n",
      "Iteration 9915, loss = 9.74760559\n",
      "Iteration 9916, loss = 9.45893367\n",
      "Iteration 9917, loss = 10.28311735\n",
      "Iteration 9918, loss = 10.17118906\n",
      "Iteration 9919, loss = 9.83086581\n",
      "Iteration 9920, loss = 9.78271243\n",
      "Iteration 9921, loss = 9.66371027\n",
      "Iteration 9922, loss = 9.44010099\n",
      "Iteration 9923, loss = 10.52824939\n",
      "Iteration 9924, loss = 10.46323213\n",
      "Iteration 9925, loss = 9.52003877\n",
      "Iteration 9926, loss = 9.39182534\n",
      "Iteration 9927, loss = 10.07743641\n",
      "Iteration 9928, loss = 9.98260501\n",
      "Iteration 9929, loss = 10.65637307\n",
      "Iteration 9930, loss = 10.37257465\n",
      "Iteration 9931, loss = 10.87851735\n",
      "Iteration 9932, loss = 10.88461414\n",
      "Iteration 9933, loss = 10.65229048\n",
      "Iteration 9934, loss = 10.98957745\n",
      "Iteration 9935, loss = 9.72645562\n",
      "Iteration 9936, loss = 10.02505803\n",
      "Iteration 9937, loss = 9.61797503\n",
      "Iteration 9938, loss = 9.60031221\n",
      "Iteration 9939, loss = 9.58404719\n",
      "Iteration 9940, loss = 9.44713533\n",
      "Iteration 9941, loss = 9.55590592\n",
      "Iteration 9942, loss = 10.02029440\n",
      "Iteration 9943, loss = 10.04159530\n",
      "Iteration 9944, loss = 9.35868414\n",
      "Iteration 9945, loss = 11.17265452\n",
      "Iteration 9946, loss = 9.91583058\n",
      "Iteration 9947, loss = 9.98092613\n",
      "Iteration 9948, loss = 10.12705671\n",
      "Iteration 9949, loss = 9.56760552\n",
      "Iteration 9950, loss = 9.99594184\n",
      "Iteration 9951, loss = 10.47275930\n",
      "Iteration 9952, loss = 11.07774145\n",
      "Iteration 9953, loss = 10.49945552\n",
      "Iteration 9954, loss = 9.98744191\n",
      "Iteration 9955, loss = 9.42017565\n",
      "Iteration 9956, loss = 10.26622641\n",
      "Iteration 9957, loss = 9.99415521\n",
      "Iteration 9958, loss = 11.02488672\n",
      "Iteration 9959, loss = 9.66679767\n",
      "Iteration 9960, loss = 9.24100215\n",
      "Iteration 9961, loss = 9.72282222\n",
      "Iteration 9962, loss = 9.82070678\n",
      "Iteration 9963, loss = 9.54852468\n",
      "Iteration 9964, loss = 9.16646438\n",
      "Iteration 9965, loss = 9.42451993\n",
      "Iteration 9966, loss = 9.42599727\n",
      "Iteration 9967, loss = 9.30413360\n",
      "Iteration 9968, loss = 9.44115552\n",
      "Iteration 9969, loss = 9.82242706\n",
      "Iteration 9970, loss = 9.64480334\n",
      "Iteration 9971, loss = 9.32872063\n",
      "Iteration 9972, loss = 9.44215824\n",
      "Iteration 9973, loss = 9.26953380\n",
      "Iteration 9974, loss = 11.14345104\n",
      "Iteration 9975, loss = 11.12797664\n",
      "Iteration 9976, loss = 10.28290729\n",
      "Iteration 9977, loss = 9.71063963\n",
      "Iteration 9978, loss = 9.62060500\n",
      "Iteration 9979, loss = 9.92803381\n",
      "Iteration 9980, loss = 9.88120402\n",
      "Iteration 9981, loss = 9.92374246\n",
      "Iteration 9982, loss = 9.66315646\n",
      "Iteration 9983, loss = 9.84486987\n",
      "Iteration 9984, loss = 9.96711488\n",
      "Iteration 9985, loss = 9.85914253\n",
      "Iteration 9986, loss = 9.93465537\n",
      "Iteration 9987, loss = 10.78007335\n",
      "Iteration 9988, loss = 9.42736868\n",
      "Iteration 9989, loss = 9.86008803\n",
      "Iteration 9990, loss = 10.40402979\n",
      "Iteration 9991, loss = 9.77435703\n",
      "Iteration 9992, loss = 10.96257664\n",
      "Iteration 9993, loss = 11.14199769\n",
      "Iteration 9994, loss = 10.02120299\n",
      "Iteration 9995, loss = 10.59881487\n",
      "Iteration 9996, loss = 9.48964123\n",
      "Iteration 9997, loss = 9.58157210\n",
      "Iteration 9998, loss = 10.36026698\n",
      "Iteration 9999, loss = 10.24404958\n",
      "Iteration 10000, loss = 9.40785586\n",
      "Iteration 10001, loss = 9.39664500\n",
      "Iteration 10002, loss = 10.01409746\n",
      "Iteration 10003, loss = 9.26941591\n",
      "Iteration 10004, loss = 9.74522938\n",
      "Iteration 10005, loss = 9.86501168\n",
      "Iteration 10006, loss = 9.80902481\n",
      "Iteration 10007, loss = 9.60575685\n",
      "Iteration 10008, loss = 10.95482266\n",
      "Iteration 10009, loss = 10.52671355\n",
      "Iteration 10010, loss = 9.74449377\n",
      "Iteration 10011, loss = 10.05809093\n",
      "Iteration 10012, loss = 10.04873588\n",
      "Iteration 10013, loss = 9.41941065\n",
      "Iteration 10014, loss = 9.90921894\n",
      "Iteration 10015, loss = 9.52827153\n",
      "Iteration 10016, loss = 10.40142382\n",
      "Iteration 10017, loss = 9.86410580\n",
      "Iteration 10018, loss = 9.45978197\n",
      "Iteration 10019, loss = 9.77936359\n",
      "Iteration 10020, loss = 10.11439831\n",
      "Iteration 10021, loss = 9.89338771\n",
      "Iteration 10022, loss = 9.52759277\n",
      "Iteration 10023, loss = 10.23957596\n",
      "Iteration 10024, loss = 10.57171500\n",
      "Iteration 10025, loss = 10.06870510\n",
      "Iteration 10026, loss = 11.11032792\n",
      "Iteration 10027, loss = 9.93920977\n",
      "Iteration 10028, loss = 9.68826164\n",
      "Iteration 10029, loss = 9.84367367\n",
      "Iteration 10030, loss = 10.07723654\n",
      "Iteration 10031, loss = 9.67231712\n",
      "Iteration 10032, loss = 9.34777331\n",
      "Iteration 10033, loss = 9.41652827\n",
      "Iteration 10034, loss = 9.06441673\n",
      "Iteration 10035, loss = 9.47096764\n",
      "Iteration 10036, loss = 9.54134395\n",
      "Iteration 10037, loss = 9.14573310\n",
      "Iteration 10038, loss = 9.35390378\n",
      "Iteration 10039, loss = 9.66959305\n",
      "Iteration 10040, loss = 9.51538348\n",
      "Iteration 10041, loss = 10.29607489\n",
      "Iteration 10042, loss = 10.26379507\n",
      "Iteration 10043, loss = 9.40105584\n",
      "Iteration 10044, loss = 9.84679239\n",
      "Iteration 10045, loss = 10.07674620\n",
      "Iteration 10046, loss = 11.01174237\n",
      "Iteration 10047, loss = 10.58164389\n",
      "Iteration 10048, loss = 10.84092776\n",
      "Iteration 10049, loss = 10.13233558\n",
      "Iteration 10050, loss = 10.67243512\n",
      "Iteration 10051, loss = 11.08802892\n",
      "Iteration 10052, loss = 9.74401673\n",
      "Iteration 10053, loss = 9.24112888\n",
      "Iteration 10054, loss = 10.36322442\n",
      "Iteration 10055, loss = 10.41551388\n",
      "Iteration 10056, loss = 9.99381805\n",
      "Iteration 10057, loss = 10.53774325\n",
      "Iteration 10058, loss = 10.11434656\n",
      "Iteration 10059, loss = 9.39199866\n",
      "Iteration 10060, loss = 10.20967684\n",
      "Iteration 10061, loss = 10.48039947\n",
      "Iteration 10062, loss = 10.20653034\n",
      "Iteration 10063, loss = 10.05441862\n",
      "Iteration 10064, loss = 9.57464708\n",
      "Iteration 10065, loss = 9.46466534\n",
      "Iteration 10066, loss = 9.51526893\n",
      "Iteration 10067, loss = 9.55766953\n",
      "Iteration 10068, loss = 9.33670897\n",
      "Iteration 10069, loss = 9.96917316\n",
      "Iteration 10070, loss = 9.61145228\n",
      "Iteration 10071, loss = 9.64218495\n",
      "Iteration 10072, loss = 9.57266495\n",
      "Iteration 10073, loss = 9.46872338\n",
      "Iteration 10074, loss = 9.05506986\n",
      "Iteration 10075, loss = 9.35977266\n",
      "Iteration 10076, loss = 9.94874055\n",
      "Iteration 10077, loss = 10.22987902\n",
      "Iteration 10078, loss = 10.45551991\n",
      "Iteration 10079, loss = 10.96743354\n",
      "Iteration 10080, loss = 10.49208337\n",
      "Iteration 10081, loss = 9.63250112\n",
      "Iteration 10082, loss = 9.48447891\n",
      "Iteration 10083, loss = 9.22643560\n",
      "Iteration 10084, loss = 9.38943491\n",
      "Iteration 10085, loss = 9.56271744\n",
      "Iteration 10086, loss = 9.47351558\n",
      "Iteration 10087, loss = 9.78120940\n",
      "Iteration 10088, loss = 9.40317616\n",
      "Iteration 10089, loss = 10.34851384\n",
      "Iteration 10090, loss = 11.42371496\n",
      "Iteration 10091, loss = 9.95971890\n",
      "Iteration 10092, loss = 10.46654194\n",
      "Iteration 10093, loss = 10.80829049\n",
      "Iteration 10094, loss = 10.62233455\n",
      "Iteration 10095, loss = 10.27022373\n",
      "Iteration 10096, loss = 10.03104717\n",
      "Iteration 10097, loss = 9.55608156\n",
      "Iteration 10098, loss = 9.32074128\n",
      "Iteration 10099, loss = 9.17139363\n",
      "Iteration 10100, loss = 9.54184129\n",
      "Iteration 10101, loss = 9.32238554\n",
      "Iteration 10102, loss = 9.64943214\n",
      "Iteration 10103, loss = 9.65461490\n",
      "Iteration 10104, loss = 10.84882364\n",
      "Iteration 10105, loss = 9.44680302\n",
      "Iteration 10106, loss = 10.18953929\n",
      "Iteration 10107, loss = 9.32857171\n",
      "Iteration 10108, loss = 9.23740214\n",
      "Iteration 10109, loss = 9.19707433\n",
      "Iteration 10110, loss = 9.56852955\n",
      "Iteration 10111, loss = 9.66997649\n",
      "Iteration 10112, loss = 9.82070865\n",
      "Iteration 10113, loss = 9.24655615\n",
      "Iteration 10114, loss = 10.33012980\n",
      "Iteration 10115, loss = 10.48600339\n",
      "Iteration 10116, loss = 10.48681514\n",
      "Iteration 10117, loss = 10.31702382\n",
      "Iteration 10118, loss = 10.16013983\n",
      "Iteration 10119, loss = 9.34521544\n",
      "Iteration 10120, loss = 9.16334238\n",
      "Iteration 10121, loss = 9.37874923\n",
      "Iteration 10122, loss = 9.92569144\n",
      "Iteration 10123, loss = 9.54339386\n",
      "Iteration 10124, loss = 9.31169592\n",
      "Iteration 10125, loss = 9.01900777\n",
      "Iteration 10126, loss = 9.37637480\n",
      "Iteration 10127, loss = 9.98334829\n",
      "Iteration 10128, loss = 9.48664480\n",
      "Iteration 10129, loss = 9.66395979\n",
      "Iteration 10130, loss = 9.60085128\n",
      "Iteration 10131, loss = 9.20984264\n",
      "Iteration 10132, loss = 9.50388076\n",
      "Iteration 10133, loss = 9.10488412\n",
      "Iteration 10134, loss = 9.10369568\n",
      "Iteration 10135, loss = 9.09032917\n",
      "Iteration 10136, loss = 9.22210453\n",
      "Iteration 10137, loss = 9.31743095\n",
      "Iteration 10138, loss = 9.12693195\n",
      "Iteration 10139, loss = 9.55739625\n",
      "Iteration 10140, loss = 10.61793256\n",
      "Iteration 10141, loss = 9.56015484\n",
      "Iteration 10142, loss = 9.94114161\n",
      "Iteration 10143, loss = 9.54378979\n",
      "Iteration 10144, loss = 9.12287693\n",
      "Iteration 10145, loss = 9.44805208\n",
      "Iteration 10146, loss = 9.69442790\n",
      "Iteration 10147, loss = 9.40766532\n",
      "Iteration 10148, loss = 9.91906484\n",
      "Iteration 10149, loss = 10.47326526\n",
      "Iteration 10150, loss = 11.10909676\n",
      "Iteration 10151, loss = 10.57921921\n",
      "Iteration 10152, loss = 10.97140385\n",
      "Iteration 10153, loss = 10.10557325\n",
      "Iteration 10154, loss = 9.96655349\n",
      "Iteration 10155, loss = 9.57750364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10156, loss = 9.83784772\n",
      "Iteration 10157, loss = 9.58764104\n",
      "Iteration 10158, loss = 9.57914938\n",
      "Iteration 10159, loss = 9.64005061\n",
      "Iteration 10160, loss = 9.97992278\n",
      "Iteration 10161, loss = 9.36090900\n",
      "Iteration 10162, loss = 10.40520188\n",
      "Iteration 10163, loss = 9.81849135\n",
      "Iteration 10164, loss = 9.41507142\n",
      "Iteration 10165, loss = 9.47677376\n",
      "Iteration 10166, loss = 9.41517231\n",
      "Iteration 10167, loss = 9.27351254\n",
      "Iteration 10168, loss = 9.29844157\n",
      "Iteration 10169, loss = 9.96079374\n",
      "Iteration 10170, loss = 9.90636043\n",
      "Iteration 10171, loss = 9.32151708\n",
      "Iteration 10172, loss = 9.06521779\n",
      "Iteration 10173, loss = 9.37906816\n",
      "Iteration 10174, loss = 9.38367585\n",
      "Iteration 10175, loss = 9.74561765\n",
      "Iteration 10176, loss = 9.67145485\n",
      "Iteration 10177, loss = 9.31460315\n",
      "Iteration 10178, loss = 9.89676353\n",
      "Iteration 10179, loss = 10.04685927\n",
      "Iteration 10180, loss = 9.68299942\n",
      "Iteration 10181, loss = 9.54962763\n",
      "Iteration 10182, loss = 10.79263084\n",
      "Iteration 10183, loss = 10.34263191\n",
      "Iteration 10184, loss = 13.00694534\n",
      "Iteration 10185, loss = 11.60702644\n",
      "Iteration 10186, loss = 12.95882791\n",
      "Iteration 10187, loss = 11.40539755\n",
      "Iteration 10188, loss = 11.20325330\n",
      "Iteration 10189, loss = 10.14577983\n",
      "Iteration 10190, loss = 9.62369897\n",
      "Iteration 10191, loss = 9.98477580\n",
      "Iteration 10192, loss = 9.15638851\n",
      "Iteration 10193, loss = 9.33950240\n",
      "Iteration 10194, loss = 9.65859377\n",
      "Iteration 10195, loss = 10.28111339\n",
      "Iteration 10196, loss = 9.23747422\n",
      "Iteration 10197, loss = 9.31801045\n",
      "Iteration 10198, loss = 9.31899846\n",
      "Iteration 10199, loss = 9.67588974\n",
      "Iteration 10200, loss = 11.58032814\n",
      "Iteration 10201, loss = 11.62991532\n",
      "Iteration 10202, loss = 9.74478176\n",
      "Iteration 10203, loss = 9.74577880\n",
      "Iteration 10204, loss = 9.23782947\n",
      "Iteration 10205, loss = 9.40604212\n",
      "Iteration 10206, loss = 9.82236872\n",
      "Iteration 10207, loss = 10.01602696\n",
      "Iteration 10208, loss = 9.29077348\n",
      "Iteration 10209, loss = 10.32354645\n",
      "Iteration 10210, loss = 12.44113558\n",
      "Iteration 10211, loss = 12.03827630\n",
      "Iteration 10212, loss = 9.98341829\n",
      "Iteration 10213, loss = 10.30719291\n",
      "Iteration 10214, loss = 9.89708020\n",
      "Iteration 10215, loss = 9.20803312\n",
      "Iteration 10216, loss = 9.51738884\n",
      "Iteration 10217, loss = 9.43622775\n",
      "Iteration 10218, loss = 9.49710371\n",
      "Iteration 10219, loss = 10.40170248\n",
      "Iteration 10220, loss = 9.70234186\n",
      "Iteration 10221, loss = 9.52408203\n",
      "Iteration 10222, loss = 9.62849871\n",
      "Iteration 10223, loss = 9.45206877\n",
      "Iteration 10224, loss = 9.13997274\n",
      "Iteration 10225, loss = 9.37900066\n",
      "Iteration 10226, loss = 10.04448769\n",
      "Iteration 10227, loss = 9.59661128\n",
      "Iteration 10228, loss = 9.19563203\n",
      "Iteration 10229, loss = 9.19403735\n",
      "Iteration 10230, loss = 9.19392095\n",
      "Iteration 10231, loss = 9.28043606\n",
      "Iteration 10232, loss = 9.14368191\n",
      "Iteration 10233, loss = 9.20576075\n",
      "Iteration 10234, loss = 9.19162327\n",
      "Iteration 10235, loss = 9.25102658\n",
      "Iteration 10236, loss = 9.30118707\n",
      "Iteration 10237, loss = 9.26944104\n",
      "Iteration 10238, loss = 9.31021466\n",
      "Iteration 10239, loss = 9.54885818\n",
      "Iteration 10240, loss = 10.12803634\n",
      "Iteration 10241, loss = 9.97258064\n",
      "Iteration 10242, loss = 10.34723088\n",
      "Iteration 10243, loss = 9.84784431\n",
      "Iteration 10244, loss = 9.11788321\n",
      "Iteration 10245, loss = 9.05121610\n",
      "Iteration 10246, loss = 8.97305152\n",
      "Iteration 10247, loss = 9.65356333\n",
      "Iteration 10248, loss = 9.85388250\n",
      "Iteration 10249, loss = 10.87450419\n",
      "Iteration 10250, loss = 12.41649522\n",
      "Iteration 10251, loss = 11.41880355\n",
      "Iteration 10252, loss = 11.54166925\n",
      "Iteration 10253, loss = 10.35860156\n",
      "Iteration 10254, loss = 9.29429491\n",
      "Iteration 10255, loss = 10.05336721\n",
      "Iteration 10256, loss = 9.91726531\n",
      "Iteration 10257, loss = 10.89904358\n",
      "Iteration 10258, loss = 9.60547977\n",
      "Iteration 10259, loss = 9.62840857\n",
      "Iteration 10260, loss = 9.63055215\n",
      "Iteration 10261, loss = 10.57271431\n",
      "Iteration 10262, loss = 9.99675199\n",
      "Iteration 10263, loss = 10.03229227\n",
      "Iteration 10264, loss = 9.30180238\n",
      "Iteration 10265, loss = 9.87164224\n",
      "Iteration 10266, loss = 9.83367954\n",
      "Iteration 10267, loss = 9.74212506\n",
      "Iteration 10268, loss = 10.04923186\n",
      "Iteration 10269, loss = 9.46919823\n",
      "Iteration 10270, loss = 9.64166109\n",
      "Iteration 10271, loss = 10.22518041\n",
      "Iteration 10272, loss = 11.03220885\n",
      "Iteration 10273, loss = 9.99314874\n",
      "Iteration 10274, loss = 10.44573268\n",
      "Iteration 10275, loss = 9.43654848\n",
      "Iteration 10276, loss = 9.57642332\n",
      "Iteration 10277, loss = 9.56419747\n",
      "Iteration 10278, loss = 9.87286110\n",
      "Iteration 10279, loss = 9.27164953\n",
      "Iteration 10280, loss = 9.33327056\n",
      "Iteration 10281, loss = 9.75403975\n",
      "Iteration 10282, loss = 9.37759118\n",
      "Iteration 10283, loss = 9.16644305\n",
      "Iteration 10284, loss = 9.24050061\n",
      "Iteration 10285, loss = 9.73227121\n",
      "Iteration 10286, loss = 9.24208690\n",
      "Iteration 10287, loss = 9.45335461\n",
      "Iteration 10288, loss = 9.14738818\n",
      "Iteration 10289, loss = 9.45236262\n",
      "Iteration 10290, loss = 10.61119518\n",
      "Iteration 10291, loss = 10.14081183\n",
      "Iteration 10292, loss = 9.48027807\n",
      "Iteration 10293, loss = 9.43232528\n",
      "Iteration 10294, loss = 9.51333306\n",
      "Iteration 10295, loss = 9.79293694\n",
      "Iteration 10296, loss = 8.93121568\n",
      "Iteration 10297, loss = 9.06964502\n",
      "Iteration 10298, loss = 8.96893683\n",
      "Iteration 10299, loss = 9.34873812\n",
      "Iteration 10300, loss = 9.60657871\n",
      "Iteration 10301, loss = 9.60888026\n",
      "Iteration 10302, loss = 10.21272563\n",
      "Iteration 10303, loss = 9.23572732\n",
      "Iteration 10304, loss = 10.49644645\n",
      "Iteration 10305, loss = 9.50207798\n",
      "Iteration 10306, loss = 9.18223504\n",
      "Iteration 10307, loss = 9.81842020\n",
      "Iteration 10308, loss = 9.70279391\n",
      "Iteration 10309, loss = 9.47984048\n",
      "Iteration 10310, loss = 9.30828316\n",
      "Iteration 10311, loss = 9.26416192\n",
      "Iteration 10312, loss = 9.03459215\n",
      "Iteration 10313, loss = 9.01812773\n",
      "Iteration 10314, loss = 8.97757161\n",
      "Iteration 10315, loss = 9.02588295\n",
      "Iteration 10316, loss = 9.05706109\n",
      "Iteration 10317, loss = 9.03566649\n",
      "Iteration 10318, loss = 9.16934454\n",
      "Iteration 10319, loss = 9.40627664\n",
      "Iteration 10320, loss = 9.06152081\n",
      "Iteration 10321, loss = 9.40153171\n",
      "Iteration 10322, loss = 10.07657346\n",
      "Iteration 10323, loss = 9.61663647\n",
      "Iteration 10324, loss = 9.48019302\n",
      "Iteration 10325, loss = 9.56880120\n",
      "Iteration 10326, loss = 9.93138618\n",
      "Iteration 10327, loss = 9.84641623\n",
      "Iteration 10328, loss = 10.39096896\n",
      "Iteration 10329, loss = 10.41895813\n",
      "Iteration 10330, loss = 10.77352351\n",
      "Iteration 10331, loss = 9.69726508\n",
      "Iteration 10332, loss = 9.78521705\n",
      "Iteration 10333, loss = 9.92578441\n",
      "Iteration 10334, loss = 9.47902444\n",
      "Iteration 10335, loss = 10.32248766\n",
      "Iteration 10336, loss = 9.98695795\n",
      "Iteration 10337, loss = 9.79328651\n",
      "Iteration 10338, loss = 11.27751513\n",
      "Iteration 10339, loss = 10.77578347\n",
      "Iteration 10340, loss = 9.85209820\n",
      "Iteration 10341, loss = 10.10120817\n",
      "Iteration 10342, loss = 10.64467983\n",
      "Iteration 10343, loss = 10.37767906\n",
      "Iteration 10344, loss = 9.55293777\n",
      "Iteration 10345, loss = 9.44645313\n",
      "Iteration 10346, loss = 9.90880427\n",
      "Iteration 10347, loss = 10.16027838\n",
      "Iteration 10348, loss = 10.10409795\n",
      "Iteration 10349, loss = 9.13895926\n",
      "Iteration 10350, loss = 10.00177445\n",
      "Iteration 10351, loss = 10.45556501\n",
      "Iteration 10352, loss = 9.45159498\n",
      "Iteration 10353, loss = 9.15142944\n",
      "Iteration 10354, loss = 9.35018141\n",
      "Iteration 10355, loss = 9.76481591\n",
      "Iteration 10356, loss = 9.61024794\n",
      "Iteration 10357, loss = 9.16729459\n",
      "Iteration 10358, loss = 9.16189285\n",
      "Iteration 10359, loss = 9.59876515\n",
      "Iteration 10360, loss = 9.32776354\n",
      "Iteration 10361, loss = 9.28920650\n",
      "Iteration 10362, loss = 8.96883134\n",
      "Iteration 10363, loss = 9.11738678\n",
      "Iteration 10364, loss = 9.50688666\n",
      "Iteration 10365, loss = 9.97616588\n",
      "Iteration 10366, loss = 9.36994930\n",
      "Iteration 10367, loss = 9.48547509\n",
      "Iteration 10368, loss = 8.84235146\n",
      "Iteration 10369, loss = 9.50345624\n",
      "Iteration 10370, loss = 9.12206302\n",
      "Iteration 10371, loss = 9.25969850\n",
      "Iteration 10372, loss = 9.16195947\n",
      "Iteration 10373, loss = 9.21885517\n",
      "Iteration 10374, loss = 9.78122467\n",
      "Iteration 10375, loss = 10.48707786\n",
      "Iteration 10376, loss = 10.61353704\n",
      "Iteration 10377, loss = 9.68371591\n",
      "Iteration 10378, loss = 9.33271744\n",
      "Iteration 10379, loss = 8.98299600\n",
      "Iteration 10380, loss = 9.29329874\n",
      "Iteration 10381, loss = 9.07987768\n",
      "Iteration 10382, loss = 9.48090799\n",
      "Iteration 10383, loss = 8.90293196\n",
      "Iteration 10384, loss = 9.33841473\n",
      "Iteration 10385, loss = 9.53788865\n",
      "Iteration 10386, loss = 9.67009705\n",
      "Iteration 10387, loss = 9.40678981\n",
      "Iteration 10388, loss = 9.14983174\n",
      "Iteration 10389, loss = 8.98464602\n",
      "Iteration 10390, loss = 9.26604084\n",
      "Iteration 10391, loss = 9.38150125\n",
      "Iteration 10392, loss = 9.02827192\n",
      "Iteration 10393, loss = 9.55607972\n",
      "Iteration 10394, loss = 8.97720117\n",
      "Iteration 10395, loss = 9.19273354\n",
      "Iteration 10396, loss = 9.45259657\n",
      "Iteration 10397, loss = 10.56351960\n",
      "Iteration 10398, loss = 9.50675624\n",
      "Iteration 10399, loss = 9.08161455\n",
      "Iteration 10400, loss = 8.97865743\n",
      "Iteration 10401, loss = 9.24431883\n",
      "Iteration 10402, loss = 8.97511342\n",
      "Iteration 10403, loss = 9.20083134\n",
      "Iteration 10404, loss = 8.88294543\n",
      "Iteration 10405, loss = 8.95745802\n",
      "Iteration 10406, loss = 9.05715247\n",
      "Iteration 10407, loss = 9.29477624\n",
      "Iteration 10408, loss = 9.00135043\n",
      "Iteration 10409, loss = 10.05840460\n",
      "Iteration 10410, loss = 10.82872135\n",
      "Iteration 10411, loss = 9.69425195\n",
      "Iteration 10412, loss = 9.47335801\n",
      "Iteration 10413, loss = 8.56313502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10414, loss = 9.21580619\n",
      "Iteration 10415, loss = 9.30532577\n",
      "Iteration 10416, loss = 9.01401975\n",
      "Iteration 10417, loss = 8.92296375\n",
      "Iteration 10418, loss = 8.94521642\n",
      "Iteration 10419, loss = 9.07186591\n",
      "Iteration 10420, loss = 9.18021796\n",
      "Iteration 10421, loss = 8.92727765\n",
      "Iteration 10422, loss = 8.83196287\n",
      "Iteration 10423, loss = 8.73402160\n",
      "Iteration 10424, loss = 8.74985277\n",
      "Iteration 10425, loss = 8.80249975\n",
      "Iteration 10426, loss = 8.83046911\n",
      "Iteration 10427, loss = 8.84470306\n",
      "Iteration 10428, loss = 9.45297918\n",
      "Iteration 10429, loss = 8.63097187\n",
      "Iteration 10430, loss = 8.93921240\n",
      "Iteration 10431, loss = 10.09663548\n",
      "Iteration 10432, loss = 10.40237809\n",
      "Iteration 10433, loss = 11.13439774\n",
      "Iteration 10434, loss = 10.52683866\n",
      "Iteration 10435, loss = 9.39413521\n",
      "Iteration 10436, loss = 9.13458078\n",
      "Iteration 10437, loss = 8.88914335\n",
      "Iteration 10438, loss = 9.18888554\n",
      "Iteration 10439, loss = 9.29343676\n",
      "Iteration 10440, loss = 10.46214354\n",
      "Iteration 10441, loss = 10.54482520\n",
      "Iteration 10442, loss = 9.49885870\n",
      "Iteration 10443, loss = 10.33169501\n",
      "Iteration 10444, loss = 9.73334489\n",
      "Iteration 10445, loss = 9.40706960\n",
      "Iteration 10446, loss = 9.32817970\n",
      "Iteration 10447, loss = 9.45370424\n",
      "Iteration 10448, loss = 9.12328348\n",
      "Iteration 10449, loss = 9.09009769\n",
      "Iteration 10450, loss = 8.72613825\n",
      "Iteration 10451, loss = 9.75188526\n",
      "Iteration 10452, loss = 9.51750600\n",
      "Iteration 10453, loss = 9.59735791\n",
      "Iteration 10454, loss = 9.68482653\n",
      "Iteration 10455, loss = 9.59499063\n",
      "Iteration 10456, loss = 8.97345917\n",
      "Iteration 10457, loss = 8.98469104\n",
      "Iteration 10458, loss = 9.04511453\n",
      "Iteration 10459, loss = 9.39596303\n",
      "Iteration 10460, loss = 9.31671148\n",
      "Iteration 10461, loss = 9.03086167\n",
      "Iteration 10462, loss = 8.63801268\n",
      "Iteration 10463, loss = 8.80589685\n",
      "Iteration 10464, loss = 8.63727550\n",
      "Iteration 10465, loss = 8.82945885\n",
      "Iteration 10466, loss = 9.04403195\n",
      "Iteration 10467, loss = 9.24020369\n",
      "Iteration 10468, loss = 9.32764354\n",
      "Iteration 10469, loss = 9.69658332\n",
      "Iteration 10470, loss = 9.74788806\n",
      "Iteration 10471, loss = 9.20269688\n",
      "Iteration 10472, loss = 9.87733762\n",
      "Iteration 10473, loss = 9.63434391\n",
      "Iteration 10474, loss = 8.97454918\n",
      "Iteration 10475, loss = 8.95938592\n",
      "Iteration 10476, loss = 8.70440189\n",
      "Iteration 10477, loss = 8.81165999\n",
      "Iteration 10478, loss = 8.97050158\n",
      "Iteration 10479, loss = 9.33540997\n",
      "Iteration 10480, loss = 9.13753306\n",
      "Iteration 10481, loss = 8.79626556\n",
      "Iteration 10482, loss = 9.02645014\n",
      "Iteration 10483, loss = 8.97945170\n",
      "Iteration 10484, loss = 9.90477363\n",
      "Iteration 10485, loss = 9.00424428\n",
      "Iteration 10486, loss = 8.85141852\n",
      "Iteration 10487, loss = 9.72217009\n",
      "Iteration 10488, loss = 10.77898524\n",
      "Iteration 10489, loss = 9.79198492\n",
      "Iteration 10490, loss = 9.09201572\n",
      "Iteration 10491, loss = 9.78201259\n",
      "Iteration 10492, loss = 9.30382822\n",
      "Iteration 10493, loss = 9.34185698\n",
      "Iteration 10494, loss = 9.75736511\n",
      "Iteration 10495, loss = 10.14208614\n",
      "Iteration 10496, loss = 10.85943495\n",
      "Iteration 10497, loss = 10.22966165\n",
      "Iteration 10498, loss = 9.63783193\n",
      "Iteration 10499, loss = 9.01955749\n",
      "Iteration 10500, loss = 9.87654560\n",
      "Iteration 10501, loss = 9.83220095\n",
      "Iteration 10502, loss = 10.13567205\n",
      "Iteration 10503, loss = 9.58912939\n",
      "Iteration 10504, loss = 9.29179248\n",
      "Iteration 10505, loss = 9.72292284\n",
      "Iteration 10506, loss = 9.52733767\n",
      "Iteration 10507, loss = 10.69366850\n",
      "Iteration 10508, loss = 10.20469492\n",
      "Iteration 10509, loss = 9.39800545\n",
      "Iteration 10510, loss = 10.07604871\n",
      "Iteration 10511, loss = 8.85892514\n",
      "Iteration 10512, loss = 9.28255161\n",
      "Iteration 10513, loss = 9.03257136\n",
      "Iteration 10514, loss = 8.81148995\n",
      "Iteration 10515, loss = 9.41810300\n",
      "Iteration 10516, loss = 9.22673391\n",
      "Iteration 10517, loss = 9.53908128\n",
      "Iteration 10518, loss = 9.34320744\n",
      "Iteration 10519, loss = 9.64968108\n",
      "Iteration 10520, loss = 9.04191983\n",
      "Iteration 10521, loss = 9.58101453\n",
      "Iteration 10522, loss = 10.84518342\n",
      "Iteration 10523, loss = 10.63826505\n",
      "Iteration 10524, loss = 9.44430196\n",
      "Iteration 10525, loss = 10.73865430\n",
      "Iteration 10526, loss = 9.55373187\n",
      "Iteration 10527, loss = 9.83090885\n",
      "Iteration 10528, loss = 10.48992530\n",
      "Iteration 10529, loss = 9.69484969\n",
      "Iteration 10530, loss = 10.03799461\n",
      "Iteration 10531, loss = 9.50068989\n",
      "Iteration 10532, loss = 9.38153887\n",
      "Iteration 10533, loss = 9.64957489\n",
      "Iteration 10534, loss = 9.51711839\n",
      "Iteration 10535, loss = 9.81862528\n",
      "Iteration 10536, loss = 10.11181537\n",
      "Iteration 10537, loss = 9.74386479\n",
      "Iteration 10538, loss = 10.08727468\n",
      "Iteration 10539, loss = 9.72210306\n",
      "Iteration 10540, loss = 9.05671505\n",
      "Iteration 10541, loss = 9.94139625\n",
      "Iteration 10542, loss = 10.18798215\n",
      "Iteration 10543, loss = 10.76131305\n",
      "Iteration 10544, loss = 9.41305632\n",
      "Iteration 10545, loss = 9.00619301\n",
      "Iteration 10546, loss = 9.32807620\n",
      "Iteration 10547, loss = 9.48563791\n",
      "Iteration 10548, loss = 9.56189718\n",
      "Iteration 10549, loss = 8.96351867\n",
      "Iteration 10550, loss = 8.73655544\n",
      "Iteration 10551, loss = 9.88007877\n",
      "Iteration 10552, loss = 10.00124972\n",
      "Iteration 10553, loss = 9.54776052\n",
      "Iteration 10554, loss = 9.24300871\n",
      "Iteration 10555, loss = 9.12785443\n",
      "Iteration 10556, loss = 9.30428668\n",
      "Iteration 10557, loss = 9.82454111\n",
      "Iteration 10558, loss = 11.33950726\n",
      "Iteration 10559, loss = 9.65845368\n",
      "Iteration 10560, loss = 9.63094310\n",
      "Iteration 10561, loss = 9.27472464\n",
      "Iteration 10562, loss = 8.56635525\n",
      "Iteration 10563, loss = 9.55403829\n",
      "Iteration 10564, loss = 9.58333485\n",
      "Iteration 10565, loss = 9.16103074\n",
      "Iteration 10566, loss = 9.25092904\n",
      "Iteration 10567, loss = 9.29158166\n",
      "Iteration 10568, loss = 10.42984329\n",
      "Iteration 10569, loss = 10.05004707\n",
      "Iteration 10570, loss = 9.13952022\n",
      "Iteration 10571, loss = 8.90704103\n",
      "Iteration 10572, loss = 9.23999853\n",
      "Iteration 10573, loss = 8.97762695\n",
      "Iteration 10574, loss = 9.05524608\n",
      "Iteration 10575, loss = 10.53141826\n",
      "Iteration 10576, loss = 10.63112143\n",
      "Iteration 10577, loss = 10.39158160\n",
      "Iteration 10578, loss = 10.03586866\n",
      "Iteration 10579, loss = 9.03051315\n",
      "Iteration 10580, loss = 9.33707792\n",
      "Iteration 10581, loss = 9.19281329\n",
      "Iteration 10582, loss = 8.86718729\n",
      "Iteration 10583, loss = 8.77728120\n",
      "Iteration 10584, loss = 8.83369066\n",
      "Iteration 10585, loss = 9.30187648\n",
      "Iteration 10586, loss = 8.84751377\n",
      "Iteration 10587, loss = 8.66595491\n",
      "Iteration 10588, loss = 8.70621214\n",
      "Iteration 10589, loss = 8.57130422\n",
      "Iteration 10590, loss = 8.91147624\n",
      "Iteration 10591, loss = 9.08263978\n",
      "Iteration 10592, loss = 9.07522225\n",
      "Iteration 10593, loss = 9.05346301\n",
      "Iteration 10594, loss = 9.00945301\n",
      "Iteration 10595, loss = 9.16504373\n",
      "Iteration 10596, loss = 9.10690933\n",
      "Iteration 10597, loss = 8.62011615\n",
      "Iteration 10598, loss = 9.08541865\n",
      "Iteration 10599, loss = 9.14747279\n",
      "Iteration 10600, loss = 9.78358283\n",
      "Iteration 10601, loss = 10.09080310\n",
      "Iteration 10602, loss = 10.97693492\n",
      "Iteration 10603, loss = 11.20653252\n",
      "Iteration 10604, loss = 10.00997314\n",
      "Iteration 10605, loss = 9.05597936\n",
      "Iteration 10606, loss = 9.44836918\n",
      "Iteration 10607, loss = 9.14255113\n",
      "Iteration 10608, loss = 9.28623954\n",
      "Iteration 10609, loss = 9.03338790\n",
      "Iteration 10610, loss = 9.68782527\n",
      "Iteration 10611, loss = 8.97654158\n",
      "Iteration 10612, loss = 8.86444974\n",
      "Iteration 10613, loss = 8.86492234\n",
      "Iteration 10614, loss = 8.72981967\n",
      "Iteration 10615, loss = 8.83841490\n",
      "Iteration 10616, loss = 8.71596014\n",
      "Iteration 10617, loss = 8.47743950\n",
      "Iteration 10618, loss = 8.86011093\n",
      "Iteration 10619, loss = 9.29034060\n",
      "Iteration 10620, loss = 8.59813053\n",
      "Iteration 10621, loss = 8.71726089\n",
      "Iteration 10622, loss = 8.60090563\n",
      "Iteration 10623, loss = 8.79594917\n",
      "Iteration 10624, loss = 9.14520194\n",
      "Iteration 10625, loss = 10.44256277\n",
      "Iteration 10626, loss = 9.74676246\n",
      "Iteration 10627, loss = 10.20381272\n",
      "Iteration 10628, loss = 10.02619537\n",
      "Iteration 10629, loss = 9.74551376\n",
      "Iteration 10630, loss = 9.80285494\n",
      "Iteration 10631, loss = 9.23729629\n",
      "Iteration 10632, loss = 9.22599945\n",
      "Iteration 10633, loss = 9.61247971\n",
      "Iteration 10634, loss = 9.21859375\n",
      "Iteration 10635, loss = 8.66544974\n",
      "Iteration 10636, loss = 9.23838481\n",
      "Iteration 10637, loss = 9.52867229\n",
      "Iteration 10638, loss = 9.07607392\n",
      "Iteration 10639, loss = 8.87344887\n",
      "Iteration 10640, loss = 9.36079494\n",
      "Iteration 10641, loss = 9.55318699\n",
      "Iteration 10642, loss = 8.70712760\n",
      "Iteration 10643, loss = 8.99081848\n",
      "Iteration 10644, loss = 8.88119213\n",
      "Iteration 10645, loss = 8.54947129\n",
      "Iteration 10646, loss = 8.96804610\n",
      "Iteration 10647, loss = 9.19019714\n",
      "Iteration 10648, loss = 9.11894194\n",
      "Iteration 10649, loss = 8.87540676\n",
      "Iteration 10650, loss = 8.60815679\n",
      "Iteration 10651, loss = 8.60174152\n",
      "Iteration 10652, loss = 9.10210634\n",
      "Iteration 10653, loss = 10.56189836\n",
      "Iteration 10654, loss = 8.77859717\n",
      "Iteration 10655, loss = 9.06037881\n",
      "Iteration 10656, loss = 8.73904742\n",
      "Iteration 10657, loss = 8.69015231\n",
      "Iteration 10658, loss = 8.95503609\n",
      "Iteration 10659, loss = 8.75490669\n",
      "Iteration 10660, loss = 9.33088377\n",
      "Iteration 10661, loss = 8.82882917\n",
      "Iteration 10662, loss = 9.19553823\n",
      "Iteration 10663, loss = 9.11088637\n",
      "Iteration 10664, loss = 9.54249018\n",
      "Iteration 10665, loss = 10.47001542\n",
      "Iteration 10666, loss = 10.78507083\n",
      "Iteration 10667, loss = 9.73087685\n",
      "Iteration 10668, loss = 8.69386853\n",
      "Iteration 10669, loss = 8.98787520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10670, loss = 9.32974045\n",
      "Iteration 10671, loss = 9.60291534\n",
      "Iteration 10672, loss = 9.15856424\n",
      "Iteration 10673, loss = 9.44772613\n",
      "Iteration 10674, loss = 9.91026327\n",
      "Iteration 10675, loss = 8.89578840\n",
      "Iteration 10676, loss = 9.05896241\n",
      "Iteration 10677, loss = 9.33596042\n",
      "Iteration 10678, loss = 9.15481664\n",
      "Iteration 10679, loss = 10.05009266\n",
      "Iteration 10680, loss = 9.80732069\n",
      "Iteration 10681, loss = 9.46326731\n",
      "Iteration 10682, loss = 9.96364596\n",
      "Iteration 10683, loss = 9.19810724\n",
      "Iteration 10684, loss = 9.23760998\n",
      "Iteration 10685, loss = 9.14638453\n",
      "Iteration 10686, loss = 8.72404470\n",
      "Iteration 10687, loss = 9.42629174\n",
      "Iteration 10688, loss = 8.90930980\n",
      "Iteration 10689, loss = 8.96878545\n",
      "Iteration 10690, loss = 9.14864353\n",
      "Iteration 10691, loss = 10.10005968\n",
      "Iteration 10692, loss = 10.93312316\n",
      "Iteration 10693, loss = 11.07029777\n",
      "Iteration 10694, loss = 9.42834517\n",
      "Iteration 10695, loss = 8.68543236\n",
      "Iteration 10696, loss = 8.99386881\n",
      "Iteration 10697, loss = 8.72657541\n",
      "Iteration 10698, loss = 8.79281390\n",
      "Iteration 10699, loss = 9.23595624\n",
      "Iteration 10700, loss = 9.45259677\n",
      "Iteration 10701, loss = 9.15055804\n",
      "Iteration 10702, loss = 8.94323583\n",
      "Iteration 10703, loss = 9.21856433\n",
      "Iteration 10704, loss = 9.42878297\n",
      "Iteration 10705, loss = 8.98630806\n",
      "Iteration 10706, loss = 9.09690339\n",
      "Iteration 10707, loss = 8.55256277\n",
      "Iteration 10708, loss = 9.37576852\n",
      "Iteration 10709, loss = 8.80847837\n",
      "Iteration 10710, loss = 9.03464493\n",
      "Iteration 10711, loss = 9.03150601\n",
      "Iteration 10712, loss = 9.01029090\n",
      "Iteration 10713, loss = 8.63010324\n",
      "Iteration 10714, loss = 9.02811658\n",
      "Iteration 10715, loss = 9.07668637\n",
      "Iteration 10716, loss = 9.26935193\n",
      "Iteration 10717, loss = 9.15130757\n",
      "Iteration 10718, loss = 9.05081777\n",
      "Iteration 10719, loss = 9.01887721\n",
      "Iteration 10720, loss = 8.53996127\n",
      "Iteration 10721, loss = 8.85882431\n",
      "Iteration 10722, loss = 9.05781580\n",
      "Iteration 10723, loss = 8.81127984\n",
      "Iteration 10724, loss = 9.64828197\n",
      "Iteration 10725, loss = 10.42449307\n",
      "Iteration 10726, loss = 9.10995469\n",
      "Iteration 10727, loss = 9.16957803\n",
      "Iteration 10728, loss = 9.10646973\n",
      "Iteration 10729, loss = 9.11937586\n",
      "Iteration 10730, loss = 8.91623855\n",
      "Iteration 10731, loss = 8.97991947\n",
      "Iteration 10732, loss = 8.67436940\n",
      "Iteration 10733, loss = 9.53881518\n",
      "Iteration 10734, loss = 8.07002653\n",
      "Iteration 10735, loss = 9.07105531\n",
      "Iteration 10736, loss = 8.73695298\n",
      "Iteration 10737, loss = 9.61510479\n",
      "Iteration 10738, loss = 9.33823974\n",
      "Iteration 10739, loss = 9.14031688\n",
      "Iteration 10740, loss = 9.23036046\n",
      "Iteration 10741, loss = 9.49993940\n",
      "Iteration 10742, loss = 8.87260430\n",
      "Iteration 10743, loss = 8.87622293\n",
      "Iteration 10744, loss = 8.66714413\n",
      "Iteration 10745, loss = 8.71430750\n",
      "Iteration 10746, loss = 8.80040941\n",
      "Iteration 10747, loss = 8.65591821\n",
      "Iteration 10748, loss = 8.83773251\n",
      "Iteration 10749, loss = 9.00009274\n",
      "Iteration 10750, loss = 9.44377828\n",
      "Iteration 10751, loss = 8.96227867\n",
      "Iteration 10752, loss = 8.97113926\n",
      "Iteration 10753, loss = 9.16842661\n",
      "Iteration 10754, loss = 8.77880835\n",
      "Iteration 10755, loss = 9.05369164\n",
      "Iteration 10756, loss = 8.45685396\n",
      "Iteration 10757, loss = 8.74381835\n",
      "Iteration 10758, loss = 9.09003346\n",
      "Iteration 10759, loss = 9.25882613\n",
      "Iteration 10760, loss = 9.19023889\n",
      "Iteration 10761, loss = 9.01126407\n",
      "Iteration 10762, loss = 9.03947359\n",
      "Iteration 10763, loss = 10.12002157\n",
      "Iteration 10764, loss = 9.24870666\n",
      "Iteration 10765, loss = 8.76764114\n",
      "Iteration 10766, loss = 9.40744835\n",
      "Iteration 10767, loss = 9.44528524\n",
      "Iteration 10768, loss = 8.67204135\n",
      "Iteration 10769, loss = 8.67390526\n",
      "Iteration 10770, loss = 8.98728945\n",
      "Iteration 10771, loss = 8.82641639\n",
      "Iteration 10772, loss = 9.20196851\n",
      "Iteration 10773, loss = 9.62584526\n",
      "Iteration 10774, loss = 8.67152022\n",
      "Iteration 10775, loss = 9.63798506\n",
      "Iteration 10776, loss = 9.64106537\n",
      "Iteration 10777, loss = 9.32702698\n",
      "Iteration 10778, loss = 8.84821748\n",
      "Iteration 10779, loss = 9.62384799\n",
      "Iteration 10780, loss = 9.79828668\n",
      "Iteration 10781, loss = 9.74525943\n",
      "Iteration 10782, loss = 9.16784244\n",
      "Iteration 10783, loss = 8.86713339\n",
      "Iteration 10784, loss = 8.52174965\n",
      "Iteration 10785, loss = 8.77753007\n",
      "Iteration 10786, loss = 9.17769647\n",
      "Iteration 10787, loss = 8.89384031\n",
      "Iteration 10788, loss = 9.40893350\n",
      "Iteration 10789, loss = 8.96736753\n",
      "Iteration 10790, loss = 9.82221633\n",
      "Iteration 10791, loss = 8.43007562\n",
      "Iteration 10792, loss = 10.71666041\n",
      "Iteration 10793, loss = 10.30839892\n",
      "Iteration 10794, loss = 9.17953525\n",
      "Iteration 10795, loss = 9.07109413\n",
      "Iteration 10796, loss = 9.01704068\n",
      "Iteration 10797, loss = 8.64040872\n",
      "Iteration 10798, loss = 8.85546921\n",
      "Iteration 10799, loss = 9.68987292\n",
      "Iteration 10800, loss = 9.96691185\n",
      "Iteration 10801, loss = 11.37282041\n",
      "Iteration 10802, loss = 9.95078695\n",
      "Iteration 10803, loss = 9.21284932\n",
      "Iteration 10804, loss = 8.57798010\n",
      "Iteration 10805, loss = 8.88577356\n",
      "Iteration 10806, loss = 9.22492502\n",
      "Iteration 10807, loss = 8.86723453\n",
      "Iteration 10808, loss = 8.90162695\n",
      "Iteration 10809, loss = 8.70285873\n",
      "Iteration 10810, loss = 8.97762567\n",
      "Iteration 10811, loss = 8.66799735\n",
      "Iteration 10812, loss = 8.96370410\n",
      "Iteration 10813, loss = 8.54774414\n",
      "Iteration 10814, loss = 9.18717971\n",
      "Iteration 10815, loss = 8.80866711\n",
      "Iteration 10816, loss = 8.57888644\n",
      "Iteration 10817, loss = 8.68897812\n",
      "Iteration 10818, loss = 8.87941421\n",
      "Iteration 10819, loss = 9.95038366\n",
      "Iteration 10820, loss = 9.43977715\n",
      "Iteration 10821, loss = 9.36571591\n",
      "Iteration 10822, loss = 9.61794466\n",
      "Iteration 10823, loss = 8.99604026\n",
      "Iteration 10824, loss = 9.34710589\n",
      "Iteration 10825, loss = 9.42455269\n",
      "Iteration 10826, loss = 9.49252712\n",
      "Iteration 10827, loss = 9.54563617\n",
      "Iteration 10828, loss = 8.82474552\n",
      "Iteration 10829, loss = 9.24716556\n",
      "Iteration 10830, loss = 9.27435166\n",
      "Iteration 10831, loss = 9.23738444\n",
      "Iteration 10832, loss = 9.59881480\n",
      "Iteration 10833, loss = 10.85455323\n",
      "Iteration 10834, loss = 11.17087422\n",
      "Iteration 10835, loss = 14.62107524\n",
      "Iteration 10836, loss = 13.00027795\n",
      "Iteration 10837, loss = 12.22940653\n",
      "Iteration 10838, loss = 9.85309277\n",
      "Iteration 10839, loss = 10.50941222\n",
      "Iteration 10840, loss = 8.93148192\n",
      "Iteration 10841, loss = 8.97027575\n",
      "Iteration 10842, loss = 8.71213159\n",
      "Iteration 10843, loss = 8.58045428\n",
      "Iteration 10844, loss = 8.51421757\n",
      "Iteration 10845, loss = 8.83527505\n",
      "Iteration 10846, loss = 8.97994235\n",
      "Iteration 10847, loss = 9.08536412\n",
      "Iteration 10848, loss = 8.69809073\n",
      "Iteration 10849, loss = 8.58884954\n",
      "Iteration 10850, loss = 9.23058352\n",
      "Iteration 10851, loss = 9.81844650\n",
      "Iteration 10852, loss = 9.79997787\n",
      "Iteration 10853, loss = 8.91130246\n",
      "Iteration 10854, loss = 9.55687746\n",
      "Iteration 10855, loss = 9.28821548\n",
      "Iteration 10856, loss = 9.36842198\n",
      "Iteration 10857, loss = 9.11136263\n",
      "Iteration 10858, loss = 9.12521598\n",
      "Iteration 10859, loss = 8.57553548\n",
      "Iteration 10860, loss = 9.01387132\n",
      "Iteration 10861, loss = 9.12789819\n",
      "Iteration 10862, loss = 9.64435643\n",
      "Iteration 10863, loss = 9.39264919\n",
      "Iteration 10864, loss = 9.08570119\n",
      "Iteration 10865, loss = 8.77344947\n",
      "Iteration 10866, loss = 9.02487639\n",
      "Iteration 10867, loss = 9.50895241\n",
      "Iteration 10868, loss = 8.73427988\n",
      "Iteration 10869, loss = 8.86356458\n",
      "Iteration 10870, loss = 9.30647466\n",
      "Iteration 10871, loss = 9.53834286\n",
      "Iteration 10872, loss = 8.97506025\n",
      "Iteration 10873, loss = 9.56237331\n",
      "Iteration 10874, loss = 9.48445450\n",
      "Iteration 10875, loss = 9.55033001\n",
      "Iteration 10876, loss = 9.65147972\n",
      "Iteration 10877, loss = 9.20419120\n",
      "Iteration 10878, loss = 9.81369106\n",
      "Iteration 10879, loss = 10.26580755\n",
      "Iteration 10880, loss = 9.31052542\n",
      "Iteration 10881, loss = 9.24780675\n",
      "Iteration 10882, loss = 10.03495510\n",
      "Iteration 10883, loss = 9.69186208\n",
      "Iteration 10884, loss = 8.76285577\n",
      "Iteration 10885, loss = 9.04668956\n",
      "Iteration 10886, loss = 8.44131151\n",
      "Iteration 10887, loss = 9.90342672\n",
      "Iteration 10888, loss = 8.77516754\n",
      "Iteration 10889, loss = 9.19931580\n",
      "Iteration 10890, loss = 8.84804274\n",
      "Iteration 10891, loss = 10.17429530\n",
      "Iteration 10892, loss = 10.81837482\n",
      "Iteration 10893, loss = 10.62237885\n",
      "Iteration 10894, loss = 9.50953530\n",
      "Iteration 10895, loss = 10.26007584\n",
      "Iteration 10896, loss = 9.70610037\n",
      "Iteration 10897, loss = 10.76168022\n",
      "Iteration 10898, loss = 10.84436379\n",
      "Iteration 10899, loss = 11.45311522\n",
      "Iteration 10900, loss = 12.41569960\n",
      "Iteration 10901, loss = 10.82102171\n",
      "Iteration 10902, loss = 10.57428185\n",
      "Iteration 10903, loss = 10.08732024\n",
      "Iteration 10904, loss = 9.38261156\n",
      "Iteration 10905, loss = 8.89450890\n",
      "Iteration 10906, loss = 9.32077365\n",
      "Iteration 10907, loss = 9.29446717\n",
      "Iteration 10908, loss = 10.28185283\n",
      "Iteration 10909, loss = 9.53613409\n",
      "Iteration 10910, loss = 9.05759025\n",
      "Iteration 10911, loss = 8.76723956\n",
      "Iteration 10912, loss = 9.01556628\n",
      "Iteration 10913, loss = 8.52801094\n",
      "Iteration 10914, loss = 8.62228810\n",
      "Iteration 10915, loss = 9.02401672\n",
      "Iteration 10916, loss = 8.97203220\n",
      "Iteration 10917, loss = 10.49079862\n",
      "Iteration 10918, loss = 9.66163034\n",
      "Iteration 10919, loss = 8.93257573\n",
      "Iteration 10920, loss = 8.59005937\n",
      "Iteration 10921, loss = 8.43251540\n",
      "Iteration 10922, loss = 8.67962714\n",
      "Iteration 10923, loss = 8.97378890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10924, loss = 8.66778309\n",
      "Iteration 10925, loss = 9.23364295\n",
      "Iteration 10926, loss = 9.00984365\n",
      "Iteration 10927, loss = 9.19910672\n",
      "Iteration 10928, loss = 8.71349447\n",
      "Iteration 10929, loss = 9.25766069\n",
      "Iteration 10930, loss = 9.46583724\n",
      "Iteration 10931, loss = 9.02227307\n",
      "Iteration 10932, loss = 9.81789718\n",
      "Iteration 10933, loss = 9.34341412\n",
      "Iteration 10934, loss = 9.59159218\n",
      "Iteration 10935, loss = 9.33766829\n",
      "Iteration 10936, loss = 9.27266188\n",
      "Iteration 10937, loss = 10.24076321\n",
      "Iteration 10938, loss = 8.83610267\n",
      "Iteration 10939, loss = 8.95217390\n",
      "Iteration 10940, loss = 9.22658435\n",
      "Iteration 10941, loss = 8.74935827\n",
      "Iteration 10942, loss = 8.74634787\n",
      "Iteration 10943, loss = 9.21726978\n",
      "Iteration 10944, loss = 9.61331279\n",
      "Iteration 10945, loss = 9.62491040\n",
      "Iteration 10946, loss = 9.50248121\n",
      "Iteration 10947, loss = 9.49622603\n",
      "Iteration 10948, loss = 8.75081181\n",
      "Iteration 10949, loss = 9.50865605\n",
      "Iteration 10950, loss = 10.27305491\n",
      "Iteration 10951, loss = 10.37254317\n",
      "Iteration 10952, loss = 9.35138349\n",
      "Iteration 10953, loss = 9.22856842\n",
      "Iteration 10954, loss = 8.69107542\n",
      "Iteration 10955, loss = 9.35787834\n",
      "Iteration 10956, loss = 9.51893450\n",
      "Iteration 10957, loss = 9.04904789\n",
      "Iteration 10958, loss = 8.86677282\n",
      "Iteration 10959, loss = 8.51515621\n",
      "Iteration 10960, loss = 9.04346066\n",
      "Iteration 10961, loss = 8.75772805\n",
      "Iteration 10962, loss = 9.97076671\n",
      "Iteration 10963, loss = 9.27962691\n",
      "Iteration 10964, loss = 9.06997461\n",
      "Iteration 10965, loss = 9.02145899\n",
      "Iteration 10966, loss = 9.39011932\n",
      "Iteration 10967, loss = 9.83889451\n",
      "Iteration 10968, loss = 8.85586844\n",
      "Iteration 10969, loss = 8.79014383\n",
      "Iteration 10970, loss = 8.64053760\n",
      "Iteration 10971, loss = 8.98017329\n",
      "Iteration 10972, loss = 8.78213275\n",
      "Iteration 10973, loss = 9.03189316\n",
      "Iteration 10974, loss = 8.46083219\n",
      "Iteration 10975, loss = 8.47888554\n",
      "Iteration 10976, loss = 8.74504366\n",
      "Iteration 10977, loss = 8.62585652\n",
      "Iteration 10978, loss = 8.45774207\n",
      "Iteration 10979, loss = 8.66904315\n",
      "Iteration 10980, loss = 8.58716997\n",
      "Iteration 10981, loss = 8.40865798\n",
      "Iteration 10982, loss = 9.18914698\n",
      "Iteration 10983, loss = 8.87167379\n",
      "Iteration 10984, loss = 8.64557858\n",
      "Iteration 10985, loss = 8.66691050\n",
      "Iteration 10986, loss = 8.93725629\n",
      "Iteration 10987, loss = 8.79200214\n",
      "Iteration 10988, loss = 8.45000760\n",
      "Iteration 10989, loss = 8.63263296\n",
      "Iteration 10990, loss = 8.48388332\n",
      "Iteration 10991, loss = 8.83370846\n",
      "Iteration 10992, loss = 8.34032218\n",
      "Iteration 10993, loss = 8.40809069\n",
      "Iteration 10994, loss = 8.52396154\n",
      "Iteration 10995, loss = 9.37362929\n",
      "Iteration 10996, loss = 9.18063211\n",
      "Iteration 10997, loss = 8.84325290\n",
      "Iteration 10998, loss = 9.02042467\n",
      "Iteration 10999, loss = 8.80753131\n",
      "Iteration 11000, loss = 8.50163157\n",
      "Iteration 11001, loss = 8.94007792\n",
      "Iteration 11002, loss = 8.57280499\n",
      "Iteration 11003, loss = 9.08008213\n",
      "Iteration 11004, loss = 8.53340814\n",
      "Iteration 11005, loss = 8.58521186\n",
      "Iteration 11006, loss = 8.61761202\n",
      "Iteration 11007, loss = 8.90833254\n",
      "Iteration 11008, loss = 9.17694457\n",
      "Iteration 11009, loss = 9.21957612\n",
      "Iteration 11010, loss = 9.30883884\n",
      "Iteration 11011, loss = 9.14551014\n",
      "Iteration 11012, loss = 8.97768135\n",
      "Iteration 11013, loss = 9.35311344\n",
      "Iteration 11014, loss = 8.56747173\n",
      "Iteration 11015, loss = 8.48454148\n",
      "Iteration 11016, loss = 8.50176403\n",
      "Iteration 11017, loss = 9.36344955\n",
      "Iteration 11018, loss = 8.59138824\n",
      "Iteration 11019, loss = 8.82589022\n",
      "Iteration 11020, loss = 8.72215126\n",
      "Iteration 11021, loss = 8.92575040\n",
      "Iteration 11022, loss = 9.01034139\n",
      "Iteration 11023, loss = 8.75701424\n",
      "Iteration 11024, loss = 8.87683567\n",
      "Iteration 11025, loss = 8.63179488\n",
      "Iteration 11026, loss = 8.76403836\n",
      "Iteration 11027, loss = 8.63728798\n",
      "Iteration 11028, loss = 8.62275729\n",
      "Iteration 11029, loss = 8.96907920\n",
      "Iteration 11030, loss = 8.56417662\n",
      "Iteration 11031, loss = 8.31661711\n",
      "Iteration 11032, loss = 9.00719384\n",
      "Iteration 11033, loss = 8.78546469\n",
      "Iteration 11034, loss = 8.62977243\n",
      "Iteration 11035, loss = 8.55794499\n",
      "Iteration 11036, loss = 9.14466611\n",
      "Iteration 11037, loss = 8.57500008\n",
      "Iteration 11038, loss = 8.76048284\n",
      "Iteration 11039, loss = 8.45981629\n",
      "Iteration 11040, loss = 8.83496475\n",
      "Iteration 11041, loss = 8.60093546\n",
      "Iteration 11042, loss = 8.67821986\n",
      "Iteration 11043, loss = 8.98426371\n",
      "Iteration 11044, loss = 8.83803668\n",
      "Iteration 11045, loss = 9.35539740\n",
      "Iteration 11046, loss = 8.89546796\n",
      "Iteration 11047, loss = 8.52579083\n",
      "Iteration 11048, loss = 9.35668840\n",
      "Iteration 11049, loss = 9.30583129\n",
      "Iteration 11050, loss = 10.14640643\n",
      "Iteration 11051, loss = 10.19651920\n",
      "Iteration 11052, loss = 8.38207467\n",
      "Iteration 11053, loss = 9.06389639\n",
      "Iteration 11054, loss = 8.55246032\n",
      "Iteration 11055, loss = 8.84373698\n",
      "Iteration 11056, loss = 8.84024109\n",
      "Iteration 11057, loss = 10.53383458\n",
      "Iteration 11058, loss = 9.40449075\n",
      "Iteration 11059, loss = 9.98173320\n",
      "Iteration 11060, loss = 10.01768047\n",
      "Iteration 11061, loss = 8.51479866\n",
      "Iteration 11062, loss = 8.65350944\n",
      "Iteration 11063, loss = 8.94284783\n",
      "Iteration 11064, loss = 8.87266475\n",
      "Iteration 11065, loss = 9.00823101\n",
      "Iteration 11066, loss = 8.70342825\n",
      "Iteration 11067, loss = 8.56724379\n",
      "Iteration 11068, loss = 8.84805699\n",
      "Iteration 11069, loss = 9.00879018\n",
      "Iteration 11070, loss = 8.52198801\n",
      "Iteration 11071, loss = 9.15540448\n",
      "Iteration 11072, loss = 9.31125844\n",
      "Iteration 11073, loss = 9.32212205\n",
      "Iteration 11074, loss = 10.11482057\n",
      "Iteration 11075, loss = 8.68963445\n",
      "Iteration 11076, loss = 9.29823908\n",
      "Iteration 11077, loss = 9.54593194\n",
      "Iteration 11078, loss = 9.59019264\n",
      "Iteration 11079, loss = 10.01902791\n",
      "Iteration 11080, loss = 9.97250794\n",
      "Iteration 11081, loss = 9.52748960\n",
      "Iteration 11082, loss = 8.95763517\n",
      "Iteration 11083, loss = 9.94543097\n",
      "Iteration 11084, loss = 8.93219027\n",
      "Iteration 11085, loss = 9.78193696\n",
      "Iteration 11086, loss = 9.02088620\n",
      "Iteration 11087, loss = 8.45339801\n",
      "Iteration 11088, loss = 10.35607702\n",
      "Iteration 11089, loss = 10.27761116\n",
      "Iteration 11090, loss = 9.58937781\n",
      "Iteration 11091, loss = 9.91475723\n",
      "Iteration 11092, loss = 8.92045133\n",
      "Iteration 11093, loss = 9.13493791\n",
      "Iteration 11094, loss = 9.62833593\n",
      "Iteration 11095, loss = 8.63023997\n",
      "Iteration 11096, loss = 8.92011071\n",
      "Iteration 11097, loss = 8.72111417\n",
      "Iteration 11098, loss = 9.05724717\n",
      "Iteration 11099, loss = 8.55139567\n",
      "Iteration 11100, loss = 8.41276581\n",
      "Iteration 11101, loss = 9.08892096\n",
      "Iteration 11102, loss = 9.14176794\n",
      "Iteration 11103, loss = 8.62690392\n",
      "Iteration 11104, loss = 8.77993987\n",
      "Iteration 11105, loss = 8.47526383\n",
      "Iteration 11106, loss = 8.56135147\n",
      "Iteration 11107, loss = 8.58420013\n",
      "Iteration 11108, loss = 8.47535821\n",
      "Iteration 11109, loss = 9.14401417\n",
      "Iteration 11110, loss = 10.40520761\n",
      "Iteration 11111, loss = 11.03437281\n",
      "Iteration 11112, loss = 9.60752935\n",
      "Iteration 11113, loss = 9.07977969\n",
      "Iteration 11114, loss = 9.24245040\n",
      "Iteration 11115, loss = 9.31466464\n",
      "Iteration 11116, loss = 8.63311067\n",
      "Iteration 11117, loss = 8.53479232\n",
      "Iteration 11118, loss = 8.36828568\n",
      "Iteration 11119, loss = 8.92245940\n",
      "Iteration 11120, loss = 8.57307443\n",
      "Iteration 11121, loss = 9.18653095\n",
      "Iteration 11122, loss = 9.21452812\n",
      "Iteration 11123, loss = 8.72899369\n",
      "Iteration 11124, loss = 8.96380977\n",
      "Iteration 11125, loss = 8.37850499\n",
      "Iteration 11126, loss = 8.87476335\n",
      "Iteration 11127, loss = 8.61669292\n",
      "Iteration 11128, loss = 9.83381578\n",
      "Iteration 11129, loss = 9.04894774\n",
      "Iteration 11130, loss = 8.88079212\n",
      "Iteration 11131, loss = 8.94450717\n",
      "Iteration 11132, loss = 8.73055032\n",
      "Iteration 11133, loss = 8.45665270\n",
      "Iteration 11134, loss = 9.09885962\n",
      "Iteration 11135, loss = 8.57624577\n",
      "Iteration 11136, loss = 8.76313117\n",
      "Iteration 11137, loss = 8.47103642\n",
      "Iteration 11138, loss = 8.19728264\n",
      "Iteration 11139, loss = 8.44985785\n",
      "Iteration 11140, loss = 8.46704743\n",
      "Iteration 11141, loss = 9.50119755\n",
      "Iteration 11142, loss = 9.84658496\n",
      "Iteration 11143, loss = 11.25158674\n",
      "Iteration 11144, loss = 11.71754691\n",
      "Iteration 11145, loss = 10.58142053\n",
      "Iteration 11146, loss = 10.27102887\n",
      "Iteration 11147, loss = 9.41892428\n",
      "Iteration 11148, loss = 9.21712530\n",
      "Iteration 11149, loss = 8.70920494\n",
      "Iteration 11150, loss = 8.49608975\n",
      "Iteration 11151, loss = 8.71906431\n",
      "Iteration 11152, loss = 8.37521464\n",
      "Iteration 11153, loss = 8.88010383\n",
      "Iteration 11154, loss = 9.69725740\n",
      "Iteration 11155, loss = 9.21928866\n",
      "Iteration 11156, loss = 9.94016127\n",
      "Iteration 11157, loss = 9.13179429\n",
      "Iteration 11158, loss = 9.07524271\n",
      "Iteration 11159, loss = 9.57052893\n",
      "Iteration 11160, loss = 10.41182996\n",
      "Iteration 11161, loss = 9.92419486\n",
      "Iteration 11162, loss = 9.20000298\n",
      "Iteration 11163, loss = 9.41152608\n",
      "Iteration 11164, loss = 8.85096647\n",
      "Iteration 11165, loss = 10.25454551\n",
      "Iteration 11166, loss = 9.11222337\n",
      "Iteration 11167, loss = 8.54431249\n",
      "Iteration 11168, loss = 8.29144192\n",
      "Iteration 11169, loss = 8.59106196\n",
      "Iteration 11170, loss = 8.37061391\n",
      "Iteration 11171, loss = 8.41235333\n",
      "Iteration 11172, loss = 9.00236660\n",
      "Iteration 11173, loss = 8.39040835\n",
      "Iteration 11174, loss = 8.83643651\n",
      "Iteration 11175, loss = 8.88248071\n",
      "Iteration 11176, loss = 8.36404343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11177, loss = 8.23789558\n",
      "Iteration 11178, loss = 8.35989897\n",
      "Iteration 11179, loss = 8.67949440\n",
      "Iteration 11180, loss = 8.52239276\n",
      "Iteration 11181, loss = 8.26716141\n",
      "Iteration 11182, loss = 8.33007665\n",
      "Iteration 11183, loss = 8.25824388\n",
      "Iteration 11184, loss = 8.23859504\n",
      "Iteration 11185, loss = 8.37299554\n",
      "Iteration 11186, loss = 8.42957690\n",
      "Iteration 11187, loss = 9.34221347\n",
      "Iteration 11188, loss = 8.35648802\n",
      "Iteration 11189, loss = 8.87082844\n",
      "Iteration 11190, loss = 8.37660193\n",
      "Iteration 11191, loss = 9.35078027\n",
      "Iteration 11192, loss = 9.17737284\n",
      "Iteration 11193, loss = 9.54879834\n",
      "Iteration 11194, loss = 9.05621145\n",
      "Iteration 11195, loss = 8.35739142\n",
      "Iteration 11196, loss = 9.32503005\n",
      "Iteration 11197, loss = 8.60996093\n",
      "Iteration 11198, loss = 9.31581185\n",
      "Iteration 11199, loss = 9.46046232\n",
      "Iteration 11200, loss = 9.58570247\n",
      "Iteration 11201, loss = 9.30773549\n",
      "Iteration 11202, loss = 9.11969723\n",
      "Iteration 11203, loss = 8.67300961\n",
      "Iteration 11204, loss = 8.79079343\n",
      "Iteration 11205, loss = 8.87180343\n",
      "Iteration 11206, loss = 8.34643665\n",
      "Iteration 11207, loss = 8.83835684\n",
      "Iteration 11208, loss = 9.28603858\n",
      "Iteration 11209, loss = 9.88413378\n",
      "Iteration 11210, loss = 9.33382365\n",
      "Iteration 11211, loss = 10.09392558\n",
      "Iteration 11212, loss = 9.81289783\n",
      "Iteration 11213, loss = 10.41795706\n",
      "Iteration 11214, loss = 9.46848690\n",
      "Iteration 11215, loss = 9.13715401\n",
      "Iteration 11216, loss = 8.78686592\n",
      "Iteration 11217, loss = 8.80225563\n",
      "Iteration 11218, loss = 8.46783769\n",
      "Iteration 11219, loss = 9.04596404\n",
      "Iteration 11220, loss = 11.68594244\n",
      "Iteration 11221, loss = 10.48018905\n",
      "Iteration 11222, loss = 10.31048998\n",
      "Iteration 11223, loss = 8.99928530\n",
      "Iteration 11224, loss = 10.01880924\n",
      "Iteration 11225, loss = 9.39273925\n",
      "Iteration 11226, loss = 8.91444126\n",
      "Iteration 11227, loss = 9.54346004\n",
      "Iteration 11228, loss = 8.85812298\n",
      "Iteration 11229, loss = 10.47468537\n",
      "Iteration 11230, loss = 10.05136199\n",
      "Iteration 11231, loss = 12.09878043\n",
      "Iteration 11232, loss = 10.74294220\n",
      "Iteration 11233, loss = 10.97391944\n",
      "Iteration 11234, loss = 10.14940071\n",
      "Iteration 11235, loss = 8.83257246\n",
      "Iteration 11236, loss = 8.66732047\n",
      "Iteration 11237, loss = 8.21503884\n",
      "Iteration 11238, loss = 8.82500236\n",
      "Iteration 11239, loss = 8.72185357\n",
      "Iteration 11240, loss = 8.59752609\n",
      "Iteration 11241, loss = 8.85353643\n",
      "Iteration 11242, loss = 8.46511478\n",
      "Iteration 11243, loss = 8.58764016\n",
      "Iteration 11244, loss = 8.81872734\n",
      "Iteration 11245, loss = 9.03457151\n",
      "Iteration 11246, loss = 9.40630926\n",
      "Iteration 11247, loss = 8.66458595\n",
      "Iteration 11248, loss = 8.89045148\n",
      "Iteration 11249, loss = 8.39441732\n",
      "Iteration 11250, loss = 9.37295098\n",
      "Iteration 11251, loss = 8.95295567\n",
      "Iteration 11252, loss = 9.00941122\n",
      "Iteration 11253, loss = 8.49267777\n",
      "Iteration 11254, loss = 8.57366633\n",
      "Iteration 11255, loss = 8.62668192\n",
      "Iteration 11256, loss = 8.53204629\n",
      "Iteration 11257, loss = 8.44279244\n",
      "Iteration 11258, loss = 8.75547305\n",
      "Iteration 11259, loss = 8.92977095\n",
      "Iteration 11260, loss = 8.81355976\n",
      "Iteration 11261, loss = 9.60845549\n",
      "Iteration 11262, loss = 10.57720048\n",
      "Iteration 11263, loss = 9.52222667\n",
      "Iteration 11264, loss = 8.55201614\n",
      "Iteration 11265, loss = 8.41811430\n",
      "Iteration 11266, loss = 8.50187407\n",
      "Iteration 11267, loss = 8.50526705\n",
      "Iteration 11268, loss = 8.61320055\n",
      "Iteration 11269, loss = 8.52420359\n",
      "Iteration 11270, loss = 9.63255601\n",
      "Iteration 11271, loss = 8.65481357\n",
      "Iteration 11272, loss = 8.67526326\n",
      "Iteration 11273, loss = 8.85387385\n",
      "Iteration 11274, loss = 8.59382233\n",
      "Iteration 11275, loss = 9.35544011\n",
      "Iteration 11276, loss = 9.66942108\n",
      "Iteration 11277, loss = 9.94766739\n",
      "Iteration 11278, loss = 11.58415043\n",
      "Iteration 11279, loss = 10.83090374\n",
      "Iteration 11280, loss = 10.03805635\n",
      "Iteration 11281, loss = 9.39165523\n",
      "Iteration 11282, loss = 9.38509307\n",
      "Iteration 11283, loss = 9.08287645\n",
      "Iteration 11284, loss = 9.02489822\n",
      "Iteration 11285, loss = 9.55095934\n",
      "Iteration 11286, loss = 9.23258435\n",
      "Iteration 11287, loss = 9.05497116\n",
      "Iteration 11288, loss = 9.34003261\n",
      "Iteration 11289, loss = 9.52126063\n",
      "Iteration 11290, loss = 9.20382908\n",
      "Iteration 11291, loss = 8.62485515\n",
      "Iteration 11292, loss = 8.05449662\n",
      "Iteration 11293, loss = 9.14000331\n",
      "Iteration 11294, loss = 8.40538520\n",
      "Iteration 11295, loss = 8.42722898\n",
      "Iteration 11296, loss = 8.54832959\n",
      "Iteration 11297, loss = 8.81667244\n",
      "Iteration 11298, loss = 8.50488575\n",
      "Iteration 11299, loss = 8.21650280\n",
      "Iteration 11300, loss = 8.30417270\n",
      "Iteration 11301, loss = 8.81330149\n",
      "Iteration 11302, loss = 9.09808729\n",
      "Iteration 11303, loss = 8.94365453\n",
      "Iteration 11304, loss = 9.73500300\n",
      "Iteration 11305, loss = 10.59394354\n",
      "Iteration 11306, loss = 9.82592822\n",
      "Iteration 11307, loss = 9.94307937\n",
      "Iteration 11308, loss = 10.02475089\n",
      "Iteration 11309, loss = 9.49679204\n",
      "Iteration 11310, loss = 9.07146071\n",
      "Iteration 11311, loss = 9.23011453\n",
      "Iteration 11312, loss = 9.57354480\n",
      "Iteration 11313, loss = 9.31737742\n",
      "Iteration 11314, loss = 8.91381664\n",
      "Iteration 11315, loss = 8.58525055\n",
      "Iteration 11316, loss = 8.63423042\n",
      "Iteration 11317, loss = 8.88129574\n",
      "Iteration 11318, loss = 8.88416637\n",
      "Iteration 11319, loss = 8.19862268\n",
      "Iteration 11320, loss = 8.68187701\n",
      "Iteration 11321, loss = 8.45378718\n",
      "Iteration 11322, loss = 8.66596407\n",
      "Iteration 11323, loss = 8.74237385\n",
      "Iteration 11324, loss = 8.67977595\n",
      "Iteration 11325, loss = 8.66577322\n",
      "Iteration 11326, loss = 8.80108735\n",
      "Iteration 11327, loss = 8.67753146\n",
      "Iteration 11328, loss = 8.99970847\n",
      "Iteration 11329, loss = 9.06658732\n",
      "Iteration 11330, loss = 8.65646777\n",
      "Iteration 11331, loss = 9.27912688\n",
      "Iteration 11332, loss = 9.58025408\n",
      "Iteration 11333, loss = 8.44384317\n",
      "Iteration 11334, loss = 8.33044783\n",
      "Iteration 11335, loss = 8.31214457\n",
      "Iteration 11336, loss = 8.95994934\n",
      "Iteration 11337, loss = 9.18580573\n",
      "Iteration 11338, loss = 9.85308029\n",
      "Iteration 11339, loss = 10.03657790\n",
      "Iteration 11340, loss = 9.56718428\n",
      "Iteration 11341, loss = 10.11077086\n",
      "Iteration 11342, loss = 9.73346529\n",
      "Iteration 11343, loss = 9.28252448\n",
      "Iteration 11344, loss = 9.31522664\n",
      "Iteration 11345, loss = 8.55031613\n",
      "Iteration 11346, loss = 8.66492824\n",
      "Iteration 11347, loss = 8.68467458\n",
      "Iteration 11348, loss = 8.40457272\n",
      "Iteration 11349, loss = 8.65982042\n",
      "Iteration 11350, loss = 8.29108277\n",
      "Iteration 11351, loss = 8.62369003\n",
      "Iteration 11352, loss = 8.46085973\n",
      "Iteration 11353, loss = 8.28768145\n",
      "Iteration 11354, loss = 8.66026696\n",
      "Iteration 11355, loss = 8.96439756\n",
      "Iteration 11356, loss = 8.72530669\n",
      "Iteration 11357, loss = 8.46955209\n",
      "Iteration 11358, loss = 8.50095596\n",
      "Iteration 11359, loss = 8.93046723\n",
      "Iteration 11360, loss = 8.32104403\n",
      "Iteration 11361, loss = 8.63190125\n",
      "Iteration 11362, loss = 8.22046326\n",
      "Iteration 11363, loss = 8.31546048\n",
      "Iteration 11364, loss = 8.29705276\n",
      "Iteration 11365, loss = 8.64884529\n",
      "Iteration 11366, loss = 9.69529352\n",
      "Iteration 11367, loss = 8.61696176\n",
      "Iteration 11368, loss = 8.82037510\n",
      "Iteration 11369, loss = 9.00858976\n",
      "Iteration 11370, loss = 8.24627458\n",
      "Iteration 11371, loss = 8.58623337\n",
      "Iteration 11372, loss = 8.17167658\n",
      "Iteration 11373, loss = 8.50841010\n",
      "Iteration 11374, loss = 8.15645469\n",
      "Iteration 11375, loss = 9.00329804\n",
      "Iteration 11376, loss = 8.50634432\n",
      "Iteration 11377, loss = 8.43320887\n",
      "Iteration 11378, loss = 8.67508048\n",
      "Iteration 11379, loss = 8.58229400\n",
      "Iteration 11380, loss = 9.15261062\n",
      "Iteration 11381, loss = 8.80157649\n",
      "Iteration 11382, loss = 8.76062045\n",
      "Iteration 11383, loss = 8.74275637\n",
      "Iteration 11384, loss = 8.78145504\n",
      "Iteration 11385, loss = 8.78548557\n",
      "Iteration 11386, loss = 9.10176872\n",
      "Iteration 11387, loss = 9.09948212\n",
      "Iteration 11388, loss = 8.64008826\n",
      "Iteration 11389, loss = 8.67313625\n",
      "Iteration 11390, loss = 8.92406524\n",
      "Iteration 11391, loss = 8.22285559\n",
      "Iteration 11392, loss = 8.38053599\n",
      "Iteration 11393, loss = 9.10169462\n",
      "Iteration 11394, loss = 8.22157299\n",
      "Iteration 11395, loss = 8.81510003\n",
      "Iteration 11396, loss = 8.48124687\n",
      "Iteration 11397, loss = 8.66116599\n",
      "Iteration 11398, loss = 8.43592733\n",
      "Iteration 11399, loss = 8.47048856\n",
      "Iteration 11400, loss = 9.08866859\n",
      "Iteration 11401, loss = 8.56915876\n",
      "Iteration 11402, loss = 8.66169378\n",
      "Iteration 11403, loss = 9.73210921\n",
      "Iteration 11404, loss = 8.42489829\n",
      "Iteration 11405, loss = 8.14695075\n",
      "Iteration 11406, loss = 8.67918952\n",
      "Iteration 11407, loss = 8.69591010\n",
      "Iteration 11408, loss = 8.95498389\n",
      "Iteration 11409, loss = 8.93356307\n",
      "Iteration 11410, loss = 8.88588317\n",
      "Iteration 11411, loss = 8.70730460\n",
      "Iteration 11412, loss = 8.54874398\n",
      "Iteration 11413, loss = 8.73753927\n",
      "Iteration 11414, loss = 8.23341532\n",
      "Iteration 11415, loss = 8.20625796\n",
      "Iteration 11416, loss = 8.59843906\n",
      "Iteration 11417, loss = 8.76102599\n",
      "Iteration 11418, loss = 8.95662701\n",
      "Iteration 11419, loss = 8.55075072\n",
      "Iteration 11420, loss = 8.61442895\n",
      "Iteration 11421, loss = 8.75748581\n",
      "Iteration 11422, loss = 8.54523224\n",
      "Iteration 11423, loss = 8.28188129\n",
      "Iteration 11424, loss = 8.13680781\n",
      "Iteration 11425, loss = 8.80692272\n",
      "Iteration 11426, loss = 8.43841315\n",
      "Iteration 11427, loss = 8.67357472\n",
      "Iteration 11428, loss = 8.46951168\n",
      "Iteration 11429, loss = 8.50438460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11430, loss = 8.09234149\n",
      "Iteration 11431, loss = 8.28904793\n",
      "Iteration 11432, loss = 8.94022874\n",
      "Iteration 11433, loss = 9.30113330\n",
      "Iteration 11434, loss = 9.81319783\n",
      "Iteration 11435, loss = 9.38199216\n",
      "Iteration 11436, loss = 8.84472527\n",
      "Iteration 11437, loss = 8.68507518\n",
      "Iteration 11438, loss = 8.36849835\n",
      "Iteration 11439, loss = 8.16381153\n",
      "Iteration 11440, loss = 8.56003447\n",
      "Iteration 11441, loss = 8.86217261\n",
      "Iteration 11442, loss = 8.90166388\n",
      "Iteration 11443, loss = 8.50666058\n",
      "Iteration 11444, loss = 8.66193833\n",
      "Iteration 11445, loss = 8.29723186\n",
      "Iteration 11446, loss = 8.45905889\n",
      "Iteration 11447, loss = 8.11224236\n",
      "Iteration 11448, loss = 8.44306370\n",
      "Iteration 11449, loss = 8.47005974\n",
      "Iteration 11450, loss = 8.86783557\n",
      "Iteration 11451, loss = 9.13113688\n",
      "Iteration 11452, loss = 8.52200181\n",
      "Iteration 11453, loss = 8.53249133\n",
      "Iteration 11454, loss = 8.40670351\n",
      "Iteration 11455, loss = 8.66352303\n",
      "Iteration 11456, loss = 9.06853646\n",
      "Iteration 11457, loss = 8.72470391\n",
      "Iteration 11458, loss = 8.38338691\n",
      "Iteration 11459, loss = 8.49495951\n",
      "Iteration 11460, loss = 8.44011679\n",
      "Iteration 11461, loss = 8.39771748\n",
      "Iteration 11462, loss = 8.73643390\n",
      "Iteration 11463, loss = 8.83091068\n",
      "Iteration 11464, loss = 8.17075836\n",
      "Iteration 11465, loss = 8.34208533\n",
      "Iteration 11466, loss = 8.12665338\n",
      "Iteration 11467, loss = 8.63534570\n",
      "Iteration 11468, loss = 8.57147829\n",
      "Iteration 11469, loss = 8.18978410\n",
      "Iteration 11470, loss = 8.21226059\n",
      "Iteration 11471, loss = 8.31601945\n",
      "Iteration 11472, loss = 8.81215051\n",
      "Iteration 11473, loss = 8.90240685\n",
      "Iteration 11474, loss = 8.89735156\n",
      "Iteration 11475, loss = 9.46225053\n",
      "Iteration 11476, loss = 9.93129090\n",
      "Iteration 11477, loss = 10.37640457\n",
      "Iteration 11478, loss = 9.23300603\n",
      "Iteration 11479, loss = 9.15905941\n",
      "Iteration 11480, loss = 8.75656374\n",
      "Iteration 11481, loss = 8.22656123\n",
      "Iteration 11482, loss = 8.67261635\n",
      "Iteration 11483, loss = 8.27950159\n",
      "Iteration 11484, loss = 8.75799327\n",
      "Iteration 11485, loss = 9.00191957\n",
      "Iteration 11486, loss = 10.50594860\n",
      "Iteration 11487, loss = 9.82139829\n",
      "Iteration 11488, loss = 10.23897409\n",
      "Iteration 11489, loss = 8.95002555\n",
      "Iteration 11490, loss = 9.84218264\n",
      "Iteration 11491, loss = 8.76013435\n",
      "Iteration 11492, loss = 9.24400245\n",
      "Iteration 11493, loss = 8.54307453\n",
      "Iteration 11494, loss = 7.98870447\n",
      "Iteration 11495, loss = 8.11256998\n",
      "Iteration 11496, loss = 8.13167518\n",
      "Iteration 11497, loss = 8.19595886\n",
      "Iteration 11498, loss = 8.19061966\n",
      "Iteration 11499, loss = 8.35249499\n",
      "Iteration 11500, loss = 8.01653257\n",
      "Iteration 11501, loss = 8.10138084\n",
      "Iteration 11502, loss = 8.88487613\n",
      "Iteration 11503, loss = 8.67609095\n",
      "Iteration 11504, loss = 8.12418101\n",
      "Iteration 11505, loss = 8.08807876\n",
      "Iteration 11506, loss = 8.05089063\n",
      "Iteration 11507, loss = 8.04334572\n",
      "Iteration 11508, loss = 8.01682283\n",
      "Iteration 11509, loss = 8.12632245\n",
      "Iteration 11510, loss = 8.70997107\n",
      "Iteration 11511, loss = 8.80086254\n",
      "Iteration 11512, loss = 8.88929142\n",
      "Iteration 11513, loss = 8.39097820\n",
      "Iteration 11514, loss = 8.59772422\n",
      "Iteration 11515, loss = 8.43304993\n",
      "Iteration 11516, loss = 8.19019638\n",
      "Iteration 11517, loss = 8.75671054\n",
      "Iteration 11518, loss = 8.34274925\n",
      "Iteration 11519, loss = 8.34903840\n",
      "Iteration 11520, loss = 8.83513593\n",
      "Iteration 11521, loss = 8.07197054\n",
      "Iteration 11522, loss = 9.01625561\n",
      "Iteration 11523, loss = 8.97358828\n",
      "Iteration 11524, loss = 9.47295309\n",
      "Iteration 11525, loss = 9.09341666\n",
      "Iteration 11526, loss = 9.33156461\n",
      "Iteration 11527, loss = 9.24816657\n",
      "Iteration 11528, loss = 8.87451807\n",
      "Iteration 11529, loss = 9.08488629\n",
      "Iteration 11530, loss = 8.87969654\n",
      "Iteration 11531, loss = 9.28347897\n",
      "Iteration 11532, loss = 8.66502509\n",
      "Iteration 11533, loss = 8.40915226\n",
      "Iteration 11534, loss = 8.10439144\n",
      "Iteration 11535, loss = 8.08990817\n",
      "Iteration 11536, loss = 8.02936144\n",
      "Iteration 11537, loss = 8.32907714\n",
      "Iteration 11538, loss = 8.31929152\n",
      "Iteration 11539, loss = 8.05577916\n",
      "Iteration 11540, loss = 8.58967657\n",
      "Iteration 11541, loss = 9.11184132\n",
      "Iteration 11542, loss = 9.01991416\n",
      "Iteration 11543, loss = 8.15818600\n",
      "Iteration 11544, loss = 8.03963181\n",
      "Iteration 11545, loss = 8.19376356\n",
      "Iteration 11546, loss = 8.58616599\n",
      "Iteration 11547, loss = 8.43632559\n",
      "Iteration 11548, loss = 8.32703530\n",
      "Iteration 11549, loss = 9.69320328\n",
      "Iteration 11550, loss = 8.21984793\n",
      "Iteration 11551, loss = 8.60009255\n",
      "Iteration 11552, loss = 8.63432739\n",
      "Iteration 11553, loss = 8.27604855\n",
      "Iteration 11554, loss = 8.60495798\n",
      "Iteration 11555, loss = 8.49879845\n",
      "Iteration 11556, loss = 8.54257675\n",
      "Iteration 11557, loss = 8.29327816\n",
      "Iteration 11558, loss = 8.88111873\n",
      "Iteration 11559, loss = 8.90809925\n",
      "Iteration 11560, loss = 9.05336508\n",
      "Iteration 11561, loss = 9.41108449\n",
      "Iteration 11562, loss = 8.98994461\n",
      "Iteration 11563, loss = 8.67035133\n",
      "Iteration 11564, loss = 9.06978250\n",
      "Iteration 11565, loss = 8.70233944\n",
      "Iteration 11566, loss = 8.69297589\n",
      "Iteration 11567, loss = 8.44072020\n",
      "Iteration 11568, loss = 9.07064691\n",
      "Iteration 11569, loss = 9.30324820\n",
      "Iteration 11570, loss = 8.95794537\n",
      "Iteration 11571, loss = 9.43056721\n",
      "Iteration 11572, loss = 8.01357124\n",
      "Iteration 11573, loss = 8.36307415\n",
      "Iteration 11574, loss = 9.35951370\n",
      "Iteration 11575, loss = 8.44089545\n",
      "Iteration 11576, loss = 9.17527717\n",
      "Iteration 11577, loss = 9.43708543\n",
      "Iteration 11578, loss = 8.33809518\n",
      "Iteration 11579, loss = 9.61053461\n",
      "Iteration 11580, loss = 8.93423049\n",
      "Iteration 11581, loss = 8.72513188\n",
      "Iteration 11582, loss = 9.50305610\n",
      "Iteration 11583, loss = 8.24898210\n",
      "Iteration 11584, loss = 8.64639696\n",
      "Iteration 11585, loss = 8.98827231\n",
      "Iteration 11586, loss = 8.72126563\n",
      "Iteration 11587, loss = 8.32688474\n",
      "Iteration 11588, loss = 8.57920863\n",
      "Iteration 11589, loss = 8.94394366\n",
      "Iteration 11590, loss = 9.68082397\n",
      "Iteration 11591, loss = 9.18754332\n",
      "Iteration 11592, loss = 8.64326267\n",
      "Iteration 11593, loss = 8.55800823\n",
      "Iteration 11594, loss = 8.44202714\n",
      "Iteration 11595, loss = 8.14541870\n",
      "Iteration 11596, loss = 8.15102531\n",
      "Iteration 11597, loss = 8.24274990\n",
      "Iteration 11598, loss = 8.08764069\n",
      "Iteration 11599, loss = 8.30048592\n",
      "Iteration 11600, loss = 8.11873831\n",
      "Iteration 11601, loss = 9.07929588\n",
      "Iteration 11602, loss = 7.97433699\n",
      "Iteration 11603, loss = 8.79072567\n",
      "Iteration 11604, loss = 8.26791289\n",
      "Iteration 11605, loss = 8.79610749\n",
      "Iteration 11606, loss = 8.83966065\n",
      "Iteration 11607, loss = 8.91864475\n",
      "Iteration 11608, loss = 8.74402848\n",
      "Iteration 11609, loss = 8.53365985\n",
      "Iteration 11610, loss = 8.73455353\n",
      "Iteration 11611, loss = 8.74854014\n",
      "Iteration 11612, loss = 8.68593859\n",
      "Iteration 11613, loss = 9.33547496\n",
      "Iteration 11614, loss = 9.58365646\n",
      "Iteration 11615, loss = 9.32680281\n",
      "Iteration 11616, loss = 9.21983836\n",
      "Iteration 11617, loss = 9.58359418\n",
      "Iteration 11618, loss = 8.62290971\n",
      "Iteration 11619, loss = 9.01420725\n",
      "Iteration 11620, loss = 9.45945809\n",
      "Iteration 11621, loss = 9.76576759\n",
      "Iteration 11622, loss = 9.95642096\n",
      "Iteration 11623, loss = 9.68505448\n",
      "Iteration 11624, loss = 8.69337953\n",
      "Iteration 11625, loss = 7.96298626\n",
      "Iteration 11626, loss = 8.05425333\n",
      "Iteration 11627, loss = 8.76672584\n",
      "Iteration 11628, loss = 9.33034362\n",
      "Iteration 11629, loss = 8.98263238\n",
      "Iteration 11630, loss = 8.73843811\n",
      "Iteration 11631, loss = 9.11847731\n",
      "Iteration 11632, loss = 8.57325056\n",
      "Iteration 11633, loss = 8.50090885\n",
      "Iteration 11634, loss = 8.46473380\n",
      "Iteration 11635, loss = 8.43443953\n",
      "Iteration 11636, loss = 9.32919178\n",
      "Iteration 11637, loss = 8.23203011\n",
      "Iteration 11638, loss = 8.96577997\n",
      "Iteration 11639, loss = 8.10055578\n",
      "Iteration 11640, loss = 8.11638628\n",
      "Iteration 11641, loss = 8.16236841\n",
      "Iteration 11642, loss = 8.04029185\n",
      "Iteration 11643, loss = 8.35376025\n",
      "Iteration 11644, loss = 8.59163881\n",
      "Iteration 11645, loss = 8.35562640\n",
      "Iteration 11646, loss = 8.64430544\n",
      "Iteration 11647, loss = 8.61022143\n",
      "Iteration 11648, loss = 8.40339104\n",
      "Iteration 11649, loss = 8.10617892\n",
      "Iteration 11650, loss = 8.02131018\n",
      "Iteration 11651, loss = 8.11582341\n",
      "Iteration 11652, loss = 8.11479106\n",
      "Iteration 11653, loss = 8.37414441\n",
      "Iteration 11654, loss = 8.49221902\n",
      "Iteration 11655, loss = 8.63828622\n",
      "Iteration 11656, loss = 9.48476809\n",
      "Iteration 11657, loss = 8.40688179\n",
      "Iteration 11658, loss = 8.63015090\n",
      "Iteration 11659, loss = 8.23121269\n",
      "Iteration 11660, loss = 8.07061469\n",
      "Iteration 11661, loss = 7.97207593\n",
      "Iteration 11662, loss = 7.94656385\n",
      "Iteration 11663, loss = 8.75760743\n",
      "Iteration 11664, loss = 8.24038032\n",
      "Iteration 11665, loss = 8.23417043\n",
      "Iteration 11666, loss = 8.21878604\n",
      "Iteration 11667, loss = 8.34287591\n",
      "Iteration 11668, loss = 8.09239318\n",
      "Iteration 11669, loss = 8.40521237\n",
      "Iteration 11670, loss = 8.03839767\n",
      "Iteration 11671, loss = 8.19653792\n",
      "Iteration 11672, loss = 8.68086052\n",
      "Iteration 11673, loss = 8.49057855\n",
      "Iteration 11674, loss = 9.51712803\n",
      "Iteration 11675, loss = 8.20105904\n",
      "Iteration 11676, loss = 8.77097958\n",
      "Iteration 11677, loss = 8.23738281\n",
      "Iteration 11678, loss = 8.28003358\n",
      "Iteration 11679, loss = 9.33199387\n",
      "Iteration 11680, loss = 8.23622739\n",
      "Iteration 11681, loss = 8.56726939\n",
      "Iteration 11682, loss = 8.14219498\n",
      "Iteration 11683, loss = 8.21240936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11684, loss = 8.22013498\n",
      "Iteration 11685, loss = 8.54450201\n",
      "Iteration 11686, loss = 8.52494164\n",
      "Iteration 11687, loss = 9.02234701\n",
      "Iteration 11688, loss = 8.49768802\n",
      "Iteration 11689, loss = 8.02558316\n",
      "Iteration 11690, loss = 8.08696950\n",
      "Iteration 11691, loss = 8.36003861\n",
      "Iteration 11692, loss = 8.55120510\n",
      "Iteration 11693, loss = 8.91031599\n",
      "Iteration 11694, loss = 9.00837671\n",
      "Iteration 11695, loss = 8.00264111\n",
      "Iteration 11696, loss = 8.42784289\n",
      "Iteration 11697, loss = 8.07586371\n",
      "Iteration 11698, loss = 8.52331430\n",
      "Iteration 11699, loss = 9.67687888\n",
      "Iteration 11700, loss = 10.20399106\n",
      "Iteration 11701, loss = 10.46312035\n",
      "Iteration 11702, loss = 9.38495723\n",
      "Iteration 11703, loss = 10.05416233\n",
      "Iteration 11704, loss = 10.14481047\n",
      "Iteration 11705, loss = 9.87120421\n",
      "Iteration 11706, loss = 9.06161776\n",
      "Iteration 11707, loss = 8.30763377\n",
      "Iteration 11708, loss = 9.12644581\n",
      "Iteration 11709, loss = 9.95105018\n",
      "Iteration 11710, loss = 8.79788792\n",
      "Iteration 11711, loss = 8.67442100\n",
      "Iteration 11712, loss = 8.18383029\n",
      "Iteration 11713, loss = 8.37865942\n",
      "Iteration 11714, loss = 9.30536986\n",
      "Iteration 11715, loss = 9.13765769\n",
      "Iteration 11716, loss = 8.71349008\n",
      "Iteration 11717, loss = 9.70493859\n",
      "Iteration 11718, loss = 10.31110288\n",
      "Iteration 11719, loss = 11.66964618\n",
      "Iteration 11720, loss = 9.77668060\n",
      "Iteration 11721, loss = 9.83626628\n",
      "Iteration 11722, loss = 10.10134792\n",
      "Iteration 11723, loss = 10.37041936\n",
      "Iteration 11724, loss = 10.46399580\n",
      "Iteration 11725, loss = 9.13202903\n",
      "Iteration 11726, loss = 9.29839906\n",
      "Iteration 11727, loss = 8.51702102\n",
      "Iteration 11728, loss = 8.33643783\n",
      "Iteration 11729, loss = 8.05067694\n",
      "Iteration 11730, loss = 8.54615126\n",
      "Iteration 11731, loss = 8.32201955\n",
      "Iteration 11732, loss = 8.74673544\n",
      "Iteration 11733, loss = 9.51731782\n",
      "Iteration 11734, loss = 8.59481606\n",
      "Iteration 11735, loss = 8.64840830\n",
      "Iteration 11736, loss = 8.33182521\n",
      "Iteration 11737, loss = 8.25718139\n",
      "Iteration 11738, loss = 8.45445716\n",
      "Iteration 11739, loss = 8.81076601\n",
      "Iteration 11740, loss = 8.79689477\n",
      "Iteration 11741, loss = 8.52037095\n",
      "Iteration 11742, loss = 8.20331503\n",
      "Iteration 11743, loss = 8.96214044\n",
      "Iteration 11744, loss = 8.65415524\n",
      "Iteration 11745, loss = 8.22398346\n",
      "Iteration 11746, loss = 8.45829921\n",
      "Iteration 11747, loss = 8.22357008\n",
      "Iteration 11748, loss = 8.58821505\n",
      "Iteration 11749, loss = 8.20644171\n",
      "Iteration 11750, loss = 8.76297107\n",
      "Iteration 11751, loss = 9.17306426\n",
      "Iteration 11752, loss = 8.57453867\n",
      "Iteration 11753, loss = 8.77994762\n",
      "Iteration 11754, loss = 8.54011855\n",
      "Iteration 11755, loss = 8.39505298\n",
      "Iteration 11756, loss = 8.09646006\n",
      "Iteration 11757, loss = 8.49867824\n",
      "Iteration 11758, loss = 8.40720085\n",
      "Iteration 11759, loss = 8.32194652\n",
      "Iteration 11760, loss = 8.03912205\n",
      "Iteration 11761, loss = 8.10474067\n",
      "Iteration 11762, loss = 8.01721659\n",
      "Iteration 11763, loss = 8.16283042\n",
      "Iteration 11764, loss = 8.05047674\n",
      "Iteration 11765, loss = 8.11681355\n",
      "Iteration 11766, loss = 8.05799987\n",
      "Iteration 11767, loss = 8.08024819\n",
      "Iteration 11768, loss = 8.53949375\n",
      "Iteration 11769, loss = 8.40346831\n",
      "Iteration 11770, loss = 8.66563284\n",
      "Iteration 11771, loss = 8.58763792\n",
      "Iteration 11772, loss = 8.20488294\n",
      "Iteration 11773, loss = 8.06572438\n",
      "Iteration 11774, loss = 8.65372013\n",
      "Iteration 11775, loss = 9.20418339\n",
      "Iteration 11776, loss = 9.31595895\n",
      "Iteration 11777, loss = 8.15945034\n",
      "Iteration 11778, loss = 8.30919662\n",
      "Iteration 11779, loss = 8.43413862\n",
      "Iteration 11780, loss = 8.02228317\n",
      "Iteration 11781, loss = 7.92513438\n",
      "Iteration 11782, loss = 7.99141562\n",
      "Iteration 11783, loss = 7.84089112\n",
      "Iteration 11784, loss = 7.99614436\n",
      "Iteration 11785, loss = 8.33081419\n",
      "Iteration 11786, loss = 8.45608247\n",
      "Iteration 11787, loss = 8.07087845\n",
      "Iteration 11788, loss = 8.11372154\n",
      "Iteration 11789, loss = 8.06599199\n",
      "Iteration 11790, loss = 8.03950415\n",
      "Iteration 11791, loss = 8.74939577\n",
      "Iteration 11792, loss = 9.35468114\n",
      "Iteration 11793, loss = 9.10380965\n",
      "Iteration 11794, loss = 9.17084774\n",
      "Iteration 11795, loss = 8.47778080\n",
      "Iteration 11796, loss = 8.81396803\n",
      "Iteration 11797, loss = 8.61653898\n",
      "Iteration 11798, loss = 7.96285245\n",
      "Iteration 11799, loss = 9.19821605\n",
      "Iteration 11800, loss = 9.21953410\n",
      "Iteration 11801, loss = 8.17850643\n",
      "Iteration 11802, loss = 8.36959855\n",
      "Iteration 11803, loss = 8.02460653\n",
      "Iteration 11804, loss = 8.32907558\n",
      "Iteration 11805, loss = 8.16335304\n",
      "Iteration 11806, loss = 8.46291151\n",
      "Iteration 11807, loss = 8.56805765\n",
      "Iteration 11808, loss = 8.99697618\n",
      "Iteration 11809, loss = 8.58824835\n",
      "Iteration 11810, loss = 8.43376461\n",
      "Iteration 11811, loss = 8.12900819\n",
      "Iteration 11812, loss = 8.58776092\n",
      "Iteration 11813, loss = 8.76908179\n",
      "Iteration 11814, loss = 8.49104853\n",
      "Iteration 11815, loss = 8.00671308\n",
      "Iteration 11816, loss = 8.07284441\n",
      "Iteration 11817, loss = 7.87064734\n",
      "Iteration 11818, loss = 7.96445175\n",
      "Iteration 11819, loss = 7.74248874\n",
      "Iteration 11820, loss = 7.93427584\n",
      "Iteration 11821, loss = 7.91812783\n",
      "Iteration 11822, loss = 8.03080085\n",
      "Iteration 11823, loss = 8.64698231\n",
      "Iteration 11824, loss = 9.29173188\n",
      "Iteration 11825, loss = 9.10987515\n",
      "Iteration 11826, loss = 9.08617147\n",
      "Iteration 11827, loss = 8.46034798\n",
      "Iteration 11828, loss = 8.46732379\n",
      "Iteration 11829, loss = 8.26111902\n",
      "Iteration 11830, loss = 8.26748860\n",
      "Iteration 11831, loss = 8.48575568\n",
      "Iteration 11832, loss = 8.17523116\n",
      "Iteration 11833, loss = 8.74602451\n",
      "Iteration 11834, loss = 7.96406726\n",
      "Iteration 11835, loss = 8.12017994\n",
      "Iteration 11836, loss = 8.56743929\n",
      "Iteration 11837, loss = 7.79332115\n",
      "Iteration 11838, loss = 8.05261501\n",
      "Iteration 11839, loss = 8.38083808\n",
      "Iteration 11840, loss = 8.09526470\n",
      "Iteration 11841, loss = 8.04286973\n",
      "Iteration 11842, loss = 7.87575599\n",
      "Iteration 11843, loss = 8.38370168\n",
      "Iteration 11844, loss = 8.19171523\n",
      "Iteration 11845, loss = 8.31643306\n",
      "Iteration 11846, loss = 8.13306299\n",
      "Iteration 11847, loss = 8.45412448\n",
      "Iteration 11848, loss = 8.94751663\n",
      "Iteration 11849, loss = 8.32009606\n",
      "Iteration 11850, loss = 8.51409699\n",
      "Iteration 11851, loss = 8.34926628\n",
      "Iteration 11852, loss = 9.05876478\n",
      "Iteration 11853, loss = 9.28075328\n",
      "Iteration 11854, loss = 9.93797056\n",
      "Iteration 11855, loss = 10.05971752\n",
      "Iteration 11856, loss = 10.41697083\n",
      "Iteration 11857, loss = 10.04398328\n",
      "Iteration 11858, loss = 10.45234921\n",
      "Iteration 11859, loss = 9.81573099\n",
      "Iteration 11860, loss = 9.24921717\n",
      "Iteration 11861, loss = 8.41783796\n",
      "Iteration 11862, loss = 9.35180312\n",
      "Iteration 11863, loss = 8.60716238\n",
      "Iteration 11864, loss = 8.10539618\n",
      "Iteration 11865, loss = 8.79082360\n",
      "Iteration 11866, loss = 8.03154253\n",
      "Iteration 11867, loss = 7.73837505\n",
      "Iteration 11868, loss = 7.86527937\n",
      "Iteration 11869, loss = 7.70924556\n",
      "Iteration 11870, loss = 7.90841773\n",
      "Iteration 11871, loss = 8.18053500\n",
      "Iteration 11872, loss = 7.77433366\n",
      "Iteration 11873, loss = 7.84856432\n",
      "Iteration 11874, loss = 8.07092241\n",
      "Iteration 11875, loss = 8.04597220\n",
      "Iteration 11876, loss = 8.18027659\n",
      "Iteration 11877, loss = 8.50292517\n",
      "Iteration 11878, loss = 8.68065937\n",
      "Iteration 11879, loss = 8.98740052\n",
      "Iteration 11880, loss = 7.91310772\n",
      "Iteration 11881, loss = 8.77003025\n",
      "Iteration 11882, loss = 9.02074762\n",
      "Iteration 11883, loss = 8.65731970\n",
      "Iteration 11884, loss = 9.20672992\n",
      "Iteration 11885, loss = 8.75461886\n",
      "Iteration 11886, loss = 8.90844987\n",
      "Iteration 11887, loss = 8.42260455\n",
      "Iteration 11888, loss = 8.61980179\n",
      "Iteration 11889, loss = 9.16245451\n",
      "Iteration 11890, loss = 8.69362667\n",
      "Iteration 11891, loss = 9.19113151\n",
      "Iteration 11892, loss = 8.27682708\n",
      "Iteration 11893, loss = 8.46111466\n",
      "Iteration 11894, loss = 8.05636254\n",
      "Iteration 11895, loss = 8.37033698\n",
      "Iteration 11896, loss = 7.99002576\n",
      "Iteration 11897, loss = 8.58271116\n",
      "Iteration 11898, loss = 8.78000188\n",
      "Iteration 11899, loss = 8.29768863\n",
      "Iteration 11900, loss = 8.24065282\n",
      "Iteration 11901, loss = 8.23742510\n",
      "Iteration 11902, loss = 7.98537692\n",
      "Iteration 11903, loss = 7.81220361\n",
      "Iteration 11904, loss = 8.34772733\n",
      "Iteration 11905, loss = 8.58952597\n",
      "Iteration 11906, loss = 8.79335804\n",
      "Iteration 11907, loss = 8.65390578\n",
      "Iteration 11908, loss = 8.48697491\n",
      "Iteration 11909, loss = 8.26907885\n",
      "Iteration 11910, loss = 8.01429678\n",
      "Iteration 11911, loss = 7.85551028\n",
      "Iteration 11912, loss = 7.86054828\n",
      "Iteration 11913, loss = 7.98259733\n",
      "Iteration 11914, loss = 7.93699808\n",
      "Iteration 11915, loss = 7.87737401\n",
      "Iteration 11916, loss = 7.89605773\n",
      "Iteration 11917, loss = 7.88171464\n",
      "Iteration 11918, loss = 8.29917648\n",
      "Iteration 11919, loss = 8.46165751\n",
      "Iteration 11920, loss = 8.07200262\n",
      "Iteration 11921, loss = 7.83529317\n",
      "Iteration 11922, loss = 8.10393887\n",
      "Iteration 11923, loss = 8.43308555\n",
      "Iteration 11924, loss = 8.43341508\n",
      "Iteration 11925, loss = 7.86173342\n",
      "Iteration 11926, loss = 8.30471738\n",
      "Iteration 11927, loss = 8.10457455\n",
      "Iteration 11928, loss = 7.87861557\n",
      "Iteration 11929, loss = 8.25881807\n",
      "Iteration 11930, loss = 8.08209629\n",
      "Iteration 11931, loss = 7.96916754\n",
      "Iteration 11932, loss = 8.24410417\n",
      "Iteration 11933, loss = 7.90091996\n",
      "Iteration 11934, loss = 8.72537467\n",
      "Iteration 11935, loss = 8.77161476\n",
      "Iteration 11936, loss = 8.61304736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11937, loss = 9.28488074\n",
      "Iteration 11938, loss = 9.59768127\n",
      "Iteration 11939, loss = 8.97302774\n",
      "Iteration 11940, loss = 9.20536224\n",
      "Iteration 11941, loss = 9.52014894\n",
      "Iteration 11942, loss = 8.63123188\n",
      "Iteration 11943, loss = 8.12198696\n",
      "Iteration 11944, loss = 7.86573081\n",
      "Iteration 11945, loss = 8.03773871\n",
      "Iteration 11946, loss = 8.22799230\n",
      "Iteration 11947, loss = 7.91960270\n",
      "Iteration 11948, loss = 8.10446690\n",
      "Iteration 11949, loss = 8.38811845\n",
      "Iteration 11950, loss = 8.84948365\n",
      "Iteration 11951, loss = 8.65833068\n",
      "Iteration 11952, loss = 8.31848483\n",
      "Iteration 11953, loss = 8.37632711\n",
      "Iteration 11954, loss = 8.38619902\n",
      "Iteration 11955, loss = 8.33143925\n",
      "Iteration 11956, loss = 8.38265281\n",
      "Iteration 11957, loss = 8.00455754\n",
      "Iteration 11958, loss = 8.24985197\n",
      "Iteration 11959, loss = 7.91971664\n",
      "Iteration 11960, loss = 8.22113215\n",
      "Iteration 11961, loss = 8.55398673\n",
      "Iteration 11962, loss = 8.45381003\n",
      "Iteration 11963, loss = 7.80146297\n",
      "Iteration 11964, loss = 8.35038148\n",
      "Iteration 11965, loss = 8.69730824\n",
      "Iteration 11966, loss = 7.86265281\n",
      "Iteration 11967, loss = 8.01070938\n",
      "Iteration 11968, loss = 7.81489866\n",
      "Iteration 11969, loss = 8.28121602\n",
      "Iteration 11970, loss = 8.43583023\n",
      "Iteration 11971, loss = 9.10151263\n",
      "Iteration 11972, loss = 10.04627664\n",
      "Iteration 11973, loss = 9.48151048\n",
      "Iteration 11974, loss = 8.39401001\n",
      "Iteration 11975, loss = 8.09956153\n",
      "Iteration 11976, loss = 8.34999471\n",
      "Iteration 11977, loss = 8.33188733\n",
      "Iteration 11978, loss = 8.38202436\n",
      "Iteration 11979, loss = 9.14793213\n",
      "Iteration 11980, loss = 8.14540318\n",
      "Iteration 11981, loss = 8.51693954\n",
      "Iteration 11982, loss = 8.60333558\n",
      "Iteration 11983, loss = 8.59010480\n",
      "Iteration 11984, loss = 8.53625027\n",
      "Iteration 11985, loss = 8.51683547\n",
      "Iteration 11986, loss = 8.40338554\n",
      "Iteration 11987, loss = 8.30435554\n",
      "Iteration 11988, loss = 8.22768768\n",
      "Iteration 11989, loss = 8.78288494\n",
      "Iteration 11990, loss = 9.13769119\n",
      "Iteration 11991, loss = 8.78302164\n",
      "Iteration 11992, loss = 8.64919736\n",
      "Iteration 11993, loss = 7.90514065\n",
      "Iteration 11994, loss = 7.90419266\n",
      "Iteration 11995, loss = 9.17469164\n",
      "Iteration 11996, loss = 8.81220580\n",
      "Iteration 11997, loss = 8.35458868\n",
      "Iteration 11998, loss = 8.07719935\n",
      "Iteration 11999, loss = 8.32741067\n",
      "Iteration 12000, loss = 8.14027671\n",
      "Iteration 12001, loss = 8.11168267\n",
      "Iteration 12002, loss = 7.96983182\n",
      "Iteration 12003, loss = 8.16567648\n",
      "Iteration 12004, loss = 9.69755698\n",
      "Iteration 12005, loss = 9.06164655\n",
      "Iteration 12006, loss = 10.12335229\n",
      "Iteration 12007, loss = 9.59619920\n",
      "Iteration 12008, loss = 9.13816202\n",
      "Iteration 12009, loss = 8.03191932\n",
      "Iteration 12010, loss = 8.10011675\n",
      "Iteration 12011, loss = 9.04574465\n",
      "Iteration 12012, loss = 8.34537955\n",
      "Iteration 12013, loss = 8.24005836\n",
      "Iteration 12014, loss = 9.05421645\n",
      "Iteration 12015, loss = 8.19030159\n",
      "Iteration 12016, loss = 8.02816796\n",
      "Iteration 12017, loss = 7.83720819\n",
      "Iteration 12018, loss = 7.95718978\n",
      "Iteration 12019, loss = 8.52547155\n",
      "Iteration 12020, loss = 8.11117372\n",
      "Iteration 12021, loss = 8.95892521\n",
      "Iteration 12022, loss = 9.47897574\n",
      "Iteration 12023, loss = 8.01220076\n",
      "Iteration 12024, loss = 9.29634806\n",
      "Iteration 12025, loss = 8.26362872\n",
      "Iteration 12026, loss = 8.54725187\n",
      "Iteration 12027, loss = 8.12369864\n",
      "Iteration 12028, loss = 8.37389248\n",
      "Iteration 12029, loss = 8.25641663\n",
      "Iteration 12030, loss = 7.76988021\n",
      "Iteration 12031, loss = 7.85483343\n",
      "Iteration 12032, loss = 8.30527851\n",
      "Iteration 12033, loss = 9.49896457\n",
      "Iteration 12034, loss = 9.34091734\n",
      "Iteration 12035, loss = 8.10841743\n",
      "Iteration 12036, loss = 8.42947736\n",
      "Iteration 12037, loss = 8.50106889\n",
      "Iteration 12038, loss = 8.22442725\n",
      "Iteration 12039, loss = 8.00145712\n",
      "Iteration 12040, loss = 8.59388915\n",
      "Iteration 12041, loss = 9.61027978\n",
      "Iteration 12042, loss = 8.88935472\n",
      "Iteration 12043, loss = 8.66715017\n",
      "Iteration 12044, loss = 8.14442026\n",
      "Iteration 12045, loss = 8.03898201\n",
      "Iteration 12046, loss = 8.04947908\n",
      "Iteration 12047, loss = 7.99732680\n",
      "Iteration 12048, loss = 7.76531420\n",
      "Iteration 12049, loss = 7.62860261\n",
      "Iteration 12050, loss = 7.87578162\n",
      "Iteration 12051, loss = 7.98667557\n",
      "Iteration 12052, loss = 7.95848002\n",
      "Iteration 12053, loss = 7.89961408\n",
      "Iteration 12054, loss = 8.06072985\n",
      "Iteration 12055, loss = 8.49663501\n",
      "Iteration 12056, loss = 9.01755995\n",
      "Iteration 12057, loss = 8.78910632\n",
      "Iteration 12058, loss = 8.04737339\n",
      "Iteration 12059, loss = 8.78921193\n",
      "Iteration 12060, loss = 8.38926345\n",
      "Iteration 12061, loss = 8.17334561\n",
      "Iteration 12062, loss = 8.20056002\n",
      "Iteration 12063, loss = 7.76791230\n",
      "Iteration 12064, loss = 8.11314546\n",
      "Iteration 12065, loss = 8.94799403\n",
      "Iteration 12066, loss = 8.35535869\n",
      "Iteration 12067, loss = 8.43642967\n",
      "Iteration 12068, loss = 8.59216935\n",
      "Iteration 12069, loss = 7.94452430\n",
      "Iteration 12070, loss = 7.97686830\n",
      "Iteration 12071, loss = 8.14003556\n",
      "Iteration 12072, loss = 8.48256962\n",
      "Iteration 12073, loss = 9.65433463\n",
      "Iteration 12074, loss = 8.88566032\n",
      "Iteration 12075, loss = 8.45984881\n",
      "Iteration 12076, loss = 7.98720573\n",
      "Iteration 12077, loss = 7.75284142\n",
      "Iteration 12078, loss = 8.07547627\n",
      "Iteration 12079, loss = 9.24794880\n",
      "Iteration 12080, loss = 8.12079031\n",
      "Iteration 12081, loss = 8.30822219\n",
      "Iteration 12082, loss = 7.99292308\n",
      "Iteration 12083, loss = 8.18465604\n",
      "Iteration 12084, loss = 8.10043112\n",
      "Iteration 12085, loss = 8.57791110\n",
      "Iteration 12086, loss = 8.69706984\n",
      "Iteration 12087, loss = 8.87725726\n",
      "Iteration 12088, loss = 8.92189349\n",
      "Iteration 12089, loss = 7.90312822\n",
      "Iteration 12090, loss = 7.67024128\n",
      "Iteration 12091, loss = 8.20775882\n",
      "Iteration 12092, loss = 8.94179185\n",
      "Iteration 12093, loss = 8.91318432\n",
      "Iteration 12094, loss = 8.19619751\n",
      "Iteration 12095, loss = 8.23203784\n",
      "Iteration 12096, loss = 8.36211317\n",
      "Iteration 12097, loss = 7.95790673\n",
      "Iteration 12098, loss = 8.58373085\n",
      "Iteration 12099, loss = 8.02990233\n",
      "Iteration 12100, loss = 8.05181998\n",
      "Iteration 12101, loss = 8.21109250\n",
      "Iteration 12102, loss = 8.18116651\n",
      "Iteration 12103, loss = 8.84321150\n",
      "Iteration 12104, loss = 8.05678899\n",
      "Iteration 12105, loss = 8.15842859\n",
      "Iteration 12106, loss = 8.31592759\n",
      "Iteration 12107, loss = 7.93240460\n",
      "Iteration 12108, loss = 8.12562134\n",
      "Iteration 12109, loss = 8.50091588\n",
      "Iteration 12110, loss = 8.70323944\n",
      "Iteration 12111, loss = 8.37800424\n",
      "Iteration 12112, loss = 8.60220317\n",
      "Iteration 12113, loss = 8.19570282\n",
      "Iteration 12114, loss = 7.79012783\n",
      "Iteration 12115, loss = 7.74739581\n",
      "Iteration 12116, loss = 8.41613101\n",
      "Iteration 12117, loss = 8.49911381\n",
      "Iteration 12118, loss = 8.06725960\n",
      "Iteration 12119, loss = 7.80474901\n",
      "Iteration 12120, loss = 8.27872540\n",
      "Iteration 12121, loss = 7.89259748\n",
      "Iteration 12122, loss = 7.73812083\n",
      "Iteration 12123, loss = 7.96144631\n",
      "Iteration 12124, loss = 7.82719007\n",
      "Iteration 12125, loss = 7.79533097\n",
      "Iteration 12126, loss = 8.13118931\n",
      "Iteration 12127, loss = 7.93296052\n",
      "Iteration 12128, loss = 7.86429773\n",
      "Iteration 12129, loss = 7.73268577\n",
      "Iteration 12130, loss = 8.07185285\n",
      "Iteration 12131, loss = 8.00074647\n",
      "Iteration 12132, loss = 8.35649295\n",
      "Iteration 12133, loss = 7.92000955\n",
      "Iteration 12134, loss = 8.17974050\n",
      "Iteration 12135, loss = 8.09809571\n",
      "Iteration 12136, loss = 8.29628543\n",
      "Iteration 12137, loss = 8.65369207\n",
      "Iteration 12138, loss = 8.70353610\n",
      "Iteration 12139, loss = 8.30680668\n",
      "Iteration 12140, loss = 8.28795250\n",
      "Iteration 12141, loss = 8.55894879\n",
      "Iteration 12142, loss = 7.69119003\n",
      "Iteration 12143, loss = 7.83624893\n",
      "Iteration 12144, loss = 8.13177211\n",
      "Iteration 12145, loss = 8.07990322\n",
      "Iteration 12146, loss = 7.91364571\n",
      "Iteration 12147, loss = 7.89879645\n",
      "Iteration 12148, loss = 8.07479629\n",
      "Iteration 12149, loss = 7.78765784\n",
      "Iteration 12150, loss = 8.64997575\n",
      "Iteration 12151, loss = 7.74803243\n",
      "Iteration 12152, loss = 7.88550463\n",
      "Iteration 12153, loss = 7.90274049\n",
      "Iteration 12154, loss = 8.66243072\n",
      "Iteration 12155, loss = 8.96215231\n",
      "Iteration 12156, loss = 8.64008835\n",
      "Iteration 12157, loss = 8.06640870\n",
      "Iteration 12158, loss = 8.07507968\n",
      "Iteration 12159, loss = 8.55781476\n",
      "Iteration 12160, loss = 8.62743938\n",
      "Iteration 12161, loss = 8.17838516\n",
      "Iteration 12162, loss = 8.29968654\n",
      "Iteration 12163, loss = 8.54593478\n",
      "Iteration 12164, loss = 7.93688708\n",
      "Iteration 12165, loss = 8.63039230\n",
      "Iteration 12166, loss = 8.31142262\n",
      "Iteration 12167, loss = 8.91406955\n",
      "Iteration 12168, loss = 9.86055003\n",
      "Iteration 12169, loss = 10.85217368\n",
      "Iteration 12170, loss = 9.92223737\n",
      "Iteration 12171, loss = 9.71632694\n",
      "Iteration 12172, loss = 10.55051100\n",
      "Iteration 12173, loss = 9.01228221\n",
      "Iteration 12174, loss = 9.35161322\n",
      "Iteration 12175, loss = 8.48464717\n",
      "Iteration 12176, loss = 9.22011966\n",
      "Iteration 12177, loss = 7.92251934\n",
      "Iteration 12178, loss = 8.61769955\n",
      "Iteration 12179, loss = 7.84846752\n",
      "Iteration 12180, loss = 8.15930416\n",
      "Iteration 12181, loss = 8.82736232\n",
      "Iteration 12182, loss = 8.15384734\n",
      "Iteration 12183, loss = 8.70308704\n",
      "Iteration 12184, loss = 8.02017709\n",
      "Iteration 12185, loss = 8.50236633\n",
      "Iteration 12186, loss = 7.96682976\n",
      "Iteration 12187, loss = 7.73137300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12188, loss = 7.57781211\n",
      "Iteration 12189, loss = 7.59744449\n",
      "Iteration 12190, loss = 7.76174119\n",
      "Iteration 12191, loss = 8.18356835\n",
      "Iteration 12192, loss = 8.57800909\n",
      "Iteration 12193, loss = 8.92746875\n",
      "Iteration 12194, loss = 8.43366364\n",
      "Iteration 12195, loss = 8.35465684\n",
      "Iteration 12196, loss = 8.29708543\n",
      "Iteration 12197, loss = 8.30862935\n",
      "Iteration 12198, loss = 8.80536728\n",
      "Iteration 12199, loss = 8.25469576\n",
      "Iteration 12200, loss = 8.00944067\n",
      "Iteration 12201, loss = 8.08531678\n",
      "Iteration 12202, loss = 9.03362467\n",
      "Iteration 12203, loss = 9.11771137\n",
      "Iteration 12204, loss = 9.20080513\n",
      "Iteration 12205, loss = 9.49229038\n",
      "Iteration 12206, loss = 8.86024762\n",
      "Iteration 12207, loss = 8.27395447\n",
      "Iteration 12208, loss = 7.79237862\n",
      "Iteration 12209, loss = 7.61983635\n",
      "Iteration 12210, loss = 8.41116493\n",
      "Iteration 12211, loss = 7.77274984\n",
      "Iteration 12212, loss = 8.32106943\n",
      "Iteration 12213, loss = 9.69930897\n",
      "Iteration 12214, loss = 8.75312594\n",
      "Iteration 12215, loss = 9.17053442\n",
      "Iteration 12216, loss = 8.57383330\n",
      "Iteration 12217, loss = 9.11656342\n",
      "Iteration 12218, loss = 9.04282581\n",
      "Iteration 12219, loss = 8.59096327\n",
      "Iteration 12220, loss = 9.45147001\n",
      "Iteration 12221, loss = 11.74440755\n",
      "Iteration 12222, loss = 11.34629921\n",
      "Iteration 12223, loss = 10.37711031\n",
      "Iteration 12224, loss = 9.42825736\n",
      "Iteration 12225, loss = 8.99448844\n",
      "Iteration 12226, loss = 8.28872723\n",
      "Iteration 12227, loss = 8.19168456\n",
      "Iteration 12228, loss = 7.77541930\n",
      "Iteration 12229, loss = 7.79211026\n",
      "Iteration 12230, loss = 7.71963869\n",
      "Iteration 12231, loss = 8.01223442\n",
      "Iteration 12232, loss = 8.16954123\n",
      "Iteration 12233, loss = 8.31795558\n",
      "Iteration 12234, loss = 8.32975027\n",
      "Iteration 12235, loss = 8.17080428\n",
      "Iteration 12236, loss = 7.85136927\n",
      "Iteration 12237, loss = 7.95992073\n",
      "Iteration 12238, loss = 8.10078157\n",
      "Iteration 12239, loss = 8.55039355\n",
      "Iteration 12240, loss = 9.06448668\n",
      "Iteration 12241, loss = 8.12696214\n",
      "Iteration 12242, loss = 7.83020009\n",
      "Iteration 12243, loss = 7.82439499\n",
      "Iteration 12244, loss = 7.60616561\n",
      "Iteration 12245, loss = 7.50593323\n",
      "Iteration 12246, loss = 7.69146512\n",
      "Iteration 12247, loss = 8.44313199\n",
      "Iteration 12248, loss = 8.89102803\n",
      "Iteration 12249, loss = 8.74523194\n",
      "Iteration 12250, loss = 9.73103220\n",
      "Iteration 12251, loss = 8.62296339\n",
      "Iteration 12252, loss = 10.58990044\n",
      "Iteration 12253, loss = 9.02858940\n",
      "Iteration 12254, loss = 9.06932151\n",
      "Iteration 12255, loss = 8.01831345\n",
      "Iteration 12256, loss = 8.24316885\n",
      "Iteration 12257, loss = 7.81353451\n",
      "Iteration 12258, loss = 7.86170339\n",
      "Iteration 12259, loss = 8.11959788\n",
      "Iteration 12260, loss = 8.41591428\n",
      "Iteration 12261, loss = 9.13306147\n",
      "Iteration 12262, loss = 9.16690463\n",
      "Iteration 12263, loss = 8.75038890\n",
      "Iteration 12264, loss = 9.04486893\n",
      "Iteration 12265, loss = 8.80557596\n",
      "Iteration 12266, loss = 8.13455835\n",
      "Iteration 12267, loss = 8.21152946\n",
      "Iteration 12268, loss = 8.12093745\n",
      "Iteration 12269, loss = 7.89851667\n",
      "Iteration 12270, loss = 7.94362188\n",
      "Iteration 12271, loss = 7.95698073\n",
      "Iteration 12272, loss = 8.14672826\n",
      "Iteration 12273, loss = 8.00414652\n",
      "Iteration 12274, loss = 8.25443783\n",
      "Iteration 12275, loss = 9.11639571\n",
      "Iteration 12276, loss = 7.85149769\n",
      "Iteration 12277, loss = 7.99320937\n",
      "Iteration 12278, loss = 8.18798228\n",
      "Iteration 12279, loss = 8.57133192\n",
      "Iteration 12280, loss = 8.17751010\n",
      "Iteration 12281, loss = 9.17182155\n",
      "Iteration 12282, loss = 8.39176034\n",
      "Iteration 12283, loss = 8.42943371\n",
      "Iteration 12284, loss = 7.70169490\n",
      "Iteration 12285, loss = 7.51979321\n",
      "Iteration 12286, loss = 7.74044895\n",
      "Iteration 12287, loss = 8.09040365\n",
      "Iteration 12288, loss = 8.13604114\n",
      "Iteration 12289, loss = 8.64092180\n",
      "Iteration 12290, loss = 7.78858229\n",
      "Iteration 12291, loss = 8.18999623\n",
      "Iteration 12292, loss = 8.00262496\n",
      "Iteration 12293, loss = 8.11449091\n",
      "Iteration 12294, loss = 7.75230816\n",
      "Iteration 12295, loss = 7.92672698\n",
      "Iteration 12296, loss = 7.85678928\n",
      "Iteration 12297, loss = 7.84724235\n",
      "Iteration 12298, loss = 7.66401899\n",
      "Iteration 12299, loss = 7.56524293\n",
      "Iteration 12300, loss = 7.88494280\n",
      "Iteration 12301, loss = 8.01988144\n",
      "Iteration 12302, loss = 8.32392202\n",
      "Iteration 12303, loss = 8.16450168\n",
      "Iteration 12304, loss = 8.31965817\n",
      "Iteration 12305, loss = 8.31188215\n",
      "Iteration 12306, loss = 8.06272198\n",
      "Iteration 12307, loss = 8.16979228\n",
      "Iteration 12308, loss = 8.59334738\n",
      "Iteration 12309, loss = 8.91771806\n",
      "Iteration 12310, loss = 8.14696196\n",
      "Iteration 12311, loss = 8.52440350\n",
      "Iteration 12312, loss = 8.20703478\n",
      "Iteration 12313, loss = 8.14652747\n",
      "Iteration 12314, loss = 7.99207064\n",
      "Iteration 12315, loss = 8.13881108\n",
      "Iteration 12316, loss = 8.11323650\n",
      "Iteration 12317, loss = 8.18739740\n",
      "Iteration 12318, loss = 7.83208881\n",
      "Iteration 12319, loss = 7.87797936\n",
      "Iteration 12320, loss = 8.20546798\n",
      "Iteration 12321, loss = 8.33714580\n",
      "Iteration 12322, loss = 8.21388656\n",
      "Iteration 12323, loss = 7.57669097\n",
      "Iteration 12324, loss = 7.68287897\n",
      "Iteration 12325, loss = 8.64261811\n",
      "Iteration 12326, loss = 7.96863965\n",
      "Iteration 12327, loss = 7.84318133\n",
      "Iteration 12328, loss = 7.79617697\n",
      "Iteration 12329, loss = 7.95419129\n",
      "Iteration 12330, loss = 8.76840662\n",
      "Iteration 12331, loss = 8.72895934\n",
      "Iteration 12332, loss = 8.98379048\n",
      "Iteration 12333, loss = 8.58336983\n",
      "Iteration 12334, loss = 8.33253620\n",
      "Iteration 12335, loss = 8.84562007\n",
      "Iteration 12336, loss = 9.03068244\n",
      "Iteration 12337, loss = 8.90261819\n",
      "Iteration 12338, loss = 8.22035403\n",
      "Iteration 12339, loss = 8.12758354\n",
      "Iteration 12340, loss = 8.09797424\n",
      "Iteration 12341, loss = 8.49889930\n",
      "Iteration 12342, loss = 9.14978812\n",
      "Iteration 12343, loss = 8.90619422\n",
      "Iteration 12344, loss = 7.73511099\n",
      "Iteration 12345, loss = 7.85979063\n",
      "Iteration 12346, loss = 8.25463578\n",
      "Iteration 12347, loss = 7.95937653\n",
      "Iteration 12348, loss = 7.83178133\n",
      "Iteration 12349, loss = 7.95580327\n",
      "Iteration 12350, loss = 8.13073790\n",
      "Iteration 12351, loss = 7.85782498\n",
      "Iteration 12352, loss = 7.45253646\n",
      "Iteration 12353, loss = 8.28906464\n",
      "Iteration 12354, loss = 7.68126623\n",
      "Iteration 12355, loss = 7.56723030\n",
      "Iteration 12356, loss = 7.86643403\n",
      "Iteration 12357, loss = 8.34982901\n",
      "Iteration 12358, loss = 8.25005138\n",
      "Iteration 12359, loss = 8.76795390\n",
      "Iteration 12360, loss = 8.42766850\n",
      "Iteration 12361, loss = 8.07752939\n",
      "Iteration 12362, loss = 8.47246296\n",
      "Iteration 12363, loss = 9.29639846\n",
      "Iteration 12364, loss = 8.58344834\n",
      "Iteration 12365, loss = 8.90329764\n",
      "Iteration 12366, loss = 8.46550609\n",
      "Iteration 12367, loss = 8.07044441\n",
      "Iteration 12368, loss = 8.39556855\n",
      "Iteration 12369, loss = 9.82452385\n",
      "Iteration 12370, loss = 8.67448780\n",
      "Iteration 12371, loss = 8.56069996\n",
      "Iteration 12372, loss = 8.45378351\n",
      "Iteration 12373, loss = 7.74705459\n",
      "Iteration 12374, loss = 7.93848269\n",
      "Iteration 12375, loss = 7.77434460\n",
      "Iteration 12376, loss = 8.17093288\n",
      "Iteration 12377, loss = 7.52268272\n",
      "Iteration 12378, loss = 7.62677515\n",
      "Iteration 12379, loss = 7.99372693\n",
      "Iteration 12380, loss = 8.06963445\n",
      "Iteration 12381, loss = 7.99203438\n",
      "Iteration 12382, loss = 7.98951952\n",
      "Iteration 12383, loss = 7.93188737\n",
      "Iteration 12384, loss = 8.15130359\n",
      "Iteration 12385, loss = 8.04130667\n",
      "Iteration 12386, loss = 7.96647578\n",
      "Iteration 12387, loss = 8.24291538\n",
      "Iteration 12388, loss = 8.20733474\n",
      "Iteration 12389, loss = 7.72163181\n",
      "Iteration 12390, loss = 7.85739632\n",
      "Iteration 12391, loss = 7.89719485\n",
      "Iteration 12392, loss = 7.79569538\n",
      "Iteration 12393, loss = 8.19075220\n",
      "Iteration 12394, loss = 8.61811851\n",
      "Iteration 12395, loss = 7.56931178\n",
      "Iteration 12396, loss = 8.54872742\n",
      "Iteration 12397, loss = 7.78901872\n",
      "Iteration 12398, loss = 8.46523430\n",
      "Iteration 12399, loss = 7.99527525\n",
      "Iteration 12400, loss = 8.35304080\n",
      "Iteration 12401, loss = 8.71443665\n",
      "Iteration 12402, loss = 8.22729650\n",
      "Iteration 12403, loss = 8.55890314\n",
      "Iteration 12404, loss = 9.52435909\n",
      "Iteration 12405, loss = 9.50669638\n",
      "Iteration 12406, loss = 9.12678151\n",
      "Iteration 12407, loss = 9.22321599\n",
      "Iteration 12408, loss = 8.06725822\n",
      "Iteration 12409, loss = 8.71142360\n",
      "Iteration 12410, loss = 7.98439787\n",
      "Iteration 12411, loss = 7.94744641\n",
      "Iteration 12412, loss = 7.83473296\n",
      "Iteration 12413, loss = 7.76124549\n",
      "Iteration 12414, loss = 7.96652842\n",
      "Iteration 12415, loss = 7.81021807\n",
      "Iteration 12416, loss = 8.04708734\n",
      "Iteration 12417, loss = 7.98452183\n",
      "Iteration 12418, loss = 7.74448009\n",
      "Iteration 12419, loss = 7.71230186\n",
      "Iteration 12420, loss = 8.56442858\n",
      "Iteration 12421, loss = 8.01672710\n",
      "Iteration 12422, loss = 7.85869801\n",
      "Iteration 12423, loss = 7.92693187\n",
      "Iteration 12424, loss = 7.75806250\n",
      "Iteration 12425, loss = 7.81325718\n",
      "Iteration 12426, loss = 7.69471588\n",
      "Iteration 12427, loss = 7.78263585\n",
      "Iteration 12428, loss = 7.77390718\n",
      "Iteration 12429, loss = 7.65713571\n",
      "Iteration 12430, loss = 8.11983872\n",
      "Iteration 12431, loss = 8.18168023\n",
      "Iteration 12432, loss = 7.59427850\n",
      "Iteration 12433, loss = 7.54863130\n",
      "Iteration 12434, loss = 7.96260228\n",
      "Iteration 12435, loss = 7.73821683\n",
      "Iteration 12436, loss = 8.28879213\n",
      "Iteration 12437, loss = 8.39229942\n",
      "Iteration 12438, loss = 7.57763446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12439, loss = 8.25936033\n",
      "Iteration 12440, loss = 7.73660524\n",
      "Iteration 12441, loss = 8.45446178\n",
      "Iteration 12442, loss = 8.37211683\n",
      "Iteration 12443, loss = 8.45341458\n",
      "Iteration 12444, loss = 8.19253794\n",
      "Iteration 12445, loss = 7.84640697\n",
      "Iteration 12446, loss = 7.85143954\n",
      "Iteration 12447, loss = 8.32245586\n",
      "Iteration 12448, loss = 8.32579657\n",
      "Iteration 12449, loss = 7.89481995\n",
      "Iteration 12450, loss = 8.09906884\n",
      "Iteration 12451, loss = 7.84276115\n",
      "Iteration 12452, loss = 7.71278706\n",
      "Iteration 12453, loss = 7.83841880\n",
      "Iteration 12454, loss = 8.39427776\n",
      "Iteration 12455, loss = 9.05697995\n",
      "Iteration 12456, loss = 8.28758730\n",
      "Iteration 12457, loss = 9.21780436\n",
      "Iteration 12458, loss = 8.51162379\n",
      "Iteration 12459, loss = 8.46525044\n",
      "Iteration 12460, loss = 8.21153853\n",
      "Iteration 12461, loss = 8.87460187\n",
      "Iteration 12462, loss = 7.42908031\n",
      "Iteration 12463, loss = 8.79363726\n",
      "Iteration 12464, loss = 8.33525304\n",
      "Iteration 12465, loss = 7.80570472\n",
      "Iteration 12466, loss = 7.80561562\n",
      "Iteration 12467, loss = 7.56639993\n",
      "Iteration 12468, loss = 7.90153959\n",
      "Iteration 12469, loss = 9.09720372\n",
      "Iteration 12470, loss = 7.86420971\n",
      "Iteration 12471, loss = 7.85098712\n",
      "Iteration 12472, loss = 7.92544647\n",
      "Iteration 12473, loss = 7.75622536\n",
      "Iteration 12474, loss = 7.91817525\n",
      "Iteration 12475, loss = 7.69734525\n",
      "Iteration 12476, loss = 8.23533417\n",
      "Iteration 12477, loss = 8.95298253\n",
      "Iteration 12478, loss = 7.74426884\n",
      "Iteration 12479, loss = 7.82856798\n",
      "Iteration 12480, loss = 7.67578326\n",
      "Iteration 12481, loss = 7.73102874\n",
      "Iteration 12482, loss = 7.66742592\n",
      "Iteration 12483, loss = 7.72602040\n",
      "Iteration 12484, loss = 7.61433231\n",
      "Iteration 12485, loss = 7.97847410\n",
      "Iteration 12486, loss = 8.19781030\n",
      "Iteration 12487, loss = 8.33913263\n",
      "Iteration 12488, loss = 7.73498553\n",
      "Iteration 12489, loss = 7.78546963\n",
      "Iteration 12490, loss = 8.34520600\n",
      "Iteration 12491, loss = 7.86326043\n",
      "Iteration 12492, loss = 8.26262646\n",
      "Iteration 12493, loss = 8.05966745\n",
      "Iteration 12494, loss = 8.13475189\n",
      "Iteration 12495, loss = 7.59560098\n",
      "Iteration 12496, loss = 7.85488537\n",
      "Iteration 12497, loss = 7.46096938\n",
      "Iteration 12498, loss = 8.00349688\n",
      "Iteration 12499, loss = 7.41702800\n",
      "Iteration 12500, loss = 7.70230286\n",
      "Iteration 12501, loss = 8.04396399\n",
      "Iteration 12502, loss = 7.55272048\n",
      "Iteration 12503, loss = 7.51645247\n",
      "Iteration 12504, loss = 7.90619620\n",
      "Iteration 12505, loss = 7.76316374\n",
      "Iteration 12506, loss = 7.66516065\n",
      "Iteration 12507, loss = 7.91901310\n",
      "Iteration 12508, loss = 7.87165387\n",
      "Iteration 12509, loss = 7.64039574\n",
      "Iteration 12510, loss = 7.83591469\n",
      "Iteration 12511, loss = 7.67231909\n",
      "Iteration 12512, loss = 7.88420801\n",
      "Iteration 12513, loss = 8.26797430\n",
      "Iteration 12514, loss = 7.97920163\n",
      "Iteration 12515, loss = 8.17227671\n",
      "Iteration 12516, loss = 7.72642563\n",
      "Iteration 12517, loss = 7.59076468\n",
      "Iteration 12518, loss = 8.10895663\n",
      "Iteration 12519, loss = 7.94953184\n",
      "Iteration 12520, loss = 8.21076505\n",
      "Iteration 12521, loss = 8.29295406\n",
      "Iteration 12522, loss = 7.95588737\n",
      "Iteration 12523, loss = 7.56899533\n",
      "Iteration 12524, loss = 7.62053865\n",
      "Iteration 12525, loss = 7.94711942\n",
      "Iteration 12526, loss = 7.98310406\n",
      "Iteration 12527, loss = 8.24974733\n",
      "Iteration 12528, loss = 8.13570139\n",
      "Iteration 12529, loss = 8.20749046\n",
      "Iteration 12530, loss = 7.58403517\n",
      "Iteration 12531, loss = 7.86905391\n",
      "Iteration 12532, loss = 7.77164874\n",
      "Iteration 12533, loss = 7.93431257\n",
      "Iteration 12534, loss = 7.64102933\n",
      "Iteration 12535, loss = 7.53641978\n",
      "Iteration 12536, loss = 7.82975870\n",
      "Iteration 12537, loss = 8.42352760\n",
      "Iteration 12538, loss = 9.08454870\n",
      "Iteration 12539, loss = 9.71385803\n",
      "Iteration 12540, loss = 9.07960319\n",
      "Iteration 12541, loss = 9.27205887\n",
      "Iteration 12542, loss = 9.54126874\n",
      "Iteration 12543, loss = 8.72928190\n",
      "Iteration 12544, loss = 8.50564020\n",
      "Iteration 12545, loss = 9.28307762\n",
      "Iteration 12546, loss = 8.72844247\n",
      "Iteration 12547, loss = 7.96156142\n",
      "Iteration 12548, loss = 7.67868531\n",
      "Iteration 12549, loss = 7.88110894\n",
      "Iteration 12550, loss = 8.36997808\n",
      "Iteration 12551, loss = 8.16514150\n",
      "Iteration 12552, loss = 7.71710951\n",
      "Iteration 12553, loss = 7.79423124\n",
      "Iteration 12554, loss = 8.51511619\n",
      "Iteration 12555, loss = 9.19367278\n",
      "Iteration 12556, loss = 8.66031385\n",
      "Iteration 12557, loss = 8.09638565\n",
      "Iteration 12558, loss = 7.49749256\n",
      "Iteration 12559, loss = 8.07811567\n",
      "Iteration 12560, loss = 8.21403078\n",
      "Iteration 12561, loss = 8.21376409\n",
      "Iteration 12562, loss = 8.37346913\n",
      "Iteration 12563, loss = 7.50725908\n",
      "Iteration 12564, loss = 7.69563465\n",
      "Iteration 12565, loss = 7.69251106\n",
      "Iteration 12566, loss = 8.44970781\n",
      "Iteration 12567, loss = 7.62695595\n",
      "Iteration 12568, loss = 8.09820850\n",
      "Iteration 12569, loss = 8.44918005\n",
      "Iteration 12570, loss = 8.68725814\n",
      "Iteration 12571, loss = 8.21672737\n",
      "Iteration 12572, loss = 8.27555166\n",
      "Iteration 12573, loss = 8.83243756\n",
      "Iteration 12574, loss = 8.35026940\n",
      "Iteration 12575, loss = 7.52593087\n",
      "Iteration 12576, loss = 7.53014091\n",
      "Iteration 12577, loss = 8.05555409\n",
      "Iteration 12578, loss = 7.59247339\n",
      "Iteration 12579, loss = 7.69831189\n",
      "Iteration 12580, loss = 7.65137299\n",
      "Iteration 12581, loss = 7.56750278\n",
      "Iteration 12582, loss = 7.92758569\n",
      "Iteration 12583, loss = 8.05255733\n",
      "Iteration 12584, loss = 7.56406798\n",
      "Iteration 12585, loss = 8.13448601\n",
      "Iteration 12586, loss = 7.90265137\n",
      "Iteration 12587, loss = 9.99892177\n",
      "Iteration 12588, loss = 10.73394785\n",
      "Iteration 12589, loss = 8.76228120\n",
      "Iteration 12590, loss = 8.22908592\n",
      "Iteration 12591, loss = 9.02978640\n",
      "Iteration 12592, loss = 9.66386823\n",
      "Iteration 12593, loss = 8.71517225\n",
      "Iteration 12594, loss = 8.36077787\n",
      "Iteration 12595, loss = 8.28812732\n",
      "Iteration 12596, loss = 7.75934297\n",
      "Iteration 12597, loss = 7.89919184\n",
      "Iteration 12598, loss = 7.57719735\n",
      "Iteration 12599, loss = 7.78211633\n",
      "Iteration 12600, loss = 7.54189172\n",
      "Iteration 12601, loss = 7.44681778\n",
      "Iteration 12602, loss = 7.94565990\n",
      "Iteration 12603, loss = 7.63799730\n",
      "Iteration 12604, loss = 7.75409740\n",
      "Iteration 12605, loss = 8.05459948\n",
      "Iteration 12606, loss = 7.96019172\n",
      "Iteration 12607, loss = 7.63973481\n",
      "Iteration 12608, loss = 7.64624000\n",
      "Iteration 12609, loss = 7.60432553\n",
      "Iteration 12610, loss = 7.61795582\n",
      "Iteration 12611, loss = 7.67579860\n",
      "Iteration 12612, loss = 8.67997135\n",
      "Iteration 12613, loss = 8.50278242\n",
      "Iteration 12614, loss = 10.15507902\n",
      "Iteration 12615, loss = 9.56765668\n",
      "Iteration 12616, loss = 8.85777211\n",
      "Iteration 12617, loss = 9.28004227\n",
      "Iteration 12618, loss = 8.29956976\n",
      "Iteration 12619, loss = 7.83921373\n",
      "Iteration 12620, loss = 7.53155255\n",
      "Iteration 12621, loss = 7.39376426\n",
      "Iteration 12622, loss = 7.82573727\n",
      "Iteration 12623, loss = 8.17955172\n",
      "Iteration 12624, loss = 8.05224133\n",
      "Iteration 12625, loss = 8.04067127\n",
      "Iteration 12626, loss = 7.98324772\n",
      "Iteration 12627, loss = 7.97523429\n",
      "Iteration 12628, loss = 7.71455168\n",
      "Iteration 12629, loss = 7.86932858\n",
      "Iteration 12630, loss = 7.79672443\n",
      "Iteration 12631, loss = 7.82798193\n",
      "Iteration 12632, loss = 8.29619275\n",
      "Iteration 12633, loss = 8.18999575\n",
      "Iteration 12634, loss = 7.75731744\n",
      "Iteration 12635, loss = 7.93453617\n",
      "Iteration 12636, loss = 8.23579392\n",
      "Iteration 12637, loss = 8.32826041\n",
      "Iteration 12638, loss = 8.41968469\n",
      "Iteration 12639, loss = 8.05544396\n",
      "Iteration 12640, loss = 7.73034247\n",
      "Iteration 12641, loss = 8.37195376\n",
      "Iteration 12642, loss = 9.28412110\n",
      "Iteration 12643, loss = 8.62797985\n",
      "Iteration 12644, loss = 7.97661773\n",
      "Iteration 12645, loss = 9.07903612\n",
      "Iteration 12646, loss = 11.76308600\n",
      "Iteration 12647, loss = 13.22044262\n",
      "Iteration 12648, loss = 12.11546053\n",
      "Iteration 12649, loss = 9.97415681\n",
      "Iteration 12650, loss = 8.97518424\n",
      "Iteration 12651, loss = 8.47438316\n",
      "Iteration 12652, loss = 7.87389048\n",
      "Iteration 12653, loss = 8.02982688\n",
      "Iteration 12654, loss = 8.11695446\n",
      "Iteration 12655, loss = 7.37367917\n",
      "Iteration 12656, loss = 9.07958440\n",
      "Iteration 12657, loss = 8.08412637\n",
      "Iteration 12658, loss = 9.13879054\n",
      "Iteration 12659, loss = 7.66191857\n",
      "Iteration 12660, loss = 7.92136621\n",
      "Iteration 12661, loss = 8.16252074\n",
      "Iteration 12662, loss = 8.15003716\n",
      "Iteration 12663, loss = 8.38655956\n",
      "Iteration 12664, loss = 7.82380323\n",
      "Iteration 12665, loss = 7.77058365\n",
      "Iteration 12666, loss = 7.59388608\n",
      "Iteration 12667, loss = 8.35090220\n",
      "Iteration 12668, loss = 7.66200553\n",
      "Iteration 12669, loss = 7.85060774\n",
      "Iteration 12670, loss = 7.91562639\n",
      "Iteration 12671, loss = 7.50002328\n",
      "Iteration 12672, loss = 8.07725589\n",
      "Iteration 12673, loss = 7.54445502\n",
      "Iteration 12674, loss = 7.69359037\n",
      "Iteration 12675, loss = 7.87914891\n",
      "Iteration 12676, loss = 7.71452825\n",
      "Iteration 12677, loss = 7.65259442\n",
      "Iteration 12678, loss = 8.08617153\n",
      "Iteration 12679, loss = 7.87714231\n",
      "Iteration 12680, loss = 7.96157613\n",
      "Iteration 12681, loss = 7.83533187\n",
      "Iteration 12682, loss = 7.71027587\n",
      "Iteration 12683, loss = 8.10892512\n",
      "Iteration 12684, loss = 7.77892055\n",
      "Iteration 12685, loss = 7.86138149\n",
      "Iteration 12686, loss = 8.24776607\n",
      "Iteration 12687, loss = 8.17676631\n",
      "Iteration 12688, loss = 7.83030019\n",
      "Iteration 12689, loss = 7.48136691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12690, loss = 7.77134769\n",
      "Iteration 12691, loss = 8.00483522\n",
      "Iteration 12692, loss = 7.94781313\n",
      "Iteration 12693, loss = 7.97552035\n",
      "Iteration 12694, loss = 7.94146359\n",
      "Iteration 12695, loss = 7.38465812\n",
      "Iteration 12696, loss = 7.97664483\n",
      "Iteration 12697, loss = 8.05451026\n",
      "Iteration 12698, loss = 7.77066520\n",
      "Iteration 12699, loss = 8.43485947\n",
      "Iteration 12700, loss = 8.16636087\n",
      "Iteration 12701, loss = 8.49081156\n",
      "Iteration 12702, loss = 7.93760433\n",
      "Iteration 12703, loss = 7.77257597\n",
      "Iteration 12704, loss = 8.09271582\n",
      "Iteration 12705, loss = 7.76574899\n",
      "Iteration 12706, loss = 8.09150826\n",
      "Iteration 12707, loss = 7.93615845\n",
      "Iteration 12708, loss = 8.31181066\n",
      "Iteration 12709, loss = 7.90112318\n",
      "Iteration 12710, loss = 9.44678474\n",
      "Iteration 12711, loss = 9.74982001\n",
      "Iteration 12712, loss = 9.65503265\n",
      "Iteration 12713, loss = 8.89313047\n",
      "Iteration 12714, loss = 8.59712379\n",
      "Iteration 12715, loss = 8.43006500\n",
      "Iteration 12716, loss = 8.01136740\n",
      "Iteration 12717, loss = 8.08379207\n",
      "Iteration 12718, loss = 7.32800287\n",
      "Iteration 12719, loss = 7.78785561\n",
      "Iteration 12720, loss = 7.83807280\n",
      "Iteration 12721, loss = 7.90409228\n",
      "Iteration 12722, loss = 7.82966573\n",
      "Iteration 12723, loss = 8.45201286\n",
      "Iteration 12724, loss = 8.71902878\n",
      "Iteration 12725, loss = 8.47781824\n",
      "Iteration 12726, loss = 8.48463668\n",
      "Iteration 12727, loss = 8.04493979\n",
      "Iteration 12728, loss = 7.84790408\n",
      "Iteration 12729, loss = 8.81944707\n",
      "Iteration 12730, loss = 9.72526616\n",
      "Iteration 12731, loss = 8.62469862\n",
      "Iteration 12732, loss = 8.08691053\n",
      "Iteration 12733, loss = 7.64917044\n",
      "Iteration 12734, loss = 8.27597731\n",
      "Iteration 12735, loss = 7.49586889\n",
      "Iteration 12736, loss = 7.83454529\n",
      "Iteration 12737, loss = 7.56104162\n",
      "Iteration 12738, loss = 7.38400979\n",
      "Iteration 12739, loss = 7.79213232\n",
      "Iteration 12740, loss = 7.88389468\n",
      "Iteration 12741, loss = 7.67580753\n",
      "Iteration 12742, loss = 7.83445463\n",
      "Iteration 12743, loss = 7.41454924\n",
      "Iteration 12744, loss = 7.43016808\n",
      "Iteration 12745, loss = 7.62938022\n",
      "Iteration 12746, loss = 7.94718620\n",
      "Iteration 12747, loss = 7.64782979\n",
      "Iteration 12748, loss = 7.66587200\n",
      "Iteration 12749, loss = 7.38306120\n",
      "Iteration 12750, loss = 7.83680334\n",
      "Iteration 12751, loss = 7.69944983\n",
      "Iteration 12752, loss = 7.45763464\n",
      "Iteration 12753, loss = 7.51752258\n",
      "Iteration 12754, loss = 7.60855905\n",
      "Iteration 12755, loss = 7.61391047\n",
      "Iteration 12756, loss = 7.39172746\n",
      "Iteration 12757, loss = 7.99010974\n",
      "Iteration 12758, loss = 8.34368524\n",
      "Iteration 12759, loss = 8.03692948\n",
      "Iteration 12760, loss = 7.61271457\n",
      "Iteration 12761, loss = 7.91814416\n",
      "Iteration 12762, loss = 7.62844233\n",
      "Iteration 12763, loss = 8.28922269\n",
      "Iteration 12764, loss = 8.85701399\n",
      "Iteration 12765, loss = 8.08978046\n",
      "Iteration 12766, loss = 7.91301824\n",
      "Iteration 12767, loss = 8.05432867\n",
      "Iteration 12768, loss = 7.86606295\n",
      "Iteration 12769, loss = 7.85821123\n",
      "Iteration 12770, loss = 7.96093805\n",
      "Iteration 12771, loss = 7.65888591\n",
      "Iteration 12772, loss = 7.83230846\n",
      "Iteration 12773, loss = 7.76881525\n",
      "Iteration 12774, loss = 7.38918620\n",
      "Iteration 12775, loss = 7.74860702\n",
      "Iteration 12776, loss = 7.83805952\n",
      "Iteration 12777, loss = 7.91156010\n",
      "Iteration 12778, loss = 8.05974086\n",
      "Iteration 12779, loss = 8.69371100\n",
      "Iteration 12780, loss = 8.03263125\n",
      "Iteration 12781, loss = 7.63112603\n",
      "Iteration 12782, loss = 8.04641227\n",
      "Iteration 12783, loss = 7.84824730\n",
      "Iteration 12784, loss = 7.81009325\n",
      "Iteration 12785, loss = 8.84632769\n",
      "Iteration 12786, loss = 8.74600802\n",
      "Iteration 12787, loss = 7.73127078\n",
      "Iteration 12788, loss = 7.47622411\n",
      "Iteration 12789, loss = 7.59699192\n",
      "Iteration 12790, loss = 7.52773143\n",
      "Iteration 12791, loss = 7.52877374\n",
      "Iteration 12792, loss = 7.45651038\n",
      "Iteration 12793, loss = 7.87934044\n",
      "Iteration 12794, loss = 7.75240383\n",
      "Iteration 12795, loss = 7.52113032\n",
      "Iteration 12796, loss = 7.38250673\n",
      "Iteration 12797, loss = 7.51964798\n",
      "Iteration 12798, loss = 8.94165137\n",
      "Iteration 12799, loss = 8.71379381\n",
      "Iteration 12800, loss = 7.53232195\n",
      "Iteration 12801, loss = 9.01625842\n",
      "Iteration 12802, loss = 7.64950169\n",
      "Iteration 12803, loss = 8.08826189\n",
      "Iteration 12804, loss = 8.63092049\n",
      "Iteration 12805, loss = 7.96735655\n",
      "Iteration 12806, loss = 7.53491461\n",
      "Iteration 12807, loss = 7.41737065\n",
      "Iteration 12808, loss = 7.78324066\n",
      "Iteration 12809, loss = 7.78196307\n",
      "Iteration 12810, loss = 7.88139006\n",
      "Iteration 12811, loss = 8.07752840\n",
      "Iteration 12812, loss = 7.91005159\n",
      "Iteration 12813, loss = 7.91441633\n",
      "Iteration 12814, loss = 7.80454973\n",
      "Iteration 12815, loss = 7.71897794\n",
      "Iteration 12816, loss = 7.30058456\n",
      "Iteration 12817, loss = 8.29940054\n",
      "Iteration 12818, loss = 8.73361768\n",
      "Iteration 12819, loss = 8.21746820\n",
      "Iteration 12820, loss = 8.11248132\n",
      "Iteration 12821, loss = 8.03526341\n",
      "Iteration 12822, loss = 7.64719606\n",
      "Iteration 12823, loss = 7.35135570\n",
      "Iteration 12824, loss = 7.61563219\n",
      "Iteration 12825, loss = 7.29308671\n",
      "Iteration 12826, loss = 7.47853010\n",
      "Iteration 12827, loss = 7.42953364\n",
      "Iteration 12828, loss = 7.34661019\n",
      "Iteration 12829, loss = 7.35365482\n",
      "Iteration 12830, loss = 7.43029458\n",
      "Iteration 12831, loss = 7.51522221\n",
      "Iteration 12832, loss = 7.28603196\n",
      "Iteration 12833, loss = 7.40574156\n",
      "Iteration 12834, loss = 7.32172682\n",
      "Iteration 12835, loss = 8.05808891\n",
      "Iteration 12836, loss = 8.25427731\n",
      "Iteration 12837, loss = 8.45197832\n",
      "Iteration 12838, loss = 8.44317145\n",
      "Iteration 12839, loss = 11.34969286\n",
      "Iteration 12840, loss = 10.29870642\n",
      "Iteration 12841, loss = 8.95763374\n",
      "Iteration 12842, loss = 9.89924114\n",
      "Iteration 12843, loss = 8.15610406\n",
      "Iteration 12844, loss = 8.71798966\n",
      "Iteration 12845, loss = 9.18738079\n",
      "Iteration 12846, loss = 8.41357124\n",
      "Iteration 12847, loss = 7.65365420\n",
      "Iteration 12848, loss = 8.12680811\n",
      "Iteration 12849, loss = 8.39954294\n",
      "Iteration 12850, loss = 7.88115511\n",
      "Iteration 12851, loss = 7.40248310\n",
      "Iteration 12852, loss = 7.46897310\n",
      "Iteration 12853, loss = 7.65098562\n",
      "Iteration 12854, loss = 8.36812970\n",
      "Iteration 12855, loss = 7.44295661\n",
      "Iteration 12856, loss = 8.19115132\n",
      "Iteration 12857, loss = 7.64137235\n",
      "Iteration 12858, loss = 7.76090476\n",
      "Iteration 12859, loss = 8.41247327\n",
      "Iteration 12860, loss = 7.79972227\n",
      "Iteration 12861, loss = 7.69246436\n",
      "Iteration 12862, loss = 7.42021684\n",
      "Iteration 12863, loss = 7.45118087\n",
      "Iteration 12864, loss = 8.28799566\n",
      "Iteration 12865, loss = 7.94850641\n",
      "Iteration 12866, loss = 8.53007454\n",
      "Iteration 12867, loss = 7.61149111\n",
      "Iteration 12868, loss = 7.44476893\n",
      "Iteration 12869, loss = 8.19529973\n",
      "Iteration 12870, loss = 8.21142191\n",
      "Iteration 12871, loss = 7.62992039\n",
      "Iteration 12872, loss = 7.50083702\n",
      "Iteration 12873, loss = 7.78402950\n",
      "Iteration 12874, loss = 8.27570752\n",
      "Iteration 12875, loss = 8.62665820\n",
      "Iteration 12876, loss = 8.17904004\n",
      "Iteration 12877, loss = 7.97910575\n",
      "Iteration 12878, loss = 8.39199508\n",
      "Iteration 12879, loss = 8.56337298\n",
      "Iteration 12880, loss = 8.57364495\n",
      "Iteration 12881, loss = 8.84804599\n",
      "Iteration 12882, loss = 8.11612674\n",
      "Iteration 12883, loss = 9.00800947\n",
      "Iteration 12884, loss = 7.92139643\n",
      "Iteration 12885, loss = 7.37166409\n",
      "Iteration 12886, loss = 7.61661298\n",
      "Iteration 12887, loss = 7.68670180\n",
      "Iteration 12888, loss = 7.47978626\n",
      "Iteration 12889, loss = 7.85383888\n",
      "Iteration 12890, loss = 8.86906659\n",
      "Iteration 12891, loss = 8.89117854\n",
      "Iteration 12892, loss = 8.01035691\n",
      "Iteration 12893, loss = 7.59177857\n",
      "Iteration 12894, loss = 7.41289293\n",
      "Iteration 12895, loss = 7.49550834\n",
      "Iteration 12896, loss = 7.57141893\n",
      "Iteration 12897, loss = 7.41009546\n",
      "Iteration 12898, loss = 7.93003361\n",
      "Iteration 12899, loss = 8.30323899\n",
      "Iteration 12900, loss = 8.24348375\n",
      "Iteration 12901, loss = 8.63428169\n",
      "Iteration 12902, loss = 8.55473717\n",
      "Iteration 12903, loss = 8.36047109\n",
      "Iteration 12904, loss = 8.52025686\n",
      "Iteration 12905, loss = 8.00655608\n",
      "Iteration 12906, loss = 7.89848219\n",
      "Iteration 12907, loss = 8.33590217\n",
      "Iteration 12908, loss = 7.74961830\n",
      "Iteration 12909, loss = 7.62477890\n",
      "Iteration 12910, loss = 8.89903332\n",
      "Iteration 12911, loss = 7.88409976\n",
      "Iteration 12912, loss = 7.57633354\n",
      "Iteration 12913, loss = 8.01450460\n",
      "Iteration 12914, loss = 7.79521775\n",
      "Iteration 12915, loss = 7.61204137\n",
      "Iteration 12916, loss = 7.38754760\n",
      "Iteration 12917, loss = 7.58778602\n",
      "Iteration 12918, loss = 7.34410775\n",
      "Iteration 12919, loss = 8.35477624\n",
      "Iteration 12920, loss = 7.93747300\n",
      "Iteration 12921, loss = 7.62878430\n",
      "Iteration 12922, loss = 7.39571273\n",
      "Iteration 12923, loss = 8.02195403\n",
      "Iteration 12924, loss = 7.65503233\n",
      "Iteration 12925, loss = 7.42320722\n",
      "Iteration 12926, loss = 8.26486308\n",
      "Iteration 12927, loss = 7.99024283\n",
      "Iteration 12928, loss = 8.32670439\n",
      "Iteration 12929, loss = 8.69449813\n",
      "Iteration 12930, loss = 8.51526583\n",
      "Iteration 12931, loss = 7.42344510\n",
      "Iteration 12932, loss = 7.39262080\n",
      "Iteration 12933, loss = 7.35844364\n",
      "Iteration 12934, loss = 7.35482074\n",
      "Iteration 12935, loss = 7.34160442\n",
      "Iteration 12936, loss = 7.58012701\n",
      "Iteration 12937, loss = 7.25309495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12938, loss = 7.67836128\n",
      "Iteration 12939, loss = 7.45954113\n",
      "Iteration 12940, loss = 7.61161141\n",
      "Iteration 12941, loss = 7.62884880\n",
      "Iteration 12942, loss = 7.61430880\n",
      "Iteration 12943, loss = 7.51114841\n",
      "Iteration 12944, loss = 7.80302525\n",
      "Iteration 12945, loss = 7.71437201\n",
      "Iteration 12946, loss = 7.40217715\n",
      "Iteration 12947, loss = 8.14368035\n",
      "Iteration 12948, loss = 8.07121264\n",
      "Iteration 12949, loss = 8.39754356\n",
      "Iteration 12950, loss = 7.48395217\n",
      "Iteration 12951, loss = 7.78719483\n",
      "Iteration 12952, loss = 7.93686612\n",
      "Iteration 12953, loss = 7.80773028\n",
      "Iteration 12954, loss = 7.46612158\n",
      "Iteration 12955, loss = 7.51840897\n",
      "Iteration 12956, loss = 8.23327406\n",
      "Iteration 12957, loss = 7.59059564\n",
      "Iteration 12958, loss = 7.74785787\n",
      "Iteration 12959, loss = 7.97057945\n",
      "Iteration 12960, loss = 8.16397228\n",
      "Iteration 12961, loss = 8.96610717\n",
      "Iteration 12962, loss = 8.46574633\n",
      "Iteration 12963, loss = 7.52743594\n",
      "Iteration 12964, loss = 7.97517081\n",
      "Iteration 12965, loss = 7.84180497\n",
      "Iteration 12966, loss = 7.35214736\n",
      "Iteration 12967, loss = 7.81040370\n",
      "Iteration 12968, loss = 7.84859456\n",
      "Iteration 12969, loss = 7.41229012\n",
      "Iteration 12970, loss = 7.38728206\n",
      "Iteration 12971, loss = 7.44919238\n",
      "Iteration 12972, loss = 7.33997327\n",
      "Iteration 12973, loss = 7.87786964\n",
      "Iteration 12974, loss = 7.39971800\n",
      "Iteration 12975, loss = 7.39067146\n",
      "Iteration 12976, loss = 7.39252014\n",
      "Iteration 12977, loss = 8.02219403\n",
      "Iteration 12978, loss = 7.77222213\n",
      "Iteration 12979, loss = 7.50845194\n",
      "Iteration 12980, loss = 7.56655173\n",
      "Iteration 12981, loss = 7.81840609\n",
      "Iteration 12982, loss = 9.40684391\n",
      "Iteration 12983, loss = 9.61627264\n",
      "Iteration 12984, loss = 8.84277147\n",
      "Iteration 12985, loss = 7.76924736\n",
      "Iteration 12986, loss = 8.08785686\n",
      "Iteration 12987, loss = 8.85725718\n",
      "Iteration 12988, loss = 9.32806165\n",
      "Iteration 12989, loss = 8.22821768\n",
      "Iteration 12990, loss = 8.40952027\n",
      "Iteration 12991, loss = 8.64316990\n",
      "Iteration 12992, loss = 9.00212619\n",
      "Iteration 12993, loss = 8.06105056\n",
      "Iteration 12994, loss = 8.81067800\n",
      "Iteration 12995, loss = 8.39619242\n",
      "Iteration 12996, loss = 7.50728654\n",
      "Iteration 12997, loss = 7.83394734\n",
      "Iteration 12998, loss = 8.54747017\n",
      "Iteration 12999, loss = 9.03864112\n",
      "Iteration 13000, loss = 8.71010944\n",
      "Iteration 13001, loss = 8.07739050\n",
      "Iteration 13002, loss = 7.76558354\n",
      "Iteration 13003, loss = 8.13430840\n",
      "Iteration 13004, loss = 7.68826851\n",
      "Iteration 13005, loss = 7.46749170\n",
      "Iteration 13006, loss = 7.45678405\n",
      "Iteration 13007, loss = 7.83061875\n",
      "Iteration 13008, loss = 8.68478246\n",
      "Iteration 13009, loss = 8.34053812\n",
      "Iteration 13010, loss = 7.84202909\n",
      "Iteration 13011, loss = 8.44307008\n",
      "Iteration 13012, loss = 10.03162206\n",
      "Iteration 13013, loss = 7.53555675\n",
      "Iteration 13014, loss = 8.43913222\n",
      "Iteration 13015, loss = 8.42740164\n",
      "Iteration 13016, loss = 8.96142133\n",
      "Iteration 13017, loss = 7.79455470\n",
      "Iteration 13018, loss = 7.75332319\n",
      "Iteration 13019, loss = 8.10852485\n",
      "Iteration 13020, loss = 8.80431776\n",
      "Iteration 13021, loss = 7.86793083\n",
      "Iteration 13022, loss = 7.84639715\n",
      "Iteration 13023, loss = 7.64987335\n",
      "Iteration 13024, loss = 8.05054078\n",
      "Iteration 13025, loss = 8.76891731\n",
      "Iteration 13026, loss = 8.50801448\n",
      "Iteration 13027, loss = 8.17789989\n",
      "Iteration 13028, loss = 8.08727732\n",
      "Iteration 13029, loss = 7.26145180\n",
      "Iteration 13030, loss = 8.30234418\n",
      "Iteration 13031, loss = 8.67359651\n",
      "Iteration 13032, loss = 8.20598297\n",
      "Iteration 13033, loss = 9.06782676\n",
      "Iteration 13034, loss = 8.72210973\n",
      "Iteration 13035, loss = 9.78665014\n",
      "Iteration 13036, loss = 8.90116848\n",
      "Iteration 13037, loss = 9.67680954\n",
      "Iteration 13038, loss = 8.14969933\n",
      "Iteration 13039, loss = 8.06695819\n",
      "Iteration 13040, loss = 8.37524650\n",
      "Iteration 13041, loss = 8.74523469\n",
      "Iteration 13042, loss = 8.09030933\n",
      "Iteration 13043, loss = 8.09321772\n",
      "Iteration 13044, loss = 8.12093816\n",
      "Iteration 13045, loss = 8.16837855\n",
      "Iteration 13046, loss = 8.23353591\n",
      "Iteration 13047, loss = 7.64987652\n",
      "Iteration 13048, loss = 7.54512026\n",
      "Iteration 13049, loss = 7.60795746\n",
      "Iteration 13050, loss = 7.91068303\n",
      "Iteration 13051, loss = 8.23984129\n",
      "Iteration 13052, loss = 7.49370954\n",
      "Iteration 13053, loss = 7.75540359\n",
      "Iteration 13054, loss = 8.40809157\n",
      "Iteration 13055, loss = 7.94167178\n",
      "Iteration 13056, loss = 7.54261802\n",
      "Iteration 13057, loss = 8.08963687\n",
      "Iteration 13058, loss = 8.20099060\n",
      "Iteration 13059, loss = 7.47692147\n",
      "Iteration 13060, loss = 7.72933573\n",
      "Iteration 13061, loss = 8.13935456\n",
      "Iteration 13062, loss = 8.93917843\n",
      "Iteration 13063, loss = 8.73552923\n",
      "Iteration 13064, loss = 8.62043334\n",
      "Iteration 13065, loss = 8.47569967\n",
      "Iteration 13066, loss = 8.56679039\n",
      "Iteration 13067, loss = 7.99135469\n",
      "Iteration 13068, loss = 8.14904160\n",
      "Iteration 13069, loss = 8.41591107\n",
      "Iteration 13070, loss = 8.78274430\n",
      "Iteration 13071, loss = 7.47297084\n",
      "Iteration 13072, loss = 7.47207048\n",
      "Iteration 13073, loss = 7.99173246\n",
      "Iteration 13074, loss = 8.30335173\n",
      "Iteration 13075, loss = 8.42624882\n",
      "Iteration 13076, loss = 7.69362582\n",
      "Iteration 13077, loss = 7.44147015\n",
      "Iteration 13078, loss = 7.43354304\n",
      "Iteration 13079, loss = 7.14618254\n",
      "Iteration 13080, loss = 7.72471419\n",
      "Iteration 13081, loss = 7.51406203\n",
      "Iteration 13082, loss = 7.42022078\n",
      "Iteration 13083, loss = 7.09344953\n",
      "Iteration 13084, loss = 7.34198482\n",
      "Iteration 13085, loss = 7.44325841\n",
      "Iteration 13086, loss = 7.72774707\n",
      "Iteration 13087, loss = 7.55516738\n",
      "Iteration 13088, loss = 9.49145200\n",
      "Iteration 13089, loss = 8.45149617\n",
      "Iteration 13090, loss = 7.97553774\n",
      "Iteration 13091, loss = 8.21577712\n",
      "Iteration 13092, loss = 8.13710617\n",
      "Iteration 13093, loss = 9.41557185\n",
      "Iteration 13094, loss = 9.07534896\n",
      "Iteration 13095, loss = 8.25459163\n",
      "Iteration 13096, loss = 7.96621382\n",
      "Iteration 13097, loss = 7.79989246\n",
      "Iteration 13098, loss = 7.67607943\n",
      "Iteration 13099, loss = 8.00776873\n",
      "Iteration 13100, loss = 8.09293901\n",
      "Iteration 13101, loss = 7.88323502\n",
      "Iteration 13102, loss = 8.14372339\n",
      "Iteration 13103, loss = 8.68498339\n",
      "Iteration 13104, loss = 8.58038345\n",
      "Iteration 13105, loss = 7.84319231\n",
      "Iteration 13106, loss = 7.34622462\n",
      "Iteration 13107, loss = 7.47459755\n",
      "Iteration 13108, loss = 7.74474726\n",
      "Iteration 13109, loss = 7.22564720\n",
      "Iteration 13110, loss = 7.14995611\n",
      "Iteration 13111, loss = 7.46980314\n",
      "Iteration 13112, loss = 7.48928129\n",
      "Iteration 13113, loss = 7.74197128\n",
      "Iteration 13114, loss = 7.73777846\n",
      "Iteration 13115, loss = 7.74672353\n",
      "Iteration 13116, loss = 7.53850990\n",
      "Iteration 13117, loss = 7.57146710\n",
      "Iteration 13118, loss = 7.50037622\n",
      "Iteration 13119, loss = 7.18633729\n",
      "Iteration 13120, loss = 7.41284295\n",
      "Iteration 13121, loss = 7.36261625\n",
      "Iteration 13122, loss = 8.64863916\n",
      "Iteration 13123, loss = 8.73644675\n",
      "Iteration 13124, loss = 7.69932732\n",
      "Iteration 13125, loss = 7.33437797\n",
      "Iteration 13126, loss = 7.71753857\n",
      "Iteration 13127, loss = 7.28989698\n",
      "Iteration 13128, loss = 7.45907616\n",
      "Iteration 13129, loss = 7.13387898\n",
      "Iteration 13130, loss = 7.66506751\n",
      "Iteration 13131, loss = 7.55053483\n",
      "Iteration 13132, loss = 7.74912506\n",
      "Iteration 13133, loss = 7.59723875\n",
      "Iteration 13134, loss = 7.25592635\n",
      "Iteration 13135, loss = 7.26813304\n",
      "Iteration 13136, loss = 7.54762244\n",
      "Iteration 13137, loss = 7.25434202\n",
      "Iteration 13138, loss = 7.35619637\n",
      "Iteration 13139, loss = 7.33828720\n",
      "Iteration 13140, loss = 7.37574017\n",
      "Iteration 13141, loss = 7.82631206\n",
      "Iteration 13142, loss = 7.98903580\n",
      "Iteration 13143, loss = 7.90328063\n",
      "Iteration 13144, loss = 7.25005476\n",
      "Iteration 13145, loss = 7.30065885\n",
      "Iteration 13146, loss = 8.04592334\n",
      "Iteration 13147, loss = 7.62200148\n",
      "Iteration 13148, loss = 8.08983452\n",
      "Iteration 13149, loss = 8.11000394\n",
      "Iteration 13150, loss = 7.71783169\n",
      "Iteration 13151, loss = 7.79481898\n",
      "Iteration 13152, loss = 8.26708401\n",
      "Iteration 13153, loss = 7.97571240\n",
      "Iteration 13154, loss = 7.73133695\n",
      "Iteration 13155, loss = 7.93462480\n",
      "Iteration 13156, loss = 7.66707449\n",
      "Iteration 13157, loss = 7.73649786\n",
      "Iteration 13158, loss = 7.80141737\n",
      "Iteration 13159, loss = 8.01108382\n",
      "Iteration 13160, loss = 7.63523245\n",
      "Iteration 13161, loss = 7.37450448\n",
      "Iteration 13162, loss = 7.65516504\n",
      "Iteration 13163, loss = 7.22623913\n",
      "Iteration 13164, loss = 7.67681515\n",
      "Iteration 13165, loss = 7.58001152\n",
      "Iteration 13166, loss = 7.84411713\n",
      "Iteration 13167, loss = 8.53578420\n",
      "Iteration 13168, loss = 9.72820222\n",
      "Iteration 13169, loss = 8.84362351\n",
      "Iteration 13170, loss = 8.74593102\n",
      "Iteration 13171, loss = 9.79730144\n",
      "Iteration 13172, loss = 9.60486873\n",
      "Iteration 13173, loss = 8.82293200\n",
      "Iteration 13174, loss = 8.48210570\n",
      "Iteration 13175, loss = 8.23620416\n",
      "Iteration 13176, loss = 8.42643959\n",
      "Iteration 13177, loss = 7.22844062\n",
      "Iteration 13178, loss = 7.54336213\n",
      "Iteration 13179, loss = 7.39993868\n",
      "Iteration 13180, loss = 7.50167893\n",
      "Iteration 13181, loss = 7.44593765\n",
      "Iteration 13182, loss = 7.73674049\n",
      "Iteration 13183, loss = 7.57666820\n",
      "Iteration 13184, loss = 7.89159013\n",
      "Iteration 13185, loss = 7.85732192\n",
      "Iteration 13186, loss = 7.51207763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13187, loss = 7.48904467\n",
      "Iteration 13188, loss = 7.68124534\n",
      "Iteration 13189, loss = 7.77071145\n",
      "Iteration 13190, loss = 7.54843488\n",
      "Iteration 13191, loss = 7.51695869\n",
      "Iteration 13192, loss = 8.38046342\n",
      "Iteration 13193, loss = 9.38853619\n",
      "Iteration 13194, loss = 8.99620781\n",
      "Iteration 13195, loss = 9.41706316\n",
      "Iteration 13196, loss = 9.00983594\n",
      "Iteration 13197, loss = 8.98939879\n",
      "Iteration 13198, loss = 9.38972604\n",
      "Iteration 13199, loss = 8.35891764\n",
      "Iteration 13200, loss = 8.17139367\n",
      "Iteration 13201, loss = 8.42234114\n",
      "Iteration 13202, loss = 7.68062156\n",
      "Iteration 13203, loss = 8.07883942\n",
      "Iteration 13204, loss = 7.93982569\n",
      "Iteration 13205, loss = 8.12693313\n",
      "Iteration 13206, loss = 8.27626415\n",
      "Iteration 13207, loss = 8.69760197\n",
      "Iteration 13208, loss = 9.81443722\n",
      "Iteration 13209, loss = 9.53075715\n",
      "Iteration 13210, loss = 9.46622175\n",
      "Iteration 13211, loss = 10.57682284\n",
      "Iteration 13212, loss = 9.13030464\n",
      "Iteration 13213, loss = 9.52286074\n",
      "Iteration 13214, loss = 8.10210899\n",
      "Iteration 13215, loss = 8.12139142\n",
      "Iteration 13216, loss = 8.25616744\n",
      "Iteration 13217, loss = 7.93691602\n",
      "Iteration 13218, loss = 8.57942432\n",
      "Iteration 13219, loss = 7.62449575\n",
      "Iteration 13220, loss = 7.49154433\n",
      "Iteration 13221, loss = 7.44298732\n",
      "Iteration 13222, loss = 8.39715355\n",
      "Iteration 13223, loss = 7.57905585\n",
      "Iteration 13224, loss = 7.47336526\n",
      "Iteration 13225, loss = 7.14149444\n",
      "Iteration 13226, loss = 7.83267852\n",
      "Iteration 13227, loss = 7.36402511\n",
      "Iteration 13228, loss = 7.38888836\n",
      "Iteration 13229, loss = 7.77193273\n",
      "Iteration 13230, loss = 7.50956663\n",
      "Iteration 13231, loss = 7.73383017\n",
      "Iteration 13232, loss = 8.03724301\n",
      "Iteration 13233, loss = 8.12513863\n",
      "Iteration 13234, loss = 8.08424274\n",
      "Iteration 13235, loss = 8.60320890\n",
      "Iteration 13236, loss = 7.94765502\n",
      "Iteration 13237, loss = 8.09286642\n",
      "Iteration 13238, loss = 7.50468887\n",
      "Iteration 13239, loss = 7.85105110\n",
      "Iteration 13240, loss = 8.36947271\n",
      "Iteration 13241, loss = 7.97644291\n",
      "Iteration 13242, loss = 8.22780533\n",
      "Iteration 13243, loss = 7.50261089\n",
      "Iteration 13244, loss = 7.77322818\n",
      "Iteration 13245, loss = 8.27709999\n",
      "Iteration 13246, loss = 7.88885120\n",
      "Iteration 13247, loss = 8.98502979\n",
      "Iteration 13248, loss = 8.35560911\n",
      "Iteration 13249, loss = 7.65593525\n",
      "Iteration 13250, loss = 8.85534349\n",
      "Iteration 13251, loss = 9.60659075\n",
      "Iteration 13252, loss = 9.47734451\n",
      "Iteration 13253, loss = 8.24543597\n",
      "Iteration 13254, loss = 7.53557769\n",
      "Iteration 13255, loss = 7.56809024\n",
      "Iteration 13256, loss = 7.20908338\n",
      "Iteration 13257, loss = 7.34139548\n",
      "Iteration 13258, loss = 7.46021708\n",
      "Iteration 13259, loss = 7.40947378\n",
      "Iteration 13260, loss = 7.14683940\n",
      "Iteration 13261, loss = 7.25583852\n",
      "Iteration 13262, loss = 8.02076462\n",
      "Iteration 13263, loss = 8.54032298\n",
      "Iteration 13264, loss = 8.98560708\n",
      "Iteration 13265, loss = 8.71705241\n",
      "Iteration 13266, loss = 7.20980757\n",
      "Iteration 13267, loss = 7.46575200\n",
      "Iteration 13268, loss = 7.72485893\n",
      "Iteration 13269, loss = 7.61376274\n",
      "Iteration 13270, loss = 7.34990592\n",
      "Iteration 13271, loss = 7.28105031\n",
      "Iteration 13272, loss = 7.29549177\n",
      "Iteration 13273, loss = 7.28957174\n",
      "Iteration 13274, loss = 7.67550298\n",
      "Iteration 13275, loss = 7.57112056\n",
      "Iteration 13276, loss = 7.67631989\n",
      "Iteration 13277, loss = 7.44978367\n",
      "Iteration 13278, loss = 7.82240339\n",
      "Iteration 13279, loss = 7.59077055\n",
      "Iteration 13280, loss = 7.16631436\n",
      "Iteration 13281, loss = 7.79481183\n",
      "Iteration 13282, loss = 8.17156406\n",
      "Iteration 13283, loss = 8.00236774\n",
      "Iteration 13284, loss = 8.11414345\n",
      "Iteration 13285, loss = 7.99206519\n",
      "Iteration 13286, loss = 7.82896947\n",
      "Iteration 13287, loss = 7.06134186\n",
      "Iteration 13288, loss = 7.22649742\n",
      "Iteration 13289, loss = 7.50015954\n",
      "Iteration 13290, loss = 7.42010938\n",
      "Iteration 13291, loss = 7.21559985\n",
      "Iteration 13292, loss = 7.52008582\n",
      "Iteration 13293, loss = 7.97859902\n",
      "Iteration 13294, loss = 7.49415275\n",
      "Iteration 13295, loss = 7.59737314\n",
      "Iteration 13296, loss = 7.94312344\n",
      "Iteration 13297, loss = 7.78968729\n",
      "Iteration 13298, loss = 7.54682362\n",
      "Iteration 13299, loss = 7.99885993\n",
      "Iteration 13300, loss = 7.71550892\n",
      "Iteration 13301, loss = 7.49194495\n",
      "Iteration 13302, loss = 7.46135154\n",
      "Iteration 13303, loss = 8.22332718\n",
      "Iteration 13304, loss = 7.72157532\n",
      "Iteration 13305, loss = 7.45655905\n",
      "Iteration 13306, loss = 8.63760252\n",
      "Iteration 13307, loss = 7.81951851\n",
      "Iteration 13308, loss = 7.33467739\n",
      "Iteration 13309, loss = 8.73940007\n",
      "Iteration 13310, loss = 7.98175747\n",
      "Iteration 13311, loss = 7.83563226\n",
      "Iteration 13312, loss = 8.03596668\n",
      "Iteration 13313, loss = 8.28569698\n",
      "Iteration 13314, loss = 8.16787919\n",
      "Iteration 13315, loss = 8.20855070\n",
      "Iteration 13316, loss = 7.72873814\n",
      "Iteration 13317, loss = 7.73436863\n",
      "Iteration 13318, loss = 7.60078114\n",
      "Iteration 13319, loss = 8.41632865\n",
      "Iteration 13320, loss = 7.60503364\n",
      "Iteration 13321, loss = 7.37454297\n",
      "Iteration 13322, loss = 7.70133843\n",
      "Iteration 13323, loss = 7.34250275\n",
      "Iteration 13324, loss = 7.25740222\n",
      "Iteration 13325, loss = 7.22767212\n",
      "Iteration 13326, loss = 7.65824147\n",
      "Iteration 13327, loss = 7.59701266\n",
      "Iteration 13328, loss = 7.59147696\n",
      "Iteration 13329, loss = 8.05458921\n",
      "Iteration 13330, loss = 7.79528388\n",
      "Iteration 13331, loss = 9.04963510\n",
      "Iteration 13332, loss = 8.35209019\n",
      "Iteration 13333, loss = 8.05893264\n",
      "Iteration 13334, loss = 7.94845854\n",
      "Iteration 13335, loss = 8.50748283\n",
      "Iteration 13336, loss = 8.15893181\n",
      "Iteration 13337, loss = 7.29594856\n",
      "Iteration 13338, loss = 7.45897355\n",
      "Iteration 13339, loss = 7.34130286\n",
      "Iteration 13340, loss = 7.40899118\n",
      "Iteration 13341, loss = 8.15114736\n",
      "Iteration 13342, loss = 8.21997456\n",
      "Iteration 13343, loss = 8.45897705\n",
      "Iteration 13344, loss = 7.81284211\n",
      "Iteration 13345, loss = 7.43534936\n",
      "Iteration 13346, loss = 7.34352316\n",
      "Iteration 13347, loss = 7.59620917\n",
      "Iteration 13348, loss = 8.36929121\n",
      "Iteration 13349, loss = 8.62724297\n",
      "Iteration 13350, loss = 9.23530189\n",
      "Iteration 13351, loss = 9.17298927\n",
      "Iteration 13352, loss = 7.26603343\n",
      "Iteration 13353, loss = 7.50676443\n",
      "Iteration 13354, loss = 7.45902748\n",
      "Iteration 13355, loss = 7.10828735\n",
      "Iteration 13356, loss = 7.12288599\n",
      "Iteration 13357, loss = 7.33254322\n",
      "Iteration 13358, loss = 7.24107831\n",
      "Iteration 13359, loss = 8.18857564\n",
      "Iteration 13360, loss = 8.34726124\n",
      "Iteration 13361, loss = 8.04162217\n",
      "Iteration 13362, loss = 7.72312234\n",
      "Iteration 13363, loss = 7.55995933\n",
      "Iteration 13364, loss = 8.22483313\n",
      "Iteration 13365, loss = 8.11758079\n",
      "Iteration 13366, loss = 9.51143469\n",
      "Iteration 13367, loss = 9.34010018\n",
      "Iteration 13368, loss = 8.96365448\n",
      "Iteration 13369, loss = 8.46451871\n",
      "Iteration 13370, loss = 8.06675304\n",
      "Iteration 13371, loss = 7.93733693\n",
      "Iteration 13372, loss = 7.26162357\n",
      "Iteration 13373, loss = 7.73444104\n",
      "Iteration 13374, loss = 8.96557449\n",
      "Iteration 13375, loss = 8.47627706\n",
      "Iteration 13376, loss = 8.83217133\n",
      "Iteration 13377, loss = 7.97545573\n",
      "Iteration 13378, loss = 7.78430114\n",
      "Iteration 13379, loss = 8.13016060\n",
      "Iteration 13380, loss = 8.38524545\n",
      "Iteration 13381, loss = 8.17360668\n",
      "Iteration 13382, loss = 8.05988243\n",
      "Iteration 13383, loss = 7.70532470\n",
      "Iteration 13384, loss = 7.94553514\n",
      "Iteration 13385, loss = 7.76099445\n",
      "Iteration 13386, loss = 7.93099929\n",
      "Iteration 13387, loss = 8.35253794\n",
      "Iteration 13388, loss = 8.07685937\n",
      "Iteration 13389, loss = 7.87911902\n",
      "Iteration 13390, loss = 7.84562105\n",
      "Iteration 13391, loss = 7.23257989\n",
      "Iteration 13392, loss = 8.18384593\n",
      "Iteration 13393, loss = 8.56749713\n",
      "Iteration 13394, loss = 8.96810702\n",
      "Iteration 13395, loss = 10.16399569\n",
      "Iteration 13396, loss = 8.60040898\n",
      "Iteration 13397, loss = 8.58815996\n",
      "Iteration 13398, loss = 8.13491617\n",
      "Iteration 13399, loss = 7.74859623\n",
      "Iteration 13400, loss = 10.80142187\n",
      "Iteration 13401, loss = 8.19291593\n",
      "Iteration 13402, loss = 8.98007505\n",
      "Iteration 13403, loss = 8.36686598\n",
      "Iteration 13404, loss = 8.96703093\n",
      "Iteration 13405, loss = 8.43339679\n",
      "Iteration 13406, loss = 7.63895409\n",
      "Iteration 13407, loss = 8.05368672\n",
      "Iteration 13408, loss = 8.14449875\n",
      "Iteration 13409, loss = 7.40113831\n",
      "Iteration 13410, loss = 8.99853990\n",
      "Iteration 13411, loss = 8.86867122\n",
      "Iteration 13412, loss = 7.89459740\n",
      "Iteration 13413, loss = 7.94950241\n",
      "Iteration 13414, loss = 9.01447182\n",
      "Iteration 13415, loss = 10.39127236\n",
      "Iteration 13416, loss = 8.89584727\n",
      "Iteration 13417, loss = 11.23548348\n",
      "Iteration 13418, loss = 10.56782364\n",
      "Iteration 13419, loss = 10.12526079\n",
      "Iteration 13420, loss = 8.12415157\n",
      "Iteration 13421, loss = 7.99003454\n",
      "Iteration 13422, loss = 7.54330876\n",
      "Iteration 13423, loss = 7.43587580\n",
      "Iteration 13424, loss = 7.25044692\n",
      "Iteration 13425, loss = 7.20331789\n",
      "Iteration 13426, loss = 7.20940604\n",
      "Iteration 13427, loss = 7.23112383\n",
      "Iteration 13428, loss = 7.00179294\n",
      "Iteration 13429, loss = 7.03290797\n",
      "Iteration 13430, loss = 7.57630884\n",
      "Iteration 13431, loss = 7.53669761\n",
      "Iteration 13432, loss = 7.46128404\n",
      "Iteration 13433, loss = 8.18812068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13434, loss = 8.23982582\n",
      "Iteration 13435, loss = 7.68489058\n",
      "Iteration 13436, loss = 7.47250335\n",
      "Iteration 13437, loss = 7.24031726\n",
      "Iteration 13438, loss = 7.46254048\n",
      "Iteration 13439, loss = 7.13573223\n",
      "Iteration 13440, loss = 7.11712504\n",
      "Iteration 13441, loss = 7.03781769\n",
      "Iteration 13442, loss = 7.12344245\n",
      "Iteration 13443, loss = 7.64699017\n",
      "Iteration 13444, loss = 8.89170319\n",
      "Iteration 13445, loss = 7.53788211\n",
      "Iteration 13446, loss = 7.16027482\n",
      "Iteration 13447, loss = 7.47225939\n",
      "Iteration 13448, loss = 7.55926746\n",
      "Iteration 13449, loss = 7.32040119\n",
      "Iteration 13450, loss = 7.07221381\n",
      "Iteration 13451, loss = 7.03021900\n",
      "Iteration 13452, loss = 7.36000920\n",
      "Iteration 13453, loss = 8.16269715\n",
      "Iteration 13454, loss = 7.96601054\n",
      "Iteration 13455, loss = 7.66749273\n",
      "Iteration 13456, loss = 7.83049440\n",
      "Iteration 13457, loss = 7.56888545\n",
      "Iteration 13458, loss = 7.67798275\n",
      "Iteration 13459, loss = 7.46183414\n",
      "Iteration 13460, loss = 7.92237093\n",
      "Iteration 13461, loss = 7.71991419\n",
      "Iteration 13462, loss = 7.53442440\n",
      "Iteration 13463, loss = 8.64855333\n",
      "Iteration 13464, loss = 8.68003756\n",
      "Iteration 13465, loss = 8.14019255\n",
      "Iteration 13466, loss = 8.39781835\n",
      "Iteration 13467, loss = 8.08743309\n",
      "Iteration 13468, loss = 8.80965040\n",
      "Iteration 13469, loss = 7.34937185\n",
      "Iteration 13470, loss = 7.51726580\n",
      "Iteration 13471, loss = 7.00828046\n",
      "Iteration 13472, loss = 7.19826540\n",
      "Iteration 13473, loss = 8.08790055\n",
      "Iteration 13474, loss = 7.38625303\n",
      "Iteration 13475, loss = 8.04895098\n",
      "Iteration 13476, loss = 7.81918874\n",
      "Iteration 13477, loss = 9.14537254\n",
      "Iteration 13478, loss = 8.72129931\n",
      "Iteration 13479, loss = 7.45367615\n",
      "Iteration 13480, loss = 7.17083960\n",
      "Iteration 13481, loss = 7.88268063\n",
      "Iteration 13482, loss = 7.11130900\n",
      "Iteration 13483, loss = 7.99529131\n",
      "Iteration 13484, loss = 7.08630663\n",
      "Iteration 13485, loss = 7.36656638\n",
      "Iteration 13486, loss = 7.45720290\n",
      "Iteration 13487, loss = 8.03775542\n",
      "Iteration 13488, loss = 7.50848820\n",
      "Iteration 13489, loss = 7.51943175\n",
      "Iteration 13490, loss = 7.44405754\n",
      "Iteration 13491, loss = 8.34588163\n",
      "Iteration 13492, loss = 7.93822999\n",
      "Iteration 13493, loss = 8.22440338\n",
      "Iteration 13494, loss = 8.27305726\n",
      "Iteration 13495, loss = 7.43539166\n",
      "Iteration 13496, loss = 8.56001904\n",
      "Iteration 13497, loss = 8.00553901\n",
      "Iteration 13498, loss = 8.75122315\n",
      "Iteration 13499, loss = 7.71329915\n",
      "Iteration 13500, loss = 7.45275757\n",
      "Iteration 13501, loss = 7.58110316\n",
      "Iteration 13502, loss = 7.64574150\n",
      "Iteration 13503, loss = 8.49402834\n",
      "Iteration 13504, loss = 7.90955435\n",
      "Iteration 13505, loss = 8.46539978\n",
      "Iteration 13506, loss = 7.80763136\n",
      "Iteration 13507, loss = 9.09671184\n",
      "Iteration 13508, loss = 8.45987557\n",
      "Iteration 13509, loss = 7.97796325\n",
      "Iteration 13510, loss = 8.89667890\n",
      "Iteration 13511, loss = 7.99044593\n",
      "Iteration 13512, loss = 7.86322724\n",
      "Iteration 13513, loss = 7.72802874\n",
      "Iteration 13514, loss = 8.86707460\n",
      "Iteration 13515, loss = 7.82915579\n",
      "Iteration 13516, loss = 8.88164653\n",
      "Iteration 13517, loss = 8.12307952\n",
      "Iteration 13518, loss = 8.06675544\n",
      "Iteration 13519, loss = 7.88546840\n",
      "Iteration 13520, loss = 7.66153164\n",
      "Iteration 13521, loss = 7.31306612\n",
      "Iteration 13522, loss = 8.35909876\n",
      "Iteration 13523, loss = 7.14785592\n",
      "Iteration 13524, loss = 7.37789282\n",
      "Iteration 13525, loss = 7.51624294\n",
      "Iteration 13526, loss = 7.73092713\n",
      "Iteration 13527, loss = 8.52572694\n",
      "Iteration 13528, loss = 7.68043397\n",
      "Iteration 13529, loss = 7.06574183\n",
      "Iteration 13530, loss = 7.32860465\n",
      "Iteration 13531, loss = 7.48533064\n",
      "Iteration 13532, loss = 7.47697001\n",
      "Iteration 13533, loss = 8.07857090\n",
      "Iteration 13534, loss = 9.26669765\n",
      "Iteration 13535, loss = 8.35408480\n",
      "Iteration 13536, loss = 7.25030307\n",
      "Iteration 13537, loss = 8.54950767\n",
      "Iteration 13538, loss = 8.95284566\n",
      "Iteration 13539, loss = 8.02765041\n",
      "Iteration 13540, loss = 7.31435797\n",
      "Iteration 13541, loss = 7.78934272\n",
      "Iteration 13542, loss = 8.09967762\n",
      "Iteration 13543, loss = 7.33174373\n",
      "Iteration 13544, loss = 7.15320944\n",
      "Iteration 13545, loss = 7.39228767\n",
      "Iteration 13546, loss = 7.15069984\n",
      "Iteration 13547, loss = 7.46753017\n",
      "Iteration 13548, loss = 7.61592248\n",
      "Iteration 13549, loss = 8.48301667\n",
      "Iteration 13550, loss = 7.33786958\n",
      "Iteration 13551, loss = 7.46276109\n",
      "Iteration 13552, loss = 8.05243290\n",
      "Iteration 13553, loss = 8.52368039\n",
      "Iteration 13554, loss = 8.04025135\n",
      "Iteration 13555, loss = 7.41064351\n",
      "Iteration 13556, loss = 8.58301681\n",
      "Iteration 13557, loss = 9.04776074\n",
      "Iteration 13558, loss = 7.62847140\n",
      "Iteration 13559, loss = 7.39901971\n",
      "Iteration 13560, loss = 7.54516678\n",
      "Iteration 13561, loss = 7.54835517\n",
      "Iteration 13562, loss = 7.65075589\n",
      "Iteration 13563, loss = 7.28665949\n",
      "Iteration 13564, loss = 7.28838134\n",
      "Iteration 13565, loss = 7.15434784\n",
      "Iteration 13566, loss = 7.01050848\n",
      "Iteration 13567, loss = 7.17731349\n",
      "Iteration 13568, loss = 7.24441377\n",
      "Iteration 13569, loss = 7.19572528\n",
      "Iteration 13570, loss = 7.16725794\n",
      "Iteration 13571, loss = 7.24105806\n",
      "Iteration 13572, loss = 7.47203731\n",
      "Iteration 13573, loss = 7.47008438\n",
      "Iteration 13574, loss = 7.15049662\n",
      "Iteration 13575, loss = 8.09172914\n",
      "Iteration 13576, loss = 7.47810724\n",
      "Iteration 13577, loss = 8.12745599\n",
      "Iteration 13578, loss = 9.20094108\n",
      "Iteration 13579, loss = 9.52081057\n",
      "Iteration 13580, loss = 7.44500446\n",
      "Iteration 13581, loss = 7.45138916\n",
      "Iteration 13582, loss = 7.92850766\n",
      "Iteration 13583, loss = 8.23659472\n",
      "Iteration 13584, loss = 7.63739256\n",
      "Iteration 13585, loss = 7.50612021\n",
      "Iteration 13586, loss = 7.34303070\n",
      "Iteration 13587, loss = 7.18274940\n",
      "Iteration 13588, loss = 7.63115108\n",
      "Iteration 13589, loss = 8.43986699\n",
      "Iteration 13590, loss = 8.04037449\n",
      "Iteration 13591, loss = 8.17509668\n",
      "Iteration 13592, loss = 8.98294146\n",
      "Iteration 13593, loss = 8.27804694\n",
      "Iteration 13594, loss = 8.40293193\n",
      "Iteration 13595, loss = 7.69661088\n",
      "Iteration 13596, loss = 7.56546431\n",
      "Iteration 13597, loss = 7.52042586\n",
      "Iteration 13598, loss = 6.93409896\n",
      "Iteration 13599, loss = 7.11236619\n",
      "Iteration 13600, loss = 7.16781149\n",
      "Iteration 13601, loss = 7.61618468\n",
      "Iteration 13602, loss = 7.17436450\n",
      "Iteration 13603, loss = 7.78999691\n",
      "Iteration 13604, loss = 7.75269507\n",
      "Iteration 13605, loss = 7.70068405\n",
      "Iteration 13606, loss = 8.11726287\n",
      "Iteration 13607, loss = 8.26246905\n",
      "Iteration 13608, loss = 7.63299526\n",
      "Iteration 13609, loss = 8.19353317\n",
      "Iteration 13610, loss = 7.72655370\n",
      "Iteration 13611, loss = 8.69690660\n",
      "Iteration 13612, loss = 8.07082607\n",
      "Iteration 13613, loss = 7.63248925\n",
      "Iteration 13614, loss = 7.44345050\n",
      "Iteration 13615, loss = 7.18414222\n",
      "Iteration 13616, loss = 7.58667493\n",
      "Iteration 13617, loss = 8.16077423\n",
      "Iteration 13618, loss = 8.40959786\n",
      "Iteration 13619, loss = 7.50345573\n",
      "Iteration 13620, loss = 7.67403041\n",
      "Iteration 13621, loss = 7.98274137\n",
      "Iteration 13622, loss = 8.53808965\n",
      "Iteration 13623, loss = 8.66155526\n",
      "Iteration 13624, loss = 8.98848251\n",
      "Iteration 13625, loss = 8.47534807\n",
      "Iteration 13626, loss = 8.31674241\n",
      "Iteration 13627, loss = 7.13129294\n",
      "Iteration 13628, loss = 7.48374884\n",
      "Iteration 13629, loss = 7.17763110\n",
      "Iteration 13630, loss = 7.19067622\n",
      "Iteration 13631, loss = 7.64406340\n",
      "Iteration 13632, loss = 7.64497877\n",
      "Iteration 13633, loss = 7.77824341\n",
      "Iteration 13634, loss = 8.01181232\n",
      "Iteration 13635, loss = 7.81183008\n",
      "Iteration 13636, loss = 8.05416633\n",
      "Iteration 13637, loss = 7.99173504\n",
      "Iteration 13638, loss = 7.20679070\n",
      "Iteration 13639, loss = 7.20170358\n",
      "Iteration 13640, loss = 7.76929088\n",
      "Iteration 13641, loss = 7.37663495\n",
      "Iteration 13642, loss = 7.82063963\n",
      "Iteration 13643, loss = 7.54820274\n",
      "Iteration 13644, loss = 7.62708739\n",
      "Iteration 13645, loss = 7.12293343\n",
      "Iteration 13646, loss = 7.79532337\n",
      "Iteration 13647, loss = 8.35557477\n",
      "Iteration 13648, loss = 9.41545866\n",
      "Iteration 13649, loss = 9.10730636\n",
      "Iteration 13650, loss = 8.95346650\n",
      "Iteration 13651, loss = 7.75196373\n",
      "Iteration 13652, loss = 8.39464464\n",
      "Iteration 13653, loss = 7.64958610\n",
      "Iteration 13654, loss = 7.99490055\n",
      "Iteration 13655, loss = 7.56987891\n",
      "Iteration 13656, loss = 7.10625165\n",
      "Iteration 13657, loss = 7.20950303\n",
      "Iteration 13658, loss = 7.30775192\n",
      "Iteration 13659, loss = 7.15985513\n",
      "Iteration 13660, loss = 7.06992493\n",
      "Iteration 13661, loss = 7.33458571\n",
      "Iteration 13662, loss = 7.33148824\n",
      "Iteration 13663, loss = 7.17524046\n",
      "Iteration 13664, loss = 7.23853237\n",
      "Iteration 13665, loss = 7.80860187\n",
      "Iteration 13666, loss = 7.68803913\n",
      "Iteration 13667, loss = 7.63241653\n",
      "Iteration 13668, loss = 7.72436348\n",
      "Iteration 13669, loss = 8.28711618\n",
      "Iteration 13670, loss = 7.74820120\n",
      "Iteration 13671, loss = 7.55847188\n",
      "Iteration 13672, loss = 7.31863538\n",
      "Iteration 13673, loss = 7.69657989\n",
      "Iteration 13674, loss = 7.56192212\n",
      "Iteration 13675, loss = 7.34359017\n",
      "Iteration 13676, loss = 7.27448392\n",
      "Iteration 13677, loss = 7.32511796\n",
      "Iteration 13678, loss = 7.71498331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13679, loss = 7.81106787\n",
      "Iteration 13680, loss = 7.77833586\n",
      "Iteration 13681, loss = 7.94432032\n",
      "Iteration 13682, loss = 7.25270788\n",
      "Iteration 13683, loss = 7.33472172\n",
      "Iteration 13684, loss = 7.26558533\n",
      "Iteration 13685, loss = 7.00399657\n",
      "Iteration 13686, loss = 7.11667437\n",
      "Iteration 13687, loss = 7.32937828\n",
      "Iteration 13688, loss = 6.89527071\n",
      "Iteration 13689, loss = 8.16136695\n",
      "Iteration 13690, loss = 7.86427327\n",
      "Iteration 13691, loss = 7.66928275\n",
      "Iteration 13692, loss = 8.20189157\n",
      "Iteration 13693, loss = 7.54952981\n",
      "Iteration 13694, loss = 7.32846180\n",
      "Iteration 13695, loss = 7.86493280\n",
      "Iteration 13696, loss = 8.43030186\n",
      "Iteration 13697, loss = 7.35432428\n",
      "Iteration 13698, loss = 7.38917216\n",
      "Iteration 13699, loss = 7.00359046\n",
      "Iteration 13700, loss = 7.46407202\n",
      "Iteration 13701, loss = 7.80345048\n",
      "Iteration 13702, loss = 7.82833113\n",
      "Iteration 13703, loss = 7.94925589\n",
      "Iteration 13704, loss = 7.47411448\n",
      "Iteration 13705, loss = 7.13609943\n",
      "Iteration 13706, loss = 7.34635756\n",
      "Iteration 13707, loss = 7.87148036\n",
      "Iteration 13708, loss = 7.55358521\n",
      "Iteration 13709, loss = 7.16999921\n",
      "Iteration 13710, loss = 7.97779479\n",
      "Iteration 13711, loss = 7.43389190\n",
      "Iteration 13712, loss = 7.64627442\n",
      "Iteration 13713, loss = 7.81136085\n",
      "Iteration 13714, loss = 8.30652138\n",
      "Iteration 13715, loss = 7.27112932\n",
      "Iteration 13716, loss = 7.02555970\n",
      "Iteration 13717, loss = 7.04886376\n",
      "Iteration 13718, loss = 6.94882082\n",
      "Iteration 13719, loss = 7.00922924\n",
      "Iteration 13720, loss = 7.10293867\n",
      "Iteration 13721, loss = 7.50248086\n",
      "Iteration 13722, loss = 7.09617288\n",
      "Iteration 13723, loss = 7.19001432\n",
      "Iteration 13724, loss = 7.47949084\n",
      "Iteration 13725, loss = 7.04041079\n",
      "Iteration 13726, loss = 7.09265846\n",
      "Iteration 13727, loss = 7.20605950\n",
      "Iteration 13728, loss = 6.95429664\n",
      "Iteration 13729, loss = 7.66176385\n",
      "Iteration 13730, loss = 7.29051671\n",
      "Iteration 13731, loss = 7.75570015\n",
      "Iteration 13732, loss = 7.25705124\n",
      "Iteration 13733, loss = 7.10805456\n",
      "Iteration 13734, loss = 7.31455632\n",
      "Iteration 13735, loss = 7.74437423\n",
      "Iteration 13736, loss = 7.93814777\n",
      "Iteration 13737, loss = 7.36557179\n",
      "Iteration 13738, loss = 7.46466229\n",
      "Iteration 13739, loss = 7.34677101\n",
      "Iteration 13740, loss = 8.05108202\n",
      "Iteration 13741, loss = 8.79095527\n",
      "Iteration 13742, loss = 8.30434445\n",
      "Iteration 13743, loss = 7.99534846\n",
      "Iteration 13744, loss = 7.44480782\n",
      "Iteration 13745, loss = 7.65512665\n",
      "Iteration 13746, loss = 7.47386190\n",
      "Iteration 13747, loss = 7.31845297\n",
      "Iteration 13748, loss = 7.58562940\n",
      "Iteration 13749, loss = 7.36554366\n",
      "Iteration 13750, loss = 7.77465610\n",
      "Iteration 13751, loss = 8.04672465\n",
      "Iteration 13752, loss = 7.75880106\n",
      "Iteration 13753, loss = 7.66091251\n",
      "Iteration 13754, loss = 8.11355043\n",
      "Iteration 13755, loss = 8.14165254\n",
      "Iteration 13756, loss = 9.48156712\n",
      "Iteration 13757, loss = 8.24346384\n",
      "Iteration 13758, loss = 7.57046232\n",
      "Iteration 13759, loss = 7.63584247\n",
      "Iteration 13760, loss = 7.23076818\n",
      "Iteration 13761, loss = 7.19397575\n",
      "Iteration 13762, loss = 7.23199237\n",
      "Iteration 13763, loss = 8.09851954\n",
      "Iteration 13764, loss = 7.69194214\n",
      "Iteration 13765, loss = 7.48871635\n",
      "Iteration 13766, loss = 7.54188003\n",
      "Iteration 13767, loss = 7.28754508\n",
      "Iteration 13768, loss = 7.05224243\n",
      "Iteration 13769, loss = 6.97288388\n",
      "Iteration 13770, loss = 7.23925373\n",
      "Iteration 13771, loss = 7.80206996\n",
      "Iteration 13772, loss = 8.31299302\n",
      "Iteration 13773, loss = 8.03126842\n",
      "Iteration 13774, loss = 9.13551718\n",
      "Iteration 13775, loss = 9.72388396\n",
      "Iteration 13776, loss = 7.94265521\n",
      "Iteration 13777, loss = 10.44211049\n",
      "Iteration 13778, loss = 9.12839225\n",
      "Iteration 13779, loss = 8.69702929\n",
      "Iteration 13780, loss = 8.89142091\n",
      "Iteration 13781, loss = 10.12540425\n",
      "Iteration 13782, loss = 10.33829478\n",
      "Iteration 13783, loss = 8.24978612\n",
      "Iteration 13784, loss = 7.37298188\n",
      "Iteration 13785, loss = 8.54017545\n",
      "Iteration 13786, loss = 7.53659160\n",
      "Iteration 13787, loss = 7.60491489\n",
      "Iteration 13788, loss = 7.08293931\n",
      "Iteration 13789, loss = 7.06073772\n",
      "Iteration 13790, loss = 7.48978277\n",
      "Iteration 13791, loss = 6.98754478\n",
      "Iteration 13792, loss = 7.02178341\n",
      "Iteration 13793, loss = 7.50498745\n",
      "Iteration 13794, loss = 7.49232780\n",
      "Iteration 13795, loss = 8.57070641\n",
      "Iteration 13796, loss = 9.04603102\n",
      "Iteration 13797, loss = 7.26187030\n",
      "Iteration 13798, loss = 7.08751378\n",
      "Iteration 13799, loss = 7.13395739\n",
      "Iteration 13800, loss = 7.40804111\n",
      "Iteration 13801, loss = 8.31539408\n",
      "Iteration 13802, loss = 8.38586387\n",
      "Iteration 13803, loss = 8.15716062\n",
      "Iteration 13804, loss = 8.19158054\n",
      "Iteration 13805, loss = 8.14593567\n",
      "Iteration 13806, loss = 8.89025935\n",
      "Iteration 13807, loss = 7.97322974\n",
      "Iteration 13808, loss = 7.74359448\n",
      "Iteration 13809, loss = 7.51735309\n",
      "Iteration 13810, loss = 7.25734803\n",
      "Iteration 13811, loss = 8.01728826\n",
      "Iteration 13812, loss = 8.21474752\n",
      "Iteration 13813, loss = 8.06820603\n",
      "Iteration 13814, loss = 7.43261577\n",
      "Iteration 13815, loss = 7.32558587\n",
      "Iteration 13816, loss = 7.17055679\n",
      "Iteration 13817, loss = 7.73014631\n",
      "Iteration 13818, loss = 7.66044275\n",
      "Iteration 13819, loss = 7.94175514\n",
      "Iteration 13820, loss = 8.46219088\n",
      "Iteration 13821, loss = 8.45381040\n",
      "Iteration 13822, loss = 7.47864252\n",
      "Iteration 13823, loss = 7.33710158\n",
      "Iteration 13824, loss = 8.05884298\n",
      "Iteration 13825, loss = 7.36941698\n",
      "Iteration 13826, loss = 7.95949725\n",
      "Iteration 13827, loss = 8.03390707\n",
      "Iteration 13828, loss = 7.07961848\n",
      "Iteration 13829, loss = 7.00413058\n",
      "Iteration 13830, loss = 7.49179628\n",
      "Iteration 13831, loss = 7.04442794\n",
      "Iteration 13832, loss = 7.30209321\n",
      "Iteration 13833, loss = 7.18992628\n",
      "Iteration 13834, loss = 7.16150718\n",
      "Iteration 13835, loss = 7.27163756\n",
      "Iteration 13836, loss = 7.65336213\n",
      "Iteration 13837, loss = 7.84178186\n",
      "Iteration 13838, loss = 7.33937741\n",
      "Iteration 13839, loss = 7.49820534\n",
      "Iteration 13840, loss = 7.30835177\n",
      "Iteration 13841, loss = 7.03137290\n",
      "Iteration 13842, loss = 7.55664883\n",
      "Iteration 13843, loss = 7.99574244\n",
      "Iteration 13844, loss = 7.78603167\n",
      "Iteration 13845, loss = 8.22276653\n",
      "Iteration 13846, loss = 8.18597532\n",
      "Iteration 13847, loss = 7.07976477\n",
      "Iteration 13848, loss = 7.28774748\n",
      "Iteration 13849, loss = 7.58248803\n",
      "Iteration 13850, loss = 7.64796571\n",
      "Iteration 13851, loss = 7.39723165\n",
      "Iteration 13852, loss = 7.57925798\n",
      "Iteration 13853, loss = 7.15071380\n",
      "Iteration 13854, loss = 8.40872185\n",
      "Iteration 13855, loss = 8.09474395\n",
      "Iteration 13856, loss = 7.83921017\n",
      "Iteration 13857, loss = 7.30402411\n",
      "Iteration 13858, loss = 7.74014107\n",
      "Iteration 13859, loss = 7.75634255\n",
      "Iteration 13860, loss = 8.78744535\n",
      "Iteration 13861, loss = 8.19090881\n",
      "Iteration 13862, loss = 7.56775162\n",
      "Iteration 13863, loss = 8.34500560\n",
      "Iteration 13864, loss = 8.22765939\n",
      "Iteration 13865, loss = 8.16490902\n",
      "Iteration 13866, loss = 7.33569717\n",
      "Iteration 13867, loss = 7.08830340\n",
      "Iteration 13868, loss = 7.31217123\n",
      "Iteration 13869, loss = 8.13934425\n",
      "Iteration 13870, loss = 8.18169146\n",
      "Iteration 13871, loss = 8.74377743\n",
      "Iteration 13872, loss = 8.96936079\n",
      "Iteration 13873, loss = 8.80241596\n",
      "Iteration 13874, loss = 8.97325431\n",
      "Iteration 13875, loss = 9.86626910\n",
      "Iteration 13876, loss = 7.98512359\n",
      "Iteration 13877, loss = 8.28315605\n",
      "Iteration 13878, loss = 7.66458513\n",
      "Iteration 13879, loss = 7.53920052\n",
      "Iteration 13880, loss = 7.39278747\n",
      "Iteration 13881, loss = 7.62253375\n",
      "Iteration 13882, loss = 7.57600873\n",
      "Iteration 13883, loss = 7.62747451\n",
      "Iteration 13884, loss = 9.56202859\n",
      "Iteration 13885, loss = 9.67429374\n",
      "Iteration 13886, loss = 8.36060182\n",
      "Iteration 13887, loss = 7.91419316\n",
      "Iteration 13888, loss = 7.12953983\n",
      "Iteration 13889, loss = 7.14229403\n",
      "Iteration 13890, loss = 7.33718110\n",
      "Iteration 13891, loss = 6.93780857\n",
      "Iteration 13892, loss = 8.25973953\n",
      "Iteration 13893, loss = 8.10040430\n",
      "Iteration 13894, loss = 7.87381789\n",
      "Iteration 13895, loss = 7.63551264\n",
      "Iteration 13896, loss = 7.73453058\n",
      "Iteration 13897, loss = 9.35167728\n",
      "Iteration 13898, loss = 9.08971454\n",
      "Iteration 13899, loss = 8.16371365\n",
      "Iteration 13900, loss = 7.87602073\n",
      "Iteration 13901, loss = 7.71302281\n",
      "Iteration 13902, loss = 7.34116879\n",
      "Iteration 13903, loss = 7.25455382\n",
      "Iteration 13904, loss = 6.99468189\n",
      "Iteration 13905, loss = 7.29504967\n",
      "Iteration 13906, loss = 7.35612745\n",
      "Iteration 13907, loss = 7.39936838\n",
      "Iteration 13908, loss = 7.92671706\n",
      "Iteration 13909, loss = 7.80415070\n",
      "Iteration 13910, loss = 7.08878558\n",
      "Iteration 13911, loss = 7.87176249\n",
      "Iteration 13912, loss = 8.45580686\n",
      "Iteration 13913, loss = 7.62080847\n",
      "Iteration 13914, loss = 7.61654015\n",
      "Iteration 13915, loss = 7.72699829\n",
      "Iteration 13916, loss = 7.69361950\n",
      "Iteration 13917, loss = 7.27495213\n",
      "Iteration 13918, loss = 8.81609439\n",
      "Iteration 13919, loss = 7.47749326\n",
      "Iteration 13920, loss = 7.10429365\n",
      "Iteration 13921, loss = 7.67309055\n",
      "Iteration 13922, loss = 7.36480702\n",
      "Iteration 13923, loss = 8.33790292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13924, loss = 8.56265239\n",
      "Iteration 13925, loss = 9.37178263\n",
      "Iteration 13926, loss = 8.79318137\n",
      "Iteration 13927, loss = 7.46735009\n",
      "Iteration 13928, loss = 8.88469943\n",
      "Iteration 13929, loss = 7.14486378\n",
      "Iteration 13930, loss = 7.95219719\n",
      "Iteration 13931, loss = 7.73704385\n",
      "Iteration 13932, loss = 7.46895267\n",
      "Iteration 13933, loss = 7.50350834\n",
      "Iteration 13934, loss = 7.71635458\n",
      "Iteration 13935, loss = 8.42132007\n",
      "Iteration 13936, loss = 7.57492335\n",
      "Iteration 13937, loss = 7.16482386\n",
      "Iteration 13938, loss = 7.55747167\n",
      "Iteration 13939, loss = 7.13675283\n",
      "Iteration 13940, loss = 7.16006920\n",
      "Iteration 13941, loss = 7.50680880\n",
      "Iteration 13942, loss = 7.56336301\n",
      "Iteration 13943, loss = 7.32027609\n",
      "Iteration 13944, loss = 7.22317359\n",
      "Iteration 13945, loss = 7.63511706\n",
      "Iteration 13946, loss = 7.71656515\n",
      "Iteration 13947, loss = 8.13256717\n",
      "Iteration 13948, loss = 7.13931419\n",
      "Iteration 13949, loss = 7.75395166\n",
      "Iteration 13950, loss = 7.32295215\n",
      "Iteration 13951, loss = 7.00961225\n",
      "Iteration 13952, loss = 7.05245102\n",
      "Iteration 13953, loss = 7.35677573\n",
      "Iteration 13954, loss = 7.47662529\n",
      "Iteration 13955, loss = 7.00104349\n",
      "Iteration 13956, loss = 6.99841446\n",
      "Iteration 13957, loss = 7.03648833\n",
      "Iteration 13958, loss = 6.94070926\n",
      "Iteration 13959, loss = 7.14336208\n",
      "Iteration 13960, loss = 7.39076009\n",
      "Iteration 13961, loss = 7.09165239\n",
      "Iteration 13962, loss = 7.64383619\n",
      "Iteration 13963, loss = 7.46285019\n",
      "Iteration 13964, loss = 7.40313205\n",
      "Iteration 13965, loss = 7.97512038\n",
      "Iteration 13966, loss = 7.61162435\n",
      "Iteration 13967, loss = 7.39358216\n",
      "Iteration 13968, loss = 7.57924407\n",
      "Iteration 13969, loss = 7.33413051\n",
      "Iteration 13970, loss = 7.84920852\n",
      "Iteration 13971, loss = 7.32782079\n",
      "Iteration 13972, loss = 7.46577133\n",
      "Iteration 13973, loss = 7.32490423\n",
      "Iteration 13974, loss = 7.42551582\n",
      "Iteration 13975, loss = 7.10314685\n",
      "Iteration 13976, loss = 6.80328635\n",
      "Iteration 13977, loss = 7.02112122\n",
      "Iteration 13978, loss = 7.14475935\n",
      "Iteration 13979, loss = 7.44371642\n",
      "Iteration 13980, loss = 7.39287362\n",
      "Iteration 13981, loss = 7.52566052\n",
      "Iteration 13982, loss = 6.92022162\n",
      "Iteration 13983, loss = 7.10636385\n",
      "Iteration 13984, loss = 7.12391204\n",
      "Iteration 13985, loss = 7.21574992\n",
      "Iteration 13986, loss = 7.80609257\n",
      "Iteration 13987, loss = 7.32170472\n",
      "Iteration 13988, loss = 7.30623404\n",
      "Iteration 13989, loss = 7.51480575\n",
      "Iteration 13990, loss = 7.54470208\n",
      "Iteration 13991, loss = 7.33795290\n",
      "Iteration 13992, loss = 7.08665325\n",
      "Iteration 13993, loss = 7.12367963\n",
      "Iteration 13994, loss = 7.78980274\n",
      "Iteration 13995, loss = 7.68050217\n",
      "Iteration 13996, loss = 7.01821365\n",
      "Iteration 13997, loss = 7.21175140\n",
      "Iteration 13998, loss = 7.30259078\n",
      "Iteration 13999, loss = 8.53888639\n",
      "Iteration 14000, loss = 7.86861908\n",
      "Iteration 14001, loss = 8.17854754\n",
      "Iteration 14002, loss = 7.62068843\n",
      "Iteration 14003, loss = 7.18250845\n",
      "Iteration 14004, loss = 7.12493689\n",
      "Iteration 14005, loss = 7.89391901\n",
      "Iteration 14006, loss = 7.91451775\n",
      "Iteration 14007, loss = 7.70905189\n",
      "Iteration 14008, loss = 7.71612134\n",
      "Iteration 14009, loss = 7.31584478\n",
      "Iteration 14010, loss = 7.00432079\n",
      "Iteration 14011, loss = 7.68120256\n",
      "Iteration 14012, loss = 6.84953614\n",
      "Iteration 14013, loss = 7.43217920\n",
      "Iteration 14014, loss = 7.33200721\n",
      "Iteration 14015, loss = 7.04971720\n",
      "Iteration 14016, loss = 7.62812982\n",
      "Iteration 14017, loss = 8.24336624\n",
      "Iteration 14018, loss = 7.35398561\n",
      "Iteration 14019, loss = 7.22378720\n",
      "Iteration 14020, loss = 7.54089489\n",
      "Iteration 14021, loss = 7.17778987\n",
      "Iteration 14022, loss = 7.85378646\n",
      "Iteration 14023, loss = 8.45885369\n",
      "Iteration 14024, loss = 7.98674967\n",
      "Iteration 14025, loss = 7.75557532\n",
      "Iteration 14026, loss = 7.61827365\n",
      "Iteration 14027, loss = 7.88227803\n",
      "Iteration 14028, loss = 8.03903267\n",
      "Iteration 14029, loss = 7.32798936\n",
      "Iteration 14030, loss = 7.70002734\n",
      "Iteration 14031, loss = 7.14026399\n",
      "Iteration 14032, loss = 7.12967059\n",
      "Iteration 14033, loss = 8.17431840\n",
      "Iteration 14034, loss = 7.13421344\n",
      "Iteration 14035, loss = 7.99070797\n",
      "Iteration 14036, loss = 7.21292505\n",
      "Iteration 14037, loss = 7.12646791\n",
      "Iteration 14038, loss = 7.13967242\n",
      "Iteration 14039, loss = 7.19521350\n",
      "Iteration 14040, loss = 7.47829304\n",
      "Iteration 14041, loss = 7.28970595\n",
      "Iteration 14042, loss = 8.37481958\n",
      "Iteration 14043, loss = 7.05397362\n",
      "Iteration 14044, loss = 7.35302211\n",
      "Iteration 14045, loss = 7.41607671\n",
      "Iteration 14046, loss = 7.19657642\n",
      "Iteration 14047, loss = 7.79037779\n",
      "Iteration 14048, loss = 8.16021439\n",
      "Iteration 14049, loss = 7.59525527\n",
      "Iteration 14050, loss = 7.55052441\n",
      "Iteration 14051, loss = 7.89793681\n",
      "Iteration 14052, loss = 7.79947566\n",
      "Iteration 14053, loss = 7.20604407\n",
      "Iteration 14054, loss = 6.72447610\n",
      "Iteration 14055, loss = 6.96099556\n",
      "Iteration 14056, loss = 6.86154397\n",
      "Iteration 14057, loss = 7.35409299\n",
      "Iteration 14058, loss = 6.90071988\n",
      "Iteration 14059, loss = 7.21901971\n",
      "Iteration 14060, loss = 6.97524414\n",
      "Iteration 14061, loss = 7.08645245\n",
      "Iteration 14062, loss = 7.43362364\n",
      "Iteration 14063, loss = 7.57659560\n",
      "Iteration 14064, loss = 7.53997211\n",
      "Iteration 14065, loss = 7.37746587\n",
      "Iteration 14066, loss = 7.65142808\n",
      "Iteration 14067, loss = 7.35793676\n",
      "Iteration 14068, loss = 6.98205740\n",
      "Iteration 14069, loss = 7.23878100\n",
      "Iteration 14070, loss = 6.92671388\n",
      "Iteration 14071, loss = 6.90832395\n",
      "Iteration 14072, loss = 7.33085435\n",
      "Iteration 14073, loss = 7.74985268\n",
      "Iteration 14074, loss = 7.97557451\n",
      "Iteration 14075, loss = 7.90514326\n",
      "Iteration 14076, loss = 8.03630971\n",
      "Iteration 14077, loss = 7.38651377\n",
      "Iteration 14078, loss = 7.49481115\n",
      "Iteration 14079, loss = 8.48384302\n",
      "Iteration 14080, loss = 7.50858534\n",
      "Iteration 14081, loss = 7.02226625\n",
      "Iteration 14082, loss = 6.90534780\n",
      "Iteration 14083, loss = 7.20991277\n",
      "Iteration 14084, loss = 7.02892520\n",
      "Iteration 14085, loss = 7.37082080\n",
      "Iteration 14086, loss = 7.03907598\n",
      "Iteration 14087, loss = 7.23135850\n",
      "Iteration 14088, loss = 7.46185658\n",
      "Iteration 14089, loss = 7.61631498\n",
      "Iteration 14090, loss = 7.49423529\n",
      "Iteration 14091, loss = 7.92683940\n",
      "Iteration 14092, loss = 7.59511774\n",
      "Iteration 14093, loss = 7.62854713\n",
      "Iteration 14094, loss = 8.43428501\n",
      "Iteration 14095, loss = 7.70407818\n",
      "Iteration 14096, loss = 7.12753147\n",
      "Iteration 14097, loss = 7.17057456\n",
      "Iteration 14098, loss = 7.25193813\n",
      "Iteration 14099, loss = 7.27217143\n",
      "Iteration 14100, loss = 8.88545010\n",
      "Iteration 14101, loss = 8.35325764\n",
      "Iteration 14102, loss = 7.29243602\n",
      "Iteration 14103, loss = 7.66886702\n",
      "Iteration 14104, loss = 7.92417808\n",
      "Iteration 14105, loss = 7.41168417\n",
      "Iteration 14106, loss = 7.44147958\n",
      "Iteration 14107, loss = 8.16946875\n",
      "Iteration 14108, loss = 8.93010148\n",
      "Iteration 14109, loss = 8.60516150\n",
      "Iteration 14110, loss = 8.71035905\n",
      "Iteration 14111, loss = 7.74795085\n",
      "Iteration 14112, loss = 7.41179847\n",
      "Iteration 14113, loss = 7.65237030\n",
      "Iteration 14114, loss = 7.58211611\n",
      "Iteration 14115, loss = 8.25627251\n",
      "Iteration 14116, loss = 8.37743284\n",
      "Iteration 14117, loss = 8.10416352\n",
      "Iteration 14118, loss = 8.18143492\n",
      "Iteration 14119, loss = 9.03477611\n",
      "Iteration 14120, loss = 8.43881724\n",
      "Iteration 14121, loss = 8.06230115\n",
      "Iteration 14122, loss = 7.83325179\n",
      "Iteration 14123, loss = 7.17571710\n",
      "Iteration 14124, loss = 6.97877128\n",
      "Iteration 14125, loss = 6.98716971\n",
      "Iteration 14126, loss = 7.07680937\n",
      "Iteration 14127, loss = 7.84253568\n",
      "Iteration 14128, loss = 7.53865214\n",
      "Iteration 14129, loss = 7.28738728\n",
      "Iteration 14130, loss = 7.84533526\n",
      "Iteration 14131, loss = 6.94636964\n",
      "Iteration 14132, loss = 7.42082286\n",
      "Iteration 14133, loss = 7.35182345\n",
      "Iteration 14134, loss = 6.76422152\n",
      "Iteration 14135, loss = 7.89756991\n",
      "Iteration 14136, loss = 7.22083403\n",
      "Iteration 14137, loss = 6.80748558\n",
      "Iteration 14138, loss = 7.07304143\n",
      "Iteration 14139, loss = 6.95241967\n",
      "Iteration 14140, loss = 7.07781327\n",
      "Iteration 14141, loss = 7.20157176\n",
      "Iteration 14142, loss = 7.67681590\n",
      "Iteration 14143, loss = 8.03636861\n",
      "Iteration 14144, loss = 7.65180037\n",
      "Iteration 14145, loss = 7.52506854\n",
      "Iteration 14146, loss = 7.93365569\n",
      "Iteration 14147, loss = 8.47357017\n",
      "Iteration 14148, loss = 8.10116978\n",
      "Iteration 14149, loss = 7.54288003\n",
      "Iteration 14150, loss = 7.38195733\n",
      "Iteration 14151, loss = 7.18566086\n",
      "Iteration 14152, loss = 7.24393895\n",
      "Iteration 14153, loss = 6.87041187\n",
      "Iteration 14154, loss = 7.17589342\n",
      "Iteration 14155, loss = 8.44956570\n",
      "Iteration 14156, loss = 8.15315749\n",
      "Iteration 14157, loss = 7.46920486\n",
      "Iteration 14158, loss = 7.91728263\n",
      "Iteration 14159, loss = 7.42558253\n",
      "Iteration 14160, loss = 7.15143111\n",
      "Iteration 14161, loss = 7.20575047\n",
      "Iteration 14162, loss = 7.86194105\n",
      "Iteration 14163, loss = 7.76008003\n",
      "Iteration 14164, loss = 7.18642720\n",
      "Iteration 14165, loss = 7.31646052\n",
      "Iteration 14166, loss = 7.37177823\n",
      "Iteration 14167, loss = 7.26252560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14168, loss = 7.97605456\n",
      "Iteration 14169, loss = 7.81166552\n",
      "Iteration 14170, loss = 7.29269714\n",
      "Iteration 14171, loss = 6.88906455\n",
      "Iteration 14172, loss = 7.17647676\n",
      "Iteration 14173, loss = 7.79915364\n",
      "Iteration 14174, loss = 7.22559110\n",
      "Iteration 14175, loss = 7.18489838\n",
      "Iteration 14176, loss = 8.19664171\n",
      "Iteration 14177, loss = 8.92507449\n",
      "Iteration 14178, loss = 8.47547533\n",
      "Iteration 14179, loss = 7.42756648\n",
      "Iteration 14180, loss = 7.12165932\n",
      "Iteration 14181, loss = 6.97376934\n",
      "Iteration 14182, loss = 7.00330603\n",
      "Iteration 14183, loss = 6.94416954\n",
      "Iteration 14184, loss = 7.03267300\n",
      "Iteration 14185, loss = 6.83175652\n",
      "Iteration 14186, loss = 6.80813432\n",
      "Iteration 14187, loss = 6.80996203\n",
      "Iteration 14188, loss = 6.84084591\n",
      "Iteration 14189, loss = 6.86584338\n",
      "Iteration 14190, loss = 6.85918222\n",
      "Iteration 14191, loss = 6.97845062\n",
      "Iteration 14192, loss = 7.29682935\n",
      "Iteration 14193, loss = 7.17073552\n",
      "Iteration 14194, loss = 6.92600826\n",
      "Iteration 14195, loss = 6.78649601\n",
      "Iteration 14196, loss = 7.10616490\n",
      "Iteration 14197, loss = 6.93306734\n",
      "Iteration 14198, loss = 6.94063756\n",
      "Iteration 14199, loss = 6.98043888\n",
      "Iteration 14200, loss = 7.39667133\n",
      "Iteration 14201, loss = 8.64469444\n",
      "Iteration 14202, loss = 9.50523158\n",
      "Iteration 14203, loss = 11.59408437\n",
      "Iteration 14204, loss = 12.03203578\n",
      "Iteration 14205, loss = 11.72357258\n",
      "Iteration 14206, loss = 13.10592398\n",
      "Iteration 14207, loss = 10.00453488\n",
      "Iteration 14208, loss = 7.87377381\n",
      "Iteration 14209, loss = 7.08061625\n",
      "Iteration 14210, loss = 8.15461379\n",
      "Iteration 14211, loss = 7.72329020\n",
      "Iteration 14212, loss = 7.07411713\n",
      "Iteration 14213, loss = 7.36030094\n",
      "Iteration 14214, loss = 7.06389896\n",
      "Iteration 14215, loss = 7.27033096\n",
      "Iteration 14216, loss = 7.93542687\n",
      "Iteration 14217, loss = 8.11899733\n",
      "Iteration 14218, loss = 7.12066226\n",
      "Iteration 14219, loss = 7.28406392\n",
      "Iteration 14220, loss = 7.67346022\n",
      "Iteration 14221, loss = 7.25460928\n",
      "Iteration 14222, loss = 7.16841281\n",
      "Iteration 14223, loss = 6.98893719\n",
      "Iteration 14224, loss = 6.98427112\n",
      "Iteration 14225, loss = 7.46089198\n",
      "Iteration 14226, loss = 7.40172212\n",
      "Iteration 14227, loss = 6.94949645\n",
      "Iteration 14228, loss = 6.98167064\n",
      "Iteration 14229, loss = 6.89909126\n",
      "Iteration 14230, loss = 6.88524091\n",
      "Iteration 14231, loss = 6.86847641\n",
      "Iteration 14232, loss = 7.10060268\n",
      "Iteration 14233, loss = 7.61530031\n",
      "Iteration 14234, loss = 7.69230124\n",
      "Iteration 14235, loss = 7.10507133\n",
      "Iteration 14236, loss = 7.52017806\n",
      "Iteration 14237, loss = 7.61738580\n",
      "Iteration 14238, loss = 7.85843832\n",
      "Iteration 14239, loss = 7.37772592\n",
      "Iteration 14240, loss = 7.15560638\n",
      "Iteration 14241, loss = 7.80207477\n",
      "Iteration 14242, loss = 7.34322182\n",
      "Iteration 14243, loss = 6.90618179\n",
      "Iteration 14244, loss = 7.11424916\n",
      "Iteration 14245, loss = 6.87149945\n",
      "Iteration 14246, loss = 7.33120420\n",
      "Iteration 14247, loss = 7.58507465\n",
      "Iteration 14248, loss = 8.04220711\n",
      "Iteration 14249, loss = 8.38286295\n",
      "Iteration 14250, loss = 7.83260554\n",
      "Iteration 14251, loss = 7.21876933\n",
      "Iteration 14252, loss = 7.22213975\n",
      "Iteration 14253, loss = 7.45190637\n",
      "Iteration 14254, loss = 8.10562607\n",
      "Iteration 14255, loss = 7.54483967\n",
      "Iteration 14256, loss = 7.71697704\n",
      "Iteration 14257, loss = 7.02221898\n",
      "Iteration 14258, loss = 7.01611820\n",
      "Iteration 14259, loss = 6.81484696\n",
      "Iteration 14260, loss = 6.97833190\n",
      "Iteration 14261, loss = 7.19936666\n",
      "Iteration 14262, loss = 7.10148327\n",
      "Iteration 14263, loss = 8.01463831\n",
      "Iteration 14264, loss = 7.47475354\n",
      "Iteration 14265, loss = 7.64229244\n",
      "Iteration 14266, loss = 7.02837126\n",
      "Iteration 14267, loss = 7.62641030\n",
      "Iteration 14268, loss = 7.44279219\n",
      "Iteration 14269, loss = 7.92956554\n",
      "Iteration 14270, loss = 7.05592034\n",
      "Iteration 14271, loss = 6.94346870\n",
      "Iteration 14272, loss = 7.37148597\n",
      "Iteration 14273, loss = 7.46482580\n",
      "Iteration 14274, loss = 7.27670485\n",
      "Iteration 14275, loss = 7.20899644\n",
      "Iteration 14276, loss = 7.26764783\n",
      "Iteration 14277, loss = 7.06260735\n",
      "Iteration 14278, loss = 7.01516847\n",
      "Iteration 14279, loss = 7.21248913\n",
      "Iteration 14280, loss = 7.27061559\n",
      "Iteration 14281, loss = 8.05714865\n",
      "Iteration 14282, loss = 8.24254442\n",
      "Iteration 14283, loss = 7.57977553\n",
      "Iteration 14284, loss = 7.19785688\n",
      "Iteration 14285, loss = 6.94513521\n",
      "Iteration 14286, loss = 7.60532451\n",
      "Iteration 14287, loss = 7.04389619\n",
      "Iteration 14288, loss = 7.17868278\n",
      "Iteration 14289, loss = 7.52153088\n",
      "Iteration 14290, loss = 7.38156601\n",
      "Iteration 14291, loss = 8.27152654\n",
      "Iteration 14292, loss = 7.36855666\n",
      "Iteration 14293, loss = 7.66560469\n",
      "Iteration 14294, loss = 7.12116987\n",
      "Iteration 14295, loss = 7.26590880\n",
      "Iteration 14296, loss = 7.28296154\n",
      "Iteration 14297, loss = 7.75036338\n",
      "Iteration 14298, loss = 8.09011193\n",
      "Iteration 14299, loss = 7.80190970\n",
      "Iteration 14300, loss = 7.48012190\n",
      "Iteration 14301, loss = 7.03247892\n",
      "Iteration 14302, loss = 7.34507474\n",
      "Iteration 14303, loss = 7.87647923\n",
      "Iteration 14304, loss = 7.36651682\n",
      "Iteration 14305, loss = 7.23933540\n",
      "Iteration 14306, loss = 7.28973983\n",
      "Iteration 14307, loss = 6.70165495\n",
      "Iteration 14308, loss = 7.28213155\n",
      "Iteration 14309, loss = 8.06982044\n",
      "Iteration 14310, loss = 8.48885789\n",
      "Iteration 14311, loss = 6.97595196\n",
      "Iteration 14312, loss = 7.51686893\n",
      "Iteration 14313, loss = 7.43341102\n",
      "Iteration 14314, loss = 7.33616689\n",
      "Iteration 14315, loss = 7.08941496\n",
      "Iteration 14316, loss = 7.17382805\n",
      "Iteration 14317, loss = 7.38520066\n",
      "Iteration 14318, loss = 7.99480102\n",
      "Iteration 14319, loss = 7.86486458\n",
      "Iteration 14320, loss = 6.99814589\n",
      "Iteration 14321, loss = 7.10730686\n",
      "Iteration 14322, loss = 7.36209246\n",
      "Iteration 14323, loss = 7.70632184\n",
      "Iteration 14324, loss = 8.17891129\n",
      "Iteration 14325, loss = 9.03817725\n",
      "Iteration 14326, loss = 7.69925578\n",
      "Iteration 14327, loss = 7.03662001\n",
      "Iteration 14328, loss = 7.01017876\n",
      "Iteration 14329, loss = 6.97967180\n",
      "Iteration 14330, loss = 6.83388206\n",
      "Iteration 14331, loss = 7.00367691\n",
      "Iteration 14332, loss = 7.12799963\n",
      "Iteration 14333, loss = 6.95900058\n",
      "Iteration 14334, loss = 7.04766315\n",
      "Iteration 14335, loss = 8.19957305\n",
      "Iteration 14336, loss = 7.88658082\n",
      "Iteration 14337, loss = 8.17359365\n",
      "Iteration 14338, loss = 8.73107149\n",
      "Iteration 14339, loss = 7.85489286\n",
      "Iteration 14340, loss = 9.46513573\n",
      "Iteration 14341, loss = 6.90639540\n",
      "Iteration 14342, loss = 7.30955646\n",
      "Iteration 14343, loss = 7.11137716\n",
      "Iteration 14344, loss = 7.45445868\n",
      "Iteration 14345, loss = 7.21306556\n",
      "Iteration 14346, loss = 8.21831748\n",
      "Iteration 14347, loss = 7.16145927\n",
      "Iteration 14348, loss = 7.92576640\n",
      "Iteration 14349, loss = 7.60523408\n",
      "Iteration 14350, loss = 7.79697006\n",
      "Iteration 14351, loss = 6.92776895\n",
      "Iteration 14352, loss = 7.41276876\n",
      "Iteration 14353, loss = 6.98065783\n",
      "Iteration 14354, loss = 8.00963068\n",
      "Iteration 14355, loss = 6.76520569\n",
      "Iteration 14356, loss = 7.06086179\n",
      "Iteration 14357, loss = 7.11424272\n",
      "Iteration 14358, loss = 7.49840854\n",
      "Iteration 14359, loss = 7.41229867\n",
      "Iteration 14360, loss = 7.13833463\n",
      "Iteration 14361, loss = 7.09298404\n",
      "Iteration 14362, loss = 6.94941500\n",
      "Iteration 14363, loss = 7.08404654\n",
      "Iteration 14364, loss = 7.13388888\n",
      "Iteration 14365, loss = 7.96466112\n",
      "Iteration 14366, loss = 7.19360021\n",
      "Iteration 14367, loss = 7.13484691\n",
      "Iteration 14368, loss = 7.29573505\n",
      "Iteration 14369, loss = 6.87781783\n",
      "Iteration 14370, loss = 7.19562557\n",
      "Iteration 14371, loss = 7.11263781\n",
      "Iteration 14372, loss = 7.25085925\n",
      "Iteration 14373, loss = 8.20257349\n",
      "Iteration 14374, loss = 8.53582894\n",
      "Iteration 14375, loss = 7.76163020\n",
      "Iteration 14376, loss = 6.95828907\n",
      "Iteration 14377, loss = 7.04423117\n",
      "Iteration 14378, loss = 7.13626842\n",
      "Iteration 14379, loss = 6.91715505\n",
      "Iteration 14380, loss = 7.07579103\n",
      "Iteration 14381, loss = 7.03362944\n",
      "Iteration 14382, loss = 7.06682907\n",
      "Iteration 14383, loss = 7.10770132\n",
      "Iteration 14384, loss = 7.02187063\n",
      "Iteration 14385, loss = 8.01150941\n",
      "Iteration 14386, loss = 8.45041931\n",
      "Iteration 14387, loss = 8.31938861\n",
      "Iteration 14388, loss = 8.10631991\n",
      "Iteration 14389, loss = 8.65538417\n",
      "Iteration 14390, loss = 8.69520059\n",
      "Iteration 14391, loss = 8.55852281\n",
      "Iteration 14392, loss = 9.16894358\n",
      "Iteration 14393, loss = 6.72544407\n",
      "Iteration 14394, loss = 6.85369526\n",
      "Iteration 14395, loss = 6.96875150\n",
      "Iteration 14396, loss = 6.85810288\n",
      "Iteration 14397, loss = 7.60791479\n",
      "Iteration 14398, loss = 8.45465841\n",
      "Iteration 14399, loss = 7.82203241\n",
      "Iteration 14400, loss = 7.43629863\n",
      "Iteration 14401, loss = 7.15589634\n",
      "Iteration 14402, loss = 6.88445884\n",
      "Iteration 14403, loss = 7.11447732\n",
      "Iteration 14404, loss = 7.11026692\n",
      "Iteration 14405, loss = 7.11810483\n",
      "Iteration 14406, loss = 7.34366322\n",
      "Iteration 14407, loss = 8.11027896\n",
      "Iteration 14408, loss = 7.57005264\n",
      "Iteration 14409, loss = 7.46376644\n",
      "Iteration 14410, loss = 8.08690112\n",
      "Iteration 14411, loss = 8.71914575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14412, loss = 7.58372499\n",
      "Iteration 14413, loss = 7.25819198\n",
      "Iteration 14414, loss = 7.99910853\n",
      "Iteration 14415, loss = 7.90118886\n",
      "Iteration 14416, loss = 7.45654243\n",
      "Iteration 14417, loss = 6.95690782\n",
      "Iteration 14418, loss = 7.02953251\n",
      "Iteration 14419, loss = 7.13990808\n",
      "Iteration 14420, loss = 8.21993311\n",
      "Iteration 14421, loss = 6.87702740\n",
      "Iteration 14422, loss = 8.25772022\n",
      "Iteration 14423, loss = 7.68218030\n",
      "Iteration 14424, loss = 7.34026271\n",
      "Iteration 14425, loss = 6.86804810\n",
      "Iteration 14426, loss = 7.21302684\n",
      "Iteration 14427, loss = 7.13017122\n",
      "Iteration 14428, loss = 7.02592960\n",
      "Iteration 14429, loss = 6.78867543\n",
      "Iteration 14430, loss = 7.03714322\n",
      "Iteration 14431, loss = 7.21062785\n",
      "Iteration 14432, loss = 7.36185913\n",
      "Iteration 14433, loss = 7.91046610\n",
      "Iteration 14434, loss = 6.84767368\n",
      "Iteration 14435, loss = 7.01337115\n",
      "Iteration 14436, loss = 6.98578264\n",
      "Iteration 14437, loss = 6.77598701\n",
      "Iteration 14438, loss = 7.46898187\n",
      "Iteration 14439, loss = 8.25025960\n",
      "Iteration 14440, loss = 7.18852863\n",
      "Iteration 14441, loss = 7.52131804\n",
      "Iteration 14442, loss = 9.35758722\n",
      "Iteration 14443, loss = 9.15838963\n",
      "Iteration 14444, loss = 9.14135103\n",
      "Iteration 14445, loss = 7.47624892\n",
      "Iteration 14446, loss = 7.36242274\n",
      "Iteration 14447, loss = 6.94847788\n",
      "Iteration 14448, loss = 7.50608813\n",
      "Iteration 14449, loss = 7.81528042\n",
      "Iteration 14450, loss = 7.49070070\n",
      "Iteration 14451, loss = 7.17080600\n",
      "Iteration 14452, loss = 7.35753021\n",
      "Iteration 14453, loss = 7.78260450\n",
      "Iteration 14454, loss = 7.67945797\n",
      "Iteration 14455, loss = 6.90246430\n",
      "Iteration 14456, loss = 6.87204252\n",
      "Iteration 14457, loss = 7.02041247\n",
      "Iteration 14458, loss = 6.74590931\n",
      "Iteration 14459, loss = 6.98489390\n",
      "Iteration 14460, loss = 7.11483067\n",
      "Iteration 14461, loss = 7.02005649\n",
      "Iteration 14462, loss = 7.31439731\n",
      "Iteration 14463, loss = 7.31164896\n",
      "Iteration 14464, loss = 6.74135945\n",
      "Iteration 14465, loss = 7.01761570\n",
      "Iteration 14466, loss = 7.07809663\n",
      "Iteration 14467, loss = 6.79037714\n",
      "Iteration 14468, loss = 7.24681311\n",
      "Iteration 14469, loss = 7.16370222\n",
      "Iteration 14470, loss = 7.11527534\n",
      "Iteration 14471, loss = 7.57432359\n",
      "Iteration 14472, loss = 7.64922700\n",
      "Iteration 14473, loss = 8.71837725\n",
      "Iteration 14474, loss = 8.26301343\n",
      "Iteration 14475, loss = 7.92036408\n",
      "Iteration 14476, loss = 7.51297749\n",
      "Iteration 14477, loss = 7.85474703\n",
      "Iteration 14478, loss = 7.65442232\n",
      "Iteration 14479, loss = 7.13587857\n",
      "Iteration 14480, loss = 7.20509801\n",
      "Iteration 14481, loss = 6.90832395\n",
      "Iteration 14482, loss = 7.36502335\n",
      "Iteration 14483, loss = 7.33794076\n",
      "Iteration 14484, loss = 7.95852169\n",
      "Iteration 14485, loss = 8.59320384\n",
      "Iteration 14486, loss = 8.76284562\n",
      "Iteration 14487, loss = 8.04110563\n",
      "Iteration 14488, loss = 8.08073860\n",
      "Iteration 14489, loss = 7.33938988\n",
      "Iteration 14490, loss = 8.86861029\n",
      "Iteration 14491, loss = 8.68264823\n",
      "Iteration 14492, loss = 8.28586129\n",
      "Iteration 14493, loss = 7.24371832\n",
      "Iteration 14494, loss = 6.84177551\n",
      "Iteration 14495, loss = 6.85553901\n",
      "Iteration 14496, loss = 6.88939210\n",
      "Iteration 14497, loss = 7.27039916\n",
      "Iteration 14498, loss = 7.32773164\n",
      "Iteration 14499, loss = 7.08770844\n",
      "Iteration 14500, loss = 7.48504041\n",
      "Iteration 14501, loss = 7.62775929\n",
      "Iteration 14502, loss = 7.23310574\n",
      "Iteration 14503, loss = 7.11313500\n",
      "Iteration 14504, loss = 6.92310846\n",
      "Iteration 14505, loss = 7.28442712\n",
      "Iteration 14506, loss = 6.98800092\n",
      "Iteration 14507, loss = 7.37454687\n",
      "Iteration 14508, loss = 7.64474861\n",
      "Iteration 14509, loss = 7.33213321\n",
      "Iteration 14510, loss = 7.17381124\n",
      "Iteration 14511, loss = 7.23850059\n",
      "Iteration 14512, loss = 7.42828464\n",
      "Iteration 14513, loss = 6.84342623\n",
      "Iteration 14514, loss = 7.13267350\n",
      "Iteration 14515, loss = 7.55924646\n",
      "Iteration 14516, loss = 6.65514234\n",
      "Iteration 14517, loss = 6.98660634\n",
      "Iteration 14518, loss = 6.86524890\n",
      "Iteration 14519, loss = 7.20320443\n",
      "Iteration 14520, loss = 7.61350344\n",
      "Iteration 14521, loss = 7.42454893\n",
      "Iteration 14522, loss = 7.49424603\n",
      "Iteration 14523, loss = 7.63769530\n",
      "Iteration 14524, loss = 7.58849343\n",
      "Iteration 14525, loss = 7.82243888\n",
      "Iteration 14526, loss = 8.23377944\n",
      "Iteration 14527, loss = 7.43361569\n",
      "Iteration 14528, loss = 7.39473475\n",
      "Iteration 14529, loss = 7.34007447\n",
      "Iteration 14530, loss = 7.44416354\n",
      "Iteration 14531, loss = 7.33602039\n",
      "Iteration 14532, loss = 7.21966419\n",
      "Iteration 14533, loss = 7.11862073\n",
      "Iteration 14534, loss = 6.84846275\n",
      "Iteration 14535, loss = 7.37601145\n",
      "Iteration 14536, loss = 7.17204837\n",
      "Iteration 14537, loss = 7.06878958\n",
      "Iteration 14538, loss = 7.26496471\n",
      "Iteration 14539, loss = 6.91723977\n",
      "Iteration 14540, loss = 7.48805817\n",
      "Iteration 14541, loss = 7.22844115\n",
      "Iteration 14542, loss = 7.59360911\n",
      "Iteration 14543, loss = 7.39080529\n",
      "Iteration 14544, loss = 7.13223325\n",
      "Iteration 14545, loss = 7.75533681\n",
      "Iteration 14546, loss = 7.11028332\n",
      "Iteration 14547, loss = 7.65876587\n",
      "Iteration 14548, loss = 6.89260531\n",
      "Iteration 14549, loss = 7.50326178\n",
      "Iteration 14550, loss = 7.99282085\n",
      "Iteration 14551, loss = 8.27882840\n",
      "Iteration 14552, loss = 7.97684865\n",
      "Iteration 14553, loss = 8.49423293\n",
      "Iteration 14554, loss = 9.47466801\n",
      "Iteration 14555, loss = 8.76700185\n",
      "Iteration 14556, loss = 8.99675305\n",
      "Iteration 14557, loss = 7.30517208\n",
      "Iteration 14558, loss = 7.27541429\n",
      "Iteration 14559, loss = 7.41047562\n",
      "Iteration 14560, loss = 7.31032981\n",
      "Iteration 14561, loss = 6.81794007\n",
      "Iteration 14562, loss = 6.92189460\n",
      "Iteration 14563, loss = 7.27495241\n",
      "Iteration 14564, loss = 6.86201877\n",
      "Iteration 14565, loss = 6.94714604\n",
      "Iteration 14566, loss = 6.78722074\n",
      "Iteration 14567, loss = 7.66213316\n",
      "Iteration 14568, loss = 7.13912860\n",
      "Iteration 14569, loss = 7.97096945\n",
      "Iteration 14570, loss = 7.92326122\n",
      "Iteration 14571, loss = 7.27771740\n",
      "Iteration 14572, loss = 6.91290393\n",
      "Iteration 14573, loss = 7.33069289\n",
      "Iteration 14574, loss = 7.34014555\n",
      "Iteration 14575, loss = 7.57290063\n",
      "Iteration 14576, loss = 7.26997703\n",
      "Iteration 14577, loss = 6.93617636\n",
      "Iteration 14578, loss = 6.82503174\n",
      "Iteration 14579, loss = 7.01092516\n",
      "Iteration 14580, loss = 7.30592238\n",
      "Iteration 14581, loss = 9.30609173\n",
      "Iteration 14582, loss = 9.27764473\n",
      "Iteration 14583, loss = 11.13897795\n",
      "Iteration 14584, loss = 8.64979803\n",
      "Iteration 14585, loss = 10.14327230\n",
      "Iteration 14586, loss = 10.35429033\n",
      "Iteration 14587, loss = 10.00619703\n",
      "Iteration 14588, loss = 8.51772532\n",
      "Iteration 14589, loss = 7.29660684\n",
      "Iteration 14590, loss = 7.72181186\n",
      "Iteration 14591, loss = 8.25488122\n",
      "Iteration 14592, loss = 9.41576204\n",
      "Iteration 14593, loss = 7.65458322\n",
      "Iteration 14594, loss = 8.95484255\n",
      "Iteration 14595, loss = 8.13850564\n",
      "Iteration 14596, loss = 8.18488173\n",
      "Iteration 14597, loss = 7.42122578\n",
      "Iteration 14598, loss = 7.59596705\n",
      "Iteration 14599, loss = 7.14033594\n",
      "Iteration 14600, loss = 7.66505994\n",
      "Iteration 14601, loss = 7.18217413\n",
      "Iteration 14602, loss = 7.41869898\n",
      "Iteration 14603, loss = 6.81897690\n",
      "Iteration 14604, loss = 7.71785987\n",
      "Iteration 14605, loss = 7.33864846\n",
      "Iteration 14606, loss = 7.06872315\n",
      "Iteration 14607, loss = 7.05743602\n",
      "Iteration 14608, loss = 7.17632509\n",
      "Iteration 14609, loss = 6.92665330\n",
      "Iteration 14610, loss = 6.98977847\n",
      "Iteration 14611, loss = 8.46962694\n",
      "Iteration 14612, loss = 8.23069403\n",
      "Iteration 14613, loss = 8.31487314\n",
      "Iteration 14614, loss = 7.43069839\n",
      "Iteration 14615, loss = 7.05957266\n",
      "Iteration 14616, loss = 7.06936513\n",
      "Iteration 14617, loss = 6.95711847\n",
      "Iteration 14618, loss = 7.11537837\n",
      "Iteration 14619, loss = 6.74561259\n",
      "Iteration 14620, loss = 6.73913927\n",
      "Iteration 14621, loss = 7.43319784\n",
      "Iteration 14622, loss = 7.88296356\n",
      "Iteration 14623, loss = 7.26407338\n",
      "Iteration 14624, loss = 8.10868918\n",
      "Iteration 14625, loss = 8.35009291\n",
      "Iteration 14626, loss = 8.11180849\n",
      "Iteration 14627, loss = 8.36071517\n",
      "Iteration 14628, loss = 8.16688612\n",
      "Iteration 14629, loss = 7.61461425\n",
      "Iteration 14630, loss = 8.03388947\n",
      "Iteration 14631, loss = 7.96422596\n",
      "Iteration 14632, loss = 9.55620113\n",
      "Iteration 14633, loss = 8.07733728\n",
      "Iteration 14634, loss = 8.16827717\n",
      "Iteration 14635, loss = 6.87702112\n",
      "Iteration 14636, loss = 7.30899996\n",
      "Iteration 14637, loss = 7.55991563\n",
      "Iteration 14638, loss = 7.35634432\n",
      "Iteration 14639, loss = 7.56894067\n",
      "Iteration 14640, loss = 8.09699211\n",
      "Iteration 14641, loss = 8.62764248\n",
      "Iteration 14642, loss = 7.28948122\n",
      "Iteration 14643, loss = 7.20139038\n",
      "Iteration 14644, loss = 7.37615862\n",
      "Iteration 14645, loss = 7.74765924\n",
      "Iteration 14646, loss = 7.27667425\n",
      "Iteration 14647, loss = 8.74579533\n",
      "Iteration 14648, loss = 8.25304715\n",
      "Iteration 14649, loss = 7.56551970\n",
      "Iteration 14650, loss = 7.22852631\n",
      "Iteration 14651, loss = 7.36418749\n",
      "Iteration 14652, loss = 7.96436753\n",
      "Iteration 14653, loss = 6.91368121\n",
      "Iteration 14654, loss = 6.65835495\n",
      "Iteration 14655, loss = 6.91298337\n",
      "Iteration 14656, loss = 6.84162765\n",
      "Iteration 14657, loss = 7.13305778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14658, loss = 7.93769068\n",
      "Iteration 14659, loss = 8.26655958\n",
      "Iteration 14660, loss = 7.90394358\n",
      "Iteration 14661, loss = 7.35518000\n",
      "Iteration 14662, loss = 7.60315333\n",
      "Iteration 14663, loss = 7.53609378\n",
      "Iteration 14664, loss = 8.11147514\n",
      "Iteration 14665, loss = 7.22287930\n",
      "Iteration 14666, loss = 7.32421886\n",
      "Iteration 14667, loss = 7.08154346\n",
      "Iteration 14668, loss = 7.76872427\n",
      "Iteration 14669, loss = 8.13778662\n",
      "Iteration 14670, loss = 8.68049684\n",
      "Iteration 14671, loss = 7.84352914\n",
      "Iteration 14672, loss = 8.26727270\n",
      "Iteration 14673, loss = 7.12793432\n",
      "Iteration 14674, loss = 7.05762347\n",
      "Iteration 14675, loss = 7.14999538\n",
      "Iteration 14676, loss = 7.04146446\n",
      "Iteration 14677, loss = 7.52578301\n",
      "Iteration 14678, loss = 7.39840747\n",
      "Iteration 14679, loss = 8.00529961\n",
      "Iteration 14680, loss = 7.34763299\n",
      "Iteration 14681, loss = 7.30613909\n",
      "Iteration 14682, loss = 6.82646834\n",
      "Iteration 14683, loss = 6.73065636\n",
      "Iteration 14684, loss = 8.26237286\n",
      "Iteration 14685, loss = 6.32383502\n",
      "Iteration 14686, loss = 7.20284463\n",
      "Iteration 14687, loss = 7.63327082\n",
      "Iteration 14688, loss = 7.32838261\n",
      "Iteration 14689, loss = 7.47942225\n",
      "Iteration 14690, loss = 8.18094514\n",
      "Iteration 14691, loss = 7.66551932\n",
      "Iteration 14692, loss = 7.53540727\n",
      "Iteration 14693, loss = 7.37200343\n",
      "Iteration 14694, loss = 9.42942549\n",
      "Iteration 14695, loss = 8.76518701\n",
      "Iteration 14696, loss = 7.80972386\n",
      "Iteration 14697, loss = 7.99755519\n",
      "Iteration 14698, loss = 7.72538015\n",
      "Iteration 14699, loss = 7.11212837\n",
      "Iteration 14700, loss = 7.31880529\n",
      "Iteration 14701, loss = 6.87008994\n",
      "Iteration 14702, loss = 7.25371884\n",
      "Iteration 14703, loss = 6.78569367\n",
      "Iteration 14704, loss = 7.51513193\n",
      "Iteration 14705, loss = 7.16496360\n",
      "Iteration 14706, loss = 7.64579970\n",
      "Iteration 14707, loss = 7.10236705\n",
      "Iteration 14708, loss = 7.49262696\n",
      "Iteration 14709, loss = 7.09507543\n",
      "Iteration 14710, loss = 6.99594093\n",
      "Iteration 14711, loss = 7.24254829\n",
      "Iteration 14712, loss = 7.12970563\n",
      "Iteration 14713, loss = 7.53344101\n",
      "Iteration 14714, loss = 7.92517236\n",
      "Iteration 14715, loss = 7.74740568\n",
      "Iteration 14716, loss = 7.53349971\n",
      "Iteration 14717, loss = 7.02005620\n",
      "Iteration 14718, loss = 7.00760193\n",
      "Iteration 14719, loss = 7.12176681\n",
      "Iteration 14720, loss = 6.69273536\n",
      "Iteration 14721, loss = 6.75675274\n",
      "Iteration 14722, loss = 6.80128834\n",
      "Iteration 14723, loss = 7.00434713\n",
      "Iteration 14724, loss = 7.23309214\n",
      "Iteration 14725, loss = 6.77534959\n",
      "Iteration 14726, loss = 6.83978327\n",
      "Iteration 14727, loss = 6.83120398\n",
      "Iteration 14728, loss = 7.26284356\n",
      "Iteration 14729, loss = 7.56410767\n",
      "Iteration 14730, loss = 8.27942681\n",
      "Iteration 14731, loss = 8.15672305\n",
      "Iteration 14732, loss = 7.47518239\n",
      "Iteration 14733, loss = 7.47379169\n",
      "Iteration 14734, loss = 7.99043256\n",
      "Iteration 14735, loss = 7.42139833\n",
      "Iteration 14736, loss = 7.34075586\n",
      "Iteration 14737, loss = 7.27523249\n",
      "Iteration 14738, loss = 8.08142494\n",
      "Iteration 14739, loss = 6.86470742\n",
      "Iteration 14740, loss = 7.28894274\n",
      "Iteration 14741, loss = 8.88945119\n",
      "Iteration 14742, loss = 7.13700505\n",
      "Iteration 14743, loss = 7.09988233\n",
      "Iteration 14744, loss = 7.78310618\n",
      "Iteration 14745, loss = 7.15259167\n",
      "Iteration 14746, loss = 7.84625103\n",
      "Iteration 14747, loss = 8.12876351\n",
      "Iteration 14748, loss = 7.10572309\n",
      "Iteration 14749, loss = 7.64821319\n",
      "Iteration 14750, loss = 7.81397952\n",
      "Iteration 14751, loss = 7.08025288\n",
      "Iteration 14752, loss = 7.10305260\n",
      "Iteration 14753, loss = 7.44211071\n",
      "Iteration 14754, loss = 7.63165483\n",
      "Iteration 14755, loss = 7.11509133\n",
      "Iteration 14756, loss = 6.69545728\n",
      "Iteration 14757, loss = 6.56163563\n",
      "Iteration 14758, loss = 6.85151127\n",
      "Iteration 14759, loss = 7.09322640\n",
      "Iteration 14760, loss = 7.50205985\n",
      "Iteration 14761, loss = 6.65223003\n",
      "Iteration 14762, loss = 6.82323086\n",
      "Iteration 14763, loss = 6.76641948\n",
      "Iteration 14764, loss = 6.65058666\n",
      "Iteration 14765, loss = 6.60475601\n",
      "Iteration 14766, loss = 6.57112420\n",
      "Iteration 14767, loss = 7.28476001\n",
      "Iteration 14768, loss = 7.36879820\n",
      "Iteration 14769, loss = 7.21223218\n",
      "Iteration 14770, loss = 7.78184042\n",
      "Iteration 14771, loss = 7.81012314\n",
      "Iteration 14772, loss = 8.08986903\n",
      "Iteration 14773, loss = 8.07899027\n",
      "Iteration 14774, loss = 7.49404392\n",
      "Iteration 14775, loss = 7.42630729\n",
      "Iteration 14776, loss = 6.88232295\n",
      "Iteration 14777, loss = 7.11813671\n",
      "Iteration 14778, loss = 7.50388013\n",
      "Iteration 14779, loss = 6.97658096\n",
      "Iteration 14780, loss = 7.41111613\n",
      "Iteration 14781, loss = 7.11239850\n",
      "Iteration 14782, loss = 7.20340443\n",
      "Iteration 14783, loss = 6.58177260\n",
      "Iteration 14784, loss = 6.67319759\n",
      "Iteration 14785, loss = 6.80432642\n",
      "Iteration 14786, loss = 6.71728911\n",
      "Iteration 14787, loss = 7.31523264\n",
      "Iteration 14788, loss = 7.76047793\n",
      "Iteration 14789, loss = 7.23377161\n",
      "Iteration 14790, loss = 6.79830601\n",
      "Iteration 14791, loss = 6.97233641\n",
      "Iteration 14792, loss = 7.03815258\n",
      "Iteration 14793, loss = 7.45063904\n",
      "Iteration 14794, loss = 8.33214736\n",
      "Iteration 14795, loss = 7.76184395\n",
      "Iteration 14796, loss = 7.53006229\n",
      "Iteration 14797, loss = 7.32362806\n",
      "Iteration 14798, loss = 7.34663862\n",
      "Iteration 14799, loss = 7.33774502\n",
      "Iteration 14800, loss = 7.07779207\n",
      "Iteration 14801, loss = 7.90256891\n",
      "Iteration 14802, loss = 8.27491480\n",
      "Iteration 14803, loss = 9.24347981\n",
      "Iteration 14804, loss = 7.67904773\n",
      "Iteration 14805, loss = 10.05300384\n",
      "Iteration 14806, loss = 9.03286161\n",
      "Iteration 14807, loss = 8.60345568\n",
      "Iteration 14808, loss = 9.06772898\n",
      "Iteration 14809, loss = 7.23341405\n",
      "Iteration 14810, loss = 7.26952234\n",
      "Iteration 14811, loss = 6.94013424\n",
      "Iteration 14812, loss = 7.21050546\n",
      "Iteration 14813, loss = 6.86351130\n",
      "Iteration 14814, loss = 7.25384971\n",
      "Iteration 14815, loss = 6.83189142\n",
      "Iteration 14816, loss = 7.15830834\n",
      "Iteration 14817, loss = 7.20413230\n",
      "Iteration 14818, loss = 7.17252095\n",
      "Iteration 14819, loss = 7.26510171\n",
      "Iteration 14820, loss = 7.76658291\n",
      "Iteration 14821, loss = 8.16693535\n",
      "Iteration 14822, loss = 7.15624621\n",
      "Iteration 14823, loss = 6.60638642\n",
      "Iteration 14824, loss = 6.63110053\n",
      "Iteration 14825, loss = 7.07855665\n",
      "Iteration 14826, loss = 7.07517795\n",
      "Iteration 14827, loss = 7.00229988\n",
      "Iteration 14828, loss = 6.92187353\n",
      "Iteration 14829, loss = 7.42602428\n",
      "Iteration 14830, loss = 7.68466912\n",
      "Iteration 14831, loss = 8.31187346\n",
      "Iteration 14832, loss = 8.54505026\n",
      "Iteration 14833, loss = 8.81094308\n",
      "Iteration 14834, loss = 7.88781704\n",
      "Iteration 14835, loss = 7.91573030\n",
      "Iteration 14836, loss = 9.02278209\n",
      "Iteration 14837, loss = 8.78748992\n",
      "Iteration 14838, loss = 7.61964665\n",
      "Iteration 14839, loss = 7.13039870\n",
      "Iteration 14840, loss = 7.09936547\n",
      "Iteration 14841, loss = 6.86777185\n",
      "Iteration 14842, loss = 6.75606503\n",
      "Iteration 14843, loss = 6.70502788\n",
      "Iteration 14844, loss = 6.80538587\n",
      "Iteration 14845, loss = 7.05604980\n",
      "Iteration 14846, loss = 7.19214443\n",
      "Iteration 14847, loss = 7.07107088\n",
      "Iteration 14848, loss = 6.77293663\n",
      "Iteration 14849, loss = 6.84170282\n",
      "Iteration 14850, loss = 7.53136486\n",
      "Iteration 14851, loss = 7.30892686\n",
      "Iteration 14852, loss = 8.15956058\n",
      "Iteration 14853, loss = 7.51303919\n",
      "Iteration 14854, loss = 7.08543348\n",
      "Iteration 14855, loss = 7.24063505\n",
      "Iteration 14856, loss = 7.55483231\n",
      "Iteration 14857, loss = 6.75941597\n",
      "Iteration 14858, loss = 7.92077745\n",
      "Iteration 14859, loss = 6.99782850\n",
      "Iteration 14860, loss = 7.58808234\n",
      "Iteration 14861, loss = 7.63041936\n",
      "Iteration 14862, loss = 8.67963031\n",
      "Iteration 14863, loss = 7.77904960\n",
      "Iteration 14864, loss = 6.95004241\n",
      "Iteration 14865, loss = 7.33601479\n",
      "Iteration 14866, loss = 8.37783844\n",
      "Iteration 14867, loss = 8.69987053\n",
      "Iteration 14868, loss = 7.73446991\n",
      "Iteration 14869, loss = 8.27788827\n",
      "Iteration 14870, loss = 7.31321982\n",
      "Iteration 14871, loss = 7.10020396\n",
      "Iteration 14872, loss = 6.91816686\n",
      "Iteration 14873, loss = 7.07078112\n",
      "Iteration 14874, loss = 6.91368074\n",
      "Iteration 14875, loss = 6.79916232\n",
      "Iteration 14876, loss = 8.78659229\n",
      "Iteration 14877, loss = 8.80477404\n",
      "Iteration 14878, loss = 7.20077120\n",
      "Iteration 14879, loss = 7.29414593\n",
      "Iteration 14880, loss = 7.25371284\n",
      "Iteration 14881, loss = 7.00884515\n",
      "Iteration 14882, loss = 6.65436769\n",
      "Iteration 14883, loss = 6.67607748\n",
      "Iteration 14884, loss = 6.84529482\n",
      "Iteration 14885, loss = 6.74825858\n",
      "Iteration 14886, loss = 6.88808842\n",
      "Iteration 14887, loss = 6.67915364\n",
      "Iteration 14888, loss = 6.76928328\n",
      "Iteration 14889, loss = 6.96258913\n",
      "Iteration 14890, loss = 6.87633685\n",
      "Iteration 14891, loss = 7.30137158\n",
      "Iteration 14892, loss = 8.94723342\n",
      "Iteration 14893, loss = 10.94878987\n",
      "Iteration 14894, loss = 10.04907038\n",
      "Iteration 14895, loss = 7.81653061\n",
      "Iteration 14896, loss = 8.69228882\n",
      "Iteration 14897, loss = 7.35596293\n",
      "Iteration 14898, loss = 7.39512061\n",
      "Iteration 14899, loss = 6.81339499\n",
      "Iteration 14900, loss = 6.91526483\n",
      "Iteration 14901, loss = 7.09795685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14902, loss = 6.86540874\n",
      "Iteration 14903, loss = 7.73067966\n",
      "Iteration 14904, loss = 7.77689143\n",
      "Iteration 14905, loss = 7.57229204\n",
      "Iteration 14906, loss = 7.53393467\n",
      "Iteration 14907, loss = 7.35905177\n",
      "Iteration 14908, loss = 7.31577926\n",
      "Iteration 14909, loss = 6.95269834\n",
      "Iteration 14910, loss = 6.59467520\n",
      "Iteration 14911, loss = 6.83756480\n",
      "Iteration 14912, loss = 7.13365893\n",
      "Iteration 14913, loss = 7.07923168\n",
      "Iteration 14914, loss = 6.66931295\n",
      "Iteration 14915, loss = 6.80422663\n",
      "Iteration 14916, loss = 7.04724016\n",
      "Iteration 14917, loss = 6.74798047\n",
      "Iteration 14918, loss = 7.06795790\n",
      "Iteration 14919, loss = 6.74201811\n",
      "Iteration 14920, loss = 6.60330165\n",
      "Iteration 14921, loss = 6.63126190\n",
      "Iteration 14922, loss = 6.80074234\n",
      "Iteration 14923, loss = 7.23799485\n",
      "Iteration 14924, loss = 6.92007411\n",
      "Iteration 14925, loss = 6.96614763\n",
      "Iteration 14926, loss = 7.15018541\n",
      "Iteration 14927, loss = 7.25488084\n",
      "Iteration 14928, loss = 7.14996127\n",
      "Iteration 14929, loss = 6.90310316\n",
      "Iteration 14930, loss = 6.78390799\n",
      "Iteration 14931, loss = 6.62172600\n",
      "Iteration 14932, loss = 6.86864487\n",
      "Iteration 14933, loss = 7.74638525\n",
      "Iteration 14934, loss = 7.18669908\n",
      "Iteration 14935, loss = 6.99669909\n",
      "Iteration 14936, loss = 6.50628998\n",
      "Iteration 14937, loss = 7.02357761\n",
      "Iteration 14938, loss = 7.03703418\n",
      "Iteration 14939, loss = 6.93607456\n",
      "Iteration 14940, loss = 6.90857536\n",
      "Iteration 14941, loss = 7.08990631\n",
      "Iteration 14942, loss = 7.24284144\n",
      "Iteration 14943, loss = 7.75727819\n",
      "Iteration 14944, loss = 7.09016816\n",
      "Iteration 14945, loss = 6.87236858\n",
      "Iteration 14946, loss = 6.84790977\n",
      "Iteration 14947, loss = 7.13825221\n",
      "Iteration 14948, loss = 6.93350841\n",
      "Iteration 14949, loss = 7.32037901\n",
      "Iteration 14950, loss = 8.00702601\n",
      "Iteration 14951, loss = 8.18109866\n",
      "Iteration 14952, loss = 7.97109016\n",
      "Iteration 14953, loss = 7.13058178\n",
      "Iteration 14954, loss = 7.36188522\n",
      "Iteration 14955, loss = 8.21102834\n",
      "Iteration 14956, loss = 7.45705343\n",
      "Iteration 14957, loss = 8.17215510\n",
      "Iteration 14958, loss = 7.40904011\n",
      "Iteration 14959, loss = 6.80869063\n",
      "Iteration 14960, loss = 7.23493529\n",
      "Iteration 14961, loss = 7.62512409\n",
      "Iteration 14962, loss = 6.99273853\n",
      "Iteration 14963, loss = 6.78385343\n",
      "Iteration 14964, loss = 6.71222102\n",
      "Iteration 14965, loss = 6.73083445\n",
      "Iteration 14966, loss = 6.79013718\n",
      "Iteration 14967, loss = 7.31445834\n",
      "Iteration 14968, loss = 8.21537661\n",
      "Iteration 14969, loss = 8.76810915\n",
      "Iteration 14970, loss = 7.92787631\n",
      "Iteration 14971, loss = 7.38962756\n",
      "Iteration 14972, loss = 7.16378643\n",
      "Iteration 14973, loss = 7.00618736\n",
      "Iteration 14974, loss = 7.20748425\n",
      "Iteration 14975, loss = 7.69027193\n",
      "Iteration 14976, loss = 8.52237799\n",
      "Iteration 14977, loss = 8.20053092\n",
      "Iteration 14978, loss = 8.18415186\n",
      "Iteration 14979, loss = 6.85201380\n",
      "Iteration 14980, loss = 7.76948074\n",
      "Iteration 14981, loss = 7.65006025\n",
      "Iteration 14982, loss = 10.08665359\n",
      "Iteration 14983, loss = 8.33936120\n",
      "Iteration 14984, loss = 7.59045731\n",
      "Iteration 14985, loss = 7.14961278\n",
      "Iteration 14986, loss = 7.04665159\n",
      "Iteration 14987, loss = 6.79251821\n",
      "Iteration 14988, loss = 6.49711959\n",
      "Iteration 14989, loss = 6.78990444\n",
      "Iteration 14990, loss = 7.51924860\n",
      "Iteration 14991, loss = 8.09073591\n",
      "Iteration 14992, loss = 7.38784529\n",
      "Iteration 14993, loss = 6.56813162\n",
      "Iteration 14994, loss = 7.00509335\n",
      "Iteration 14995, loss = 7.18626434\n",
      "Iteration 14996, loss = 7.03282193\n",
      "Iteration 14997, loss = 6.67467535\n",
      "Iteration 14998, loss = 6.60758820\n",
      "Iteration 14999, loss = 6.69114098\n",
      "Iteration 15000, loss = 7.23152414\n",
      "Iteration 15001, loss = 7.12606781\n",
      "Iteration 15002, loss = 7.23810457\n",
      "Iteration 15003, loss = 7.06107924\n",
      "Iteration 15004, loss = 6.92076099\n",
      "Iteration 15005, loss = 6.62126237\n",
      "Iteration 15006, loss = 6.95669544\n",
      "Iteration 15007, loss = 7.46092782\n",
      "Iteration 15008, loss = 7.19792982\n",
      "Iteration 15009, loss = 6.91558574\n",
      "Iteration 15010, loss = 7.01170956\n",
      "Iteration 15011, loss = 6.81363535\n",
      "Iteration 15012, loss = 6.89242635\n",
      "Iteration 15013, loss = 6.72564150\n",
      "Iteration 15014, loss = 6.91174143\n",
      "Iteration 15015, loss = 7.43763592\n",
      "Iteration 15016, loss = 7.97242272\n",
      "Iteration 15017, loss = 8.26375033\n",
      "Iteration 15018, loss = 7.54209058\n",
      "Iteration 15019, loss = 7.51704529\n",
      "Iteration 15020, loss = 7.22697738\n",
      "Iteration 15021, loss = 7.52225747\n",
      "Iteration 15022, loss = 6.89462451\n",
      "Iteration 15023, loss = 7.62618614\n",
      "Iteration 15024, loss = 8.78481961\n",
      "Iteration 15025, loss = 9.31652475\n",
      "Iteration 15026, loss = 7.07078488\n",
      "Iteration 15027, loss = 7.11380671\n",
      "Iteration 15028, loss = 7.47323175\n",
      "Iteration 15029, loss = 7.26439676\n",
      "Iteration 15030, loss = 6.63178284\n",
      "Iteration 15031, loss = 7.09117498\n",
      "Iteration 15032, loss = 6.91416270\n",
      "Iteration 15033, loss = 7.67437428\n",
      "Iteration 15034, loss = 6.68408622\n",
      "Iteration 15035, loss = 7.92366975\n",
      "Iteration 15036, loss = 7.34125016\n",
      "Iteration 15037, loss = 7.24846934\n",
      "Iteration 15038, loss = 7.22582128\n",
      "Iteration 15039, loss = 7.23246060\n",
      "Iteration 15040, loss = 6.82212603\n",
      "Iteration 15041, loss = 6.60475471\n",
      "Iteration 15042, loss = 6.73808871\n",
      "Iteration 15043, loss = 7.24108792\n",
      "Iteration 15044, loss = 8.85950422\n",
      "Iteration 15045, loss = 8.20301165\n",
      "Iteration 15046, loss = 7.21044579\n",
      "Iteration 15047, loss = 7.09667834\n",
      "Iteration 15048, loss = 6.80826599\n",
      "Iteration 15049, loss = 7.18740337\n",
      "Iteration 15050, loss = 6.88655382\n",
      "Iteration 15051, loss = 7.12498910\n",
      "Iteration 15052, loss = 7.35481215\n",
      "Iteration 15053, loss = 6.72764022\n",
      "Iteration 15054, loss = 6.83892179\n",
      "Iteration 15055, loss = 6.76251335\n",
      "Iteration 15056, loss = 6.98569089\n",
      "Iteration 15057, loss = 7.35879070\n",
      "Iteration 15058, loss = 7.46838927\n",
      "Iteration 15059, loss = 7.20437727\n",
      "Iteration 15060, loss = 7.36057378\n",
      "Iteration 15061, loss = 7.42901449\n",
      "Iteration 15062, loss = 6.83782999\n",
      "Iteration 15063, loss = 6.71729326\n",
      "Iteration 15064, loss = 7.10594841\n",
      "Iteration 15065, loss = 7.18723455\n",
      "Iteration 15066, loss = 7.47031808\n",
      "Iteration 15067, loss = 7.59504264\n",
      "Iteration 15068, loss = 7.25005832\n",
      "Iteration 15069, loss = 8.43319026\n",
      "Iteration 15070, loss = 8.72001321\n",
      "Iteration 15071, loss = 9.30320151\n",
      "Iteration 15072, loss = 9.92975744\n",
      "Iteration 15073, loss = 8.20624664\n",
      "Iteration 15074, loss = 7.08134547\n",
      "Iteration 15075, loss = 6.70097619\n",
      "Iteration 15076, loss = 6.86264734\n",
      "Iteration 15077, loss = 6.90400214\n",
      "Iteration 15078, loss = 6.75019937\n",
      "Iteration 15079, loss = 6.76389122\n",
      "Iteration 15080, loss = 6.81463591\n",
      "Iteration 15081, loss = 7.08250573\n",
      "Iteration 15082, loss = 8.22565448\n",
      "Iteration 15083, loss = 7.46052999\n",
      "Iteration 15084, loss = 7.71300926\n",
      "Iteration 15085, loss = 7.13248920\n",
      "Iteration 15086, loss = 6.92336457\n",
      "Iteration 15087, loss = 6.79422455\n",
      "Iteration 15088, loss = 6.92265732\n",
      "Iteration 15089, loss = 7.24065578\n",
      "Iteration 15090, loss = 6.48699708\n",
      "Iteration 15091, loss = 6.87256323\n",
      "Iteration 15092, loss = 6.73879017\n",
      "Iteration 15093, loss = 7.26972249\n",
      "Iteration 15094, loss = 7.41207020\n",
      "Iteration 15095, loss = 7.68550689\n",
      "Iteration 15096, loss = 7.28118050\n",
      "Iteration 15097, loss = 7.00629137\n",
      "Iteration 15098, loss = 6.98963192\n",
      "Iteration 15099, loss = 8.15031142\n",
      "Iteration 15100, loss = 8.06118230\n",
      "Iteration 15101, loss = 7.40066312\n",
      "Iteration 15102, loss = 7.02931463\n",
      "Iteration 15103, loss = 7.56827382\n",
      "Iteration 15104, loss = 8.48586350\n",
      "Iteration 15105, loss = 6.82067211\n",
      "Iteration 15106, loss = 7.41451078\n",
      "Iteration 15107, loss = 7.44353287\n",
      "Iteration 15108, loss = 7.48902349\n",
      "Iteration 15109, loss = 7.09541312\n",
      "Iteration 15110, loss = 6.95997123\n",
      "Iteration 15111, loss = 7.27586088\n",
      "Iteration 15112, loss = 6.71776654\n",
      "Iteration 15113, loss = 7.02105848\n",
      "Iteration 15114, loss = 7.42264734\n",
      "Iteration 15115, loss = 7.03820049\n",
      "Iteration 15116, loss = 6.58348549\n",
      "Iteration 15117, loss = 6.69564938\n",
      "Iteration 15118, loss = 7.22656096\n",
      "Iteration 15119, loss = 7.10124771\n",
      "Iteration 15120, loss = 7.44008673\n",
      "Iteration 15121, loss = 7.40679336\n",
      "Iteration 15122, loss = 7.77888607\n",
      "Iteration 15123, loss = 7.02486903\n",
      "Iteration 15124, loss = 7.68709609\n",
      "Iteration 15125, loss = 7.23893358\n",
      "Iteration 15126, loss = 7.02232346\n",
      "Iteration 15127, loss = 6.87481544\n",
      "Iteration 15128, loss = 7.42388904\n",
      "Iteration 15129, loss = 7.67868885\n",
      "Iteration 15130, loss = 7.49132166\n",
      "Iteration 15131, loss = 6.73406653\n",
      "Iteration 15132, loss = 6.80088778\n",
      "Iteration 15133, loss = 7.06734306\n",
      "Iteration 15134, loss = 6.92735737\n",
      "Iteration 15135, loss = 7.23758876\n",
      "Iteration 15136, loss = 6.59802403\n",
      "Iteration 15137, loss = 6.70727436\n",
      "Iteration 15138, loss = 7.54707958\n",
      "Iteration 15139, loss = 7.92747816\n",
      "Iteration 15140, loss = 7.41164952\n",
      "Iteration 15141, loss = 6.72667961\n",
      "Iteration 15142, loss = 6.87285342\n",
      "Iteration 15143, loss = 6.87448895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15144, loss = 6.55500734\n",
      "Iteration 15145, loss = 6.57050905\n",
      "Iteration 15146, loss = 7.12861915\n",
      "Iteration 15147, loss = 7.52305459\n",
      "Iteration 15148, loss = 7.54666366\n",
      "Iteration 15149, loss = 7.42619619\n",
      "Iteration 15150, loss = 6.74698511\n",
      "Iteration 15151, loss = 8.07805971\n",
      "Iteration 15152, loss = 7.06529571\n",
      "Iteration 15153, loss = 8.33627183\n",
      "Iteration 15154, loss = 8.90534135\n",
      "Iteration 15155, loss = 8.50233024\n",
      "Iteration 15156, loss = 7.53920097\n",
      "Iteration 15157, loss = 7.44390056\n",
      "Iteration 15158, loss = 7.26211751\n",
      "Iteration 15159, loss = 7.15700373\n",
      "Iteration 15160, loss = 7.58197677\n",
      "Iteration 15161, loss = 8.06837453\n",
      "Iteration 15162, loss = 7.66631455\n",
      "Iteration 15163, loss = 8.12345524\n",
      "Iteration 15164, loss = 7.34428558\n",
      "Iteration 15165, loss = 7.29510155\n",
      "Iteration 15166, loss = 7.39838174\n",
      "Iteration 15167, loss = 7.98057689\n",
      "Iteration 15168, loss = 8.00681284\n",
      "Iteration 15169, loss = 7.49522291\n",
      "Iteration 15170, loss = 6.73104685\n",
      "Iteration 15171, loss = 6.63508250\n",
      "Iteration 15172, loss = 6.68795683\n",
      "Iteration 15173, loss = 6.51284599\n",
      "Iteration 15174, loss = 6.80342066\n",
      "Iteration 15175, loss = 6.72918683\n",
      "Iteration 15176, loss = 6.95019616\n",
      "Iteration 15177, loss = 6.44200746\n",
      "Iteration 15178, loss = 6.84646468\n",
      "Iteration 15179, loss = 6.62378772\n",
      "Iteration 15180, loss = 6.95512932\n",
      "Iteration 15181, loss = 6.72117703\n",
      "Iteration 15182, loss = 6.44098241\n",
      "Iteration 15183, loss = 6.79644377\n",
      "Iteration 15184, loss = 6.80507479\n",
      "Iteration 15185, loss = 7.12962525\n",
      "Iteration 15186, loss = 7.22130412\n",
      "Iteration 15187, loss = 8.51586321\n",
      "Iteration 15188, loss = 7.63491363\n",
      "Iteration 15189, loss = 8.03002589\n",
      "Iteration 15190, loss = 7.24537805\n",
      "Iteration 15191, loss = 7.47248305\n",
      "Iteration 15192, loss = 8.10046039\n",
      "Iteration 15193, loss = 7.99355864\n",
      "Iteration 15194, loss = 8.32874471\n",
      "Iteration 15195, loss = 7.31286187\n",
      "Iteration 15196, loss = 6.71402328\n",
      "Iteration 15197, loss = 6.57296329\n",
      "Iteration 15198, loss = 6.80182782\n",
      "Iteration 15199, loss = 6.98522760\n",
      "Iteration 15200, loss = 6.81902442\n",
      "Iteration 15201, loss = 6.81800592\n",
      "Iteration 15202, loss = 8.34547308\n",
      "Iteration 15203, loss = 7.73329765\n",
      "Iteration 15204, loss = 8.14373979\n",
      "Iteration 15205, loss = 8.17780356\n",
      "Iteration 15206, loss = 7.74296479\n",
      "Iteration 15207, loss = 7.16343093\n",
      "Iteration 15208, loss = 6.54182994\n",
      "Iteration 15209, loss = 6.98690276\n",
      "Iteration 15210, loss = 6.82768559\n",
      "Iteration 15211, loss = 6.98738145\n",
      "Iteration 15212, loss = 7.01471055\n",
      "Iteration 15213, loss = 7.05243415\n",
      "Iteration 15214, loss = 6.67244848\n",
      "Iteration 15215, loss = 7.24853490\n",
      "Iteration 15216, loss = 6.81581713\n",
      "Iteration 15217, loss = 7.39220227\n",
      "Iteration 15218, loss = 7.16312542\n",
      "Iteration 15219, loss = 8.28031401\n",
      "Iteration 15220, loss = 8.46753547\n",
      "Iteration 15221, loss = 7.11646327\n",
      "Iteration 15222, loss = 7.17176513\n",
      "Iteration 15223, loss = 7.79372269\n",
      "Iteration 15224, loss = 7.78360585\n",
      "Iteration 15225, loss = 8.01032325\n",
      "Iteration 15226, loss = 7.88496180\n",
      "Iteration 15227, loss = 7.14122198\n",
      "Iteration 15228, loss = 6.82109587\n",
      "Iteration 15229, loss = 7.08170949\n",
      "Iteration 15230, loss = 6.54204096\n",
      "Iteration 15231, loss = 7.32847703\n",
      "Iteration 15232, loss = 6.89522612\n",
      "Iteration 15233, loss = 6.91783769\n",
      "Iteration 15234, loss = 6.85218715\n",
      "Iteration 15235, loss = 7.17303462\n",
      "Iteration 15236, loss = 6.87965497\n",
      "Iteration 15237, loss = 6.94519923\n",
      "Iteration 15238, loss = 7.00700943\n",
      "Iteration 15239, loss = 7.08171665\n",
      "Iteration 15240, loss = 6.52094666\n",
      "Iteration 15241, loss = 6.84087886\n",
      "Iteration 15242, loss = 7.59774052\n",
      "Iteration 15243, loss = 7.42312265\n",
      "Iteration 15244, loss = 7.10226837\n",
      "Iteration 15245, loss = 8.14509767\n",
      "Iteration 15246, loss = 7.43438978\n",
      "Iteration 15247, loss = 6.98884798\n",
      "Iteration 15248, loss = 7.10460650\n",
      "Iteration 15249, loss = 7.35312675\n",
      "Iteration 15250, loss = 7.52412853\n",
      "Iteration 15251, loss = 7.03849279\n",
      "Iteration 15252, loss = 7.22039182\n",
      "Iteration 15253, loss = 6.65398384\n",
      "Iteration 15254, loss = 6.51489490\n",
      "Iteration 15255, loss = 7.64620409\n",
      "Iteration 15256, loss = 7.01284514\n",
      "Iteration 15257, loss = 8.04234783\n",
      "Iteration 15258, loss = 7.50900488\n",
      "Iteration 15259, loss = 7.68453077\n",
      "Iteration 15260, loss = 8.02392245\n",
      "Iteration 15261, loss = 8.78344856\n",
      "Iteration 15262, loss = 8.54187596\n",
      "Iteration 15263, loss = 7.95675751\n",
      "Iteration 15264, loss = 7.56101749\n",
      "Iteration 15265, loss = 8.33558221\n",
      "Iteration 15266, loss = 7.14381299\n",
      "Iteration 15267, loss = 6.70501628\n",
      "Iteration 15268, loss = 7.40570301\n",
      "Iteration 15269, loss = 7.41703849\n",
      "Iteration 15270, loss = 7.24289913\n",
      "Iteration 15271, loss = 6.96461418\n",
      "Iteration 15272, loss = 6.86619988\n",
      "Iteration 15273, loss = 7.10212488\n",
      "Iteration 15274, loss = 7.89247024\n",
      "Iteration 15275, loss = 6.57491723\n",
      "Iteration 15276, loss = 7.44635131\n",
      "Iteration 15277, loss = 7.90448194\n",
      "Iteration 15278, loss = 7.72821259\n",
      "Iteration 15279, loss = 6.99240334\n",
      "Iteration 15280, loss = 6.96234939\n",
      "Iteration 15281, loss = 6.77137901\n",
      "Iteration 15282, loss = 6.89402204\n",
      "Iteration 15283, loss = 7.70066282\n",
      "Iteration 15284, loss = 7.23896449\n",
      "Iteration 15285, loss = 8.61691018\n",
      "Iteration 15286, loss = 7.70989046\n",
      "Iteration 15287, loss = 7.87527822\n",
      "Iteration 15288, loss = 6.79408302\n",
      "Iteration 15289, loss = 6.55721947\n",
      "Iteration 15290, loss = 6.55456708\n",
      "Iteration 15291, loss = 6.76159996\n",
      "Iteration 15292, loss = 6.64293654\n",
      "Iteration 15293, loss = 6.56919736\n",
      "Iteration 15294, loss = 6.97521360\n",
      "Iteration 15295, loss = 7.18768805\n",
      "Iteration 15296, loss = 6.66892967\n",
      "Iteration 15297, loss = 6.77883084\n",
      "Iteration 15298, loss = 6.84947724\n",
      "Iteration 15299, loss = 6.69402634\n",
      "Iteration 15300, loss = 6.96247855\n",
      "Iteration 15301, loss = 7.18206205\n",
      "Iteration 15302, loss = 7.08653227\n",
      "Iteration 15303, loss = 7.49286275\n",
      "Iteration 15304, loss = 7.15778636\n",
      "Iteration 15305, loss = 7.14047970\n",
      "Iteration 15306, loss = 6.84231583\n",
      "Iteration 15307, loss = 6.74831450\n",
      "Iteration 15308, loss = 6.65303853\n",
      "Iteration 15309, loss = 7.04347202\n",
      "Iteration 15310, loss = 6.93437689\n",
      "Iteration 15311, loss = 6.98502052\n",
      "Iteration 15312, loss = 6.48554361\n",
      "Iteration 15313, loss = 6.37956432\n",
      "Iteration 15314, loss = 6.41754537\n",
      "Iteration 15315, loss = 6.65954959\n",
      "Iteration 15316, loss = 7.08486310\n",
      "Iteration 15317, loss = 6.72247603\n",
      "Iteration 15318, loss = 6.90026191\n",
      "Iteration 15319, loss = 6.61027620\n",
      "Iteration 15320, loss = 6.98995626\n",
      "Iteration 15321, loss = 7.41424315\n",
      "Iteration 15322, loss = 8.14324132\n",
      "Iteration 15323, loss = 7.12713785\n",
      "Iteration 15324, loss = 8.22294217\n",
      "Iteration 15325, loss = 9.27087393\n",
      "Iteration 15326, loss = 7.63677301\n",
      "Iteration 15327, loss = 6.86391567\n",
      "Iteration 15328, loss = 6.79317898\n",
      "Iteration 15329, loss = 6.59266278\n",
      "Iteration 15330, loss = 6.99891352\n",
      "Iteration 15331, loss = 6.57724845\n",
      "Iteration 15332, loss = 6.58985934\n",
      "Iteration 15333, loss = 6.87146037\n",
      "Iteration 15334, loss = 6.97847109\n",
      "Iteration 15335, loss = 6.79443038\n",
      "Iteration 15336, loss = 6.59267521\n",
      "Iteration 15337, loss = 6.65867607\n",
      "Iteration 15338, loss = 6.58405079\n",
      "Iteration 15339, loss = 7.13883412\n",
      "Iteration 15340, loss = 6.91948828\n",
      "Iteration 15341, loss = 7.24899505\n",
      "Iteration 15342, loss = 6.87341865\n",
      "Iteration 15343, loss = 7.17313078\n",
      "Iteration 15344, loss = 7.16020424\n",
      "Iteration 15345, loss = 6.54164642\n",
      "Iteration 15346, loss = 6.85872811\n",
      "Iteration 15347, loss = 7.60990519\n",
      "Iteration 15348, loss = 8.21221796\n",
      "Iteration 15349, loss = 7.10660794\n",
      "Iteration 15350, loss = 7.56436480\n",
      "Iteration 15351, loss = 7.78000846\n",
      "Iteration 15352, loss = 8.51063869\n",
      "Iteration 15353, loss = 9.25971098\n",
      "Iteration 15354, loss = 7.73098659\n",
      "Iteration 15355, loss = 7.04741471\n",
      "Iteration 15356, loss = 6.70222480\n",
      "Iteration 15357, loss = 6.75658922\n",
      "Iteration 15358, loss = 6.69578638\n",
      "Iteration 15359, loss = 6.66213308\n",
      "Iteration 15360, loss = 6.86725928\n",
      "Iteration 15361, loss = 6.92647115\n",
      "Iteration 15362, loss = 6.99176828\n",
      "Iteration 15363, loss = 6.86649484\n",
      "Iteration 15364, loss = 6.95526866\n",
      "Iteration 15365, loss = 7.44183368\n",
      "Iteration 15366, loss = 7.27359919\n",
      "Iteration 15367, loss = 7.43878459\n",
      "Iteration 15368, loss = 7.66668340\n",
      "Iteration 15369, loss = 6.54769682\n",
      "Iteration 15370, loss = 6.46240802\n",
      "Iteration 15371, loss = 6.85252910\n",
      "Iteration 15372, loss = 6.68406688\n",
      "Iteration 15373, loss = 6.81177413\n",
      "Iteration 15374, loss = 6.68381753\n",
      "Iteration 15375, loss = 6.67547917\n",
      "Iteration 15376, loss = 7.06491781\n",
      "Iteration 15377, loss = 7.13540570\n",
      "Iteration 15378, loss = 6.54524000\n",
      "Iteration 15379, loss = 6.78455470\n",
      "Iteration 15380, loss = 6.98386346\n",
      "Iteration 15381, loss = 7.15638786\n",
      "Iteration 15382, loss = 7.31545007\n",
      "Iteration 15383, loss = 7.42828752\n",
      "Iteration 15384, loss = 8.12390887\n",
      "Iteration 15385, loss = 8.31789136\n",
      "Iteration 15386, loss = 7.29005724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15387, loss = 7.33154521\n",
      "Iteration 15388, loss = 7.35127506\n",
      "Iteration 15389, loss = 6.90038375\n",
      "Iteration 15390, loss = 7.45812928\n",
      "Iteration 15391, loss = 6.45462193\n",
      "Iteration 15392, loss = 6.74407376\n",
      "Iteration 15393, loss = 7.71205681\n",
      "Iteration 15394, loss = 7.80943019\n",
      "Iteration 15395, loss = 7.90749044\n",
      "Iteration 15396, loss = 7.22608069\n",
      "Iteration 15397, loss = 6.86571332\n",
      "Iteration 15398, loss = 7.05971043\n",
      "Iteration 15399, loss = 6.67558974\n",
      "Iteration 15400, loss = 6.61766452\n",
      "Iteration 15401, loss = 7.10901332\n",
      "Iteration 15402, loss = 7.25318948\n",
      "Iteration 15403, loss = 6.60625796\n",
      "Iteration 15404, loss = 6.74245363\n",
      "Iteration 15405, loss = 7.25061989\n",
      "Iteration 15406, loss = 6.92319088\n",
      "Iteration 15407, loss = 6.71446056\n",
      "Iteration 15408, loss = 7.22816999\n",
      "Iteration 15409, loss = 8.67684124\n",
      "Iteration 15410, loss = 9.15936658\n",
      "Iteration 15411, loss = 8.47956010\n",
      "Iteration 15412, loss = 7.78534881\n",
      "Iteration 15413, loss = 6.80310386\n",
      "Iteration 15414, loss = 7.63034051\n",
      "Iteration 15415, loss = 7.06892659\n",
      "Iteration 15416, loss = 7.06301837\n",
      "Iteration 15417, loss = 7.87160192\n",
      "Iteration 15418, loss = 7.69915113\n",
      "Iteration 15419, loss = 7.64445226\n",
      "Iteration 15420, loss = 7.36312588\n",
      "Iteration 15421, loss = 7.25108555\n",
      "Iteration 15422, loss = 7.43583201\n",
      "Iteration 15423, loss = 7.00363728\n",
      "Iteration 15424, loss = 7.13968865\n",
      "Iteration 15425, loss = 6.99451403\n",
      "Iteration 15426, loss = 7.63214956\n",
      "Iteration 15427, loss = 6.68998164\n",
      "Iteration 15428, loss = 7.08233228\n",
      "Iteration 15429, loss = 8.34235552\n",
      "Iteration 15430, loss = 7.92427426\n",
      "Iteration 15431, loss = 8.93293919\n",
      "Iteration 15432, loss = 8.52958191\n",
      "Iteration 15433, loss = 8.33878681\n",
      "Iteration 15434, loss = 9.09805804\n",
      "Iteration 15435, loss = 9.02293206\n",
      "Iteration 15436, loss = 7.90451276\n",
      "Iteration 15437, loss = 6.84814370\n",
      "Iteration 15438, loss = 6.94277640\n",
      "Iteration 15439, loss = 6.87881581\n",
      "Iteration 15440, loss = 6.78038654\n",
      "Iteration 15441, loss = 6.87411692\n",
      "Iteration 15442, loss = 6.89762408\n",
      "Iteration 15443, loss = 7.22147550\n",
      "Iteration 15444, loss = 6.86272206\n",
      "Iteration 15445, loss = 7.06952183\n",
      "Iteration 15446, loss = 6.75889245\n",
      "Iteration 15447, loss = 7.03571397\n",
      "Iteration 15448, loss = 7.51718644\n",
      "Iteration 15449, loss = 7.62838173\n",
      "Iteration 15450, loss = 7.12337250\n",
      "Iteration 15451, loss = 7.85495803\n",
      "Iteration 15452, loss = 7.87206354\n",
      "Iteration 15453, loss = 6.84482157\n",
      "Iteration 15454, loss = 7.32422285\n",
      "Iteration 15455, loss = 7.45769893\n",
      "Iteration 15456, loss = 7.20596150\n",
      "Iteration 15457, loss = 7.26767569\n",
      "Iteration 15458, loss = 7.55745554\n",
      "Iteration 15459, loss = 7.76600377\n",
      "Iteration 15460, loss = 7.19799552\n",
      "Iteration 15461, loss = 7.35727578\n",
      "Iteration 15462, loss = 7.02753024\n",
      "Iteration 15463, loss = 7.07970111\n",
      "Iteration 15464, loss = 6.68540798\n",
      "Iteration 15465, loss = 7.34351576\n",
      "Iteration 15466, loss = 6.87861671\n",
      "Iteration 15467, loss = 6.83440655\n",
      "Iteration 15468, loss = 7.24093031\n",
      "Iteration 15469, loss = 7.56256029\n",
      "Iteration 15470, loss = 7.14361989\n",
      "Iteration 15471, loss = 6.49935958\n",
      "Iteration 15472, loss = 6.57001155\n",
      "Iteration 15473, loss = 7.02216324\n",
      "Iteration 15474, loss = 7.19786969\n",
      "Iteration 15475, loss = 6.73702818\n",
      "Iteration 15476, loss = 6.68020570\n",
      "Iteration 15477, loss = 6.82569846\n",
      "Iteration 15478, loss = 7.49340155\n",
      "Iteration 15479, loss = 7.72832142\n",
      "Iteration 15480, loss = 6.54962044\n",
      "Iteration 15481, loss = 6.77397168\n",
      "Iteration 15482, loss = 6.61275443\n",
      "Iteration 15483, loss = 6.84365205\n",
      "Iteration 15484, loss = 6.98614237\n",
      "Iteration 15485, loss = 7.74843553\n",
      "Iteration 15486, loss = 7.31707788\n",
      "Iteration 15487, loss = 7.85337361\n",
      "Iteration 15488, loss = 9.35772644\n",
      "Iteration 15489, loss = 7.29810375\n",
      "Iteration 15490, loss = 7.10483740\n",
      "Iteration 15491, loss = 6.68969456\n",
      "Iteration 15492, loss = 6.80196492\n",
      "Iteration 15493, loss = 7.25233112\n",
      "Iteration 15494, loss = 6.59099465\n",
      "Iteration 15495, loss = 6.87445612\n",
      "Iteration 15496, loss = 6.82662561\n",
      "Iteration 15497, loss = 7.07834782\n",
      "Iteration 15498, loss = 7.46680316\n",
      "Iteration 15499, loss = 6.92572162\n",
      "Iteration 15500, loss = 7.64316307\n",
      "Iteration 15501, loss = 6.93540974\n",
      "Iteration 15502, loss = 7.01649367\n",
      "Iteration 15503, loss = 6.69825625\n",
      "Iteration 15504, loss = 6.94044647\n",
      "Iteration 15505, loss = 7.34643229\n",
      "Iteration 15506, loss = 7.70140363\n",
      "Iteration 15507, loss = 7.26573559\n",
      "Iteration 15508, loss = 7.71062899\n",
      "Iteration 15509, loss = 8.21029211\n",
      "Iteration 15510, loss = 7.13914754\n",
      "Iteration 15511, loss = 6.66400824\n",
      "Iteration 15512, loss = 7.32738544\n",
      "Iteration 15513, loss = 6.70799598\n",
      "Iteration 15514, loss = 7.63994649\n",
      "Iteration 15515, loss = 7.00216282\n",
      "Iteration 15516, loss = 8.62862062\n",
      "Iteration 15517, loss = 8.51231425\n",
      "Iteration 15518, loss = 7.97313128\n",
      "Iteration 15519, loss = 8.13535875\n",
      "Iteration 15520, loss = 9.06987122\n",
      "Iteration 15521, loss = 8.40388203\n",
      "Iteration 15522, loss = 7.02657656\n",
      "Iteration 15523, loss = 8.32855856\n",
      "Iteration 15524, loss = 7.73274356\n",
      "Iteration 15525, loss = 8.43390986\n",
      "Iteration 15526, loss = 7.26106414\n",
      "Iteration 15527, loss = 7.23966733\n",
      "Iteration 15528, loss = 7.78954913\n",
      "Iteration 15529, loss = 7.80565456\n",
      "Iteration 15530, loss = 7.57402380\n",
      "Iteration 15531, loss = 7.54905850\n",
      "Iteration 15532, loss = 7.09770107\n",
      "Iteration 15533, loss = 7.27903131\n",
      "Iteration 15534, loss = 7.05716334\n",
      "Iteration 15535, loss = 7.99461414\n",
      "Iteration 15536, loss = 7.70643733\n",
      "Iteration 15537, loss = 8.47674573\n",
      "Iteration 15538, loss = 7.35270362\n",
      "Iteration 15539, loss = 6.58075820\n",
      "Iteration 15540, loss = 7.24607849\n",
      "Iteration 15541, loss = 8.51428602\n",
      "Iteration 15542, loss = 7.31390998\n",
      "Iteration 15543, loss = 7.04282825\n",
      "Iteration 15544, loss = 6.86647575\n",
      "Iteration 15545, loss = 7.12952836\n",
      "Iteration 15546, loss = 7.24991310\n",
      "Iteration 15547, loss = 6.77120238\n",
      "Iteration 15548, loss = 6.67587805\n",
      "Iteration 15549, loss = 7.05861914\n",
      "Iteration 15550, loss = 7.47650406\n",
      "Iteration 15551, loss = 6.83892397\n",
      "Iteration 15552, loss = 7.00904039\n",
      "Iteration 15553, loss = 7.15158918\n",
      "Iteration 15554, loss = 6.86274996\n",
      "Iteration 15555, loss = 7.44481217\n",
      "Iteration 15556, loss = 7.28792637\n",
      "Iteration 15557, loss = 6.55886541\n",
      "Iteration 15558, loss = 6.78865033\n",
      "Iteration 15559, loss = 7.07014100\n",
      "Iteration 15560, loss = 6.42289944\n",
      "Iteration 15561, loss = 7.16536039\n",
      "Iteration 15562, loss = 6.77055337\n",
      "Iteration 15563, loss = 6.80988107\n",
      "Iteration 15564, loss = 7.38689396\n",
      "Iteration 15565, loss = 7.39210078\n",
      "Iteration 15566, loss = 8.03610991\n",
      "Iteration 15567, loss = 8.25551437\n",
      "Iteration 15568, loss = 7.56964347\n",
      "Iteration 15569, loss = 7.84700470\n",
      "Iteration 15570, loss = 7.41064739\n",
      "Iteration 15571, loss = 7.17345833\n",
      "Iteration 15572, loss = 7.74711427\n",
      "Iteration 15573, loss = 7.52840065\n",
      "Iteration 15574, loss = 7.11133302\n",
      "Iteration 15575, loss = 6.90093640\n",
      "Iteration 15576, loss = 7.34436089\n",
      "Iteration 15577, loss = 6.91917185\n",
      "Iteration 15578, loss = 6.50522279\n",
      "Iteration 15579, loss = 6.80087440\n",
      "Iteration 15580, loss = 6.41272446\n",
      "Iteration 15581, loss = 6.81672967\n",
      "Iteration 15582, loss = 7.14878604\n",
      "Iteration 15583, loss = 7.00947761\n",
      "Iteration 15584, loss = 7.14932812\n",
      "Iteration 15585, loss = 6.79829422\n",
      "Iteration 15586, loss = 6.65132633\n",
      "Iteration 15587, loss = 7.36371748\n",
      "Iteration 15588, loss = 6.72871204\n",
      "Iteration 15589, loss = 6.50165857\n",
      "Iteration 15590, loss = 6.63637282\n",
      "Iteration 15591, loss = 6.92161045\n",
      "Iteration 15592, loss = 7.29922328\n",
      "Iteration 15593, loss = 7.10841263\n",
      "Iteration 15594, loss = 6.87846163\n",
      "Iteration 15595, loss = 7.03761257\n",
      "Iteration 15596, loss = 6.78169410\n",
      "Iteration 15597, loss = 6.99015039\n",
      "Iteration 15598, loss = 7.04771482\n",
      "Iteration 15599, loss = 7.21729045\n",
      "Iteration 15600, loss = 8.55674185\n",
      "Iteration 15601, loss = 9.17850390\n",
      "Iteration 15602, loss = 9.28357692\n",
      "Iteration 15603, loss = 9.43088097\n",
      "Iteration 15604, loss = 7.97435619\n",
      "Iteration 15605, loss = 9.04180286\n",
      "Iteration 15606, loss = 8.08318906\n",
      "Iteration 15607, loss = 6.86315026\n",
      "Iteration 15608, loss = 6.81074410\n",
      "Iteration 15609, loss = 6.54953578\n",
      "Iteration 15610, loss = 7.91763190\n",
      "Iteration 15611, loss = 6.99061762\n",
      "Iteration 15612, loss = 7.13768613\n",
      "Iteration 15613, loss = 6.64461409\n",
      "Iteration 15614, loss = 6.63098524\n",
      "Iteration 15615, loss = 7.10866545\n",
      "Iteration 15616, loss = 7.90443204\n",
      "Iteration 15617, loss = 7.36198775\n",
      "Iteration 15618, loss = 7.02852179\n",
      "Iteration 15619, loss = 7.57141390\n",
      "Iteration 15620, loss = 6.71679313\n",
      "Iteration 15621, loss = 7.09604350\n",
      "Iteration 15622, loss = 7.97441427\n",
      "Iteration 15623, loss = 6.67796594\n",
      "Iteration 15624, loss = 7.04954420\n",
      "Iteration 15625, loss = 6.58656804\n",
      "Iteration 15626, loss = 7.44606154\n",
      "Iteration 15627, loss = 6.57544269\n",
      "Iteration 15628, loss = 6.55729837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15629, loss = 6.81699174\n",
      "Iteration 15630, loss = 8.07106843\n",
      "Iteration 15631, loss = 7.34676375\n",
      "Iteration 15632, loss = 7.33552939\n",
      "Iteration 15633, loss = 6.65930981\n",
      "Iteration 15634, loss = 6.61761741\n",
      "Iteration 15635, loss = 6.82209469\n",
      "Iteration 15636, loss = 6.85603944\n",
      "Iteration 15637, loss = 6.68210011\n",
      "Iteration 15638, loss = 6.74635283\n",
      "Iteration 15639, loss = 7.08530534\n",
      "Iteration 15640, loss = 7.37403632\n",
      "Iteration 15641, loss = 7.02874151\n",
      "Iteration 15642, loss = 7.16459722\n",
      "Iteration 15643, loss = 7.07958041\n",
      "Iteration 15644, loss = 6.54685595\n",
      "Iteration 15645, loss = 6.95087050\n",
      "Iteration 15646, loss = 6.71486771\n",
      "Iteration 15647, loss = 6.59211686\n",
      "Iteration 15648, loss = 6.52560165\n",
      "Iteration 15649, loss = 6.51835187\n",
      "Iteration 15650, loss = 6.44646557\n",
      "Iteration 15651, loss = 6.56170231\n",
      "Iteration 15652, loss = 7.11644874\n",
      "Iteration 15653, loss = 6.64616390\n",
      "Iteration 15654, loss = 6.67010135\n",
      "Iteration 15655, loss = 6.87011128\n",
      "Iteration 15656, loss = 6.67752845\n",
      "Iteration 15657, loss = 6.52396304\n",
      "Iteration 15658, loss = 6.62494377\n",
      "Iteration 15659, loss = 6.90143668\n",
      "Iteration 15660, loss = 6.88998775\n",
      "Iteration 15661, loss = 6.86447281\n",
      "Iteration 15662, loss = 6.59965989\n",
      "Iteration 15663, loss = 6.95101964\n",
      "Iteration 15664, loss = 6.94050393\n",
      "Iteration 15665, loss = 6.50909028\n",
      "Iteration 15666, loss = 6.58732548\n",
      "Iteration 15667, loss = 6.62152212\n",
      "Iteration 15668, loss = 7.02798978\n",
      "Iteration 15669, loss = 6.80260536\n",
      "Iteration 15670, loss = 6.64478476\n",
      "Iteration 15671, loss = 7.44552882\n",
      "Iteration 15672, loss = 7.15788736\n",
      "Iteration 15673, loss = 8.09287056\n",
      "Iteration 15674, loss = 7.54334995\n",
      "Iteration 15675, loss = 7.16657108\n",
      "Iteration 15676, loss = 8.69458695\n",
      "Iteration 15677, loss = 8.75699602\n",
      "Iteration 15678, loss = 7.42278360\n",
      "Iteration 15679, loss = 8.16745840\n",
      "Iteration 15680, loss = 9.33747073\n",
      "Iteration 15681, loss = 7.98556443\n",
      "Iteration 15682, loss = 8.73870532\n",
      "Iteration 15683, loss = 8.26471852\n",
      "Iteration 15684, loss = 7.97784751\n",
      "Iteration 15685, loss = 8.04763189\n",
      "Iteration 15686, loss = 7.84407642\n",
      "Iteration 15687, loss = 7.67243798\n",
      "Iteration 15688, loss = 7.35981747\n",
      "Iteration 15689, loss = 6.54463630\n",
      "Iteration 15690, loss = 7.40815433\n",
      "Iteration 15691, loss = 7.49426005\n",
      "Iteration 15692, loss = 7.84468596\n",
      "Iteration 15693, loss = 6.57780144\n",
      "Iteration 15694, loss = 6.83465538\n",
      "Iteration 15695, loss = 6.81640038\n",
      "Iteration 15696, loss = 6.45412665\n",
      "Iteration 15697, loss = 6.62587167\n",
      "Iteration 15698, loss = 6.86008584\n",
      "Iteration 15699, loss = 6.29032027\n",
      "Iteration 15700, loss = 6.53260968\n",
      "Iteration 15701, loss = 6.71071977\n",
      "Iteration 15702, loss = 7.31476539\n",
      "Iteration 15703, loss = 6.87683675\n",
      "Iteration 15704, loss = 6.86494408\n",
      "Iteration 15705, loss = 6.44663516\n",
      "Iteration 15706, loss = 6.65115658\n",
      "Iteration 15707, loss = 6.63449030\n",
      "Iteration 15708, loss = 7.06259493\n",
      "Iteration 15709, loss = 6.98598265\n",
      "Iteration 15710, loss = 6.76859362\n",
      "Iteration 15711, loss = 6.39698299\n",
      "Iteration 15712, loss = 7.11461063\n",
      "Iteration 15713, loss = 6.96584009\n",
      "Iteration 15714, loss = 6.96860993\n",
      "Iteration 15715, loss = 7.70961000\n",
      "Iteration 15716, loss = 7.20837126\n",
      "Iteration 15717, loss = 6.87043888\n",
      "Iteration 15718, loss = 6.99674310\n",
      "Iteration 15719, loss = 6.85626875\n",
      "Iteration 15720, loss = 7.12399523\n",
      "Iteration 15721, loss = 7.69016601\n",
      "Iteration 15722, loss = 8.48579965\n",
      "Iteration 15723, loss = 7.65604490\n",
      "Iteration 15724, loss = 8.00544596\n",
      "Iteration 15725, loss = 7.35556533\n",
      "Iteration 15726, loss = 7.27181774\n",
      "Iteration 15727, loss = 7.87055813\n",
      "Iteration 15728, loss = 7.89585945\n",
      "Iteration 15729, loss = 7.73946306\n",
      "Iteration 15730, loss = 7.76818511\n",
      "Iteration 15731, loss = 7.02208768\n",
      "Iteration 15732, loss = 6.84430942\n",
      "Iteration 15733, loss = 6.68214074\n",
      "Iteration 15734, loss = 6.59184274\n",
      "Iteration 15735, loss = 6.83460763\n",
      "Iteration 15736, loss = 7.47543607\n",
      "Iteration 15737, loss = 7.78665082\n",
      "Iteration 15738, loss = 6.70776136\n",
      "Iteration 15739, loss = 6.44108946\n",
      "Iteration 15740, loss = 6.72440454\n",
      "Iteration 15741, loss = 6.83679700\n",
      "Iteration 15742, loss = 7.32915894\n",
      "Iteration 15743, loss = 7.25809732\n",
      "Iteration 15744, loss = 7.81576245\n",
      "Iteration 15745, loss = 7.40927201\n",
      "Iteration 15746, loss = 6.62366305\n",
      "Iteration 15747, loss = 6.80672428\n",
      "Iteration 15748, loss = 7.05153912\n",
      "Iteration 15749, loss = 7.14657886\n",
      "Iteration 15750, loss = 6.72518074\n",
      "Iteration 15751, loss = 7.05114946\n",
      "Iteration 15752, loss = 7.68070640\n",
      "Iteration 15753, loss = 6.90568799\n",
      "Iteration 15754, loss = 6.48965738\n",
      "Iteration 15755, loss = 6.57155824\n",
      "Iteration 15756, loss = 7.04461380\n",
      "Iteration 15757, loss = 6.97965718\n",
      "Iteration 15758, loss = 7.36986149\n",
      "Iteration 15759, loss = 7.00858459\n",
      "Iteration 15760, loss = 7.24143347\n",
      "Iteration 15761, loss = 6.78766564\n",
      "Iteration 15762, loss = 7.26099284\n",
      "Iteration 15763, loss = 7.06317378\n",
      "Iteration 15764, loss = 6.76079724\n",
      "Iteration 15765, loss = 6.86712253\n",
      "Iteration 15766, loss = 7.12584464\n",
      "Iteration 15767, loss = 7.14432192\n",
      "Iteration 15768, loss = 6.96540198\n",
      "Iteration 15769, loss = 7.60382216\n",
      "Iteration 15770, loss = 7.19097664\n",
      "Iteration 15771, loss = 6.51421319\n",
      "Iteration 15772, loss = 7.08726073\n",
      "Iteration 15773, loss = 6.59140991\n",
      "Iteration 15774, loss = 6.60751665\n",
      "Iteration 15775, loss = 7.64804570\n",
      "Iteration 15776, loss = 7.22335120\n",
      "Iteration 15777, loss = 7.37660591\n",
      "Iteration 15778, loss = 7.09719883\n",
      "Iteration 15779, loss = 7.35470599\n",
      "Iteration 15780, loss = 7.34536450\n",
      "Iteration 15781, loss = 8.75447201\n",
      "Iteration 15782, loss = 8.83026216\n",
      "Iteration 15783, loss = 10.23833482\n",
      "Iteration 15784, loss = 9.78854432\n",
      "Iteration 15785, loss = 9.36637626\n",
      "Iteration 15786, loss = 7.72781751\n",
      "Iteration 15787, loss = 7.16112003\n",
      "Iteration 15788, loss = 6.66215125\n",
      "Iteration 15789, loss = 6.35917211\n",
      "Iteration 15790, loss = 7.08001002\n",
      "Iteration 15791, loss = 6.70884994\n",
      "Iteration 15792, loss = 7.09377827\n",
      "Iteration 15793, loss = 6.49592013\n",
      "Iteration 15794, loss = 6.41878861\n",
      "Iteration 15795, loss = 6.77773397\n",
      "Iteration 15796, loss = 7.51919135\n",
      "Iteration 15797, loss = 7.73873869\n",
      "Iteration 15798, loss = 6.60377932\n",
      "Iteration 15799, loss = 7.03750055\n",
      "Iteration 15800, loss = 6.86648729\n",
      "Iteration 15801, loss = 7.32195792\n",
      "Iteration 15802, loss = 6.94236000\n",
      "Iteration 15803, loss = 6.72881144\n",
      "Iteration 15804, loss = 6.64625464\n",
      "Iteration 15805, loss = 6.57992772\n",
      "Iteration 15806, loss = 6.63772429\n",
      "Iteration 15807, loss = 6.83677527\n",
      "Iteration 15808, loss = 6.52345988\n",
      "Iteration 15809, loss = 6.57279091\n",
      "Iteration 15810, loss = 7.33017560\n",
      "Iteration 15811, loss = 6.87033905\n",
      "Iteration 15812, loss = 7.45471279\n",
      "Iteration 15813, loss = 7.57888863\n",
      "Iteration 15814, loss = 7.26852351\n",
      "Iteration 15815, loss = 6.99283381\n",
      "Iteration 15816, loss = 7.26365512\n",
      "Iteration 15817, loss = 7.06854059\n",
      "Iteration 15818, loss = 6.70361117\n",
      "Iteration 15819, loss = 6.80510713\n",
      "Iteration 15820, loss = 7.22448271\n",
      "Iteration 15821, loss = 6.98991086\n",
      "Iteration 15822, loss = 7.57314148\n",
      "Iteration 15823, loss = 7.14749004\n",
      "Iteration 15824, loss = 7.23702152\n",
      "Iteration 15825, loss = 7.96094418\n",
      "Iteration 15826, loss = 7.17241879\n",
      "Iteration 15827, loss = 6.53672656\n",
      "Iteration 15828, loss = 6.79911079\n",
      "Iteration 15829, loss = 6.58830942\n",
      "Iteration 15830, loss = 6.32991496\n",
      "Iteration 15831, loss = 6.56033842\n",
      "Iteration 15832, loss = 6.70528330\n",
      "Iteration 15833, loss = 7.04995035\n",
      "Iteration 15834, loss = 7.12430332\n",
      "Iteration 15835, loss = 8.23948019\n",
      "Iteration 15836, loss = 8.18336328\n",
      "Iteration 15837, loss = 6.47903216\n",
      "Iteration 15838, loss = 6.69215644\n",
      "Iteration 15839, loss = 6.82545285\n",
      "Iteration 15840, loss = 6.45767802\n",
      "Iteration 15841, loss = 6.59666997\n",
      "Iteration 15842, loss = 6.64220708\n",
      "Iteration 15843, loss = 6.53530331\n",
      "Iteration 15844, loss = 6.67042349\n",
      "Iteration 15845, loss = 6.60000825\n",
      "Iteration 15846, loss = 6.60155917\n",
      "Iteration 15847, loss = 7.14607618\n",
      "Iteration 15848, loss = 6.90202473\n",
      "Iteration 15849, loss = 7.06879039\n",
      "Iteration 15850, loss = 6.94647593\n",
      "Iteration 15851, loss = 7.40003608\n",
      "Iteration 15852, loss = 7.86774403\n",
      "Iteration 15853, loss = 7.54814600\n",
      "Iteration 15854, loss = 7.66497964\n",
      "Iteration 15855, loss = 7.61065889\n",
      "Iteration 15856, loss = 6.59687040\n",
      "Iteration 15857, loss = 6.66871822\n",
      "Iteration 15858, loss = 6.94243539\n",
      "Iteration 15859, loss = 6.66244670\n",
      "Iteration 15860, loss = 7.16167979\n",
      "Iteration 15861, loss = 7.19587004\n",
      "Iteration 15862, loss = 8.23886778\n",
      "Iteration 15863, loss = 7.59689061\n",
      "Iteration 15864, loss = 8.14315176\n",
      "Iteration 15865, loss = 6.71617595\n",
      "Iteration 15866, loss = 7.00224704\n",
      "Iteration 15867, loss = 6.41735537\n",
      "Iteration 15868, loss = 6.84614250\n",
      "Iteration 15869, loss = 6.94460044\n",
      "Iteration 15870, loss = 7.91628836\n",
      "Iteration 15871, loss = 9.36407817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15872, loss = 7.76721660\n",
      "Iteration 15873, loss = 7.78630441\n",
      "Iteration 15874, loss = 7.85562693\n",
      "Iteration 15875, loss = 7.67768876\n",
      "Iteration 15876, loss = 7.31457442\n",
      "Iteration 15877, loss = 7.02264562\n",
      "Iteration 15878, loss = 7.32552510\n",
      "Iteration 15879, loss = 7.16633568\n",
      "Iteration 15880, loss = 8.19283660\n",
      "Iteration 15881, loss = 7.48019061\n",
      "Iteration 15882, loss = 6.97298765\n",
      "Iteration 15883, loss = 6.85287130\n",
      "Iteration 15884, loss = 6.66503377\n",
      "Iteration 15885, loss = 6.92664883\n",
      "Iteration 15886, loss = 6.85820480\n",
      "Iteration 15887, loss = 6.88077674\n",
      "Iteration 15888, loss = 7.38063020\n",
      "Iteration 15889, loss = 7.75590098\n",
      "Iteration 15890, loss = 8.37852600\n",
      "Iteration 15891, loss = 6.61115189\n",
      "Iteration 15892, loss = 7.58293777\n",
      "Iteration 15893, loss = 6.72775186\n",
      "Iteration 15894, loss = 6.66963369\n",
      "Iteration 15895, loss = 6.71301606\n",
      "Iteration 15896, loss = 6.50311475\n",
      "Iteration 15897, loss = 7.15585140\n",
      "Iteration 15898, loss = 7.16776519\n",
      "Iteration 15899, loss = 6.79412695\n",
      "Iteration 15900, loss = 7.02715648\n",
      "Iteration 15901, loss = 6.76020158\n",
      "Iteration 15902, loss = 7.27318619\n",
      "Iteration 15903, loss = 7.63841094\n",
      "Iteration 15904, loss = 7.00376183\n",
      "Iteration 15905, loss = 6.78552681\n",
      "Iteration 15906, loss = 7.00257542\n",
      "Iteration 15907, loss = 6.78400520\n",
      "Iteration 15908, loss = 7.04477717\n",
      "Iteration 15909, loss = 7.04222735\n",
      "Iteration 15910, loss = 7.94138651\n",
      "Iteration 15911, loss = 6.81021820\n",
      "Iteration 15912, loss = 7.42799514\n",
      "Iteration 15913, loss = 6.94024296\n",
      "Iteration 15914, loss = 6.54081668\n",
      "Iteration 15915, loss = 6.57297087\n",
      "Iteration 15916, loss = 7.02294052\n",
      "Iteration 15917, loss = 6.41277826\n",
      "Iteration 15918, loss = 6.60833391\n",
      "Iteration 15919, loss = 7.31393257\n",
      "Iteration 15920, loss = 6.58470021\n",
      "Iteration 15921, loss = 6.70150429\n",
      "Iteration 15922, loss = 6.91209261\n",
      "Iteration 15923, loss = 7.25286926\n",
      "Iteration 15924, loss = 7.26919508\n",
      "Iteration 15925, loss = 7.13548144\n",
      "Iteration 15926, loss = 7.25040888\n",
      "Iteration 15927, loss = 7.01104744\n",
      "Iteration 15928, loss = 6.37753340\n",
      "Iteration 15929, loss = 6.50839929\n",
      "Iteration 15930, loss = 6.55824422\n",
      "Iteration 15931, loss = 6.43048022\n",
      "Iteration 15932, loss = 6.53115463\n",
      "Iteration 15933, loss = 7.10478161\n",
      "Iteration 15934, loss = 7.03717994\n",
      "Iteration 15935, loss = 6.77863476\n",
      "Iteration 15936, loss = 6.87676336\n",
      "Iteration 15937, loss = 6.95743797\n",
      "Iteration 15938, loss = 6.85280651\n",
      "Iteration 15939, loss = 6.79639245\n",
      "Iteration 15940, loss = 6.73263983\n",
      "Iteration 15941, loss = 7.05715875\n",
      "Iteration 15942, loss = 7.17237089\n",
      "Iteration 15943, loss = 6.62659833\n",
      "Iteration 15944, loss = 7.72955552\n",
      "Iteration 15945, loss = 7.70487100\n",
      "Iteration 15946, loss = 7.13666464\n",
      "Iteration 15947, loss = 7.51566176\n",
      "Iteration 15948, loss = 8.11580439\n",
      "Iteration 15949, loss = 8.85342154\n",
      "Iteration 15950, loss = 7.54169790\n",
      "Iteration 15951, loss = 6.64717154\n",
      "Iteration 15952, loss = 7.08719144\n",
      "Iteration 15953, loss = 6.45790430\n",
      "Iteration 15954, loss = 6.36108469\n",
      "Iteration 15955, loss = 6.93191076\n",
      "Iteration 15956, loss = 6.49334901\n",
      "Iteration 15957, loss = 6.54298379\n",
      "Iteration 15958, loss = 7.35631499\n",
      "Iteration 15959, loss = 6.70410290\n",
      "Iteration 15960, loss = 6.80385250\n",
      "Iteration 15961, loss = 7.30188351\n",
      "Iteration 15962, loss = 6.73424889\n",
      "Iteration 15963, loss = 6.92987315\n",
      "Iteration 15964, loss = 6.63155919\n",
      "Iteration 15965, loss = 6.89902536\n",
      "Iteration 15966, loss = 7.13335485\n",
      "Iteration 15967, loss = 7.30883235\n",
      "Iteration 15968, loss = 6.69835875\n",
      "Iteration 15969, loss = 7.51915402\n",
      "Iteration 15970, loss = 7.46947893\n",
      "Iteration 15971, loss = 7.60688865\n",
      "Iteration 15972, loss = 7.06634285\n",
      "Iteration 15973, loss = 6.64446771\n",
      "Iteration 15974, loss = 6.59654043\n",
      "Iteration 15975, loss = 7.13814431\n",
      "Iteration 15976, loss = 7.48792670\n",
      "Iteration 15977, loss = 6.40214567\n",
      "Iteration 15978, loss = 8.13451260\n",
      "Iteration 15979, loss = 7.44669859\n",
      "Iteration 15980, loss = 6.87139878\n",
      "Iteration 15981, loss = 6.38681212\n",
      "Iteration 15982, loss = 6.81796423\n",
      "Iteration 15983, loss = 7.58733297\n",
      "Iteration 15984, loss = 7.04119460\n",
      "Iteration 15985, loss = 6.82633991\n",
      "Iteration 15986, loss = 6.87585691\n",
      "Iteration 15987, loss = 6.42562705\n",
      "Iteration 15988, loss = 6.85530853\n",
      "Iteration 15989, loss = 7.04778891\n",
      "Iteration 15990, loss = 6.63988005\n",
      "Iteration 15991, loss = 6.60547415\n",
      "Iteration 15992, loss = 6.67743116\n",
      "Iteration 15993, loss = 6.89679953\n",
      "Iteration 15994, loss = 6.53964409\n",
      "Iteration 15995, loss = 7.17182713\n",
      "Iteration 15996, loss = 7.07672472\n",
      "Iteration 15997, loss = 7.45630028\n",
      "Iteration 15998, loss = 7.41614955\n",
      "Iteration 15999, loss = 8.97054011\n",
      "Iteration 16000, loss = 7.64154552\n",
      "Iteration 16001, loss = 8.30429360\n",
      "Iteration 16002, loss = 8.34784853\n",
      "Iteration 16003, loss = 8.68928549\n",
      "Iteration 16004, loss = 7.21783983\n",
      "Iteration 16005, loss = 6.44230415\n",
      "Iteration 16006, loss = 6.35963790\n",
      "Iteration 16007, loss = 6.72139670\n",
      "Iteration 16008, loss = 6.36214084\n",
      "Iteration 16009, loss = 6.38031543\n",
      "Iteration 16010, loss = 6.64437755\n",
      "Iteration 16011, loss = 6.55019283\n",
      "Iteration 16012, loss = 6.64232726\n",
      "Iteration 16013, loss = 6.21723429\n",
      "Iteration 16014, loss = 6.45418918\n",
      "Iteration 16015, loss = 6.54682852\n",
      "Iteration 16016, loss = 6.96792161\n",
      "Iteration 16017, loss = 6.58797235\n",
      "Iteration 16018, loss = 6.97363437\n",
      "Iteration 16019, loss = 6.39911119\n",
      "Iteration 16020, loss = 6.92426705\n",
      "Iteration 16021, loss = 6.73643273\n",
      "Iteration 16022, loss = 6.72515562\n",
      "Iteration 16023, loss = 7.18434982\n",
      "Iteration 16024, loss = 6.58384637\n",
      "Iteration 16025, loss = 7.56672414\n",
      "Iteration 16026, loss = 7.26882871\n",
      "Iteration 16027, loss = 7.47345995\n",
      "Iteration 16028, loss = 7.08062382\n",
      "Iteration 16029, loss = 6.95673680\n",
      "Iteration 16030, loss = 6.42114912\n",
      "Iteration 16031, loss = 6.66486558\n",
      "Iteration 16032, loss = 6.61968202\n",
      "Iteration 16033, loss = 6.37344712\n",
      "Iteration 16034, loss = 6.54249416\n",
      "Iteration 16035, loss = 6.59732773\n",
      "Iteration 16036, loss = 6.93493660\n",
      "Iteration 16037, loss = 6.59804860\n",
      "Iteration 16038, loss = 6.69554806\n",
      "Iteration 16039, loss = 7.01338211\n",
      "Iteration 16040, loss = 6.71204739\n",
      "Iteration 16041, loss = 6.57742573\n",
      "Iteration 16042, loss = 6.77386334\n",
      "Iteration 16043, loss = 6.51912855\n",
      "Iteration 16044, loss = 7.05620232\n",
      "Iteration 16045, loss = 7.43836175\n",
      "Iteration 16046, loss = 6.55750777\n",
      "Iteration 16047, loss = 6.48728685\n",
      "Iteration 16048, loss = 6.25362742\n",
      "Iteration 16049, loss = 6.48907430\n",
      "Iteration 16050, loss = 7.39091624\n",
      "Iteration 16051, loss = 7.06254168\n",
      "Iteration 16052, loss = 7.29317511\n",
      "Iteration 16053, loss = 7.79981310\n",
      "Iteration 16054, loss = 7.82908841\n",
      "Iteration 16055, loss = 7.77228364\n",
      "Iteration 16056, loss = 8.10980639\n",
      "Iteration 16057, loss = 6.64063643\n",
      "Iteration 16058, loss = 6.69328262\n",
      "Iteration 16059, loss = 7.19268633\n",
      "Iteration 16060, loss = 6.83145503\n",
      "Iteration 16061, loss = 6.98005474\n",
      "Iteration 16062, loss = 6.83797484\n",
      "Iteration 16063, loss = 6.74979551\n",
      "Iteration 16064, loss = 7.21037137\n",
      "Iteration 16065, loss = 6.99452553\n",
      "Iteration 16066, loss = 6.90989614\n",
      "Iteration 16067, loss = 6.94801198\n",
      "Iteration 16068, loss = 6.52244202\n",
      "Iteration 16069, loss = 6.74034296\n",
      "Iteration 16070, loss = 6.81035292\n",
      "Iteration 16071, loss = 6.34436266\n",
      "Iteration 16072, loss = 6.50819246\n",
      "Iteration 16073, loss = 6.63414975\n",
      "Iteration 16074, loss = 6.57794818\n",
      "Iteration 16075, loss = 6.31305447\n",
      "Iteration 16076, loss = 7.14565361\n",
      "Iteration 16077, loss = 7.92267233\n",
      "Iteration 16078, loss = 7.19235338\n",
      "Iteration 16079, loss = 6.63511047\n",
      "Iteration 16080, loss = 6.88627254\n",
      "Iteration 16081, loss = 6.38588343\n",
      "Iteration 16082, loss = 6.70462778\n",
      "Iteration 16083, loss = 6.29820050\n",
      "Iteration 16084, loss = 6.25831789\n",
      "Iteration 16085, loss = 6.58973202\n",
      "Iteration 16086, loss = 6.41488928\n",
      "Iteration 16087, loss = 6.63729659\n",
      "Iteration 16088, loss = 7.50952217\n",
      "Iteration 16089, loss = 8.03032725\n",
      "Iteration 16090, loss = 7.25005278\n",
      "Iteration 16091, loss = 7.42611035\n",
      "Iteration 16092, loss = 7.27555218\n",
      "Iteration 16093, loss = 6.71231713\n",
      "Iteration 16094, loss = 7.07466066\n",
      "Iteration 16095, loss = 7.04694729\n",
      "Iteration 16096, loss = 7.24201437\n",
      "Iteration 16097, loss = 7.97269654\n",
      "Iteration 16098, loss = 7.74829006\n",
      "Iteration 16099, loss = 8.76033374\n",
      "Iteration 16100, loss = 7.69665241\n",
      "Iteration 16101, loss = 8.07514276\n",
      "Iteration 16102, loss = 7.18182436\n",
      "Iteration 16103, loss = 6.98860548\n",
      "Iteration 16104, loss = 7.24457887\n",
      "Iteration 16105, loss = 7.21845054\n",
      "Iteration 16106, loss = 7.14847113\n",
      "Iteration 16107, loss = 7.91013875\n",
      "Iteration 16108, loss = 7.25275431\n",
      "Iteration 16109, loss = 7.00885835\n",
      "Iteration 16110, loss = 7.20834784\n",
      "Iteration 16111, loss = 6.82673979\n",
      "Iteration 16112, loss = 7.11977636\n",
      "Iteration 16113, loss = 7.06515852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16114, loss = 6.86787274\n",
      "Iteration 16115, loss = 6.80170116\n",
      "Iteration 16116, loss = 7.41318081\n",
      "Iteration 16117, loss = 6.45160997\n",
      "Iteration 16118, loss = 7.00630639\n",
      "Iteration 16119, loss = 7.20940412\n",
      "Iteration 16120, loss = 7.02581913\n",
      "Iteration 16121, loss = 7.10372117\n",
      "Iteration 16122, loss = 6.82261592\n",
      "Iteration 16123, loss = 6.75272553\n",
      "Iteration 16124, loss = 7.06964002\n",
      "Iteration 16125, loss = 6.52238875\n",
      "Iteration 16126, loss = 6.46908195\n",
      "Iteration 16127, loss = 6.65269800\n",
      "Iteration 16128, loss = 6.32202478\n",
      "Iteration 16129, loss = 6.24256510\n",
      "Iteration 16130, loss = 6.44293937\n",
      "Iteration 16131, loss = 6.48518165\n",
      "Iteration 16132, loss = 6.84820871\n",
      "Iteration 16133, loss = 7.16104949\n",
      "Iteration 16134, loss = 7.11589692\n",
      "Iteration 16135, loss = 6.59382521\n",
      "Iteration 16136, loss = 6.53256916\n",
      "Iteration 16137, loss = 6.75728937\n",
      "Iteration 16138, loss = 6.53768869\n",
      "Iteration 16139, loss = 7.80628921\n",
      "Iteration 16140, loss = 7.31403582\n",
      "Iteration 16141, loss = 7.02767821\n",
      "Iteration 16142, loss = 7.05089892\n",
      "Iteration 16143, loss = 6.67808629\n",
      "Iteration 16144, loss = 6.80896336\n",
      "Iteration 16145, loss = 7.01884151\n",
      "Iteration 16146, loss = 7.04038334\n",
      "Iteration 16147, loss = 6.67306346\n",
      "Iteration 16148, loss = 6.70640261\n",
      "Iteration 16149, loss = 6.21782712\n",
      "Iteration 16150, loss = 6.36549261\n",
      "Iteration 16151, loss = 6.69884843\n",
      "Iteration 16152, loss = 6.19739974\n",
      "Iteration 16153, loss = 6.61535067\n",
      "Iteration 16154, loss = 6.47051522\n",
      "Iteration 16155, loss = 7.68453068\n",
      "Iteration 16156, loss = 6.84464768\n",
      "Iteration 16157, loss = 7.58732188\n",
      "Iteration 16158, loss = 7.34805608\n",
      "Iteration 16159, loss = 6.67098885\n",
      "Iteration 16160, loss = 6.47105954\n",
      "Iteration 16161, loss = 6.71776022\n",
      "Iteration 16162, loss = 6.75469119\n",
      "Iteration 16163, loss = 7.12035084\n",
      "Iteration 16164, loss = 6.59572127\n",
      "Iteration 16165, loss = 7.29669103\n",
      "Iteration 16166, loss = 6.71540431\n",
      "Iteration 16167, loss = 6.56522281\n",
      "Iteration 16168, loss = 6.56463597\n",
      "Iteration 16169, loss = 6.36837510\n",
      "Iteration 16170, loss = 7.00801140\n",
      "Iteration 16171, loss = 6.43247102\n",
      "Iteration 16172, loss = 6.72852526\n",
      "Iteration 16173, loss = 6.55774032\n",
      "Iteration 16174, loss = 6.88217386\n",
      "Iteration 16175, loss = 7.49170978\n",
      "Iteration 16176, loss = 6.79562582\n",
      "Iteration 16177, loss = 6.45374673\n",
      "Iteration 16178, loss = 6.32189468\n",
      "Iteration 16179, loss = 6.58786097\n",
      "Iteration 16180, loss = 6.91980439\n",
      "Iteration 16181, loss = 6.56609068\n",
      "Iteration 16182, loss = 7.00404450\n",
      "Iteration 16183, loss = 7.39069805\n",
      "Iteration 16184, loss = 6.47522710\n",
      "Iteration 16185, loss = 6.47180504\n",
      "Iteration 16186, loss = 7.35494259\n",
      "Iteration 16187, loss = 6.89027531\n",
      "Iteration 16188, loss = 6.73354957\n",
      "Iteration 16189, loss = 6.36399163\n",
      "Iteration 16190, loss = 6.69317451\n",
      "Iteration 16191, loss = 7.89870531\n",
      "Iteration 16192, loss = 7.38954487\n",
      "Iteration 16193, loss = 6.91271451\n",
      "Iteration 16194, loss = 7.53232195\n",
      "Iteration 16195, loss = 6.70807458\n",
      "Iteration 16196, loss = 8.19838708\n",
      "Iteration 16197, loss = 6.88152973\n",
      "Iteration 16198, loss = 7.80138444\n",
      "Iteration 16199, loss = 7.22540663\n",
      "Iteration 16200, loss = 6.78445220\n",
      "Iteration 16201, loss = 6.50695679\n",
      "Iteration 16202, loss = 6.62081298\n",
      "Iteration 16203, loss = 6.50510930\n",
      "Iteration 16204, loss = 6.72876507\n",
      "Iteration 16205, loss = 6.60646753\n",
      "Iteration 16206, loss = 6.57843193\n",
      "Iteration 16207, loss = 6.95239520\n",
      "Iteration 16208, loss = 6.33802085\n",
      "Iteration 16209, loss = 6.42503579\n",
      "Iteration 16210, loss = 6.55064488\n",
      "Iteration 16211, loss = 6.48966662\n",
      "Iteration 16212, loss = 7.36390663\n",
      "Iteration 16213, loss = 6.57772972\n",
      "Iteration 16214, loss = 6.25711645\n",
      "Iteration 16215, loss = 6.49362360\n",
      "Iteration 16216, loss = 8.01222458\n",
      "Iteration 16217, loss = 7.10632331\n",
      "Iteration 16218, loss = 7.57540918\n",
      "Iteration 16219, loss = 7.45754636\n",
      "Iteration 16220, loss = 6.76881738\n",
      "Iteration 16221, loss = 6.95957240\n",
      "Iteration 16222, loss = 8.36988031\n",
      "Iteration 16223, loss = 7.26860986\n",
      "Iteration 16224, loss = 7.62522790\n",
      "Iteration 16225, loss = 7.67360225\n",
      "Iteration 16226, loss = 7.90081825\n",
      "Iteration 16227, loss = 7.98896812\n",
      "Iteration 16228, loss = 7.24968393\n",
      "Iteration 16229, loss = 7.30146265\n",
      "Iteration 16230, loss = 6.89684492\n",
      "Iteration 16231, loss = 6.77105742\n",
      "Iteration 16232, loss = 7.37998145\n",
      "Iteration 16233, loss = 7.16198435\n",
      "Iteration 16234, loss = 6.47369283\n",
      "Iteration 16235, loss = 7.52404162\n",
      "Iteration 16236, loss = 8.08946586\n",
      "Iteration 16237, loss = 7.49009912\n",
      "Iteration 16238, loss = 7.24913218\n",
      "Iteration 16239, loss = 6.99468564\n",
      "Iteration 16240, loss = 7.77388137\n",
      "Iteration 16241, loss = 7.94954680\n",
      "Iteration 16242, loss = 7.12142128\n",
      "Iteration 16243, loss = 6.86212854\n",
      "Iteration 16244, loss = 7.04511326\n",
      "Iteration 16245, loss = 6.27510119\n",
      "Iteration 16246, loss = 6.42114978\n",
      "Iteration 16247, loss = 6.43804639\n",
      "Iteration 16248, loss = 6.79759535\n",
      "Iteration 16249, loss = 7.04897704\n",
      "Iteration 16250, loss = 6.53574500\n",
      "Iteration 16251, loss = 6.58524920\n",
      "Iteration 16252, loss = 6.96146331\n",
      "Iteration 16253, loss = 7.01474499\n",
      "Iteration 16254, loss = 7.31843908\n",
      "Iteration 16255, loss = 6.39268630\n",
      "Iteration 16256, loss = 6.52993633\n",
      "Iteration 16257, loss = 6.55139897\n",
      "Iteration 16258, loss = 6.72966442\n",
      "Iteration 16259, loss = 6.46157936\n",
      "Iteration 16260, loss = 6.77489503\n",
      "Iteration 16261, loss = 7.50518714\n",
      "Iteration 16262, loss = 6.86188474\n",
      "Iteration 16263, loss = 6.65600628\n",
      "Iteration 16264, loss = 6.71181638\n",
      "Iteration 16265, loss = 7.07563002\n",
      "Iteration 16266, loss = 6.63769682\n",
      "Iteration 16267, loss = 6.23528548\n",
      "Iteration 16268, loss = 6.97759641\n",
      "Iteration 16269, loss = 6.91670541\n",
      "Iteration 16270, loss = 6.44654127\n",
      "Iteration 16271, loss = 6.30305020\n",
      "Iteration 16272, loss = 6.47868328\n",
      "Iteration 16273, loss = 7.25547478\n",
      "Iteration 16274, loss = 6.21668005\n",
      "Iteration 16275, loss = 7.64453789\n",
      "Iteration 16276, loss = 6.98033151\n",
      "Iteration 16277, loss = 7.12844718\n",
      "Iteration 16278, loss = 7.40705532\n",
      "Iteration 16279, loss = 8.22315867\n",
      "Iteration 16280, loss = 6.89004529\n",
      "Iteration 16281, loss = 8.06424215\n",
      "Iteration 16282, loss = 7.77464026\n",
      "Iteration 16283, loss = 7.73744059\n",
      "Iteration 16284, loss = 8.21239860\n",
      "Iteration 16285, loss = 7.63185924\n",
      "Iteration 16286, loss = 8.28817492\n",
      "Iteration 16287, loss = 7.80156193\n",
      "Iteration 16288, loss = 6.67898090\n",
      "Iteration 16289, loss = 8.34748852\n",
      "Iteration 16290, loss = 9.45267689\n",
      "Iteration 16291, loss = 9.25137067\n",
      "Iteration 16292, loss = 8.75341213\n",
      "Iteration 16293, loss = 8.29951942\n",
      "Iteration 16294, loss = 7.82338462\n",
      "Iteration 16295, loss = 7.04516024\n",
      "Iteration 16296, loss = 6.96600141\n",
      "Iteration 16297, loss = 6.71789277\n",
      "Iteration 16298, loss = 6.99177276\n",
      "Iteration 16299, loss = 6.59673500\n",
      "Iteration 16300, loss = 6.51291904\n",
      "Iteration 16301, loss = 6.33151084\n",
      "Iteration 16302, loss = 6.71728879\n",
      "Iteration 16303, loss = 6.30407520\n",
      "Iteration 16304, loss = 6.63872497\n",
      "Iteration 16305, loss = 6.25417058\n",
      "Iteration 16306, loss = 6.38626517\n",
      "Iteration 16307, loss = 6.56202915\n",
      "Iteration 16308, loss = 6.79639037\n",
      "Iteration 16309, loss = 7.19471443\n",
      "Iteration 16310, loss = 7.13612535\n",
      "Iteration 16311, loss = 7.15466956\n",
      "Iteration 16312, loss = 7.12431492\n",
      "Iteration 16313, loss = 7.86074121\n",
      "Iteration 16314, loss = 8.03539920\n",
      "Iteration 16315, loss = 8.90361509\n",
      "Iteration 16316, loss = 7.60089813\n",
      "Iteration 16317, loss = 7.34964904\n",
      "Iteration 16318, loss = 6.27457975\n",
      "Iteration 16319, loss = 6.50330397\n",
      "Iteration 16320, loss = 6.96957101\n",
      "Iteration 16321, loss = 6.78772021\n",
      "Iteration 16322, loss = 7.04762439\n",
      "Iteration 16323, loss = 7.79729153\n",
      "Iteration 16324, loss = 6.51439338\n",
      "Iteration 16325, loss = 7.09008936\n",
      "Iteration 16326, loss = 6.67043374\n",
      "Iteration 16327, loss = 6.41454770\n",
      "Iteration 16328, loss = 6.64348849\n",
      "Iteration 16329, loss = 6.37629541\n",
      "Iteration 16330, loss = 6.81301280\n",
      "Iteration 16331, loss = 6.45156816\n",
      "Iteration 16332, loss = 6.28506868\n",
      "Iteration 16333, loss = 6.28269556\n",
      "Iteration 16334, loss = 6.35001997\n",
      "Iteration 16335, loss = 6.65435471\n",
      "Iteration 16336, loss = 6.50855692\n",
      "Iteration 16337, loss = 7.27197058\n",
      "Iteration 16338, loss = 7.13997442\n",
      "Iteration 16339, loss = 7.34914419\n",
      "Iteration 16340, loss = 7.14573212\n",
      "Iteration 16341, loss = 6.12170796\n",
      "Iteration 16342, loss = 6.52612724\n",
      "Iteration 16343, loss = 6.46244682\n",
      "Iteration 16344, loss = 6.67020680\n",
      "Iteration 16345, loss = 6.64060125\n",
      "Iteration 16346, loss = 6.97365836\n",
      "Iteration 16347, loss = 6.67651852\n",
      "Iteration 16348, loss = 6.46304664\n",
      "Iteration 16349, loss = 7.12649207\n",
      "Iteration 16350, loss = 6.99717431\n",
      "Iteration 16351, loss = 7.34217007\n",
      "Iteration 16352, loss = 7.00799483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16353, loss = 7.71513040\n",
      "Iteration 16354, loss = 6.63778994\n",
      "Iteration 16355, loss = 7.34111549\n",
      "Iteration 16356, loss = 6.82598779\n",
      "Iteration 16357, loss = 6.55755258\n",
      "Iteration 16358, loss = 6.60237700\n",
      "Iteration 16359, loss = 6.51760947\n",
      "Iteration 16360, loss = 6.67900657\n",
      "Iteration 16361, loss = 6.25450592\n",
      "Iteration 16362, loss = 6.25989257\n",
      "Iteration 16363, loss = 6.79536362\n",
      "Iteration 16364, loss = 6.65999680\n",
      "Iteration 16365, loss = 7.82735998\n",
      "Iteration 16366, loss = 6.87793385\n",
      "Iteration 16367, loss = 7.14520125\n",
      "Iteration 16368, loss = 7.10738625\n",
      "Iteration 16369, loss = 7.34746191\n",
      "Iteration 16370, loss = 7.81024584\n",
      "Iteration 16371, loss = 7.79658759\n",
      "Iteration 16372, loss = 6.84739166\n",
      "Iteration 16373, loss = 8.21567098\n",
      "Iteration 16374, loss = 7.65779873\n",
      "Iteration 16375, loss = 7.20749959\n",
      "Iteration 16376, loss = 6.57541723\n",
      "Iteration 16377, loss = 6.50257160\n",
      "Iteration 16378, loss = 6.66397641\n",
      "Iteration 16379, loss = 6.67022089\n",
      "Iteration 16380, loss = 7.04359634\n",
      "Iteration 16381, loss = 7.08760983\n",
      "Iteration 16382, loss = 6.60161780\n",
      "Iteration 16383, loss = 6.49264116\n",
      "Iteration 16384, loss = 6.25088915\n",
      "Iteration 16385, loss = 6.64449726\n",
      "Iteration 16386, loss = 6.35433455\n",
      "Iteration 16387, loss = 6.53326114\n",
      "Iteration 16388, loss = 6.58039878\n",
      "Iteration 16389, loss = 6.43768486\n",
      "Iteration 16390, loss = 6.65743236\n",
      "Iteration 16391, loss = 6.16889210\n",
      "Iteration 16392, loss = 6.42038363\n",
      "Iteration 16393, loss = 6.56240176\n",
      "Iteration 16394, loss = 6.61600417\n",
      "Iteration 16395, loss = 6.66690312\n",
      "Iteration 16396, loss = 6.77226087\n",
      "Iteration 16397, loss = 6.47477380\n",
      "Iteration 16398, loss = 7.06050361\n",
      "Iteration 16399, loss = 6.74223002\n",
      "Iteration 16400, loss = 6.49465460\n",
      "Iteration 16401, loss = 6.90902542\n",
      "Iteration 16402, loss = 7.47546030\n",
      "Iteration 16403, loss = 6.85079902\n",
      "Iteration 16404, loss = 7.57703224\n",
      "Iteration 16405, loss = 6.64462872\n",
      "Iteration 16406, loss = 7.23585415\n",
      "Iteration 16407, loss = 9.67148014\n",
      "Iteration 16408, loss = 8.21047872\n",
      "Iteration 16409, loss = 7.93274778\n",
      "Iteration 16410, loss = 7.05237881\n",
      "Iteration 16411, loss = 7.22811871\n",
      "Iteration 16412, loss = 7.59698351\n",
      "Iteration 16413, loss = 7.77610377\n",
      "Iteration 16414, loss = 7.71701064\n",
      "Iteration 16415, loss = 7.03502081\n",
      "Iteration 16416, loss = 6.46154108\n",
      "Iteration 16417, loss = 6.55630139\n",
      "Iteration 16418, loss = 7.00361365\n",
      "Iteration 16419, loss = 7.33316099\n",
      "Iteration 16420, loss = 6.39945896\n",
      "Iteration 16421, loss = 7.22934668\n",
      "Iteration 16422, loss = 7.78771944\n",
      "Iteration 16423, loss = 7.60523862\n",
      "Iteration 16424, loss = 7.34980814\n",
      "Iteration 16425, loss = 7.03279266\n",
      "Iteration 16426, loss = 6.96489677\n",
      "Iteration 16427, loss = 7.32927078\n",
      "Iteration 16428, loss = 7.13805573\n",
      "Iteration 16429, loss = 7.56777263\n",
      "Iteration 16430, loss = 7.52993076\n",
      "Iteration 16431, loss = 6.57220761\n",
      "Iteration 16432, loss = 6.88508290\n",
      "Iteration 16433, loss = 7.16174247\n",
      "Iteration 16434, loss = 6.79339390\n",
      "Iteration 16435, loss = 6.67767874\n",
      "Iteration 16436, loss = 6.94882491\n",
      "Iteration 16437, loss = 6.25585970\n",
      "Iteration 16438, loss = 7.16546943\n",
      "Iteration 16439, loss = 7.47429482\n",
      "Iteration 16440, loss = 7.34100837\n",
      "Iteration 16441, loss = 6.38915751\n",
      "Iteration 16442, loss = 6.46672589\n",
      "Iteration 16443, loss = 6.37273962\n",
      "Iteration 16444, loss = 6.73918929\n",
      "Iteration 16445, loss = 6.63541450\n",
      "Iteration 16446, loss = 6.77564690\n",
      "Iteration 16447, loss = 6.87712629\n",
      "Iteration 16448, loss = 7.37334136\n",
      "Iteration 16449, loss = 6.48403145\n",
      "Iteration 16450, loss = 6.99268220\n",
      "Iteration 16451, loss = 6.64242235\n",
      "Iteration 16452, loss = 6.24963920\n",
      "Iteration 16453, loss = 6.46570623\n",
      "Iteration 16454, loss = 6.52934642\n",
      "Iteration 16455, loss = 6.76833001\n",
      "Iteration 16456, loss = 6.77692654\n",
      "Iteration 16457, loss = 7.25814082\n",
      "Iteration 16458, loss = 6.80793613\n",
      "Iteration 16459, loss = 7.24332379\n",
      "Iteration 16460, loss = 6.98529456\n",
      "Iteration 16461, loss = 7.27215732\n",
      "Iteration 16462, loss = 7.21441752\n",
      "Iteration 16463, loss = 6.38395834\n",
      "Iteration 16464, loss = 6.30582972\n",
      "Iteration 16465, loss = 6.32761090\n",
      "Iteration 16466, loss = 6.40479168\n",
      "Iteration 16467, loss = 6.17914404\n",
      "Iteration 16468, loss = 6.29971976\n",
      "Iteration 16469, loss = 6.26327313\n",
      "Iteration 16470, loss = 6.94616766\n",
      "Iteration 16471, loss = 7.55562028\n",
      "Iteration 16472, loss = 6.45111970\n",
      "Iteration 16473, loss = 6.80574284\n",
      "Iteration 16474, loss = 6.81955304\n",
      "Iteration 16475, loss = 6.98348541\n",
      "Iteration 16476, loss = 7.51519254\n",
      "Iteration 16477, loss = 8.59122918\n",
      "Iteration 16478, loss = 7.65487223\n",
      "Iteration 16479, loss = 7.82531926\n",
      "Iteration 16480, loss = 7.62167383\n",
      "Iteration 16481, loss = 8.06600840\n",
      "Iteration 16482, loss = 6.50102408\n",
      "Iteration 16483, loss = 6.58055139\n",
      "Iteration 16484, loss = 6.97005588\n",
      "Iteration 16485, loss = 6.60848413\n",
      "Iteration 16486, loss = 6.74424013\n",
      "Iteration 16487, loss = 6.50229032\n",
      "Iteration 16488, loss = 6.77990627\n",
      "Iteration 16489, loss = 7.07753293\n",
      "Iteration 16490, loss = 7.81484748\n",
      "Iteration 16491, loss = 8.36028048\n",
      "Iteration 16492, loss = 7.40854710\n",
      "Iteration 16493, loss = 6.88426560\n",
      "Iteration 16494, loss = 6.88619009\n",
      "Iteration 16495, loss = 6.91585527\n",
      "Iteration 16496, loss = 6.60825589\n",
      "Iteration 16497, loss = 7.03477385\n",
      "Iteration 16498, loss = 6.91771193\n",
      "Iteration 16499, loss = 6.27316114\n",
      "Iteration 16500, loss = 6.66578798\n",
      "Iteration 16501, loss = 7.27247458\n",
      "Iteration 16502, loss = 6.76991989\n",
      "Iteration 16503, loss = 6.45612582\n",
      "Iteration 16504, loss = 6.84758658\n",
      "Iteration 16505, loss = 6.72447954\n",
      "Iteration 16506, loss = 6.60826583\n",
      "Iteration 16507, loss = 6.84301424\n",
      "Iteration 16508, loss = 7.94015755\n",
      "Iteration 16509, loss = 7.70504756\n",
      "Iteration 16510, loss = 8.14439677\n",
      "Iteration 16511, loss = 7.58599363\n",
      "Iteration 16512, loss = 8.01075706\n",
      "Iteration 16513, loss = 6.91450140\n",
      "Iteration 16514, loss = 6.81459388\n",
      "Iteration 16515, loss = 6.84222806\n",
      "Iteration 16516, loss = 6.84147641\n",
      "Iteration 16517, loss = 6.64846336\n",
      "Iteration 16518, loss = 6.39157823\n",
      "Iteration 16519, loss = 6.58459270\n",
      "Iteration 16520, loss = 7.06279998\n",
      "Iteration 16521, loss = 6.74357319\n",
      "Iteration 16522, loss = 6.58383765\n",
      "Iteration 16523, loss = 7.43733537\n",
      "Iteration 16524, loss = 6.98472881\n",
      "Iteration 16525, loss = 8.21975718\n",
      "Iteration 16526, loss = 9.61232041\n",
      "Iteration 16527, loss = 8.28921755\n",
      "Iteration 16528, loss = 8.24074933\n",
      "Iteration 16529, loss = 6.81896562\n",
      "Iteration 16530, loss = 7.69084826\n",
      "Iteration 16531, loss = 7.13461705\n",
      "Iteration 16532, loss = 7.62691780\n",
      "Iteration 16533, loss = 7.50219424\n",
      "Iteration 16534, loss = 6.79239044\n",
      "Iteration 16535, loss = 6.63149444\n",
      "Iteration 16536, loss = 6.27426675\n",
      "Iteration 16537, loss = 6.50977741\n",
      "Iteration 16538, loss = 6.35377986\n",
      "Iteration 16539, loss = 6.75759139\n",
      "Iteration 16540, loss = 6.45752351\n",
      "Iteration 16541, loss = 6.50247802\n",
      "Iteration 16542, loss = 6.28890720\n",
      "Iteration 16543, loss = 6.71323036\n",
      "Iteration 16544, loss = 7.12861784\n",
      "Iteration 16545, loss = 7.31893199\n",
      "Iteration 16546, loss = 7.44737558\n",
      "Iteration 16547, loss = 6.49827845\n",
      "Iteration 16548, loss = 6.95559546\n",
      "Iteration 16549, loss = 6.31524167\n",
      "Iteration 16550, loss = 6.78017945\n",
      "Iteration 16551, loss = 7.11922910\n",
      "Iteration 16552, loss = 7.52723255\n",
      "Iteration 16553, loss = 7.31441921\n",
      "Iteration 16554, loss = 7.05386152\n",
      "Iteration 16555, loss = 6.34324330\n",
      "Iteration 16556, loss = 6.28909811\n",
      "Iteration 16557, loss = 6.42920848\n",
      "Iteration 16558, loss = 6.40485089\n",
      "Iteration 16559, loss = 6.45528882\n",
      "Iteration 16560, loss = 6.17281013\n",
      "Iteration 16561, loss = 6.27686751\n",
      "Iteration 16562, loss = 6.64039036\n",
      "Iteration 16563, loss = 6.62607535\n",
      "Iteration 16564, loss = 6.66708644\n",
      "Iteration 16565, loss = 7.14827387\n",
      "Iteration 16566, loss = 8.44709185\n",
      "Iteration 16567, loss = 7.20074299\n",
      "Iteration 16568, loss = 8.11612753\n",
      "Iteration 16569, loss = 8.29475659\n",
      "Iteration 16570, loss = 8.04060195\n",
      "Iteration 16571, loss = 7.94548612\n",
      "Iteration 16572, loss = 7.56071345\n",
      "Iteration 16573, loss = 6.78068113\n",
      "Iteration 16574, loss = 6.68538602\n",
      "Iteration 16575, loss = 7.03078545\n",
      "Iteration 16576, loss = 6.79618786\n",
      "Iteration 16577, loss = 6.39721791\n",
      "Iteration 16578, loss = 6.55273883\n",
      "Iteration 16579, loss = 6.37261059\n",
      "Iteration 16580, loss = 6.59062373\n",
      "Iteration 16581, loss = 6.57506562\n",
      "Iteration 16582, loss = 6.45638429\n",
      "Iteration 16583, loss = 6.80199716\n",
      "Iteration 16584, loss = 6.59526810\n",
      "Iteration 16585, loss = 6.77151141\n",
      "Iteration 16586, loss = 6.97177129\n",
      "Iteration 16587, loss = 7.78025193\n",
      "Iteration 16588, loss = 7.67881274\n",
      "Iteration 16589, loss = 7.64960808\n",
      "Iteration 16590, loss = 6.24056833\n",
      "Iteration 16591, loss = 7.39149483\n",
      "Iteration 16592, loss = 6.50429296\n",
      "Iteration 16593, loss = 6.60483324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16594, loss = 6.56984741\n",
      "Iteration 16595, loss = 6.47716853\n",
      "Iteration 16596, loss = 7.23076928\n",
      "Iteration 16597, loss = 6.77488664\n",
      "Iteration 16598, loss = 6.45903450\n",
      "Iteration 16599, loss = 6.87291836\n",
      "Iteration 16600, loss = 6.73061267\n",
      "Iteration 16601, loss = 6.50562584\n",
      "Iteration 16602, loss = 6.38860960\n",
      "Iteration 16603, loss = 6.30417470\n",
      "Iteration 16604, loss = 6.50585505\n",
      "Iteration 16605, loss = 7.08330872\n",
      "Iteration 16606, loss = 6.83761795\n",
      "Iteration 16607, loss = 6.76637013\n",
      "Iteration 16608, loss = 6.46171527\n",
      "Iteration 16609, loss = 6.29431888\n",
      "Iteration 16610, loss = 6.24246113\n",
      "Iteration 16611, loss = 6.33101309\n",
      "Iteration 16612, loss = 6.75726801\n",
      "Iteration 16613, loss = 7.40458958\n",
      "Iteration 16614, loss = 7.25131638\n",
      "Iteration 16615, loss = 8.16141728\n",
      "Iteration 16616, loss = 7.69659995\n",
      "Iteration 16617, loss = 7.76333898\n",
      "Iteration 16618, loss = 8.17402097\n",
      "Iteration 16619, loss = 8.45524076\n",
      "Iteration 16620, loss = 7.53820713\n",
      "Iteration 16621, loss = 7.09086322\n",
      "Iteration 16622, loss = 6.53122463\n",
      "Iteration 16623, loss = 6.23705354\n",
      "Iteration 16624, loss = 6.20971415\n",
      "Iteration 16625, loss = 6.57320149\n",
      "Iteration 16626, loss = 6.24284660\n",
      "Iteration 16627, loss = 6.42700451\n",
      "Iteration 16628, loss = 6.43206313\n",
      "Iteration 16629, loss = 6.67469675\n",
      "Iteration 16630, loss = 6.57667980\n",
      "Iteration 16631, loss = 6.81922179\n",
      "Iteration 16632, loss = 7.18646702\n",
      "Iteration 16633, loss = 6.72181983\n",
      "Iteration 16634, loss = 6.52818930\n",
      "Iteration 16635, loss = 7.05568700\n",
      "Iteration 16636, loss = 7.36267628\n",
      "Iteration 16637, loss = 7.30175783\n",
      "Iteration 16638, loss = 6.74630454\n",
      "Iteration 16639, loss = 6.40048760\n",
      "Iteration 16640, loss = 8.46210494\n",
      "Iteration 16641, loss = 6.86915506\n",
      "Iteration 16642, loss = 6.61403859\n",
      "Iteration 16643, loss = 6.91968226\n",
      "Iteration 16644, loss = 6.91320529\n",
      "Iteration 16645, loss = 7.12264193\n",
      "Iteration 16646, loss = 6.86870954\n",
      "Iteration 16647, loss = 7.35679566\n",
      "Iteration 16648, loss = 6.47060636\n",
      "Iteration 16649, loss = 8.03619139\n",
      "Iteration 16650, loss = 7.18790873\n",
      "Iteration 16651, loss = 6.78000374\n",
      "Iteration 16652, loss = 6.90063916\n",
      "Iteration 16653, loss = 6.78255057\n",
      "Iteration 16654, loss = 6.52031282\n",
      "Iteration 16655, loss = 6.75381902\n",
      "Iteration 16656, loss = 7.55300847\n",
      "Iteration 16657, loss = 9.29053846\n",
      "Iteration 16658, loss = 8.58301697\n",
      "Iteration 16659, loss = 9.97239082\n",
      "Iteration 16660, loss = 6.60918699\n",
      "Iteration 16661, loss = 6.86757879\n",
      "Iteration 16662, loss = 6.66277197\n",
      "Iteration 16663, loss = 6.25094035\n",
      "Iteration 16664, loss = 6.11943806\n",
      "Iteration 16665, loss = 7.46343770\n",
      "Iteration 16666, loss = 7.07708008\n",
      "Iteration 16667, loss = 7.00278137\n",
      "Iteration 16668, loss = 7.68237694\n",
      "Iteration 16669, loss = 6.80918559\n",
      "Iteration 16670, loss = 6.42888004\n",
      "Iteration 16671, loss = 6.34035090\n",
      "Iteration 16672, loss = 6.18275635\n",
      "Iteration 16673, loss = 6.24871380\n",
      "Iteration 16674, loss = 6.22259928\n",
      "Iteration 16675, loss = 7.02122746\n",
      "Iteration 16676, loss = 7.15768578\n",
      "Iteration 16677, loss = 7.22005167\n",
      "Iteration 16678, loss = 6.68607870\n",
      "Iteration 16679, loss = 8.33277549\n",
      "Iteration 16680, loss = 7.10999635\n",
      "Iteration 16681, loss = 6.95632819\n",
      "Iteration 16682, loss = 6.42697498\n",
      "Iteration 16683, loss = 6.30584932\n",
      "Iteration 16684, loss = 6.43504410\n",
      "Iteration 16685, loss = 6.70686926\n",
      "Iteration 16686, loss = 7.25213826\n",
      "Iteration 16687, loss = 6.96426853\n",
      "Iteration 16688, loss = 6.64507138\n",
      "Iteration 16689, loss = 6.57498933\n",
      "Iteration 16690, loss = 6.70799237\n",
      "Iteration 16691, loss = 7.22075987\n",
      "Iteration 16692, loss = 6.20271930\n",
      "Iteration 16693, loss = 6.59735109\n",
      "Iteration 16694, loss = 6.54564587\n",
      "Iteration 16695, loss = 6.61252330\n",
      "Iteration 16696, loss = 6.54490676\n",
      "Iteration 16697, loss = 6.55375447\n",
      "Iteration 16698, loss = 6.46797027\n",
      "Iteration 16699, loss = 6.58808557\n",
      "Iteration 16700, loss = 6.59193186\n",
      "Iteration 16701, loss = 7.27918623\n",
      "Iteration 16702, loss = 6.36695132\n",
      "Iteration 16703, loss = 6.79787509\n",
      "Iteration 16704, loss = 6.55505662\n",
      "Iteration 16705, loss = 6.91060379\n",
      "Iteration 16706, loss = 6.90117729\n",
      "Iteration 16707, loss = 6.33334935\n",
      "Iteration 16708, loss = 6.43671129\n",
      "Iteration 16709, loss = 6.71052458\n",
      "Iteration 16710, loss = 6.84722086\n",
      "Iteration 16711, loss = 6.71397087\n",
      "Iteration 16712, loss = 7.48668776\n",
      "Iteration 16713, loss = 6.29292793\n",
      "Iteration 16714, loss = 6.36838647\n",
      "Iteration 16715, loss = 6.14651217\n",
      "Iteration 16716, loss = 6.50731243\n",
      "Iteration 16717, loss = 6.94610880\n",
      "Iteration 16718, loss = 7.69202543\n",
      "Iteration 16719, loss = 6.93429603\n",
      "Iteration 16720, loss = 6.93976596\n",
      "Iteration 16721, loss = 6.64924751\n",
      "Iteration 16722, loss = 6.65928363\n",
      "Iteration 16723, loss = 6.77559078\n",
      "Iteration 16724, loss = 7.14906039\n",
      "Iteration 16725, loss = 7.44503975\n",
      "Iteration 16726, loss = 7.13402473\n",
      "Iteration 16727, loss = 6.48192656\n",
      "Iteration 16728, loss = 6.45568377\n",
      "Iteration 16729, loss = 6.36501567\n",
      "Iteration 16730, loss = 6.41762493\n",
      "Iteration 16731, loss = 6.63978283\n",
      "Iteration 16732, loss = 6.81607461\n",
      "Iteration 16733, loss = 7.06166749\n",
      "Iteration 16734, loss = 6.73578931\n",
      "Iteration 16735, loss = 6.75952983\n",
      "Iteration 16736, loss = 6.56108656\n",
      "Iteration 16737, loss = 6.64037064\n",
      "Iteration 16738, loss = 7.04369795\n",
      "Iteration 16739, loss = 6.71012349\n",
      "Iteration 16740, loss = 6.79956620\n",
      "Iteration 16741, loss = 6.47411788\n",
      "Iteration 16742, loss = 6.53724377\n",
      "Iteration 16743, loss = 6.84455794\n",
      "Iteration 16744, loss = 7.00736512\n",
      "Iteration 16745, loss = 6.53394072\n",
      "Iteration 16746, loss = 6.51527001\n",
      "Iteration 16747, loss = 6.49382381\n",
      "Iteration 16748, loss = 6.75509525\n",
      "Iteration 16749, loss = 7.22069426\n",
      "Iteration 16750, loss = 6.69133655\n",
      "Iteration 16751, loss = 7.58454856\n",
      "Iteration 16752, loss = 7.22112937\n",
      "Iteration 16753, loss = 7.05751845\n",
      "Iteration 16754, loss = 6.98349944\n",
      "Iteration 16755, loss = 7.19889174\n",
      "Iteration 16756, loss = 6.23358161\n",
      "Iteration 16757, loss = 7.76985423\n",
      "Iteration 16758, loss = 6.65043785\n",
      "Iteration 16759, loss = 6.55459557\n",
      "Iteration 16760, loss = 6.97657601\n",
      "Iteration 16761, loss = 6.96818013\n",
      "Iteration 16762, loss = 6.33571826\n",
      "Iteration 16763, loss = 6.47086952\n",
      "Iteration 16764, loss = 6.75736463\n",
      "Iteration 16765, loss = 7.28610500\n",
      "Iteration 16766, loss = 6.95287927\n",
      "Iteration 16767, loss = 7.63600956\n",
      "Iteration 16768, loss = 6.93425904\n",
      "Iteration 16769, loss = 6.87609702\n",
      "Iteration 16770, loss = 6.26070991\n",
      "Iteration 16771, loss = 6.96709405\n",
      "Iteration 16772, loss = 6.97550304\n",
      "Iteration 16773, loss = 6.73177956\n",
      "Iteration 16774, loss = 6.56891833\n",
      "Iteration 16775, loss = 6.37440057\n",
      "Iteration 16776, loss = 7.09648066\n",
      "Iteration 16777, loss = 7.52977499\n",
      "Iteration 16778, loss = 7.22360942\n",
      "Iteration 16779, loss = 7.27938259\n",
      "Iteration 16780, loss = 6.96758843\n",
      "Iteration 16781, loss = 6.44794734\n",
      "Iteration 16782, loss = 7.33499311\n",
      "Iteration 16783, loss = 7.14136628\n",
      "Iteration 16784, loss = 6.55376875\n",
      "Iteration 16785, loss = 6.22583557\n",
      "Iteration 16786, loss = 6.98644381\n",
      "Iteration 16787, loss = 6.86564136\n",
      "Iteration 16788, loss = 6.52422432\n",
      "Iteration 16789, loss = 7.04983887\n",
      "Iteration 16790, loss = 6.63592766\n",
      "Iteration 16791, loss = 6.71276300\n",
      "Iteration 16792, loss = 6.41953563\n",
      "Iteration 16793, loss = 6.64764294\n",
      "Iteration 16794, loss = 7.98088258\n",
      "Iteration 16795, loss = 7.52712978\n",
      "Iteration 16796, loss = 6.87871949\n",
      "Iteration 16797, loss = 6.40672771\n",
      "Iteration 16798, loss = 6.68599533\n",
      "Iteration 16799, loss = 7.24942066\n",
      "Iteration 16800, loss = 7.03632334\n",
      "Iteration 16801, loss = 7.85260034\n",
      "Iteration 16802, loss = 6.89008541\n",
      "Iteration 16803, loss = 6.46550338\n",
      "Iteration 16804, loss = 7.09390886\n",
      "Iteration 16805, loss = 6.97715710\n",
      "Iteration 16806, loss = 7.90417012\n",
      "Iteration 16807, loss = 6.95916954\n",
      "Iteration 16808, loss = 7.99771414\n",
      "Iteration 16809, loss = 7.26074916\n",
      "Iteration 16810, loss = 6.65633913\n",
      "Iteration 16811, loss = 7.24918193\n",
      "Iteration 16812, loss = 6.76248348\n",
      "Iteration 16813, loss = 6.52778076\n",
      "Iteration 16814, loss = 7.03506770\n",
      "Iteration 16815, loss = 6.40947796\n",
      "Iteration 16816, loss = 6.78360899\n",
      "Iteration 16817, loss = 6.61721276\n",
      "Iteration 16818, loss = 6.39041268\n",
      "Iteration 16819, loss = 6.58831672\n",
      "Iteration 16820, loss = 6.24862364\n",
      "Iteration 16821, loss = 6.54526450\n",
      "Iteration 16822, loss = 6.46082022\n",
      "Iteration 16823, loss = 6.26162140\n",
      "Iteration 16824, loss = 6.07050111\n",
      "Iteration 16825, loss = 6.22710857\n",
      "Iteration 16826, loss = 6.27501934\n",
      "Iteration 16827, loss = 6.24410405\n",
      "Iteration 16828, loss = 6.22394775\n",
      "Iteration 16829, loss = 6.58460143\n",
      "Iteration 16830, loss = 7.72972407\n",
      "Iteration 16831, loss = 7.13422677\n",
      "Iteration 16832, loss = 6.57153682\n",
      "Iteration 16833, loss = 7.33596950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16834, loss = 6.32835245\n",
      "Iteration 16835, loss = 6.52125402\n",
      "Iteration 16836, loss = 6.48584477\n",
      "Iteration 16837, loss = 6.10765403\n",
      "Iteration 16838, loss = 6.64961450\n",
      "Iteration 16839, loss = 6.19711618\n",
      "Iteration 16840, loss = 6.21407005\n",
      "Iteration 16841, loss = 6.16193066\n",
      "Iteration 16842, loss = 6.64579466\n",
      "Iteration 16843, loss = 6.65946217\n",
      "Iteration 16844, loss = 6.78280599\n",
      "Iteration 16845, loss = 6.79322880\n",
      "Iteration 16846, loss = 6.66387377\n",
      "Iteration 16847, loss = 6.65392463\n",
      "Iteration 16848, loss = 7.13370228\n",
      "Iteration 16849, loss = 7.29394913\n",
      "Iteration 16850, loss = 8.08457084\n",
      "Iteration 16851, loss = 6.93750930\n",
      "Iteration 16852, loss = 6.49174694\n",
      "Iteration 16853, loss = 6.70311541\n",
      "Iteration 16854, loss = 6.89219768\n",
      "Iteration 16855, loss = 6.58331642\n",
      "Iteration 16856, loss = 6.73717283\n",
      "Iteration 16857, loss = 6.69636248\n",
      "Iteration 16858, loss = 6.90743496\n",
      "Iteration 16859, loss = 6.14036648\n",
      "Iteration 16860, loss = 7.24165992\n",
      "Iteration 16861, loss = 6.76185570\n",
      "Iteration 16862, loss = 6.86150632\n",
      "Iteration 16863, loss = 6.78778805\n",
      "Iteration 16864, loss = 6.73276201\n",
      "Iteration 16865, loss = 7.35216795\n",
      "Iteration 16866, loss = 6.75593259\n",
      "Iteration 16867, loss = 8.06928053\n",
      "Iteration 16868, loss = 8.14246765\n",
      "Iteration 16869, loss = 8.36222588\n",
      "Iteration 16870, loss = 8.43628690\n",
      "Iteration 16871, loss = 7.71520865\n",
      "Iteration 16872, loss = 7.01927122\n",
      "Iteration 16873, loss = 7.07927095\n",
      "Iteration 16874, loss = 7.41468614\n",
      "Iteration 16875, loss = 7.17120124\n",
      "Iteration 16876, loss = 6.82065233\n",
      "Iteration 16877, loss = 6.30515211\n",
      "Iteration 16878, loss = 6.74634497\n",
      "Iteration 16879, loss = 6.65188188\n",
      "Iteration 16880, loss = 6.21265823\n",
      "Iteration 16881, loss = 6.37794571\n",
      "Iteration 16882, loss = 6.60879693\n",
      "Iteration 16883, loss = 6.36144071\n",
      "Iteration 16884, loss = 6.20754674\n",
      "Iteration 16885, loss = 6.27346482\n",
      "Iteration 16886, loss = 6.28621665\n",
      "Iteration 16887, loss = 6.67762850\n",
      "Iteration 16888, loss = 6.34364870\n",
      "Iteration 16889, loss = 6.70505714\n",
      "Iteration 16890, loss = 6.12988802\n",
      "Iteration 16891, loss = 6.30202295\n",
      "Iteration 16892, loss = 6.26683389\n",
      "Iteration 16893, loss = 6.37499994\n",
      "Iteration 16894, loss = 6.37676256\n",
      "Iteration 16895, loss = 6.91204213\n",
      "Iteration 16896, loss = 6.43373474\n",
      "Iteration 16897, loss = 6.52238661\n",
      "Iteration 16898, loss = 6.41562884\n",
      "Iteration 16899, loss = 6.22851762\n",
      "Iteration 16900, loss = 6.70073387\n",
      "Iteration 16901, loss = 6.62855802\n",
      "Iteration 16902, loss = 6.08218987\n",
      "Iteration 16903, loss = 6.12134925\n",
      "Iteration 16904, loss = 6.77259135\n",
      "Iteration 16905, loss = 6.43710939\n",
      "Iteration 16906, loss = 7.09768061\n",
      "Iteration 16907, loss = 6.76020831\n",
      "Iteration 16908, loss = 7.24672971\n",
      "Iteration 16909, loss = 7.92022280\n",
      "Iteration 16910, loss = 7.50312606\n",
      "Iteration 16911, loss = 7.14480072\n",
      "Iteration 16912, loss = 6.12262325\n",
      "Iteration 16913, loss = 6.47331936\n",
      "Iteration 16914, loss = 6.08292252\n",
      "Iteration 16915, loss = 6.34702402\n",
      "Iteration 16916, loss = 6.10455932\n",
      "Iteration 16917, loss = 6.87475612\n",
      "Iteration 16918, loss = 5.87588528\n",
      "Iteration 16919, loss = 6.56132495\n",
      "Iteration 16920, loss = 6.21015362\n",
      "Iteration 16921, loss = 6.84796254\n",
      "Iteration 16922, loss = 6.14698722\n",
      "Iteration 16923, loss = 6.21202864\n",
      "Iteration 16924, loss = 6.52029839\n",
      "Iteration 16925, loss = 6.60353610\n",
      "Iteration 16926, loss = 7.51216858\n",
      "Iteration 16927, loss = 7.70973809\n",
      "Iteration 16928, loss = 8.62342554\n",
      "Iteration 16929, loss = 8.38361804\n",
      "Iteration 16930, loss = 7.68293830\n",
      "Iteration 16931, loss = 7.30433817\n",
      "Iteration 16932, loss = 6.39863086\n",
      "Iteration 16933, loss = 6.42964085\n",
      "Iteration 16934, loss = 6.29003601\n",
      "Iteration 16935, loss = 6.01909540\n",
      "Iteration 16936, loss = 6.20989836\n",
      "Iteration 16937, loss = 7.05161716\n",
      "Iteration 16938, loss = 6.17422796\n",
      "Iteration 16939, loss = 6.32738878\n",
      "Iteration 16940, loss = 6.25905402\n",
      "Iteration 16941, loss = 6.57046264\n",
      "Iteration 16942, loss = 6.01066780\n",
      "Iteration 16943, loss = 6.06099135\n",
      "Iteration 16944, loss = 6.23505819\n",
      "Iteration 16945, loss = 6.51283628\n",
      "Iteration 16946, loss = 6.43769899\n",
      "Iteration 16947, loss = 6.28175553\n",
      "Iteration 16948, loss = 7.09798378\n",
      "Iteration 16949, loss = 6.68535387\n",
      "Iteration 16950, loss = 6.42093382\n",
      "Iteration 16951, loss = 6.47822757\n",
      "Iteration 16952, loss = 6.28785885\n",
      "Iteration 16953, loss = 6.90345559\n",
      "Iteration 16954, loss = 6.60986583\n",
      "Iteration 16955, loss = 6.55575511\n",
      "Iteration 16956, loss = 7.06220361\n",
      "Iteration 16957, loss = 6.25276516\n",
      "Iteration 16958, loss = 6.01259083\n",
      "Iteration 16959, loss = 6.10492471\n",
      "Iteration 16960, loss = 6.38057738\n",
      "Iteration 16961, loss = 6.31836293\n",
      "Iteration 16962, loss = 6.37872134\n",
      "Iteration 16963, loss = 6.13336000\n",
      "Iteration 16964, loss = 6.37177052\n",
      "Iteration 16965, loss = 6.66887677\n",
      "Iteration 16966, loss = 8.03499451\n",
      "Iteration 16967, loss = 8.32792231\n",
      "Iteration 16968, loss = 6.78908112\n",
      "Iteration 16969, loss = 6.71972495\n",
      "Iteration 16970, loss = 6.74268202\n",
      "Iteration 16971, loss = 7.18037086\n",
      "Iteration 16972, loss = 7.21486277\n",
      "Iteration 16973, loss = 6.94595794\n",
      "Iteration 16974, loss = 7.11624597\n",
      "Iteration 16975, loss = 7.79459116\n",
      "Iteration 16976, loss = 7.36003250\n",
      "Iteration 16977, loss = 6.48676482\n",
      "Iteration 16978, loss = 7.35194321\n",
      "Iteration 16979, loss = 7.28426296\n",
      "Iteration 16980, loss = 7.85291392\n",
      "Iteration 16981, loss = 7.07838866\n",
      "Iteration 16982, loss = 6.24270082\n",
      "Iteration 16983, loss = 6.62493792\n",
      "Iteration 16984, loss = 6.35116769\n",
      "Iteration 16985, loss = 6.43665704\n",
      "Iteration 16986, loss = 6.01803992\n",
      "Iteration 16987, loss = 6.16637754\n",
      "Iteration 16988, loss = 6.19957554\n",
      "Iteration 16989, loss = 7.35446731\n",
      "Iteration 16990, loss = 6.81272387\n",
      "Iteration 16991, loss = 6.69552474\n",
      "Iteration 16992, loss = 6.76659277\n",
      "Iteration 16993, loss = 6.46892451\n",
      "Iteration 16994, loss = 6.05570416\n",
      "Iteration 16995, loss = 6.34899612\n",
      "Iteration 16996, loss = 6.46333760\n",
      "Iteration 16997, loss = 6.25560418\n",
      "Iteration 16998, loss = 6.76443872\n",
      "Iteration 16999, loss = 6.52818715\n",
      "Iteration 17000, loss = 6.17560392\n",
      "Iteration 17001, loss = 6.60306145\n",
      "Iteration 17002, loss = 6.51740462\n",
      "Iteration 17003, loss = 6.56031367\n",
      "Iteration 17004, loss = 6.45067608\n",
      "Iteration 17005, loss = 7.17913075\n",
      "Iteration 17006, loss = 7.03289130\n",
      "Iteration 17007, loss = 7.13539674\n",
      "Iteration 17008, loss = 7.60798546\n",
      "Iteration 17009, loss = 10.47835270\n",
      "Iteration 17010, loss = 8.23501528\n",
      "Iteration 17011, loss = 7.23208929\n",
      "Iteration 17012, loss = 6.84521596\n",
      "Iteration 17013, loss = 6.50083459\n",
      "Iteration 17014, loss = 6.34188987\n",
      "Iteration 17015, loss = 6.30126183\n",
      "Iteration 17016, loss = 6.54845538\n",
      "Iteration 17017, loss = 6.92923815\n",
      "Iteration 17018, loss = 7.59535252\n",
      "Iteration 17019, loss = 6.23221576\n",
      "Iteration 17020, loss = 6.29511143\n",
      "Iteration 17021, loss = 6.28232945\n",
      "Iteration 17022, loss = 6.45798623\n",
      "Iteration 17023, loss = 6.38147906\n",
      "Iteration 17024, loss = 6.26587853\n",
      "Iteration 17025, loss = 6.64627392\n",
      "Iteration 17026, loss = 6.27331570\n",
      "Iteration 17027, loss = 6.59863121\n",
      "Iteration 17028, loss = 6.55738046\n",
      "Iteration 17029, loss = 6.74472131\n",
      "Iteration 17030, loss = 7.33636769\n",
      "Iteration 17031, loss = 6.41200700\n",
      "Iteration 17032, loss = 6.94313621\n",
      "Iteration 17033, loss = 7.62820096\n",
      "Iteration 17034, loss = 7.17040924\n",
      "Iteration 17035, loss = 6.92274497\n",
      "Iteration 17036, loss = 7.34991663\n",
      "Iteration 17037, loss = 6.96360011\n",
      "Iteration 17038, loss = 6.67965391\n",
      "Iteration 17039, loss = 6.72226029\n",
      "Iteration 17040, loss = 6.32401840\n",
      "Iteration 17041, loss = 6.83114822\n",
      "Iteration 17042, loss = 7.38526696\n",
      "Iteration 17043, loss = 6.48998564\n",
      "Iteration 17044, loss = 6.83066815\n",
      "Iteration 17045, loss = 6.92951937\n",
      "Iteration 17046, loss = 6.31030358\n",
      "Iteration 17047, loss = 6.26842530\n",
      "Iteration 17048, loss = 6.27482405\n",
      "Iteration 17049, loss = 5.97128513\n",
      "Iteration 17050, loss = 6.10034031\n",
      "Iteration 17051, loss = 6.36201584\n",
      "Iteration 17052, loss = 6.94897015\n",
      "Iteration 17053, loss = 6.23202310\n",
      "Iteration 17054, loss = 6.17202266\n",
      "Iteration 17055, loss = 6.46649415\n",
      "Iteration 17056, loss = 7.30656024\n",
      "Iteration 17057, loss = 6.97651974\n",
      "Iteration 17058, loss = 6.46681296\n",
      "Iteration 17059, loss = 6.60411142\n",
      "Iteration 17060, loss = 6.83415872\n",
      "Iteration 17061, loss = 7.25743217\n",
      "Iteration 17062, loss = 7.18280597\n",
      "Iteration 17063, loss = 7.37172450\n",
      "Iteration 17064, loss = 7.33617273\n",
      "Iteration 17065, loss = 7.34846332\n",
      "Iteration 17066, loss = 7.37145295\n",
      "Iteration 17067, loss = 6.10427336\n",
      "Iteration 17068, loss = 6.44740050\n",
      "Iteration 17069, loss = 6.23135961\n",
      "Iteration 17070, loss = 6.61554528\n",
      "Iteration 17071, loss = 7.19812995\n",
      "Iteration 17072, loss = 6.28551574\n",
      "Iteration 17073, loss = 6.39708146\n",
      "Iteration 17074, loss = 7.33562952\n",
      "Iteration 17075, loss = 8.71647054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17076, loss = 9.00998559\n",
      "Iteration 17077, loss = 8.28357362\n",
      "Iteration 17078, loss = 7.72275182\n",
      "Iteration 17079, loss = 7.24717201\n",
      "Iteration 17080, loss = 8.02799909\n",
      "Iteration 17081, loss = 7.22034670\n",
      "Iteration 17082, loss = 6.64826665\n",
      "Iteration 17083, loss = 6.76087028\n",
      "Iteration 17084, loss = 7.37459302\n",
      "Iteration 17085, loss = 7.26883642\n",
      "Iteration 17086, loss = 6.51135960\n",
      "Iteration 17087, loss = 6.33007763\n",
      "Iteration 17088, loss = 6.93664238\n",
      "Iteration 17089, loss = 6.64172774\n",
      "Iteration 17090, loss = 6.95728211\n",
      "Iteration 17091, loss = 8.49301685\n",
      "Iteration 17092, loss = 8.10082616\n",
      "Iteration 17093, loss = 8.11883639\n",
      "Iteration 17094, loss = 6.68469166\n",
      "Iteration 17095, loss = 6.09365265\n",
      "Iteration 17096, loss = 6.24189497\n",
      "Iteration 17097, loss = 5.91604087\n",
      "Iteration 17098, loss = 6.55300541\n",
      "Iteration 17099, loss = 7.24172231\n",
      "Iteration 17100, loss = 7.00903438\n",
      "Iteration 17101, loss = 7.84020580\n",
      "Iteration 17102, loss = 6.89234162\n",
      "Iteration 17103, loss = 7.19887531\n",
      "Iteration 17104, loss = 7.76470161\n",
      "Iteration 17105, loss = 6.71779077\n",
      "Iteration 17106, loss = 5.96205309\n",
      "Iteration 17107, loss = 6.01662688\n",
      "Iteration 17108, loss = 6.08854446\n",
      "Iteration 17109, loss = 6.25109491\n",
      "Iteration 17110, loss = 7.33318225\n",
      "Iteration 17111, loss = 9.08205618\n",
      "Iteration 17112, loss = 8.16627597\n",
      "Iteration 17113, loss = 7.49765516\n",
      "Iteration 17114, loss = 6.88892424\n",
      "Iteration 17115, loss = 6.79419904\n",
      "Iteration 17116, loss = 6.67327922\n",
      "Iteration 17117, loss = 6.39440491\n",
      "Iteration 17118, loss = 6.25264758\n",
      "Iteration 17119, loss = 7.67476625\n",
      "Iteration 17120, loss = 7.39391767\n",
      "Iteration 17121, loss = 6.42251071\n",
      "Iteration 17122, loss = 6.67987079\n",
      "Iteration 17123, loss = 6.89090411\n",
      "Iteration 17124, loss = 7.04395144\n",
      "Iteration 17125, loss = 6.44029914\n",
      "Iteration 17126, loss = 6.18765505\n",
      "Iteration 17127, loss = 5.90996809\n",
      "Iteration 17128, loss = 6.01733034\n",
      "Iteration 17129, loss = 6.09151902\n",
      "Iteration 17130, loss = 6.75630308\n",
      "Iteration 17131, loss = 6.11945745\n",
      "Iteration 17132, loss = 6.53336668\n",
      "Iteration 17133, loss = 6.70878773\n",
      "Iteration 17134, loss = 7.62894689\n",
      "Iteration 17135, loss = 7.89888969\n",
      "Iteration 17136, loss = 7.35053162\n",
      "Iteration 17137, loss = 6.46881753\n",
      "Iteration 17138, loss = 6.21637709\n",
      "Iteration 17139, loss = 6.26115059\n",
      "Iteration 17140, loss = 6.45799479\n",
      "Iteration 17141, loss = 6.88431568\n",
      "Iteration 17142, loss = 6.39330160\n",
      "Iteration 17143, loss = 6.77898337\n",
      "Iteration 17144, loss = 7.19869035\n",
      "Iteration 17145, loss = 6.50780572\n",
      "Iteration 17146, loss = 6.21146114\n",
      "Iteration 17147, loss = 6.22622297\n",
      "Iteration 17148, loss = 6.05614651\n",
      "Iteration 17149, loss = 6.02758196\n",
      "Iteration 17150, loss = 6.32512032\n",
      "Iteration 17151, loss = 6.05935181\n",
      "Iteration 17152, loss = 6.44739489\n",
      "Iteration 17153, loss = 5.93834140\n",
      "Iteration 17154, loss = 6.12917204\n",
      "Iteration 17155, loss = 6.23624915\n",
      "Iteration 17156, loss = 6.23818378\n",
      "Iteration 17157, loss = 6.71097131\n",
      "Iteration 17158, loss = 6.92055433\n",
      "Iteration 17159, loss = 7.20872600\n",
      "Iteration 17160, loss = 6.28485541\n",
      "Iteration 17161, loss = 6.16929545\n",
      "Iteration 17162, loss = 6.24000109\n",
      "Iteration 17163, loss = 6.57462432\n",
      "Iteration 17164, loss = 5.96714126\n",
      "Iteration 17165, loss = 6.17900624\n",
      "Iteration 17166, loss = 5.82375766\n",
      "Iteration 17167, loss = 5.82019444\n",
      "Iteration 17168, loss = 6.85133716\n",
      "Iteration 17169, loss = 6.76092126\n",
      "Iteration 17170, loss = 6.63825951\n",
      "Iteration 17171, loss = 6.99514369\n",
      "Iteration 17172, loss = 5.89140511\n",
      "Iteration 17173, loss = 6.38740595\n",
      "Iteration 17174, loss = 6.61862097\n",
      "Iteration 17175, loss = 7.31431186\n",
      "Iteration 17176, loss = 8.09130271\n",
      "Iteration 17177, loss = 6.65737106\n",
      "Iteration 17178, loss = 6.49006442\n",
      "Iteration 17179, loss = 6.69874142\n",
      "Iteration 17180, loss = 6.55852106\n",
      "Iteration 17181, loss = 5.96108978\n",
      "Iteration 17182, loss = 6.51945975\n",
      "Iteration 17183, loss = 6.44337492\n",
      "Iteration 17184, loss = 5.88068061\n",
      "Iteration 17185, loss = 6.21502043\n",
      "Iteration 17186, loss = 6.02535883\n",
      "Iteration 17187, loss = 7.18154426\n",
      "Iteration 17188, loss = 6.79770906\n",
      "Iteration 17189, loss = 6.36727610\n",
      "Iteration 17190, loss = 6.71837906\n",
      "Iteration 17191, loss = 6.78849372\n",
      "Iteration 17192, loss = 7.15256443\n",
      "Iteration 17193, loss = 6.17676901\n",
      "Iteration 17194, loss = 6.35016288\n",
      "Iteration 17195, loss = 6.02136527\n",
      "Iteration 17196, loss = 6.65001420\n",
      "Iteration 17197, loss = 7.62746467\n",
      "Iteration 17198, loss = 6.88370886\n",
      "Iteration 17199, loss = 7.53836794\n",
      "Iteration 17200, loss = 7.39748088\n",
      "Iteration 17201, loss = 6.86969525\n",
      "Iteration 17202, loss = 6.46383729\n",
      "Iteration 17203, loss = 6.53665728\n",
      "Iteration 17204, loss = 6.31367555\n",
      "Iteration 17205, loss = 6.16546536\n",
      "Iteration 17206, loss = 6.20352350\n",
      "Iteration 17207, loss = 5.98587095\n",
      "Iteration 17208, loss = 6.36573849\n",
      "Iteration 17209, loss = 7.46847835\n",
      "Iteration 17210, loss = 8.22528523\n",
      "Iteration 17211, loss = 8.52503313\n",
      "Iteration 17212, loss = 7.23087487\n",
      "Iteration 17213, loss = 7.33810815\n",
      "Iteration 17214, loss = 7.22139407\n",
      "Iteration 17215, loss = 6.64537076\n",
      "Iteration 17216, loss = 7.42636607\n",
      "Iteration 17217, loss = 7.81018235\n",
      "Iteration 17218, loss = 7.21761844\n",
      "Iteration 17219, loss = 6.57768306\n",
      "Iteration 17220, loss = 6.48733161\n",
      "Iteration 17221, loss = 6.07829234\n",
      "Iteration 17222, loss = 5.93616644\n",
      "Iteration 17223, loss = 6.06313348\n",
      "Iteration 17224, loss = 6.06007714\n",
      "Iteration 17225, loss = 6.00951831\n",
      "Iteration 17226, loss = 5.96346312\n",
      "Iteration 17227, loss = 6.94573227\n",
      "Iteration 17228, loss = 7.31263858\n",
      "Iteration 17229, loss = 6.26051448\n",
      "Iteration 17230, loss = 6.62145236\n",
      "Iteration 17231, loss = 6.04862917\n",
      "Iteration 17232, loss = 7.27407005\n",
      "Iteration 17233, loss = 6.72320159\n",
      "Iteration 17234, loss = 7.40046744\n",
      "Iteration 17235, loss = 7.65967181\n",
      "Iteration 17236, loss = 6.31302654\n",
      "Iteration 17237, loss = 6.35573667\n",
      "Iteration 17238, loss = 6.40966044\n",
      "Iteration 17239, loss = 6.94206247\n",
      "Iteration 17240, loss = 6.76227754\n",
      "Iteration 17241, loss = 6.46761588\n",
      "Iteration 17242, loss = 6.29995657\n",
      "Iteration 17243, loss = 6.17863479\n",
      "Iteration 17244, loss = 6.26419480\n",
      "Iteration 17245, loss = 6.76955299\n",
      "Iteration 17246, loss = 6.10894029\n",
      "Iteration 17247, loss = 5.91530561\n",
      "Iteration 17248, loss = 6.40822080\n",
      "Iteration 17249, loss = 6.23958083\n",
      "Iteration 17250, loss = 6.63436349\n",
      "Iteration 17251, loss = 7.43053760\n",
      "Iteration 17252, loss = 6.34942451\n",
      "Iteration 17253, loss = 6.25115807\n",
      "Iteration 17254, loss = 6.97208415\n",
      "Iteration 17255, loss = 6.43937021\n",
      "Iteration 17256, loss = 6.40317852\n",
      "Iteration 17257, loss = 6.66531361\n",
      "Iteration 17258, loss = 6.17344766\n",
      "Iteration 17259, loss = 6.03679053\n",
      "Iteration 17260, loss = 7.39024460\n",
      "Iteration 17261, loss = 6.77022226\n",
      "Iteration 17262, loss = 7.51287024\n",
      "Iteration 17263, loss = 6.33963587\n",
      "Iteration 17264, loss = 7.20076843\n",
      "Iteration 17265, loss = 6.14441347\n",
      "Iteration 17266, loss = 6.01042266\n",
      "Iteration 17267, loss = 6.39728995\n",
      "Iteration 17268, loss = 6.37365351\n",
      "Iteration 17269, loss = 6.68288914\n",
      "Iteration 17270, loss = 7.45150179\n",
      "Iteration 17271, loss = 6.87966880\n",
      "Iteration 17272, loss = 6.39419709\n",
      "Iteration 17273, loss = 6.58801188\n",
      "Iteration 17274, loss = 6.15199419\n",
      "Iteration 17275, loss = 7.26372044\n",
      "Iteration 17276, loss = 7.29545944\n",
      "Iteration 17277, loss = 7.14615811\n",
      "Iteration 17278, loss = 7.86879706\n",
      "Iteration 17279, loss = 6.49096105\n",
      "Iteration 17280, loss = 5.70765583\n",
      "Iteration 17281, loss = 6.63715062\n",
      "Iteration 17282, loss = 6.11140310\n",
      "Iteration 17283, loss = 7.16557946\n",
      "Iteration 17284, loss = 8.01264055\n",
      "Iteration 17285, loss = 6.90529224\n",
      "Iteration 17286, loss = 6.45034248\n",
      "Iteration 17287, loss = 6.04457711\n",
      "Iteration 17288, loss = 6.01757521\n",
      "Iteration 17289, loss = 5.90220775\n",
      "Iteration 17290, loss = 5.97516213\n",
      "Iteration 17291, loss = 5.76287963\n",
      "Iteration 17292, loss = 6.68289325\n",
      "Iteration 17293, loss = 6.30822100\n",
      "Iteration 17294, loss = 8.18993268\n",
      "Iteration 17295, loss = 7.24427280\n",
      "Iteration 17296, loss = 6.87795860\n",
      "Iteration 17297, loss = 6.66016344\n",
      "Iteration 17298, loss = 6.37725187\n",
      "Iteration 17299, loss = 6.58777632\n",
      "Iteration 17300, loss = 6.31892430\n",
      "Iteration 17301, loss = 6.60725897\n",
      "Iteration 17302, loss = 6.18057096\n",
      "Iteration 17303, loss = 6.19848978\n",
      "Iteration 17304, loss = 6.14227813\n",
      "Iteration 17305, loss = 5.85210609\n",
      "Iteration 17306, loss = 6.24075056\n",
      "Iteration 17307, loss = 6.26483198\n",
      "Iteration 17308, loss = 7.18058928\n",
      "Iteration 17309, loss = 6.91295612\n",
      "Iteration 17310, loss = 7.27849023\n",
      "Iteration 17311, loss = 7.24195900\n",
      "Iteration 17312, loss = 6.41326842\n",
      "Iteration 17313, loss = 6.28303029\n",
      "Iteration 17314, loss = 7.38431569\n",
      "Iteration 17315, loss = 6.69772390\n",
      "Iteration 17316, loss = 6.52306607\n",
      "Iteration 17317, loss = 5.97299443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17318, loss = 6.13449992\n",
      "Iteration 17319, loss = 6.88531195\n",
      "Iteration 17320, loss = 6.43750848\n",
      "Iteration 17321, loss = 6.71542110\n",
      "Iteration 17322, loss = 6.20439681\n",
      "Iteration 17323, loss = 6.00102724\n",
      "Iteration 17324, loss = 5.85161236\n",
      "Iteration 17325, loss = 6.01650925\n",
      "Iteration 17326, loss = 6.30366531\n",
      "Iteration 17327, loss = 6.28665661\n",
      "Iteration 17328, loss = 5.94686189\n",
      "Iteration 17329, loss = 6.39886765\n",
      "Iteration 17330, loss = 6.36172949\n",
      "Iteration 17331, loss = 6.25631958\n",
      "Iteration 17332, loss = 6.49120429\n",
      "Iteration 17333, loss = 6.48879687\n",
      "Iteration 17334, loss = 6.25229852\n",
      "Iteration 17335, loss = 6.02349934\n",
      "Iteration 17336, loss = 6.14869901\n",
      "Iteration 17337, loss = 5.89948001\n",
      "Iteration 17338, loss = 6.26005865\n",
      "Iteration 17339, loss = 6.21780714\n",
      "Iteration 17340, loss = 6.29524819\n",
      "Iteration 17341, loss = 6.10026633\n",
      "Iteration 17342, loss = 6.40520338\n",
      "Iteration 17343, loss = 6.67613692\n",
      "Iteration 17344, loss = 6.54660880\n",
      "Iteration 17345, loss = 6.92118939\n",
      "Iteration 17346, loss = 6.66939353\n",
      "Iteration 17347, loss = 6.79935869\n",
      "Iteration 17348, loss = 6.41325063\n",
      "Iteration 17349, loss = 6.80533713\n",
      "Iteration 17350, loss = 6.93148382\n",
      "Iteration 17351, loss = 7.31542806\n",
      "Iteration 17352, loss = 8.64245731\n",
      "Iteration 17353, loss = 6.51073404\n",
      "Iteration 17354, loss = 7.19254529\n",
      "Iteration 17355, loss = 7.17168110\n",
      "Iteration 17356, loss = 6.71139305\n",
      "Iteration 17357, loss = 7.88860935\n",
      "Iteration 17358, loss = 7.28850167\n",
      "Iteration 17359, loss = 6.54416547\n",
      "Iteration 17360, loss = 6.39921919\n",
      "Iteration 17361, loss = 6.40759165\n",
      "Iteration 17362, loss = 6.13219512\n",
      "Iteration 17363, loss = 6.09452338\n",
      "Iteration 17364, loss = 6.58484308\n",
      "Iteration 17365, loss = 7.25330939\n",
      "Iteration 17366, loss = 7.18050160\n",
      "Iteration 17367, loss = 6.79986649\n",
      "Iteration 17368, loss = 6.55387530\n",
      "Iteration 17369, loss = 6.13997235\n",
      "Iteration 17370, loss = 6.01903706\n",
      "Iteration 17371, loss = 6.60808200\n",
      "Iteration 17372, loss = 5.67570493\n",
      "Iteration 17373, loss = 6.19656167\n",
      "Iteration 17374, loss = 5.95694838\n",
      "Iteration 17375, loss = 5.94867391\n",
      "Iteration 17376, loss = 6.30609952\n",
      "Iteration 17377, loss = 7.39259299\n",
      "Iteration 17378, loss = 6.20464233\n",
      "Iteration 17379, loss = 6.05291213\n",
      "Iteration 17380, loss = 5.91629735\n",
      "Iteration 17381, loss = 6.15511673\n",
      "Iteration 17382, loss = 6.40359402\n",
      "Iteration 17383, loss = 7.14375637\n",
      "Iteration 17384, loss = 7.28057420\n",
      "Iteration 17385, loss = 6.25632435\n",
      "Iteration 17386, loss = 6.16927197\n",
      "Iteration 17387, loss = 6.22033007\n",
      "Iteration 17388, loss = 5.98768605\n",
      "Iteration 17389, loss = 5.82503184\n",
      "Iteration 17390, loss = 5.82388081\n",
      "Iteration 17391, loss = 6.04814751\n",
      "Iteration 17392, loss = 6.18524377\n",
      "Iteration 17393, loss = 5.77200716\n",
      "Iteration 17394, loss = 6.14800446\n",
      "Iteration 17395, loss = 6.04907428\n",
      "Iteration 17396, loss = 6.39832886\n",
      "Iteration 17397, loss = 6.80773855\n",
      "Iteration 17398, loss = 6.60775406\n",
      "Iteration 17399, loss = 6.16044541\n",
      "Iteration 17400, loss = 6.06542791\n",
      "Iteration 17401, loss = 6.18354566\n",
      "Iteration 17402, loss = 5.76834219\n",
      "Iteration 17403, loss = 5.95920948\n",
      "Iteration 17404, loss = 6.61865498\n",
      "Iteration 17405, loss = 5.99267956\n",
      "Iteration 17406, loss = 6.38669579\n",
      "Iteration 17407, loss = 6.43531906\n",
      "Iteration 17408, loss = 6.31076737\n",
      "Iteration 17409, loss = 6.22391218\n",
      "Iteration 17410, loss = 6.34333311\n",
      "Iteration 17411, loss = 6.10940051\n",
      "Iteration 17412, loss = 6.13365488\n",
      "Iteration 17413, loss = 5.89156945\n",
      "Iteration 17414, loss = 6.55977205\n",
      "Iteration 17415, loss = 6.87339202\n",
      "Iteration 17416, loss = 7.36232888\n",
      "Iteration 17417, loss = 7.69261142\n",
      "Iteration 17418, loss = 6.44250321\n",
      "Iteration 17419, loss = 6.21313206\n",
      "Iteration 17420, loss = 6.27906278\n",
      "Iteration 17421, loss = 6.37058784\n",
      "Iteration 17422, loss = 6.41891697\n",
      "Iteration 17423, loss = 5.86851900\n",
      "Iteration 17424, loss = 6.34693709\n",
      "Iteration 17425, loss = 5.93578602\n",
      "Iteration 17426, loss = 5.86754019\n",
      "Iteration 17427, loss = 6.29709295\n",
      "Iteration 17428, loss = 6.56204473\n",
      "Iteration 17429, loss = 7.01859979\n",
      "Iteration 17430, loss = 6.64658033\n",
      "Iteration 17431, loss = 6.31874532\n",
      "Iteration 17432, loss = 6.26705517\n",
      "Iteration 17433, loss = 6.27317945\n",
      "Iteration 17434, loss = 6.62893340\n",
      "Iteration 17435, loss = 6.46135254\n",
      "Iteration 17436, loss = 6.28182302\n",
      "Iteration 17437, loss = 6.07933842\n",
      "Iteration 17438, loss = 6.15069351\n",
      "Iteration 17439, loss = 6.88795621\n",
      "Iteration 17440, loss = 7.37390657\n",
      "Iteration 17441, loss = 6.40118535\n",
      "Iteration 17442, loss = 6.48566236\n",
      "Iteration 17443, loss = 7.16456373\n",
      "Iteration 17444, loss = 6.64892188\n",
      "Iteration 17445, loss = 6.97901374\n",
      "Iteration 17446, loss = 6.39100810\n",
      "Iteration 17447, loss = 5.84028924\n",
      "Iteration 17448, loss = 6.24348534\n",
      "Iteration 17449, loss = 6.39183318\n",
      "Iteration 17450, loss = 6.49746473\n",
      "Iteration 17451, loss = 6.42985325\n",
      "Iteration 17452, loss = 7.14976702\n",
      "Iteration 17453, loss = 6.56730343\n",
      "Iteration 17454, loss = 6.27543622\n",
      "Iteration 17455, loss = 7.52476340\n",
      "Iteration 17456, loss = 7.52286296\n",
      "Iteration 17457, loss = 7.67387702\n",
      "Iteration 17458, loss = 6.64510440\n",
      "Iteration 17459, loss = 6.32909809\n",
      "Iteration 17460, loss = 6.64946923\n",
      "Iteration 17461, loss = 6.52128969\n",
      "Iteration 17462, loss = 7.13032834\n",
      "Iteration 17463, loss = 6.94842932\n",
      "Iteration 17464, loss = 6.55894832\n",
      "Iteration 17465, loss = 5.89754576\n",
      "Iteration 17466, loss = 6.54798668\n",
      "Iteration 17467, loss = 6.64421825\n",
      "Iteration 17468, loss = 7.05522791\n",
      "Iteration 17469, loss = 6.30962136\n",
      "Iteration 17470, loss = 6.60797972\n",
      "Iteration 17471, loss = 6.29625826\n",
      "Iteration 17472, loss = 6.08396135\n",
      "Iteration 17473, loss = 6.39633042\n",
      "Iteration 17474, loss = 6.50178561\n",
      "Iteration 17475, loss = 6.32409890\n",
      "Iteration 17476, loss = 6.01321720\n",
      "Iteration 17477, loss = 6.51163380\n",
      "Iteration 17478, loss = 6.91021858\n",
      "Iteration 17479, loss = 7.15149254\n",
      "Iteration 17480, loss = 7.73173586\n",
      "Iteration 17481, loss = 7.24061862\n",
      "Iteration 17482, loss = 7.26790258\n",
      "Iteration 17483, loss = 7.46035987\n",
      "Iteration 17484, loss = 7.55129619\n",
      "Iteration 17485, loss = 7.30286355\n",
      "Iteration 17486, loss = 6.71080560\n",
      "Iteration 17487, loss = 7.34741657\n",
      "Iteration 17488, loss = 6.73574336\n",
      "Iteration 17489, loss = 6.23063677\n",
      "Iteration 17490, loss = 6.05618087\n",
      "Iteration 17491, loss = 6.44280280\n",
      "Iteration 17492, loss = 7.34055973\n",
      "Iteration 17493, loss = 6.39605302\n",
      "Iteration 17494, loss = 6.15173275\n",
      "Iteration 17495, loss = 6.35506120\n",
      "Iteration 17496, loss = 6.08417538\n",
      "Iteration 17497, loss = 5.93837373\n",
      "Iteration 17498, loss = 6.01673874\n",
      "Iteration 17499, loss = 5.85699271\n",
      "Iteration 17500, loss = 6.08428289\n",
      "Iteration 17501, loss = 6.09094722\n",
      "Iteration 17502, loss = 6.12599175\n",
      "Iteration 17503, loss = 6.35667575\n",
      "Iteration 17504, loss = 6.88498129\n",
      "Iteration 17505, loss = 7.44369655\n",
      "Iteration 17506, loss = 6.70690353\n",
      "Iteration 17507, loss = 6.52342821\n",
      "Iteration 17508, loss = 6.10213174\n",
      "Iteration 17509, loss = 6.93863661\n",
      "Iteration 17510, loss = 6.16798240\n",
      "Iteration 17511, loss = 6.26044784\n",
      "Iteration 17512, loss = 6.51322758\n",
      "Iteration 17513, loss = 6.26986093\n",
      "Iteration 17514, loss = 5.92472876\n",
      "Iteration 17515, loss = 6.15658175\n",
      "Iteration 17516, loss = 5.96162176\n",
      "Iteration 17517, loss = 6.07490206\n",
      "Iteration 17518, loss = 6.28245814\n",
      "Iteration 17519, loss = 5.87069844\n",
      "Iteration 17520, loss = 6.42766904\n",
      "Iteration 17521, loss = 6.11780366\n",
      "Iteration 17522, loss = 6.08101585\n",
      "Iteration 17523, loss = 5.94623648\n",
      "Iteration 17524, loss = 5.95097850\n",
      "Iteration 17525, loss = 6.24620271\n",
      "Iteration 17526, loss = 5.93745358\n",
      "Iteration 17527, loss = 6.61680678\n",
      "Iteration 17528, loss = 5.64577354\n",
      "Iteration 17529, loss = 5.93532587\n",
      "Iteration 17530, loss = 5.85862595\n",
      "Iteration 17531, loss = 6.12324857\n",
      "Iteration 17532, loss = 5.87912309\n",
      "Iteration 17533, loss = 6.88931378\n",
      "Iteration 17534, loss = 6.14598975\n",
      "Iteration 17535, loss = 6.95476995\n",
      "Iteration 17536, loss = 6.69159278\n",
      "Iteration 17537, loss = 7.09042729\n",
      "Iteration 17538, loss = 6.32911554\n",
      "Iteration 17539, loss = 7.39800555\n",
      "Iteration 17540, loss = 7.54679122\n",
      "Iteration 17541, loss = 6.39216263\n",
      "Iteration 17542, loss = 6.26291122\n",
      "Iteration 17543, loss = 6.50239925\n",
      "Iteration 17544, loss = 6.31813404\n",
      "Iteration 17545, loss = 6.09935369\n",
      "Iteration 17546, loss = 6.10440420\n",
      "Iteration 17547, loss = 6.09661601\n",
      "Iteration 17548, loss = 5.90198977\n",
      "Iteration 17549, loss = 7.17579245\n",
      "Iteration 17550, loss = 6.37999775\n",
      "Iteration 17551, loss = 6.17102331\n",
      "Iteration 17552, loss = 6.51025043\n",
      "Iteration 17553, loss = 6.12781347\n",
      "Iteration 17554, loss = 6.08817599\n",
      "Iteration 17555, loss = 5.83533325\n",
      "Iteration 17556, loss = 6.26035265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17557, loss = 5.73744792\n",
      "Iteration 17558, loss = 5.88387329\n",
      "Iteration 17559, loss = 5.92064884\n",
      "Iteration 17560, loss = 6.15678712\n",
      "Iteration 17561, loss = 6.59264969\n",
      "Iteration 17562, loss = 7.89707061\n",
      "Iteration 17563, loss = 7.26848639\n",
      "Iteration 17564, loss = 6.33892202\n",
      "Iteration 17565, loss = 6.36948028\n",
      "Iteration 17566, loss = 5.71211272\n",
      "Iteration 17567, loss = 6.14088648\n",
      "Iteration 17568, loss = 6.32116456\n",
      "Iteration 17569, loss = 5.98454996\n",
      "Iteration 17570, loss = 6.85378344\n",
      "Iteration 17571, loss = 6.13870324\n",
      "Iteration 17572, loss = 6.65840814\n",
      "Iteration 17573, loss = 6.56859816\n",
      "Iteration 17574, loss = 6.45123370\n",
      "Iteration 17575, loss = 6.51195335\n",
      "Iteration 17576, loss = 7.16441292\n",
      "Iteration 17577, loss = 6.40218125\n",
      "Iteration 17578, loss = 6.19545831\n",
      "Iteration 17579, loss = 6.02360842\n",
      "Iteration 17580, loss = 6.78301763\n",
      "Iteration 17581, loss = 6.44236186\n",
      "Iteration 17582, loss = 6.65468990\n",
      "Iteration 17583, loss = 6.45725102\n",
      "Iteration 17584, loss = 7.00100823\n",
      "Iteration 17585, loss = 6.18461006\n",
      "Iteration 17586, loss = 6.38357028\n",
      "Iteration 17587, loss = 6.60434654\n",
      "Iteration 17588, loss = 6.78529833\n",
      "Iteration 17589, loss = 7.22406721\n",
      "Iteration 17590, loss = 7.97635369\n",
      "Iteration 17591, loss = 6.81109531\n",
      "Iteration 17592, loss = 7.24108030\n",
      "Iteration 17593, loss = 6.88036791\n",
      "Iteration 17594, loss = 6.22165590\n",
      "Iteration 17595, loss = 6.49374147\n",
      "Iteration 17596, loss = 6.54322545\n",
      "Iteration 17597, loss = 6.42593730\n",
      "Iteration 17598, loss = 6.05611438\n",
      "Iteration 17599, loss = 7.36586875\n",
      "Iteration 17600, loss = 7.31516700\n",
      "Iteration 17601, loss = 6.41346835\n",
      "Iteration 17602, loss = 6.38948855\n",
      "Iteration 17603, loss = 6.63847260\n",
      "Iteration 17604, loss = 7.06056608\n",
      "Iteration 17605, loss = 6.50575886\n",
      "Iteration 17606, loss = 5.92835063\n",
      "Iteration 17607, loss = 5.89726471\n",
      "Iteration 17608, loss = 6.00683240\n",
      "Iteration 17609, loss = 6.24539299\n",
      "Iteration 17610, loss = 6.78334694\n",
      "Iteration 17611, loss = 7.57264422\n",
      "Iteration 17612, loss = 7.57372237\n",
      "Iteration 17613, loss = 7.20236803\n",
      "Iteration 17614, loss = 6.79771555\n",
      "Iteration 17615, loss = 6.84696466\n",
      "Iteration 17616, loss = 7.00954552\n",
      "Iteration 17617, loss = 6.65266918\n",
      "Iteration 17618, loss = 7.94427946\n",
      "Iteration 17619, loss = 7.33708025\n",
      "Iteration 17620, loss = 7.12732038\n",
      "Iteration 17621, loss = 7.10791338\n",
      "Iteration 17622, loss = 6.61617268\n",
      "Iteration 17623, loss = 6.02731011\n",
      "Iteration 17624, loss = 6.04835756\n",
      "Iteration 17625, loss = 5.72246906\n",
      "Iteration 17626, loss = 5.93158958\n",
      "Iteration 17627, loss = 6.02746936\n",
      "Iteration 17628, loss = 5.99107449\n",
      "Iteration 17629, loss = 5.96204318\n",
      "Iteration 17630, loss = 5.99643912\n",
      "Iteration 17631, loss = 5.99969507\n",
      "Iteration 17632, loss = 6.34420929\n",
      "Iteration 17633, loss = 6.54592674\n",
      "Iteration 17634, loss = 6.49389772\n",
      "Iteration 17635, loss = 6.76682908\n",
      "Iteration 17636, loss = 6.45509421\n",
      "Iteration 17637, loss = 6.02483653\n",
      "Iteration 17638, loss = 6.16742210\n",
      "Iteration 17639, loss = 6.20883138\n",
      "Iteration 17640, loss = 5.80385975\n",
      "Iteration 17641, loss = 5.96025395\n",
      "Iteration 17642, loss = 5.87137990\n",
      "Iteration 17643, loss = 5.93839998\n",
      "Iteration 17644, loss = 5.96926651\n",
      "Iteration 17645, loss = 6.80439504\n",
      "Iteration 17646, loss = 6.45628764\n",
      "Iteration 17647, loss = 6.21673113\n",
      "Iteration 17648, loss = 6.02266465\n",
      "Iteration 17649, loss = 6.38340131\n",
      "Iteration 17650, loss = 5.98583925\n",
      "Iteration 17651, loss = 6.45499194\n",
      "Iteration 17652, loss = 6.00265213\n",
      "Iteration 17653, loss = 6.97549845\n",
      "Iteration 17654, loss = 6.41062117\n",
      "Iteration 17655, loss = 7.01676275\n",
      "Iteration 17656, loss = 6.29854270\n",
      "Iteration 17657, loss = 6.19103709\n",
      "Iteration 17658, loss = 6.07080760\n",
      "Iteration 17659, loss = 6.44428780\n",
      "Iteration 17660, loss = 5.69382013\n",
      "Iteration 17661, loss = 6.45426969\n",
      "Iteration 17662, loss = 6.10214093\n",
      "Iteration 17663, loss = 7.44111926\n",
      "Iteration 17664, loss = 6.17561163\n",
      "Iteration 17665, loss = 6.60324822\n",
      "Iteration 17666, loss = 6.03412927\n",
      "Iteration 17667, loss = 6.27420369\n",
      "Iteration 17668, loss = 6.45585083\n",
      "Iteration 17669, loss = 6.14794211\n",
      "Iteration 17670, loss = 6.63438066\n",
      "Iteration 17671, loss = 6.53058415\n",
      "Iteration 17672, loss = 6.37632462\n",
      "Iteration 17673, loss = 6.33073169\n",
      "Iteration 17674, loss = 5.85785566\n",
      "Iteration 17675, loss = 6.10063874\n",
      "Iteration 17676, loss = 6.22911601\n",
      "Iteration 17677, loss = 7.44729218\n",
      "Iteration 17678, loss = 7.54275085\n",
      "Iteration 17679, loss = 6.56548626\n",
      "Iteration 17680, loss = 8.17092417\n",
      "Iteration 17681, loss = 8.24774020\n",
      "Iteration 17682, loss = 6.80200498\n",
      "Iteration 17683, loss = 6.20674495\n",
      "Iteration 17684, loss = 7.58225441\n",
      "Iteration 17685, loss = 6.63446149\n",
      "Iteration 17686, loss = 6.59180477\n",
      "Iteration 17687, loss = 6.77432854\n",
      "Iteration 17688, loss = 7.51754980\n",
      "Iteration 17689, loss = 7.88272032\n",
      "Iteration 17690, loss = 6.70957122\n",
      "Iteration 17691, loss = 6.46388927\n",
      "Iteration 17692, loss = 6.00751415\n",
      "Iteration 17693, loss = 6.01243633\n",
      "Iteration 17694, loss = 6.16129419\n",
      "Iteration 17695, loss = 5.90364202\n",
      "Iteration 17696, loss = 5.82033144\n",
      "Iteration 17697, loss = 5.92747094\n",
      "Iteration 17698, loss = 6.26149772\n",
      "Iteration 17699, loss = 6.62817559\n",
      "Iteration 17700, loss = 5.74806155\n",
      "Iteration 17701, loss = 6.08045987\n",
      "Iteration 17702, loss = 5.90120042\n",
      "Iteration 17703, loss = 6.24934490\n",
      "Iteration 17704, loss = 6.23764319\n",
      "Iteration 17705, loss = 6.41873001\n",
      "Iteration 17706, loss = 6.32115258\n",
      "Iteration 17707, loss = 6.05288922\n",
      "Iteration 17708, loss = 6.07625123\n",
      "Iteration 17709, loss = 5.83083371\n",
      "Iteration 17710, loss = 6.11722621\n",
      "Iteration 17711, loss = 6.07729566\n",
      "Iteration 17712, loss = 7.28393921\n",
      "Iteration 17713, loss = 6.95713412\n",
      "Iteration 17714, loss = 6.68901877\n",
      "Iteration 17715, loss = 5.95189855\n",
      "Iteration 17716, loss = 6.95458374\n",
      "Iteration 17717, loss = 6.90871758\n",
      "Iteration 17718, loss = 6.70709919\n",
      "Iteration 17719, loss = 6.71700928\n",
      "Iteration 17720, loss = 6.39890364\n",
      "Iteration 17721, loss = 6.40424580\n",
      "Iteration 17722, loss = 5.70188350\n",
      "Iteration 17723, loss = 5.90897871\n",
      "Iteration 17724, loss = 5.75858931\n",
      "Iteration 17725, loss = 6.19664395\n",
      "Iteration 17726, loss = 5.97049338\n",
      "Iteration 17727, loss = 6.02307030\n",
      "Iteration 17728, loss = 5.85619852\n",
      "Iteration 17729, loss = 6.37214857\n",
      "Iteration 17730, loss = 7.19492788\n",
      "Iteration 17731, loss = 7.22897512\n",
      "Iteration 17732, loss = 7.37922072\n",
      "Iteration 17733, loss = 6.37443909\n",
      "Iteration 17734, loss = 6.71771796\n",
      "Iteration 17735, loss = 5.96641686\n",
      "Iteration 17736, loss = 6.06724780\n",
      "Iteration 17737, loss = 6.62657677\n",
      "Iteration 17738, loss = 6.60287069\n",
      "Iteration 17739, loss = 6.83466965\n",
      "Iteration 17740, loss = 7.38611327\n",
      "Iteration 17741, loss = 6.77334694\n",
      "Iteration 17742, loss = 6.19469843\n",
      "Iteration 17743, loss = 6.08153268\n",
      "Iteration 17744, loss = 6.33735800\n",
      "Iteration 17745, loss = 5.77365513\n",
      "Iteration 17746, loss = 6.03587590\n",
      "Iteration 17747, loss = 6.03286323\n",
      "Iteration 17748, loss = 5.69661611\n",
      "Iteration 17749, loss = 6.18859071\n",
      "Iteration 17750, loss = 6.87438715\n",
      "Iteration 17751, loss = 6.26475982\n",
      "Iteration 17752, loss = 6.10538595\n",
      "Iteration 17753, loss = 6.90600652\n",
      "Iteration 17754, loss = 9.14301453\n",
      "Iteration 17755, loss = 8.09179804\n",
      "Iteration 17756, loss = 8.22466337\n",
      "Iteration 17757, loss = 7.76909829\n",
      "Iteration 17758, loss = 7.26574546\n",
      "Iteration 17759, loss = 6.59508102\n",
      "Iteration 17760, loss = 7.43122959\n",
      "Iteration 17761, loss = 6.38482999\n",
      "Iteration 17762, loss = 7.08936136\n",
      "Iteration 17763, loss = 6.63555329\n",
      "Iteration 17764, loss = 6.49714199\n",
      "Iteration 17765, loss = 6.77554225\n",
      "Iteration 17766, loss = 7.08850525\n",
      "Iteration 17767, loss = 8.27532748\n",
      "Iteration 17768, loss = 8.13930556\n",
      "Iteration 17769, loss = 8.65345572\n",
      "Iteration 17770, loss = 7.43038456\n",
      "Iteration 17771, loss = 7.17005011\n",
      "Iteration 17772, loss = 6.09623626\n",
      "Iteration 17773, loss = 6.82840882\n",
      "Iteration 17774, loss = 6.30974097\n",
      "Iteration 17775, loss = 6.41375119\n",
      "Iteration 17776, loss = 6.42517120\n",
      "Iteration 17777, loss = 6.28630456\n",
      "Iteration 17778, loss = 7.30892040\n",
      "Iteration 17779, loss = 6.19175970\n",
      "Iteration 17780, loss = 6.81148592\n",
      "Iteration 17781, loss = 7.02397121\n",
      "Iteration 17782, loss = 7.05468953\n",
      "Iteration 17783, loss = 5.94621725\n",
      "Iteration 17784, loss = 6.36315390\n",
      "Iteration 17785, loss = 6.21422903\n",
      "Iteration 17786, loss = 6.17353025\n",
      "Iteration 17787, loss = 6.14745141\n",
      "Iteration 17788, loss = 6.76082798\n",
      "Iteration 17789, loss = 7.17977610\n",
      "Iteration 17790, loss = 6.92279215\n",
      "Iteration 17791, loss = 6.55228433\n",
      "Iteration 17792, loss = 6.21478794\n",
      "Iteration 17793, loss = 6.95020368\n",
      "Iteration 17794, loss = 6.96460193\n",
      "Iteration 17795, loss = 5.91097331\n",
      "Iteration 17796, loss = 6.22062319\n",
      "Iteration 17797, loss = 5.83280227\n",
      "Iteration 17798, loss = 6.10794177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17799, loss = 6.53063171\n",
      "Iteration 17800, loss = 5.88169620\n",
      "Iteration 17801, loss = 6.10523548\n",
      "Iteration 17802, loss = 5.65185332\n",
      "Iteration 17803, loss = 5.82410994\n",
      "Iteration 17804, loss = 5.96100505\n",
      "Iteration 17805, loss = 6.16418141\n",
      "Iteration 17806, loss = 5.90778683\n",
      "Iteration 17807, loss = 5.70260907\n",
      "Iteration 17808, loss = 5.72324795\n",
      "Iteration 17809, loss = 5.69744687\n",
      "Iteration 17810, loss = 5.84630380\n",
      "Iteration 17811, loss = 5.63411967\n",
      "Iteration 17812, loss = 5.77126774\n",
      "Iteration 17813, loss = 5.73725775\n",
      "Iteration 17814, loss = 5.89870368\n",
      "Iteration 17815, loss = 5.88296340\n",
      "Iteration 17816, loss = 5.81080807\n",
      "Iteration 17817, loss = 5.89205352\n",
      "Iteration 17818, loss = 5.63005060\n",
      "Iteration 17819, loss = 6.25531838\n",
      "Iteration 17820, loss = 6.29782996\n",
      "Iteration 17821, loss = 6.99944980\n",
      "Iteration 17822, loss = 6.44871341\n",
      "Iteration 17823, loss = 6.17311301\n",
      "Iteration 17824, loss = 6.59195665\n",
      "Iteration 17825, loss = 6.25505421\n",
      "Iteration 17826, loss = 5.90865880\n",
      "Iteration 17827, loss = 6.03815058\n",
      "Iteration 17828, loss = 5.97038307\n",
      "Iteration 17829, loss = 5.82510284\n",
      "Iteration 17830, loss = 6.76892104\n",
      "Iteration 17831, loss = 6.82539242\n",
      "Iteration 17832, loss = 6.30964798\n",
      "Iteration 17833, loss = 5.94292667\n",
      "Iteration 17834, loss = 6.87476559\n",
      "Iteration 17835, loss = 6.59900499\n",
      "Iteration 17836, loss = 6.56250851\n",
      "Iteration 17837, loss = 5.84482259\n",
      "Iteration 17838, loss = 6.12151233\n",
      "Iteration 17839, loss = 6.26790571\n",
      "Iteration 17840, loss = 6.70956251\n",
      "Iteration 17841, loss = 6.37478287\n",
      "Iteration 17842, loss = 6.06368041\n",
      "Iteration 17843, loss = 6.38735539\n",
      "Iteration 17844, loss = 5.88865833\n",
      "Iteration 17845, loss = 5.73200367\n",
      "Iteration 17846, loss = 5.98658621\n",
      "Iteration 17847, loss = 6.28061456\n",
      "Iteration 17848, loss = 6.29313283\n",
      "Iteration 17849, loss = 6.97913314\n",
      "Iteration 17850, loss = 6.75930748\n",
      "Iteration 17851, loss = 6.06890806\n",
      "Iteration 17852, loss = 6.30492896\n",
      "Iteration 17853, loss = 6.09932718\n",
      "Iteration 17854, loss = 6.03375555\n",
      "Iteration 17855, loss = 6.25936750\n",
      "Iteration 17856, loss = 6.27803340\n",
      "Iteration 17857, loss = 5.88160743\n",
      "Iteration 17858, loss = 6.05221903\n",
      "Iteration 17859, loss = 5.88634507\n",
      "Iteration 17860, loss = 5.75822793\n",
      "Iteration 17861, loss = 5.77236944\n",
      "Iteration 17862, loss = 5.70904303\n",
      "Iteration 17863, loss = 5.98410780\n",
      "Iteration 17864, loss = 6.79678888\n",
      "Iteration 17865, loss = 7.17406614\n",
      "Iteration 17866, loss = 5.96887004\n",
      "Iteration 17867, loss = 6.95505035\n",
      "Iteration 17868, loss = 6.93077690\n",
      "Iteration 17869, loss = 7.68610910\n",
      "Iteration 17870, loss = 7.83188802\n",
      "Iteration 17871, loss = 7.17815828\n",
      "Iteration 17872, loss = 6.38440969\n",
      "Iteration 17873, loss = 6.11927519\n",
      "Iteration 17874, loss = 6.27723139\n",
      "Iteration 17875, loss = 5.80338301\n",
      "Iteration 17876, loss = 6.36300801\n",
      "Iteration 17877, loss = 6.15700230\n",
      "Iteration 17878, loss = 5.91251974\n",
      "Iteration 17879, loss = 5.90135007\n",
      "Iteration 17880, loss = 5.99247130\n",
      "Iteration 17881, loss = 6.04741645\n",
      "Iteration 17882, loss = 5.76364394\n",
      "Iteration 17883, loss = 5.86037596\n",
      "Iteration 17884, loss = 6.35225737\n",
      "Iteration 17885, loss = 5.92334532\n",
      "Iteration 17886, loss = 6.49250250\n",
      "Iteration 17887, loss = 6.24231251\n",
      "Iteration 17888, loss = 6.29966899\n",
      "Iteration 17889, loss = 6.47692609\n",
      "Iteration 17890, loss = 7.29398122\n",
      "Iteration 17891, loss = 7.02147433\n",
      "Iteration 17892, loss = 8.20199557\n",
      "Iteration 17893, loss = 6.85095584\n",
      "Iteration 17894, loss = 5.83697039\n",
      "Iteration 17895, loss = 6.41019547\n",
      "Iteration 17896, loss = 5.81040007\n",
      "Iteration 17897, loss = 8.42008022\n",
      "Iteration 17898, loss = 6.96313527\n",
      "Iteration 17899, loss = 7.57426603\n",
      "Iteration 17900, loss = 6.27017012\n",
      "Iteration 17901, loss = 6.09089946\n",
      "Iteration 17902, loss = 6.41461963\n",
      "Iteration 17903, loss = 6.18439817\n",
      "Iteration 17904, loss = 5.90679904\n",
      "Iteration 17905, loss = 6.19662244\n",
      "Iteration 17906, loss = 5.83668452\n",
      "Iteration 17907, loss = 5.81159991\n",
      "Iteration 17908, loss = 6.31026135\n",
      "Iteration 17909, loss = 6.50038560\n",
      "Iteration 17910, loss = 6.85271230\n",
      "Iteration 17911, loss = 7.31289168\n",
      "Iteration 17912, loss = 7.10690735\n",
      "Iteration 17913, loss = 7.58022582\n",
      "Iteration 17914, loss = 6.66674251\n",
      "Iteration 17915, loss = 6.25020255\n",
      "Iteration 17916, loss = 7.10655526\n",
      "Iteration 17917, loss = 6.20586916\n",
      "Iteration 17918, loss = 6.42840034\n",
      "Iteration 17919, loss = 7.02327187\n",
      "Iteration 17920, loss = 6.70822081\n",
      "Iteration 17921, loss = 6.52411300\n",
      "Iteration 17922, loss = 7.01922917\n",
      "Iteration 17923, loss = 6.34579048\n",
      "Iteration 17924, loss = 6.71228070\n",
      "Iteration 17925, loss = 5.75202883\n",
      "Iteration 17926, loss = 5.73921451\n",
      "Iteration 17927, loss = 6.41223273\n",
      "Iteration 17928, loss = 6.22383044\n",
      "Iteration 17929, loss = 6.12457977\n",
      "Iteration 17930, loss = 6.64406234\n",
      "Iteration 17931, loss = 6.52505456\n",
      "Iteration 17932, loss = 6.56207217\n",
      "Iteration 17933, loss = 5.83654316\n",
      "Iteration 17934, loss = 6.33298014\n",
      "Iteration 17935, loss = 6.88391655\n",
      "Iteration 17936, loss = 6.46951648\n",
      "Iteration 17937, loss = 6.28489265\n",
      "Iteration 17938, loss = 6.56023289\n",
      "Iteration 17939, loss = 6.14926426\n",
      "Iteration 17940, loss = 6.44757023\n",
      "Iteration 17941, loss = 6.15575822\n",
      "Iteration 17942, loss = 5.73425914\n",
      "Iteration 17943, loss = 6.09116214\n",
      "Iteration 17944, loss = 6.56811298\n",
      "Iteration 17945, loss = 6.04519948\n",
      "Iteration 17946, loss = 6.78732944\n",
      "Iteration 17947, loss = 6.84875048\n",
      "Iteration 17948, loss = 6.49753033\n",
      "Iteration 17949, loss = 6.38243547\n",
      "Iteration 17950, loss = 6.07961404\n",
      "Iteration 17951, loss = 6.58053158\n",
      "Iteration 17952, loss = 6.05743059\n",
      "Iteration 17953, loss = 5.87008176\n",
      "Iteration 17954, loss = 6.27827833\n",
      "Iteration 17955, loss = 6.18653472\n",
      "Iteration 17956, loss = 6.19658391\n",
      "Iteration 17957, loss = 5.98880618\n",
      "Iteration 17958, loss = 5.82016710\n",
      "Iteration 17959, loss = 5.85813306\n",
      "Iteration 17960, loss = 5.70219417\n",
      "Iteration 17961, loss = 5.93225349\n",
      "Iteration 17962, loss = 6.28977481\n",
      "Iteration 17963, loss = 6.39532262\n",
      "Iteration 17964, loss = 6.20037749\n",
      "Iteration 17965, loss = 6.41838881\n",
      "Iteration 17966, loss = 6.19743932\n",
      "Iteration 17967, loss = 6.12735010\n",
      "Iteration 17968, loss = 6.65362850\n",
      "Iteration 17969, loss = 5.93734230\n",
      "Iteration 17970, loss = 5.65934801\n",
      "Iteration 17971, loss = 6.41536070\n",
      "Iteration 17972, loss = 5.72216679\n",
      "Iteration 17973, loss = 6.63128507\n",
      "Iteration 17974, loss = 5.96397575\n",
      "Iteration 17975, loss = 6.32134500\n",
      "Iteration 17976, loss = 6.68137040\n",
      "Iteration 17977, loss = 7.19351198\n",
      "Iteration 17978, loss = 6.30205738\n",
      "Iteration 17979, loss = 5.52028029\n",
      "Iteration 17980, loss = 7.06395155\n",
      "Iteration 17981, loss = 6.39225477\n",
      "Iteration 17982, loss = 7.03690206\n",
      "Iteration 17983, loss = 6.97227383\n",
      "Iteration 17984, loss = 6.98848511\n",
      "Iteration 17985, loss = 6.53434545\n",
      "Iteration 17986, loss = 7.08023098\n",
      "Iteration 17987, loss = 6.87742322\n",
      "Iteration 17988, loss = 7.50191410\n",
      "Iteration 17989, loss = 7.06451790\n",
      "Iteration 17990, loss = 7.39370775\n",
      "Iteration 17991, loss = 6.41965961\n",
      "Iteration 17992, loss = 6.39382005\n",
      "Iteration 17993, loss = 6.62127592\n",
      "Iteration 17994, loss = 6.75102230\n",
      "Iteration 17995, loss = 5.70859266\n",
      "Iteration 17996, loss = 5.84816865\n",
      "Iteration 17997, loss = 5.75871244\n",
      "Iteration 17998, loss = 6.26191487\n",
      "Iteration 17999, loss = 6.09943884\n",
      "Iteration 18000, loss = 5.78341875\n",
      "Iteration 18001, loss = 5.76380012\n",
      "Iteration 18002, loss = 5.89109891\n",
      "Iteration 18003, loss = 6.10163625\n",
      "Iteration 18004, loss = 6.31771236\n",
      "Iteration 18005, loss = 6.14097825\n",
      "Iteration 18006, loss = 6.57197013\n",
      "Iteration 18007, loss = 6.31593846\n",
      "Iteration 18008, loss = 6.20719859\n",
      "Iteration 18009, loss = 6.03396878\n",
      "Iteration 18010, loss = 5.80571886\n",
      "Iteration 18011, loss = 6.27695648\n",
      "Iteration 18012, loss = 6.04082676\n",
      "Iteration 18013, loss = 5.85533908\n",
      "Iteration 18014, loss = 5.98257768\n",
      "Iteration 18015, loss = 6.46889493\n",
      "Iteration 18016, loss = 7.37717776\n",
      "Iteration 18017, loss = 7.65200801\n",
      "Iteration 18018, loss = 6.93213202\n",
      "Iteration 18019, loss = 6.28795210\n",
      "Iteration 18020, loss = 6.70513980\n",
      "Iteration 18021, loss = 6.37482341\n",
      "Iteration 18022, loss = 6.58409977\n",
      "Iteration 18023, loss = 5.95678261\n",
      "Iteration 18024, loss = 5.62259035\n",
      "Iteration 18025, loss = 5.79411614\n",
      "Iteration 18026, loss = 6.05790037\n",
      "Iteration 18027, loss = 6.59056868\n",
      "Iteration 18028, loss = 6.63096025\n",
      "Iteration 18029, loss = 6.67246508\n",
      "Iteration 18030, loss = 6.05942791\n",
      "Iteration 18031, loss = 5.56390207\n",
      "Iteration 18032, loss = 5.56514819\n",
      "Iteration 18033, loss = 5.52305659\n",
      "Iteration 18034, loss = 5.68987524\n",
      "Iteration 18035, loss = 5.65069205\n",
      "Iteration 18036, loss = 5.96821886\n",
      "Iteration 18037, loss = 5.92922977\n",
      "Iteration 18038, loss = 5.86112257\n",
      "Iteration 18039, loss = 6.52443632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18040, loss = 5.76412205\n",
      "Iteration 18041, loss = 6.11391187\n",
      "Iteration 18042, loss = 6.10018657\n",
      "Iteration 18043, loss = 6.21091388\n",
      "Iteration 18044, loss = 6.51497834\n",
      "Iteration 18045, loss = 6.53960524\n",
      "Iteration 18046, loss = 6.01951544\n",
      "Iteration 18047, loss = 6.18108428\n",
      "Iteration 18048, loss = 5.92540850\n",
      "Iteration 18049, loss = 6.03673092\n",
      "Iteration 18050, loss = 6.73810854\n",
      "Iteration 18051, loss = 6.60130082\n",
      "Iteration 18052, loss = 6.30463261\n",
      "Iteration 18053, loss = 6.11031559\n",
      "Iteration 18054, loss = 5.85727501\n",
      "Iteration 18055, loss = 5.74685855\n",
      "Iteration 18056, loss = 5.66892779\n",
      "Iteration 18057, loss = 5.85426164\n",
      "Iteration 18058, loss = 5.97027770\n",
      "Iteration 18059, loss = 5.81275855\n",
      "Iteration 18060, loss = 6.41545368\n",
      "Iteration 18061, loss = 6.95378285\n",
      "Iteration 18062, loss = 6.57438828\n",
      "Iteration 18063, loss = 6.31841437\n",
      "Iteration 18064, loss = 6.36070705\n",
      "Iteration 18065, loss = 6.38689829\n",
      "Iteration 18066, loss = 6.21114463\n",
      "Iteration 18067, loss = 5.93185473\n",
      "Iteration 18068, loss = 7.92320686\n",
      "Iteration 18069, loss = 6.45955829\n",
      "Iteration 18070, loss = 6.64501368\n",
      "Iteration 18071, loss = 6.34773572\n",
      "Iteration 18072, loss = 6.04870658\n",
      "Iteration 18073, loss = 6.08097884\n",
      "Iteration 18074, loss = 6.92162817\n",
      "Iteration 18075, loss = 6.37336798\n",
      "Iteration 18076, loss = 7.67696927\n",
      "Iteration 18077, loss = 8.66349963\n",
      "Iteration 18078, loss = 8.23267727\n",
      "Iteration 18079, loss = 7.32259059\n",
      "Iteration 18080, loss = 6.65081112\n",
      "Iteration 18081, loss = 6.81185697\n",
      "Iteration 18082, loss = 5.88275699\n",
      "Iteration 18083, loss = 7.05443926\n",
      "Iteration 18084, loss = 6.41955424\n",
      "Iteration 18085, loss = 6.02099507\n",
      "Iteration 18086, loss = 6.05730129\n",
      "Iteration 18087, loss = 6.12801731\n",
      "Iteration 18088, loss = 5.91848452\n",
      "Iteration 18089, loss = 6.54032728\n",
      "Iteration 18090, loss = 6.42484471\n",
      "Iteration 18091, loss = 6.63333502\n",
      "Iteration 18092, loss = 6.83892867\n",
      "Iteration 18093, loss = 6.86034475\n",
      "Iteration 18094, loss = 6.68720026\n",
      "Iteration 18095, loss = 6.47235173\n",
      "Iteration 18096, loss = 6.69023404\n",
      "Iteration 18097, loss = 7.13659284\n",
      "Iteration 18098, loss = 7.48384257\n",
      "Iteration 18099, loss = 8.70012341\n",
      "Iteration 18100, loss = 7.86353599\n",
      "Iteration 18101, loss = 6.93182044\n",
      "Iteration 18102, loss = 6.32236641\n",
      "Iteration 18103, loss = 5.78992483\n",
      "Iteration 18104, loss = 5.81696504\n",
      "Iteration 18105, loss = 5.60335566\n",
      "Iteration 18106, loss = 5.96625290\n",
      "Iteration 18107, loss = 5.85384644\n",
      "Iteration 18108, loss = 6.13471303\n",
      "Iteration 18109, loss = 7.47202641\n",
      "Iteration 18110, loss = 6.74525166\n",
      "Iteration 18111, loss = 6.97108009\n",
      "Iteration 18112, loss = 6.52300209\n",
      "Iteration 18113, loss = 6.22309974\n",
      "Iteration 18114, loss = 6.44555084\n",
      "Iteration 18115, loss = 6.33015325\n",
      "Iteration 18116, loss = 7.06673306\n",
      "Iteration 18117, loss = 7.16880835\n",
      "Iteration 18118, loss = 6.83747491\n",
      "Iteration 18119, loss = 6.40703895\n",
      "Iteration 18120, loss = 6.20980360\n",
      "Iteration 18121, loss = 6.22014730\n",
      "Iteration 18122, loss = 6.45785068\n",
      "Iteration 18123, loss = 6.94596315\n",
      "Iteration 18124, loss = 7.25267042\n",
      "Iteration 18125, loss = 6.60180742\n",
      "Iteration 18126, loss = 7.07632262\n",
      "Iteration 18127, loss = 6.58003896\n",
      "Iteration 18128, loss = 6.32736125\n",
      "Iteration 18129, loss = 6.08481883\n",
      "Iteration 18130, loss = 6.03055025\n",
      "Iteration 18131, loss = 6.39593110\n",
      "Iteration 18132, loss = 6.71661630\n",
      "Iteration 18133, loss = 6.43615831\n",
      "Iteration 18134, loss = 7.98820418\n",
      "Iteration 18135, loss = 8.51142077\n",
      "Iteration 18136, loss = 7.22779868\n",
      "Iteration 18137, loss = 6.12132855\n",
      "Iteration 18138, loss = 5.72355993\n",
      "Iteration 18139, loss = 6.19350555\n",
      "Iteration 18140, loss = 6.23760703\n",
      "Iteration 18141, loss = 5.55205368\n",
      "Iteration 18142, loss = 5.69737779\n",
      "Iteration 18143, loss = 5.56631317\n",
      "Iteration 18144, loss = 5.81773903\n",
      "Iteration 18145, loss = 5.90756647\n",
      "Iteration 18146, loss = 6.67345975\n",
      "Iteration 18147, loss = 5.76493340\n",
      "Iteration 18148, loss = 5.81956387\n",
      "Iteration 18149, loss = 6.14017214\n",
      "Iteration 18150, loss = 5.64201338\n",
      "Iteration 18151, loss = 5.81004283\n",
      "Iteration 18152, loss = 6.79498022\n",
      "Iteration 18153, loss = 5.74622957\n",
      "Iteration 18154, loss = 6.02402420\n",
      "Iteration 18155, loss = 7.18898880\n",
      "Iteration 18156, loss = 7.18414950\n",
      "Iteration 18157, loss = 6.68441414\n",
      "Iteration 18158, loss = 5.93752047\n",
      "Iteration 18159, loss = 5.82473433\n",
      "Iteration 18160, loss = 6.44174758\n",
      "Iteration 18161, loss = 5.64689692\n",
      "Iteration 18162, loss = 5.82524301\n",
      "Iteration 18163, loss = 6.36459455\n",
      "Iteration 18164, loss = 5.98432123\n",
      "Iteration 18165, loss = 5.98340679\n",
      "Iteration 18166, loss = 5.87788266\n",
      "Iteration 18167, loss = 6.03587761\n",
      "Iteration 18168, loss = 6.06935810\n",
      "Iteration 18169, loss = 6.20954756\n",
      "Iteration 18170, loss = 6.11807839\n",
      "Iteration 18171, loss = 7.14260410\n",
      "Iteration 18172, loss = 7.06044092\n",
      "Iteration 18173, loss = 6.39571451\n",
      "Iteration 18174, loss = 6.17567874\n",
      "Iteration 18175, loss = 6.83876639\n",
      "Iteration 18176, loss = 6.50270268\n",
      "Iteration 18177, loss = 6.65148200\n",
      "Iteration 18178, loss = 5.96153610\n",
      "Iteration 18179, loss = 6.04329089\n",
      "Iteration 18180, loss = 5.99612731\n",
      "Iteration 18181, loss = 6.70096754\n",
      "Iteration 18182, loss = 6.26642542\n",
      "Iteration 18183, loss = 6.17029502\n",
      "Iteration 18184, loss = 6.44757133\n",
      "Iteration 18185, loss = 6.85186984\n",
      "Iteration 18186, loss = 7.80854915\n",
      "Iteration 18187, loss = 7.44163492\n",
      "Iteration 18188, loss = 7.52381541\n",
      "Iteration 18189, loss = 6.66340133\n",
      "Iteration 18190, loss = 7.61289581\n",
      "Iteration 18191, loss = 5.96008890\n",
      "Iteration 18192, loss = 5.85365398\n",
      "Iteration 18193, loss = 5.84038176\n",
      "Iteration 18194, loss = 5.59659957\n",
      "Iteration 18195, loss = 5.96880805\n",
      "Iteration 18196, loss = 6.77762495\n",
      "Iteration 18197, loss = 6.34249769\n",
      "Iteration 18198, loss = 6.35226170\n",
      "Iteration 18199, loss = 6.02922349\n",
      "Iteration 18200, loss = 6.20348944\n",
      "Iteration 18201, loss = 5.46040938\n",
      "Iteration 18202, loss = 6.36034676\n",
      "Iteration 18203, loss = 6.44763317\n",
      "Iteration 18204, loss = 6.52359738\n",
      "Iteration 18205, loss = 6.67258697\n",
      "Iteration 18206, loss = 6.21003629\n",
      "Iteration 18207, loss = 6.86924517\n",
      "Iteration 18208, loss = 6.95792295\n",
      "Iteration 18209, loss = 8.06303560\n",
      "Iteration 18210, loss = 8.09701872\n",
      "Iteration 18211, loss = 7.88481574\n",
      "Iteration 18212, loss = 8.26020502\n",
      "Iteration 18213, loss = 6.80325662\n",
      "Iteration 18214, loss = 7.80144837\n",
      "Iteration 18215, loss = 6.95911818\n",
      "Iteration 18216, loss = 6.67016561\n",
      "Iteration 18217, loss = 6.17646119\n",
      "Iteration 18218, loss = 5.92699209\n",
      "Iteration 18219, loss = 5.71153990\n",
      "Iteration 18220, loss = 5.60882969\n",
      "Iteration 18221, loss = 5.47986945\n",
      "Iteration 18222, loss = 5.81422112\n",
      "Iteration 18223, loss = 5.76000139\n",
      "Iteration 18224, loss = 5.54544812\n",
      "Iteration 18225, loss = 5.92974973\n",
      "Iteration 18226, loss = 5.97899314\n",
      "Iteration 18227, loss = 6.76390970\n",
      "Iteration 18228, loss = 5.99258806\n",
      "Iteration 18229, loss = 6.34016355\n",
      "Iteration 18230, loss = 6.29824646\n",
      "Iteration 18231, loss = 8.99623435\n",
      "Iteration 18232, loss = 7.67905727\n",
      "Iteration 18233, loss = 7.46987494\n",
      "Iteration 18234, loss = 6.00043765\n",
      "Iteration 18235, loss = 5.60770666\n",
      "Iteration 18236, loss = 5.76741613\n",
      "Iteration 18237, loss = 5.72266528\n",
      "Iteration 18238, loss = 5.64647226\n",
      "Iteration 18239, loss = 5.94402577\n",
      "Iteration 18240, loss = 5.58347791\n",
      "Iteration 18241, loss = 5.61106134\n",
      "Iteration 18242, loss = 6.25204278\n",
      "Iteration 18243, loss = 7.08390567\n",
      "Iteration 18244, loss = 6.13683569\n",
      "Iteration 18245, loss = 5.95160662\n",
      "Iteration 18246, loss = 6.25340242\n",
      "Iteration 18247, loss = 6.26942746\n",
      "Iteration 18248, loss = 5.68980651\n",
      "Iteration 18249, loss = 5.47225882\n",
      "Iteration 18250, loss = 5.56851484\n",
      "Iteration 18251, loss = 6.40398461\n",
      "Iteration 18252, loss = 6.11982602\n",
      "Iteration 18253, loss = 6.14783304\n",
      "Iteration 18254, loss = 6.44486284\n",
      "Iteration 18255, loss = 6.69230645\n",
      "Iteration 18256, loss = 6.25962811\n",
      "Iteration 18257, loss = 6.49256727\n",
      "Iteration 18258, loss = 6.47765793\n",
      "Iteration 18259, loss = 5.86620452\n",
      "Iteration 18260, loss = 6.42073894\n",
      "Iteration 18261, loss = 6.30489483\n",
      "Iteration 18262, loss = 5.82044783\n",
      "Iteration 18263, loss = 5.83038384\n",
      "Iteration 18264, loss = 6.06143189\n",
      "Iteration 18265, loss = 6.78909853\n",
      "Iteration 18266, loss = 6.62228734\n",
      "Iteration 18267, loss = 5.88466161\n",
      "Iteration 18268, loss = 5.93465676\n",
      "Iteration 18269, loss = 5.87433327\n",
      "Iteration 18270, loss = 5.94524603\n",
      "Iteration 18271, loss = 5.73377871\n",
      "Iteration 18272, loss = 6.05516639\n",
      "Iteration 18273, loss = 5.73995709\n",
      "Iteration 18274, loss = 6.18739856\n",
      "Iteration 18275, loss = 6.28445321\n",
      "Iteration 18276, loss = 5.61955139\n",
      "Iteration 18277, loss = 5.71513421\n",
      "Iteration 18278, loss = 5.52932863\n",
      "Iteration 18279, loss = 6.00038879\n",
      "Iteration 18280, loss = 5.70790121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18281, loss = 6.56158741\n",
      "Iteration 18282, loss = 5.79939228\n",
      "Iteration 18283, loss = 5.58780885\n",
      "Iteration 18284, loss = 5.97303198\n",
      "Iteration 18285, loss = 5.98425366\n",
      "Iteration 18286, loss = 6.84145974\n",
      "Iteration 18287, loss = 6.91984279\n",
      "Iteration 18288, loss = 7.33086611\n",
      "Iteration 18289, loss = 7.30570900\n",
      "Iteration 18290, loss = 6.24402572\n",
      "Iteration 18291, loss = 7.58825899\n",
      "Iteration 18292, loss = 6.20854697\n",
      "Iteration 18293, loss = 6.36994593\n",
      "Iteration 18294, loss = 5.80622132\n",
      "Iteration 18295, loss = 5.85856166\n",
      "Iteration 18296, loss = 5.83920074\n",
      "Iteration 18297, loss = 6.47540514\n",
      "Iteration 18298, loss = 6.15134867\n",
      "Iteration 18299, loss = 6.13533395\n",
      "Iteration 18300, loss = 6.40816365\n",
      "Iteration 18301, loss = 6.31241299\n",
      "Iteration 18302, loss = 6.35743315\n",
      "Iteration 18303, loss = 6.80048585\n",
      "Iteration 18304, loss = 6.49633234\n",
      "Iteration 18305, loss = 6.25306590\n",
      "Iteration 18306, loss = 5.85341827\n",
      "Iteration 18307, loss = 6.07021118\n",
      "Iteration 18308, loss = 6.19719760\n",
      "Iteration 18309, loss = 5.66560192\n",
      "Iteration 18310, loss = 5.91269447\n",
      "Iteration 18311, loss = 7.20920880\n",
      "Iteration 18312, loss = 7.06566082\n",
      "Iteration 18313, loss = 7.90476778\n",
      "Iteration 18314, loss = 6.92466129\n",
      "Iteration 18315, loss = 7.00551764\n",
      "Iteration 18316, loss = 6.59667235\n",
      "Iteration 18317, loss = 6.99588398\n",
      "Iteration 18318, loss = 6.48569188\n",
      "Iteration 18319, loss = 6.21735452\n",
      "Iteration 18320, loss = 5.89407532\n",
      "Iteration 18321, loss = 5.71249949\n",
      "Iteration 18322, loss = 6.05373238\n",
      "Iteration 18323, loss = 5.94839010\n",
      "Iteration 18324, loss = 5.54597979\n",
      "Iteration 18325, loss = 5.93249850\n",
      "Iteration 18326, loss = 6.60611659\n",
      "Iteration 18327, loss = 6.04696703\n",
      "Iteration 18328, loss = 6.27241848\n",
      "Iteration 18329, loss = 5.83415449\n",
      "Iteration 18330, loss = 6.31323614\n",
      "Iteration 18331, loss = 5.72473635\n",
      "Iteration 18332, loss = 5.71168093\n",
      "Iteration 18333, loss = 5.87256329\n",
      "Iteration 18334, loss = 6.37409712\n",
      "Iteration 18335, loss = 6.24496230\n",
      "Iteration 18336, loss = 6.31764464\n",
      "Iteration 18337, loss = 7.29008977\n",
      "Iteration 18338, loss = 7.70522152\n",
      "Iteration 18339, loss = 8.08220764\n",
      "Iteration 18340, loss = 6.28997392\n",
      "Iteration 18341, loss = 5.79986913\n",
      "Iteration 18342, loss = 5.96174352\n",
      "Iteration 18343, loss = 6.17735301\n",
      "Iteration 18344, loss = 6.24651670\n",
      "Iteration 18345, loss = 6.33013999\n",
      "Iteration 18346, loss = 6.27578480\n",
      "Iteration 18347, loss = 6.38517080\n",
      "Iteration 18348, loss = 5.90850046\n",
      "Iteration 18349, loss = 5.88118641\n",
      "Iteration 18350, loss = 5.80693100\n",
      "Iteration 18351, loss = 6.02912245\n",
      "Iteration 18352, loss = 6.67600137\n",
      "Iteration 18353, loss = 6.94657809\n",
      "Iteration 18354, loss = 7.11972041\n",
      "Iteration 18355, loss = 6.74286676\n",
      "Iteration 18356, loss = 6.11501739\n",
      "Iteration 18357, loss = 5.67110868\n",
      "Iteration 18358, loss = 5.70212575\n",
      "Iteration 18359, loss = 5.58550312\n",
      "Iteration 18360, loss = 5.45673283\n",
      "Iteration 18361, loss = 5.55749915\n",
      "Iteration 18362, loss = 5.78010408\n",
      "Iteration 18363, loss = 5.45146606\n",
      "Iteration 18364, loss = 5.63888737\n",
      "Iteration 18365, loss = 5.60063647\n",
      "Iteration 18366, loss = 5.63200614\n",
      "Iteration 18367, loss = 5.68160511\n",
      "Iteration 18368, loss = 6.12727913\n",
      "Iteration 18369, loss = 6.20639045\n",
      "Iteration 18370, loss = 6.19910768\n",
      "Iteration 18371, loss = 6.11636129\n",
      "Iteration 18372, loss = 5.84330393\n",
      "Iteration 18373, loss = 5.87977150\n",
      "Iteration 18374, loss = 5.56834013\n",
      "Iteration 18375, loss = 5.80281849\n",
      "Iteration 18376, loss = 6.53462365\n",
      "Iteration 18377, loss = 7.02499813\n",
      "Iteration 18378, loss = 6.79872987\n",
      "Iteration 18379, loss = 6.01136587\n",
      "Iteration 18380, loss = 5.92502412\n",
      "Iteration 18381, loss = 5.98995131\n",
      "Iteration 18382, loss = 5.90755078\n",
      "Iteration 18383, loss = 6.20757823\n",
      "Iteration 18384, loss = 6.54106486\n",
      "Iteration 18385, loss = 6.41482955\n",
      "Iteration 18386, loss = 6.21960715\n",
      "Iteration 18387, loss = 6.40270035\n",
      "Iteration 18388, loss = 6.65389535\n",
      "Iteration 18389, loss = 5.83393884\n",
      "Iteration 18390, loss = 6.82185996\n",
      "Iteration 18391, loss = 6.32097590\n",
      "Iteration 18392, loss = 5.99463023\n",
      "Iteration 18393, loss = 6.09460094\n",
      "Iteration 18394, loss = 6.05924930\n",
      "Iteration 18395, loss = 5.64224633\n",
      "Iteration 18396, loss = 5.78911956\n",
      "Iteration 18397, loss = 6.17111731\n",
      "Iteration 18398, loss = 6.33277821\n",
      "Iteration 18399, loss = 6.14926615\n",
      "Iteration 18400, loss = 7.09932354\n",
      "Iteration 18401, loss = 6.29799771\n",
      "Iteration 18402, loss = 5.78600007\n",
      "Iteration 18403, loss = 5.58407401\n",
      "Iteration 18404, loss = 5.56200975\n",
      "Iteration 18405, loss = 5.72967538\n",
      "Iteration 18406, loss = 5.67883118\n",
      "Iteration 18407, loss = 6.03135570\n",
      "Iteration 18408, loss = 5.56398798\n",
      "Iteration 18409, loss = 6.07693555\n",
      "Iteration 18410, loss = 5.74055647\n",
      "Iteration 18411, loss = 5.72493636\n",
      "Iteration 18412, loss = 6.62485060\n",
      "Iteration 18413, loss = 5.80411886\n",
      "Iteration 18414, loss = 6.95651878\n",
      "Iteration 18415, loss = 6.84076346\n",
      "Iteration 18416, loss = 6.38091193\n",
      "Iteration 18417, loss = 6.33946682\n",
      "Iteration 18418, loss = 6.01636183\n",
      "Iteration 18419, loss = 6.52085672\n",
      "Iteration 18420, loss = 6.73141755\n",
      "Iteration 18421, loss = 6.23916803\n",
      "Iteration 18422, loss = 5.82003243\n",
      "Iteration 18423, loss = 6.53077331\n",
      "Iteration 18424, loss = 5.80476479\n",
      "Iteration 18425, loss = 5.89064331\n",
      "Iteration 18426, loss = 5.94810971\n",
      "Iteration 18427, loss = 5.68186710\n",
      "Iteration 18428, loss = 5.47998948\n",
      "Iteration 18429, loss = 6.13859465\n",
      "Iteration 18430, loss = 6.45705763\n",
      "Iteration 18431, loss = 6.83738543\n",
      "Iteration 18432, loss = 6.03016884\n",
      "Iteration 18433, loss = 6.23271661\n",
      "Iteration 18434, loss = 6.02563918\n",
      "Iteration 18435, loss = 7.17525732\n",
      "Iteration 18436, loss = 6.90037937\n",
      "Iteration 18437, loss = 8.68324543\n",
      "Iteration 18438, loss = 6.52850797\n",
      "Iteration 18439, loss = 6.28667292\n",
      "Iteration 18440, loss = 5.67826489\n",
      "Iteration 18441, loss = 5.57860113\n",
      "Iteration 18442, loss = 5.74109772\n",
      "Iteration 18443, loss = 6.18555571\n",
      "Iteration 18444, loss = 5.75463659\n",
      "Iteration 18445, loss = 6.77553076\n",
      "Iteration 18446, loss = 6.51282905\n",
      "Iteration 18447, loss = 6.80271748\n",
      "Iteration 18448, loss = 6.42563873\n",
      "Iteration 18449, loss = 5.92148725\n",
      "Iteration 18450, loss = 6.06438698\n",
      "Iteration 18451, loss = 5.85239455\n",
      "Iteration 18452, loss = 7.09533648\n",
      "Iteration 18453, loss = 5.82899772\n",
      "Iteration 18454, loss = 6.99583189\n",
      "Iteration 18455, loss = 5.81013537\n",
      "Iteration 18456, loss = 5.73304179\n",
      "Iteration 18457, loss = 5.94783657\n",
      "Iteration 18458, loss = 5.86135858\n",
      "Iteration 18459, loss = 5.96077979\n",
      "Iteration 18460, loss = 6.77293708\n",
      "Iteration 18461, loss = 6.69717695\n",
      "Iteration 18462, loss = 6.93296344\n",
      "Iteration 18463, loss = 5.90363997\n",
      "Iteration 18464, loss = 6.01953701\n",
      "Iteration 18465, loss = 5.76591916\n",
      "Iteration 18466, loss = 6.41006069\n",
      "Iteration 18467, loss = 7.31436081\n",
      "Iteration 18468, loss = 6.28688324\n",
      "Iteration 18469, loss = 6.16822105\n",
      "Iteration 18470, loss = 6.55111016\n",
      "Iteration 18471, loss = 6.23395344\n",
      "Iteration 18472, loss = 5.91379336\n",
      "Iteration 18473, loss = 6.58075981\n",
      "Iteration 18474, loss = 6.11854243\n",
      "Iteration 18475, loss = 6.66632160\n",
      "Iteration 18476, loss = 7.51724150\n",
      "Iteration 18477, loss = 6.96530763\n",
      "Iteration 18478, loss = 6.37054212\n",
      "Iteration 18479, loss = 5.88174990\n",
      "Iteration 18480, loss = 5.75621125\n",
      "Iteration 18481, loss = 5.72560633\n",
      "Iteration 18482, loss = 6.23739092\n",
      "Iteration 18483, loss = 6.94181227\n",
      "Iteration 18484, loss = 6.47796637\n",
      "Iteration 18485, loss = 5.76062618\n",
      "Iteration 18486, loss = 6.03516926\n",
      "Iteration 18487, loss = 6.37679921\n",
      "Iteration 18488, loss = 6.00668367\n",
      "Iteration 18489, loss = 5.89704953\n",
      "Iteration 18490, loss = 5.68397564\n",
      "Iteration 18491, loss = 5.87421408\n",
      "Iteration 18492, loss = 5.65613055\n",
      "Iteration 18493, loss = 5.50454635\n",
      "Iteration 18494, loss = 5.50205403\n",
      "Iteration 18495, loss = 5.47400378\n",
      "Iteration 18496, loss = 5.60888102\n",
      "Iteration 18497, loss = 5.50767627\n",
      "Iteration 18498, loss = 5.66638623\n",
      "Iteration 18499, loss = 5.67849701\n",
      "Iteration 18500, loss = 5.46621428\n",
      "Iteration 18501, loss = 5.77269074\n",
      "Iteration 18502, loss = 6.04332724\n",
      "Iteration 18503, loss = 6.27127192\n",
      "Iteration 18504, loss = 5.68033424\n",
      "Iteration 18505, loss = 5.54577199\n",
      "Iteration 18506, loss = 6.56150672\n",
      "Iteration 18507, loss = 5.59589089\n",
      "Iteration 18508, loss = 9.37519317\n",
      "Iteration 18509, loss = 7.12964684\n",
      "Iteration 18510, loss = 6.68492608\n",
      "Iteration 18511, loss = 6.33163233\n",
      "Iteration 18512, loss = 7.41094411\n",
      "Iteration 18513, loss = 6.14908455\n",
      "Iteration 18514, loss = 6.28837641\n",
      "Iteration 18515, loss = 6.13484407\n",
      "Iteration 18516, loss = 6.38160169\n",
      "Iteration 18517, loss = 6.29702259\n",
      "Iteration 18518, loss = 5.74334444\n",
      "Iteration 18519, loss = 6.21891923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18520, loss = 5.95043536\n",
      "Iteration 18521, loss = 6.18122440\n",
      "Iteration 18522, loss = 5.78143029\n",
      "Iteration 18523, loss = 5.42673068\n",
      "Iteration 18524, loss = 5.61038325\n",
      "Iteration 18525, loss = 5.45068682\n",
      "Iteration 18526, loss = 5.63649751\n",
      "Iteration 18527, loss = 6.07057493\n",
      "Iteration 18528, loss = 5.63456074\n",
      "Iteration 18529, loss = 5.51551870\n",
      "Iteration 18530, loss = 5.50061278\n",
      "Iteration 18531, loss = 5.66991674\n",
      "Iteration 18532, loss = 6.04478885\n",
      "Iteration 18533, loss = 6.71688521\n",
      "Iteration 18534, loss = 6.35036793\n",
      "Iteration 18535, loss = 6.45145698\n",
      "Iteration 18536, loss = 6.69033000\n",
      "Iteration 18537, loss = 8.43337387\n",
      "Iteration 18538, loss = 9.01634872\n",
      "Iteration 18539, loss = 7.40593220\n",
      "Iteration 18540, loss = 7.37407480\n",
      "Iteration 18541, loss = 6.72212015\n",
      "Iteration 18542, loss = 6.33894887\n",
      "Iteration 18543, loss = 6.34562468\n",
      "Iteration 18544, loss = 6.14880116\n",
      "Iteration 18545, loss = 5.53876474\n",
      "Iteration 18546, loss = 5.54440950\n",
      "Iteration 18547, loss = 5.90020086\n",
      "Iteration 18548, loss = 7.40073781\n",
      "Iteration 18549, loss = 7.55900005\n",
      "Iteration 18550, loss = 5.74501070\n",
      "Iteration 18551, loss = 5.88937963\n",
      "Iteration 18552, loss = 6.54056989\n",
      "Iteration 18553, loss = 6.39939986\n",
      "Iteration 18554, loss = 6.34962668\n",
      "Iteration 18555, loss = 6.79304570\n",
      "Iteration 18556, loss = 6.86075254\n",
      "Iteration 18557, loss = 6.59209148\n",
      "Iteration 18558, loss = 6.11853407\n",
      "Iteration 18559, loss = 5.79243728\n",
      "Iteration 18560, loss = 5.65535882\n",
      "Iteration 18561, loss = 5.50805057\n",
      "Iteration 18562, loss = 5.56820708\n",
      "Iteration 18563, loss = 5.89060699\n",
      "Iteration 18564, loss = 5.60505985\n",
      "Iteration 18565, loss = 5.66101366\n",
      "Iteration 18566, loss = 5.49842378\n",
      "Iteration 18567, loss = 5.67092508\n",
      "Iteration 18568, loss = 5.61754352\n",
      "Iteration 18569, loss = 5.42363711\n",
      "Iteration 18570, loss = 5.77377875\n",
      "Iteration 18571, loss = 5.75942648\n",
      "Iteration 18572, loss = 5.87330509\n",
      "Iteration 18573, loss = 5.81764289\n",
      "Iteration 18574, loss = 5.68312929\n",
      "Iteration 18575, loss = 5.70411962\n",
      "Iteration 18576, loss = 5.74308834\n",
      "Iteration 18577, loss = 5.90620074\n",
      "Iteration 18578, loss = 5.43653206\n",
      "Iteration 18579, loss = 5.82635914\n",
      "Iteration 18580, loss = 5.54931631\n",
      "Iteration 18581, loss = 5.45403579\n",
      "Iteration 18582, loss = 5.41105583\n",
      "Iteration 18583, loss = 5.61220905\n",
      "Iteration 18584, loss = 5.55289235\n",
      "Iteration 18585, loss = 6.00592536\n",
      "Iteration 18586, loss = 5.80776130\n",
      "Iteration 18587, loss = 5.77093519\n",
      "Iteration 18588, loss = 5.77413844\n",
      "Iteration 18589, loss = 6.03512633\n",
      "Iteration 18590, loss = 5.86161270\n",
      "Iteration 18591, loss = 5.71473146\n",
      "Iteration 18592, loss = 6.51163384\n",
      "Iteration 18593, loss = 6.91322172\n",
      "Iteration 18594, loss = 6.14328239\n",
      "Iteration 18595, loss = 5.94200227\n",
      "Iteration 18596, loss = 5.91935594\n",
      "Iteration 18597, loss = 5.71260397\n",
      "Iteration 18598, loss = 5.60555454\n",
      "Iteration 18599, loss = 6.13284991\n",
      "Iteration 18600, loss = 6.21791281\n",
      "Iteration 18601, loss = 5.58617232\n",
      "Iteration 18602, loss = 6.10518666\n",
      "Iteration 18603, loss = 6.40526591\n",
      "Iteration 18604, loss = 5.93205512\n",
      "Iteration 18605, loss = 6.93092348\n",
      "Iteration 18606, loss = 6.57170757\n",
      "Iteration 18607, loss = 7.88584952\n",
      "Iteration 18608, loss = 7.94801450\n",
      "Iteration 18609, loss = 8.24704761\n",
      "Iteration 18610, loss = 6.92191732\n",
      "Iteration 18611, loss = 5.64675238\n",
      "Iteration 18612, loss = 5.78358867\n",
      "Iteration 18613, loss = 5.76992300\n",
      "Iteration 18614, loss = 5.44478302\n",
      "Iteration 18615, loss = 6.51650233\n",
      "Iteration 18616, loss = 7.63962971\n",
      "Iteration 18617, loss = 6.56226387\n",
      "Iteration 18618, loss = 7.09248485\n",
      "Iteration 18619, loss = 7.19593513\n",
      "Iteration 18620, loss = 6.56110387\n",
      "Iteration 18621, loss = 5.46953892\n",
      "Iteration 18622, loss = 6.47922060\n",
      "Iteration 18623, loss = 6.87579361\n",
      "Iteration 18624, loss = 9.23482396\n",
      "Iteration 18625, loss = 7.10557314\n",
      "Iteration 18626, loss = 6.15676894\n",
      "Iteration 18627, loss = 6.73056251\n",
      "Iteration 18628, loss = 7.71133577\n",
      "Iteration 18629, loss = 7.31288729\n",
      "Iteration 18630, loss = 6.52511837\n",
      "Iteration 18631, loss = 6.85702720\n",
      "Iteration 18632, loss = 7.65721440\n",
      "Iteration 18633, loss = 6.49612042\n",
      "Iteration 18634, loss = 5.66159562\n",
      "Iteration 18635, loss = 5.77178992\n",
      "Iteration 18636, loss = 5.89610810\n",
      "Iteration 18637, loss = 6.05402916\n",
      "Iteration 18638, loss = 5.79483075\n",
      "Iteration 18639, loss = 5.71263701\n",
      "Iteration 18640, loss = 5.73022542\n",
      "Iteration 18641, loss = 5.27160730\n",
      "Iteration 18642, loss = 6.04936742\n",
      "Iteration 18643, loss = 6.09056312\n",
      "Iteration 18644, loss = 6.01699602\n",
      "Iteration 18645, loss = 5.86059328\n",
      "Iteration 18646, loss = 5.49835827\n",
      "Iteration 18647, loss = 5.98996154\n",
      "Iteration 18648, loss = 5.67491329\n",
      "Iteration 18649, loss = 5.74893025\n",
      "Iteration 18650, loss = 6.27300669\n",
      "Iteration 18651, loss = 6.16398568\n",
      "Iteration 18652, loss = 5.80055187\n",
      "Iteration 18653, loss = 5.43426780\n",
      "Iteration 18654, loss = 5.51008950\n",
      "Iteration 18655, loss = 5.45822915\n",
      "Iteration 18656, loss = 5.51168308\n",
      "Iteration 18657, loss = 5.33175332\n",
      "Iteration 18658, loss = 5.44406307\n",
      "Iteration 18659, loss = 5.50686391\n",
      "Iteration 18660, loss = 5.49672511\n",
      "Iteration 18661, loss = 5.73439933\n",
      "Iteration 18662, loss = 5.67908221\n",
      "Iteration 18663, loss = 6.29921456\n",
      "Iteration 18664, loss = 6.34909816\n",
      "Iteration 18665, loss = 6.40627545\n",
      "Iteration 18666, loss = 6.56445182\n",
      "Iteration 18667, loss = 6.93690920\n",
      "Iteration 18668, loss = 5.69546715\n",
      "Iteration 18669, loss = 7.34946937\n",
      "Iteration 18670, loss = 5.52334757\n",
      "Iteration 18671, loss = 6.30755276\n",
      "Iteration 18672, loss = 5.71161139\n",
      "Iteration 18673, loss = 5.43712740\n",
      "Iteration 18674, loss = 5.65992512\n",
      "Iteration 18675, loss = 6.34995865\n",
      "Iteration 18676, loss = 5.65444178\n",
      "Iteration 18677, loss = 5.53084737\n",
      "Iteration 18678, loss = 5.46104308\n",
      "Iteration 18679, loss = 5.65083018\n",
      "Iteration 18680, loss = 5.58538382\n",
      "Iteration 18681, loss = 6.09731413\n",
      "Iteration 18682, loss = 5.86195179\n",
      "Iteration 18683, loss = 6.25035043\n",
      "Iteration 18684, loss = 5.78065219\n",
      "Iteration 18685, loss = 6.70722003\n",
      "Iteration 18686, loss = 6.78796252\n",
      "Iteration 18687, loss = 6.26412849\n",
      "Iteration 18688, loss = 5.84680078\n",
      "Iteration 18689, loss = 6.08985385\n",
      "Iteration 18690, loss = 5.56130463\n",
      "Iteration 18691, loss = 5.60076105\n",
      "Iteration 18692, loss = 5.77576278\n",
      "Iteration 18693, loss = 5.68299108\n",
      "Iteration 18694, loss = 5.94651781\n",
      "Iteration 18695, loss = 6.12902083\n",
      "Iteration 18696, loss = 5.65130037\n",
      "Iteration 18697, loss = 5.74116247\n",
      "Iteration 18698, loss = 6.08623363\n",
      "Iteration 18699, loss = 5.93914085\n",
      "Iteration 18700, loss = 6.01115621\n",
      "Iteration 18701, loss = 5.79022763\n",
      "Iteration 18702, loss = 6.11031668\n",
      "Iteration 18703, loss = 6.50647868\n",
      "Iteration 18704, loss = 5.64574502\n",
      "Iteration 18705, loss = 5.95902143\n",
      "Iteration 18706, loss = 6.44367010\n",
      "Iteration 18707, loss = 5.66370261\n",
      "Iteration 18708, loss = 5.54660297\n",
      "Iteration 18709, loss = 5.80015137\n",
      "Iteration 18710, loss = 6.68453673\n",
      "Iteration 18711, loss = 6.43931522\n",
      "Iteration 18712, loss = 6.13271933\n",
      "Iteration 18713, loss = 6.70928347\n",
      "Iteration 18714, loss = 6.58627258\n",
      "Iteration 18715, loss = 6.39227936\n",
      "Iteration 18716, loss = 6.76085881\n",
      "Iteration 18717, loss = 6.21321968\n",
      "Iteration 18718, loss = 6.42036647\n",
      "Iteration 18719, loss = 5.91203770\n",
      "Iteration 18720, loss = 5.60650381\n",
      "Iteration 18721, loss = 6.06393420\n",
      "Iteration 18722, loss = 5.67496748\n",
      "Iteration 18723, loss = 5.70470943\n",
      "Iteration 18724, loss = 5.62880180\n",
      "Iteration 18725, loss = 5.63288015\n",
      "Iteration 18726, loss = 6.72862937\n",
      "Iteration 18727, loss = 5.79167951\n",
      "Iteration 18728, loss = 5.61717698\n",
      "Iteration 18729, loss = 5.66549666\n",
      "Iteration 18730, loss = 5.25543440\n",
      "Iteration 18731, loss = 6.04925778\n",
      "Iteration 18732, loss = 5.58587030\n",
      "Iteration 18733, loss = 5.54277233\n",
      "Iteration 18734, loss = 5.73028909\n",
      "Iteration 18735, loss = 5.67099133\n",
      "Iteration 18736, loss = 5.27949257\n",
      "Iteration 18737, loss = 5.65535773\n",
      "Iteration 18738, loss = 6.13675842\n",
      "Iteration 18739, loss = 5.91596755\n",
      "Iteration 18740, loss = 6.36819415\n",
      "Iteration 18741, loss = 6.54528395\n",
      "Iteration 18742, loss = 6.25519113\n",
      "Iteration 18743, loss = 5.44512522\n",
      "Iteration 18744, loss = 5.60210575\n",
      "Iteration 18745, loss = 6.25706586\n",
      "Iteration 18746, loss = 6.33761071\n",
      "Iteration 18747, loss = 6.86188868\n",
      "Iteration 18748, loss = 7.48490183\n",
      "Iteration 18749, loss = 6.20799077\n",
      "Iteration 18750, loss = 5.93213848\n",
      "Iteration 18751, loss = 6.28562625\n",
      "Iteration 18752, loss = 6.35082455\n",
      "Iteration 18753, loss = 5.73883745\n",
      "Iteration 18754, loss = 5.79444642\n",
      "Iteration 18755, loss = 5.37722562\n",
      "Iteration 18756, loss = 5.62561371\n",
      "Iteration 18757, loss = 5.99193733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18758, loss = 6.60939146\n",
      "Iteration 18759, loss = 6.86588545\n",
      "Iteration 18760, loss = 6.87754766\n",
      "Iteration 18761, loss = 6.30235594\n",
      "Iteration 18762, loss = 6.76680366\n",
      "Iteration 18763, loss = 6.93470517\n",
      "Iteration 18764, loss = 6.59849692\n",
      "Iteration 18765, loss = 6.11941222\n",
      "Iteration 18766, loss = 6.44950488\n",
      "Iteration 18767, loss = 5.68406335\n",
      "Iteration 18768, loss = 6.16148086\n",
      "Iteration 18769, loss = 5.78098593\n",
      "Iteration 18770, loss = 6.28805824\n",
      "Iteration 18771, loss = 6.50065956\n",
      "Iteration 18772, loss = 6.02479460\n",
      "Iteration 18773, loss = 5.75174123\n",
      "Iteration 18774, loss = 6.14529447\n",
      "Iteration 18775, loss = 6.09050702\n",
      "Iteration 18776, loss = 5.82557858\n",
      "Iteration 18777, loss = 6.31789767\n",
      "Iteration 18778, loss = 5.97998978\n",
      "Iteration 18779, loss = 5.65094100\n",
      "Iteration 18780, loss = 5.82306809\n",
      "Iteration 18781, loss = 6.01562724\n",
      "Iteration 18782, loss = 5.76445163\n",
      "Iteration 18783, loss = 6.03122027\n",
      "Iteration 18784, loss = 6.03461234\n",
      "Iteration 18785, loss = 6.12196164\n",
      "Iteration 18786, loss = 6.16278760\n",
      "Iteration 18787, loss = 7.06768677\n",
      "Iteration 18788, loss = 6.55271789\n",
      "Iteration 18789, loss = 6.63558948\n",
      "Iteration 18790, loss = 6.63836952\n",
      "Iteration 18791, loss = 6.73901083\n",
      "Iteration 18792, loss = 7.85483342\n",
      "Iteration 18793, loss = 6.62531836\n",
      "Iteration 18794, loss = 6.96148156\n",
      "Iteration 18795, loss = 7.00612010\n",
      "Iteration 18796, loss = 7.54744573\n",
      "Iteration 18797, loss = 6.60672798\n",
      "Iteration 18798, loss = 7.07434591\n",
      "Iteration 18799, loss = 7.81162995\n",
      "Iteration 18800, loss = 7.37655790\n",
      "Iteration 18801, loss = 6.19270494\n",
      "Iteration 18802, loss = 6.41679638\n",
      "Iteration 18803, loss = 5.91374831\n",
      "Iteration 18804, loss = 6.06894994\n",
      "Iteration 18805, loss = 6.08437276\n",
      "Iteration 18806, loss = 6.13600618\n",
      "Iteration 18807, loss = 5.82459118\n",
      "Iteration 18808, loss = 6.00922512\n",
      "Iteration 18809, loss = 5.95421587\n",
      "Iteration 18810, loss = 5.68150815\n",
      "Iteration 18811, loss = 5.49256100\n",
      "Iteration 18812, loss = 5.57587632\n",
      "Iteration 18813, loss = 5.96353348\n",
      "Iteration 18814, loss = 5.56272183\n",
      "Iteration 18815, loss = 5.78993455\n",
      "Iteration 18816, loss = 5.56587636\n",
      "Iteration 18817, loss = 6.73814658\n",
      "Iteration 18818, loss = 6.99103040\n",
      "Iteration 18819, loss = 6.01297755\n",
      "Iteration 18820, loss = 6.21460115\n",
      "Iteration 18821, loss = 6.39048425\n",
      "Iteration 18822, loss = 5.80270021\n",
      "Iteration 18823, loss = 5.45026127\n",
      "Iteration 18824, loss = 5.29694799\n",
      "Iteration 18825, loss = 5.50008570\n",
      "Iteration 18826, loss = 5.35100905\n",
      "Iteration 18827, loss = 5.69855817\n",
      "Iteration 18828, loss = 5.54411977\n",
      "Iteration 18829, loss = 5.71825228\n",
      "Iteration 18830, loss = 5.92822895\n",
      "Iteration 18831, loss = 5.60584478\n",
      "Iteration 18832, loss = 5.99371624\n",
      "Iteration 18833, loss = 5.89284697\n",
      "Iteration 18834, loss = 6.10932232\n",
      "Iteration 18835, loss = 5.95892744\n",
      "Iteration 18836, loss = 6.11099211\n",
      "Iteration 18837, loss = 5.70185531\n",
      "Iteration 18838, loss = 5.63333839\n",
      "Iteration 18839, loss = 5.68844808\n",
      "Iteration 18840, loss = 5.47594978\n",
      "Iteration 18841, loss = 5.92477958\n",
      "Iteration 18842, loss = 6.93645397\n",
      "Iteration 18843, loss = 5.67452677\n",
      "Iteration 18844, loss = 6.39840611\n",
      "Iteration 18845, loss = 6.92846627\n",
      "Iteration 18846, loss = 7.07029928\n",
      "Iteration 18847, loss = 6.05675123\n",
      "Iteration 18848, loss = 6.18591264\n",
      "Iteration 18849, loss = 6.01511736\n",
      "Iteration 18850, loss = 6.41854857\n",
      "Iteration 18851, loss = 5.36467767\n",
      "Iteration 18852, loss = 5.85300166\n",
      "Iteration 18853, loss = 5.72400678\n",
      "Iteration 18854, loss = 5.59919772\n",
      "Iteration 18855, loss = 5.26480734\n",
      "Iteration 18856, loss = 5.87222506\n",
      "Iteration 18857, loss = 6.08225802\n",
      "Iteration 18858, loss = 6.15877879\n",
      "Iteration 18859, loss = 6.00807125\n",
      "Iteration 18860, loss = 5.64627388\n",
      "Iteration 18861, loss = 6.02538638\n",
      "Iteration 18862, loss = 6.83976367\n",
      "Iteration 18863, loss = 6.61087976\n",
      "Iteration 18864, loss = 7.57244585\n",
      "Iteration 18865, loss = 7.10157450\n",
      "Iteration 18866, loss = 6.26412591\n",
      "Iteration 18867, loss = 5.64822756\n",
      "Iteration 18868, loss = 5.99424374\n",
      "Iteration 18869, loss = 6.09299317\n",
      "Iteration 18870, loss = 5.73609708\n",
      "Iteration 18871, loss = 5.85265429\n",
      "Iteration 18872, loss = 5.86812353\n",
      "Iteration 18873, loss = 6.08947058\n",
      "Iteration 18874, loss = 5.59137491\n",
      "Iteration 18875, loss = 6.14984358\n",
      "Iteration 18876, loss = 6.34076197\n",
      "Iteration 18877, loss = 6.16985666\n",
      "Iteration 18878, loss = 5.90573597\n",
      "Iteration 18879, loss = 6.14820557\n",
      "Iteration 18880, loss = 6.17554170\n",
      "Iteration 18881, loss = 6.14979974\n",
      "Iteration 18882, loss = 5.89427576\n",
      "Iteration 18883, loss = 5.36021627\n",
      "Iteration 18884, loss = 5.77661203\n",
      "Iteration 18885, loss = 5.56188651\n",
      "Iteration 18886, loss = 5.66083807\n",
      "Iteration 18887, loss = 5.80875667\n",
      "Iteration 18888, loss = 5.39727267\n",
      "Iteration 18889, loss = 5.75629037\n",
      "Iteration 18890, loss = 6.29467428\n",
      "Iteration 18891, loss = 6.28136524\n",
      "Iteration 18892, loss = 6.68833623\n",
      "Iteration 18893, loss = 7.28728506\n",
      "Iteration 18894, loss = 6.00177831\n",
      "Iteration 18895, loss = 7.47855626\n",
      "Iteration 18896, loss = 6.23662760\n",
      "Iteration 18897, loss = 6.55373919\n",
      "Iteration 18898, loss = 6.91300307\n",
      "Iteration 18899, loss = 6.35669817\n",
      "Iteration 18900, loss = 6.95287818\n",
      "Iteration 18901, loss = 7.33901313\n",
      "Iteration 18902, loss = 6.77816691\n",
      "Iteration 18903, loss = 6.02254090\n",
      "Iteration 18904, loss = 5.45384230\n",
      "Iteration 18905, loss = 5.48891312\n",
      "Iteration 18906, loss = 5.66256142\n",
      "Iteration 18907, loss = 6.14665700\n",
      "Iteration 18908, loss = 5.91484598\n",
      "Iteration 18909, loss = 5.74895219\n",
      "Iteration 18910, loss = 5.64061483\n",
      "Iteration 18911, loss = 6.03028878\n",
      "Iteration 18912, loss = 5.86864671\n",
      "Iteration 18913, loss = 6.10848198\n",
      "Iteration 18914, loss = 5.49407694\n",
      "Iteration 18915, loss = 5.79369871\n",
      "Iteration 18916, loss = 6.30463994\n",
      "Iteration 18917, loss = 5.70694999\n",
      "Iteration 18918, loss = 5.41190244\n",
      "Iteration 18919, loss = 5.47207773\n",
      "Iteration 18920, loss = 5.67040420\n",
      "Iteration 18921, loss = 5.49412446\n",
      "Iteration 18922, loss = 5.47721846\n",
      "Iteration 18923, loss = 5.44415032\n",
      "Iteration 18924, loss = 5.56345695\n",
      "Iteration 18925, loss = 5.86521607\n",
      "Iteration 18926, loss = 6.17654523\n",
      "Iteration 18927, loss = 5.59119209\n",
      "Iteration 18928, loss = 5.85233284\n",
      "Iteration 18929, loss = 5.65726714\n",
      "Iteration 18930, loss = 5.74877108\n",
      "Iteration 18931, loss = 5.88985382\n",
      "Iteration 18932, loss = 6.06049569\n",
      "Iteration 18933, loss = 5.52581253\n",
      "Iteration 18934, loss = 5.57501021\n",
      "Iteration 18935, loss = 5.65493907\n",
      "Iteration 18936, loss = 5.66097474\n",
      "Iteration 18937, loss = 5.49033378\n",
      "Iteration 18938, loss = 6.12828917\n",
      "Iteration 18939, loss = 6.40221605\n",
      "Iteration 18940, loss = 6.11107764\n",
      "Iteration 18941, loss = 6.05312049\n",
      "Iteration 18942, loss = 5.38604162\n",
      "Iteration 18943, loss = 5.37725286\n",
      "Iteration 18944, loss = 5.77282311\n",
      "Iteration 18945, loss = 5.48143733\n",
      "Iteration 18946, loss = 5.88171831\n",
      "Iteration 18947, loss = 5.77109388\n",
      "Iteration 18948, loss = 5.65969383\n",
      "Iteration 18949, loss = 5.66799888\n",
      "Iteration 18950, loss = 5.63660535\n",
      "Iteration 18951, loss = 5.48804511\n",
      "Iteration 18952, loss = 5.84590374\n",
      "Iteration 18953, loss = 5.95086531\n",
      "Iteration 18954, loss = 6.76584836\n",
      "Iteration 18955, loss = 6.22115589\n",
      "Iteration 18956, loss = 6.05250843\n",
      "Iteration 18957, loss = 5.73074823\n",
      "Iteration 18958, loss = 6.40112909\n",
      "Iteration 18959, loss = 6.20876611\n",
      "Iteration 18960, loss = 6.69809575\n",
      "Iteration 18961, loss = 6.37298904\n",
      "Iteration 18962, loss = 6.16724509\n",
      "Iteration 18963, loss = 5.60899251\n",
      "Iteration 18964, loss = 5.70981577\n",
      "Iteration 18965, loss = 5.34889555\n",
      "Iteration 18966, loss = 6.04927153\n",
      "Iteration 18967, loss = 6.28983590\n",
      "Iteration 18968, loss = 7.90821966\n",
      "Iteration 18969, loss = 6.51265501\n",
      "Iteration 18970, loss = 6.15391348\n",
      "Iteration 18971, loss = 6.83097259\n",
      "Iteration 18972, loss = 6.09823345\n",
      "Iteration 18973, loss = 6.14390182\n",
      "Iteration 18974, loss = 5.92256045\n",
      "Iteration 18975, loss = 6.32791051\n",
      "Iteration 18976, loss = 6.10216044\n",
      "Iteration 18977, loss = 7.18098234\n",
      "Iteration 18978, loss = 6.94344537\n",
      "Iteration 18979, loss = 8.69462182\n",
      "Iteration 18980, loss = 8.78986324\n",
      "Iteration 18981, loss = 7.62567303\n",
      "Iteration 18982, loss = 6.67451766\n",
      "Iteration 18983, loss = 6.71308574\n",
      "Iteration 18984, loss = 6.04116635\n",
      "Iteration 18985, loss = 5.60474965\n",
      "Iteration 18986, loss = 5.69512974\n",
      "Iteration 18987, loss = 5.54264660\n",
      "Iteration 18988, loss = 5.65348817\n",
      "Iteration 18989, loss = 5.98335230\n",
      "Iteration 18990, loss = 5.97694662\n",
      "Iteration 18991, loss = 6.65023802\n",
      "Iteration 18992, loss = 5.63064117\n",
      "Iteration 18993, loss = 5.86303725\n",
      "Iteration 18994, loss = 5.26999053\n",
      "Iteration 18995, loss = 5.71398176\n",
      "Iteration 18996, loss = 5.82542992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18997, loss = 5.81217764\n",
      "Iteration 18998, loss = 5.62415020\n",
      "Iteration 18999, loss = 5.69590942\n",
      "Iteration 19000, loss = 5.75496391\n",
      "Iteration 19001, loss = 5.45681249\n",
      "Iteration 19002, loss = 5.62820984\n",
      "Iteration 19003, loss = 5.49564658\n",
      "Iteration 19004, loss = 5.46132010\n",
      "Iteration 19005, loss = 5.69654803\n",
      "Iteration 19006, loss = 5.81837224\n",
      "Iteration 19007, loss = 6.57406221\n",
      "Iteration 19008, loss = 5.89156353\n",
      "Iteration 19009, loss = 6.59855484\n",
      "Iteration 19010, loss = 7.55214613\n",
      "Iteration 19011, loss = 6.16750707\n",
      "Iteration 19012, loss = 5.85775440\n",
      "Iteration 19013, loss = 6.61342777\n",
      "Iteration 19014, loss = 5.79811073\n",
      "Iteration 19015, loss = 5.76465871\n",
      "Iteration 19016, loss = 6.86914516\n",
      "Iteration 19017, loss = 6.41580992\n",
      "Iteration 19018, loss = 6.30412246\n",
      "Iteration 19019, loss = 5.96414975\n",
      "Iteration 19020, loss = 5.88993932\n",
      "Iteration 19021, loss = 5.36343608\n",
      "Iteration 19022, loss = 6.09342206\n",
      "Iteration 19023, loss = 5.83878869\n",
      "Iteration 19024, loss = 6.41634054\n",
      "Iteration 19025, loss = 5.38092457\n",
      "Iteration 19026, loss = 5.61667813\n",
      "Iteration 19027, loss = 5.54174411\n",
      "Iteration 19028, loss = 5.89289356\n",
      "Iteration 19029, loss = 5.61318176\n",
      "Iteration 19030, loss = 5.36726634\n",
      "Iteration 19031, loss = 5.33718363\n",
      "Iteration 19032, loss = 5.54723675\n",
      "Iteration 19033, loss = 5.35802794\n",
      "Iteration 19034, loss = 5.56023803\n",
      "Iteration 19035, loss = 5.82934956\n",
      "Iteration 19036, loss = 5.69874244\n",
      "Iteration 19037, loss = 6.25941797\n",
      "Iteration 19038, loss = 5.56665922\n",
      "Iteration 19039, loss = 6.03962178\n",
      "Iteration 19040, loss = 6.19149271\n",
      "Iteration 19041, loss = 6.45114744\n",
      "Iteration 19042, loss = 6.08446525\n",
      "Iteration 19043, loss = 6.60272035\n",
      "Iteration 19044, loss = 8.37501860\n",
      "Iteration 19045, loss = 7.38474084\n",
      "Iteration 19046, loss = 7.18771753\n",
      "Iteration 19047, loss = 6.42476296\n",
      "Iteration 19048, loss = 6.23298426\n",
      "Iteration 19049, loss = 6.08649154\n",
      "Iteration 19050, loss = 5.80065651\n",
      "Iteration 19051, loss = 5.42948392\n",
      "Iteration 19052, loss = 5.90011188\n",
      "Iteration 19053, loss = 5.81545038\n",
      "Iteration 19054, loss = 5.99494142\n",
      "Iteration 19055, loss = 5.94188389\n",
      "Iteration 19056, loss = 7.04538598\n",
      "Iteration 19057, loss = 6.42309961\n",
      "Iteration 19058, loss = 7.08591015\n",
      "Iteration 19059, loss = 7.68054009\n",
      "Iteration 19060, loss = 7.47436127\n",
      "Iteration 19061, loss = 6.89013007\n",
      "Iteration 19062, loss = 7.09817533\n",
      "Iteration 19063, loss = 6.13013409\n",
      "Iteration 19064, loss = 6.59259036\n",
      "Iteration 19065, loss = 5.54408028\n",
      "Iteration 19066, loss = 5.55471956\n",
      "Iteration 19067, loss = 5.42338295\n",
      "Iteration 19068, loss = 5.87145905\n",
      "Iteration 19069, loss = 5.50217806\n",
      "Iteration 19070, loss = 5.57362924\n",
      "Iteration 19071, loss = 5.50393557\n",
      "Iteration 19072, loss = 5.49963768\n",
      "Iteration 19073, loss = 5.85476052\n",
      "Iteration 19074, loss = 5.66885184\n",
      "Iteration 19075, loss = 5.94637564\n",
      "Iteration 19076, loss = 5.81151004\n",
      "Iteration 19077, loss = 5.57673691\n",
      "Iteration 19078, loss = 5.54046071\n",
      "Iteration 19079, loss = 5.89212888\n",
      "Iteration 19080, loss = 5.36771279\n",
      "Iteration 19081, loss = 5.35092368\n",
      "Iteration 19082, loss = 5.28028267\n",
      "Iteration 19083, loss = 5.45819934\n",
      "Iteration 19084, loss = 5.58698160\n",
      "Iteration 19085, loss = 5.76258740\n",
      "Iteration 19086, loss = 5.93075337\n",
      "Iteration 19087, loss = 6.05352314\n",
      "Iteration 19088, loss = 6.53243842\n",
      "Iteration 19089, loss = 7.30148395\n",
      "Iteration 19090, loss = 7.32459457\n",
      "Iteration 19091, loss = 6.34910774\n",
      "Iteration 19092, loss = 6.04866289\n",
      "Iteration 19093, loss = 5.92985698\n",
      "Iteration 19094, loss = 6.10859037\n",
      "Iteration 19095, loss = 7.35198722\n",
      "Iteration 19096, loss = 6.42911397\n",
      "Iteration 19097, loss = 6.79122049\n",
      "Iteration 19098, loss = 6.18636788\n",
      "Iteration 19099, loss = 6.94688076\n",
      "Iteration 19100, loss = 7.63095416\n",
      "Iteration 19101, loss = 6.37829149\n",
      "Iteration 19102, loss = 5.57570685\n",
      "Iteration 19103, loss = 5.50466710\n",
      "Iteration 19104, loss = 5.49141329\n",
      "Iteration 19105, loss = 5.46116188\n",
      "Iteration 19106, loss = 5.98217448\n",
      "Iteration 19107, loss = 5.46129003\n",
      "Iteration 19108, loss = 6.12972544\n",
      "Iteration 19109, loss = 6.42218208\n",
      "Iteration 19110, loss = 6.07501412\n",
      "Iteration 19111, loss = 6.39413925\n",
      "Iteration 19112, loss = 5.76526065\n",
      "Iteration 19113, loss = 6.61993803\n",
      "Iteration 19114, loss = 6.47854445\n",
      "Iteration 19115, loss = 5.96580945\n",
      "Iteration 19116, loss = 6.26842919\n",
      "Iteration 19117, loss = 5.39217999\n",
      "Iteration 19118, loss = 5.39499902\n",
      "Iteration 19119, loss = 5.99398481\n",
      "Iteration 19120, loss = 6.78717574\n",
      "Iteration 19121, loss = 6.05530789\n",
      "Iteration 19122, loss = 5.66198298\n",
      "Iteration 19123, loss = 6.82317196\n",
      "Iteration 19124, loss = 5.89030945\n",
      "Iteration 19125, loss = 6.11977676\n",
      "Iteration 19126, loss = 6.64716373\n",
      "Iteration 19127, loss = 5.99264283\n",
      "Iteration 19128, loss = 6.92111572\n",
      "Iteration 19129, loss = 7.28105137\n",
      "Iteration 19130, loss = 7.21292555\n",
      "Iteration 19131, loss = 7.36320032\n",
      "Iteration 19132, loss = 5.81032139\n",
      "Iteration 19133, loss = 5.74536457\n",
      "Iteration 19134, loss = 5.74416153\n",
      "Iteration 19135, loss = 5.66078363\n",
      "Iteration 19136, loss = 5.72523904\n",
      "Iteration 19137, loss = 6.38641601\n",
      "Iteration 19138, loss = 5.77724047\n",
      "Iteration 19139, loss = 6.11826841\n",
      "Iteration 19140, loss = 5.36838953\n",
      "Iteration 19141, loss = 5.23034777\n",
      "Iteration 19142, loss = 5.43670346\n",
      "Iteration 19143, loss = 5.58876288\n",
      "Iteration 19144, loss = 5.52320567\n",
      "Iteration 19145, loss = 6.56754890\n",
      "Iteration 19146, loss = 6.83143779\n",
      "Iteration 19147, loss = 6.47523243\n",
      "Iteration 19148, loss = 5.83520446\n",
      "Iteration 19149, loss = 6.17880331\n",
      "Iteration 19150, loss = 6.65190093\n",
      "Iteration 19151, loss = 7.01648450\n",
      "Iteration 19152, loss = 5.85252719\n",
      "Iteration 19153, loss = 5.79263809\n",
      "Iteration 19154, loss = 5.79685099\n",
      "Iteration 19155, loss = 5.48780811\n",
      "Iteration 19156, loss = 5.39724171\n",
      "Iteration 19157, loss = 5.59956125\n",
      "Iteration 19158, loss = 5.30796206\n",
      "Iteration 19159, loss = 5.59536294\n",
      "Iteration 19160, loss = 5.49166073\n",
      "Iteration 19161, loss = 5.72218160\n",
      "Iteration 19162, loss = 5.77589270\n",
      "Iteration 19163, loss = 5.36255965\n",
      "Iteration 19164, loss = 5.28561680\n",
      "Iteration 19165, loss = 5.62603727\n",
      "Iteration 19166, loss = 5.75134575\n",
      "Iteration 19167, loss = 5.57636921\n",
      "Iteration 19168, loss = 6.21542191\n",
      "Iteration 19169, loss = 5.79868678\n",
      "Iteration 19170, loss = 5.69893760\n",
      "Iteration 19171, loss = 5.53415649\n",
      "Iteration 19172, loss = 5.46082137\n",
      "Iteration 19173, loss = 5.97181424\n",
      "Iteration 19174, loss = 6.23122486\n",
      "Iteration 19175, loss = 6.69152144\n",
      "Iteration 19176, loss = 7.08799225\n",
      "Iteration 19177, loss = 6.51451148\n",
      "Iteration 19178, loss = 6.09778600\n",
      "Iteration 19179, loss = 5.63510675\n",
      "Iteration 19180, loss = 5.62598418\n",
      "Iteration 19181, loss = 5.42952344\n",
      "Iteration 19182, loss = 5.63826621\n",
      "Iteration 19183, loss = 5.40891547\n",
      "Iteration 19184, loss = 5.72754635\n",
      "Iteration 19185, loss = 6.22700458\n",
      "Iteration 19186, loss = 6.71204109\n",
      "Iteration 19187, loss = 6.15947524\n",
      "Iteration 19188, loss = 5.98330731\n",
      "Iteration 19189, loss = 6.78824759\n",
      "Iteration 19190, loss = 7.36371168\n",
      "Iteration 19191, loss = 5.71159512\n",
      "Iteration 19192, loss = 5.82439239\n",
      "Iteration 19193, loss = 5.95694386\n",
      "Iteration 19194, loss = 5.66664911\n",
      "Iteration 19195, loss = 6.11381412\n",
      "Iteration 19196, loss = 6.09630515\n",
      "Iteration 19197, loss = 6.47010237\n",
      "Iteration 19198, loss = 6.22928036\n",
      "Iteration 19199, loss = 5.45239479\n",
      "Iteration 19200, loss = 5.33034499\n",
      "Iteration 19201, loss = 5.51675010\n",
      "Iteration 19202, loss = 5.43387916\n",
      "Iteration 19203, loss = 5.66904743\n",
      "Iteration 19204, loss = 6.36613816\n",
      "Iteration 19205, loss = 6.47828555\n",
      "Iteration 19206, loss = 7.56403285\n",
      "Iteration 19207, loss = 6.90543421\n",
      "Iteration 19208, loss = 7.11184866\n",
      "Iteration 19209, loss = 6.88781754\n",
      "Iteration 19210, loss = 6.24671413\n",
      "Iteration 19211, loss = 6.79417456\n",
      "Iteration 19212, loss = 7.32153052\n",
      "Iteration 19213, loss = 7.27040370\n",
      "Iteration 19214, loss = 6.66466724\n",
      "Iteration 19215, loss = 7.44088591\n",
      "Iteration 19216, loss = 6.31097734\n",
      "Iteration 19217, loss = 7.79618916\n",
      "Iteration 19218, loss = 7.02737737\n",
      "Iteration 19219, loss = 8.04128144\n",
      "Iteration 19220, loss = 5.89625433\n",
      "Iteration 19221, loss = 5.73312748\n",
      "Iteration 19222, loss = 5.49154104\n",
      "Iteration 19223, loss = 6.14206324\n",
      "Iteration 19224, loss = 5.88581910\n",
      "Iteration 19225, loss = 5.85465799\n",
      "Iteration 19226, loss = 5.74977443\n",
      "Iteration 19227, loss = 6.00653849\n",
      "Iteration 19228, loss = 5.69673229\n",
      "Iteration 19229, loss = 5.72423464\n",
      "Iteration 19230, loss = 5.72784129\n",
      "Iteration 19231, loss = 5.83430007\n",
      "Iteration 19232, loss = 5.49971167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19233, loss = 5.88801371\n",
      "Iteration 19234, loss = 5.31706587\n",
      "Iteration 19235, loss = 5.69370084\n",
      "Iteration 19236, loss = 6.39555252\n",
      "Iteration 19237, loss = 6.44649190\n",
      "Iteration 19238, loss = 6.07116224\n",
      "Iteration 19239, loss = 6.25032197\n",
      "Iteration 19240, loss = 6.33228291\n",
      "Iteration 19241, loss = 7.74385987\n",
      "Iteration 19242, loss = 7.01738142\n",
      "Iteration 19243, loss = 5.89204773\n",
      "Iteration 19244, loss = 5.83054801\n",
      "Iteration 19245, loss = 5.54390210\n",
      "Iteration 19246, loss = 5.93956710\n",
      "Iteration 19247, loss = 5.79508375\n",
      "Iteration 19248, loss = 5.35650412\n",
      "Iteration 19249, loss = 5.49100014\n",
      "Iteration 19250, loss = 5.25457917\n",
      "Iteration 19251, loss = 5.84177301\n",
      "Iteration 19252, loss = 5.88632856\n",
      "Iteration 19253, loss = 5.85458836\n",
      "Iteration 19254, loss = 6.25949834\n",
      "Iteration 19255, loss = 5.54416689\n",
      "Iteration 19256, loss = 5.79537647\n",
      "Iteration 19257, loss = 5.93065781\n",
      "Iteration 19258, loss = 5.83177609\n",
      "Iteration 19259, loss = 5.62274888\n",
      "Iteration 19260, loss = 5.62865221\n",
      "Iteration 19261, loss = 5.99676166\n",
      "Iteration 19262, loss = 5.77188632\n",
      "Iteration 19263, loss = 5.63840156\n",
      "Iteration 19264, loss = 6.15142162\n",
      "Iteration 19265, loss = 6.10784836\n",
      "Iteration 19266, loss = 6.46546682\n",
      "Iteration 19267, loss = 6.86914147\n",
      "Iteration 19268, loss = 5.89039428\n",
      "Iteration 19269, loss = 5.72098652\n",
      "Iteration 19270, loss = 5.91970099\n",
      "Iteration 19271, loss = 5.91690274\n",
      "Iteration 19272, loss = 6.53547058\n",
      "Iteration 19273, loss = 7.27815317\n",
      "Iteration 19274, loss = 6.35340168\n",
      "Iteration 19275, loss = 6.76943729\n",
      "Iteration 19276, loss = 6.88720561\n",
      "Iteration 19277, loss = 5.82233738\n",
      "Iteration 19278, loss = 5.89701141\n",
      "Iteration 19279, loss = 5.50538205\n",
      "Iteration 19280, loss = 5.72835277\n",
      "Iteration 19281, loss = 5.92825047\n",
      "Iteration 19282, loss = 6.23980704\n",
      "Iteration 19283, loss = 5.72753657\n",
      "Iteration 19284, loss = 6.28874220\n",
      "Iteration 19285, loss = 6.09599990\n",
      "Iteration 19286, loss = 7.20802433\n",
      "Iteration 19287, loss = 6.91953360\n",
      "Iteration 19288, loss = 6.45296978\n",
      "Iteration 19289, loss = 6.13607806\n",
      "Iteration 19290, loss = 5.79103205\n",
      "Iteration 19291, loss = 5.78117218\n",
      "Iteration 19292, loss = 5.46399506\n",
      "Iteration 19293, loss = 5.66451933\n",
      "Iteration 19294, loss = 6.04277928\n",
      "Iteration 19295, loss = 6.06312891\n",
      "Iteration 19296, loss = 6.38575152\n",
      "Iteration 19297, loss = 5.99482583\n",
      "Iteration 19298, loss = 6.21495393\n",
      "Iteration 19299, loss = 6.13765956\n",
      "Iteration 19300, loss = 6.30135463\n",
      "Iteration 19301, loss = 6.04357390\n",
      "Iteration 19302, loss = 5.48821040\n",
      "Iteration 19303, loss = 5.27297965\n",
      "Iteration 19304, loss = 5.49182867\n",
      "Iteration 19305, loss = 6.35502753\n",
      "Iteration 19306, loss = 7.01580507\n",
      "Iteration 19307, loss = 5.74774392\n",
      "Iteration 19308, loss = 5.88022320\n",
      "Iteration 19309, loss = 6.19766355\n",
      "Iteration 19310, loss = 6.42488086\n",
      "Iteration 19311, loss = 6.69009419\n",
      "Iteration 19312, loss = 6.70684441\n",
      "Iteration 19313, loss = 5.66391538\n",
      "Iteration 19314, loss = 5.59648685\n",
      "Iteration 19315, loss = 5.54891683\n",
      "Iteration 19316, loss = 6.01069309\n",
      "Iteration 19317, loss = 6.41142222\n",
      "Iteration 19318, loss = 6.44172451\n",
      "Iteration 19319, loss = 6.45285649\n",
      "Iteration 19320, loss = 5.95680061\n",
      "Iteration 19321, loss = 5.71393500\n",
      "Iteration 19322, loss = 6.24282409\n",
      "Iteration 19323, loss = 6.04130311\n",
      "Iteration 19324, loss = 5.80833723\n",
      "Iteration 19325, loss = 6.01733238\n",
      "Iteration 19326, loss = 5.91359607\n",
      "Iteration 19327, loss = 7.65164417\n",
      "Iteration 19328, loss = 5.60262388\n",
      "Iteration 19329, loss = 8.28115438\n",
      "Iteration 19330, loss = 7.51331091\n",
      "Iteration 19331, loss = 6.63522645\n",
      "Iteration 19332, loss = 6.07859728\n",
      "Iteration 19333, loss = 6.07271457\n",
      "Iteration 19334, loss = 6.34714393\n",
      "Iteration 19335, loss = 6.28346581\n",
      "Iteration 19336, loss = 6.10606347\n",
      "Iteration 19337, loss = 5.62779291\n",
      "Iteration 19338, loss = 5.83826839\n",
      "Iteration 19339, loss = 5.67654582\n",
      "Iteration 19340, loss = 5.95922042\n",
      "Iteration 19341, loss = 6.47090522\n",
      "Iteration 19342, loss = 5.91078966\n",
      "Iteration 19343, loss = 6.01237183\n",
      "Iteration 19344, loss = 5.37788456\n",
      "Iteration 19345, loss = 5.90535282\n",
      "Iteration 19346, loss = 5.92176288\n",
      "Iteration 19347, loss = 6.50274095\n",
      "Iteration 19348, loss = 6.06380883\n",
      "Iteration 19349, loss = 7.13688032\n",
      "Iteration 19350, loss = 6.86009720\n",
      "Iteration 19351, loss = 5.46726700\n",
      "Iteration 19352, loss = 5.69716714\n",
      "Iteration 19353, loss = 5.37902023\n",
      "Iteration 19354, loss = 5.60076539\n",
      "Iteration 19355, loss = 5.24807519\n",
      "Iteration 19356, loss = 5.81044978\n",
      "Iteration 19357, loss = 5.60692387\n",
      "Iteration 19358, loss = 5.47928320\n",
      "Iteration 19359, loss = 5.21838543\n",
      "Iteration 19360, loss = 5.85560194\n",
      "Iteration 19361, loss = 7.15251628\n",
      "Iteration 19362, loss = 6.51469689\n",
      "Iteration 19363, loss = 5.69553928\n",
      "Iteration 19364, loss = 5.35012998\n",
      "Iteration 19365, loss = 5.13347965\n",
      "Iteration 19366, loss = 5.25836993\n",
      "Iteration 19367, loss = 5.51129192\n",
      "Iteration 19368, loss = 5.85703219\n",
      "Iteration 19369, loss = 5.37943724\n",
      "Iteration 19370, loss = 5.34132153\n",
      "Iteration 19371, loss = 5.33479536\n",
      "Iteration 19372, loss = 5.26576728\n",
      "Iteration 19373, loss = 6.39959454\n",
      "Iteration 19374, loss = 6.01436081\n",
      "Iteration 19375, loss = 5.65683372\n",
      "Iteration 19376, loss = 5.88813569\n",
      "Iteration 19377, loss = 5.77367833\n",
      "Iteration 19378, loss = 6.92762996\n",
      "Iteration 19379, loss = 6.94603296\n",
      "Iteration 19380, loss = 6.11981275\n",
      "Iteration 19381, loss = 6.32902233\n",
      "Iteration 19382, loss = 5.90618515\n",
      "Iteration 19383, loss = 6.76093356\n",
      "Iteration 19384, loss = 5.68069045\n",
      "Iteration 19385, loss = 5.37656670\n",
      "Iteration 19386, loss = 5.11907214\n",
      "Iteration 19387, loss = 5.44166092\n",
      "Iteration 19388, loss = 5.68321227\n",
      "Iteration 19389, loss = 5.55127752\n",
      "Iteration 19390, loss = 5.61732402\n",
      "Iteration 19391, loss = 5.62167280\n",
      "Iteration 19392, loss = 5.90586530\n",
      "Iteration 19393, loss = 5.96601860\n",
      "Iteration 19394, loss = 6.10414553\n",
      "Iteration 19395, loss = 6.10450290\n",
      "Iteration 19396, loss = 6.20211044\n",
      "Iteration 19397, loss = 5.62487613\n",
      "Iteration 19398, loss = 5.28550804\n",
      "Iteration 19399, loss = 5.29299222\n",
      "Iteration 19400, loss = 6.29041878\n",
      "Iteration 19401, loss = 5.98139806\n",
      "Iteration 19402, loss = 6.29973264\n",
      "Iteration 19403, loss = 6.09290808\n",
      "Iteration 19404, loss = 5.60944220\n",
      "Iteration 19405, loss = 5.54636614\n",
      "Iteration 19406, loss = 5.72504740\n",
      "Iteration 19407, loss = 6.47137476\n",
      "Iteration 19408, loss = 6.59263767\n",
      "Iteration 19409, loss = 6.51495131\n",
      "Iteration 19410, loss = 6.08986787\n",
      "Iteration 19411, loss = 5.66361030\n",
      "Iteration 19412, loss = 5.54435506\n",
      "Iteration 19413, loss = 5.25578510\n",
      "Iteration 19414, loss = 5.66993194\n",
      "Iteration 19415, loss = 5.85777824\n",
      "Iteration 19416, loss = 5.67278676\n",
      "Iteration 19417, loss = 5.81790657\n",
      "Iteration 19418, loss = 5.78305743\n",
      "Iteration 19419, loss = 5.46655421\n",
      "Iteration 19420, loss = 5.23736282\n",
      "Iteration 19421, loss = 5.63919752\n",
      "Iteration 19422, loss = 5.86867780\n",
      "Iteration 19423, loss = 5.65660831\n",
      "Iteration 19424, loss = 5.40137423\n",
      "Iteration 19425, loss = 6.12630543\n",
      "Iteration 19426, loss = 6.00988093\n",
      "Iteration 19427, loss = 5.92425564\n",
      "Iteration 19428, loss = 6.90309960\n",
      "Iteration 19429, loss = 7.56729816\n",
      "Iteration 19430, loss = 7.42744552\n",
      "Iteration 19431, loss = 7.10238502\n",
      "Iteration 19432, loss = 5.96574782\n",
      "Iteration 19433, loss = 6.05184500\n",
      "Iteration 19434, loss = 5.25274006\n",
      "Iteration 19435, loss = 5.47913635\n",
      "Iteration 19436, loss = 6.96249762\n",
      "Iteration 19437, loss = 5.81727354\n",
      "Iteration 19438, loss = 5.48289344\n",
      "Iteration 19439, loss = 5.57146600\n",
      "Iteration 19440, loss = 6.51195550\n",
      "Iteration 19441, loss = 5.73820571\n",
      "Iteration 19442, loss = 5.71463047\n",
      "Iteration 19443, loss = 5.39975689\n",
      "Iteration 19444, loss = 5.69018134\n",
      "Iteration 19445, loss = 5.29858035\n",
      "Iteration 19446, loss = 5.72522852\n",
      "Iteration 19447, loss = 5.64797422\n",
      "Iteration 19448, loss = 5.90451102\n",
      "Iteration 19449, loss = 6.02702035\n",
      "Iteration 19450, loss = 6.33232014\n",
      "Iteration 19451, loss = 6.37159574\n",
      "Iteration 19452, loss = 5.18277273\n",
      "Iteration 19453, loss = 5.48567836\n",
      "Iteration 19454, loss = 6.01836491\n",
      "Iteration 19455, loss = 5.76797995\n",
      "Iteration 19456, loss = 5.71784917\n",
      "Iteration 19457, loss = 5.84503189\n",
      "Iteration 19458, loss = 6.45846893\n",
      "Iteration 19459, loss = 6.15380534\n",
      "Iteration 19460, loss = 6.74895168\n",
      "Iteration 19461, loss = 6.69582868\n",
      "Iteration 19462, loss = 5.95987652\n",
      "Iteration 19463, loss = 6.46741366\n",
      "Iteration 19464, loss = 6.87489013\n",
      "Iteration 19465, loss = 6.02408310\n",
      "Iteration 19466, loss = 5.87586988\n",
      "Iteration 19467, loss = 5.94531174\n",
      "Iteration 19468, loss = 6.12252532\n",
      "Iteration 19469, loss = 5.70770296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19470, loss = 5.60959612\n",
      "Iteration 19471, loss = 6.27354039\n",
      "Iteration 19472, loss = 6.17426647\n",
      "Iteration 19473, loss = 7.61556553\n",
      "Iteration 19474, loss = 6.60708898\n",
      "Iteration 19475, loss = 5.86262548\n",
      "Iteration 19476, loss = 5.47812192\n",
      "Iteration 19477, loss = 5.59816741\n",
      "Iteration 19478, loss = 5.50270710\n",
      "Iteration 19479, loss = 5.44686977\n",
      "Iteration 19480, loss = 5.99427890\n",
      "Iteration 19481, loss = 6.26710502\n",
      "Iteration 19482, loss = 5.05499767\n",
      "Iteration 19483, loss = 5.51897196\n",
      "Iteration 19484, loss = 5.55302384\n",
      "Iteration 19485, loss = 5.75196424\n",
      "Iteration 19486, loss = 6.25596817\n",
      "Iteration 19487, loss = 6.04481922\n",
      "Iteration 19488, loss = 5.71637437\n",
      "Iteration 19489, loss = 5.70200164\n",
      "Iteration 19490, loss = 6.58061592\n",
      "Iteration 19491, loss = 6.30181267\n",
      "Iteration 19492, loss = 5.91114357\n",
      "Iteration 19493, loss = 7.37074462\n",
      "Iteration 19494, loss = 6.25188573\n",
      "Iteration 19495, loss = 5.68200263\n",
      "Iteration 19496, loss = 5.75020536\n",
      "Iteration 19497, loss = 5.98529837\n",
      "Iteration 19498, loss = 5.89900743\n",
      "Iteration 19499, loss = 6.11959703\n",
      "Iteration 19500, loss = 5.44143108\n",
      "Iteration 19501, loss = 5.73686513\n",
      "Iteration 19502, loss = 5.35509713\n",
      "Iteration 19503, loss = 5.16955476\n",
      "Iteration 19504, loss = 5.09393747\n",
      "Iteration 19505, loss = 5.28630899\n",
      "Iteration 19506, loss = 5.34458141\n",
      "Iteration 19507, loss = 6.52730164\n",
      "Iteration 19508, loss = 5.86450296\n",
      "Iteration 19509, loss = 5.61756319\n",
      "Iteration 19510, loss = 5.74105718\n",
      "Iteration 19511, loss = 5.70079895\n",
      "Iteration 19512, loss = 5.39962227\n",
      "Iteration 19513, loss = 5.44499819\n",
      "Iteration 19514, loss = 5.25604483\n",
      "Iteration 19515, loss = 5.55517229\n",
      "Iteration 19516, loss = 5.41192408\n",
      "Iteration 19517, loss = 5.92935435\n",
      "Iteration 19518, loss = 5.41602627\n",
      "Iteration 19519, loss = 5.34367453\n",
      "Iteration 19520, loss = 5.32904170\n",
      "Iteration 19521, loss = 5.56310521\n",
      "Iteration 19522, loss = 5.00558830\n",
      "Iteration 19523, loss = 6.27032316\n",
      "Iteration 19524, loss = 5.52021172\n",
      "Iteration 19525, loss = 6.27314110\n",
      "Iteration 19526, loss = 5.61723196\n",
      "Iteration 19527, loss = 5.37143011\n",
      "Iteration 19528, loss = 5.63289972\n",
      "Iteration 19529, loss = 5.60653495\n",
      "Iteration 19530, loss = 6.29443121\n",
      "Iteration 19531, loss = 5.67398672\n",
      "Iteration 19532, loss = 6.16484871\n",
      "Iteration 19533, loss = 5.63590369\n",
      "Iteration 19534, loss = 6.72622435\n",
      "Iteration 19535, loss = 6.22518124\n",
      "Iteration 19536, loss = 5.62027584\n",
      "Iteration 19537, loss = 5.80314220\n",
      "Iteration 19538, loss = 5.92126293\n",
      "Iteration 19539, loss = 6.72238241\n",
      "Iteration 19540, loss = 5.84552863\n",
      "Iteration 19541, loss = 5.87379074\n",
      "Iteration 19542, loss = 5.85753786\n",
      "Iteration 19543, loss = 5.74230663\n",
      "Iteration 19544, loss = 6.06661430\n",
      "Iteration 19545, loss = 5.87367617\n",
      "Iteration 19546, loss = 5.98087282\n",
      "Iteration 19547, loss = 6.05608923\n",
      "Iteration 19548, loss = 5.95576586\n",
      "Iteration 19549, loss = 6.54139741\n",
      "Iteration 19550, loss = 6.51281868\n",
      "Iteration 19551, loss = 6.62101643\n",
      "Iteration 19552, loss = 5.38664298\n",
      "Iteration 19553, loss = 5.74356347\n",
      "Iteration 19554, loss = 5.75325055\n",
      "Iteration 19555, loss = 5.51099148\n",
      "Iteration 19556, loss = 5.62314131\n",
      "Iteration 19557, loss = 5.75104219\n",
      "Iteration 19558, loss = 5.47825688\n",
      "Iteration 19559, loss = 5.04473086\n",
      "Iteration 19560, loss = 5.45815179\n",
      "Iteration 19561, loss = 5.66311326\n",
      "Iteration 19562, loss = 5.47695136\n",
      "Iteration 19563, loss = 5.40297925\n",
      "Iteration 19564, loss = 5.26645468\n",
      "Iteration 19565, loss = 6.54059110\n",
      "Iteration 19566, loss = 5.64489144\n",
      "Iteration 19567, loss = 6.38315298\n",
      "Iteration 19568, loss = 5.86982354\n",
      "Iteration 19569, loss = 5.35903923\n",
      "Iteration 19570, loss = 5.98288426\n",
      "Iteration 19571, loss = 5.89729418\n",
      "Iteration 19572, loss = 5.60756006\n",
      "Iteration 19573, loss = 6.02613131\n",
      "Iteration 19574, loss = 5.79886493\n",
      "Iteration 19575, loss = 5.67301856\n",
      "Iteration 19576, loss = 5.52085430\n",
      "Iteration 19577, loss = 6.11568302\n",
      "Iteration 19578, loss = 5.86874533\n",
      "Iteration 19579, loss = 6.15116213\n",
      "Iteration 19580, loss = 6.08248331\n",
      "Iteration 19581, loss = 5.62704742\n",
      "Iteration 19582, loss = 5.08219940\n",
      "Iteration 19583, loss = 5.82039351\n",
      "Iteration 19584, loss = 5.68391777\n",
      "Iteration 19585, loss = 5.66834250\n",
      "Iteration 19586, loss = 5.35540649\n",
      "Iteration 19587, loss = 6.04189659\n",
      "Iteration 19588, loss = 5.39758873\n",
      "Iteration 19589, loss = 5.68366369\n",
      "Iteration 19590, loss = 5.82647566\n",
      "Iteration 19591, loss = 6.03020979\n",
      "Iteration 19592, loss = 5.53540427\n",
      "Iteration 19593, loss = 5.33047751\n",
      "Iteration 19594, loss = 5.52785333\n",
      "Iteration 19595, loss = 5.18893561\n",
      "Iteration 19596, loss = 5.21459589\n",
      "Iteration 19597, loss = 5.68957857\n",
      "Iteration 19598, loss = 6.37840352\n",
      "Iteration 19599, loss = 6.12704321\n",
      "Iteration 19600, loss = 6.20962783\n",
      "Iteration 19601, loss = 6.66885880\n",
      "Iteration 19602, loss = 5.99453785\n",
      "Iteration 19603, loss = 5.63122921\n",
      "Iteration 19604, loss = 5.79296822\n",
      "Iteration 19605, loss = 5.42087281\n",
      "Iteration 19606, loss = 5.50727005\n",
      "Iteration 19607, loss = 6.27096531\n",
      "Iteration 19608, loss = 6.81959980\n",
      "Iteration 19609, loss = 6.00947913\n",
      "Iteration 19610, loss = 6.25961444\n",
      "Iteration 19611, loss = 6.48018680\n",
      "Iteration 19612, loss = 6.51339642\n",
      "Iteration 19613, loss = 5.73748807\n",
      "Iteration 19614, loss = 5.57259459\n",
      "Iteration 19615, loss = 5.46195984\n",
      "Iteration 19616, loss = 5.31449845\n",
      "Iteration 19617, loss = 5.42767825\n",
      "Iteration 19618, loss = 5.58285383\n",
      "Iteration 19619, loss = 6.02573220\n",
      "Iteration 19620, loss = 7.21174580\n",
      "Iteration 19621, loss = 8.24337869\n",
      "Iteration 19622, loss = 5.87624304\n",
      "Iteration 19623, loss = 5.46761398\n",
      "Iteration 19624, loss = 5.97265823\n",
      "Iteration 19625, loss = 5.49375444\n",
      "Iteration 19626, loss = 5.23594549\n",
      "Iteration 19627, loss = 5.34496184\n",
      "Iteration 19628, loss = 5.57066172\n",
      "Iteration 19629, loss = 6.66718659\n",
      "Iteration 19630, loss = 6.80813309\n",
      "Iteration 19631, loss = 5.91433787\n",
      "Iteration 19632, loss = 5.49442076\n",
      "Iteration 19633, loss = 6.36110007\n",
      "Iteration 19634, loss = 6.78881350\n",
      "Iteration 19635, loss = 6.06578853\n",
      "Iteration 19636, loss = 5.64053312\n",
      "Iteration 19637, loss = 5.94196692\n",
      "Iteration 19638, loss = 5.73778006\n",
      "Iteration 19639, loss = 7.85223326\n",
      "Iteration 19640, loss = 8.42486003\n",
      "Iteration 19641, loss = 7.50200174\n",
      "Iteration 19642, loss = 6.58181192\n",
      "Iteration 19643, loss = 6.62457635\n",
      "Iteration 19644, loss = 6.39979611\n",
      "Iteration 19645, loss = 5.43952125\n",
      "Iteration 19646, loss = 5.70278736\n",
      "Iteration 19647, loss = 5.52523462\n",
      "Iteration 19648, loss = 5.43187912\n",
      "Iteration 19649, loss = 6.04532211\n",
      "Iteration 19650, loss = 5.35267232\n",
      "Iteration 19651, loss = 5.56001182\n",
      "Iteration 19652, loss = 5.18989309\n",
      "Iteration 19653, loss = 5.25159551\n",
      "Iteration 19654, loss = 5.57604125\n",
      "Iteration 19655, loss = 5.99695627\n",
      "Iteration 19656, loss = 5.58852487\n",
      "Iteration 19657, loss = 5.50606506\n",
      "Iteration 19658, loss = 5.62610217\n",
      "Iteration 19659, loss = 5.73903151\n",
      "Iteration 19660, loss = 5.86496827\n",
      "Iteration 19661, loss = 5.57128262\n",
      "Iteration 19662, loss = 5.36538004\n",
      "Iteration 19663, loss = 5.47596302\n",
      "Iteration 19664, loss = 5.40316738\n",
      "Iteration 19665, loss = 5.34345646\n",
      "Iteration 19666, loss = 5.46607648\n",
      "Iteration 19667, loss = 5.72526897\n",
      "Iteration 19668, loss = 6.07082354\n",
      "Iteration 19669, loss = 5.64312369\n",
      "Iteration 19670, loss = 5.31370049\n",
      "Iteration 19671, loss = 6.09218733\n",
      "Iteration 19672, loss = 5.49831254\n",
      "Iteration 19673, loss = 5.95639909\n",
      "Iteration 19674, loss = 5.86837453\n",
      "Iteration 19675, loss = 5.89772028\n",
      "Iteration 19676, loss = 6.35113142\n",
      "Iteration 19677, loss = 6.17864234\n",
      "Iteration 19678, loss = 5.90522534\n",
      "Iteration 19679, loss = 6.05401686\n",
      "Iteration 19680, loss = 6.68087283\n",
      "Iteration 19681, loss = 6.06899462\n",
      "Iteration 19682, loss = 6.52955324\n",
      "Iteration 19683, loss = 6.27843587\n",
      "Iteration 19684, loss = 6.34768766\n",
      "Iteration 19685, loss = 6.76589999\n",
      "Iteration 19686, loss = 7.21771232\n",
      "Iteration 19687, loss = 6.59248025\n",
      "Iteration 19688, loss = 6.94077823\n",
      "Iteration 19689, loss = 6.35226587\n",
      "Iteration 19690, loss = 7.20744198\n",
      "Iteration 19691, loss = 5.97179302\n",
      "Iteration 19692, loss = 6.00229292\n",
      "Iteration 19693, loss = 5.28818912\n",
      "Iteration 19694, loss = 5.57890887\n",
      "Iteration 19695, loss = 5.47146829\n",
      "Iteration 19696, loss = 6.16778121\n",
      "Iteration 19697, loss = 5.90687948\n",
      "Iteration 19698, loss = 5.81426715\n",
      "Iteration 19699, loss = 5.47718564\n",
      "Iteration 19700, loss = 5.69394336\n",
      "Iteration 19701, loss = 5.77040588\n",
      "Iteration 19702, loss = 7.54753989\n",
      "Iteration 19703, loss = 6.64805858\n",
      "Iteration 19704, loss = 6.19496079\n",
      "Iteration 19705, loss = 6.48433441\n",
      "Iteration 19706, loss = 6.98133916\n",
      "Iteration 19707, loss = 5.71753069\n",
      "Iteration 19708, loss = 5.73697107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19709, loss = 5.41861337\n",
      "Iteration 19710, loss = 6.28632297\n",
      "Iteration 19711, loss = 8.76828572\n",
      "Iteration 19712, loss = 7.33589708\n",
      "Iteration 19713, loss = 7.35916570\n",
      "Iteration 19714, loss = 6.35275757\n",
      "Iteration 19715, loss = 5.65615915\n",
      "Iteration 19716, loss = 5.49761078\n",
      "Iteration 19717, loss = 5.59538478\n",
      "Iteration 19718, loss = 5.72016479\n",
      "Iteration 19719, loss = 5.60505823\n",
      "Iteration 19720, loss = 5.86369401\n",
      "Iteration 19721, loss = 5.67158412\n",
      "Iteration 19722, loss = 6.19394288\n",
      "Iteration 19723, loss = 6.42842390\n",
      "Iteration 19724, loss = 5.73539760\n",
      "Iteration 19725, loss = 5.95819503\n",
      "Iteration 19726, loss = 5.59118209\n",
      "Iteration 19727, loss = 5.50891914\n",
      "Iteration 19728, loss = 5.63468939\n",
      "Iteration 19729, loss = 5.85919064\n",
      "Iteration 19730, loss = 5.21061002\n",
      "Iteration 19731, loss = 5.24478831\n",
      "Iteration 19732, loss = 5.27669044\n",
      "Iteration 19733, loss = 5.19981950\n",
      "Iteration 19734, loss = 5.37180115\n",
      "Iteration 19735, loss = 5.25901848\n",
      "Iteration 19736, loss = 5.58400190\n",
      "Iteration 19737, loss = 5.28134814\n",
      "Iteration 19738, loss = 5.27956789\n",
      "Iteration 19739, loss = 5.09403506\n",
      "Iteration 19740, loss = 5.66258012\n",
      "Iteration 19741, loss = 5.68702207\n",
      "Iteration 19742, loss = 5.85523882\n",
      "Iteration 19743, loss = 6.31079557\n",
      "Iteration 19744, loss = 7.75382697\n",
      "Iteration 19745, loss = 7.09302625\n",
      "Iteration 19746, loss = 7.28355030\n",
      "Iteration 19747, loss = 6.68039596\n",
      "Iteration 19748, loss = 6.53903257\n",
      "Iteration 19749, loss = 7.11914879\n",
      "Iteration 19750, loss = 7.76729635\n",
      "Iteration 19751, loss = 6.92579118\n",
      "Iteration 19752, loss = 6.48422036\n",
      "Iteration 19753, loss = 7.46360325\n",
      "Iteration 19754, loss = 6.38046592\n",
      "Iteration 19755, loss = 6.24855455\n",
      "Iteration 19756, loss = 5.38811408\n",
      "Iteration 19757, loss = 5.95084173\n",
      "Iteration 19758, loss = 5.23709879\n",
      "Iteration 19759, loss = 5.42854470\n",
      "Iteration 19760, loss = 5.51651287\n",
      "Iteration 19761, loss = 5.88220914\n",
      "Iteration 19762, loss = 6.54867356\n",
      "Iteration 19763, loss = 8.63030650\n",
      "Iteration 19764, loss = 9.44516981\n",
      "Iteration 19765, loss = 7.72459884\n",
      "Iteration 19766, loss = 8.71133780\n",
      "Iteration 19767, loss = 6.83305964\n",
      "Iteration 19768, loss = 6.19058445\n",
      "Iteration 19769, loss = 5.92255711\n",
      "Iteration 19770, loss = 6.88950888\n",
      "Iteration 19771, loss = 6.20656698\n",
      "Iteration 19772, loss = 7.13364602\n",
      "Iteration 19773, loss = 6.56644908\n",
      "Iteration 19774, loss = 6.63889572\n",
      "Iteration 19775, loss = 5.79960771\n",
      "Iteration 19776, loss = 5.85279485\n",
      "Iteration 19777, loss = 5.82611794\n",
      "Iteration 19778, loss = 5.35553109\n",
      "Iteration 19779, loss = 5.45390459\n",
      "Iteration 19780, loss = 5.15969023\n",
      "Iteration 19781, loss = 5.42892142\n",
      "Iteration 19782, loss = 6.22927943\n",
      "Iteration 19783, loss = 6.12585524\n",
      "Iteration 19784, loss = 7.24993963\n",
      "Iteration 19785, loss = 5.82430798\n",
      "Iteration 19786, loss = 6.98898588\n",
      "Iteration 19787, loss = 6.60411297\n",
      "Iteration 19788, loss = 6.72077166\n",
      "Iteration 19789, loss = 5.75342855\n",
      "Iteration 19790, loss = 5.33522781\n",
      "Iteration 19791, loss = 5.49053972\n",
      "Iteration 19792, loss = 6.75288393\n",
      "Iteration 19793, loss = 5.10637701\n",
      "Iteration 19794, loss = 5.68080039\n",
      "Iteration 19795, loss = 5.37938235\n",
      "Iteration 19796, loss = 5.66154593\n",
      "Iteration 19797, loss = 5.38232253\n",
      "Iteration 19798, loss = 5.37727277\n",
      "Iteration 19799, loss = 5.15825970\n",
      "Iteration 19800, loss = 5.09281128\n",
      "Iteration 19801, loss = 5.28789638\n",
      "Iteration 19802, loss = 5.50273459\n",
      "Iteration 19803, loss = 5.48109343\n",
      "Iteration 19804, loss = 5.62222932\n",
      "Iteration 19805, loss = 5.33166558\n",
      "Iteration 19806, loss = 5.33519432\n",
      "Iteration 19807, loss = 5.50643462\n",
      "Iteration 19808, loss = 5.57470657\n",
      "Iteration 19809, loss = 5.19899783\n",
      "Iteration 19810, loss = 5.22430838\n",
      "Iteration 19811, loss = 5.73150161\n",
      "Iteration 19812, loss = 5.54248220\n",
      "Iteration 19813, loss = 6.43165673\n",
      "Iteration 19814, loss = 6.10942509\n",
      "Iteration 19815, loss = 5.92242711\n",
      "Iteration 19816, loss = 5.88712017\n",
      "Iteration 19817, loss = 5.82929447\n",
      "Iteration 19818, loss = 6.27877038\n",
      "Iteration 19819, loss = 5.49545281\n",
      "Iteration 19820, loss = 5.50744165\n",
      "Iteration 19821, loss = 5.52866118\n",
      "Iteration 19822, loss = 5.33285229\n",
      "Iteration 19823, loss = 5.10336355\n",
      "Iteration 19824, loss = 5.65933917\n",
      "Iteration 19825, loss = 5.70637708\n",
      "Iteration 19826, loss = 5.72275519\n",
      "Iteration 19827, loss = 5.64209784\n",
      "Iteration 19828, loss = 5.50191128\n",
      "Iteration 19829, loss = 5.67065962\n",
      "Iteration 19830, loss = 5.66071010\n",
      "Iteration 19831, loss = 6.24147520\n",
      "Iteration 19832, loss = 5.88240200\n",
      "Iteration 19833, loss = 5.69501689\n",
      "Iteration 19834, loss = 6.92241688\n",
      "Iteration 19835, loss = 6.62938856\n",
      "Iteration 19836, loss = 6.67870852\n",
      "Iteration 19837, loss = 6.75808486\n",
      "Iteration 19838, loss = 6.87020875\n",
      "Iteration 19839, loss = 5.92477364\n",
      "Iteration 19840, loss = 5.84452349\n",
      "Iteration 19841, loss = 6.09995584\n",
      "Iteration 19842, loss = 5.74440205\n",
      "Iteration 19843, loss = 6.15670111\n",
      "Iteration 19844, loss = 5.48835390\n",
      "Iteration 19845, loss = 5.62356181\n",
      "Iteration 19846, loss = 5.91535800\n",
      "Iteration 19847, loss = 5.78335502\n",
      "Iteration 19848, loss = 6.20566262\n",
      "Iteration 19849, loss = 5.39983401\n",
      "Iteration 19850, loss = 5.54424294\n",
      "Iteration 19851, loss = 5.08390614\n",
      "Iteration 19852, loss = 5.39580306\n",
      "Iteration 19853, loss = 5.31631656\n",
      "Iteration 19854, loss = 5.06434949\n",
      "Iteration 19855, loss = 5.56777150\n",
      "Iteration 19856, loss = 5.65979872\n",
      "Iteration 19857, loss = 5.37229508\n",
      "Iteration 19858, loss = 5.46576965\n",
      "Iteration 19859, loss = 5.52462855\n",
      "Iteration 19860, loss = 5.08210009\n",
      "Iteration 19861, loss = 5.32229713\n",
      "Iteration 19862, loss = 5.25221226\n",
      "Iteration 19863, loss = 5.57184233\n",
      "Iteration 19864, loss = 5.39094933\n",
      "Iteration 19865, loss = 6.04297231\n",
      "Iteration 19866, loss = 5.15012239\n",
      "Iteration 19867, loss = 5.96247071\n",
      "Iteration 19868, loss = 5.50919556\n",
      "Iteration 19869, loss = 5.29313687\n",
      "Iteration 19870, loss = 5.44955500\n",
      "Iteration 19871, loss = 5.67484112\n",
      "Iteration 19872, loss = 5.69596841\n",
      "Iteration 19873, loss = 5.04775756\n",
      "Iteration 19874, loss = 5.36829109\n",
      "Iteration 19875, loss = 5.42717962\n",
      "Iteration 19876, loss = 5.39781156\n",
      "Iteration 19877, loss = 5.16240369\n",
      "Iteration 19878, loss = 5.25218077\n",
      "Iteration 19879, loss = 5.34337295\n",
      "Iteration 19880, loss = 5.22960949\n",
      "Iteration 19881, loss = 5.14273495\n",
      "Iteration 19882, loss = 5.04131064\n",
      "Iteration 19883, loss = 5.44526039\n",
      "Iteration 19884, loss = 5.04003283\n",
      "Iteration 19885, loss = 5.34021712\n",
      "Iteration 19886, loss = 5.62885168\n",
      "Iteration 19887, loss = 5.05584758\n",
      "Iteration 19888, loss = 5.78713828\n",
      "Iteration 19889, loss = 8.89251901\n",
      "Iteration 19890, loss = 9.41909047\n",
      "Iteration 19891, loss = 10.43442017\n",
      "Iteration 19892, loss = 9.44175817\n",
      "Iteration 19893, loss = 10.14525736\n",
      "Iteration 19894, loss = 8.50653752\n",
      "Iteration 19895, loss = 6.88936941\n",
      "Iteration 19896, loss = 6.50919513\n",
      "Iteration 19897, loss = 5.95129088\n",
      "Iteration 19898, loss = 5.81580795\n",
      "Iteration 19899, loss = 5.94784460\n",
      "Iteration 19900, loss = 6.13072084\n",
      "Iteration 19901, loss = 5.88116081\n",
      "Iteration 19902, loss = 5.60943050\n",
      "Iteration 19903, loss = 6.57823525\n",
      "Iteration 19904, loss = 6.01545473\n",
      "Iteration 19905, loss = 6.78589938\n",
      "Iteration 19906, loss = 5.78936273\n",
      "Iteration 19907, loss = 5.11349812\n",
      "Iteration 19908, loss = 5.72932195\n",
      "Iteration 19909, loss = 5.25859947\n",
      "Iteration 19910, loss = 5.31641627\n",
      "Iteration 19911, loss = 5.62808028\n",
      "Iteration 19912, loss = 5.63459285\n",
      "Iteration 19913, loss = 5.60119873\n",
      "Iteration 19914, loss = 5.20645415\n",
      "Iteration 19915, loss = 5.44782191\n",
      "Iteration 19916, loss = 5.19877497\n",
      "Iteration 19917, loss = 6.22601515\n",
      "Iteration 19918, loss = 6.10190807\n",
      "Iteration 19919, loss = 5.42230372\n",
      "Iteration 19920, loss = 5.81650514\n",
      "Iteration 19921, loss = 5.39123905\n",
      "Iteration 19922, loss = 5.06756224\n",
      "Iteration 19923, loss = 5.11124808\n",
      "Iteration 19924, loss = 5.16676620\n",
      "Iteration 19925, loss = 5.82545747\n",
      "Iteration 19926, loss = 5.86693856\n",
      "Iteration 19927, loss = 6.34191857\n",
      "Iteration 19928, loss = 6.73508565\n",
      "Iteration 19929, loss = 6.72391044\n",
      "Iteration 19930, loss = 5.74959932\n",
      "Iteration 19931, loss = 5.72571757\n",
      "Iteration 19932, loss = 6.23219696\n",
      "Iteration 19933, loss = 5.99457929\n",
      "Iteration 19934, loss = 5.97996788\n",
      "Iteration 19935, loss = 5.30416342\n",
      "Iteration 19936, loss = 5.21374854\n",
      "Iteration 19937, loss = 5.50494127\n",
      "Iteration 19938, loss = 5.87991751\n",
      "Iteration 19939, loss = 5.49904328\n",
      "Iteration 19940, loss = 5.63455043\n",
      "Iteration 19941, loss = 5.73106309\n",
      "Iteration 19942, loss = 5.68035387\n",
      "Iteration 19943, loss = 5.29537577\n",
      "Iteration 19944, loss = 5.68240336\n",
      "Iteration 19945, loss = 5.22396524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19946, loss = 5.60287023\n",
      "Iteration 19947, loss = 5.34652791\n",
      "Iteration 19948, loss = 5.53434146\n",
      "Iteration 19949, loss = 5.17586012\n",
      "Iteration 19950, loss = 5.47540103\n",
      "Iteration 19951, loss = 5.43890105\n",
      "Iteration 19952, loss = 5.33885966\n",
      "Iteration 19953, loss = 5.23684977\n",
      "Iteration 19954, loss = 5.59490039\n",
      "Iteration 19955, loss = 5.64796125\n",
      "Iteration 19956, loss = 5.18581682\n",
      "Iteration 19957, loss = 5.11899010\n",
      "Iteration 19958, loss = 5.47533928\n",
      "Iteration 19959, loss = 6.42143057\n",
      "Iteration 19960, loss = 6.10775081\n",
      "Iteration 19961, loss = 6.17717296\n",
      "Iteration 19962, loss = 5.79412031\n",
      "Iteration 19963, loss = 5.32789156\n",
      "Iteration 19964, loss = 5.47363331\n",
      "Iteration 19965, loss = 5.65705396\n",
      "Iteration 19966, loss = 6.02352869\n",
      "Iteration 19967, loss = 6.79499135\n",
      "Iteration 19968, loss = 6.14962221\n",
      "Iteration 19969, loss = 7.42914646\n",
      "Iteration 19970, loss = 7.93886810\n",
      "Iteration 19971, loss = 6.55470250\n",
      "Iteration 19972, loss = 6.45700022\n",
      "Iteration 19973, loss = 5.52055375\n",
      "Iteration 19974, loss = 6.49436206\n",
      "Iteration 19975, loss = 6.42627214\n",
      "Iteration 19976, loss = 6.43066058\n",
      "Iteration 19977, loss = 5.74564293\n",
      "Iteration 19978, loss = 5.49842017\n",
      "Iteration 19979, loss = 5.46797130\n",
      "Iteration 19980, loss = 5.38164545\n",
      "Iteration 19981, loss = 5.40236333\n",
      "Iteration 19982, loss = 5.24198084\n",
      "Iteration 19983, loss = 5.44972546\n",
      "Iteration 19984, loss = 5.55359013\n",
      "Iteration 19985, loss = 6.45041736\n",
      "Iteration 19986, loss = 6.10402329\n",
      "Iteration 19987, loss = 5.67445552\n",
      "Iteration 19988, loss = 5.70037096\n",
      "Iteration 19989, loss = 5.51258871\n",
      "Iteration 19990, loss = 6.01632788\n",
      "Iteration 19991, loss = 6.55717663\n",
      "Iteration 19992, loss = 6.52683459\n",
      "Iteration 19993, loss = 5.84081313\n",
      "Iteration 19994, loss = 5.92821361\n",
      "Iteration 19995, loss = 5.52135162\n",
      "Iteration 19996, loss = 5.65690389\n",
      "Iteration 19997, loss = 5.80718099\n",
      "Iteration 19998, loss = 5.18234426\n",
      "Iteration 19999, loss = 5.12330942\n",
      "Iteration 20000, loss = 5.39317220\n",
      "Iteration 20001, loss = 5.40331217\n",
      "Iteration 20002, loss = 5.48068441\n",
      "Iteration 20003, loss = 5.28547866\n",
      "Iteration 20004, loss = 5.17069223\n",
      "Iteration 20005, loss = 5.76857169\n",
      "Iteration 20006, loss = 5.67142631\n",
      "Iteration 20007, loss = 5.79848732\n",
      "Iteration 20008, loss = 7.05828158\n",
      "Iteration 20009, loss = 7.16363821\n",
      "Iteration 20010, loss = 5.72631002\n",
      "Iteration 20011, loss = 5.32749540\n",
      "Iteration 20012, loss = 5.36945240\n",
      "Iteration 20013, loss = 5.47861749\n",
      "Iteration 20014, loss = 5.48609908\n",
      "Iteration 20015, loss = 5.07713363\n",
      "Iteration 20016, loss = 5.06388006\n",
      "Iteration 20017, loss = 5.16621771\n",
      "Iteration 20018, loss = 6.06042193\n",
      "Iteration 20019, loss = 6.70721161\n",
      "Iteration 20020, loss = 6.36805605\n",
      "Iteration 20021, loss = 5.89452268\n",
      "Iteration 20022, loss = 5.98755022\n",
      "Iteration 20023, loss = 6.05247277\n",
      "Iteration 20024, loss = 5.62223130\n",
      "Iteration 20025, loss = 5.67789932\n",
      "Iteration 20026, loss = 5.44646526\n",
      "Iteration 20027, loss = 6.00036281\n",
      "Iteration 20028, loss = 5.96786923\n",
      "Iteration 20029, loss = 5.16941946\n",
      "Iteration 20030, loss = 5.28388796\n",
      "Iteration 20031, loss = 5.86256580\n",
      "Iteration 20032, loss = 5.33793595\n",
      "Iteration 20033, loss = 5.15086301\n",
      "Iteration 20034, loss = 5.64761410\n",
      "Iteration 20035, loss = 5.65986769\n",
      "Iteration 20036, loss = 5.10377592\n",
      "Iteration 20037, loss = 5.09433342\n",
      "Iteration 20038, loss = 5.22340807\n",
      "Iteration 20039, loss = 5.74005865\n",
      "Iteration 20040, loss = 6.45949707\n",
      "Iteration 20041, loss = 5.85893416\n",
      "Iteration 20042, loss = 5.65053325\n",
      "Iteration 20043, loss = 5.50976675\n",
      "Iteration 20044, loss = 5.52088658\n",
      "Iteration 20045, loss = 5.70035000\n",
      "Iteration 20046, loss = 5.92134783\n",
      "Iteration 20047, loss = 5.53626998\n",
      "Iteration 20048, loss = 5.51629443\n",
      "Iteration 20049, loss = 5.65266556\n",
      "Iteration 20050, loss = 5.31613672\n",
      "Iteration 20051, loss = 5.43758664\n",
      "Iteration 20052, loss = 5.93115947\n",
      "Iteration 20053, loss = 6.38272718\n",
      "Iteration 20054, loss = 6.21527405\n",
      "Iteration 20055, loss = 6.45582806\n",
      "Iteration 20056, loss = 6.91233748\n",
      "Iteration 20057, loss = 6.21766469\n",
      "Iteration 20058, loss = 5.59522644\n",
      "Iteration 20059, loss = 6.46121539\n",
      "Iteration 20060, loss = 5.50924092\n",
      "Iteration 20061, loss = 5.39286972\n",
      "Iteration 20062, loss = 5.20370523\n",
      "Iteration 20063, loss = 5.26545705\n",
      "Iteration 20064, loss = 5.78788140\n",
      "Iteration 20065, loss = 5.76928983\n",
      "Iteration 20066, loss = 6.09087956\n",
      "Iteration 20067, loss = 6.68472728\n",
      "Iteration 20068, loss = 7.84235094\n",
      "Iteration 20069, loss = 6.68939345\n",
      "Iteration 20070, loss = 6.26051306\n",
      "Iteration 20071, loss = 6.51263323\n",
      "Iteration 20072, loss = 5.80518818\n",
      "Iteration 20073, loss = 5.82564686\n",
      "Iteration 20074, loss = 5.83677229\n",
      "Iteration 20075, loss = 5.76012446\n",
      "Iteration 20076, loss = 5.41980307\n",
      "Iteration 20077, loss = 5.62093778\n",
      "Iteration 20078, loss = 5.24744534\n",
      "Iteration 20079, loss = 5.63111831\n",
      "Iteration 20080, loss = 5.59257055\n",
      "Iteration 20081, loss = 5.09185593\n",
      "Iteration 20082, loss = 5.21221466\n",
      "Iteration 20083, loss = 5.40253967\n",
      "Iteration 20084, loss = 5.40333656\n",
      "Iteration 20085, loss = 6.16202370\n",
      "Iteration 20086, loss = 5.60655304\n",
      "Iteration 20087, loss = 5.17812478\n",
      "Iteration 20088, loss = 5.49927995\n",
      "Iteration 20089, loss = 5.36466086\n",
      "Iteration 20090, loss = 6.29168241\n",
      "Iteration 20091, loss = 5.85139384\n",
      "Iteration 20092, loss = 5.44206538\n",
      "Iteration 20093, loss = 5.48678841\n",
      "Iteration 20094, loss = 5.64214471\n",
      "Iteration 20095, loss = 5.50889853\n",
      "Iteration 20096, loss = 5.47378629\n",
      "Iteration 20097, loss = 5.75977798\n",
      "Iteration 20098, loss = 5.21905362\n",
      "Iteration 20099, loss = 5.39651054\n",
      "Iteration 20100, loss = 5.90030259\n",
      "Iteration 20101, loss = 5.77722561\n",
      "Iteration 20102, loss = 5.94782380\n",
      "Iteration 20103, loss = 6.84407507\n",
      "Iteration 20104, loss = 7.28816384\n",
      "Iteration 20105, loss = 6.55453320\n",
      "Iteration 20106, loss = 6.84019077\n",
      "Iteration 20107, loss = 6.51598617\n",
      "Iteration 20108, loss = 7.64021203\n",
      "Iteration 20109, loss = 6.90153646\n",
      "Iteration 20110, loss = 6.03280558\n",
      "Iteration 20111, loss = 6.36295448\n",
      "Iteration 20112, loss = 5.63523071\n",
      "Iteration 20113, loss = 5.64087836\n",
      "Iteration 20114, loss = 5.86565665\n",
      "Iteration 20115, loss = 5.58597981\n",
      "Iteration 20116, loss = 5.90622260\n",
      "Iteration 20117, loss = 6.33961081\n",
      "Iteration 20118, loss = 5.63383020\n",
      "Iteration 20119, loss = 6.41104273\n",
      "Iteration 20120, loss = 6.52494551\n",
      "Iteration 20121, loss = 6.74242096\n",
      "Iteration 20122, loss = 6.42525781\n",
      "Iteration 20123, loss = 6.47536128\n",
      "Iteration 20124, loss = 6.05923157\n",
      "Iteration 20125, loss = 6.18443338\n",
      "Iteration 20126, loss = 6.48678719\n",
      "Iteration 20127, loss = 5.44544778\n",
      "Iteration 20128, loss = 5.21632178\n",
      "Iteration 20129, loss = 5.15597805\n",
      "Iteration 20130, loss = 5.73705625\n",
      "Iteration 20131, loss = 5.33955329\n",
      "Iteration 20132, loss = 5.60045367\n",
      "Iteration 20133, loss = 5.35625963\n",
      "Iteration 20134, loss = 5.51336701\n",
      "Iteration 20135, loss = 5.47179956\n",
      "Iteration 20136, loss = 5.73617141\n",
      "Iteration 20137, loss = 5.69849169\n",
      "Iteration 20138, loss = 5.92115415\n",
      "Iteration 20139, loss = 5.69669963\n",
      "Iteration 20140, loss = 5.63757931\n",
      "Iteration 20141, loss = 5.33951988\n",
      "Iteration 20142, loss = 5.24188280\n",
      "Iteration 20143, loss = 5.31178381\n",
      "Iteration 20144, loss = 6.12997034\n",
      "Iteration 20145, loss = 5.31680106\n",
      "Iteration 20146, loss = 5.82457017\n",
      "Iteration 20147, loss = 5.60349329\n",
      "Iteration 20148, loss = 5.47771000\n",
      "Iteration 20149, loss = 5.03155155\n",
      "Iteration 20150, loss = 5.07424981\n",
      "Iteration 20151, loss = 5.45384151\n",
      "Iteration 20152, loss = 5.93075049\n",
      "Iteration 20153, loss = 5.71336864\n",
      "Iteration 20154, loss = 5.21126410\n",
      "Iteration 20155, loss = 5.52808048\n",
      "Iteration 20156, loss = 5.49910659\n",
      "Iteration 20157, loss = 5.84882742\n",
      "Iteration 20158, loss = 7.16205565\n",
      "Iteration 20159, loss = 7.35505528\n",
      "Iteration 20160, loss = 6.26366439\n",
      "Iteration 20161, loss = 5.83057213\n",
      "Iteration 20162, loss = 5.93169210\n",
      "Iteration 20163, loss = 5.35724834\n",
      "Iteration 20164, loss = 5.88082025\n",
      "Iteration 20165, loss = 5.88580468\n",
      "Iteration 20166, loss = 6.18759877\n",
      "Iteration 20167, loss = 6.05953489\n",
      "Iteration 20168, loss = 6.27947398\n",
      "Iteration 20169, loss = 6.15031839\n",
      "Iteration 20170, loss = 5.22538682\n",
      "Iteration 20171, loss = 5.51401042\n",
      "Iteration 20172, loss = 6.19698219\n",
      "Iteration 20173, loss = 5.13386661\n",
      "Iteration 20174, loss = 5.29783710\n",
      "Iteration 20175, loss = 5.37803447\n",
      "Iteration 20176, loss = 5.75066168\n",
      "Iteration 20177, loss = 5.59016543\n",
      "Iteration 20178, loss = 5.57202197\n",
      "Iteration 20179, loss = 6.30908265\n",
      "Iteration 20180, loss = 5.93587818\n",
      "Iteration 20181, loss = 5.67328802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20182, loss = 5.68839735\n",
      "Iteration 20183, loss = 5.60884495\n",
      "Iteration 20184, loss = 6.12223899\n",
      "Iteration 20185, loss = 5.59149911\n",
      "Iteration 20186, loss = 5.66021095\n",
      "Iteration 20187, loss = 5.31470941\n",
      "Iteration 20188, loss = 5.35528147\n",
      "Iteration 20189, loss = 5.46721244\n",
      "Iteration 20190, loss = 5.55209272\n",
      "Iteration 20191, loss = 5.70596935\n",
      "Iteration 20192, loss = 5.22361724\n",
      "Iteration 20193, loss = 6.00844102\n",
      "Iteration 20194, loss = 6.82515264\n",
      "Iteration 20195, loss = 6.39273974\n",
      "Iteration 20196, loss = 5.36788728\n",
      "Iteration 20197, loss = 5.92792213\n",
      "Iteration 20198, loss = 5.76362430\n",
      "Iteration 20199, loss = 5.26081741\n",
      "Iteration 20200, loss = 5.29797242\n",
      "Iteration 20201, loss = 5.08161662\n",
      "Iteration 20202, loss = 5.30753228\n",
      "Iteration 20203, loss = 5.96536904\n",
      "Iteration 20204, loss = 6.55913856\n",
      "Iteration 20205, loss = 5.98861633\n",
      "Iteration 20206, loss = 6.23391815\n",
      "Iteration 20207, loss = 6.29458900\n",
      "Iteration 20208, loss = 5.36149161\n",
      "Iteration 20209, loss = 5.56016672\n",
      "Iteration 20210, loss = 5.08042704\n",
      "Iteration 20211, loss = 5.77457721\n",
      "Iteration 20212, loss = 5.32459025\n",
      "Iteration 20213, loss = 5.53906657\n",
      "Iteration 20214, loss = 6.06083217\n",
      "Iteration 20215, loss = 5.99062772\n",
      "Iteration 20216, loss = 5.51181926\n",
      "Iteration 20217, loss = 5.73378974\n",
      "Iteration 20218, loss = 5.95163666\n",
      "Iteration 20219, loss = 5.84289316\n",
      "Iteration 20220, loss = 5.42320571\n",
      "Iteration 20221, loss = 5.58095626\n",
      "Iteration 20222, loss = 5.35678546\n",
      "Iteration 20223, loss = 5.47438514\n",
      "Iteration 20224, loss = 5.30196155\n",
      "Iteration 20225, loss = 5.19228594\n",
      "Iteration 20226, loss = 5.62077119\n",
      "Iteration 20227, loss = 5.81484390\n",
      "Iteration 20228, loss = 5.46475370\n",
      "Iteration 20229, loss = 5.60110470\n",
      "Iteration 20230, loss = 5.19528322\n",
      "Iteration 20231, loss = 5.22998731\n",
      "Iteration 20232, loss = 5.72879003\n",
      "Iteration 20233, loss = 5.67722980\n",
      "Iteration 20234, loss = 6.32056662\n",
      "Iteration 20235, loss = 5.32116821\n",
      "Iteration 20236, loss = 6.05535203\n",
      "Iteration 20237, loss = 5.38008521\n",
      "Iteration 20238, loss = 5.12961718\n",
      "Iteration 20239, loss = 5.06875322\n",
      "Iteration 20240, loss = 5.21805224\n",
      "Iteration 20241, loss = 5.28635897\n",
      "Iteration 20242, loss = 5.25535104\n",
      "Iteration 20243, loss = 5.17125640\n",
      "Iteration 20244, loss = 5.55924012\n",
      "Iteration 20245, loss = 6.09652236\n",
      "Iteration 20246, loss = 6.31966151\n",
      "Iteration 20247, loss = 6.40246108\n",
      "Iteration 20248, loss = 5.92322154\n",
      "Iteration 20249, loss = 5.95566440\n",
      "Iteration 20250, loss = 5.87920641\n",
      "Iteration 20251, loss = 5.24599992\n",
      "Iteration 20252, loss = 5.02458028\n",
      "Iteration 20253, loss = 5.24297043\n",
      "Iteration 20254, loss = 5.31944477\n",
      "Iteration 20255, loss = 5.69393369\n",
      "Iteration 20256, loss = 5.56370395\n",
      "Iteration 20257, loss = 5.66264531\n",
      "Iteration 20258, loss = 5.69833675\n",
      "Iteration 20259, loss = 6.13992882\n",
      "Iteration 20260, loss = 6.23408564\n",
      "Iteration 20261, loss = 5.77374791\n",
      "Iteration 20262, loss = 5.58610342\n",
      "Iteration 20263, loss = 5.09652780\n",
      "Iteration 20264, loss = 5.22205665\n",
      "Iteration 20265, loss = 5.43518123\n",
      "Iteration 20266, loss = 5.20904804\n",
      "Iteration 20267, loss = 5.65343079\n",
      "Iteration 20268, loss = 5.89280413\n",
      "Iteration 20269, loss = 5.85233554\n",
      "Iteration 20270, loss = 5.94974778\n",
      "Iteration 20271, loss = 5.57446035\n",
      "Iteration 20272, loss = 5.29636257\n",
      "Iteration 20273, loss = 5.55453151\n",
      "Iteration 20274, loss = 6.34604782\n",
      "Iteration 20275, loss = 5.24047596\n",
      "Iteration 20276, loss = 5.50259713\n",
      "Iteration 20277, loss = 4.98060388\n",
      "Iteration 20278, loss = 5.92912894\n",
      "Iteration 20279, loss = 4.99323948\n",
      "Iteration 20280, loss = 5.39423156\n",
      "Iteration 20281, loss = 5.25335171\n",
      "Iteration 20282, loss = 5.22007475\n",
      "Iteration 20283, loss = 5.85758736\n",
      "Iteration 20284, loss = 6.20917945\n",
      "Iteration 20285, loss = 5.59666456\n",
      "Iteration 20286, loss = 5.94298427\n",
      "Iteration 20287, loss = 5.48587744\n",
      "Iteration 20288, loss = 5.22444469\n",
      "Iteration 20289, loss = 5.44645836\n",
      "Iteration 20290, loss = 5.25254685\n",
      "Iteration 20291, loss = 5.17675102\n",
      "Iteration 20292, loss = 5.31150710\n",
      "Iteration 20293, loss = 5.53924629\n",
      "Iteration 20294, loss = 5.59938953\n",
      "Iteration 20295, loss = 6.31430289\n",
      "Iteration 20296, loss = 5.34614343\n",
      "Iteration 20297, loss = 5.58582840\n",
      "Iteration 20298, loss = 5.38769674\n",
      "Iteration 20299, loss = 5.60424507\n",
      "Iteration 20300, loss = 5.79779298\n",
      "Iteration 20301, loss = 6.44449601\n",
      "Iteration 20302, loss = 5.67236056\n",
      "Iteration 20303, loss = 6.56494542\n",
      "Iteration 20304, loss = 5.98284088\n",
      "Iteration 20305, loss = 6.36828892\n",
      "Iteration 20306, loss = 6.77568069\n",
      "Iteration 20307, loss = 6.88244522\n",
      "Iteration 20308, loss = 5.55380151\n",
      "Iteration 20309, loss = 6.01305009\n",
      "Iteration 20310, loss = 6.45646584\n",
      "Iteration 20311, loss = 5.81876096\n",
      "Iteration 20312, loss = 5.83672261\n",
      "Iteration 20313, loss = 6.68024384\n",
      "Iteration 20314, loss = 6.80595885\n",
      "Iteration 20315, loss = 6.40135874\n",
      "Iteration 20316, loss = 6.85078527\n",
      "Iteration 20317, loss = 6.75036581\n",
      "Iteration 20318, loss = 6.12600430\n",
      "Iteration 20319, loss = 6.14084850\n",
      "Iteration 20320, loss = 6.01384168\n",
      "Iteration 20321, loss = 5.42647469\n",
      "Iteration 20322, loss = 5.48984562\n",
      "Iteration 20323, loss = 6.60353036\n",
      "Iteration 20324, loss = 6.00742084\n",
      "Iteration 20325, loss = 6.23261849\n",
      "Iteration 20326, loss = 5.51674335\n",
      "Iteration 20327, loss = 6.10925881\n",
      "Iteration 20328, loss = 5.80680988\n",
      "Iteration 20329, loss = 5.90153885\n",
      "Iteration 20330, loss = 5.88391696\n",
      "Iteration 20331, loss = 5.87735079\n",
      "Iteration 20332, loss = 5.27111692\n",
      "Iteration 20333, loss = 5.44770928\n",
      "Iteration 20334, loss = 5.94028570\n",
      "Iteration 20335, loss = 6.31360131\n",
      "Iteration 20336, loss = 5.86724439\n",
      "Iteration 20337, loss = 5.91221600\n",
      "Iteration 20338, loss = 5.92772936\n",
      "Iteration 20339, loss = 5.54695694\n",
      "Iteration 20340, loss = 5.85148555\n",
      "Iteration 20341, loss = 5.89652093\n",
      "Iteration 20342, loss = 5.96205652\n",
      "Iteration 20343, loss = 5.93373310\n",
      "Iteration 20344, loss = 5.52709276\n",
      "Iteration 20345, loss = 5.19432818\n",
      "Iteration 20346, loss = 5.54020701\n",
      "Iteration 20347, loss = 5.15421210\n",
      "Iteration 20348, loss = 5.54501757\n",
      "Iteration 20349, loss = 5.14241081\n",
      "Iteration 20350, loss = 5.56842544\n",
      "Iteration 20351, loss = 5.36187773\n",
      "Iteration 20352, loss = 5.64498900\n",
      "Iteration 20353, loss = 5.50520056\n",
      "Iteration 20354, loss = 5.57041844\n",
      "Iteration 20355, loss = 5.54751446\n",
      "Iteration 20356, loss = 5.19443680\n",
      "Iteration 20357, loss = 5.90384637\n",
      "Iteration 20358, loss = 5.93221477\n",
      "Iteration 20359, loss = 5.58719401\n",
      "Iteration 20360, loss = 5.20232041\n",
      "Iteration 20361, loss = 5.39101471\n",
      "Iteration 20362, loss = 5.74240654\n",
      "Iteration 20363, loss = 5.73356940\n",
      "Iteration 20364, loss = 5.43590731\n",
      "Iteration 20365, loss = 5.19342342\n",
      "Iteration 20366, loss = 5.47463338\n",
      "Iteration 20367, loss = 5.21616340\n",
      "Iteration 20368, loss = 5.41637734\n",
      "Iteration 20369, loss = 5.08117362\n",
      "Iteration 20370, loss = 5.22651739\n",
      "Iteration 20371, loss = 5.57044851\n",
      "Iteration 20372, loss = 5.22897141\n",
      "Iteration 20373, loss = 5.22991215\n",
      "Iteration 20374, loss = 5.20459677\n",
      "Iteration 20375, loss = 5.38423427\n",
      "Iteration 20376, loss = 5.12740591\n",
      "Iteration 20377, loss = 5.10738391\n",
      "Iteration 20378, loss = 5.11379442\n",
      "Iteration 20379, loss = 5.52520777\n",
      "Iteration 20380, loss = 5.41148420\n",
      "Iteration 20381, loss = 5.39538136\n",
      "Iteration 20382, loss = 5.93246591\n",
      "Iteration 20383, loss = 5.58456432\n",
      "Iteration 20384, loss = 5.19783166\n",
      "Iteration 20385, loss = 5.38763892\n",
      "Iteration 20386, loss = 5.77902120\n",
      "Iteration 20387, loss = 5.73560498\n",
      "Iteration 20388, loss = 5.59336697\n",
      "Iteration 20389, loss = 5.08789769\n",
      "Iteration 20390, loss = 5.78582936\n",
      "Iteration 20391, loss = 5.47431867\n",
      "Iteration 20392, loss = 5.41004573\n",
      "Iteration 20393, loss = 5.54740682\n",
      "Iteration 20394, loss = 5.52029219\n",
      "Iteration 20395, loss = 5.21487225\n",
      "Iteration 20396, loss = 5.06236093\n",
      "Iteration 20397, loss = 5.83867828\n",
      "Iteration 20398, loss = 5.86783831\n",
      "Iteration 20399, loss = 5.82064448\n",
      "Iteration 20400, loss = 5.15224429\n",
      "Iteration 20401, loss = 6.10940478\n",
      "Iteration 20402, loss = 5.69295175\n",
      "Iteration 20403, loss = 6.37903430\n",
      "Iteration 20404, loss = 7.40554278\n",
      "Iteration 20405, loss = 7.59511038\n",
      "Iteration 20406, loss = 5.90779551\n",
      "Iteration 20407, loss = 5.64664908\n",
      "Iteration 20408, loss = 5.71131106\n",
      "Iteration 20409, loss = 6.21500419\n",
      "Iteration 20410, loss = 5.93344662\n",
      "Iteration 20411, loss = 6.55143153\n",
      "Iteration 20412, loss = 6.39299478\n",
      "Iteration 20413, loss = 6.26685398\n",
      "Iteration 20414, loss = 6.41151815\n",
      "Iteration 20415, loss = 6.26338839\n",
      "Iteration 20416, loss = 5.67316223\n",
      "Iteration 20417, loss = 5.50262809\n",
      "Iteration 20418, loss = 5.27160355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20419, loss = 5.40806885\n",
      "Iteration 20420, loss = 6.27475424\n",
      "Iteration 20421, loss = 7.95673258\n",
      "Iteration 20422, loss = 7.14657091\n",
      "Iteration 20423, loss = 5.77907108\n",
      "Iteration 20424, loss = 5.95715841\n",
      "Iteration 20425, loss = 6.48871804\n",
      "Iteration 20426, loss = 6.45478790\n",
      "Iteration 20427, loss = 7.81067066\n",
      "Iteration 20428, loss = 6.01676223\n",
      "Iteration 20429, loss = 6.52928687\n",
      "Iteration 20430, loss = 6.08215873\n",
      "Iteration 20431, loss = 5.69756991\n",
      "Iteration 20432, loss = 5.23903991\n",
      "Iteration 20433, loss = 5.29689866\n",
      "Iteration 20434, loss = 5.13266951\n",
      "Iteration 20435, loss = 5.53595808\n",
      "Iteration 20436, loss = 5.33915422\n",
      "Iteration 20437, loss = 5.29414683\n",
      "Iteration 20438, loss = 5.51552259\n",
      "Iteration 20439, loss = 5.91946074\n",
      "Iteration 20440, loss = 5.45528087\n",
      "Iteration 20441, loss = 6.16688569\n",
      "Iteration 20442, loss = 5.76260547\n",
      "Iteration 20443, loss = 5.38018418\n",
      "Iteration 20444, loss = 5.56744149\n",
      "Iteration 20445, loss = 5.51502710\n",
      "Iteration 20446, loss = 5.14497653\n",
      "Iteration 20447, loss = 5.69109772\n",
      "Iteration 20448, loss = 5.24833753\n",
      "Iteration 20449, loss = 5.41509881\n",
      "Iteration 20450, loss = 5.85331934\n",
      "Iteration 20451, loss = 6.81053317\n",
      "Iteration 20452, loss = 7.06538369\n",
      "Iteration 20453, loss = 6.46050666\n",
      "Iteration 20454, loss = 7.04627012\n",
      "Iteration 20455, loss = 6.07398218\n",
      "Iteration 20456, loss = 5.99511019\n",
      "Iteration 20457, loss = 5.92269825\n",
      "Iteration 20458, loss = 6.33720201\n",
      "Iteration 20459, loss = 6.07027412\n",
      "Iteration 20460, loss = 5.12832674\n",
      "Iteration 20461, loss = 5.99670749\n",
      "Iteration 20462, loss = 6.13990915\n",
      "Iteration 20463, loss = 5.49698884\n",
      "Iteration 20464, loss = 5.09086069\n",
      "Iteration 20465, loss = 5.60499615\n",
      "Iteration 20466, loss = 4.97071767\n",
      "Iteration 20467, loss = 5.16513951\n",
      "Iteration 20468, loss = 5.31179279\n",
      "Iteration 20469, loss = 5.69671850\n",
      "Iteration 20470, loss = 5.66751810\n",
      "Iteration 20471, loss = 5.75506429\n",
      "Iteration 20472, loss = 6.16982598\n",
      "Iteration 20473, loss = 5.57356770\n",
      "Iteration 20474, loss = 5.17635261\n",
      "Iteration 20475, loss = 5.39837154\n",
      "Iteration 20476, loss = 5.24804424\n",
      "Iteration 20477, loss = 5.16056919\n",
      "Iteration 20478, loss = 5.24796058\n",
      "Iteration 20479, loss = 5.16480824\n",
      "Iteration 20480, loss = 5.67779820\n",
      "Iteration 20481, loss = 6.02471885\n",
      "Iteration 20482, loss = 6.73014518\n",
      "Iteration 20483, loss = 7.47616305\n",
      "Iteration 20484, loss = 6.68230910\n",
      "Iteration 20485, loss = 6.37847142\n",
      "Iteration 20486, loss = 6.46211832\n",
      "Iteration 20487, loss = 5.77232016\n",
      "Iteration 20488, loss = 6.11101240\n",
      "Iteration 20489, loss = 5.92356161\n",
      "Iteration 20490, loss = 5.37494083\n",
      "Iteration 20491, loss = 5.35499433\n",
      "Iteration 20492, loss = 5.83579380\n",
      "Iteration 20493, loss = 6.01453148\n",
      "Iteration 20494, loss = 5.42182056\n",
      "Iteration 20495, loss = 5.27855300\n",
      "Iteration 20496, loss = 5.15547318\n",
      "Iteration 20497, loss = 5.53429156\n",
      "Iteration 20498, loss = 5.53461911\n",
      "Iteration 20499, loss = 6.15064207\n",
      "Iteration 20500, loss = 5.49513698\n",
      "Iteration 20501, loss = 5.45276257\n",
      "Iteration 20502, loss = 5.30774839\n",
      "Iteration 20503, loss = 5.09176964\n",
      "Iteration 20504, loss = 6.35750163\n",
      "Iteration 20505, loss = 5.55604007\n",
      "Iteration 20506, loss = 5.60569453\n",
      "Iteration 20507, loss = 5.76515040\n",
      "Iteration 20508, loss = 5.44895699\n",
      "Iteration 20509, loss = 5.30974833\n",
      "Iteration 20510, loss = 5.65679500\n",
      "Iteration 20511, loss = 5.25342972\n",
      "Iteration 20512, loss = 5.26307887\n",
      "Iteration 20513, loss = 5.34714644\n",
      "Iteration 20514, loss = 5.66025971\n",
      "Iteration 20515, loss = 5.91728944\n",
      "Iteration 20516, loss = 5.10331839\n",
      "Iteration 20517, loss = 5.40971980\n",
      "Iteration 20518, loss = 5.84485337\n",
      "Iteration 20519, loss = 6.53900187\n",
      "Iteration 20520, loss = 5.80004065\n",
      "Iteration 20521, loss = 5.37138641\n",
      "Iteration 20522, loss = 5.28528852\n",
      "Iteration 20523, loss = 5.42386252\n",
      "Iteration 20524, loss = 5.40333483\n",
      "Iteration 20525, loss = 5.54850671\n",
      "Iteration 20526, loss = 6.02971682\n",
      "Iteration 20527, loss = 5.77992894\n",
      "Iteration 20528, loss = 7.26469207\n",
      "Iteration 20529, loss = 6.71597617\n",
      "Iteration 20530, loss = 6.48937027\n",
      "Iteration 20531, loss = 5.59964063\n",
      "Iteration 20532, loss = 5.98676586\n",
      "Iteration 20533, loss = 5.86585702\n",
      "Iteration 20534, loss = 7.10382256\n",
      "Iteration 20535, loss = 6.36287705\n",
      "Iteration 20536, loss = 7.14576484\n",
      "Iteration 20537, loss = 7.07892211\n",
      "Iteration 20538, loss = 6.04939081\n",
      "Iteration 20539, loss = 5.83818337\n",
      "Iteration 20540, loss = 5.26748224\n",
      "Iteration 20541, loss = 4.97993488\n",
      "Iteration 20542, loss = 5.44093102\n",
      "Iteration 20543, loss = 5.13649842\n",
      "Iteration 20544, loss = 5.29264504\n",
      "Iteration 20545, loss = 5.60716952\n",
      "Iteration 20546, loss = 5.42501700\n",
      "Iteration 20547, loss = 5.49751871\n",
      "Iteration 20548, loss = 5.44715727\n",
      "Iteration 20549, loss = 5.69939882\n",
      "Iteration 20550, loss = 5.17776217\n",
      "Iteration 20551, loss = 5.73161506\n",
      "Iteration 20552, loss = 5.98354911\n",
      "Iteration 20553, loss = 5.15701322\n",
      "Iteration 20554, loss = 5.18351319\n",
      "Iteration 20555, loss = 5.30402271\n",
      "Iteration 20556, loss = 5.63981692\n",
      "Iteration 20557, loss = 5.53263371\n",
      "Iteration 20558, loss = 5.48075847\n",
      "Iteration 20559, loss = 5.22039738\n",
      "Iteration 20560, loss = 5.43350243\n",
      "Iteration 20561, loss = 5.57698439\n",
      "Iteration 20562, loss = 5.33054446\n",
      "Iteration 20563, loss = 5.56514380\n",
      "Iteration 20564, loss = 5.52161226\n",
      "Iteration 20565, loss = 5.66044998\n",
      "Iteration 20566, loss = 5.79583700\n",
      "Iteration 20567, loss = 6.16737852\n",
      "Iteration 20568, loss = 5.53538885\n",
      "Iteration 20569, loss = 5.89811319\n",
      "Iteration 20570, loss = 5.86601078\n",
      "Iteration 20571, loss = 5.94793054\n",
      "Iteration 20572, loss = 4.98723606\n",
      "Iteration 20573, loss = 4.94590767\n",
      "Iteration 20574, loss = 5.29897159\n",
      "Iteration 20575, loss = 5.12717462\n",
      "Iteration 20576, loss = 5.66653803\n",
      "Iteration 20577, loss = 6.43688151\n",
      "Iteration 20578, loss = 6.44987872\n",
      "Iteration 20579, loss = 5.53417198\n",
      "Iteration 20580, loss = 5.27436172\n",
      "Iteration 20581, loss = 5.29775972\n",
      "Iteration 20582, loss = 5.56684583\n",
      "Iteration 20583, loss = 6.36284995\n",
      "Iteration 20584, loss = 6.50179127\n",
      "Iteration 20585, loss = 6.74174952\n",
      "Iteration 20586, loss = 6.15998207\n",
      "Iteration 20587, loss = 5.65483314\n",
      "Iteration 20588, loss = 5.82178416\n",
      "Iteration 20589, loss = 6.21153509\n",
      "Iteration 20590, loss = 6.49098402\n",
      "Iteration 20591, loss = 5.76212478\n",
      "Iteration 20592, loss = 6.02601585\n",
      "Iteration 20593, loss = 5.88331092\n",
      "Iteration 20594, loss = 5.65802633\n",
      "Iteration 20595, loss = 5.42198163\n",
      "Iteration 20596, loss = 5.22457124\n",
      "Iteration 20597, loss = 5.35526405\n",
      "Iteration 20598, loss = 4.94186717\n",
      "Iteration 20599, loss = 5.08549604\n",
      "Iteration 20600, loss = 4.98397371\n",
      "Iteration 20601, loss = 5.04473594\n",
      "Iteration 20602, loss = 5.08628927\n",
      "Iteration 20603, loss = 5.09806441\n",
      "Iteration 20604, loss = 5.45935170\n",
      "Iteration 20605, loss = 5.17879288\n",
      "Iteration 20606, loss = 5.05502210\n",
      "Iteration 20607, loss = 5.10227227\n",
      "Iteration 20608, loss = 5.34512323\n",
      "Iteration 20609, loss = 5.69847923\n",
      "Iteration 20610, loss = 5.42547652\n",
      "Iteration 20611, loss = 5.43320484\n",
      "Iteration 20612, loss = 4.96902357\n",
      "Iteration 20613, loss = 5.45679170\n",
      "Iteration 20614, loss = 5.33789417\n",
      "Iteration 20615, loss = 5.51930274\n",
      "Iteration 20616, loss = 6.13088595\n",
      "Iteration 20617, loss = 5.51468591\n",
      "Iteration 20618, loss = 5.75440522\n",
      "Iteration 20619, loss = 4.89232925\n",
      "Iteration 20620, loss = 5.39667635\n",
      "Iteration 20621, loss = 6.23399444\n",
      "Iteration 20622, loss = 5.62476845\n",
      "Iteration 20623, loss = 5.13806970\n",
      "Iteration 20624, loss = 5.71651955\n",
      "Iteration 20625, loss = 5.88046519\n",
      "Iteration 20626, loss = 5.90625492\n",
      "Iteration 20627, loss = 5.77229096\n",
      "Iteration 20628, loss = 5.41852029\n",
      "Iteration 20629, loss = 5.52408702\n",
      "Iteration 20630, loss = 5.47617394\n",
      "Iteration 20631, loss = 5.27617397\n",
      "Iteration 20632, loss = 5.56729170\n",
      "Iteration 20633, loss = 6.18388068\n",
      "Iteration 20634, loss = 5.20314273\n",
      "Iteration 20635, loss = 5.39289770\n",
      "Iteration 20636, loss = 5.45489025\n",
      "Iteration 20637, loss = 5.71595509\n",
      "Iteration 20638, loss = 6.21956159\n",
      "Iteration 20639, loss = 6.60697227\n",
      "Iteration 20640, loss = 5.92740961\n",
      "Iteration 20641, loss = 5.82507906\n",
      "Iteration 20642, loss = 5.94570985\n",
      "Iteration 20643, loss = 5.16022509\n",
      "Iteration 20644, loss = 5.49241285\n",
      "Iteration 20645, loss = 5.47535961\n",
      "Iteration 20646, loss = 5.01775778\n",
      "Iteration 20647, loss = 5.39108590\n",
      "Iteration 20648, loss = 4.98645776\n",
      "Iteration 20649, loss = 4.90085712\n",
      "Iteration 20650, loss = 5.16442240\n",
      "Iteration 20651, loss = 5.68761353\n",
      "Iteration 20652, loss = 6.37893767\n",
      "Iteration 20653, loss = 5.37961718\n",
      "Iteration 20654, loss = 5.79628679\n",
      "Iteration 20655, loss = 5.48150991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20656, loss = 5.15568218\n",
      "Iteration 20657, loss = 5.32834168\n",
      "Iteration 20658, loss = 6.12205056\n",
      "Iteration 20659, loss = 5.65550525\n",
      "Iteration 20660, loss = 5.49351514\n",
      "Iteration 20661, loss = 5.53726001\n",
      "Iteration 20662, loss = 5.20486885\n",
      "Iteration 20663, loss = 5.29950060\n",
      "Iteration 20664, loss = 5.07117208\n",
      "Iteration 20665, loss = 5.31086179\n",
      "Iteration 20666, loss = 6.45656198\n",
      "Iteration 20667, loss = 5.58069671\n",
      "Iteration 20668, loss = 6.51226477\n",
      "Iteration 20669, loss = 6.55323203\n",
      "Iteration 20670, loss = 5.58988234\n",
      "Iteration 20671, loss = 5.80217184\n",
      "Iteration 20672, loss = 6.35785260\n",
      "Iteration 20673, loss = 6.46291973\n",
      "Iteration 20674, loss = 6.58230098\n",
      "Iteration 20675, loss = 5.76922226\n",
      "Iteration 20676, loss = 6.03694579\n",
      "Iteration 20677, loss = 5.33934751\n",
      "Iteration 20678, loss = 5.24812122\n",
      "Iteration 20679, loss = 5.14867136\n",
      "Iteration 20680, loss = 5.47724149\n",
      "Iteration 20681, loss = 5.53828895\n",
      "Iteration 20682, loss = 5.32319989\n",
      "Iteration 20683, loss = 5.46860185\n",
      "Iteration 20684, loss = 5.69280987\n",
      "Iteration 20685, loss = 5.96325971\n",
      "Iteration 20686, loss = 6.26185594\n",
      "Iteration 20687, loss = 5.53517355\n",
      "Iteration 20688, loss = 5.53984365\n",
      "Iteration 20689, loss = 5.16193513\n",
      "Iteration 20690, loss = 5.41058892\n",
      "Iteration 20691, loss = 5.69439350\n",
      "Iteration 20692, loss = 5.62419662\n",
      "Iteration 20693, loss = 5.09399732\n",
      "Iteration 20694, loss = 5.70550949\n",
      "Iteration 20695, loss = 5.72369907\n",
      "Iteration 20696, loss = 6.04856264\n",
      "Iteration 20697, loss = 5.45550380\n",
      "Iteration 20698, loss = 5.60092464\n",
      "Iteration 20699, loss = 5.50037132\n",
      "Iteration 20700, loss = 5.43268124\n",
      "Iteration 20701, loss = 5.45246483\n",
      "Iteration 20702, loss = 6.02621613\n",
      "Iteration 20703, loss = 6.64007925\n",
      "Iteration 20704, loss = 6.98880581\n",
      "Iteration 20705, loss = 7.19919752\n",
      "Iteration 20706, loss = 5.83082260\n",
      "Iteration 20707, loss = 5.20231396\n",
      "Iteration 20708, loss = 5.96448067\n",
      "Iteration 20709, loss = 5.49773859\n",
      "Iteration 20710, loss = 5.98350032\n",
      "Iteration 20711, loss = 5.58368038\n",
      "Iteration 20712, loss = 5.65217061\n",
      "Iteration 20713, loss = 5.92257869\n",
      "Iteration 20714, loss = 5.83273604\n",
      "Iteration 20715, loss = 5.30562655\n",
      "Iteration 20716, loss = 4.93258977\n",
      "Iteration 20717, loss = 5.12491346\n",
      "Iteration 20718, loss = 6.19409035\n",
      "Iteration 20719, loss = 5.72115045\n",
      "Iteration 20720, loss = 5.38872054\n",
      "Iteration 20721, loss = 5.25782615\n",
      "Iteration 20722, loss = 5.16556259\n",
      "Iteration 20723, loss = 5.09654128\n",
      "Iteration 20724, loss = 5.06646949\n",
      "Iteration 20725, loss = 5.79317617\n",
      "Iteration 20726, loss = 6.24410713\n",
      "Iteration 20727, loss = 6.19849801\n",
      "Iteration 20728, loss = 5.58552463\n",
      "Iteration 20729, loss = 5.30930266\n",
      "Iteration 20730, loss = 5.13664627\n",
      "Iteration 20731, loss = 5.22876662\n",
      "Iteration 20732, loss = 5.28170023\n",
      "Iteration 20733, loss = 5.34165383\n",
      "Iteration 20734, loss = 5.70310131\n",
      "Iteration 20735, loss = 5.07567576\n",
      "Iteration 20736, loss = 5.66259376\n",
      "Iteration 20737, loss = 5.04381060\n",
      "Iteration 20738, loss = 6.46267804\n",
      "Iteration 20739, loss = 5.37553257\n",
      "Iteration 20740, loss = 5.27608003\n",
      "Iteration 20741, loss = 6.09930717\n",
      "Iteration 20742, loss = 6.37579674\n",
      "Iteration 20743, loss = 6.60732308\n",
      "Iteration 20744, loss = 5.99192942\n",
      "Iteration 20745, loss = 5.35613846\n",
      "Iteration 20746, loss = 5.11215662\n",
      "Iteration 20747, loss = 5.74851775\n",
      "Iteration 20748, loss = 4.93117708\n",
      "Iteration 20749, loss = 5.09862460\n",
      "Iteration 20750, loss = 5.51030633\n",
      "Iteration 20751, loss = 5.47046000\n",
      "Iteration 20752, loss = 5.74696004\n",
      "Iteration 20753, loss = 6.18066971\n",
      "Iteration 20754, loss = 6.01954054\n",
      "Iteration 20755, loss = 5.93654673\n",
      "Iteration 20756, loss = 5.07998048\n",
      "Iteration 20757, loss = 5.03328029\n",
      "Iteration 20758, loss = 5.18449140\n",
      "Iteration 20759, loss = 6.17670924\n",
      "Iteration 20760, loss = 5.33903038\n",
      "Iteration 20761, loss = 5.35905534\n",
      "Iteration 20762, loss = 5.62180503\n",
      "Iteration 20763, loss = 6.09945918\n",
      "Iteration 20764, loss = 5.58184172\n",
      "Iteration 20765, loss = 7.15210401\n",
      "Iteration 20766, loss = 7.44517680\n",
      "Iteration 20767, loss = 6.02567320\n",
      "Iteration 20768, loss = 5.40024337\n",
      "Iteration 20769, loss = 5.62013621\n",
      "Iteration 20770, loss = 6.34966228\n",
      "Iteration 20771, loss = 6.17731031\n",
      "Iteration 20772, loss = 6.05546421\n",
      "Iteration 20773, loss = 5.75193924\n",
      "Iteration 20774, loss = 4.98499042\n",
      "Iteration 20775, loss = 5.24594571\n",
      "Iteration 20776, loss = 5.89365637\n",
      "Iteration 20777, loss = 5.27174679\n",
      "Iteration 20778, loss = 5.89956427\n",
      "Iteration 20779, loss = 5.34375099\n",
      "Iteration 20780, loss = 5.28108175\n",
      "Iteration 20781, loss = 5.53560410\n",
      "Iteration 20782, loss = 6.76002417\n",
      "Iteration 20783, loss = 6.42398883\n",
      "Iteration 20784, loss = 5.76324423\n",
      "Iteration 20785, loss = 6.27506224\n",
      "Iteration 20786, loss = 6.52066348\n",
      "Iteration 20787, loss = 6.08393247\n",
      "Iteration 20788, loss = 5.75601867\n",
      "Iteration 20789, loss = 5.71584113\n",
      "Iteration 20790, loss = 5.30166791\n",
      "Iteration 20791, loss = 5.17820274\n",
      "Iteration 20792, loss = 5.88914016\n",
      "Iteration 20793, loss = 5.43707664\n",
      "Iteration 20794, loss = 5.66301981\n",
      "Iteration 20795, loss = 5.85044378\n",
      "Iteration 20796, loss = 5.75327037\n",
      "Iteration 20797, loss = 5.16455272\n",
      "Iteration 20798, loss = 5.88582866\n",
      "Iteration 20799, loss = 5.42222221\n",
      "Iteration 20800, loss = 6.56495660\n",
      "Iteration 20801, loss = 5.72068858\n",
      "Iteration 20802, loss = 5.60039391\n",
      "Iteration 20803, loss = 5.30873866\n",
      "Iteration 20804, loss = 5.98425894\n",
      "Iteration 20805, loss = 5.67013455\n",
      "Iteration 20806, loss = 5.49154369\n",
      "Iteration 20807, loss = 5.53789217\n",
      "Iteration 20808, loss = 5.70901705\n",
      "Iteration 20809, loss = 5.20445723\n",
      "Iteration 20810, loss = 5.12357118\n",
      "Iteration 20811, loss = 5.23213177\n",
      "Iteration 20812, loss = 5.26585569\n",
      "Iteration 20813, loss = 5.14018379\n",
      "Iteration 20814, loss = 4.92245678\n",
      "Iteration 20815, loss = 4.91065567\n",
      "Iteration 20816, loss = 5.23832388\n",
      "Iteration 20817, loss = 5.49828575\n",
      "Iteration 20818, loss = 5.96767184\n",
      "Iteration 20819, loss = 5.84421745\n",
      "Iteration 20820, loss = 5.68223385\n",
      "Iteration 20821, loss = 5.79113282\n",
      "Iteration 20822, loss = 5.35888423\n",
      "Iteration 20823, loss = 5.78320097\n",
      "Iteration 20824, loss = 5.43413928\n",
      "Iteration 20825, loss = 5.45526527\n",
      "Iteration 20826, loss = 5.92919180\n",
      "Iteration 20827, loss = 5.02619473\n",
      "Iteration 20828, loss = 5.34485430\n",
      "Iteration 20829, loss = 5.04804954\n",
      "Iteration 20830, loss = 5.16439543\n",
      "Iteration 20831, loss = 4.78890043\n",
      "Iteration 20832, loss = 5.14568583\n",
      "Iteration 20833, loss = 5.30428101\n",
      "Iteration 20834, loss = 5.13760440\n",
      "Iteration 20835, loss = 5.42392709\n",
      "Iteration 20836, loss = 5.64456853\n",
      "Iteration 20837, loss = 5.83457366\n",
      "Iteration 20838, loss = 5.23355386\n",
      "Iteration 20839, loss = 5.21853218\n",
      "Iteration 20840, loss = 5.06022281\n",
      "Iteration 20841, loss = 5.18971803\n",
      "Iteration 20842, loss = 6.73481746\n",
      "Iteration 20843, loss = 6.33619218\n",
      "Iteration 20844, loss = 5.74519378\n",
      "Iteration 20845, loss = 5.24363853\n",
      "Iteration 20846, loss = 5.21550822\n",
      "Iteration 20847, loss = 5.83454489\n",
      "Iteration 20848, loss = 5.92520278\n",
      "Iteration 20849, loss = 6.04187050\n",
      "Iteration 20850, loss = 5.65870656\n",
      "Iteration 20851, loss = 5.75587932\n",
      "Iteration 20852, loss = 5.22290089\n",
      "Iteration 20853, loss = 5.07355486\n",
      "Iteration 20854, loss = 5.39196008\n",
      "Iteration 20855, loss = 5.70804896\n",
      "Iteration 20856, loss = 5.60221511\n",
      "Iteration 20857, loss = 6.13192576\n",
      "Iteration 20858, loss = 5.87007411\n",
      "Iteration 20859, loss = 5.81815375\n",
      "Iteration 20860, loss = 7.22630142\n",
      "Iteration 20861, loss = 5.58239921\n",
      "Iteration 20862, loss = 5.93705568\n",
      "Iteration 20863, loss = 5.25790246\n",
      "Iteration 20864, loss = 6.05361935\n",
      "Iteration 20865, loss = 5.82941165\n",
      "Iteration 20866, loss = 5.74054365\n",
      "Iteration 20867, loss = 5.51452548\n",
      "Iteration 20868, loss = 5.69408171\n",
      "Iteration 20869, loss = 5.18627203\n",
      "Iteration 20870, loss = 5.31690597\n",
      "Iteration 20871, loss = 5.18042050\n",
      "Iteration 20872, loss = 5.81044437\n",
      "Iteration 20873, loss = 6.10178826\n",
      "Iteration 20874, loss = 6.85138695\n",
      "Iteration 20875, loss = 6.31110228\n",
      "Iteration 20876, loss = 6.90316876\n",
      "Iteration 20877, loss = 6.15739693\n",
      "Iteration 20878, loss = 6.26396683\n",
      "Iteration 20879, loss = 5.43126406\n",
      "Iteration 20880, loss = 6.16148798\n",
      "Iteration 20881, loss = 5.81550221\n",
      "Iteration 20882, loss = 5.50829180\n",
      "Iteration 20883, loss = 5.24255548\n",
      "Iteration 20884, loss = 5.28266228\n",
      "Iteration 20885, loss = 5.57747010\n",
      "Iteration 20886, loss = 5.32942681\n",
      "Iteration 20887, loss = 5.82105339\n",
      "Iteration 20888, loss = 5.41653137\n",
      "Iteration 20889, loss = 5.81912626\n",
      "Iteration 20890, loss = 7.55921411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20891, loss = 9.30417375\n",
      "Iteration 20892, loss = 7.72420427\n",
      "Iteration 20893, loss = 5.91352524\n",
      "Iteration 20894, loss = 6.04921257\n",
      "Iteration 20895, loss = 5.47712939\n",
      "Iteration 20896, loss = 5.46075119\n",
      "Iteration 20897, loss = 5.32035606\n",
      "Iteration 20898, loss = 5.07467830\n",
      "Iteration 20899, loss = 5.57201818\n",
      "Iteration 20900, loss = 5.32578090\n",
      "Iteration 20901, loss = 5.22582011\n",
      "Iteration 20902, loss = 5.47356950\n",
      "Iteration 20903, loss = 6.59313021\n",
      "Iteration 20904, loss = 6.17326842\n",
      "Iteration 20905, loss = 6.43222931\n",
      "Iteration 20906, loss = 6.11609468\n",
      "Iteration 20907, loss = 5.22400488\n",
      "Iteration 20908, loss = 5.52932845\n",
      "Iteration 20909, loss = 5.41488748\n",
      "Iteration 20910, loss = 5.04950587\n",
      "Iteration 20911, loss = 5.24018330\n",
      "Iteration 20912, loss = 5.28154645\n",
      "Iteration 20913, loss = 5.61679363\n",
      "Iteration 20914, loss = 5.69912313\n",
      "Iteration 20915, loss = 6.09802960\n",
      "Iteration 20916, loss = 6.33267899\n",
      "Iteration 20917, loss = 5.97803462\n",
      "Iteration 20918, loss = 5.86628195\n",
      "Iteration 20919, loss = 5.17186020\n",
      "Iteration 20920, loss = 5.27532296\n",
      "Iteration 20921, loss = 5.19849331\n",
      "Iteration 20922, loss = 5.05080631\n",
      "Iteration 20923, loss = 4.95743061\n",
      "Iteration 20924, loss = 5.67398738\n",
      "Iteration 20925, loss = 5.47375564\n",
      "Iteration 20926, loss = 5.43288948\n",
      "Iteration 20927, loss = 5.26383469\n",
      "Iteration 20928, loss = 5.37975774\n",
      "Iteration 20929, loss = 5.53243223\n",
      "Iteration 20930, loss = 5.52491573\n",
      "Iteration 20931, loss = 5.52654126\n",
      "Iteration 20932, loss = 5.04493660\n",
      "Iteration 20933, loss = 5.11249459\n",
      "Iteration 20934, loss = 5.64988246\n",
      "Iteration 20935, loss = 5.94505903\n",
      "Iteration 20936, loss = 6.01630760\n",
      "Iteration 20937, loss = 6.04251699\n",
      "Iteration 20938, loss = 6.25215484\n",
      "Iteration 20939, loss = 6.37341723\n",
      "Iteration 20940, loss = 5.44489630\n",
      "Iteration 20941, loss = 5.13391110\n",
      "Iteration 20942, loss = 5.57730678\n",
      "Iteration 20943, loss = 5.74646809\n",
      "Iteration 20944, loss = 5.56985843\n",
      "Iteration 20945, loss = 5.03793754\n",
      "Iteration 20946, loss = 6.23034226\n",
      "Iteration 20947, loss = 6.28122742\n",
      "Iteration 20948, loss = 6.04504104\n",
      "Iteration 20949, loss = 5.41645743\n",
      "Iteration 20950, loss = 5.86111787\n",
      "Iteration 20951, loss = 6.49561002\n",
      "Iteration 20952, loss = 5.17381665\n",
      "Iteration 20953, loss = 6.12047410\n",
      "Iteration 20954, loss = 6.65160980\n",
      "Iteration 20955, loss = 6.04375043\n",
      "Iteration 20956, loss = 5.81559415\n",
      "Iteration 20957, loss = 5.46308736\n",
      "Iteration 20958, loss = 5.85980539\n",
      "Iteration 20959, loss = 5.07773557\n",
      "Iteration 20960, loss = 5.15938422\n",
      "Iteration 20961, loss = 5.44204778\n",
      "Iteration 20962, loss = 5.24215392\n",
      "Iteration 20963, loss = 5.58685836\n",
      "Iteration 20964, loss = 4.97021635\n",
      "Iteration 20965, loss = 5.57174548\n",
      "Iteration 20966, loss = 5.25811285\n",
      "Iteration 20967, loss = 5.64876825\n",
      "Iteration 20968, loss = 5.75190346\n",
      "Iteration 20969, loss = 5.76322272\n",
      "Iteration 20970, loss = 5.80285728\n",
      "Iteration 20971, loss = 5.97255801\n",
      "Iteration 20972, loss = 5.70640077\n",
      "Iteration 20973, loss = 6.44064219\n",
      "Iteration 20974, loss = 6.53916217\n",
      "Iteration 20975, loss = 7.15993710\n",
      "Iteration 20976, loss = 6.73480519\n",
      "Iteration 20977, loss = 7.11773842\n",
      "Iteration 20978, loss = 6.31016448\n",
      "Iteration 20979, loss = 6.64052789\n",
      "Iteration 20980, loss = 6.69326257\n",
      "Iteration 20981, loss = 5.96823365\n",
      "Iteration 20982, loss = 5.89464229\n",
      "Iteration 20983, loss = 5.30867380\n",
      "Iteration 20984, loss = 5.23986317\n",
      "Iteration 20985, loss = 5.86383145\n",
      "Iteration 20986, loss = 5.63706246\n",
      "Iteration 20987, loss = 5.86234668\n",
      "Iteration 20988, loss = 5.18112500\n",
      "Iteration 20989, loss = 5.13150584\n",
      "Iteration 20990, loss = 5.83452991\n",
      "Iteration 20991, loss = 7.97779273\n",
      "Iteration 20992, loss = 7.49885491\n",
      "Iteration 20993, loss = 7.57227707\n",
      "Iteration 20994, loss = 6.18498670\n",
      "Iteration 20995, loss = 5.52669637\n",
      "Iteration 20996, loss = 5.04749238\n",
      "Iteration 20997, loss = 5.04215853\n",
      "Iteration 20998, loss = 5.81117925\n",
      "Iteration 20999, loss = 5.70788419\n",
      "Iteration 21000, loss = 5.84667681\n",
      "Iteration 21001, loss = 5.45158811\n",
      "Iteration 21002, loss = 6.14283464\n",
      "Iteration 21003, loss = 5.42014153\n",
      "Iteration 21004, loss = 5.23320947\n",
      "Iteration 21005, loss = 5.34163625\n",
      "Iteration 21006, loss = 6.39472847\n",
      "Iteration 21007, loss = 6.64091437\n",
      "Iteration 21008, loss = 5.96295531\n",
      "Iteration 21009, loss = 5.17930864\n",
      "Iteration 21010, loss = 6.03403229\n",
      "Iteration 21011, loss = 6.59576636\n",
      "Iteration 21012, loss = 8.28670887\n",
      "Iteration 21013, loss = 8.59397416\n",
      "Iteration 21014, loss = 7.85749069\n",
      "Iteration 21015, loss = 5.93751958\n",
      "Iteration 21016, loss = 5.50612116\n",
      "Iteration 21017, loss = 5.63264642\n",
      "Iteration 21018, loss = 5.71423167\n",
      "Iteration 21019, loss = 6.24459355\n",
      "Iteration 21020, loss = 6.00697427\n",
      "Iteration 21021, loss = 6.84704712\n",
      "Iteration 21022, loss = 5.76362858\n",
      "Iteration 21023, loss = 6.34098557\n",
      "Iteration 21024, loss = 5.68873567\n",
      "Iteration 21025, loss = 5.25053772\n",
      "Iteration 21026, loss = 5.07496300\n",
      "Iteration 21027, loss = 5.72154396\n",
      "Iteration 21028, loss = 5.13240760\n",
      "Iteration 21029, loss = 4.94236103\n",
      "Iteration 21030, loss = 5.65270952\n",
      "Iteration 21031, loss = 5.98347448\n",
      "Iteration 21032, loss = 6.47076729\n",
      "Iteration 21033, loss = 6.16891641\n",
      "Iteration 21034, loss = 5.67338757\n",
      "Iteration 21035, loss = 5.17089488\n",
      "Iteration 21036, loss = 5.79222854\n",
      "Iteration 21037, loss = 5.98430909\n",
      "Iteration 21038, loss = 6.46680754\n",
      "Iteration 21039, loss = 5.49434647\n",
      "Iteration 21040, loss = 6.01912599\n",
      "Iteration 21041, loss = 6.35501226\n",
      "Iteration 21042, loss = 6.39522095\n",
      "Iteration 21043, loss = 5.23747419\n",
      "Iteration 21044, loss = 5.24027034\n",
      "Iteration 21045, loss = 4.97850518\n",
      "Iteration 21046, loss = 5.00622471\n",
      "Iteration 21047, loss = 5.05642882\n",
      "Iteration 21048, loss = 5.42802574\n",
      "Iteration 21049, loss = 4.93628440\n",
      "Iteration 21050, loss = 4.92307597\n",
      "Iteration 21051, loss = 5.15093063\n",
      "Iteration 21052, loss = 5.11151392\n",
      "Iteration 21053, loss = 5.03063203\n",
      "Iteration 21054, loss = 5.18718369\n",
      "Iteration 21055, loss = 5.16866199\n",
      "Iteration 21056, loss = 5.48207479\n",
      "Iteration 21057, loss = 6.12166081\n",
      "Iteration 21058, loss = 6.93223090\n",
      "Iteration 21059, loss = 6.44703474\n",
      "Iteration 21060, loss = 6.24832175\n",
      "Iteration 21061, loss = 6.42087598\n",
      "Iteration 21062, loss = 6.28326783\n",
      "Iteration 21063, loss = 6.03785112\n",
      "Iteration 21064, loss = 5.30993894\n",
      "Iteration 21065, loss = 6.42054901\n",
      "Iteration 21066, loss = 5.37460500\n",
      "Iteration 21067, loss = 7.49367258\n",
      "Iteration 21068, loss = 7.50011089\n",
      "Iteration 21069, loss = 9.36542275\n",
      "Iteration 21070, loss = 7.03763352\n",
      "Iteration 21071, loss = 7.94785817\n",
      "Iteration 21072, loss = 5.57075444\n",
      "Iteration 21073, loss = 6.03377558\n",
      "Iteration 21074, loss = 4.92828008\n",
      "Iteration 21075, loss = 5.41254483\n",
      "Iteration 21076, loss = 5.72663994\n",
      "Iteration 21077, loss = 5.94058274\n",
      "Iteration 21078, loss = 5.93006188\n",
      "Iteration 21079, loss = 6.47488201\n",
      "Iteration 21080, loss = 5.96468639\n",
      "Iteration 21081, loss = 5.69349452\n",
      "Iteration 21082, loss = 5.36004714\n",
      "Iteration 21083, loss = 5.18272806\n",
      "Iteration 21084, loss = 4.99975446\n",
      "Iteration 21085, loss = 5.14677038\n",
      "Iteration 21086, loss = 4.82393209\n",
      "Iteration 21087, loss = 5.45871228\n",
      "Iteration 21088, loss = 5.01361841\n",
      "Iteration 21089, loss = 5.05210151\n",
      "Iteration 21090, loss = 4.91065550\n",
      "Iteration 21091, loss = 5.29941404\n",
      "Iteration 21092, loss = 6.17630327\n",
      "Iteration 21093, loss = 5.84247605\n",
      "Iteration 21094, loss = 6.49456688\n",
      "Iteration 21095, loss = 5.69567031\n",
      "Iteration 21096, loss = 5.93803391\n",
      "Iteration 21097, loss = 5.73039492\n",
      "Iteration 21098, loss = 5.40850203\n",
      "Iteration 21099, loss = 5.43970575\n",
      "Iteration 21100, loss = 5.39986551\n",
      "Iteration 21101, loss = 5.58812303\n",
      "Iteration 21102, loss = 5.13648017\n",
      "Iteration 21103, loss = 4.95984772\n",
      "Iteration 21104, loss = 5.39517762\n",
      "Iteration 21105, loss = 6.16839704\n",
      "Iteration 21106, loss = 6.11859795\n",
      "Iteration 21107, loss = 5.49378569\n",
      "Iteration 21108, loss = 5.09857826\n",
      "Iteration 21109, loss = 5.49738023\n",
      "Iteration 21110, loss = 5.34535524\n",
      "Iteration 21111, loss = 6.26160394\n",
      "Iteration 21112, loss = 7.23382952\n",
      "Iteration 21113, loss = 5.66246036\n",
      "Iteration 21114, loss = 5.33781400\n",
      "Iteration 21115, loss = 4.91105865\n",
      "Iteration 21116, loss = 5.36261308\n",
      "Iteration 21117, loss = 5.85376034\n",
      "Iteration 21118, loss = 5.69431814\n",
      "Iteration 21119, loss = 5.46565554\n",
      "Iteration 21120, loss = 5.11339458\n",
      "Iteration 21121, loss = 5.21122350\n",
      "Iteration 21122, loss = 5.26244012\n",
      "Iteration 21123, loss = 5.76649223\n",
      "Iteration 21124, loss = 5.18789411\n",
      "Iteration 21125, loss = 4.92764663\n",
      "Iteration 21126, loss = 5.16693081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21127, loss = 5.21732609\n",
      "Iteration 21128, loss = 4.97142244\n",
      "Iteration 21129, loss = 5.24073095\n",
      "Iteration 21130, loss = 5.74492767\n",
      "Iteration 21131, loss = 5.75505780\n",
      "Iteration 21132, loss = 4.90291704\n",
      "Iteration 21133, loss = 5.24008336\n",
      "Iteration 21134, loss = 5.07924227\n",
      "Iteration 21135, loss = 5.04722408\n",
      "Iteration 21136, loss = 5.07406743\n",
      "Iteration 21137, loss = 5.01436045\n",
      "Iteration 21138, loss = 5.65185806\n",
      "Iteration 21139, loss = 5.28541218\n",
      "Iteration 21140, loss = 5.64626768\n",
      "Iteration 21141, loss = 6.10087611\n",
      "Iteration 21142, loss = 5.14458168\n",
      "Iteration 21143, loss = 5.57429358\n",
      "Iteration 21144, loss = 5.61531206\n",
      "Iteration 21145, loss = 5.17536640\n",
      "Iteration 21146, loss = 4.92269620\n",
      "Iteration 21147, loss = 4.81765338\n",
      "Iteration 21148, loss = 4.95049139\n",
      "Iteration 21149, loss = 5.08273501\n",
      "Iteration 21150, loss = 5.26149767\n",
      "Iteration 21151, loss = 4.80219359\n",
      "Iteration 21152, loss = 5.42788457\n",
      "Iteration 21153, loss = 6.81800002\n",
      "Iteration 21154, loss = 5.46867706\n",
      "Iteration 21155, loss = 5.70456983\n",
      "Iteration 21156, loss = 5.15067767\n",
      "Iteration 21157, loss = 5.82825126\n",
      "Iteration 21158, loss = 6.15071838\n",
      "Iteration 21159, loss = 5.44591960\n",
      "Iteration 21160, loss = 5.29604752\n",
      "Iteration 21161, loss = 5.21088278\n",
      "Iteration 21162, loss = 5.84557568\n",
      "Iteration 21163, loss = 5.24091137\n",
      "Iteration 21164, loss = 5.67400519\n",
      "Iteration 21165, loss = 5.30619185\n",
      "Iteration 21166, loss = 5.54076621\n",
      "Iteration 21167, loss = 5.58090781\n",
      "Iteration 21168, loss = 6.25726861\n",
      "Iteration 21169, loss = 6.31543659\n",
      "Iteration 21170, loss = 7.23015564\n",
      "Iteration 21171, loss = 6.67458944\n",
      "Iteration 21172, loss = 5.53649017\n",
      "Iteration 21173, loss = 5.73685885\n",
      "Iteration 21174, loss = 5.66235436\n",
      "Iteration 21175, loss = 5.44726739\n",
      "Iteration 21176, loss = 5.50140668\n",
      "Iteration 21177, loss = 5.18697047\n",
      "Iteration 21178, loss = 5.29345764\n",
      "Iteration 21179, loss = 5.74893251\n",
      "Iteration 21180, loss = 5.87041901\n",
      "Iteration 21181, loss = 5.86388782\n",
      "Iteration 21182, loss = 5.78324592\n",
      "Iteration 21183, loss = 6.50824588\n",
      "Iteration 21184, loss = 5.53422384\n",
      "Iteration 21185, loss = 5.37124806\n",
      "Iteration 21186, loss = 5.27152446\n",
      "Iteration 21187, loss = 5.16324885\n",
      "Iteration 21188, loss = 5.19236596\n",
      "Iteration 21189, loss = 5.00222042\n",
      "Iteration 21190, loss = 5.22990168\n",
      "Iteration 21191, loss = 5.45444228\n",
      "Iteration 21192, loss = 5.23887962\n",
      "Iteration 21193, loss = 5.01943572\n",
      "Iteration 21194, loss = 5.20084438\n",
      "Iteration 21195, loss = 5.33044228\n",
      "Iteration 21196, loss = 4.80829269\n",
      "Iteration 21197, loss = 4.94054706\n",
      "Iteration 21198, loss = 5.11928168\n",
      "Iteration 21199, loss = 5.15594443\n",
      "Iteration 21200, loss = 4.98988571\n",
      "Iteration 21201, loss = 5.50544320\n",
      "Iteration 21202, loss = 5.38544389\n",
      "Iteration 21203, loss = 5.10627958\n",
      "Iteration 21204, loss = 6.18274251\n",
      "Iteration 21205, loss = 5.56631549\n",
      "Iteration 21206, loss = 6.15929973\n",
      "Iteration 21207, loss = 6.39775885\n",
      "Iteration 21208, loss = 5.68713135\n",
      "Iteration 21209, loss = 5.59942542\n",
      "Iteration 21210, loss = 5.00318376\n",
      "Iteration 21211, loss = 5.31743183\n",
      "Iteration 21212, loss = 5.58716344\n",
      "Iteration 21213, loss = 6.32568970\n",
      "Iteration 21214, loss = 6.02324116\n",
      "Iteration 21215, loss = 6.28113306\n",
      "Iteration 21216, loss = 6.03036881\n",
      "Iteration 21217, loss = 5.24905965\n",
      "Iteration 21218, loss = 5.34391451\n",
      "Iteration 21219, loss = 4.85032636\n",
      "Iteration 21220, loss = 5.01527291\n",
      "Iteration 21221, loss = 5.07805948\n",
      "Iteration 21222, loss = 5.44219010\n",
      "Iteration 21223, loss = 5.34308414\n",
      "Iteration 21224, loss = 5.48403996\n",
      "Iteration 21225, loss = 5.19742171\n",
      "Iteration 21226, loss = 5.40825372\n",
      "Iteration 21227, loss = 5.81310462\n",
      "Iteration 21228, loss = 6.11348933\n",
      "Iteration 21229, loss = 5.42920936\n",
      "Iteration 21230, loss = 5.04391015\n",
      "Iteration 21231, loss = 5.30136527\n",
      "Iteration 21232, loss = 5.82430493\n",
      "Iteration 21233, loss = 5.32577608\n",
      "Iteration 21234, loss = 4.81575478\n",
      "Iteration 21235, loss = 5.78503038\n",
      "Iteration 21236, loss = 5.39100599\n",
      "Iteration 21237, loss = 5.37224182\n",
      "Iteration 21238, loss = 5.41246437\n",
      "Iteration 21239, loss = 6.03507957\n",
      "Iteration 21240, loss = 6.24669983\n",
      "Iteration 21241, loss = 5.69196435\n",
      "Iteration 21242, loss = 5.74208592\n",
      "Iteration 21243, loss = 5.70711421\n",
      "Iteration 21244, loss = 5.27982534\n",
      "Iteration 21245, loss = 5.27323486\n",
      "Iteration 21246, loss = 5.25420280\n",
      "Iteration 21247, loss = 5.01259899\n",
      "Iteration 21248, loss = 5.24823929\n",
      "Iteration 21249, loss = 5.33012914\n",
      "Iteration 21250, loss = 4.95903655\n",
      "Iteration 21251, loss = 5.43541058\n",
      "Iteration 21252, loss = 5.32032514\n",
      "Iteration 21253, loss = 5.41180523\n",
      "Iteration 21254, loss = 6.09332904\n",
      "Iteration 21255, loss = 4.94267326\n",
      "Iteration 21256, loss = 6.44044551\n",
      "Iteration 21257, loss = 5.40559963\n",
      "Iteration 21258, loss = 5.97751723\n",
      "Iteration 21259, loss = 6.00844358\n",
      "Iteration 21260, loss = 5.87058081\n",
      "Iteration 21261, loss = 6.45645786\n",
      "Iteration 21262, loss = 5.73771389\n",
      "Iteration 21263, loss = 6.00265333\n",
      "Iteration 21264, loss = 5.47425632\n",
      "Iteration 21265, loss = 5.27461502\n",
      "Iteration 21266, loss = 5.61781927\n",
      "Iteration 21267, loss = 4.80706621\n",
      "Iteration 21268, loss = 6.45604687\n",
      "Iteration 21269, loss = 7.25097507\n",
      "Iteration 21270, loss = 6.03785770\n",
      "Iteration 21271, loss = 5.40750366\n",
      "Iteration 21272, loss = 5.27363023\n",
      "Iteration 21273, loss = 5.31758981\n",
      "Iteration 21274, loss = 5.41223954\n",
      "Iteration 21275, loss = 5.02937660\n",
      "Iteration 21276, loss = 4.98787081\n",
      "Iteration 21277, loss = 4.93216283\n",
      "Iteration 21278, loss = 5.00586416\n",
      "Iteration 21279, loss = 5.94596706\n",
      "Iteration 21280, loss = 5.26115349\n",
      "Iteration 21281, loss = 5.99913036\n",
      "Iteration 21282, loss = 5.33169686\n",
      "Iteration 21283, loss = 5.40256436\n",
      "Iteration 21284, loss = 5.42156890\n",
      "Iteration 21285, loss = 5.06860564\n",
      "Iteration 21286, loss = 5.65246733\n",
      "Iteration 21287, loss = 5.05735394\n",
      "Iteration 21288, loss = 5.31580361\n",
      "Iteration 21289, loss = 7.23628243\n",
      "Iteration 21290, loss = 6.03018950\n",
      "Iteration 21291, loss = 5.84335104\n",
      "Iteration 21292, loss = 5.62844920\n",
      "Iteration 21293, loss = 5.62546639\n",
      "Iteration 21294, loss = 5.29425053\n",
      "Iteration 21295, loss = 4.89374961\n",
      "Iteration 21296, loss = 5.14620498\n",
      "Iteration 21297, loss = 5.01590830\n",
      "Iteration 21298, loss = 5.48927945\n",
      "Iteration 21299, loss = 5.56656560\n",
      "Iteration 21300, loss = 6.18334574\n",
      "Iteration 21301, loss = 5.37963556\n",
      "Iteration 21302, loss = 5.96017761\n",
      "Iteration 21303, loss = 5.67280716\n",
      "Iteration 21304, loss = 6.46605314\n",
      "Iteration 21305, loss = 5.19675341\n",
      "Iteration 21306, loss = 5.61003354\n",
      "Iteration 21307, loss = 4.94542764\n",
      "Iteration 21308, loss = 6.95245133\n",
      "Iteration 21309, loss = 7.13589751\n",
      "Iteration 21310, loss = 7.36989829\n",
      "Iteration 21311, loss = 5.69223556\n",
      "Iteration 21312, loss = 6.04852296\n",
      "Iteration 21313, loss = 5.39461094\n",
      "Iteration 21314, loss = 5.85511019\n",
      "Iteration 21315, loss = 4.81397092\n",
      "Iteration 21316, loss = 5.41540568\n",
      "Iteration 21317, loss = 5.49744131\n",
      "Iteration 21318, loss = 5.81163514\n",
      "Iteration 21319, loss = 6.28949594\n",
      "Iteration 21320, loss = 6.37765539\n",
      "Iteration 21321, loss = 6.15536534\n",
      "Iteration 21322, loss = 5.57559340\n",
      "Iteration 21323, loss = 5.12029979\n",
      "Iteration 21324, loss = 5.06794907\n",
      "Iteration 21325, loss = 5.59065568\n",
      "Iteration 21326, loss = 5.62901603\n",
      "Iteration 21327, loss = 5.34018053\n",
      "Iteration 21328, loss = 5.67099741\n",
      "Iteration 21329, loss = 5.51645577\n",
      "Iteration 21330, loss = 5.19292730\n",
      "Iteration 21331, loss = 5.42824984\n",
      "Iteration 21332, loss = 5.15823153\n",
      "Iteration 21333, loss = 5.29364615\n",
      "Iteration 21334, loss = 4.97925975\n",
      "Iteration 21335, loss = 4.91077816\n",
      "Iteration 21336, loss = 5.52431700\n",
      "Iteration 21337, loss = 5.26922963\n",
      "Iteration 21338, loss = 5.03543662\n",
      "Iteration 21339, loss = 4.92244933\n",
      "Iteration 21340, loss = 5.07513129\n",
      "Iteration 21341, loss = 4.95244424\n",
      "Iteration 21342, loss = 5.28659942\n",
      "Iteration 21343, loss = 5.25885250\n",
      "Iteration 21344, loss = 5.44492654\n",
      "Iteration 21345, loss = 4.95878448\n",
      "Iteration 21346, loss = 5.45232634\n",
      "Iteration 21347, loss = 6.42532451\n",
      "Iteration 21348, loss = 5.81320261\n",
      "Iteration 21349, loss = 4.95037910\n",
      "Iteration 21350, loss = 5.06932125\n",
      "Iteration 21351, loss = 4.81818933\n",
      "Iteration 21352, loss = 4.89353356\n",
      "Iteration 21353, loss = 4.83342887\n",
      "Iteration 21354, loss = 4.85345237\n",
      "Iteration 21355, loss = 5.07418120\n",
      "Iteration 21356, loss = 5.81064153\n",
      "Iteration 21357, loss = 6.37705395\n",
      "Iteration 21358, loss = 6.84005810\n",
      "Iteration 21359, loss = 6.39478331\n",
      "Iteration 21360, loss = 6.10180764\n",
      "Iteration 21361, loss = 4.97853610\n",
      "Iteration 21362, loss = 5.42001874\n",
      "Iteration 21363, loss = 5.11636089\n",
      "Iteration 21364, loss = 5.31683240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21365, loss = 5.06361495\n",
      "Iteration 21366, loss = 5.30636880\n",
      "Iteration 21367, loss = 5.15747504\n",
      "Iteration 21368, loss = 6.27880058\n",
      "Iteration 21369, loss = 6.11082620\n",
      "Iteration 21370, loss = 6.70604661\n",
      "Iteration 21371, loss = 6.43390833\n",
      "Iteration 21372, loss = 5.78384505\n",
      "Iteration 21373, loss = 6.57928811\n",
      "Iteration 21374, loss = 6.46847200\n",
      "Iteration 21375, loss = 6.25010276\n",
      "Iteration 21376, loss = 6.00719993\n",
      "Iteration 21377, loss = 4.88341889\n",
      "Iteration 21378, loss = 5.00464107\n",
      "Iteration 21379, loss = 4.92831793\n",
      "Iteration 21380, loss = 4.97624061\n",
      "Iteration 21381, loss = 5.21981196\n",
      "Iteration 21382, loss = 5.35378811\n",
      "Iteration 21383, loss = 5.27079598\n",
      "Iteration 21384, loss = 4.81404309\n",
      "Iteration 21385, loss = 5.22095795\n",
      "Iteration 21386, loss = 5.73887888\n",
      "Iteration 21387, loss = 4.99273799\n",
      "Iteration 21388, loss = 5.20599810\n",
      "Iteration 21389, loss = 4.88690425\n",
      "Iteration 21390, loss = 5.49476022\n",
      "Iteration 21391, loss = 5.82842318\n",
      "Iteration 21392, loss = 5.95314172\n",
      "Iteration 21393, loss = 6.22982422\n",
      "Iteration 21394, loss = 7.36058762\n",
      "Iteration 21395, loss = 6.29624568\n",
      "Iteration 21396, loss = 5.83338410\n",
      "Iteration 21397, loss = 6.12866133\n",
      "Iteration 21398, loss = 6.26740940\n",
      "Iteration 21399, loss = 6.42387420\n",
      "Iteration 21400, loss = 6.17882992\n",
      "Iteration 21401, loss = 5.98480951\n",
      "Iteration 21402, loss = 4.80123769\n",
      "Iteration 21403, loss = 5.36240598\n",
      "Iteration 21404, loss = 4.92929360\n",
      "Iteration 21405, loss = 5.42976753\n",
      "Iteration 21406, loss = 5.18991118\n",
      "Iteration 21407, loss = 4.93713498\n",
      "Iteration 21408, loss = 4.98857869\n",
      "Iteration 21409, loss = 5.58134518\n",
      "Iteration 21410, loss = 5.67873113\n",
      "Iteration 21411, loss = 5.91671931\n",
      "Iteration 21412, loss = 6.02365679\n",
      "Iteration 21413, loss = 5.08146605\n",
      "Iteration 21414, loss = 5.07901509\n",
      "Iteration 21415, loss = 5.13012487\n",
      "Iteration 21416, loss = 5.03493856\n",
      "Iteration 21417, loss = 5.30299782\n",
      "Iteration 21418, loss = 4.98619193\n",
      "Iteration 21419, loss = 5.37286675\n",
      "Iteration 21420, loss = 5.04935602\n",
      "Iteration 21421, loss = 4.95625459\n",
      "Iteration 21422, loss = 4.74123332\n",
      "Iteration 21423, loss = 6.72514673\n",
      "Iteration 21424, loss = 5.78025508\n",
      "Iteration 21425, loss = 5.67137019\n",
      "Iteration 21426, loss = 5.56196652\n",
      "Iteration 21427, loss = 5.17696419\n",
      "Iteration 21428, loss = 4.87009322\n",
      "Iteration 21429, loss = 4.83763467\n",
      "Iteration 21430, loss = 5.02301770\n",
      "Iteration 21431, loss = 5.73685204\n",
      "Iteration 21432, loss = 5.51681201\n",
      "Iteration 21433, loss = 6.05940252\n",
      "Iteration 21434, loss = 5.94210757\n",
      "Iteration 21435, loss = 5.88569698\n",
      "Iteration 21436, loss = 5.44520440\n",
      "Iteration 21437, loss = 5.15960082\n",
      "Iteration 21438, loss = 5.77144016\n",
      "Iteration 21439, loss = 5.99455450\n",
      "Iteration 21440, loss = 5.06747326\n",
      "Iteration 21441, loss = 4.90135272\n",
      "Iteration 21442, loss = 4.88679196\n",
      "Iteration 21443, loss = 4.87591921\n",
      "Iteration 21444, loss = 5.03456284\n",
      "Iteration 21445, loss = 4.98590894\n",
      "Iteration 21446, loss = 4.92121590\n",
      "Iteration 21447, loss = 4.62454581\n",
      "Iteration 21448, loss = 5.17344568\n",
      "Iteration 21449, loss = 4.87970232\n",
      "Iteration 21450, loss = 5.66624202\n",
      "Iteration 21451, loss = 5.07777426\n",
      "Iteration 21452, loss = 5.50890602\n",
      "Iteration 21453, loss = 5.22345198\n",
      "Iteration 21454, loss = 4.95417773\n",
      "Iteration 21455, loss = 4.90107768\n",
      "Iteration 21456, loss = 4.72889233\n",
      "Iteration 21457, loss = 6.54932958\n",
      "Iteration 21458, loss = 5.79988111\n",
      "Iteration 21459, loss = 5.30171869\n",
      "Iteration 21460, loss = 5.19604692\n",
      "Iteration 21461, loss = 5.38724742\n",
      "Iteration 21462, loss = 5.25764221\n",
      "Iteration 21463, loss = 4.76518767\n",
      "Iteration 21464, loss = 4.81646848\n",
      "Iteration 21465, loss = 4.88127229\n",
      "Iteration 21466, loss = 5.06664199\n",
      "Iteration 21467, loss = 5.10890291\n",
      "Iteration 21468, loss = 4.83545267\n",
      "Iteration 21469, loss = 4.85256763\n",
      "Iteration 21470, loss = 6.35921801\n",
      "Iteration 21471, loss = 5.70963591\n",
      "Iteration 21472, loss = 6.14037456\n",
      "Iteration 21473, loss = 5.43184204\n",
      "Iteration 21474, loss = 5.27667617\n",
      "Iteration 21475, loss = 4.93712468\n",
      "Iteration 21476, loss = 5.30499560\n",
      "Iteration 21477, loss = 5.27056971\n",
      "Iteration 21478, loss = 5.53261646\n",
      "Iteration 21479, loss = 5.39685151\n",
      "Iteration 21480, loss = 5.12099616\n",
      "Iteration 21481, loss = 4.86658776\n",
      "Iteration 21482, loss = 5.24282801\n",
      "Iteration 21483, loss = 4.89120564\n",
      "Iteration 21484, loss = 5.11577333\n",
      "Iteration 21485, loss = 5.03190027\n",
      "Iteration 21486, loss = 4.72118854\n",
      "Iteration 21487, loss = 5.32927370\n",
      "Iteration 21488, loss = 6.19921232\n",
      "Iteration 21489, loss = 6.25884152\n",
      "Iteration 21490, loss = 6.30149226\n",
      "Iteration 21491, loss = 5.99638280\n",
      "Iteration 21492, loss = 5.91466184\n",
      "Iteration 21493, loss = 5.13906688\n",
      "Iteration 21494, loss = 5.56765494\n",
      "Iteration 21495, loss = 5.75840251\n",
      "Iteration 21496, loss = 5.10745786\n",
      "Iteration 21497, loss = 5.71863259\n",
      "Iteration 21498, loss = 5.55835988\n",
      "Iteration 21499, loss = 6.01903524\n",
      "Iteration 21500, loss = 5.66957834\n",
      "Iteration 21501, loss = 5.94907131\n",
      "Iteration 21502, loss = 5.48112476\n",
      "Iteration 21503, loss = 5.76468521\n",
      "Iteration 21504, loss = 5.14370066\n",
      "Iteration 21505, loss = 6.05716541\n",
      "Iteration 21506, loss = 4.96862948\n",
      "Iteration 21507, loss = 5.80424066\n",
      "Iteration 21508, loss = 5.56270794\n",
      "Iteration 21509, loss = 5.59522269\n",
      "Iteration 21510, loss = 6.14920889\n",
      "Iteration 21511, loss = 5.46674454\n",
      "Iteration 21512, loss = 6.18808409\n",
      "Iteration 21513, loss = 5.75869075\n",
      "Iteration 21514, loss = 5.58283317\n",
      "Iteration 21515, loss = 6.28487532\n",
      "Iteration 21516, loss = 5.80173106\n",
      "Iteration 21517, loss = 5.61448070\n",
      "Iteration 21518, loss = 5.67894860\n",
      "Iteration 21519, loss = 6.44240236\n",
      "Iteration 21520, loss = 5.69657063\n",
      "Iteration 21521, loss = 5.91737663\n",
      "Iteration 21522, loss = 5.20803965\n",
      "Iteration 21523, loss = 5.21028578\n",
      "Iteration 21524, loss = 5.21989278\n",
      "Iteration 21525, loss = 5.38833545\n",
      "Iteration 21526, loss = 5.11258328\n",
      "Iteration 21527, loss = 5.35835204\n",
      "Iteration 21528, loss = 6.75003758\n",
      "Iteration 21529, loss = 5.81948370\n",
      "Iteration 21530, loss = 5.80861949\n",
      "Iteration 21531, loss = 5.13184162\n",
      "Iteration 21532, loss = 5.03649051\n",
      "Iteration 21533, loss = 5.35404860\n",
      "Iteration 21534, loss = 5.89447674\n",
      "Iteration 21535, loss = 6.47046435\n",
      "Iteration 21536, loss = 6.02643831\n",
      "Iteration 21537, loss = 5.20642773\n",
      "Iteration 21538, loss = 4.95542495\n",
      "Iteration 21539, loss = 5.02390322\n",
      "Iteration 21540, loss = 5.00668843\n",
      "Iteration 21541, loss = 4.76530472\n",
      "Iteration 21542, loss = 4.83348211\n",
      "Iteration 21543, loss = 4.83088954\n",
      "Iteration 21544, loss = 5.65293431\n",
      "Iteration 21545, loss = 5.75721133\n",
      "Iteration 21546, loss = 6.13367061\n",
      "Iteration 21547, loss = 5.40838004\n",
      "Iteration 21548, loss = 5.47679630\n",
      "Iteration 21549, loss = 5.83983395\n",
      "Iteration 21550, loss = 5.64134623\n",
      "Iteration 21551, loss = 5.25610204\n",
      "Iteration 21552, loss = 5.33592916\n",
      "Iteration 21553, loss = 5.57431124\n",
      "Iteration 21554, loss = 5.67722479\n",
      "Iteration 21555, loss = 5.44040964\n",
      "Iteration 21556, loss = 5.34768617\n",
      "Iteration 21557, loss = 5.50968479\n",
      "Iteration 21558, loss = 7.06361901\n",
      "Iteration 21559, loss = 7.33262807\n",
      "Iteration 21560, loss = 5.96889674\n",
      "Iteration 21561, loss = 5.76830593\n",
      "Iteration 21562, loss = 4.82042708\n",
      "Iteration 21563, loss = 4.86369635\n",
      "Iteration 21564, loss = 4.70960556\n",
      "Iteration 21565, loss = 4.98979336\n",
      "Iteration 21566, loss = 4.83409658\n",
      "Iteration 21567, loss = 4.79407330\n",
      "Iteration 21568, loss = 5.06657253\n",
      "Iteration 21569, loss = 4.70607608\n",
      "Iteration 21570, loss = 4.78489330\n",
      "Iteration 21571, loss = 5.20549991\n",
      "Iteration 21572, loss = 5.10067745\n",
      "Iteration 21573, loss = 5.16905768\n",
      "Iteration 21574, loss = 5.07952974\n",
      "Iteration 21575, loss = 5.56444014\n",
      "Iteration 21576, loss = 5.24480474\n",
      "Iteration 21577, loss = 5.17701291\n",
      "Iteration 21578, loss = 5.55270835\n",
      "Iteration 21579, loss = 5.02924469\n",
      "Iteration 21580, loss = 5.19560613\n",
      "Iteration 21581, loss = 4.98204293\n",
      "Iteration 21582, loss = 5.03100385\n",
      "Iteration 21583, loss = 6.09278848\n",
      "Iteration 21584, loss = 5.56227404\n",
      "Iteration 21585, loss = 6.73629280\n",
      "Iteration 21586, loss = 6.00730891\n",
      "Iteration 21587, loss = 4.98871059\n",
      "Iteration 21588, loss = 4.87672224\n",
      "Iteration 21589, loss = 4.85225299\n",
      "Iteration 21590, loss = 4.72095483\n",
      "Iteration 21591, loss = 4.88579534\n",
      "Iteration 21592, loss = 6.56069127\n",
      "Iteration 21593, loss = 6.41069646\n",
      "Iteration 21594, loss = 5.74366176\n",
      "Iteration 21595, loss = 5.40917467\n",
      "Iteration 21596, loss = 6.29598197\n",
      "Iteration 21597, loss = 6.16234059\n",
      "Iteration 21598, loss = 5.26217527\n",
      "Iteration 21599, loss = 6.29389615\n",
      "Iteration 21600, loss = 5.68422395\n",
      "Iteration 21601, loss = 5.85598464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21602, loss = 5.01643473\n",
      "Iteration 21603, loss = 5.81154970\n",
      "Iteration 21604, loss = 5.39661720\n",
      "Iteration 21605, loss = 5.47724928\n",
      "Iteration 21606, loss = 5.35763152\n",
      "Iteration 21607, loss = 5.77559521\n",
      "Iteration 21608, loss = 5.83201903\n",
      "Iteration 21609, loss = 5.85465028\n",
      "Iteration 21610, loss = 5.54128414\n",
      "Iteration 21611, loss = 5.58384166\n",
      "Iteration 21612, loss = 5.62413862\n",
      "Iteration 21613, loss = 7.24515835\n",
      "Iteration 21614, loss = 7.40825296\n",
      "Iteration 21615, loss = 6.84188494\n",
      "Iteration 21616, loss = 6.32758356\n",
      "Iteration 21617, loss = 5.94861466\n",
      "Iteration 21618, loss = 6.97796505\n",
      "Iteration 21619, loss = 5.12227764\n",
      "Iteration 21620, loss = 5.50603615\n",
      "Iteration 21621, loss = 5.15847549\n",
      "Iteration 21622, loss = 5.13219190\n",
      "Iteration 21623, loss = 5.94065515\n",
      "Iteration 21624, loss = 5.63675942\n",
      "Iteration 21625, loss = 4.97557616\n",
      "Iteration 21626, loss = 5.51796041\n",
      "Iteration 21627, loss = 5.18354980\n",
      "Iteration 21628, loss = 5.39044325\n",
      "Iteration 21629, loss = 5.83285427\n",
      "Iteration 21630, loss = 5.15386953\n",
      "Iteration 21631, loss = 5.10309854\n",
      "Iteration 21632, loss = 4.95370615\n",
      "Iteration 21633, loss = 5.25006240\n",
      "Iteration 21634, loss = 5.93766350\n",
      "Iteration 21635, loss = 5.18078567\n",
      "Iteration 21636, loss = 5.08554798\n",
      "Iteration 21637, loss = 4.68515990\n",
      "Iteration 21638, loss = 4.81572795\n",
      "Iteration 21639, loss = 5.19092163\n",
      "Iteration 21640, loss = 5.54059938\n",
      "Iteration 21641, loss = 6.40903863\n",
      "Iteration 21642, loss = 6.05432700\n",
      "Iteration 21643, loss = 6.47973314\n",
      "Iteration 21644, loss = 6.49269905\n",
      "Iteration 21645, loss = 7.68032172\n",
      "Iteration 21646, loss = 8.26096480\n",
      "Iteration 21647, loss = 6.32089273\n",
      "Iteration 21648, loss = 5.91399497\n",
      "Iteration 21649, loss = 6.00437579\n",
      "Iteration 21650, loss = 4.55857113\n",
      "Iteration 21651, loss = 6.54083694\n",
      "Iteration 21652, loss = 6.05539975\n",
      "Iteration 21653, loss = 6.12815182\n",
      "Iteration 21654, loss = 5.26348217\n",
      "Iteration 21655, loss = 5.22195934\n",
      "Iteration 21656, loss = 4.94104343\n",
      "Iteration 21657, loss = 5.26559052\n",
      "Iteration 21658, loss = 5.32367681\n",
      "Iteration 21659, loss = 4.62654432\n",
      "Iteration 21660, loss = 4.98956167\n",
      "Iteration 21661, loss = 5.65835943\n",
      "Iteration 21662, loss = 5.40781724\n",
      "Iteration 21663, loss = 5.87333561\n",
      "Iteration 21664, loss = 5.73520083\n",
      "Iteration 21665, loss = 5.29144087\n",
      "Iteration 21666, loss = 4.94310136\n",
      "Iteration 21667, loss = 5.95954418\n",
      "Iteration 21668, loss = 5.32848671\n",
      "Iteration 21669, loss = 5.76334491\n",
      "Iteration 21670, loss = 4.92562015\n",
      "Iteration 21671, loss = 6.06107266\n",
      "Iteration 21672, loss = 4.82865732\n",
      "Iteration 21673, loss = 5.18235737\n",
      "Iteration 21674, loss = 4.78555195\n",
      "Iteration 21675, loss = 5.24679766\n",
      "Iteration 21676, loss = 4.95540126\n",
      "Iteration 21677, loss = 5.69157462\n",
      "Iteration 21678, loss = 6.67380476\n",
      "Iteration 21679, loss = 6.97794497\n",
      "Iteration 21680, loss = 7.25854402\n",
      "Iteration 21681, loss = 6.19836023\n",
      "Iteration 21682, loss = 5.48706043\n",
      "Iteration 21683, loss = 4.89222166\n",
      "Iteration 21684, loss = 5.69976495\n",
      "Iteration 21685, loss = 5.02339015\n",
      "Iteration 21686, loss = 5.19830007\n",
      "Iteration 21687, loss = 5.00820115\n",
      "Iteration 21688, loss = 5.07307487\n",
      "Iteration 21689, loss = 5.23172592\n",
      "Iteration 21690, loss = 5.73431425\n",
      "Iteration 21691, loss = 6.34101892\n",
      "Iteration 21692, loss = 6.03333702\n",
      "Iteration 21693, loss = 4.79862954\n",
      "Iteration 21694, loss = 4.88508980\n",
      "Iteration 21695, loss = 5.05803808\n",
      "Iteration 21696, loss = 5.31155326\n",
      "Iteration 21697, loss = 5.41085016\n",
      "Iteration 21698, loss = 5.42037700\n",
      "Iteration 21699, loss = 5.08995262\n",
      "Iteration 21700, loss = 5.17702906\n",
      "Iteration 21701, loss = 4.68950649\n",
      "Iteration 21702, loss = 4.53585196\n",
      "Iteration 21703, loss = 5.32442188\n",
      "Iteration 21704, loss = 6.00386807\n",
      "Iteration 21705, loss = 5.76771473\n",
      "Iteration 21706, loss = 6.04581126\n",
      "Iteration 21707, loss = 7.04810957\n",
      "Iteration 21708, loss = 5.51749972\n",
      "Iteration 21709, loss = 5.69766691\n",
      "Iteration 21710, loss = 4.81505088\n",
      "Iteration 21711, loss = 5.00055055\n",
      "Iteration 21712, loss = 4.83414645\n",
      "Iteration 21713, loss = 4.85834410\n",
      "Iteration 21714, loss = 4.74467037\n",
      "Iteration 21715, loss = 4.66675936\n",
      "Iteration 21716, loss = 4.70317851\n",
      "Iteration 21717, loss = 4.88732797\n",
      "Iteration 21718, loss = 5.14399856\n",
      "Iteration 21719, loss = 5.37766806\n",
      "Iteration 21720, loss = 5.93809428\n",
      "Iteration 21721, loss = 5.47139186\n",
      "Iteration 21722, loss = 6.71402113\n",
      "Iteration 21723, loss = 5.35283178\n",
      "Iteration 21724, loss = 5.95429595\n",
      "Iteration 21725, loss = 5.25634117\n",
      "Iteration 21726, loss = 5.97651117\n",
      "Iteration 21727, loss = 5.97557532\n",
      "Iteration 21728, loss = 5.87520812\n",
      "Iteration 21729, loss = 5.38967880\n",
      "Iteration 21730, loss = 4.70479392\n",
      "Iteration 21731, loss = 5.06015639\n",
      "Iteration 21732, loss = 4.86345465\n",
      "Iteration 21733, loss = 4.85287601\n",
      "Iteration 21734, loss = 4.87470101\n",
      "Iteration 21735, loss = 4.89060968\n",
      "Iteration 21736, loss = 5.12432710\n",
      "Iteration 21737, loss = 4.79480444\n",
      "Iteration 21738, loss = 5.16093844\n",
      "Iteration 21739, loss = 4.98438705\n",
      "Iteration 21740, loss = 5.03463864\n",
      "Iteration 21741, loss = 5.00429868\n",
      "Iteration 21742, loss = 5.32695231\n",
      "Iteration 21743, loss = 5.58925065\n",
      "Iteration 21744, loss = 5.12388583\n",
      "Iteration 21745, loss = 6.36429099\n",
      "Iteration 21746, loss = 6.17823922\n",
      "Iteration 21747, loss = 5.57499898\n",
      "Iteration 21748, loss = 5.71544521\n",
      "Iteration 21749, loss = 5.96839146\n",
      "Iteration 21750, loss = 5.36600201\n",
      "Iteration 21751, loss = 5.43467011\n",
      "Iteration 21752, loss = 6.24260942\n",
      "Iteration 21753, loss = 6.58964805\n",
      "Iteration 21754, loss = 6.40561764\n",
      "Iteration 21755, loss = 5.26187574\n",
      "Iteration 21756, loss = 5.52577627\n",
      "Iteration 21757, loss = 5.99700022\n",
      "Iteration 21758, loss = 5.10611028\n",
      "Iteration 21759, loss = 4.85830449\n",
      "Iteration 21760, loss = 4.82339322\n",
      "Iteration 21761, loss = 4.85225273\n",
      "Iteration 21762, loss = 4.74560482\n",
      "Iteration 21763, loss = 6.17025696\n",
      "Iteration 21764, loss = 5.34371215\n",
      "Iteration 21765, loss = 4.79823296\n",
      "Iteration 21766, loss = 4.79701749\n",
      "Iteration 21767, loss = 4.92036424\n",
      "Iteration 21768, loss = 4.69843307\n",
      "Iteration 21769, loss = 4.49775990\n",
      "Iteration 21770, loss = 4.56807568\n",
      "Iteration 21771, loss = 4.93599694\n",
      "Iteration 21772, loss = 5.18177404\n",
      "Iteration 21773, loss = 5.64655006\n",
      "Iteration 21774, loss = 4.93937349\n",
      "Iteration 21775, loss = 4.84999026\n",
      "Iteration 21776, loss = 4.79073773\n",
      "Iteration 21777, loss = 4.88049699\n",
      "Iteration 21778, loss = 4.92908727\n",
      "Iteration 21779, loss = 4.88032708\n",
      "Iteration 21780, loss = 4.78986883\n",
      "Iteration 21781, loss = 4.79077346\n",
      "Iteration 21782, loss = 4.61083927\n",
      "Iteration 21783, loss = 5.39479542\n",
      "Iteration 21784, loss = 6.09682776\n",
      "Iteration 21785, loss = 5.01503351\n",
      "Iteration 21786, loss = 5.01661407\n",
      "Iteration 21787, loss = 4.85194530\n",
      "Iteration 21788, loss = 4.84914262\n",
      "Iteration 21789, loss = 4.78344317\n",
      "Iteration 21790, loss = 5.15465194\n",
      "Iteration 21791, loss = 5.01205331\n",
      "Iteration 21792, loss = 5.40456377\n",
      "Iteration 21793, loss = 4.61873729\n",
      "Iteration 21794, loss = 5.83273121\n",
      "Iteration 21795, loss = 5.29641468\n",
      "Iteration 21796, loss = 5.52973251\n",
      "Iteration 21797, loss = 5.44907421\n",
      "Iteration 21798, loss = 6.65579925\n",
      "Iteration 21799, loss = 4.88831734\n",
      "Iteration 21800, loss = 5.10037700\n",
      "Iteration 21801, loss = 5.38391484\n",
      "Iteration 21802, loss = 5.86801371\n",
      "Iteration 21803, loss = 5.73752156\n",
      "Iteration 21804, loss = 6.21088747\n",
      "Iteration 21805, loss = 6.23176501\n",
      "Iteration 21806, loss = 5.03014551\n",
      "Iteration 21807, loss = 4.98801154\n",
      "Iteration 21808, loss = 4.89674964\n",
      "Iteration 21809, loss = 5.03048859\n",
      "Iteration 21810, loss = 4.91634877\n",
      "Iteration 21811, loss = 5.56425793\n",
      "Iteration 21812, loss = 5.04625392\n",
      "Iteration 21813, loss = 5.24461382\n",
      "Iteration 21814, loss = 5.30576713\n",
      "Iteration 21815, loss = 4.93688202\n",
      "Iteration 21816, loss = 4.71106991\n",
      "Iteration 21817, loss = 4.58926417\n",
      "Iteration 21818, loss = 4.45422971\n",
      "Iteration 21819, loss = 4.60776857\n",
      "Iteration 21820, loss = 5.22955888\n",
      "Iteration 21821, loss = 6.06466204\n",
      "Iteration 21822, loss = 5.39531567\n",
      "Iteration 21823, loss = 5.01170508\n",
      "Iteration 21824, loss = 6.07256440\n",
      "Iteration 21825, loss = 5.07173393\n",
      "Iteration 21826, loss = 6.09559460\n",
      "Iteration 21827, loss = 5.31320061\n",
      "Iteration 21828, loss = 4.88102282\n",
      "Iteration 21829, loss = 4.83106693\n",
      "Iteration 21830, loss = 5.24657509\n",
      "Iteration 21831, loss = 4.75726599\n",
      "Iteration 21832, loss = 5.07459180\n",
      "Iteration 21833, loss = 4.91453881\n",
      "Iteration 21834, loss = 5.26930139\n",
      "Iteration 21835, loss = 6.07805845\n",
      "Iteration 21836, loss = 5.82902268\n",
      "Iteration 21837, loss = 6.67515520\n",
      "Iteration 21838, loss = 6.00924146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21839, loss = 6.19917275\n",
      "Iteration 21840, loss = 6.53910304\n",
      "Iteration 21841, loss = 5.79075968\n",
      "Iteration 21842, loss = 5.90728099\n",
      "Iteration 21843, loss = 6.25329890\n",
      "Iteration 21844, loss = 5.11358415\n",
      "Iteration 21845, loss = 5.53437316\n",
      "Iteration 21846, loss = 5.01752280\n",
      "Iteration 21847, loss = 5.34383767\n",
      "Iteration 21848, loss = 5.20088570\n",
      "Iteration 21849, loss = 5.79376571\n",
      "Iteration 21850, loss = 4.95485200\n",
      "Iteration 21851, loss = 4.78224791\n",
      "Iteration 21852, loss = 4.71458293\n",
      "Iteration 21853, loss = 4.66601047\n",
      "Iteration 21854, loss = 5.27892366\n",
      "Iteration 21855, loss = 4.89431486\n",
      "Iteration 21856, loss = 4.96681529\n",
      "Iteration 21857, loss = 5.18132932\n",
      "Iteration 21858, loss = 5.41035933\n",
      "Iteration 21859, loss = 5.05217475\n",
      "Iteration 21860, loss = 4.65167672\n",
      "Iteration 21861, loss = 4.96242223\n",
      "Iteration 21862, loss = 4.59637896\n",
      "Iteration 21863, loss = 4.60853483\n",
      "Iteration 21864, loss = 5.19950187\n",
      "Iteration 21865, loss = 4.67053620\n",
      "Iteration 21866, loss = 4.81470323\n",
      "Iteration 21867, loss = 4.93261695\n",
      "Iteration 21868, loss = 5.07373213\n",
      "Iteration 21869, loss = 4.72640317\n",
      "Iteration 21870, loss = 4.80184001\n",
      "Iteration 21871, loss = 5.16382180\n",
      "Iteration 21872, loss = 4.71922596\n",
      "Iteration 21873, loss = 5.00764866\n",
      "Iteration 21874, loss = 5.56439227\n",
      "Iteration 21875, loss = 5.99913172\n",
      "Iteration 21876, loss = 5.43963554\n",
      "Iteration 21877, loss = 5.44521206\n",
      "Iteration 21878, loss = 5.01345305\n",
      "Iteration 21879, loss = 5.02877356\n",
      "Iteration 21880, loss = 5.77506833\n",
      "Iteration 21881, loss = 6.04831190\n",
      "Iteration 21882, loss = 5.63739979\n",
      "Iteration 21883, loss = 5.25168114\n",
      "Iteration 21884, loss = 5.53011661\n",
      "Iteration 21885, loss = 4.72991480\n",
      "Iteration 21886, loss = 4.69397785\n",
      "Iteration 21887, loss = 4.73485320\n",
      "Iteration 21888, loss = 4.73454719\n",
      "Iteration 21889, loss = 4.46277711\n",
      "Iteration 21890, loss = 5.27789448\n",
      "Iteration 21891, loss = 5.06418943\n",
      "Iteration 21892, loss = 5.59655538\n",
      "Iteration 21893, loss = 4.97557154\n",
      "Iteration 21894, loss = 5.44588852\n",
      "Iteration 21895, loss = 6.34451486\n",
      "Iteration 21896, loss = 5.69612844\n",
      "Iteration 21897, loss = 5.46198895\n",
      "Iteration 21898, loss = 5.44212450\n",
      "Iteration 21899, loss = 5.12582606\n",
      "Iteration 21900, loss = 4.84934821\n",
      "Iteration 21901, loss = 5.18716558\n",
      "Iteration 21902, loss = 5.48309680\n",
      "Iteration 21903, loss = 4.87208576\n",
      "Iteration 21904, loss = 5.05888102\n",
      "Iteration 21905, loss = 4.90390201\n",
      "Iteration 21906, loss = 4.89101633\n",
      "Iteration 21907, loss = 4.60263892\n",
      "Iteration 21908, loss = 4.87711909\n",
      "Iteration 21909, loss = 5.11905733\n",
      "Iteration 21910, loss = 5.89195211\n",
      "Iteration 21911, loss = 5.57013273\n",
      "Iteration 21912, loss = 5.19181326\n",
      "Iteration 21913, loss = 5.21743831\n",
      "Iteration 21914, loss = 5.23445101\n",
      "Iteration 21915, loss = 5.12496254\n",
      "Iteration 21916, loss = 5.01541982\n",
      "Iteration 21917, loss = 5.37238103\n",
      "Iteration 21918, loss = 4.88869150\n",
      "Iteration 21919, loss = 5.66483918\n",
      "Iteration 21920, loss = 6.08393064\n",
      "Iteration 21921, loss = 5.91372987\n",
      "Iteration 21922, loss = 5.30700177\n",
      "Iteration 21923, loss = 5.01775861\n",
      "Iteration 21924, loss = 4.76165005\n",
      "Iteration 21925, loss = 4.78961793\n",
      "Iteration 21926, loss = 4.80510071\n",
      "Iteration 21927, loss = 5.07222554\n",
      "Iteration 21928, loss = 4.96096074\n",
      "Iteration 21929, loss = 5.33092542\n",
      "Iteration 21930, loss = 5.07278022\n",
      "Iteration 21931, loss = 4.87106879\n",
      "Iteration 21932, loss = 4.73993902\n",
      "Iteration 21933, loss = 5.16652261\n",
      "Iteration 21934, loss = 5.08547029\n",
      "Iteration 21935, loss = 5.03307931\n",
      "Iteration 21936, loss = 5.14760630\n",
      "Iteration 21937, loss = 5.17973136\n",
      "Iteration 21938, loss = 5.61871266\n",
      "Iteration 21939, loss = 4.75166385\n",
      "Iteration 21940, loss = 5.37834679\n",
      "Iteration 21941, loss = 5.69459164\n",
      "Iteration 21942, loss = 6.27365078\n",
      "Iteration 21943, loss = 5.74957247\n",
      "Iteration 21944, loss = 5.23754074\n",
      "Iteration 21945, loss = 4.93728758\n",
      "Iteration 21946, loss = 5.21749285\n",
      "Iteration 21947, loss = 4.56369532\n",
      "Iteration 21948, loss = 4.97664673\n",
      "Iteration 21949, loss = 4.65229942\n",
      "Iteration 21950, loss = 4.74728228\n",
      "Iteration 21951, loss = 4.70418037\n",
      "Iteration 21952, loss = 5.15964752\n",
      "Iteration 21953, loss = 6.15022352\n",
      "Iteration 21954, loss = 6.57530045\n",
      "Iteration 21955, loss = 5.22515458\n",
      "Iteration 21956, loss = 4.68589287\n",
      "Iteration 21957, loss = 4.63529208\n",
      "Iteration 21958, loss = 4.48660837\n",
      "Iteration 21959, loss = 4.71816288\n",
      "Iteration 21960, loss = 4.97514061\n",
      "Iteration 21961, loss = 5.08347349\n",
      "Iteration 21962, loss = 4.85182911\n",
      "Iteration 21963, loss = 5.07041432\n",
      "Iteration 21964, loss = 5.75296025\n",
      "Iteration 21965, loss = 6.75074216\n",
      "Iteration 21966, loss = 6.79039234\n",
      "Iteration 21967, loss = 7.44721910\n",
      "Iteration 21968, loss = 6.25981569\n",
      "Iteration 21969, loss = 6.47501875\n",
      "Iteration 21970, loss = 6.67789379\n",
      "Iteration 21971, loss = 6.78927987\n",
      "Iteration 21972, loss = 5.52479041\n",
      "Iteration 21973, loss = 5.74329129\n",
      "Iteration 21974, loss = 4.58372460\n",
      "Iteration 21975, loss = 5.45196700\n",
      "Iteration 21976, loss = 5.23432271\n",
      "Iteration 21977, loss = 5.75986324\n",
      "Iteration 21978, loss = 5.83869205\n",
      "Iteration 21979, loss = 4.80374865\n",
      "Iteration 21980, loss = 5.65238237\n",
      "Iteration 21981, loss = 4.83564774\n",
      "Iteration 21982, loss = 4.66613682\n",
      "Iteration 21983, loss = 4.58368391\n",
      "Iteration 21984, loss = 4.74952382\n",
      "Iteration 21985, loss = 5.21218724\n",
      "Iteration 21986, loss = 5.40714091\n",
      "Iteration 21987, loss = 5.33096835\n",
      "Iteration 21988, loss = 4.67576326\n",
      "Iteration 21989, loss = 5.06639154\n",
      "Iteration 21990, loss = 5.34832546\n",
      "Iteration 21991, loss = 5.19738746\n",
      "Iteration 21992, loss = 4.71270866\n",
      "Iteration 21993, loss = 4.73963906\n",
      "Iteration 21994, loss = 4.91144985\n",
      "Iteration 21995, loss = 4.57553432\n",
      "Iteration 21996, loss = 5.04337895\n",
      "Iteration 21997, loss = 5.21410581\n",
      "Iteration 21998, loss = 6.58573618\n",
      "Iteration 21999, loss = 6.19543984\n",
      "Iteration 22000, loss = 5.55505882\n",
      "Iteration 22001, loss = 5.24388387\n",
      "Iteration 22002, loss = 5.68888938\n",
      "Iteration 22003, loss = 5.39058344\n",
      "Iteration 22004, loss = 5.27689186\n",
      "Iteration 22005, loss = 4.86287776\n",
      "Iteration 22006, loss = 4.78582901\n",
      "Iteration 22007, loss = 5.02064733\n",
      "Iteration 22008, loss = 5.43513546\n",
      "Iteration 22009, loss = 5.51265934\n",
      "Iteration 22010, loss = 5.24003532\n",
      "Iteration 22011, loss = 5.29853241\n",
      "Iteration 22012, loss = 5.28085356\n",
      "Iteration 22013, loss = 5.45800899\n",
      "Iteration 22014, loss = 5.06895040\n",
      "Iteration 22015, loss = 5.37323535\n",
      "Iteration 22016, loss = 5.72439194\n",
      "Iteration 22017, loss = 8.05477674\n",
      "Iteration 22018, loss = 5.39674303\n",
      "Iteration 22019, loss = 5.24219422\n",
      "Iteration 22020, loss = 5.00511238\n",
      "Iteration 22021, loss = 4.72728521\n",
      "Iteration 22022, loss = 4.57128374\n",
      "Iteration 22023, loss = 4.96819168\n",
      "Iteration 22024, loss = 4.93884249\n",
      "Iteration 22025, loss = 5.06369759\n",
      "Iteration 22026, loss = 4.66772313\n",
      "Iteration 22027, loss = 5.96146487\n",
      "Iteration 22028, loss = 5.70348143\n",
      "Iteration 22029, loss = 5.33861162\n",
      "Iteration 22030, loss = 6.11898776\n",
      "Iteration 22031, loss = 5.26409910\n",
      "Iteration 22032, loss = 5.39780168\n",
      "Iteration 22033, loss = 5.05683959\n",
      "Iteration 22034, loss = 4.65616101\n",
      "Iteration 22035, loss = 4.64019000\n",
      "Iteration 22036, loss = 4.51098835\n",
      "Iteration 22037, loss = 4.90264782\n",
      "Iteration 22038, loss = 4.93229105\n",
      "Iteration 22039, loss = 5.23658364\n",
      "Iteration 22040, loss = 4.94052104\n",
      "Iteration 22041, loss = 4.81225849\n",
      "Iteration 22042, loss = 4.96258591\n",
      "Iteration 22043, loss = 5.28124334\n",
      "Iteration 22044, loss = 5.01525583\n",
      "Iteration 22045, loss = 5.25812949\n",
      "Iteration 22046, loss = 5.51822211\n",
      "Iteration 22047, loss = 5.47774541\n",
      "Iteration 22048, loss = 5.55226523\n",
      "Iteration 22049, loss = 5.30556839\n",
      "Iteration 22050, loss = 4.97226566\n",
      "Iteration 22051, loss = 4.62737241\n",
      "Iteration 22052, loss = 4.69752579\n",
      "Iteration 22053, loss = 4.98526306\n",
      "Iteration 22054, loss = 5.06811447\n",
      "Iteration 22055, loss = 4.96851842\n",
      "Iteration 22056, loss = 5.02836755\n",
      "Iteration 22057, loss = 4.73633010\n",
      "Iteration 22058, loss = 4.56237137\n",
      "Iteration 22059, loss = 4.98369888\n",
      "Iteration 22060, loss = 4.94515629\n",
      "Iteration 22061, loss = 4.61414428\n",
      "Iteration 22062, loss = 4.63687343\n",
      "Iteration 22063, loss = 4.98734705\n",
      "Iteration 22064, loss = 4.53050737\n",
      "Iteration 22065, loss = 4.94175213\n",
      "Iteration 22066, loss = 4.88120060\n",
      "Iteration 22067, loss = 4.91618777\n",
      "Iteration 22068, loss = 5.33055409\n",
      "Iteration 22069, loss = 5.06317932\n",
      "Iteration 22070, loss = 4.68792307\n",
      "Iteration 22071, loss = 4.68870898\n",
      "Iteration 22072, loss = 4.80624862\n",
      "Iteration 22073, loss = 5.45784966\n",
      "Iteration 22074, loss = 4.91803287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22075, loss = 4.78323275\n",
      "Iteration 22076, loss = 5.27906341\n",
      "Iteration 22077, loss = 5.66991436\n",
      "Iteration 22078, loss = 5.78991895\n",
      "Iteration 22079, loss = 5.59180551\n",
      "Iteration 22080, loss = 4.96119937\n",
      "Iteration 22081, loss = 4.37085420\n",
      "Iteration 22082, loss = 4.64619125\n",
      "Iteration 22083, loss = 5.26958135\n",
      "Iteration 22084, loss = 5.55142655\n",
      "Iteration 22085, loss = 6.15772648\n",
      "Iteration 22086, loss = 5.33794520\n",
      "Iteration 22087, loss = 4.87902157\n",
      "Iteration 22088, loss = 4.75307358\n",
      "Iteration 22089, loss = 4.79991904\n",
      "Iteration 22090, loss = 4.90669687\n",
      "Iteration 22091, loss = 4.82927446\n",
      "Iteration 22092, loss = 4.71599491\n",
      "Iteration 22093, loss = 4.64670961\n",
      "Iteration 22094, loss = 4.47995041\n",
      "Iteration 22095, loss = 5.10810070\n",
      "Iteration 22096, loss = 5.29296477\n",
      "Iteration 22097, loss = 6.88926200\n",
      "Iteration 22098, loss = 5.16067638\n",
      "Iteration 22099, loss = 5.19202376\n",
      "Iteration 22100, loss = 4.99367062\n",
      "Iteration 22101, loss = 5.13515054\n",
      "Iteration 22102, loss = 5.09531471\n",
      "Iteration 22103, loss = 5.47260229\n",
      "Iteration 22104, loss = 5.18396610\n",
      "Iteration 22105, loss = 4.85551619\n",
      "Iteration 22106, loss = 5.05317906\n",
      "Iteration 22107, loss = 4.42021074\n",
      "Iteration 22108, loss = 5.35339507\n",
      "Iteration 22109, loss = 5.00985184\n",
      "Iteration 22110, loss = 5.19989053\n",
      "Iteration 22111, loss = 4.66927304\n",
      "Iteration 22112, loss = 5.00726028\n",
      "Iteration 22113, loss = 5.18833941\n",
      "Iteration 22114, loss = 5.13596727\n",
      "Iteration 22115, loss = 5.12889511\n",
      "Iteration 22116, loss = 4.55265454\n",
      "Iteration 22117, loss = 4.75082482\n",
      "Iteration 22118, loss = 5.00499573\n",
      "Iteration 22119, loss = 4.98868359\n",
      "Iteration 22120, loss = 5.10630942\n",
      "Iteration 22121, loss = 6.22783765\n",
      "Iteration 22122, loss = 6.10403192\n",
      "Iteration 22123, loss = 6.26347694\n",
      "Iteration 22124, loss = 5.43230188\n",
      "Iteration 22125, loss = 5.12145778\n",
      "Iteration 22126, loss = 5.64207273\n",
      "Iteration 22127, loss = 6.22534235\n",
      "Iteration 22128, loss = 5.36312006\n",
      "Iteration 22129, loss = 5.25084319\n",
      "Iteration 22130, loss = 5.30014475\n",
      "Iteration 22131, loss = 4.72116907\n",
      "Iteration 22132, loss = 4.94250076\n",
      "Iteration 22133, loss = 4.68725035\n",
      "Iteration 22134, loss = 5.47033582\n",
      "Iteration 22135, loss = 4.92293526\n",
      "Iteration 22136, loss = 5.30814425\n",
      "Iteration 22137, loss = 5.11979915\n",
      "Iteration 22138, loss = 5.32798536\n",
      "Iteration 22139, loss = 5.22914896\n",
      "Iteration 22140, loss = 6.18041977\n",
      "Iteration 22141, loss = 6.73579040\n",
      "Iteration 22142, loss = 5.56199998\n",
      "Iteration 22143, loss = 5.09871596\n",
      "Iteration 22144, loss = 4.71429161\n",
      "Iteration 22145, loss = 4.68910098\n",
      "Iteration 22146, loss = 4.71459457\n",
      "Iteration 22147, loss = 4.45390827\n",
      "Iteration 22148, loss = 4.93786138\n",
      "Iteration 22149, loss = 5.10375278\n",
      "Iteration 22150, loss = 4.58905968\n",
      "Iteration 22151, loss = 4.70683438\n",
      "Iteration 22152, loss = 4.61603212\n",
      "Iteration 22153, loss = 4.68406942\n",
      "Iteration 22154, loss = 4.43535214\n",
      "Iteration 22155, loss = 5.13766863\n",
      "Iteration 22156, loss = 5.64690052\n",
      "Iteration 22157, loss = 5.07190945\n",
      "Iteration 22158, loss = 5.32204271\n",
      "Iteration 22159, loss = 5.27799011\n",
      "Iteration 22160, loss = 5.20355701\n",
      "Iteration 22161, loss = 4.70215448\n",
      "Iteration 22162, loss = 5.40903066\n",
      "Iteration 22163, loss = 5.36899637\n",
      "Iteration 22164, loss = 5.18625927\n",
      "Iteration 22165, loss = 4.94206818\n",
      "Iteration 22166, loss = 5.18704062\n",
      "Iteration 22167, loss = 4.85500335\n",
      "Iteration 22168, loss = 4.82503581\n",
      "Iteration 22169, loss = 5.69269620\n",
      "Iteration 22170, loss = 5.65523425\n",
      "Iteration 22171, loss = 5.91326020\n",
      "Iteration 22172, loss = 4.51127117\n",
      "Iteration 22173, loss = 4.68467780\n",
      "Iteration 22174, loss = 4.73043170\n",
      "Iteration 22175, loss = 4.60268517\n",
      "Iteration 22176, loss = 4.65242141\n",
      "Iteration 22177, loss = 4.59822495\n",
      "Iteration 22178, loss = 5.04658885\n",
      "Iteration 22179, loss = 4.99866550\n",
      "Iteration 22180, loss = 4.96427645\n",
      "Iteration 22181, loss = 4.51255849\n",
      "Iteration 22182, loss = 5.40693976\n",
      "Iteration 22183, loss = 5.61165509\n",
      "Iteration 22184, loss = 6.25897874\n",
      "Iteration 22185, loss = 5.10187385\n",
      "Iteration 22186, loss = 5.11998356\n",
      "Iteration 22187, loss = 4.95994538\n",
      "Iteration 22188, loss = 4.64608982\n",
      "Iteration 22189, loss = 4.68169091\n",
      "Iteration 22190, loss = 4.83374630\n",
      "Iteration 22191, loss = 4.82054399\n",
      "Iteration 22192, loss = 5.02354114\n",
      "Iteration 22193, loss = 4.45159330\n",
      "Iteration 22194, loss = 4.94739289\n",
      "Iteration 22195, loss = 4.80184659\n",
      "Iteration 22196, loss = 5.62184809\n",
      "Iteration 22197, loss = 6.06962438\n",
      "Iteration 22198, loss = 5.97026898\n",
      "Iteration 22199, loss = 6.91496950\n",
      "Iteration 22200, loss = 6.23563404\n",
      "Iteration 22201, loss = 5.87320753\n",
      "Iteration 22202, loss = 4.91941695\n",
      "Iteration 22203, loss = 4.84782280\n",
      "Iteration 22204, loss = 4.78480220\n",
      "Iteration 22205, loss = 4.94727269\n",
      "Iteration 22206, loss = 4.81453798\n",
      "Iteration 22207, loss = 4.66452226\n",
      "Iteration 22208, loss = 4.67938444\n",
      "Iteration 22209, loss = 4.57540888\n",
      "Iteration 22210, loss = 4.57802301\n",
      "Iteration 22211, loss = 4.75175163\n",
      "Iteration 22212, loss = 4.99046748\n",
      "Iteration 22213, loss = 4.97588343\n",
      "Iteration 22214, loss = 4.82006234\n",
      "Iteration 22215, loss = 4.95019030\n",
      "Iteration 22216, loss = 4.99878209\n",
      "Iteration 22217, loss = 4.66325780\n",
      "Iteration 22218, loss = 5.45798447\n",
      "Iteration 22219, loss = 4.91980928\n",
      "Iteration 22220, loss = 5.76113789\n",
      "Iteration 22221, loss = 5.45370185\n",
      "Iteration 22222, loss = 4.68651449\n",
      "Iteration 22223, loss = 5.06859839\n",
      "Iteration 22224, loss = 6.04743591\n",
      "Iteration 22225, loss = 6.30179442\n",
      "Iteration 22226, loss = 5.39728068\n",
      "Iteration 22227, loss = 5.98262744\n",
      "Iteration 22228, loss = 4.89258654\n",
      "Iteration 22229, loss = 5.09091732\n",
      "Iteration 22230, loss = 4.58087278\n",
      "Iteration 22231, loss = 4.88930464\n",
      "Iteration 22232, loss = 5.03428965\n",
      "Iteration 22233, loss = 4.83382352\n",
      "Iteration 22234, loss = 4.71729376\n",
      "Iteration 22235, loss = 4.50759080\n",
      "Iteration 22236, loss = 4.65473751\n",
      "Iteration 22237, loss = 4.72432646\n",
      "Iteration 22238, loss = 4.95551052\n",
      "Iteration 22239, loss = 5.24153623\n",
      "Iteration 22240, loss = 4.73358217\n",
      "Iteration 22241, loss = 4.60588825\n",
      "Iteration 22242, loss = 4.84488197\n",
      "Iteration 22243, loss = 5.05663100\n",
      "Iteration 22244, loss = 5.23785822\n",
      "Iteration 22245, loss = 5.43490390\n",
      "Iteration 22246, loss = 5.41631770\n",
      "Iteration 22247, loss = 5.24990557\n",
      "Iteration 22248, loss = 4.50741019\n",
      "Iteration 22249, loss = 5.52937067\n",
      "Iteration 22250, loss = 5.50037596\n",
      "Iteration 22251, loss = 5.18178574\n",
      "Iteration 22252, loss = 4.74424594\n",
      "Iteration 22253, loss = 5.38765917\n",
      "Iteration 22254, loss = 5.25128875\n",
      "Iteration 22255, loss = 5.97932538\n",
      "Iteration 22256, loss = 6.67453123\n",
      "Iteration 22257, loss = 6.06898800\n",
      "Iteration 22258, loss = 5.67010185\n",
      "Iteration 22259, loss = 5.17656523\n",
      "Iteration 22260, loss = 4.59649732\n",
      "Iteration 22261, loss = 5.32252806\n",
      "Iteration 22262, loss = 5.18485497\n",
      "Iteration 22263, loss = 5.38381945\n",
      "Iteration 22264, loss = 5.12815242\n",
      "Iteration 22265, loss = 5.39535270\n",
      "Iteration 22266, loss = 7.07354666\n",
      "Iteration 22267, loss = 7.52066299\n",
      "Iteration 22268, loss = 6.53000850\n",
      "Iteration 22269, loss = 4.34113970\n",
      "Iteration 22270, loss = 5.69492160\n",
      "Iteration 22271, loss = 4.82957145\n",
      "Iteration 22272, loss = 5.73291487\n",
      "Iteration 22273, loss = 6.77153115\n",
      "Iteration 22274, loss = 6.40515755\n",
      "Iteration 22275, loss = 5.62615643\n",
      "Iteration 22276, loss = 5.00784897\n",
      "Iteration 22277, loss = 4.65478537\n",
      "Iteration 22278, loss = 4.52673064\n",
      "Iteration 22279, loss = 4.80432275\n",
      "Iteration 22280, loss = 4.97370193\n",
      "Iteration 22281, loss = 5.83286595\n",
      "Iteration 22282, loss = 5.34325455\n",
      "Iteration 22283, loss = 4.98885543\n",
      "Iteration 22284, loss = 5.10002014\n",
      "Iteration 22285, loss = 5.16289608\n",
      "Iteration 22286, loss = 4.98402594\n",
      "Iteration 22287, loss = 5.19865277\n",
      "Iteration 22288, loss = 6.30627636\n",
      "Iteration 22289, loss = 5.42747063\n",
      "Iteration 22290, loss = 5.13039555\n",
      "Iteration 22291, loss = 5.37538440\n",
      "Iteration 22292, loss = 5.31080150\n",
      "Iteration 22293, loss = 5.14494870\n",
      "Iteration 22294, loss = 5.66144969\n",
      "Iteration 22295, loss = 5.41157544\n",
      "Iteration 22296, loss = 4.42742639\n",
      "Iteration 22297, loss = 4.65803375\n",
      "Iteration 22298, loss = 4.61344509\n",
      "Iteration 22299, loss = 4.82301716\n",
      "Iteration 22300, loss = 5.04887656\n",
      "Iteration 22301, loss = 4.55615860\n",
      "Iteration 22302, loss = 4.71742740\n",
      "Iteration 22303, loss = 4.45718541\n",
      "Iteration 22304, loss = 5.00167877\n",
      "Iteration 22305, loss = 4.75777325\n",
      "Iteration 22306, loss = 4.60357774\n",
      "Iteration 22307, loss = 4.69743544\n",
      "Iteration 22308, loss = 4.48080411\n",
      "Iteration 22309, loss = 6.37156575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22310, loss = 5.21390366\n",
      "Iteration 22311, loss = 5.53104537\n",
      "Iteration 22312, loss = 5.87674456\n",
      "Iteration 22313, loss = 6.15397482\n",
      "Iteration 22314, loss = 4.88904302\n",
      "Iteration 22315, loss = 5.50997850\n",
      "Iteration 22316, loss = 5.34040356\n",
      "Iteration 22317, loss = 4.66499756\n",
      "Iteration 22318, loss = 4.74303649\n",
      "Iteration 22319, loss = 4.82013490\n",
      "Iteration 22320, loss = 5.24560582\n",
      "Iteration 22321, loss = 5.00624469\n",
      "Iteration 22322, loss = 4.63640568\n",
      "Iteration 22323, loss = 5.30272595\n",
      "Iteration 22324, loss = 5.55971980\n",
      "Iteration 22325, loss = 5.70270392\n",
      "Iteration 22326, loss = 4.82337814\n",
      "Iteration 22327, loss = 5.74435442\n",
      "Iteration 22328, loss = 6.29584739\n",
      "Iteration 22329, loss = 5.07694827\n",
      "Iteration 22330, loss = 5.16181069\n",
      "Iteration 22331, loss = 4.97394909\n",
      "Iteration 22332, loss = 5.07023378\n",
      "Iteration 22333, loss = 6.03075273\n",
      "Iteration 22334, loss = 6.87453278\n",
      "Iteration 22335, loss = 8.42777614\n",
      "Iteration 22336, loss = 7.55058169\n",
      "Iteration 22337, loss = 7.04426138\n",
      "Iteration 22338, loss = 6.36258876\n",
      "Iteration 22339, loss = 6.97654860\n",
      "Iteration 22340, loss = 5.95477799\n",
      "Iteration 22341, loss = 5.35132708\n",
      "Iteration 22342, loss = 4.96273037\n",
      "Iteration 22343, loss = 5.71794822\n",
      "Iteration 22344, loss = 5.92490076\n",
      "Iteration 22345, loss = 5.39691897\n",
      "Iteration 22346, loss = 4.74136478\n",
      "Iteration 22347, loss = 5.39571081\n",
      "Iteration 22348, loss = 5.55506705\n",
      "Iteration 22349, loss = 5.23204429\n",
      "Iteration 22350, loss = 4.87246191\n",
      "Iteration 22351, loss = 5.67849177\n",
      "Iteration 22352, loss = 5.18259069\n",
      "Iteration 22353, loss = 4.76316130\n",
      "Iteration 22354, loss = 5.29007747\n",
      "Iteration 22355, loss = 5.48359961\n",
      "Iteration 22356, loss = 4.82001836\n",
      "Iteration 22357, loss = 4.65049411\n",
      "Iteration 22358, loss = 5.24680200\n",
      "Iteration 22359, loss = 5.14641580\n",
      "Iteration 22360, loss = 5.04149216\n",
      "Iteration 22361, loss = 6.51367375\n",
      "Iteration 22362, loss = 7.43968558\n",
      "Iteration 22363, loss = 6.36468525\n",
      "Iteration 22364, loss = 5.73488108\n",
      "Iteration 22365, loss = 5.44088020\n",
      "Iteration 22366, loss = 5.42473521\n",
      "Iteration 22367, loss = 5.43753523\n",
      "Iteration 22368, loss = 5.47677823\n",
      "Iteration 22369, loss = 5.12658072\n",
      "Iteration 22370, loss = 5.01973901\n",
      "Iteration 22371, loss = 4.67699048\n",
      "Iteration 22372, loss = 4.33555500\n",
      "Iteration 22373, loss = 4.75279648\n",
      "Iteration 22374, loss = 4.81601592\n",
      "Iteration 22375, loss = 4.58276408\n",
      "Iteration 22376, loss = 4.66208231\n",
      "Iteration 22377, loss = 4.69098743\n",
      "Iteration 22378, loss = 4.48786622\n",
      "Iteration 22379, loss = 4.61955130\n",
      "Iteration 22380, loss = 4.98445878\n",
      "Iteration 22381, loss = 5.41335116\n",
      "Iteration 22382, loss = 5.24405238\n",
      "Iteration 22383, loss = 4.88266607\n",
      "Iteration 22384, loss = 4.90174762\n",
      "Iteration 22385, loss = 4.58313101\n",
      "Iteration 22386, loss = 5.33371754\n",
      "Iteration 22387, loss = 4.64107494\n",
      "Iteration 22388, loss = 4.58983015\n",
      "Iteration 22389, loss = 4.52393901\n",
      "Iteration 22390, loss = 4.86111739\n",
      "Iteration 22391, loss = 4.90774471\n",
      "Iteration 22392, loss = 4.99252180\n",
      "Iteration 22393, loss = 4.87058057\n",
      "Iteration 22394, loss = 4.72487109\n",
      "Iteration 22395, loss = 4.67396613\n",
      "Iteration 22396, loss = 4.59298337\n",
      "Iteration 22397, loss = 4.52610296\n",
      "Iteration 22398, loss = 4.82064611\n",
      "Iteration 22399, loss = 5.29866025\n",
      "Iteration 22400, loss = 5.17376389\n",
      "Iteration 22401, loss = 4.45263221\n",
      "Iteration 22402, loss = 4.88894976\n",
      "Iteration 22403, loss = 4.74748040\n",
      "Iteration 22404, loss = 4.82571895\n",
      "Iteration 22405, loss = 4.70533024\n",
      "Iteration 22406, loss = 4.66620086\n",
      "Iteration 22407, loss = 4.44373235\n",
      "Iteration 22408, loss = 4.57337316\n",
      "Iteration 22409, loss = 4.64521871\n",
      "Iteration 22410, loss = 4.92750937\n",
      "Iteration 22411, loss = 5.23326954\n",
      "Iteration 22412, loss = 4.68141532\n",
      "Iteration 22413, loss = 5.14334674\n",
      "Iteration 22414, loss = 5.80212337\n",
      "Iteration 22415, loss = 5.04602138\n",
      "Iteration 22416, loss = 5.77442894\n",
      "Iteration 22417, loss = 5.08398983\n",
      "Iteration 22418, loss = 5.62778322\n",
      "Iteration 22419, loss = 6.83943336\n",
      "Iteration 22420, loss = 5.33526021\n",
      "Iteration 22421, loss = 5.70294339\n",
      "Iteration 22422, loss = 5.00027120\n",
      "Iteration 22423, loss = 4.72440196\n",
      "Iteration 22424, loss = 4.93652256\n",
      "Iteration 22425, loss = 4.92230078\n",
      "Iteration 22426, loss = 4.75962562\n",
      "Iteration 22427, loss = 5.86429875\n",
      "Iteration 22428, loss = 5.16431302\n",
      "Iteration 22429, loss = 4.78232947\n",
      "Iteration 22430, loss = 4.96975216\n",
      "Iteration 22431, loss = 4.45001659\n",
      "Iteration 22432, loss = 4.60379696\n",
      "Iteration 22433, loss = 4.94125738\n",
      "Iteration 22434, loss = 5.15238723\n",
      "Iteration 22435, loss = 4.87906011\n",
      "Iteration 22436, loss = 4.48885550\n",
      "Iteration 22437, loss = 4.86385128\n",
      "Iteration 22438, loss = 5.11511164\n",
      "Iteration 22439, loss = 4.44488832\n",
      "Iteration 22440, loss = 4.78949079\n",
      "Iteration 22441, loss = 5.13862010\n",
      "Iteration 22442, loss = 4.59530603\n",
      "Iteration 22443, loss = 5.20719417\n",
      "Iteration 22444, loss = 7.02645752\n",
      "Iteration 22445, loss = 6.30558801\n",
      "Iteration 22446, loss = 7.42723362\n",
      "Iteration 22447, loss = 6.12726457\n",
      "Iteration 22448, loss = 5.27341398\n",
      "Iteration 22449, loss = 4.50814641\n",
      "Iteration 22450, loss = 4.70201377\n",
      "Iteration 22451, loss = 5.02066130\n",
      "Iteration 22452, loss = 5.12756070\n",
      "Iteration 22453, loss = 5.12057311\n",
      "Iteration 22454, loss = 5.37462510\n",
      "Iteration 22455, loss = 6.24331708\n",
      "Iteration 22456, loss = 5.93138926\n",
      "Iteration 22457, loss = 5.37958283\n",
      "Iteration 22458, loss = 5.21078910\n",
      "Iteration 22459, loss = 4.87857966\n",
      "Iteration 22460, loss = 4.63208087\n",
      "Iteration 22461, loss = 4.94064324\n",
      "Iteration 22462, loss = 5.50017136\n",
      "Iteration 22463, loss = 4.94451385\n",
      "Iteration 22464, loss = 5.36829259\n",
      "Iteration 22465, loss = 4.79063354\n",
      "Iteration 22466, loss = 4.98465042\n",
      "Iteration 22467, loss = 5.01604390\n",
      "Iteration 22468, loss = 4.41188780\n",
      "Iteration 22469, loss = 4.82027050\n",
      "Iteration 22470, loss = 4.62570080\n",
      "Iteration 22471, loss = 5.12417189\n",
      "Iteration 22472, loss = 5.41145292\n",
      "Iteration 22473, loss = 6.00828659\n",
      "Iteration 22474, loss = 5.34410077\n",
      "Iteration 22475, loss = 4.65425762\n",
      "Iteration 22476, loss = 4.68224921\n",
      "Iteration 22477, loss = 5.04985440\n",
      "Iteration 22478, loss = 4.59111788\n",
      "Iteration 22479, loss = 4.60813303\n",
      "Iteration 22480, loss = 5.47917400\n",
      "Iteration 22481, loss = 5.16457829\n",
      "Iteration 22482, loss = 5.36895899\n",
      "Iteration 22483, loss = 5.16350889\n",
      "Iteration 22484, loss = 5.54284389\n",
      "Iteration 22485, loss = 6.04627117\n",
      "Iteration 22486, loss = 5.55030469\n",
      "Iteration 22487, loss = 5.56717269\n",
      "Iteration 22488, loss = 4.69647030\n",
      "Iteration 22489, loss = 4.54480071\n",
      "Iteration 22490, loss = 5.21878588\n",
      "Iteration 22491, loss = 5.27324240\n",
      "Iteration 22492, loss = 4.94180391\n",
      "Iteration 22493, loss = 4.54520548\n",
      "Iteration 22494, loss = 4.28540468\n",
      "Iteration 22495, loss = 4.53847879\n",
      "Iteration 22496, loss = 5.03950187\n",
      "Iteration 22497, loss = 5.04133537\n",
      "Iteration 22498, loss = 4.70213715\n",
      "Iteration 22499, loss = 4.95923173\n",
      "Iteration 22500, loss = 4.89779027\n",
      "Iteration 22501, loss = 5.04647258\n",
      "Iteration 22502, loss = 4.80594479\n",
      "Iteration 22503, loss = 4.94122718\n",
      "Iteration 22504, loss = 4.32024973\n",
      "Iteration 22505, loss = 4.66868963\n",
      "Iteration 22506, loss = 4.52353779\n",
      "Iteration 22507, loss = 4.97824100\n",
      "Iteration 22508, loss = 5.00550684\n",
      "Iteration 22509, loss = 4.57219926\n",
      "Iteration 22510, loss = 5.12898507\n",
      "Iteration 22511, loss = 4.85545757\n",
      "Iteration 22512, loss = 5.18880096\n",
      "Iteration 22513, loss = 5.16458180\n",
      "Iteration 22514, loss = 5.74272017\n",
      "Iteration 22515, loss = 5.76934963\n",
      "Iteration 22516, loss = 5.46099900\n",
      "Iteration 22517, loss = 5.17226970\n",
      "Iteration 22518, loss = 5.80349727\n",
      "Iteration 22519, loss = 4.52775112\n",
      "Iteration 22520, loss = 4.93188496\n",
      "Iteration 22521, loss = 4.59953023\n",
      "Iteration 22522, loss = 4.30997809\n",
      "Iteration 22523, loss = 4.38271982\n",
      "Iteration 22524, loss = 4.32763024\n",
      "Iteration 22525, loss = 4.38674357\n",
      "Iteration 22526, loss = 4.47560796\n",
      "Iteration 22527, loss = 4.99231470\n",
      "Iteration 22528, loss = 4.84971971\n",
      "Iteration 22529, loss = 5.13787849\n",
      "Iteration 22530, loss = 5.07720155\n",
      "Iteration 22531, loss = 5.38856829\n",
      "Iteration 22532, loss = 6.51931431\n",
      "Iteration 22533, loss = 5.83529832\n",
      "Iteration 22534, loss = 5.77055725\n",
      "Iteration 22535, loss = 5.73645215\n",
      "Iteration 22536, loss = 5.37698027\n",
      "Iteration 22537, loss = 4.99498216\n",
      "Iteration 22538, loss = 5.38050781\n",
      "Iteration 22539, loss = 5.69150171\n",
      "Iteration 22540, loss = 4.97360285\n",
      "Iteration 22541, loss = 4.76447579\n",
      "Iteration 22542, loss = 5.01171434\n",
      "Iteration 22543, loss = 5.01877522\n",
      "Iteration 22544, loss = 5.00803907\n",
      "Iteration 22545, loss = 4.87581638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22546, loss = 4.73612995\n",
      "Iteration 22547, loss = 4.39780490\n",
      "Iteration 22548, loss = 4.35186700\n",
      "Iteration 22549, loss = 4.34361818\n",
      "Iteration 22550, loss = 4.63052930\n",
      "Iteration 22551, loss = 4.45588431\n",
      "Iteration 22552, loss = 4.64924395\n",
      "Iteration 22553, loss = 4.97221643\n",
      "Iteration 22554, loss = 5.72062793\n",
      "Iteration 22555, loss = 5.34864597\n",
      "Iteration 22556, loss = 5.61762447\n",
      "Iteration 22557, loss = 4.53456689\n",
      "Iteration 22558, loss = 4.36125638\n",
      "Iteration 22559, loss = 4.31702487\n",
      "Iteration 22560, loss = 4.57564768\n",
      "Iteration 22561, loss = 4.84010062\n",
      "Iteration 22562, loss = 4.97866199\n",
      "Iteration 22563, loss = 5.13514783\n",
      "Iteration 22564, loss = 5.39581057\n",
      "Iteration 22565, loss = 4.74168906\n",
      "Iteration 22566, loss = 5.11221111\n",
      "Iteration 22567, loss = 5.15923447\n",
      "Iteration 22568, loss = 5.30116143\n",
      "Iteration 22569, loss = 4.66535762\n",
      "Iteration 22570, loss = 5.08005406\n",
      "Iteration 22571, loss = 4.80380743\n",
      "Iteration 22572, loss = 4.58551597\n",
      "Iteration 22573, loss = 4.72524168\n",
      "Iteration 22574, loss = 4.83500796\n",
      "Iteration 22575, loss = 4.40769899\n",
      "Iteration 22576, loss = 4.75561775\n",
      "Iteration 22577, loss = 4.35931983\n",
      "Iteration 22578, loss = 4.94805870\n",
      "Iteration 22579, loss = 4.73440426\n",
      "Iteration 22580, loss = 4.99977919\n",
      "Iteration 22581, loss = 5.13536120\n",
      "Iteration 22582, loss = 4.88721081\n",
      "Iteration 22583, loss = 4.52846110\n",
      "Iteration 22584, loss = 4.77819944\n",
      "Iteration 22585, loss = 4.43511645\n",
      "Iteration 22586, loss = 4.56659644\n",
      "Iteration 22587, loss = 4.65501489\n",
      "Iteration 22588, loss = 4.34745450\n",
      "Iteration 22589, loss = 4.86518312\n",
      "Iteration 22590, loss = 5.38655421\n",
      "Iteration 22591, loss = 5.29495131\n",
      "Iteration 22592, loss = 5.61574850\n",
      "Iteration 22593, loss = 6.40012737\n",
      "Iteration 22594, loss = 5.64773017\n",
      "Iteration 22595, loss = 5.53389228\n",
      "Iteration 22596, loss = 5.06557822\n",
      "Iteration 22597, loss = 5.52597642\n",
      "Iteration 22598, loss = 5.20494616\n",
      "Iteration 22599, loss = 5.44947413\n",
      "Iteration 22600, loss = 5.12363357\n",
      "Iteration 22601, loss = 4.70476344\n",
      "Iteration 22602, loss = 4.59704820\n",
      "Iteration 22603, loss = 4.63573983\n",
      "Iteration 22604, loss = 4.54813997\n",
      "Iteration 22605, loss = 4.79134851\n",
      "Iteration 22606, loss = 4.98986626\n",
      "Iteration 22607, loss = 4.86717795\n",
      "Iteration 22608, loss = 4.56751333\n",
      "Iteration 22609, loss = 4.80257869\n",
      "Iteration 22610, loss = 5.15677909\n",
      "Iteration 22611, loss = 4.54011393\n",
      "Iteration 22612, loss = 4.93359539\n",
      "Iteration 22613, loss = 4.74167455\n",
      "Iteration 22614, loss = 4.42332061\n",
      "Iteration 22615, loss = 4.66044406\n",
      "Iteration 22616, loss = 4.42202223\n",
      "Iteration 22617, loss = 5.30877225\n",
      "Iteration 22618, loss = 6.34239307\n",
      "Iteration 22619, loss = 6.13670693\n",
      "Iteration 22620, loss = 5.00709902\n",
      "Iteration 22621, loss = 4.47537441\n",
      "Iteration 22622, loss = 5.24192473\n",
      "Iteration 22623, loss = 4.70966894\n",
      "Iteration 22624, loss = 5.74022024\n",
      "Iteration 22625, loss = 4.81522827\n",
      "Iteration 22626, loss = 4.84139097\n",
      "Iteration 22627, loss = 4.98848017\n",
      "Iteration 22628, loss = 5.17770588\n",
      "Iteration 22629, loss = 5.47494135\n",
      "Iteration 22630, loss = 5.11050837\n",
      "Iteration 22631, loss = 5.31285665\n",
      "Iteration 22632, loss = 5.96232027\n",
      "Iteration 22633, loss = 4.95881933\n",
      "Iteration 22634, loss = 5.01388411\n",
      "Iteration 22635, loss = 4.80344743\n",
      "Iteration 22636, loss = 4.89563884\n",
      "Iteration 22637, loss = 4.77459857\n",
      "Iteration 22638, loss = 4.37544534\n",
      "Iteration 22639, loss = 4.69714579\n",
      "Iteration 22640, loss = 4.55936899\n",
      "Iteration 22641, loss = 4.54935604\n",
      "Iteration 22642, loss = 4.73329694\n",
      "Iteration 22643, loss = 4.64576717\n",
      "Iteration 22644, loss = 4.77671748\n",
      "Iteration 22645, loss = 4.83514066\n",
      "Iteration 22646, loss = 4.67578232\n",
      "Iteration 22647, loss = 4.64698965\n",
      "Iteration 22648, loss = 4.46556857\n",
      "Iteration 22649, loss = 4.52779086\n",
      "Iteration 22650, loss = 4.59891933\n",
      "Iteration 22651, loss = 4.54353237\n",
      "Iteration 22652, loss = 4.73814156\n",
      "Iteration 22653, loss = 5.18159643\n",
      "Iteration 22654, loss = 4.84072612\n",
      "Iteration 22655, loss = 5.16710928\n",
      "Iteration 22656, loss = 4.42638014\n",
      "Iteration 22657, loss = 5.90933376\n",
      "Iteration 22658, loss = 6.38788561\n",
      "Iteration 22659, loss = 7.33365484\n",
      "Iteration 22660, loss = 5.45080190\n",
      "Iteration 22661, loss = 5.81179530\n",
      "Iteration 22662, loss = 6.23148589\n",
      "Iteration 22663, loss = 5.58227181\n",
      "Iteration 22664, loss = 5.04452865\n",
      "Iteration 22665, loss = 4.82482623\n",
      "Iteration 22666, loss = 4.59396030\n",
      "Iteration 22667, loss = 4.48772681\n",
      "Iteration 22668, loss = 4.63516861\n",
      "Iteration 22669, loss = 4.48216644\n",
      "Iteration 22670, loss = 4.62138749\n",
      "Iteration 22671, loss = 5.10733750\n",
      "Iteration 22672, loss = 4.75846707\n",
      "Iteration 22673, loss = 4.71129878\n",
      "Iteration 22674, loss = 4.87859891\n",
      "Iteration 22675, loss = 4.92413869\n",
      "Iteration 22676, loss = 4.86006400\n",
      "Iteration 22677, loss = 4.40349008\n",
      "Iteration 22678, loss = 5.14100776\n",
      "Iteration 22679, loss = 4.85919670\n",
      "Iteration 22680, loss = 5.39411071\n",
      "Iteration 22681, loss = 4.22940980\n",
      "Iteration 22682, loss = 4.89384193\n",
      "Iteration 22683, loss = 5.37651639\n",
      "Iteration 22684, loss = 4.82941250\n",
      "Iteration 22685, loss = 4.61547692\n",
      "Iteration 22686, loss = 4.49119499\n",
      "Iteration 22687, loss = 5.51154338\n",
      "Iteration 22688, loss = 4.94086653\n",
      "Iteration 22689, loss = 4.47374981\n",
      "Iteration 22690, loss = 5.07799833\n",
      "Iteration 22691, loss = 5.44569453\n",
      "Iteration 22692, loss = 5.90719486\n",
      "Iteration 22693, loss = 5.00399111\n",
      "Iteration 22694, loss = 4.69288322\n",
      "Iteration 22695, loss = 4.40390988\n",
      "Iteration 22696, loss = 4.49419906\n",
      "Iteration 22697, loss = 4.46282527\n",
      "Iteration 22698, loss = 4.48632722\n",
      "Iteration 22699, loss = 4.74959517\n",
      "Iteration 22700, loss = 4.72160033\n",
      "Iteration 22701, loss = 4.84362351\n",
      "Iteration 22702, loss = 4.78953278\n",
      "Iteration 22703, loss = 4.97682855\n",
      "Iteration 22704, loss = 5.10760056\n",
      "Iteration 22705, loss = 6.33223352\n",
      "Iteration 22706, loss = 4.65681916\n",
      "Iteration 22707, loss = 5.04237910\n",
      "Iteration 22708, loss = 4.82427644\n",
      "Iteration 22709, loss = 5.65103904\n",
      "Iteration 22710, loss = 5.59513446\n",
      "Iteration 22711, loss = 5.84641874\n",
      "Iteration 22712, loss = 5.43270264\n",
      "Iteration 22713, loss = 4.80189000\n",
      "Iteration 22714, loss = 4.93218274\n",
      "Iteration 22715, loss = 4.68621806\n",
      "Iteration 22716, loss = 4.69834475\n",
      "Iteration 22717, loss = 4.45074935\n",
      "Iteration 22718, loss = 4.73987694\n",
      "Iteration 22719, loss = 4.35741689\n",
      "Iteration 22720, loss = 4.86321889\n",
      "Iteration 22721, loss = 6.82642095\n",
      "Iteration 22722, loss = 5.04379416\n",
      "Iteration 22723, loss = 4.98779513\n",
      "Iteration 22724, loss = 4.63937703\n",
      "Iteration 22725, loss = 4.52649070\n",
      "Iteration 22726, loss = 5.42747844\n",
      "Iteration 22727, loss = 4.67830551\n",
      "Iteration 22728, loss = 5.58885740\n",
      "Iteration 22729, loss = 5.23372895\n",
      "Iteration 22730, loss = 5.09706220\n",
      "Iteration 22731, loss = 5.00801504\n",
      "Iteration 22732, loss = 5.27779714\n",
      "Iteration 22733, loss = 5.49381542\n",
      "Iteration 22734, loss = 5.86707607\n",
      "Iteration 22735, loss = 5.46386017\n",
      "Iteration 22736, loss = 4.58648870\n",
      "Iteration 22737, loss = 4.35741292\n",
      "Iteration 22738, loss = 5.11711228\n",
      "Iteration 22739, loss = 5.70855046\n",
      "Iteration 22740, loss = 5.45071010\n",
      "Iteration 22741, loss = 6.13904392\n",
      "Iteration 22742, loss = 5.18080566\n",
      "Iteration 22743, loss = 5.21048291\n",
      "Iteration 22744, loss = 5.58667917\n",
      "Iteration 22745, loss = 5.52519597\n",
      "Iteration 22746, loss = 5.04142928\n",
      "Iteration 22747, loss = 4.41775775\n",
      "Iteration 22748, loss = 4.63862403\n",
      "Iteration 22749, loss = 4.74379228\n",
      "Iteration 22750, loss = 5.11556240\n",
      "Iteration 22751, loss = 4.47502957\n",
      "Iteration 22752, loss = 5.18953877\n",
      "Iteration 22753, loss = 4.81138010\n",
      "Iteration 22754, loss = 4.73881112\n",
      "Iteration 22755, loss = 4.63248316\n",
      "Iteration 22756, loss = 4.74684084\n",
      "Iteration 22757, loss = 5.29605132\n",
      "Iteration 22758, loss = 5.65196212\n",
      "Iteration 22759, loss = 6.48648083\n",
      "Iteration 22760, loss = 5.54786856\n",
      "Iteration 22761, loss = 5.14355184\n",
      "Iteration 22762, loss = 6.39040377\n",
      "Iteration 22763, loss = 5.56638604\n",
      "Iteration 22764, loss = 5.39059580\n",
      "Iteration 22765, loss = 5.47862128\n",
      "Iteration 22766, loss = 5.00883899\n",
      "Iteration 22767, loss = 5.11567660\n",
      "Iteration 22768, loss = 5.26679146\n",
      "Iteration 22769, loss = 4.99485389\n",
      "Iteration 22770, loss = 5.11988335\n",
      "Iteration 22771, loss = 4.22584980\n",
      "Iteration 22772, loss = 5.07066294\n",
      "Iteration 22773, loss = 5.89218030\n",
      "Iteration 22774, loss = 5.64937187\n",
      "Iteration 22775, loss = 4.83977202\n",
      "Iteration 22776, loss = 5.04664000\n",
      "Iteration 22777, loss = 5.16470537\n",
      "Iteration 22778, loss = 4.74542122\n",
      "Iteration 22779, loss = 5.43035607\n",
      "Iteration 22780, loss = 5.28591822\n",
      "Iteration 22781, loss = 5.36661186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22782, loss = 5.03014528\n",
      "Iteration 22783, loss = 5.16032848\n",
      "Iteration 22784, loss = 5.10396411\n",
      "Iteration 22785, loss = 5.05839783\n",
      "Iteration 22786, loss = 5.71738983\n",
      "Iteration 22787, loss = 5.74116553\n",
      "Iteration 22788, loss = 5.48121421\n",
      "Iteration 22789, loss = 5.42574369\n",
      "Iteration 22790, loss = 5.40887762\n",
      "Iteration 22791, loss = 4.55027727\n",
      "Iteration 22792, loss = 4.64176287\n",
      "Iteration 22793, loss = 4.67829346\n",
      "Iteration 22794, loss = 4.55691833\n",
      "Iteration 22795, loss = 4.33469240\n",
      "Iteration 22796, loss = 4.43193406\n",
      "Iteration 22797, loss = 4.62965370\n",
      "Iteration 22798, loss = 5.82982536\n",
      "Iteration 22799, loss = 5.53274404\n",
      "Iteration 22800, loss = 6.07506718\n",
      "Iteration 22801, loss = 5.51692467\n",
      "Iteration 22802, loss = 4.96850569\n",
      "Iteration 22803, loss = 5.11307171\n",
      "Iteration 22804, loss = 4.41360888\n",
      "Iteration 22805, loss = 4.62709868\n",
      "Iteration 22806, loss = 4.77470500\n",
      "Iteration 22807, loss = 4.45023280\n",
      "Iteration 22808, loss = 4.53157768\n",
      "Iteration 22809, loss = 4.77785186\n",
      "Iteration 22810, loss = 4.99962825\n",
      "Iteration 22811, loss = 4.35759367\n",
      "Iteration 22812, loss = 4.25657139\n",
      "Iteration 22813, loss = 4.91489641\n",
      "Iteration 22814, loss = 4.58766997\n",
      "Iteration 22815, loss = 5.15883335\n",
      "Iteration 22816, loss = 4.70006547\n",
      "Iteration 22817, loss = 4.45526246\n",
      "Iteration 22818, loss = 5.29027085\n",
      "Iteration 22819, loss = 4.57402177\n",
      "Iteration 22820, loss = 4.57131783\n",
      "Iteration 22821, loss = 4.72147584\n",
      "Iteration 22822, loss = 5.17337528\n",
      "Iteration 22823, loss = 5.43423082\n",
      "Iteration 22824, loss = 5.26280097\n",
      "Iteration 22825, loss = 5.49506976\n",
      "Iteration 22826, loss = 5.39845046\n",
      "Iteration 22827, loss = 5.16398900\n",
      "Iteration 22828, loss = 4.71789239\n",
      "Iteration 22829, loss = 5.07382518\n",
      "Iteration 22830, loss = 6.04276642\n",
      "Iteration 22831, loss = 5.79902613\n",
      "Iteration 22832, loss = 6.04894920\n",
      "Iteration 22833, loss = 6.04574763\n",
      "Iteration 22834, loss = 4.88092399\n",
      "Iteration 22835, loss = 5.05983880\n",
      "Iteration 22836, loss = 4.91330155\n",
      "Iteration 22837, loss = 4.52661361\n",
      "Iteration 22838, loss = 4.43541297\n",
      "Iteration 22839, loss = 4.23673520\n",
      "Iteration 22840, loss = 4.37425273\n",
      "Iteration 22841, loss = 4.67742022\n",
      "Iteration 22842, loss = 4.32413328\n",
      "Iteration 22843, loss = 4.36050354\n",
      "Iteration 22844, loss = 4.17034980\n",
      "Iteration 22845, loss = 4.12951269\n",
      "Iteration 22846, loss = 4.27250202\n",
      "Iteration 22847, loss = 4.66067815\n",
      "Iteration 22848, loss = 4.47312500\n",
      "Iteration 22849, loss = 4.96508242\n",
      "Iteration 22850, loss = 4.74853148\n",
      "Iteration 22851, loss = 4.61539755\n",
      "Iteration 22852, loss = 4.20038083\n",
      "Iteration 22853, loss = 5.02611854\n",
      "Iteration 22854, loss = 4.49265156\n",
      "Iteration 22855, loss = 4.58725437\n",
      "Iteration 22856, loss = 5.24974780\n",
      "Iteration 22857, loss = 5.07520517\n",
      "Iteration 22858, loss = 4.48204606\n",
      "Iteration 22859, loss = 4.74598674\n",
      "Iteration 22860, loss = 4.39593275\n",
      "Iteration 22861, loss = 5.05442248\n",
      "Iteration 22862, loss = 4.57606432\n",
      "Iteration 22863, loss = 4.58672227\n",
      "Iteration 22864, loss = 4.55230674\n",
      "Iteration 22865, loss = 4.85862615\n",
      "Iteration 22866, loss = 5.70969540\n",
      "Iteration 22867, loss = 5.66251000\n",
      "Iteration 22868, loss = 5.80621864\n",
      "Iteration 22869, loss = 6.08849841\n",
      "Iteration 22870, loss = 6.30298316\n",
      "Iteration 22871, loss = 4.66003980\n",
      "Iteration 22872, loss = 4.93000509\n",
      "Iteration 22873, loss = 4.84308720\n",
      "Iteration 22874, loss = 5.51012691\n",
      "Iteration 22875, loss = 5.06905882\n",
      "Iteration 22876, loss = 4.71136836\n",
      "Iteration 22877, loss = 4.44786127\n",
      "Iteration 22878, loss = 4.79995925\n",
      "Iteration 22879, loss = 4.79567546\n",
      "Iteration 22880, loss = 4.79965574\n",
      "Iteration 22881, loss = 4.71130906\n",
      "Iteration 22882, loss = 4.63832778\n",
      "Iteration 22883, loss = 5.02324272\n",
      "Iteration 22884, loss = 4.98561588\n",
      "Iteration 22885, loss = 4.86343849\n",
      "Iteration 22886, loss = 4.81218502\n",
      "Iteration 22887, loss = 4.73470466\n",
      "Iteration 22888, loss = 4.69103042\n",
      "Iteration 22889, loss = 5.31877065\n",
      "Iteration 22890, loss = 5.55661460\n",
      "Iteration 22891, loss = 5.57070633\n",
      "Iteration 22892, loss = 4.13462284\n",
      "Iteration 22893, loss = 5.00636223\n",
      "Iteration 22894, loss = 4.27900916\n",
      "Iteration 22895, loss = 4.74843098\n",
      "Iteration 22896, loss = 4.33066069\n",
      "Iteration 22897, loss = 4.40517685\n",
      "Iteration 22898, loss = 4.63010060\n",
      "Iteration 22899, loss = 4.79343941\n",
      "Iteration 22900, loss = 4.70333419\n",
      "Iteration 22901, loss = 4.69713827\n",
      "Iteration 22902, loss = 4.55550576\n",
      "Iteration 22903, loss = 4.34253581\n",
      "Iteration 22904, loss = 4.52124999\n",
      "Iteration 22905, loss = 4.49999175\n",
      "Iteration 22906, loss = 5.09811623\n",
      "Iteration 22907, loss = 6.80984291\n",
      "Iteration 22908, loss = 6.21076456\n",
      "Iteration 22909, loss = 5.23049801\n",
      "Iteration 22910, loss = 5.51398790\n",
      "Iteration 22911, loss = 5.41481479\n",
      "Iteration 22912, loss = 5.94861744\n",
      "Iteration 22913, loss = 5.63543831\n",
      "Iteration 22914, loss = 5.18514435\n",
      "Iteration 22915, loss = 4.82738793\n",
      "Iteration 22916, loss = 4.80264408\n",
      "Iteration 22917, loss = 4.78357392\n",
      "Iteration 22918, loss = 5.88151379\n",
      "Iteration 22919, loss = 4.83095292\n",
      "Iteration 22920, loss = 4.60481501\n",
      "Iteration 22921, loss = 4.83315656\n",
      "Iteration 22922, loss = 6.29465066\n",
      "Iteration 22923, loss = 5.97132870\n",
      "Iteration 22924, loss = 6.23815663\n",
      "Iteration 22925, loss = 5.50607920\n",
      "Iteration 22926, loss = 5.29409804\n",
      "Iteration 22927, loss = 5.47441517\n",
      "Iteration 22928, loss = 5.07728866\n",
      "Iteration 22929, loss = 5.56343783\n",
      "Iteration 22930, loss = 4.68708631\n",
      "Iteration 22931, loss = 5.64003562\n",
      "Iteration 22932, loss = 5.02714360\n",
      "Iteration 22933, loss = 4.90574450\n",
      "Iteration 22934, loss = 4.98807376\n",
      "Iteration 22935, loss = 4.84033862\n",
      "Iteration 22936, loss = 4.64529724\n",
      "Iteration 22937, loss = 4.99122114\n",
      "Iteration 22938, loss = 4.83207357\n",
      "Iteration 22939, loss = 5.03289423\n",
      "Iteration 22940, loss = 5.19474340\n",
      "Iteration 22941, loss = 5.73541415\n",
      "Iteration 22942, loss = 5.11815589\n",
      "Iteration 22943, loss = 4.98307962\n",
      "Iteration 22944, loss = 4.99265016\n",
      "Iteration 22945, loss = 5.46503323\n",
      "Iteration 22946, loss = 4.83012355\n",
      "Iteration 22947, loss = 5.21524703\n",
      "Iteration 22948, loss = 5.68957684\n",
      "Iteration 22949, loss = 6.00209832\n",
      "Iteration 22950, loss = 5.97821604\n",
      "Iteration 22951, loss = 8.21106134\n",
      "Iteration 22952, loss = 6.31494821\n",
      "Iteration 22953, loss = 6.15234162\n",
      "Iteration 22954, loss = 5.60754468\n",
      "Iteration 22955, loss = 4.92814813\n",
      "Iteration 22956, loss = 5.58542002\n",
      "Iteration 22957, loss = 4.95190960\n",
      "Iteration 22958, loss = 4.66500569\n",
      "Iteration 22959, loss = 4.36746115\n",
      "Iteration 22960, loss = 4.34980157\n",
      "Iteration 22961, loss = 4.21187679\n",
      "Iteration 22962, loss = 4.38854123\n",
      "Iteration 22963, loss = 4.35637945\n",
      "Iteration 22964, loss = 5.56996776\n",
      "Iteration 22965, loss = 5.04740431\n",
      "Iteration 22966, loss = 4.88532015\n",
      "Iteration 22967, loss = 4.59897542\n",
      "Iteration 22968, loss = 4.38382077\n",
      "Iteration 22969, loss = 4.33585804\n",
      "Iteration 22970, loss = 4.34435996\n",
      "Iteration 22971, loss = 4.22369528\n",
      "Iteration 22972, loss = 4.37082193\n",
      "Iteration 22973, loss = 4.68982819\n",
      "Iteration 22974, loss = 4.51406729\n",
      "Iteration 22975, loss = 4.59468906\n",
      "Iteration 22976, loss = 4.99368360\n",
      "Iteration 22977, loss = 4.73884885\n",
      "Iteration 22978, loss = 4.47675586\n",
      "Iteration 22979, loss = 4.85691690\n",
      "Iteration 22980, loss = 5.52199857\n",
      "Iteration 22981, loss = 6.34933918\n",
      "Iteration 22982, loss = 5.56386423\n",
      "Iteration 22983, loss = 5.55313483\n",
      "Iteration 22984, loss = 5.37171949\n",
      "Iteration 22985, loss = 5.24269051\n",
      "Iteration 22986, loss = 4.80944749\n",
      "Iteration 22987, loss = 4.39354757\n",
      "Iteration 22988, loss = 4.81047333\n",
      "Iteration 22989, loss = 4.45316277\n",
      "Iteration 22990, loss = 4.44516608\n",
      "Iteration 22991, loss = 5.72444011\n",
      "Iteration 22992, loss = 5.63843144\n",
      "Iteration 22993, loss = 4.85103911\n",
      "Iteration 22994, loss = 4.72263089\n",
      "Iteration 22995, loss = 5.14784322\n",
      "Iteration 22996, loss = 4.24392343\n",
      "Iteration 22997, loss = 4.29064361\n",
      "Iteration 22998, loss = 4.25981196\n",
      "Iteration 22999, loss = 4.48032719\n",
      "Iteration 23000, loss = 4.52902777\n",
      "Iteration 23001, loss = 5.13202365\n",
      "Iteration 23002, loss = 5.82904632\n",
      "Iteration 23003, loss = 4.99472871\n",
      "Iteration 23004, loss = 5.26000938\n",
      "Iteration 23005, loss = 5.01114433\n",
      "Iteration 23006, loss = 4.82203923\n",
      "Iteration 23007, loss = 4.64691369\n",
      "Iteration 23008, loss = 4.50684415\n",
      "Iteration 23009, loss = 5.00471581\n",
      "Iteration 23010, loss = 5.09043753\n",
      "Iteration 23011, loss = 4.72170372\n",
      "Iteration 23012, loss = 4.60226378\n",
      "Iteration 23013, loss = 5.08113489\n",
      "Iteration 23014, loss = 4.59320879\n",
      "Iteration 23015, loss = 4.54620130\n",
      "Iteration 23016, loss = 5.17456040\n",
      "Iteration 23017, loss = 5.02243675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23018, loss = 6.31872830\n",
      "Iteration 23019, loss = 5.60983678\n",
      "Iteration 23020, loss = 4.60131007\n",
      "Iteration 23021, loss = 4.25732427\n",
      "Iteration 23022, loss = 4.27792190\n",
      "Iteration 23023, loss = 4.29177776\n",
      "Iteration 23024, loss = 4.22320594\n",
      "Iteration 23025, loss = 5.35780022\n",
      "Iteration 23026, loss = 4.38008440\n",
      "Iteration 23027, loss = 5.06385310\n",
      "Iteration 23028, loss = 4.93971847\n",
      "Iteration 23029, loss = 5.63100653\n",
      "Iteration 23030, loss = 5.17228392\n",
      "Iteration 23031, loss = 4.83157188\n",
      "Iteration 23032, loss = 4.63366399\n",
      "Iteration 23033, loss = 4.25418099\n",
      "Iteration 23034, loss = 4.86223368\n",
      "Iteration 23035, loss = 5.72428258\n",
      "Iteration 23036, loss = 6.01070374\n",
      "Iteration 23037, loss = 4.79265932\n",
      "Iteration 23038, loss = 4.80387857\n",
      "Iteration 23039, loss = 5.20182747\n",
      "Iteration 23040, loss = 4.70158205\n",
      "Iteration 23041, loss = 5.06062933\n",
      "Iteration 23042, loss = 4.58259705\n",
      "Iteration 23043, loss = 4.76148826\n",
      "Iteration 23044, loss = 4.53300078\n",
      "Iteration 23045, loss = 4.27980630\n",
      "Iteration 23046, loss = 4.37749198\n",
      "Iteration 23047, loss = 4.59065515\n",
      "Iteration 23048, loss = 4.68470023\n",
      "Iteration 23049, loss = 4.63481380\n",
      "Iteration 23050, loss = 4.81494448\n",
      "Iteration 23051, loss = 4.74504109\n",
      "Iteration 23052, loss = 4.79606540\n",
      "Iteration 23053, loss = 5.33134709\n",
      "Iteration 23054, loss = 5.56673348\n",
      "Iteration 23055, loss = 6.06983912\n",
      "Iteration 23056, loss = 4.66314999\n",
      "Iteration 23057, loss = 6.07914237\n",
      "Iteration 23058, loss = 5.69560717\n",
      "Iteration 23059, loss = 6.24800788\n",
      "Iteration 23060, loss = 5.56849941\n",
      "Iteration 23061, loss = 4.36747612\n",
      "Iteration 23062, loss = 4.67857795\n",
      "Iteration 23063, loss = 5.27770484\n",
      "Iteration 23064, loss = 5.25349733\n",
      "Iteration 23065, loss = 5.04813199\n",
      "Iteration 23066, loss = 4.72322978\n",
      "Iteration 23067, loss = 4.88806280\n",
      "Iteration 23068, loss = 4.52616881\n",
      "Iteration 23069, loss = 4.89961045\n",
      "Iteration 23070, loss = 5.09343380\n",
      "Iteration 23071, loss = 5.47003567\n",
      "Iteration 23072, loss = 6.27888759\n",
      "Iteration 23073, loss = 4.38697668\n",
      "Iteration 23074, loss = 4.34801027\n",
      "Iteration 23075, loss = 4.33209715\n",
      "Iteration 23076, loss = 4.66460029\n",
      "Iteration 23077, loss = 4.34172945\n",
      "Iteration 23078, loss = 4.53705196\n",
      "Iteration 23079, loss = 4.52236636\n",
      "Iteration 23080, loss = 5.02433920\n",
      "Iteration 23081, loss = 5.27964218\n",
      "Iteration 23082, loss = 5.47806484\n",
      "Iteration 23083, loss = 4.92019164\n",
      "Iteration 23084, loss = 4.86067120\n",
      "Iteration 23085, loss = 4.60699497\n",
      "Iteration 23086, loss = 4.61008319\n",
      "Iteration 23087, loss = 5.57567053\n",
      "Iteration 23088, loss = 5.41759291\n",
      "Iteration 23089, loss = 5.29895919\n",
      "Iteration 23090, loss = 5.24385893\n",
      "Iteration 23091, loss = 4.50011442\n",
      "Iteration 23092, loss = 4.22670507\n",
      "Iteration 23093, loss = 4.59770043\n",
      "Iteration 23094, loss = 4.24771775\n",
      "Iteration 23095, loss = 4.64117311\n",
      "Iteration 23096, loss = 4.29268341\n",
      "Iteration 23097, loss = 4.46351555\n",
      "Iteration 23098, loss = 4.56147250\n",
      "Iteration 23099, loss = 5.54563451\n",
      "Iteration 23100, loss = 6.84080379\n",
      "Iteration 23101, loss = 5.68014225\n",
      "Iteration 23102, loss = 5.75937546\n",
      "Iteration 23103, loss = 6.45879211\n",
      "Iteration 23104, loss = 6.40195305\n",
      "Iteration 23105, loss = 7.25489213\n",
      "Iteration 23106, loss = 5.67834291\n",
      "Iteration 23107, loss = 4.90037979\n",
      "Iteration 23108, loss = 4.68984237\n",
      "Iteration 23109, loss = 4.77690243\n",
      "Iteration 23110, loss = 4.95870330\n",
      "Iteration 23111, loss = 4.88381967\n",
      "Iteration 23112, loss = 4.38837798\n",
      "Iteration 23113, loss = 4.87040204\n",
      "Iteration 23114, loss = 5.31684366\n",
      "Iteration 23115, loss = 4.67367322\n",
      "Iteration 23116, loss = 4.80319971\n",
      "Iteration 23117, loss = 5.00122388\n",
      "Iteration 23118, loss = 4.77117839\n",
      "Iteration 23119, loss = 5.55180138\n",
      "Iteration 23120, loss = 6.27147215\n",
      "Iteration 23121, loss = 7.18162571\n",
      "Iteration 23122, loss = 6.48628758\n",
      "Iteration 23123, loss = 6.39389640\n",
      "Iteration 23124, loss = 6.87610482\n",
      "Iteration 23125, loss = 6.45939600\n",
      "Iteration 23126, loss = 5.54725419\n",
      "Iteration 23127, loss = 7.24159026\n",
      "Iteration 23128, loss = 5.71807129\n",
      "Iteration 23129, loss = 4.89969987\n",
      "Iteration 23130, loss = 4.40210154\n",
      "Iteration 23131, loss = 4.55938387\n",
      "Iteration 23132, loss = 4.33217974\n",
      "Iteration 23133, loss = 4.65170665\n",
      "Iteration 23134, loss = 4.59144039\n",
      "Iteration 23135, loss = 4.62622645\n",
      "Iteration 23136, loss = 5.00396645\n",
      "Iteration 23137, loss = 4.53918712\n",
      "Iteration 23138, loss = 4.44121114\n",
      "Iteration 23139, loss = 4.23163956\n",
      "Iteration 23140, loss = 5.38442620\n",
      "Iteration 23141, loss = 6.48894026\n",
      "Iteration 23142, loss = 4.98938470\n",
      "Iteration 23143, loss = 5.86612039\n",
      "Iteration 23144, loss = 5.24980610\n",
      "Iteration 23145, loss = 4.65919209\n",
      "Iteration 23146, loss = 4.82652892\n",
      "Iteration 23147, loss = 4.25075825\n",
      "Iteration 23148, loss = 4.42235429\n",
      "Iteration 23149, loss = 4.34370557\n",
      "Iteration 23150, loss = 4.92068996\n",
      "Iteration 23151, loss = 4.17735772\n",
      "Iteration 23152, loss = 4.23819898\n",
      "Iteration 23153, loss = 4.51875277\n",
      "Iteration 23154, loss = 4.11157679\n",
      "Iteration 23155, loss = 4.67108729\n",
      "Iteration 23156, loss = 5.14571738\n",
      "Iteration 23157, loss = 5.22931394\n",
      "Iteration 23158, loss = 5.40010558\n",
      "Iteration 23159, loss = 4.84231349\n",
      "Iteration 23160, loss = 5.10206891\n",
      "Iteration 23161, loss = 5.46133082\n",
      "Iteration 23162, loss = 5.03828194\n",
      "Iteration 23163, loss = 4.15768102\n",
      "Iteration 23164, loss = 4.24330165\n",
      "Iteration 23165, loss = 4.42108133\n",
      "Iteration 23166, loss = 4.39561056\n",
      "Iteration 23167, loss = 4.43405299\n",
      "Iteration 23168, loss = 4.21371565\n",
      "Iteration 23169, loss = 4.12806335\n",
      "Iteration 23170, loss = 4.37493921\n",
      "Iteration 23171, loss = 4.52252145\n",
      "Iteration 23172, loss = 4.77181837\n",
      "Iteration 23173, loss = 5.03476572\n",
      "Iteration 23174, loss = 4.67336285\n",
      "Iteration 23175, loss = 4.64948326\n",
      "Iteration 23176, loss = 4.07003676\n",
      "Iteration 23177, loss = 4.35678049\n",
      "Iteration 23178, loss = 4.56774423\n",
      "Iteration 23179, loss = 4.33223882\n",
      "Iteration 23180, loss = 4.34129520\n",
      "Iteration 23181, loss = 4.59810377\n",
      "Iteration 23182, loss = 4.59799081\n",
      "Iteration 23183, loss = 4.27793939\n",
      "Iteration 23184, loss = 4.40966999\n",
      "Iteration 23185, loss = 4.84145401\n",
      "Iteration 23186, loss = 5.47926317\n",
      "Iteration 23187, loss = 4.95420895\n",
      "Iteration 23188, loss = 4.56078141\n",
      "Iteration 23189, loss = 4.60120625\n",
      "Iteration 23190, loss = 4.62684337\n",
      "Iteration 23191, loss = 4.91005927\n",
      "Iteration 23192, loss = 4.40983291\n",
      "Iteration 23193, loss = 4.36289246\n",
      "Iteration 23194, loss = 4.36720175\n",
      "Iteration 23195, loss = 4.89401126\n",
      "Iteration 23196, loss = 4.39839395\n",
      "Iteration 23197, loss = 4.28800987\n",
      "Iteration 23198, loss = 4.64291424\n",
      "Iteration 23199, loss = 4.49377750\n",
      "Iteration 23200, loss = 4.64559825\n",
      "Iteration 23201, loss = 4.29215168\n",
      "Iteration 23202, loss = 4.40944928\n",
      "Iteration 23203, loss = 4.17408344\n",
      "Iteration 23204, loss = 4.42718945\n",
      "Iteration 23205, loss = 4.42871983\n",
      "Iteration 23206, loss = 4.07501871\n",
      "Iteration 23207, loss = 4.52908427\n",
      "Iteration 23208, loss = 4.39091389\n",
      "Iteration 23209, loss = 4.38787795\n",
      "Iteration 23210, loss = 4.27782681\n",
      "Iteration 23211, loss = 4.77942954\n",
      "Iteration 23212, loss = 4.46172391\n",
      "Iteration 23213, loss = 4.51973485\n",
      "Iteration 23214, loss = 4.36075172\n",
      "Iteration 23215, loss = 4.26319882\n",
      "Iteration 23216, loss = 4.41120869\n",
      "Iteration 23217, loss = 4.42299066\n",
      "Iteration 23218, loss = 5.74429890\n",
      "Iteration 23219, loss = 5.67706880\n",
      "Iteration 23220, loss = 6.28087214\n",
      "Iteration 23221, loss = 5.99020637\n",
      "Iteration 23222, loss = 5.30591628\n",
      "Iteration 23223, loss = 4.71250478\n",
      "Iteration 23224, loss = 4.77271506\n",
      "Iteration 23225, loss = 4.52581877\n",
      "Iteration 23226, loss = 4.67056134\n",
      "Iteration 23227, loss = 4.78381660\n",
      "Iteration 23228, loss = 5.01920512\n",
      "Iteration 23229, loss = 4.96577337\n",
      "Iteration 23230, loss = 5.45732787\n",
      "Iteration 23231, loss = 4.38099551\n",
      "Iteration 23232, loss = 5.88015597\n",
      "Iteration 23233, loss = 5.44678242\n",
      "Iteration 23234, loss = 5.86811067\n",
      "Iteration 23235, loss = 4.78917137\n",
      "Iteration 23236, loss = 4.52771811\n",
      "Iteration 23237, loss = 4.44153100\n",
      "Iteration 23238, loss = 4.49996195\n",
      "Iteration 23239, loss = 5.85756604\n",
      "Iteration 23240, loss = 4.85454154\n",
      "Iteration 23241, loss = 4.87330544\n",
      "Iteration 23242, loss = 4.81332536\n",
      "Iteration 23243, loss = 4.80612284\n",
      "Iteration 23244, loss = 4.30303924\n",
      "Iteration 23245, loss = 4.30616779\n",
      "Iteration 23246, loss = 4.40684726\n",
      "Iteration 23247, loss = 5.94999732\n",
      "Iteration 23248, loss = 6.43726459\n",
      "Iteration 23249, loss = 5.69511087\n",
      "Iteration 23250, loss = 5.31291266\n",
      "Iteration 23251, loss = 4.59716437\n",
      "Iteration 23252, loss = 4.31770757\n",
      "Iteration 23253, loss = 4.60430066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23254, loss = 4.93885959\n",
      "Iteration 23255, loss = 4.96494969\n",
      "Iteration 23256, loss = 5.06566843\n",
      "Iteration 23257, loss = 6.11083931\n",
      "Iteration 23258, loss = 4.99960712\n",
      "Iteration 23259, loss = 5.48762265\n",
      "Iteration 23260, loss = 4.99497949\n",
      "Iteration 23261, loss = 4.41332537\n",
      "Iteration 23262, loss = 4.38568277\n",
      "Iteration 23263, loss = 4.23535681\n",
      "Iteration 23264, loss = 4.64683872\n",
      "Iteration 23265, loss = 5.43913791\n",
      "Iteration 23266, loss = 5.21910657\n",
      "Iteration 23267, loss = 4.57515340\n",
      "Iteration 23268, loss = 4.27098199\n",
      "Iteration 23269, loss = 4.24991622\n",
      "Iteration 23270, loss = 4.78539403\n",
      "Iteration 23271, loss = 4.23608154\n",
      "Iteration 23272, loss = 4.26927947\n",
      "Iteration 23273, loss = 4.36074294\n",
      "Iteration 23274, loss = 4.85559384\n",
      "Iteration 23275, loss = 4.96783119\n",
      "Iteration 23276, loss = 5.61038953\n",
      "Iteration 23277, loss = 4.66878723\n",
      "Iteration 23278, loss = 4.78830872\n",
      "Iteration 23279, loss = 4.52559749\n",
      "Iteration 23280, loss = 4.65199185\n",
      "Iteration 23281, loss = 4.21674034\n",
      "Iteration 23282, loss = 4.66006196\n",
      "Iteration 23283, loss = 4.34375510\n",
      "Iteration 23284, loss = 4.52407587\n",
      "Iteration 23285, loss = 4.82614549\n",
      "Iteration 23286, loss = 4.24108402\n",
      "Iteration 23287, loss = 4.45170721\n",
      "Iteration 23288, loss = 4.81286947\n",
      "Iteration 23289, loss = 4.28501797\n",
      "Iteration 23290, loss = 4.26335746\n",
      "Iteration 23291, loss = 4.51209518\n",
      "Iteration 23292, loss = 5.54921864\n",
      "Iteration 23293, loss = 5.75425987\n",
      "Iteration 23294, loss = 5.85318154\n",
      "Iteration 23295, loss = 4.93668859\n",
      "Iteration 23296, loss = 4.87424323\n",
      "Iteration 23297, loss = 4.95738931\n",
      "Iteration 23298, loss = 5.32163267\n",
      "Iteration 23299, loss = 4.93152676\n",
      "Iteration 23300, loss = 4.85852889\n",
      "Iteration 23301, loss = 4.64493167\n",
      "Iteration 23302, loss = 4.11662364\n",
      "Iteration 23303, loss = 4.22518960\n",
      "Iteration 23304, loss = 4.70859257\n",
      "Iteration 23305, loss = 4.51629111\n",
      "Iteration 23306, loss = 5.03754762\n",
      "Iteration 23307, loss = 5.65551579\n",
      "Iteration 23308, loss = 5.85473936\n",
      "Iteration 23309, loss = 5.69333552\n",
      "Iteration 23310, loss = 5.10240281\n",
      "Iteration 23311, loss = 4.91723283\n",
      "Iteration 23312, loss = 5.96774931\n",
      "Iteration 23313, loss = 5.91906561\n",
      "Iteration 23314, loss = 5.96124361\n",
      "Iteration 23315, loss = 6.02185520\n",
      "Iteration 23316, loss = 4.97716339\n",
      "Iteration 23317, loss = 4.82168459\n",
      "Iteration 23318, loss = 4.80680637\n",
      "Iteration 23319, loss = 5.00972911\n",
      "Iteration 23320, loss = 4.41746663\n",
      "Iteration 23321, loss = 4.91999141\n",
      "Iteration 23322, loss = 5.24532491\n",
      "Iteration 23323, loss = 4.54746980\n",
      "Iteration 23324, loss = 4.17739607\n",
      "Iteration 23325, loss = 5.06185533\n",
      "Iteration 23326, loss = 4.52410998\n",
      "Iteration 23327, loss = 4.86768157\n",
      "Iteration 23328, loss = 4.70322889\n",
      "Iteration 23329, loss = 5.47862294\n",
      "Iteration 23330, loss = 6.40413340\n",
      "Iteration 23331, loss = 7.38866107\n",
      "Iteration 23332, loss = 6.04406648\n",
      "Iteration 23333, loss = 5.80042907\n",
      "Iteration 23334, loss = 5.43805301\n",
      "Iteration 23335, loss = 5.57498206\n",
      "Iteration 23336, loss = 6.20235047\n",
      "Iteration 23337, loss = 5.23814692\n",
      "Iteration 23338, loss = 5.26743588\n",
      "Iteration 23339, loss = 4.59556181\n",
      "Iteration 23340, loss = 4.65987949\n",
      "Iteration 23341, loss = 4.68219727\n",
      "Iteration 23342, loss = 5.34186469\n",
      "Iteration 23343, loss = 5.00203180\n",
      "Iteration 23344, loss = 5.03467060\n",
      "Iteration 23345, loss = 4.76651456\n",
      "Iteration 23346, loss = 4.46323712\n",
      "Iteration 23347, loss = 4.36945268\n",
      "Iteration 23348, loss = 4.35642634\n",
      "Iteration 23349, loss = 5.19161566\n",
      "Iteration 23350, loss = 4.43258993\n",
      "Iteration 23351, loss = 4.25420051\n",
      "Iteration 23352, loss = 4.56511607\n",
      "Iteration 23353, loss = 5.20586093\n",
      "Iteration 23354, loss = 5.06766629\n",
      "Iteration 23355, loss = 4.62439981\n",
      "Iteration 23356, loss = 4.60102002\n",
      "Iteration 23357, loss = 5.24124920\n",
      "Iteration 23358, loss = 5.25837882\n",
      "Iteration 23359, loss = 4.50611963\n",
      "Iteration 23360, loss = 4.93091832\n",
      "Iteration 23361, loss = 4.46598346\n",
      "Iteration 23362, loss = 4.25667410\n",
      "Iteration 23363, loss = 4.62458166\n",
      "Iteration 23364, loss = 5.28410135\n",
      "Iteration 23365, loss = 4.40402797\n",
      "Iteration 23366, loss = 4.06889138\n",
      "Iteration 23367, loss = 4.22372589\n",
      "Iteration 23368, loss = 4.19986463\n",
      "Iteration 23369, loss = 4.45096330\n",
      "Iteration 23370, loss = 4.68169521\n",
      "Iteration 23371, loss = 4.81477750\n",
      "Iteration 23372, loss = 4.72661422\n",
      "Iteration 23373, loss = 4.58853158\n",
      "Iteration 23374, loss = 4.51021673\n",
      "Iteration 23375, loss = 4.11987228\n",
      "Iteration 23376, loss = 4.25338905\n",
      "Iteration 23377, loss = 4.12336629\n",
      "Iteration 23378, loss = 4.39156746\n",
      "Iteration 23379, loss = 4.85325600\n",
      "Iteration 23380, loss = 4.81337688\n",
      "Iteration 23381, loss = 4.80842483\n",
      "Iteration 23382, loss = 5.25174120\n",
      "Iteration 23383, loss = 5.20149900\n",
      "Iteration 23384, loss = 5.83434513\n",
      "Iteration 23385, loss = 5.47379095\n",
      "Iteration 23386, loss = 4.67430492\n",
      "Iteration 23387, loss = 5.07821883\n",
      "Iteration 23388, loss = 5.45612791\n",
      "Iteration 23389, loss = 4.65656987\n",
      "Iteration 23390, loss = 4.72854055\n",
      "Iteration 23391, loss = 4.75076535\n",
      "Iteration 23392, loss = 4.82820379\n",
      "Iteration 23393, loss = 4.54391724\n",
      "Iteration 23394, loss = 4.76995035\n",
      "Iteration 23395, loss = 5.14891500\n",
      "Iteration 23396, loss = 4.34134835\n",
      "Iteration 23397, loss = 4.49639081\n",
      "Iteration 23398, loss = 5.11707027\n",
      "Iteration 23399, loss = 4.50978771\n",
      "Iteration 23400, loss = 4.76887452\n",
      "Iteration 23401, loss = 4.67214545\n",
      "Iteration 23402, loss = 4.23833191\n",
      "Iteration 23403, loss = 4.36133693\n",
      "Iteration 23404, loss = 4.33811000\n",
      "Iteration 23405, loss = 4.45757143\n",
      "Iteration 23406, loss = 4.55158363\n",
      "Iteration 23407, loss = 4.72779569\n",
      "Iteration 23408, loss = 4.62454625\n",
      "Iteration 23409, loss = 4.72291359\n",
      "Iteration 23410, loss = 5.33119447\n",
      "Iteration 23411, loss = 5.48404772\n",
      "Iteration 23412, loss = 5.24258746\n",
      "Iteration 23413, loss = 5.67204619\n",
      "Iteration 23414, loss = 5.04293603\n",
      "Iteration 23415, loss = 4.51776523\n",
      "Iteration 23416, loss = 4.89870959\n",
      "Iteration 23417, loss = 8.21976490\n",
      "Iteration 23418, loss = 6.36343539\n",
      "Iteration 23419, loss = 5.49369851\n",
      "Iteration 23420, loss = 5.54041444\n",
      "Iteration 23421, loss = 4.45351739\n",
      "Iteration 23422, loss = 4.53478197\n",
      "Iteration 23423, loss = 4.39241673\n",
      "Iteration 23424, loss = 4.96863266\n",
      "Iteration 23425, loss = 5.12718360\n",
      "Iteration 23426, loss = 4.38696414\n",
      "Iteration 23427, loss = 4.51838625\n",
      "Iteration 23428, loss = 4.78181845\n",
      "Iteration 23429, loss = 5.09674078\n",
      "Iteration 23430, loss = 5.09838662\n",
      "Iteration 23431, loss = 5.24883722\n",
      "Iteration 23432, loss = 5.21174161\n",
      "Iteration 23433, loss = 4.31811186\n",
      "Iteration 23434, loss = 4.72036962\n",
      "Iteration 23435, loss = 5.00893449\n",
      "Iteration 23436, loss = 4.75233045\n",
      "Iteration 23437, loss = 4.97563915\n",
      "Iteration 23438, loss = 5.14365535\n",
      "Iteration 23439, loss = 5.40181880\n",
      "Iteration 23440, loss = 5.45776213\n",
      "Iteration 23441, loss = 5.78734583\n",
      "Iteration 23442, loss = 4.43031785\n",
      "Iteration 23443, loss = 4.39373655\n",
      "Iteration 23444, loss = 4.31360167\n",
      "Iteration 23445, loss = 4.38091569\n",
      "Iteration 23446, loss = 4.22175296\n",
      "Iteration 23447, loss = 4.21431468\n",
      "Iteration 23448, loss = 4.21044732\n",
      "Iteration 23449, loss = 4.29529902\n",
      "Iteration 23450, loss = 4.76093069\n",
      "Iteration 23451, loss = 5.17545482\n",
      "Iteration 23452, loss = 5.09445425\n",
      "Iteration 23453, loss = 4.89768415\n",
      "Iteration 23454, loss = 4.90642767\n",
      "Iteration 23455, loss = 4.77416092\n",
      "Iteration 23456, loss = 4.83375041\n",
      "Iteration 23457, loss = 5.11336993\n",
      "Iteration 23458, loss = 5.94413866\n",
      "Iteration 23459, loss = 6.43432536\n",
      "Iteration 23460, loss = 5.49836378\n",
      "Iteration 23461, loss = 5.12082102\n",
      "Iteration 23462, loss = 4.68420379\n",
      "Iteration 23463, loss = 5.25283145\n",
      "Iteration 23464, loss = 4.62141394\n",
      "Iteration 23465, loss = 4.57319103\n",
      "Iteration 23466, loss = 4.47740404\n",
      "Iteration 23467, loss = 4.39791852\n",
      "Iteration 23468, loss = 4.69008707\n",
      "Iteration 23469, loss = 4.41256139\n",
      "Iteration 23470, loss = 4.33858956\n",
      "Iteration 23471, loss = 4.39431571\n",
      "Iteration 23472, loss = 4.45320549\n",
      "Iteration 23473, loss = 4.16423592\n",
      "Iteration 23474, loss = 4.22979537\n",
      "Iteration 23475, loss = 4.60051522\n",
      "Iteration 23476, loss = 4.78209414\n",
      "Iteration 23477, loss = 4.69968202\n",
      "Iteration 23478, loss = 4.29852924\n",
      "Iteration 23479, loss = 5.37416324\n",
      "Iteration 23480, loss = 5.12626066\n",
      "Iteration 23481, loss = 4.04918945\n",
      "Iteration 23482, loss = 4.74863910\n",
      "Iteration 23483, loss = 5.31523135\n",
      "Iteration 23484, loss = 4.89047000\n",
      "Iteration 23485, loss = 5.09894903\n",
      "Iteration 23486, loss = 4.69910703\n",
      "Iteration 23487, loss = 4.85168051\n",
      "Iteration 23488, loss = 4.45691087\n",
      "Iteration 23489, loss = 4.40920860\n",
      "Iteration 23490, loss = 4.65123649\n",
      "Iteration 23491, loss = 5.01192682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23492, loss = 4.69554526\n",
      "Iteration 23493, loss = 4.87453614\n",
      "Iteration 23494, loss = 6.15617463\n",
      "Iteration 23495, loss = 6.09108869\n",
      "Iteration 23496, loss = 4.96393191\n",
      "Iteration 23497, loss = 5.24348679\n",
      "Iteration 23498, loss = 4.62796020\n",
      "Iteration 23499, loss = 4.92015195\n",
      "Iteration 23500, loss = 4.75360857\n",
      "Iteration 23501, loss = 4.58077429\n",
      "Iteration 23502, loss = 4.28994319\n",
      "Iteration 23503, loss = 4.73866756\n",
      "Iteration 23504, loss = 4.46744652\n",
      "Iteration 23505, loss = 5.05730656\n",
      "Iteration 23506, loss = 5.25556282\n",
      "Iteration 23507, loss = 5.59125264\n",
      "Iteration 23508, loss = 5.53344240\n",
      "Iteration 23509, loss = 4.50369661\n",
      "Iteration 23510, loss = 5.06275810\n",
      "Iteration 23511, loss = 4.55922733\n",
      "Iteration 23512, loss = 4.93666395\n",
      "Iteration 23513, loss = 4.89135415\n",
      "Iteration 23514, loss = 5.65753357\n",
      "Iteration 23515, loss = 5.35238510\n",
      "Iteration 23516, loss = 5.29666897\n",
      "Iteration 23517, loss = 5.22325591\n",
      "Iteration 23518, loss = 5.52380216\n",
      "Iteration 23519, loss = 5.41479829\n",
      "Iteration 23520, loss = 4.99228675\n",
      "Iteration 23521, loss = 4.98180057\n",
      "Iteration 23522, loss = 4.79270757\n",
      "Iteration 23523, loss = 5.41879697\n",
      "Iteration 23524, loss = 4.86058809\n",
      "Iteration 23525, loss = 4.64630898\n",
      "Iteration 23526, loss = 4.54520438\n",
      "Iteration 23527, loss = 4.26640681\n",
      "Iteration 23528, loss = 4.28384498\n",
      "Iteration 23529, loss = 4.48429694\n",
      "Iteration 23530, loss = 5.05026954\n",
      "Iteration 23531, loss = 4.83273949\n",
      "Iteration 23532, loss = 4.77647952\n",
      "Iteration 23533, loss = 4.71856930\n",
      "Iteration 23534, loss = 5.24410928\n",
      "Iteration 23535, loss = 5.50052086\n",
      "Iteration 23536, loss = 4.63371765\n",
      "Iteration 23537, loss = 4.47791382\n",
      "Iteration 23538, loss = 5.02370712\n",
      "Iteration 23539, loss = 5.07245889\n",
      "Iteration 23540, loss = 4.53306766\n",
      "Iteration 23541, loss = 4.27847701\n",
      "Iteration 23542, loss = 4.30057915\n",
      "Iteration 23543, loss = 4.27728500\n",
      "Iteration 23544, loss = 4.58683939\n",
      "Iteration 23545, loss = 4.71992640\n",
      "Iteration 23546, loss = 4.73617274\n",
      "Iteration 23547, loss = 4.62614159\n",
      "Iteration 23548, loss = 4.68148147\n",
      "Iteration 23549, loss = 4.27881356\n",
      "Iteration 23550, loss = 4.34536145\n",
      "Iteration 23551, loss = 4.31269017\n",
      "Iteration 23552, loss = 4.95684215\n",
      "Iteration 23553, loss = 5.28582190\n",
      "Iteration 23554, loss = 5.66201059\n",
      "Iteration 23555, loss = 4.92055488\n",
      "Iteration 23556, loss = 4.60310172\n",
      "Iteration 23557, loss = 4.43167551\n",
      "Iteration 23558, loss = 4.67745906\n",
      "Iteration 23559, loss = 4.50968870\n",
      "Iteration 23560, loss = 4.29777066\n",
      "Iteration 23561, loss = 5.52827773\n",
      "Iteration 23562, loss = 5.20921422\n",
      "Iteration 23563, loss = 5.15667827\n",
      "Iteration 23564, loss = 4.58024699\n",
      "Iteration 23565, loss = 4.54370409\n",
      "Iteration 23566, loss = 4.42360163\n",
      "Iteration 23567, loss = 4.36740158\n",
      "Iteration 23568, loss = 4.40822954\n",
      "Iteration 23569, loss = 4.26765671\n",
      "Iteration 23570, loss = 4.25956995\n",
      "Iteration 23571, loss = 4.26598528\n",
      "Iteration 23572, loss = 4.10711919\n",
      "Iteration 23573, loss = 4.14685894\n",
      "Iteration 23574, loss = 4.33579878\n",
      "Iteration 23575, loss = 4.43331701\n",
      "Iteration 23576, loss = 4.43045949\n",
      "Iteration 23577, loss = 4.59308935\n",
      "Iteration 23578, loss = 4.68866446\n",
      "Iteration 23579, loss = 4.35762613\n",
      "Iteration 23580, loss = 4.82554257\n",
      "Iteration 23581, loss = 5.29789391\n",
      "Iteration 23582, loss = 5.56065990\n",
      "Iteration 23583, loss = 4.64194674\n",
      "Iteration 23584, loss = 4.80436281\n",
      "Iteration 23585, loss = 4.45276818\n",
      "Iteration 23586, loss = 4.36361498\n",
      "Iteration 23587, loss = 4.16353347\n",
      "Iteration 23588, loss = 4.81022256\n",
      "Iteration 23589, loss = 4.80075960\n",
      "Iteration 23590, loss = 4.57511610\n",
      "Iteration 23591, loss = 4.44970516\n",
      "Iteration 23592, loss = 5.11195171\n",
      "Iteration 23593, loss = 4.61051884\n",
      "Iteration 23594, loss = 5.04668700\n",
      "Iteration 23595, loss = 5.44847040\n",
      "Iteration 23596, loss = 5.15102600\n",
      "Iteration 23597, loss = 4.85457892\n",
      "Iteration 23598, loss = 5.51389777\n",
      "Iteration 23599, loss = 5.48424698\n",
      "Iteration 23600, loss = 4.47671408\n",
      "Iteration 23601, loss = 5.01189903\n",
      "Iteration 23602, loss = 5.04487333\n",
      "Iteration 23603, loss = 4.35613913\n",
      "Iteration 23604, loss = 4.65304185\n",
      "Iteration 23605, loss = 4.77505428\n",
      "Iteration 23606, loss = 4.52803593\n",
      "Iteration 23607, loss = 4.59820222\n",
      "Iteration 23608, loss = 4.36256257\n",
      "Iteration 23609, loss = 4.17545910\n",
      "Iteration 23610, loss = 4.83581461\n",
      "Iteration 23611, loss = 5.53013488\n",
      "Iteration 23612, loss = 5.47152055\n",
      "Iteration 23613, loss = 4.89269250\n",
      "Iteration 23614, loss = 4.85968678\n",
      "Iteration 23615, loss = 5.26234241\n",
      "Iteration 23616, loss = 4.52570587\n",
      "Iteration 23617, loss = 5.55405391\n",
      "Iteration 23618, loss = 5.17869369\n",
      "Iteration 23619, loss = 5.02451111\n",
      "Iteration 23620, loss = 4.58886911\n",
      "Iteration 23621, loss = 4.81242843\n",
      "Iteration 23622, loss = 4.88653312\n",
      "Iteration 23623, loss = 4.64704217\n",
      "Iteration 23624, loss = 4.13952032\n",
      "Iteration 23625, loss = 4.95023977\n",
      "Iteration 23626, loss = 5.03970752\n",
      "Iteration 23627, loss = 5.22847093\n",
      "Iteration 23628, loss = 4.72758479\n",
      "Iteration 23629, loss = 5.27230722\n",
      "Iteration 23630, loss = 5.26637457\n",
      "Iteration 23631, loss = 4.60536275\n",
      "Iteration 23632, loss = 4.71606980\n",
      "Iteration 23633, loss = 4.39391245\n",
      "Iteration 23634, loss = 5.53235812\n",
      "Iteration 23635, loss = 4.66174095\n",
      "Iteration 23636, loss = 4.41720084\n",
      "Iteration 23637, loss = 4.14534950\n",
      "Iteration 23638, loss = 4.18444739\n",
      "Iteration 23639, loss = 4.59729640\n",
      "Iteration 23640, loss = 4.02233829\n",
      "Iteration 23641, loss = 5.19825497\n",
      "Iteration 23642, loss = 5.74564321\n",
      "Iteration 23643, loss = 5.20826133\n",
      "Iteration 23644, loss = 4.78009113\n",
      "Iteration 23645, loss = 5.02892854\n",
      "Iteration 23646, loss = 4.38833629\n",
      "Iteration 23647, loss = 4.09272565\n",
      "Iteration 23648, loss = 4.44144028\n",
      "Iteration 23649, loss = 4.33033973\n",
      "Iteration 23650, loss = 4.76853751\n",
      "Iteration 23651, loss = 4.83349498\n",
      "Iteration 23652, loss = 5.16020308\n",
      "Iteration 23653, loss = 4.80980757\n",
      "Iteration 23654, loss = 5.21652066\n",
      "Iteration 23655, loss = 4.34134848\n",
      "Iteration 23656, loss = 4.20139265\n",
      "Iteration 23657, loss = 4.09506347\n",
      "Iteration 23658, loss = 4.24272697\n",
      "Iteration 23659, loss = 4.17053017\n",
      "Iteration 23660, loss = 4.40258459\n",
      "Iteration 23661, loss = 4.72197206\n",
      "Iteration 23662, loss = 4.49165706\n",
      "Iteration 23663, loss = 4.58283558\n",
      "Iteration 23664, loss = 5.15891388\n",
      "Iteration 23665, loss = 4.50682564\n",
      "Iteration 23666, loss = 4.40345166\n",
      "Iteration 23667, loss = 4.42688159\n",
      "Iteration 23668, loss = 5.04723432\n",
      "Iteration 23669, loss = 4.63260171\n",
      "Iteration 23670, loss = 5.21499439\n",
      "Iteration 23671, loss = 4.68046520\n",
      "Iteration 23672, loss = 5.27511034\n",
      "Iteration 23673, loss = 4.80857103\n",
      "Iteration 23674, loss = 4.77255699\n",
      "Iteration 23675, loss = 4.80909977\n",
      "Iteration 23676, loss = 4.22100001\n",
      "Iteration 23677, loss = 4.25888254\n",
      "Iteration 23678, loss = 5.01165836\n",
      "Iteration 23679, loss = 5.08808928\n",
      "Iteration 23680, loss = 4.33920084\n",
      "Iteration 23681, loss = 4.55650789\n",
      "Iteration 23682, loss = 4.58043260\n",
      "Iteration 23683, loss = 5.44155911\n",
      "Iteration 23684, loss = 4.59185401\n",
      "Iteration 23685, loss = 4.58781989\n",
      "Iteration 23686, loss = 4.87608744\n",
      "Iteration 23687, loss = 4.44270714\n",
      "Iteration 23688, loss = 4.20118932\n",
      "Iteration 23689, loss = 4.59076848\n",
      "Iteration 23690, loss = 4.22095258\n",
      "Iteration 23691, loss = 4.27177176\n",
      "Iteration 23692, loss = 4.26837025\n",
      "Iteration 23693, loss = 4.55247031\n",
      "Iteration 23694, loss = 4.55185525\n",
      "Iteration 23695, loss = 5.14739491\n",
      "Iteration 23696, loss = 5.58241834\n",
      "Iteration 23697, loss = 5.55948925\n",
      "Iteration 23698, loss = 4.36817101\n",
      "Iteration 23699, loss = 4.19677626\n",
      "Iteration 23700, loss = 5.10031752\n",
      "Iteration 23701, loss = 5.05133115\n",
      "Iteration 23702, loss = 4.89611481\n",
      "Iteration 23703, loss = 5.15008391\n",
      "Iteration 23704, loss = 4.64724883\n",
      "Iteration 23705, loss = 4.42107105\n",
      "Iteration 23706, loss = 5.24808599\n",
      "Iteration 23707, loss = 4.62845873\n",
      "Iteration 23708, loss = 5.25210225\n",
      "Iteration 23709, loss = 5.59089781\n",
      "Iteration 23710, loss = 4.97365756\n",
      "Iteration 23711, loss = 5.64984059\n",
      "Iteration 23712, loss = 4.96974632\n",
      "Iteration 23713, loss = 5.25874656\n",
      "Iteration 23714, loss = 5.27731757\n",
      "Iteration 23715, loss = 5.14998210\n",
      "Iteration 23716, loss = 5.75971404\n",
      "Iteration 23717, loss = 4.62854986\n",
      "Iteration 23718, loss = 4.55740018\n",
      "Iteration 23719, loss = 4.29369663\n",
      "Iteration 23720, loss = 4.30362970\n",
      "Iteration 23721, loss = 4.45233358\n",
      "Iteration 23722, loss = 4.06377820\n",
      "Iteration 23723, loss = 4.35046234\n",
      "Iteration 23724, loss = 3.99211508\n",
      "Iteration 23725, loss = 4.25138098\n",
      "Iteration 23726, loss = 4.23574354\n",
      "Iteration 23727, loss = 4.39147261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23728, loss = 4.56940925\n",
      "Iteration 23729, loss = 4.62127083\n",
      "Iteration 23730, loss = 3.98578887\n",
      "Iteration 23731, loss = 4.56010039\n",
      "Iteration 23732, loss = 5.60252012\n",
      "Iteration 23733, loss = 5.15002761\n",
      "Iteration 23734, loss = 4.69361203\n",
      "Iteration 23735, loss = 4.60159871\n",
      "Iteration 23736, loss = 4.46812651\n",
      "Iteration 23737, loss = 4.65739600\n",
      "Iteration 23738, loss = 4.24575363\n",
      "Iteration 23739, loss = 4.39477465\n",
      "Iteration 23740, loss = 4.61562752\n",
      "Iteration 23741, loss = 4.50394231\n",
      "Iteration 23742, loss = 4.43599357\n",
      "Iteration 23743, loss = 4.26801054\n",
      "Iteration 23744, loss = 4.76853495\n",
      "Iteration 23745, loss = 4.73882243\n",
      "Iteration 23746, loss = 5.16927061\n",
      "Iteration 23747, loss = 4.57598837\n",
      "Iteration 23748, loss = 4.40627246\n",
      "Iteration 23749, loss = 3.91731720\n",
      "Iteration 23750, loss = 4.95222984\n",
      "Iteration 23751, loss = 5.56225201\n",
      "Iteration 23752, loss = 4.86002379\n",
      "Iteration 23753, loss = 5.36619784\n",
      "Iteration 23754, loss = 5.41499712\n",
      "Iteration 23755, loss = 4.11905976\n",
      "Iteration 23756, loss = 4.96806461\n",
      "Iteration 23757, loss = 4.05399608\n",
      "Iteration 23758, loss = 4.61815995\n",
      "Iteration 23759, loss = 5.20701282\n",
      "Iteration 23760, loss = 4.68004104\n",
      "Iteration 23761, loss = 4.93961299\n",
      "Iteration 23762, loss = 4.82384325\n",
      "Iteration 23763, loss = 5.06954675\n",
      "Iteration 23764, loss = 5.07661262\n",
      "Iteration 23765, loss = 5.09702587\n",
      "Iteration 23766, loss = 5.12342018\n",
      "Iteration 23767, loss = 5.99661317\n",
      "Iteration 23768, loss = 6.49324825\n",
      "Iteration 23769, loss = 5.69336825\n",
      "Iteration 23770, loss = 7.11435707\n",
      "Iteration 23771, loss = 5.79624337\n",
      "Iteration 23772, loss = 5.16665732\n",
      "Iteration 23773, loss = 4.46567130\n",
      "Iteration 23774, loss = 4.86868854\n",
      "Iteration 23775, loss = 5.01675500\n",
      "Iteration 23776, loss = 5.89762409\n",
      "Iteration 23777, loss = 5.45060181\n",
      "Iteration 23778, loss = 4.57001316\n",
      "Iteration 23779, loss = 4.26249856\n",
      "Iteration 23780, loss = 4.45857042\n",
      "Iteration 23781, loss = 4.37038287\n",
      "Iteration 23782, loss = 5.88381080\n",
      "Iteration 23783, loss = 5.86481701\n",
      "Iteration 23784, loss = 5.31084446\n",
      "Iteration 23785, loss = 4.64078846\n",
      "Iteration 23786, loss = 5.64942900\n",
      "Iteration 23787, loss = 4.75045330\n",
      "Iteration 23788, loss = 4.72208478\n",
      "Iteration 23789, loss = 4.74849822\n",
      "Iteration 23790, loss = 4.76063252\n",
      "Iteration 23791, loss = 4.48563238\n",
      "Iteration 23792, loss = 4.12329592\n",
      "Iteration 23793, loss = 4.42059003\n",
      "Iteration 23794, loss = 4.36277434\n",
      "Iteration 23795, loss = 4.56085570\n",
      "Iteration 23796, loss = 4.38650945\n",
      "Iteration 23797, loss = 4.39008494\n",
      "Iteration 23798, loss = 5.23710498\n",
      "Iteration 23799, loss = 4.39951000\n",
      "Iteration 23800, loss = 4.46851493\n",
      "Iteration 23801, loss = 4.41739853\n",
      "Iteration 23802, loss = 4.31230143\n",
      "Iteration 23803, loss = 3.91164159\n",
      "Iteration 23804, loss = 4.29100688\n",
      "Iteration 23805, loss = 4.17966531\n",
      "Iteration 23806, loss = 4.01761743\n",
      "Iteration 23807, loss = 4.08039641\n",
      "Iteration 23808, loss = 4.44906099\n",
      "Iteration 23809, loss = 4.29050239\n",
      "Iteration 23810, loss = 4.19011976\n",
      "Iteration 23811, loss = 4.12065095\n",
      "Iteration 23812, loss = 4.42814605\n",
      "Iteration 23813, loss = 4.76386403\n",
      "Iteration 23814, loss = 4.71353276\n",
      "Iteration 23815, loss = 4.78733696\n",
      "Iteration 23816, loss = 4.46033728\n",
      "Iteration 23817, loss = 5.49341943\n",
      "Iteration 23818, loss = 5.00618122\n",
      "Iteration 23819, loss = 4.92206045\n",
      "Iteration 23820, loss = 4.74741219\n",
      "Iteration 23821, loss = 5.57916692\n",
      "Iteration 23822, loss = 5.43592032\n",
      "Iteration 23823, loss = 4.49470521\n",
      "Iteration 23824, loss = 4.05500982\n",
      "Iteration 23825, loss = 4.58373947\n",
      "Iteration 23826, loss = 4.66727383\n",
      "Iteration 23827, loss = 4.29115871\n",
      "Iteration 23828, loss = 4.10211815\n",
      "Iteration 23829, loss = 4.62589624\n",
      "Iteration 23830, loss = 4.67783685\n",
      "Iteration 23831, loss = 5.06066458\n",
      "Iteration 23832, loss = 4.93797912\n",
      "Iteration 23833, loss = 4.88990271\n",
      "Iteration 23834, loss = 4.86505316\n",
      "Iteration 23835, loss = 4.47575220\n",
      "Iteration 23836, loss = 4.15638866\n",
      "Iteration 23837, loss = 4.57161328\n",
      "Iteration 23838, loss = 4.59381594\n",
      "Iteration 23839, loss = 4.26537156\n",
      "Iteration 23840, loss = 4.62545147\n",
      "Iteration 23841, loss = 4.65383875\n",
      "Iteration 23842, loss = 4.43733889\n",
      "Iteration 23843, loss = 4.49534533\n",
      "Iteration 23844, loss = 4.05804249\n",
      "Iteration 23845, loss = 5.22456870\n",
      "Iteration 23846, loss = 5.50823484\n",
      "Iteration 23847, loss = 4.67043450\n",
      "Iteration 23848, loss = 4.80713156\n",
      "Iteration 23849, loss = 5.98227893\n",
      "Iteration 23850, loss = 7.11893257\n",
      "Iteration 23851, loss = 5.68395205\n",
      "Iteration 23852, loss = 5.72270731\n",
      "Iteration 23853, loss = 5.25956043\n",
      "Iteration 23854, loss = 5.19886943\n",
      "Iteration 23855, loss = 4.28293058\n",
      "Iteration 23856, loss = 4.44572709\n",
      "Iteration 23857, loss = 4.59320317\n",
      "Iteration 23858, loss = 4.45761948\n",
      "Iteration 23859, loss = 4.79541488\n",
      "Iteration 23860, loss = 4.41587092\n",
      "Iteration 23861, loss = 4.41446399\n",
      "Iteration 23862, loss = 4.21978921\n",
      "Iteration 23863, loss = 4.76137932\n",
      "Iteration 23864, loss = 5.27652498\n",
      "Iteration 23865, loss = 5.32424208\n",
      "Iteration 23866, loss = 4.78025735\n",
      "Iteration 23867, loss = 4.85515164\n",
      "Iteration 23868, loss = 6.29463362\n",
      "Iteration 23869, loss = 5.41935241\n",
      "Iteration 23870, loss = 5.59329264\n",
      "Iteration 23871, loss = 5.57218254\n",
      "Iteration 23872, loss = 4.83882372\n",
      "Iteration 23873, loss = 5.03647433\n",
      "Iteration 23874, loss = 7.74793934\n",
      "Iteration 23875, loss = 6.34888029\n",
      "Iteration 23876, loss = 6.26039081\n",
      "Iteration 23877, loss = 5.43182577\n",
      "Iteration 23878, loss = 4.58582559\n",
      "Iteration 23879, loss = 5.06468936\n",
      "Iteration 23880, loss = 5.32548610\n",
      "Iteration 23881, loss = 4.90741051\n",
      "Iteration 23882, loss = 4.46631614\n",
      "Iteration 23883, loss = 4.14744232\n",
      "Iteration 23884, loss = 4.16799932\n",
      "Iteration 23885, loss = 4.34933069\n",
      "Iteration 23886, loss = 4.82444426\n",
      "Iteration 23887, loss = 4.51752906\n",
      "Iteration 23888, loss = 4.15837941\n",
      "Iteration 23889, loss = 4.67372473\n",
      "Iteration 23890, loss = 4.54748740\n",
      "Iteration 23891, loss = 4.57271405\n",
      "Iteration 23892, loss = 4.15908878\n",
      "Iteration 23893, loss = 4.06896470\n",
      "Iteration 23894, loss = 4.19060208\n",
      "Iteration 23895, loss = 4.45381665\n",
      "Iteration 23896, loss = 4.31143086\n",
      "Iteration 23897, loss = 4.32017805\n",
      "Iteration 23898, loss = 4.13945504\n",
      "Iteration 23899, loss = 4.43852600\n",
      "Iteration 23900, loss = 4.65806813\n",
      "Iteration 23901, loss = 4.50394318\n",
      "Iteration 23902, loss = 4.17081355\n",
      "Iteration 23903, loss = 4.11398773\n",
      "Iteration 23904, loss = 4.43208389\n",
      "Iteration 23905, loss = 4.63253922\n",
      "Iteration 23906, loss = 4.88232191\n",
      "Iteration 23907, loss = 5.15583548\n",
      "Iteration 23908, loss = 5.71924320\n",
      "Iteration 23909, loss = 4.92556933\n",
      "Iteration 23910, loss = 4.77878449\n",
      "Iteration 23911, loss = 4.49404681\n",
      "Iteration 23912, loss = 4.98049612\n",
      "Iteration 23913, loss = 4.56579358\n",
      "Iteration 23914, loss = 4.21477513\n",
      "Iteration 23915, loss = 4.26210547\n",
      "Iteration 23916, loss = 4.36361853\n",
      "Iteration 23917, loss = 4.11540834\n",
      "Iteration 23918, loss = 4.45274002\n",
      "Iteration 23919, loss = 4.48208096\n",
      "Iteration 23920, loss = 4.62496094\n",
      "Iteration 23921, loss = 4.60571716\n",
      "Iteration 23922, loss = 4.89879997\n",
      "Iteration 23923, loss = 5.09705514\n",
      "Iteration 23924, loss = 5.11880093\n",
      "Iteration 23925, loss = 4.70217355\n",
      "Iteration 23926, loss = 4.24619681\n",
      "Iteration 23927, loss = 4.41705581\n",
      "Iteration 23928, loss = 4.24345000\n",
      "Iteration 23929, loss = 4.26715206\n",
      "Iteration 23930, loss = 4.15452056\n",
      "Iteration 23931, loss = 4.44386841\n",
      "Iteration 23932, loss = 4.60399102\n",
      "Iteration 23933, loss = 5.08654616\n",
      "Iteration 23934, loss = 4.76354193\n",
      "Iteration 23935, loss = 4.34005009\n",
      "Iteration 23936, loss = 4.23270096\n",
      "Iteration 23937, loss = 4.02051643\n",
      "Iteration 23938, loss = 4.04456887\n",
      "Iteration 23939, loss = 4.43848872\n",
      "Iteration 23940, loss = 4.38846757\n",
      "Iteration 23941, loss = 4.79352862\n",
      "Iteration 23942, loss = 5.44274257\n",
      "Iteration 23943, loss = 4.45981353\n",
      "Iteration 23944, loss = 4.67006631\n",
      "Iteration 23945, loss = 4.24489347\n",
      "Iteration 23946, loss = 4.79113094\n",
      "Iteration 23947, loss = 4.63513615\n",
      "Iteration 23948, loss = 4.51220146\n",
      "Iteration 23949, loss = 3.86806512\n",
      "Iteration 23950, loss = 4.41568481\n",
      "Iteration 23951, loss = 4.87794042\n",
      "Iteration 23952, loss = 5.04772564\n",
      "Iteration 23953, loss = 6.45329017\n",
      "Iteration 23954, loss = 5.12538342\n",
      "Iteration 23955, loss = 4.93159937\n",
      "Iteration 23956, loss = 4.37518723\n",
      "Iteration 23957, loss = 5.30771204\n",
      "Iteration 23958, loss = 5.86520680\n",
      "Iteration 23959, loss = 5.50088084\n",
      "Iteration 23960, loss = 4.97764586\n",
      "Iteration 23961, loss = 4.69043451\n",
      "Iteration 23962, loss = 4.31710576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23963, loss = 4.30446131\n",
      "Iteration 23964, loss = 4.58450068\n",
      "Iteration 23965, loss = 4.47109778\n",
      "Iteration 23966, loss = 4.77538019\n",
      "Iteration 23967, loss = 5.92434030\n",
      "Iteration 23968, loss = 5.88183807\n",
      "Iteration 23969, loss = 6.05704479\n",
      "Iteration 23970, loss = 5.11803404\n",
      "Iteration 23971, loss = 5.43709659\n",
      "Iteration 23972, loss = 5.00726133\n",
      "Iteration 23973, loss = 5.00129381\n",
      "Iteration 23974, loss = 5.49775183\n",
      "Iteration 23975, loss = 4.76085914\n",
      "Iteration 23976, loss = 4.54999376\n",
      "Iteration 23977, loss = 4.24437879\n",
      "Iteration 23978, loss = 4.66209045\n",
      "Iteration 23979, loss = 4.60291140\n",
      "Iteration 23980, loss = 4.05418373\n",
      "Iteration 23981, loss = 4.36144711\n",
      "Iteration 23982, loss = 4.35451607\n",
      "Iteration 23983, loss = 4.53320843\n",
      "Iteration 23984, loss = 4.15361745\n",
      "Iteration 23985, loss = 4.60997778\n",
      "Iteration 23986, loss = 4.29695411\n",
      "Iteration 23987, loss = 4.54574180\n",
      "Iteration 23988, loss = 4.34800130\n",
      "Iteration 23989, loss = 4.22916167\n",
      "Iteration 23990, loss = 4.55970560\n",
      "Iteration 23991, loss = 4.06714538\n",
      "Iteration 23992, loss = 4.51122339\n",
      "Iteration 23993, loss = 4.65903413\n",
      "Iteration 23994, loss = 4.55823906\n",
      "Iteration 23995, loss = 5.33247997\n",
      "Iteration 23996, loss = 5.77251835\n",
      "Iteration 23997, loss = 5.80247693\n",
      "Iteration 23998, loss = 5.39526159\n",
      "Iteration 23999, loss = 5.04481527\n",
      "Iteration 24000, loss = 4.61198093\n",
      "Iteration 24001, loss = 4.42081080\n",
      "Iteration 24002, loss = 4.84288326\n",
      "Iteration 24003, loss = 4.33568521\n",
      "Iteration 24004, loss = 4.23127808\n",
      "Iteration 24005, loss = 4.47858771\n",
      "Iteration 24006, loss = 4.70836204\n",
      "Iteration 24007, loss = 4.25013481\n",
      "Iteration 24008, loss = 4.69069827\n",
      "Iteration 24009, loss = 4.32506817\n",
      "Iteration 24010, loss = 4.08479408\n",
      "Iteration 24011, loss = 4.24437607\n",
      "Iteration 24012, loss = 5.11314587\n",
      "Iteration 24013, loss = 4.80147501\n",
      "Iteration 24014, loss = 4.92662751\n",
      "Iteration 24015, loss = 5.71004795\n",
      "Iteration 24016, loss = 4.96667531\n",
      "Iteration 24017, loss = 4.07909080\n",
      "Iteration 24018, loss = 4.35352419\n",
      "Iteration 24019, loss = 4.02233479\n",
      "Iteration 24020, loss = 4.28154034\n",
      "Iteration 24021, loss = 3.79565602\n",
      "Iteration 24022, loss = 4.15984325\n",
      "Iteration 24023, loss = 4.87108899\n",
      "Iteration 24024, loss = 5.23330878\n",
      "Iteration 24025, loss = 5.42142080\n",
      "Iteration 24026, loss = 7.14580463\n",
      "Iteration 24027, loss = 8.42559938\n",
      "Iteration 24028, loss = 6.35509649\n",
      "Iteration 24029, loss = 6.07251480\n",
      "Iteration 24030, loss = 4.95405984\n",
      "Iteration 24031, loss = 4.45300471\n",
      "Iteration 24032, loss = 5.38787853\n",
      "Iteration 24033, loss = 5.36814837\n",
      "Iteration 24034, loss = 4.95251818\n",
      "Iteration 24035, loss = 5.55959391\n",
      "Iteration 24036, loss = 5.00724810\n",
      "Iteration 24037, loss = 4.91410483\n",
      "Iteration 24038, loss = 4.98021759\n",
      "Iteration 24039, loss = 4.64296546\n",
      "Iteration 24040, loss = 4.35363798\n",
      "Iteration 24041, loss = 4.07408323\n",
      "Iteration 24042, loss = 4.25542121\n",
      "Iteration 24043, loss = 4.38677905\n",
      "Iteration 24044, loss = 4.32929298\n",
      "Iteration 24045, loss = 4.35305332\n",
      "Iteration 24046, loss = 4.22928061\n",
      "Iteration 24047, loss = 4.23318074\n",
      "Iteration 24048, loss = 4.13404532\n",
      "Iteration 24049, loss = 4.44364890\n",
      "Iteration 24050, loss = 5.10505946\n",
      "Iteration 24051, loss = 4.66343581\n",
      "Iteration 24052, loss = 4.29922078\n",
      "Iteration 24053, loss = 4.96693352\n",
      "Iteration 24054, loss = 3.95773905\n",
      "Iteration 24055, loss = 4.80128529\n",
      "Iteration 24056, loss = 5.37337478\n",
      "Iteration 24057, loss = 5.11389988\n",
      "Iteration 24058, loss = 4.62106021\n",
      "Iteration 24059, loss = 4.59026812\n",
      "Iteration 24060, loss = 4.42767641\n",
      "Iteration 24061, loss = 4.10926535\n",
      "Iteration 24062, loss = 3.95066799\n",
      "Iteration 24063, loss = 4.17949983\n",
      "Iteration 24064, loss = 4.19081895\n",
      "Iteration 24065, loss = 4.32360380\n",
      "Iteration 24066, loss = 4.62312182\n",
      "Iteration 24067, loss = 5.08949647\n",
      "Iteration 24068, loss = 5.33055647\n",
      "Iteration 24069, loss = 3.90179457\n",
      "Iteration 24070, loss = 4.56390090\n",
      "Iteration 24071, loss = 4.42424664\n",
      "Iteration 24072, loss = 4.14851137\n",
      "Iteration 24073, loss = 4.49549590\n",
      "Iteration 24074, loss = 4.17980921\n",
      "Iteration 24075, loss = 4.18556808\n",
      "Iteration 24076, loss = 4.24044641\n",
      "Iteration 24077, loss = 4.23719329\n",
      "Iteration 24078, loss = 5.38369061\n",
      "Iteration 24079, loss = 5.59790923\n",
      "Iteration 24080, loss = 5.08477733\n",
      "Iteration 24081, loss = 5.66750240\n",
      "Iteration 24082, loss = 5.04311916\n",
      "Iteration 24083, loss = 4.84458350\n",
      "Iteration 24084, loss = 5.32149811\n",
      "Iteration 24085, loss = 6.27996514\n",
      "Iteration 24086, loss = 7.33346079\n",
      "Iteration 24087, loss = 4.72177514\n",
      "Iteration 24088, loss = 5.15220844\n",
      "Iteration 24089, loss = 4.51625231\n",
      "Iteration 24090, loss = 4.36356902\n",
      "Iteration 24091, loss = 5.13032193\n",
      "Iteration 24092, loss = 5.21114556\n",
      "Iteration 24093, loss = 4.44624272\n",
      "Iteration 24094, loss = 4.43284338\n",
      "Iteration 24095, loss = 4.58035881\n",
      "Iteration 24096, loss = 4.85994777\n",
      "Iteration 24097, loss = 5.33351628\n",
      "Iteration 24098, loss = 4.71004876\n",
      "Iteration 24099, loss = 4.71795873\n",
      "Iteration 24100, loss = 4.92909317\n",
      "Iteration 24101, loss = 4.39070882\n",
      "Iteration 24102, loss = 4.15045873\n",
      "Iteration 24103, loss = 4.59285346\n",
      "Iteration 24104, loss = 4.35743710\n",
      "Iteration 24105, loss = 4.35801066\n",
      "Iteration 24106, loss = 4.45644401\n",
      "Iteration 24107, loss = 4.69306742\n",
      "Iteration 24108, loss = 5.26850726\n",
      "Iteration 24109, loss = 5.15749626\n",
      "Iteration 24110, loss = 4.27127388\n",
      "Iteration 24111, loss = 4.21340715\n",
      "Iteration 24112, loss = 4.35128049\n",
      "Iteration 24113, loss = 5.15383104\n",
      "Iteration 24114, loss = 5.09394865\n",
      "Iteration 24115, loss = 5.65126145\n",
      "Iteration 24116, loss = 5.49160310\n",
      "Iteration 24117, loss = 6.34752710\n",
      "Iteration 24118, loss = 5.08132100\n",
      "Iteration 24119, loss = 4.54898940\n",
      "Iteration 24120, loss = 4.16359999\n",
      "Iteration 24121, loss = 4.35911418\n",
      "Iteration 24122, loss = 4.42713157\n",
      "Iteration 24123, loss = 4.08474576\n",
      "Iteration 24124, loss = 4.33755048\n",
      "Iteration 24125, loss = 4.20596731\n",
      "Iteration 24126, loss = 4.36116538\n",
      "Iteration 24127, loss = 4.21968868\n",
      "Iteration 24128, loss = 4.51137065\n",
      "Iteration 24129, loss = 4.36502070\n",
      "Iteration 24130, loss = 5.24736369\n",
      "Iteration 24131, loss = 4.80727029\n",
      "Iteration 24132, loss = 4.91814323\n",
      "Iteration 24133, loss = 4.69177161\n",
      "Iteration 24134, loss = 5.38832539\n",
      "Iteration 24135, loss = 5.39714552\n",
      "Iteration 24136, loss = 5.00524774\n",
      "Iteration 24137, loss = 5.65534212\n",
      "Iteration 24138, loss = 4.94385987\n",
      "Iteration 24139, loss = 4.74056636\n",
      "Iteration 24140, loss = 4.63333106\n",
      "Iteration 24141, loss = 4.68790925\n",
      "Iteration 24142, loss = 4.42848006\n",
      "Iteration 24143, loss = 4.25682945\n",
      "Iteration 24144, loss = 3.88947581\n",
      "Iteration 24145, loss = 4.35820208\n",
      "Iteration 24146, loss = 4.27194796\n",
      "Iteration 24147, loss = 4.16244007\n",
      "Iteration 24148, loss = 4.68552038\n",
      "Iteration 24149, loss = 4.79004102\n",
      "Iteration 24150, loss = 5.25823047\n",
      "Iteration 24151, loss = 5.76395062\n",
      "Iteration 24152, loss = 5.05238601\n",
      "Iteration 24153, loss = 5.20625257\n",
      "Iteration 24154, loss = 4.92298824\n",
      "Iteration 24155, loss = 5.29248325\n",
      "Iteration 24156, loss = 5.00144400\n",
      "Iteration 24157, loss = 4.28389788\n",
      "Iteration 24158, loss = 4.35534261\n",
      "Iteration 24159, loss = 4.30496744\n",
      "Iteration 24160, loss = 5.02151468\n",
      "Iteration 24161, loss = 5.05112274\n",
      "Iteration 24162, loss = 5.39339446\n",
      "Iteration 24163, loss = 4.90452346\n",
      "Iteration 24164, loss = 4.82238134\n",
      "Iteration 24165, loss = 5.01639048\n",
      "Iteration 24166, loss = 4.39258564\n",
      "Iteration 24167, loss = 5.22206974\n",
      "Iteration 24168, loss = 5.30788021\n",
      "Iteration 24169, loss = 5.03071176\n",
      "Iteration 24170, loss = 4.36419073\n",
      "Iteration 24171, loss = 4.54129649\n",
      "Iteration 24172, loss = 4.82320047\n",
      "Iteration 24173, loss = 4.53425802\n",
      "Iteration 24174, loss = 4.34792451\n",
      "Iteration 24175, loss = 4.29749680\n",
      "Iteration 24176, loss = 4.66062737\n",
      "Iteration 24177, loss = 4.68238574\n",
      "Iteration 24178, loss = 4.74411056\n",
      "Iteration 24179, loss = 4.57775059\n",
      "Iteration 24180, loss = 5.00530066\n",
      "Iteration 24181, loss = 5.19119325\n",
      "Iteration 24182, loss = 5.04960841\n",
      "Iteration 24183, loss = 6.44456439\n",
      "Iteration 24184, loss = 5.37809541\n",
      "Iteration 24185, loss = 5.04886551\n",
      "Iteration 24186, loss = 5.43455557\n",
      "Iteration 24187, loss = 4.84667980\n",
      "Iteration 24188, loss = 4.47528264\n",
      "Iteration 24189, loss = 4.47038624\n",
      "Iteration 24190, loss = 4.18457254\n",
      "Iteration 24191, loss = 5.63093891\n",
      "Iteration 24192, loss = 5.99197914\n",
      "Iteration 24193, loss = 5.38406917\n",
      "Iteration 24194, loss = 5.36156330\n",
      "Iteration 24195, loss = 5.27900423\n",
      "Iteration 24196, loss = 4.18544688\n",
      "Iteration 24197, loss = 4.13566192\n",
      "Iteration 24198, loss = 4.53546342\n",
      "Iteration 24199, loss = 4.40571623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24200, loss = 4.32899341\n",
      "Iteration 24201, loss = 4.08451907\n",
      "Iteration 24202, loss = 4.26611400\n",
      "Iteration 24203, loss = 4.07032911\n",
      "Iteration 24204, loss = 3.95624534\n",
      "Iteration 24205, loss = 4.01645126\n",
      "Iteration 24206, loss = 5.13969900\n",
      "Iteration 24207, loss = 5.39633317\n",
      "Iteration 24208, loss = 4.65356510\n",
      "Iteration 24209, loss = 4.34585408\n",
      "Iteration 24210, loss = 4.92179373\n",
      "Iteration 24211, loss = 5.23909560\n",
      "Iteration 24212, loss = 5.02589409\n",
      "Iteration 24213, loss = 4.45124074\n",
      "Iteration 24214, loss = 4.57313838\n",
      "Iteration 24215, loss = 5.01019133\n",
      "Iteration 24216, loss = 6.97104093\n",
      "Iteration 24217, loss = 7.05627420\n",
      "Iteration 24218, loss = 6.13334294\n",
      "Iteration 24219, loss = 4.80188746\n",
      "Iteration 24220, loss = 4.73120462\n",
      "Iteration 24221, loss = 5.41206903\n",
      "Iteration 24222, loss = 5.02590007\n",
      "Iteration 24223, loss = 4.42109471\n",
      "Iteration 24224, loss = 4.86019062\n",
      "Iteration 24225, loss = 5.93852055\n",
      "Iteration 24226, loss = 5.71920297\n",
      "Iteration 24227, loss = 6.41360316\n",
      "Iteration 24228, loss = 5.96771980\n",
      "Iteration 24229, loss = 5.72883027\n",
      "Iteration 24230, loss = 5.18623942\n",
      "Iteration 24231, loss = 5.38700309\n",
      "Iteration 24232, loss = 5.46557029\n",
      "Iteration 24233, loss = 5.00431301\n",
      "Iteration 24234, loss = 4.31729451\n",
      "Iteration 24235, loss = 4.32393093\n",
      "Iteration 24236, loss = 3.91756618\n",
      "Iteration 24237, loss = 4.18785816\n",
      "Iteration 24238, loss = 4.54945392\n",
      "Iteration 24239, loss = 4.52328846\n",
      "Iteration 24240, loss = 4.38140510\n",
      "Iteration 24241, loss = 4.68516938\n",
      "Iteration 24242, loss = 4.21100622\n",
      "Iteration 24243, loss = 4.61874514\n",
      "Iteration 24244, loss = 4.83757601\n",
      "Iteration 24245, loss = 4.17617161\n",
      "Iteration 24246, loss = 4.28672023\n",
      "Iteration 24247, loss = 4.10899052\n",
      "Iteration 24248, loss = 4.51467435\n",
      "Iteration 24249, loss = 5.41200713\n",
      "Iteration 24250, loss = 5.68933916\n",
      "Iteration 24251, loss = 5.05457096\n",
      "Iteration 24252, loss = 5.04937201\n",
      "Iteration 24253, loss = 4.99956864\n",
      "Iteration 24254, loss = 4.57775345\n",
      "Iteration 24255, loss = 4.15550602\n",
      "Iteration 24256, loss = 4.14846779\n",
      "Iteration 24257, loss = 4.10582646\n",
      "Iteration 24258, loss = 4.15445277\n",
      "Iteration 24259, loss = 4.39155009\n",
      "Iteration 24260, loss = 4.07348004\n",
      "Iteration 24261, loss = 4.29898506\n",
      "Iteration 24262, loss = 3.99675808\n",
      "Iteration 24263, loss = 4.34675987\n",
      "Iteration 24264, loss = 4.40267006\n",
      "Iteration 24265, loss = 4.28054143\n",
      "Iteration 24266, loss = 4.12331739\n",
      "Iteration 24267, loss = 4.16859596\n",
      "Iteration 24268, loss = 4.06725426\n",
      "Iteration 24269, loss = 4.14548874\n",
      "Iteration 24270, loss = 6.01596127\n",
      "Iteration 24271, loss = 7.00388646\n",
      "Iteration 24272, loss = 7.31343959\n",
      "Iteration 24273, loss = 7.74893944\n",
      "Iteration 24274, loss = 5.99858043\n",
      "Iteration 24275, loss = 6.15050237\n",
      "Iteration 24276, loss = 5.31411270\n",
      "Iteration 24277, loss = 4.78185068\n",
      "Iteration 24278, loss = 5.04661279\n",
      "Iteration 24279, loss = 4.30737168\n",
      "Iteration 24280, loss = 4.79703822\n",
      "Iteration 24281, loss = 4.94631895\n",
      "Iteration 24282, loss = 5.74150418\n",
      "Iteration 24283, loss = 5.19347679\n",
      "Iteration 24284, loss = 4.88284414\n",
      "Iteration 24285, loss = 4.43325795\n",
      "Iteration 24286, loss = 4.95481343\n",
      "Iteration 24287, loss = 3.99864715\n",
      "Iteration 24288, loss = 4.82876378\n",
      "Iteration 24289, loss = 5.48553617\n",
      "Iteration 24290, loss = 5.45676575\n",
      "Iteration 24291, loss = 5.07142853\n",
      "Iteration 24292, loss = 4.25086105\n",
      "Iteration 24293, loss = 4.13445099\n",
      "Iteration 24294, loss = 4.35537891\n",
      "Iteration 24295, loss = 4.32744041\n",
      "Iteration 24296, loss = 4.06333296\n",
      "Iteration 24297, loss = 4.29091579\n",
      "Iteration 24298, loss = 4.17610084\n",
      "Iteration 24299, loss = 3.88840645\n",
      "Iteration 24300, loss = 4.26206211\n",
      "Iteration 24301, loss = 4.21025995\n",
      "Iteration 24302, loss = 4.75310925\n",
      "Iteration 24303, loss = 4.82601882\n",
      "Iteration 24304, loss = 4.26066828\n",
      "Iteration 24305, loss = 4.23920255\n",
      "Iteration 24306, loss = 3.78286357\n",
      "Iteration 24307, loss = 4.27908855\n",
      "Iteration 24308, loss = 4.18830462\n",
      "Iteration 24309, loss = 5.15391987\n",
      "Iteration 24310, loss = 5.17055151\n",
      "Iteration 24311, loss = 4.94666030\n",
      "Iteration 24312, loss = 5.18462381\n",
      "Iteration 24313, loss = 4.78707641\n",
      "Iteration 24314, loss = 4.17932744\n",
      "Iteration 24315, loss = 4.06558291\n",
      "Iteration 24316, loss = 4.57895001\n",
      "Iteration 24317, loss = 4.83344248\n",
      "Iteration 24318, loss = 4.22178148\n",
      "Iteration 24319, loss = 4.17837732\n",
      "Iteration 24320, loss = 4.71342031\n",
      "Iteration 24321, loss = 4.33044640\n",
      "Iteration 24322, loss = 4.25521054\n",
      "Iteration 24323, loss = 4.37134895\n",
      "Iteration 24324, loss = 5.90298370\n",
      "Iteration 24325, loss = 5.85461630\n",
      "Iteration 24326, loss = 5.88618359\n",
      "Iteration 24327, loss = 4.52449385\n",
      "Iteration 24328, loss = 3.83531518\n",
      "Iteration 24329, loss = 4.80040488\n",
      "Iteration 24330, loss = 4.13811003\n",
      "Iteration 24331, loss = 4.82266292\n",
      "Iteration 24332, loss = 4.92213551\n",
      "Iteration 24333, loss = 4.89305729\n",
      "Iteration 24334, loss = 4.66267356\n",
      "Iteration 24335, loss = 4.34282781\n",
      "Iteration 24336, loss = 4.44412429\n",
      "Iteration 24337, loss = 4.28113957\n",
      "Iteration 24338, loss = 4.19728154\n",
      "Iteration 24339, loss = 4.15810558\n",
      "Iteration 24340, loss = 4.04578437\n",
      "Iteration 24341, loss = 3.96962087\n",
      "Iteration 24342, loss = 4.06557778\n",
      "Iteration 24343, loss = 4.37146240\n",
      "Iteration 24344, loss = 4.28850770\n",
      "Iteration 24345, loss = 4.04706667\n",
      "Iteration 24346, loss = 4.13334224\n",
      "Iteration 24347, loss = 3.88356464\n",
      "Iteration 24348, loss = 3.90714843\n",
      "Iteration 24349, loss = 4.86224660\n",
      "Iteration 24350, loss = 4.53216698\n",
      "Iteration 24351, loss = 4.29462858\n",
      "Iteration 24352, loss = 4.56788185\n",
      "Iteration 24353, loss = 4.98268705\n",
      "Iteration 24354, loss = 4.50949875\n",
      "Iteration 24355, loss = 4.64225561\n",
      "Iteration 24356, loss = 4.75248061\n",
      "Iteration 24357, loss = 4.14639000\n",
      "Iteration 24358, loss = 3.98281374\n",
      "Iteration 24359, loss = 4.09222156\n",
      "Iteration 24360, loss = 4.21907690\n",
      "Iteration 24361, loss = 4.23745033\n",
      "Iteration 24362, loss = 4.06178920\n",
      "Iteration 24363, loss = 4.15124069\n",
      "Iteration 24364, loss = 4.43764366\n",
      "Iteration 24365, loss = 3.91596313\n",
      "Iteration 24366, loss = 4.04888438\n",
      "Iteration 24367, loss = 4.31348735\n",
      "Iteration 24368, loss = 4.19506299\n",
      "Iteration 24369, loss = 4.24917516\n",
      "Iteration 24370, loss = 4.97927820\n",
      "Iteration 24371, loss = 5.18504661\n",
      "Iteration 24372, loss = 4.96925864\n",
      "Iteration 24373, loss = 4.77535051\n",
      "Iteration 24374, loss = 4.97011618\n",
      "Iteration 24375, loss = 4.77655681\n",
      "Iteration 24376, loss = 6.41817061\n",
      "Iteration 24377, loss = 5.70772690\n",
      "Iteration 24378, loss = 6.49372296\n",
      "Iteration 24379, loss = 6.34912381\n",
      "Iteration 24380, loss = 6.15669688\n",
      "Iteration 24381, loss = 4.72883589\n",
      "Iteration 24382, loss = 5.26683421\n",
      "Iteration 24383, loss = 4.25071927\n",
      "Iteration 24384, loss = 4.10175851\n",
      "Iteration 24385, loss = 3.99827918\n",
      "Iteration 24386, loss = 3.96480570\n",
      "Iteration 24387, loss = 3.93211058\n",
      "Iteration 24388, loss = 4.44775990\n",
      "Iteration 24389, loss = 4.38738549\n",
      "Iteration 24390, loss = 4.86301467\n",
      "Iteration 24391, loss = 5.13386885\n",
      "Iteration 24392, loss = 5.51174917\n",
      "Iteration 24393, loss = 4.93078459\n",
      "Iteration 24394, loss = 4.38907425\n",
      "Iteration 24395, loss = 5.93804925\n",
      "Iteration 24396, loss = 5.55210369\n",
      "Iteration 24397, loss = 4.61436573\n",
      "Iteration 24398, loss = 4.48863835\n",
      "Iteration 24399, loss = 4.85754658\n",
      "Iteration 24400, loss = 3.99308899\n",
      "Iteration 24401, loss = 3.94671966\n",
      "Iteration 24402, loss = 4.11866386\n",
      "Iteration 24403, loss = 4.65521420\n",
      "Iteration 24404, loss = 4.73483797\n",
      "Iteration 24405, loss = 4.31949239\n",
      "Iteration 24406, loss = 4.06946455\n",
      "Iteration 24407, loss = 4.50828873\n",
      "Iteration 24408, loss = 4.40109438\n",
      "Iteration 24409, loss = 5.62241593\n",
      "Iteration 24410, loss = 5.22740004\n",
      "Iteration 24411, loss = 4.87878366\n",
      "Iteration 24412, loss = 4.88451901\n",
      "Iteration 24413, loss = 5.05706710\n",
      "Iteration 24414, loss = 5.31513968\n",
      "Iteration 24415, loss = 7.16792637\n",
      "Iteration 24416, loss = 6.15784857\n",
      "Iteration 24417, loss = 5.05703492\n",
      "Iteration 24418, loss = 5.78660866\n",
      "Iteration 24419, loss = 4.77390247\n",
      "Iteration 24420, loss = 4.86604878\n",
      "Iteration 24421, loss = 4.74033921\n",
      "Iteration 24422, loss = 5.98623189\n",
      "Iteration 24423, loss = 5.19411469\n",
      "Iteration 24424, loss = 5.06244036\n",
      "Iteration 24425, loss = 4.59688322\n",
      "Iteration 24426, loss = 5.83442075\n",
      "Iteration 24427, loss = 6.71080048\n",
      "Iteration 24428, loss = 4.09709970\n",
      "Iteration 24429, loss = 6.16313110\n",
      "Iteration 24430, loss = 7.44893092\n",
      "Iteration 24431, loss = 6.88511423\n",
      "Iteration 24432, loss = 6.72378047\n",
      "Iteration 24433, loss = 6.50680794\n",
      "Iteration 24434, loss = 6.02988455\n",
      "Iteration 24435, loss = 6.60286098\n",
      "Iteration 24436, loss = 5.79748596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24437, loss = 5.52505875\n",
      "Iteration 24438, loss = 5.73038218\n",
      "Iteration 24439, loss = 5.63818848\n",
      "Iteration 24440, loss = 5.32066032\n",
      "Iteration 24441, loss = 5.20241484\n",
      "Iteration 24442, loss = 5.32279459\n",
      "Iteration 24443, loss = 5.25023971\n",
      "Iteration 24444, loss = 5.47328419\n",
      "Iteration 24445, loss = 5.64471562\n",
      "Iteration 24446, loss = 5.91989205\n",
      "Iteration 24447, loss = 5.22147936\n",
      "Iteration 24448, loss = 6.17108973\n",
      "Iteration 24449, loss = 6.54416031\n",
      "Iteration 24450, loss = 6.95907913\n",
      "Iteration 24451, loss = 5.96541004\n",
      "Iteration 24452, loss = 4.98638576\n",
      "Iteration 24453, loss = 4.23330526\n",
      "Iteration 24454, loss = 4.26388025\n",
      "Iteration 24455, loss = 4.11656449\n",
      "Iteration 24456, loss = 4.52064926\n",
      "Iteration 24457, loss = 4.15346080\n",
      "Iteration 24458, loss = 4.57867675\n",
      "Iteration 24459, loss = 4.49652893\n",
      "Iteration 24460, loss = 5.71213346\n",
      "Iteration 24461, loss = 5.20266346\n",
      "Iteration 24462, loss = 4.91062691\n",
      "Iteration 24463, loss = 4.71324332\n",
      "Iteration 24464, loss = 4.69295399\n",
      "Iteration 24465, loss = 4.52580979\n",
      "Iteration 24466, loss = 4.53387616\n",
      "Iteration 24467, loss = 4.72479110\n",
      "Iteration 24468, loss = 4.38680161\n",
      "Iteration 24469, loss = 4.10711507\n",
      "Iteration 24470, loss = 4.18959755\n",
      "Iteration 24471, loss = 4.09590397\n",
      "Iteration 24472, loss = 4.03229100\n",
      "Iteration 24473, loss = 4.20058134\n",
      "Iteration 24474, loss = 5.00297778\n",
      "Iteration 24475, loss = 4.70176830\n",
      "Iteration 24476, loss = 4.44984104\n",
      "Iteration 24477, loss = 4.61812327\n",
      "Iteration 24478, loss = 5.00980272\n",
      "Iteration 24479, loss = 4.81665614\n",
      "Iteration 24480, loss = 5.25397318\n",
      "Iteration 24481, loss = 4.84411111\n",
      "Iteration 24482, loss = 5.55188964\n",
      "Iteration 24483, loss = 5.00589884\n",
      "Iteration 24484, loss = 5.24008141\n",
      "Iteration 24485, loss = 4.83787269\n",
      "Iteration 24486, loss = 4.47850366\n",
      "Iteration 24487, loss = 5.34821394\n",
      "Iteration 24488, loss = 5.13003095\n",
      "Iteration 24489, loss = 4.21587362\n",
      "Iteration 24490, loss = 4.49384130\n",
      "Iteration 24491, loss = 4.92730715\n",
      "Iteration 24492, loss = 4.92110969\n",
      "Iteration 24493, loss = 5.24873583\n",
      "Iteration 24494, loss = 4.27740428\n",
      "Iteration 24495, loss = 4.14815535\n",
      "Iteration 24496, loss = 5.18545037\n",
      "Iteration 24497, loss = 5.18596643\n",
      "Iteration 24498, loss = 4.91521302\n",
      "Iteration 24499, loss = 4.52806449\n",
      "Iteration 24500, loss = 4.25569540\n",
      "Iteration 24501, loss = 4.19238922\n",
      "Iteration 24502, loss = 4.23613456\n",
      "Iteration 24503, loss = 4.35129418\n",
      "Iteration 24504, loss = 4.91040709\n",
      "Iteration 24505, loss = 3.92020737\n",
      "Iteration 24506, loss = 4.66450155\n",
      "Iteration 24507, loss = 4.34050579\n",
      "Iteration 24508, loss = 4.61823455\n",
      "Iteration 24509, loss = 4.52259680\n",
      "Iteration 24510, loss = 4.73921071\n",
      "Iteration 24511, loss = 4.16512382\n",
      "Iteration 24512, loss = 3.90521302\n",
      "Iteration 24513, loss = 3.85803861\n",
      "Iteration 24514, loss = 4.07845053\n",
      "Iteration 24515, loss = 4.32819520\n",
      "Iteration 24516, loss = 4.41500109\n",
      "Iteration 24517, loss = 4.19457196\n",
      "Iteration 24518, loss = 4.09749667\n",
      "Iteration 24519, loss = 3.85820687\n",
      "Iteration 24520, loss = 4.05993016\n",
      "Iteration 24521, loss = 4.63787873\n",
      "Iteration 24522, loss = 4.51883837\n",
      "Iteration 24523, loss = 4.26099256\n",
      "Iteration 24524, loss = 5.17498718\n",
      "Iteration 24525, loss = 4.27493477\n",
      "Iteration 24526, loss = 5.67213717\n",
      "Iteration 24527, loss = 6.12089000\n",
      "Iteration 24528, loss = 5.92204537\n",
      "Iteration 24529, loss = 6.32461430\n",
      "Iteration 24530, loss = 5.40991912\n",
      "Iteration 24531, loss = 4.76287858\n",
      "Iteration 24532, loss = 5.04577879\n",
      "Iteration 24533, loss = 5.80688983\n",
      "Iteration 24534, loss = 6.75878292\n",
      "Iteration 24535, loss = 7.49064967\n",
      "Iteration 24536, loss = 5.08892577\n",
      "Iteration 24537, loss = 4.42104837\n",
      "Iteration 24538, loss = 4.07243514\n",
      "Iteration 24539, loss = 4.32303601\n",
      "Iteration 24540, loss = 4.33569405\n",
      "Iteration 24541, loss = 4.25157981\n",
      "Iteration 24542, loss = 4.29631296\n",
      "Iteration 24543, loss = 4.34379090\n",
      "Iteration 24544, loss = 4.65563203\n",
      "Iteration 24545, loss = 4.52710866\n",
      "Iteration 24546, loss = 4.39069404\n",
      "Iteration 24547, loss = 4.16945931\n",
      "Iteration 24548, loss = 4.02848852\n",
      "Iteration 24549, loss = 4.24377570\n",
      "Iteration 24550, loss = 4.66589462\n",
      "Iteration 24551, loss = 4.86821411\n",
      "Iteration 24552, loss = 4.46633623\n",
      "Iteration 24553, loss = 3.97118987\n",
      "Iteration 24554, loss = 4.27585120\n",
      "Iteration 24555, loss = 4.10129938\n",
      "Iteration 24556, loss = 4.17618889\n",
      "Iteration 24557, loss = 4.35244268\n",
      "Iteration 24558, loss = 4.66148211\n",
      "Iteration 24559, loss = 4.39703395\n",
      "Iteration 24560, loss = 4.14549725\n",
      "Iteration 24561, loss = 4.16945623\n",
      "Iteration 24562, loss = 4.21007318\n",
      "Iteration 24563, loss = 4.09558226\n",
      "Iteration 24564, loss = 4.24394473\n",
      "Iteration 24565, loss = 4.61049738\n",
      "Iteration 24566, loss = 4.43457104\n",
      "Iteration 24567, loss = 4.32267492\n",
      "Iteration 24568, loss = 4.34043320\n",
      "Iteration 24569, loss = 4.03093519\n",
      "Iteration 24570, loss = 4.16481248\n",
      "Iteration 24571, loss = 4.04707670\n",
      "Iteration 24572, loss = 4.08126725\n",
      "Iteration 24573, loss = 4.14368713\n",
      "Iteration 24574, loss = 4.21734851\n",
      "Iteration 24575, loss = 4.95338957\n",
      "Iteration 24576, loss = 4.22314311\n",
      "Iteration 24577, loss = 3.96743256\n",
      "Iteration 24578, loss = 4.34958062\n",
      "Iteration 24579, loss = 4.45848519\n",
      "Iteration 24580, loss = 4.41006981\n",
      "Iteration 24581, loss = 4.02432929\n",
      "Iteration 24582, loss = 3.74310147\n",
      "Iteration 24583, loss = 4.58325807\n",
      "Iteration 24584, loss = 4.53079646\n",
      "Iteration 24585, loss = 4.41177520\n",
      "Iteration 24586, loss = 4.17507622\n",
      "Iteration 24587, loss = 4.68529263\n",
      "Iteration 24588, loss = 4.43717028\n",
      "Iteration 24589, loss = 5.06520854\n",
      "Iteration 24590, loss = 4.11432868\n",
      "Iteration 24591, loss = 4.89311665\n",
      "Iteration 24592, loss = 5.74745183\n",
      "Iteration 24593, loss = 7.23701004\n",
      "Iteration 24594, loss = 6.57835652\n",
      "Iteration 24595, loss = 7.57974953\n",
      "Iteration 24596, loss = 6.49195591\n",
      "Iteration 24597, loss = 5.02725457\n",
      "Iteration 24598, loss = 4.84701768\n",
      "Iteration 24599, loss = 4.45534303\n",
      "Iteration 24600, loss = 4.68646051\n",
      "Iteration 24601, loss = 4.35680253\n",
      "Iteration 24602, loss = 4.74295300\n",
      "Iteration 24603, loss = 4.87671316\n",
      "Iteration 24604, loss = 4.93789619\n",
      "Iteration 24605, loss = 5.28405709\n",
      "Iteration 24606, loss = 5.17087718\n",
      "Iteration 24607, loss = 4.75162983\n",
      "Iteration 24608, loss = 3.88792852\n",
      "Iteration 24609, loss = 3.94155743\n",
      "Iteration 24610, loss = 4.26540071\n",
      "Iteration 24611, loss = 4.14474524\n",
      "Iteration 24612, loss = 4.08309274\n",
      "Iteration 24613, loss = 4.72504109\n",
      "Iteration 24614, loss = 4.93683446\n",
      "Iteration 24615, loss = 5.52841701\n",
      "Iteration 24616, loss = 5.62233843\n",
      "Iteration 24617, loss = 6.36847838\n",
      "Iteration 24618, loss = 5.34588913\n",
      "Iteration 24619, loss = 5.02782093\n",
      "Iteration 24620, loss = 5.03682303\n",
      "Iteration 24621, loss = 6.50015869\n",
      "Iteration 24622, loss = 6.88773366\n",
      "Iteration 24623, loss = 6.02674504\n",
      "Iteration 24624, loss = 6.01067351\n",
      "Iteration 24625, loss = 4.42928603\n",
      "Iteration 24626, loss = 5.27913248\n",
      "Iteration 24627, loss = 4.54221098\n",
      "Iteration 24628, loss = 4.70558647\n",
      "Iteration 24629, loss = 4.28056506\n",
      "Iteration 24630, loss = 4.89111443\n",
      "Iteration 24631, loss = 5.35147451\n",
      "Iteration 24632, loss = 5.45824952\n",
      "Iteration 24633, loss = 4.91541750\n",
      "Iteration 24634, loss = 5.22215731\n",
      "Iteration 24635, loss = 4.19389007\n",
      "Iteration 24636, loss = 5.64127410\n",
      "Iteration 24637, loss = 4.37309737\n",
      "Iteration 24638, loss = 4.15454151\n",
      "Iteration 24639, loss = 4.60342350\n",
      "Iteration 24640, loss = 4.30700059\n",
      "Iteration 24641, loss = 4.23266146\n",
      "Iteration 24642, loss = 4.04796422\n",
      "Iteration 24643, loss = 4.40543022\n",
      "Iteration 24644, loss = 4.59159623\n",
      "Iteration 24645, loss = 4.91104507\n",
      "Iteration 24646, loss = 4.45802479\n",
      "Iteration 24647, loss = 4.76439762\n",
      "Iteration 24648, loss = 4.92496112\n",
      "Iteration 24649, loss = 4.20127964\n",
      "Iteration 24650, loss = 4.27356635\n",
      "Iteration 24651, loss = 4.21450688\n",
      "Iteration 24652, loss = 3.97994434\n",
      "Iteration 24653, loss = 3.84380777\n",
      "Iteration 24654, loss = 3.88184319\n",
      "Iteration 24655, loss = 4.77375108\n",
      "Iteration 24656, loss = 4.19891472\n",
      "Iteration 24657, loss = 4.24746803\n",
      "Iteration 24658, loss = 4.30560527\n",
      "Iteration 24659, loss = 4.74031995\n",
      "Iteration 24660, loss = 5.46521325\n",
      "Iteration 24661, loss = 5.10490265\n",
      "Iteration 24662, loss = 4.13935181\n",
      "Iteration 24663, loss = 4.13167242\n",
      "Iteration 24664, loss = 4.17314242\n",
      "Iteration 24665, loss = 4.79663459\n",
      "Iteration 24666, loss = 4.81018647\n",
      "Iteration 24667, loss = 5.04122474\n",
      "Iteration 24668, loss = 6.07908131\n",
      "Iteration 24669, loss = 6.54200792\n",
      "Iteration 24670, loss = 5.71186354\n",
      "Iteration 24671, loss = 4.92304335\n",
      "Iteration 24672, loss = 4.80083323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24673, loss = 4.53379594\n",
      "Iteration 24674, loss = 5.41326573\n",
      "Iteration 24675, loss = 5.24428124\n",
      "Iteration 24676, loss = 4.67689531\n",
      "Iteration 24677, loss = 4.61672324\n",
      "Iteration 24678, loss = 5.53625985\n",
      "Iteration 24679, loss = 4.09745109\n",
      "Iteration 24680, loss = 4.89766239\n",
      "Iteration 24681, loss = 4.14654032\n",
      "Iteration 24682, loss = 4.18978965\n",
      "Iteration 24683, loss = 4.45680571\n",
      "Iteration 24684, loss = 6.20064714\n",
      "Iteration 24685, loss = 4.55477342\n",
      "Iteration 24686, loss = 4.70769306\n",
      "Iteration 24687, loss = 4.58299935\n",
      "Iteration 24688, loss = 4.01920561\n",
      "Iteration 24689, loss = 4.78564852\n",
      "Iteration 24690, loss = 4.71313696\n",
      "Iteration 24691, loss = 5.20904256\n",
      "Iteration 24692, loss = 4.50387967\n",
      "Iteration 24693, loss = 4.35112045\n",
      "Iteration 24694, loss = 4.17713169\n",
      "Iteration 24695, loss = 4.34883991\n",
      "Iteration 24696, loss = 4.24318512\n",
      "Iteration 24697, loss = 3.97658851\n",
      "Iteration 24698, loss = 3.90698992\n",
      "Iteration 24699, loss = 4.28233338\n",
      "Iteration 24700, loss = 3.97588836\n",
      "Iteration 24701, loss = 4.21303583\n",
      "Iteration 24702, loss = 3.97659199\n",
      "Iteration 24703, loss = 4.92035519\n",
      "Iteration 24704, loss = 5.24496772\n",
      "Iteration 24705, loss = 6.38892298\n",
      "Iteration 24706, loss = 6.64844210\n",
      "Iteration 24707, loss = 5.22949494\n",
      "Iteration 24708, loss = 4.86914768\n",
      "Iteration 24709, loss = 4.61629633\n",
      "Iteration 24710, loss = 4.03333552\n",
      "Iteration 24711, loss = 4.41954084\n",
      "Iteration 24712, loss = 5.01503845\n",
      "Iteration 24713, loss = 4.44166623\n",
      "Iteration 24714, loss = 4.20103805\n",
      "Iteration 24715, loss = 4.04470432\n",
      "Iteration 24716, loss = 3.95999329\n",
      "Iteration 24717, loss = 4.11638440\n",
      "Iteration 24718, loss = 3.95208543\n",
      "Iteration 24719, loss = 3.89880006\n",
      "Iteration 24720, loss = 3.83204579\n",
      "Iteration 24721, loss = 3.78741300\n",
      "Iteration 24722, loss = 4.03096991\n",
      "Iteration 24723, loss = 4.01560379\n",
      "Iteration 24724, loss = 3.93442458\n",
      "Iteration 24725, loss = 4.13921382\n",
      "Iteration 24726, loss = 4.40309275\n",
      "Iteration 24727, loss = 4.08564995\n",
      "Iteration 24728, loss = 3.98354122\n",
      "Iteration 24729, loss = 4.18739375\n",
      "Iteration 24730, loss = 3.89896099\n",
      "Iteration 24731, loss = 3.71736220\n",
      "Iteration 24732, loss = 4.19272155\n",
      "Iteration 24733, loss = 4.33430156\n",
      "Iteration 24734, loss = 4.50295262\n",
      "Iteration 24735, loss = 5.85783334\n",
      "Iteration 24736, loss = 6.92321732\n",
      "Iteration 24737, loss = 5.54819016\n",
      "Iteration 24738, loss = 5.36373904\n",
      "Iteration 24739, loss = 5.63075378\n",
      "Iteration 24740, loss = 4.98139521\n",
      "Iteration 24741, loss = 4.72671365\n",
      "Iteration 24742, loss = 4.41161314\n",
      "Iteration 24743, loss = 4.66238595\n",
      "Iteration 24744, loss = 4.18097793\n",
      "Iteration 24745, loss = 4.92121572\n",
      "Iteration 24746, loss = 5.04803379\n",
      "Iteration 24747, loss = 4.80274626\n",
      "Iteration 24748, loss = 4.51728933\n",
      "Iteration 24749, loss = 4.80911135\n",
      "Iteration 24750, loss = 5.12586396\n",
      "Iteration 24751, loss = 4.78640553\n",
      "Iteration 24752, loss = 5.26742736\n",
      "Iteration 24753, loss = 4.79350471\n",
      "Iteration 24754, loss = 5.26908404\n",
      "Iteration 24755, loss = 5.03998351\n",
      "Iteration 24756, loss = 4.89031745\n",
      "Iteration 24757, loss = 5.11621684\n",
      "Iteration 24758, loss = 4.34615382\n",
      "Iteration 24759, loss = 4.23729697\n",
      "Iteration 24760, loss = 4.28386429\n",
      "Iteration 24761, loss = 4.24974036\n",
      "Iteration 24762, loss = 4.58486228\n",
      "Iteration 24763, loss = 4.17525640\n",
      "Iteration 24764, loss = 4.01569939\n",
      "Iteration 24765, loss = 4.62825668\n",
      "Iteration 24766, loss = 4.85215435\n",
      "Iteration 24767, loss = 4.49691968\n",
      "Iteration 24768, loss = 4.43820151\n",
      "Iteration 24769, loss = 4.85461105\n",
      "Iteration 24770, loss = 4.95378777\n",
      "Iteration 24771, loss = 5.61685990\n",
      "Iteration 24772, loss = 4.81723047\n",
      "Iteration 24773, loss = 4.66454427\n",
      "Iteration 24774, loss = 5.08384847\n",
      "Iteration 24775, loss = 4.61916891\n",
      "Iteration 24776, loss = 5.06996900\n",
      "Iteration 24777, loss = 4.77502298\n",
      "Iteration 24778, loss = 4.08156238\n",
      "Iteration 24779, loss = 3.80081476\n",
      "Iteration 24780, loss = 4.68157819\n",
      "Iteration 24781, loss = 5.04820023\n",
      "Iteration 24782, loss = 4.25160640\n",
      "Iteration 24783, loss = 5.93497066\n",
      "Iteration 24784, loss = 4.57508886\n",
      "Iteration 24785, loss = 4.71932090\n",
      "Iteration 24786, loss = 4.45424463\n",
      "Iteration 24787, loss = 4.33709531\n",
      "Iteration 24788, loss = 4.21314161\n",
      "Iteration 24789, loss = 4.33757655\n",
      "Iteration 24790, loss = 3.95597731\n",
      "Iteration 24791, loss = 4.63631032\n",
      "Iteration 24792, loss = 4.45892401\n",
      "Iteration 24793, loss = 4.90165423\n",
      "Iteration 24794, loss = 4.41985239\n",
      "Iteration 24795, loss = 4.23946644\n",
      "Iteration 24796, loss = 3.92777196\n",
      "Iteration 24797, loss = 4.28911610\n",
      "Iteration 24798, loss = 5.85837700\n",
      "Iteration 24799, loss = 5.29728588\n",
      "Iteration 24800, loss = 5.82673201\n",
      "Iteration 24801, loss = 6.49687974\n",
      "Iteration 24802, loss = 5.86546820\n",
      "Iteration 24803, loss = 6.70805946\n",
      "Iteration 24804, loss = 5.05566018\n",
      "Iteration 24805, loss = 4.29567161\n",
      "Iteration 24806, loss = 4.50114283\n",
      "Iteration 24807, loss = 5.40135765\n",
      "Iteration 24808, loss = 5.61194612\n",
      "Iteration 24809, loss = 4.85232407\n",
      "Iteration 24810, loss = 4.77074602\n",
      "Iteration 24811, loss = 4.97305719\n",
      "Iteration 24812, loss = 4.34861373\n",
      "Iteration 24813, loss = 4.62827690\n",
      "Iteration 24814, loss = 4.47174266\n",
      "Iteration 24815, loss = 4.63051616\n",
      "Iteration 24816, loss = 4.31497657\n",
      "Iteration 24817, loss = 4.34167566\n",
      "Iteration 24818, loss = 5.39222529\n",
      "Iteration 24819, loss = 4.96132585\n",
      "Iteration 24820, loss = 4.33500072\n",
      "Iteration 24821, loss = 4.66807472\n",
      "Iteration 24822, loss = 4.86561378\n",
      "Iteration 24823, loss = 4.55987091\n",
      "Iteration 24824, loss = 4.66008179\n",
      "Iteration 24825, loss = 4.89897543\n",
      "Iteration 24826, loss = 4.82734309\n",
      "Iteration 24827, loss = 4.20195355\n",
      "Iteration 24828, loss = 4.01797334\n",
      "Iteration 24829, loss = 3.97171739\n",
      "Iteration 24830, loss = 4.06290981\n",
      "Iteration 24831, loss = 4.25482519\n",
      "Iteration 24832, loss = 4.44912516\n",
      "Iteration 24833, loss = 6.75725815\n",
      "Iteration 24834, loss = 6.51100110\n",
      "Iteration 24835, loss = 5.99146453\n",
      "Iteration 24836, loss = 5.67116967\n",
      "Iteration 24837, loss = 4.25011825\n",
      "Iteration 24838, loss = 4.47436135\n",
      "Iteration 24839, loss = 5.25907917\n",
      "Iteration 24840, loss = 4.95115516\n",
      "Iteration 24841, loss = 4.55942989\n",
      "Iteration 24842, loss = 4.30725512\n",
      "Iteration 24843, loss = 4.78604282\n",
      "Iteration 24844, loss = 5.00289018\n",
      "Iteration 24845, loss = 4.32805883\n",
      "Iteration 24846, loss = 4.65315325\n",
      "Iteration 24847, loss = 5.12296254\n",
      "Iteration 24848, loss = 6.05920674\n",
      "Iteration 24849, loss = 5.36870428\n",
      "Iteration 24850, loss = 5.28826572\n",
      "Iteration 24851, loss = 4.46695656\n",
      "Iteration 24852, loss = 4.88123029\n",
      "Iteration 24853, loss = 4.03793845\n",
      "Iteration 24854, loss = 4.01111978\n",
      "Iteration 24855, loss = 3.79993010\n",
      "Iteration 24856, loss = 3.88872673\n",
      "Iteration 24857, loss = 3.97910017\n",
      "Iteration 24858, loss = 4.00760591\n",
      "Iteration 24859, loss = 3.98014077\n",
      "Iteration 24860, loss = 4.49671224\n",
      "Iteration 24861, loss = 4.25986642\n",
      "Iteration 24862, loss = 4.35222794\n",
      "Iteration 24863, loss = 4.07839665\n",
      "Iteration 24864, loss = 4.08045883\n",
      "Iteration 24865, loss = 5.43050835\n",
      "Iteration 24866, loss = 4.84275037\n",
      "Iteration 24867, loss = 4.19159212\n",
      "Iteration 24868, loss = 4.27051505\n",
      "Iteration 24869, loss = 5.19860085\n",
      "Iteration 24870, loss = 5.71854121\n",
      "Iteration 24871, loss = 5.29389034\n",
      "Iteration 24872, loss = 4.99860290\n",
      "Iteration 24873, loss = 4.91954499\n",
      "Iteration 24874, loss = 4.65971857\n",
      "Iteration 24875, loss = 4.28571696\n",
      "Iteration 24876, loss = 5.06847171\n",
      "Iteration 24877, loss = 5.88413260\n",
      "Iteration 24878, loss = 6.54167917\n",
      "Iteration 24879, loss = 5.37099637\n",
      "Iteration 24880, loss = 4.61215884\n",
      "Iteration 24881, loss = 5.36610274\n",
      "Iteration 24882, loss = 5.79118758\n",
      "Iteration 24883, loss = 4.74788481\n",
      "Iteration 24884, loss = 4.84165243\n",
      "Iteration 24885, loss = 4.33231264\n",
      "Iteration 24886, loss = 4.08279449\n",
      "Iteration 24887, loss = 4.79254645\n",
      "Iteration 24888, loss = 4.58697880\n",
      "Iteration 24889, loss = 4.04384070\n",
      "Iteration 24890, loss = 4.22398549\n",
      "Iteration 24891, loss = 4.41446878\n",
      "Iteration 24892, loss = 3.96471067\n",
      "Iteration 24893, loss = 4.18343322\n",
      "Iteration 24894, loss = 4.44127882\n",
      "Iteration 24895, loss = 4.08852327\n",
      "Iteration 24896, loss = 4.19774556\n",
      "Iteration 24897, loss = 3.99836889\n",
      "Iteration 24898, loss = 4.59378437\n",
      "Iteration 24899, loss = 3.86063931\n",
      "Iteration 24900, loss = 4.88832413\n",
      "Iteration 24901, loss = 4.86715481\n",
      "Iteration 24902, loss = 5.30572254\n",
      "Iteration 24903, loss = 5.89470955\n",
      "Iteration 24904, loss = 7.56899903\n",
      "Iteration 24905, loss = 6.21181285\n",
      "Iteration 24906, loss = 4.91249808\n",
      "Iteration 24907, loss = 4.73310288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24908, loss = 4.65763444\n",
      "Iteration 24909, loss = 5.16065932\n",
      "Iteration 24910, loss = 4.67641084\n",
      "Iteration 24911, loss = 5.43735622\n",
      "Iteration 24912, loss = 4.71221965\n",
      "Iteration 24913, loss = 5.13815306\n",
      "Iteration 24914, loss = 5.02759992\n",
      "Iteration 24915, loss = 6.07525890\n",
      "Iteration 24916, loss = 4.81687799\n",
      "Iteration 24917, loss = 4.91978815\n",
      "Iteration 24918, loss = 4.43404967\n",
      "Iteration 24919, loss = 4.78553976\n",
      "Iteration 24920, loss = 4.31576992\n",
      "Iteration 24921, loss = 4.63492990\n",
      "Iteration 24922, loss = 4.11050870\n",
      "Iteration 24923, loss = 3.97902286\n",
      "Iteration 24924, loss = 3.97422751\n",
      "Iteration 24925, loss = 4.06726623\n",
      "Iteration 24926, loss = 4.41551102\n",
      "Iteration 24927, loss = 6.11022602\n",
      "Iteration 24928, loss = 5.81850051\n",
      "Iteration 24929, loss = 5.40866969\n",
      "Iteration 24930, loss = 4.25734728\n",
      "Iteration 24931, loss = 4.10609079\n",
      "Iteration 24932, loss = 4.32428025\n",
      "Iteration 24933, loss = 4.44065270\n",
      "Iteration 24934, loss = 4.14449971\n",
      "Iteration 24935, loss = 4.30418777\n",
      "Iteration 24936, loss = 4.22343241\n",
      "Iteration 24937, loss = 4.12660557\n",
      "Iteration 24938, loss = 3.86927487\n",
      "Iteration 24939, loss = 3.86909525\n",
      "Iteration 24940, loss = 4.18210639\n",
      "Iteration 24941, loss = 4.61555590\n",
      "Iteration 24942, loss = 4.55328745\n",
      "Iteration 24943, loss = 4.86300960\n",
      "Iteration 24944, loss = 4.64585959\n",
      "Iteration 24945, loss = 4.88366050\n",
      "Iteration 24946, loss = 4.20181444\n",
      "Iteration 24947, loss = 4.67234991\n",
      "Iteration 24948, loss = 5.19807404\n",
      "Iteration 24949, loss = 5.25819448\n",
      "Iteration 24950, loss = 4.43333528\n",
      "Iteration 24951, loss = 3.99410573\n",
      "Iteration 24952, loss = 4.31819399\n",
      "Iteration 24953, loss = 4.84423672\n",
      "Iteration 24954, loss = 4.40324459\n",
      "Iteration 24955, loss = 3.77416734\n",
      "Iteration 24956, loss = 3.74350945\n",
      "Iteration 24957, loss = 4.69244900\n",
      "Iteration 24958, loss = 4.22516813\n",
      "Iteration 24959, loss = 4.78134378\n",
      "Iteration 24960, loss = 4.34713074\n",
      "Iteration 24961, loss = 3.78086387\n",
      "Iteration 24962, loss = 3.81918738\n",
      "Iteration 24963, loss = 3.80489101\n",
      "Iteration 24964, loss = 4.05363894\n",
      "Iteration 24965, loss = 4.00213796\n",
      "Iteration 24966, loss = 4.17382187\n",
      "Iteration 24967, loss = 4.17705670\n",
      "Iteration 24968, loss = 4.40111803\n",
      "Iteration 24969, loss = 5.21310669\n",
      "Iteration 24970, loss = 6.08718913\n",
      "Iteration 24971, loss = 5.13483643\n",
      "Iteration 24972, loss = 4.74765922\n",
      "Iteration 24973, loss = 4.22944828\n",
      "Iteration 24974, loss = 4.81297815\n",
      "Iteration 24975, loss = 4.40819806\n",
      "Iteration 24976, loss = 4.30640233\n",
      "Iteration 24977, loss = 4.05124124\n",
      "Iteration 24978, loss = 3.82220713\n",
      "Iteration 24979, loss = 4.95544340\n",
      "Iteration 24980, loss = 5.75735305\n",
      "Iteration 24981, loss = 5.58846744\n",
      "Iteration 24982, loss = 4.41829716\n",
      "Iteration 24983, loss = 4.43711821\n",
      "Iteration 24984, loss = 5.15657391\n",
      "Iteration 24985, loss = 5.95244224\n",
      "Iteration 24986, loss = 6.35325357\n",
      "Iteration 24987, loss = 5.16975149\n",
      "Iteration 24988, loss = 4.45797821\n",
      "Iteration 24989, loss = 4.86698371\n",
      "Iteration 24990, loss = 4.41699869\n",
      "Iteration 24991, loss = 4.54028873\n",
      "Iteration 24992, loss = 4.80079147\n",
      "Iteration 24993, loss = 4.92701214\n",
      "Iteration 24994, loss = 5.86606399\n",
      "Iteration 24995, loss = 4.65520667\n",
      "Iteration 24996, loss = 4.45771069\n",
      "Iteration 24997, loss = 4.62779187\n",
      "Iteration 24998, loss = 4.30031758\n",
      "Iteration 24999, loss = 4.13730685\n",
      "Iteration 25000, loss = 4.39430287\n",
      "Iteration 25001, loss = 4.37798827\n",
      "Iteration 25002, loss = 4.04332477\n",
      "Iteration 25003, loss = 4.01550056\n",
      "Iteration 25004, loss = 4.17489974\n",
      "Iteration 25005, loss = 3.93553818\n",
      "Iteration 25006, loss = 4.06567688\n",
      "Iteration 25007, loss = 4.06769280\n",
      "Iteration 25008, loss = 3.83104018\n",
      "Iteration 25009, loss = 3.90385223\n",
      "Iteration 25010, loss = 4.27392463\n",
      "Iteration 25011, loss = 5.26561974\n",
      "Iteration 25012, loss = 5.14997736\n",
      "Iteration 25013, loss = 5.37044520\n",
      "Iteration 25014, loss = 5.57204248\n",
      "Iteration 25015, loss = 5.96122962\n",
      "Iteration 25016, loss = 4.71873381\n",
      "Iteration 25017, loss = 3.92841744\n",
      "Iteration 25018, loss = 4.48822184\n",
      "Iteration 25019, loss = 4.91842183\n",
      "Iteration 25020, loss = 5.09889945\n",
      "Iteration 25021, loss = 5.48141594\n",
      "Iteration 25022, loss = 6.17157928\n",
      "Iteration 25023, loss = 6.18309488\n",
      "Iteration 25024, loss = 5.27276366\n",
      "Iteration 25025, loss = 5.37691704\n",
      "Iteration 25026, loss = 4.80203158\n",
      "Iteration 25027, loss = 4.91328364\n",
      "Iteration 25028, loss = 4.08056877\n",
      "Iteration 25029, loss = 4.19391913\n",
      "Iteration 25030, loss = 4.55946199\n",
      "Iteration 25031, loss = 4.21985517\n",
      "Iteration 25032, loss = 4.07175828\n",
      "Iteration 25033, loss = 3.85169782\n",
      "Iteration 25034, loss = 4.16422681\n",
      "Iteration 25035, loss = 4.39153323\n",
      "Iteration 25036, loss = 4.61761337\n",
      "Iteration 25037, loss = 4.92268671\n",
      "Iteration 25038, loss = 4.75097555\n",
      "Iteration 25039, loss = 4.56368740\n",
      "Iteration 25040, loss = 4.32819882\n",
      "Iteration 25041, loss = 4.64147981\n",
      "Iteration 25042, loss = 4.03975675\n",
      "Iteration 25043, loss = 3.96781252\n",
      "Iteration 25044, loss = 4.60215908\n",
      "Iteration 25045, loss = 4.18252453\n",
      "Iteration 25046, loss = 4.59520888\n",
      "Iteration 25047, loss = 4.60767741\n",
      "Iteration 25048, loss = 4.80035923\n",
      "Iteration 25049, loss = 4.17177115\n",
      "Iteration 25050, loss = 4.35315844\n",
      "Iteration 25051, loss = 4.24007897\n",
      "Iteration 25052, loss = 4.20642769\n",
      "Iteration 25053, loss = 4.18332735\n",
      "Iteration 25054, loss = 3.80459817\n",
      "Iteration 25055, loss = 4.60420807\n",
      "Iteration 25056, loss = 4.28298026\n",
      "Iteration 25057, loss = 4.96371917\n",
      "Iteration 25058, loss = 4.55291339\n",
      "Iteration 25059, loss = 4.65490405\n",
      "Iteration 25060, loss = 4.83991906\n",
      "Iteration 25061, loss = 5.12432045\n",
      "Iteration 25062, loss = 4.67551794\n",
      "Iteration 25063, loss = 4.50026754\n",
      "Iteration 25064, loss = 4.87375008\n",
      "Iteration 25065, loss = 5.70416128\n",
      "Iteration 25066, loss = 5.17977460\n",
      "Iteration 25067, loss = 4.93668673\n",
      "Iteration 25068, loss = 5.48452788\n",
      "Iteration 25069, loss = 5.49606752\n",
      "Iteration 25070, loss = 5.66504341\n",
      "Iteration 25071, loss = 5.25572644\n",
      "Iteration 25072, loss = 4.29091432\n",
      "Iteration 25073, loss = 4.13764132\n",
      "Iteration 25074, loss = 4.54139913\n",
      "Iteration 25075, loss = 3.93946401\n",
      "Iteration 25076, loss = 3.98178639\n",
      "Iteration 25077, loss = 4.35519075\n",
      "Iteration 25078, loss = 4.41310602\n",
      "Iteration 25079, loss = 5.17885822\n",
      "Iteration 25080, loss = 5.20280323\n",
      "Iteration 25081, loss = 4.39137173\n",
      "Iteration 25082, loss = 4.05064181\n",
      "Iteration 25083, loss = 4.96584570\n",
      "Iteration 25084, loss = 4.26425200\n",
      "Iteration 25085, loss = 5.58841617\n",
      "Iteration 25086, loss = 6.94038950\n",
      "Iteration 25087, loss = 6.29670570\n",
      "Iteration 25088, loss = 6.69985574\n",
      "Iteration 25089, loss = 5.49253546\n",
      "Iteration 25090, loss = 4.59448297\n",
      "Iteration 25091, loss = 4.81217062\n",
      "Iteration 25092, loss = 4.78714450\n",
      "Iteration 25093, loss = 4.47157363\n",
      "Iteration 25094, loss = 4.11369857\n",
      "Iteration 25095, loss = 3.89298287\n",
      "Iteration 25096, loss = 4.05160085\n",
      "Iteration 25097, loss = 5.00239113\n",
      "Iteration 25098, loss = 4.33991628\n",
      "Iteration 25099, loss = 3.93569085\n",
      "Iteration 25100, loss = 3.89698500\n",
      "Iteration 25101, loss = 4.17456069\n",
      "Iteration 25102, loss = 3.87488499\n",
      "Iteration 25103, loss = 3.91999728\n",
      "Iteration 25104, loss = 4.18548974\n",
      "Iteration 25105, loss = 4.14530959\n",
      "Iteration 25106, loss = 4.14802175\n",
      "Iteration 25107, loss = 3.92302779\n",
      "Iteration 25108, loss = 4.37346297\n",
      "Iteration 25109, loss = 5.11392434\n",
      "Iteration 25110, loss = 4.59082657\n",
      "Iteration 25111, loss = 5.73044669\n",
      "Iteration 25112, loss = 5.91462012\n",
      "Iteration 25113, loss = 6.14247054\n",
      "Iteration 25114, loss = 4.87689096\n",
      "Iteration 25115, loss = 4.41758704\n",
      "Iteration 25116, loss = 4.47914715\n",
      "Iteration 25117, loss = 4.20425543\n",
      "Iteration 25118, loss = 3.73073900\n",
      "Iteration 25119, loss = 3.97636161\n",
      "Iteration 25120, loss = 3.91609285\n",
      "Iteration 25121, loss = 3.91786523\n",
      "Iteration 25122, loss = 4.06442310\n",
      "Iteration 25123, loss = 4.17682779\n",
      "Iteration 25124, loss = 3.97307445\n",
      "Iteration 25125, loss = 4.35479999\n",
      "Iteration 25126, loss = 4.13116519\n",
      "Iteration 25127, loss = 3.96163668\n",
      "Iteration 25128, loss = 4.35697066\n",
      "Iteration 25129, loss = 4.18373080\n",
      "Iteration 25130, loss = 5.39931174\n",
      "Iteration 25131, loss = 5.09736521\n",
      "Iteration 25132, loss = 5.00727181\n",
      "Iteration 25133, loss = 4.42639255\n",
      "Iteration 25134, loss = 4.38570648\n",
      "Iteration 25135, loss = 4.02115359\n",
      "Iteration 25136, loss = 4.02980754\n",
      "Iteration 25137, loss = 3.88739256\n",
      "Iteration 25138, loss = 4.12441669\n",
      "Iteration 25139, loss = 4.19990398\n",
      "Iteration 25140, loss = 4.34070753\n",
      "Iteration 25141, loss = 3.97916087\n",
      "Iteration 25142, loss = 4.40640932\n",
      "Iteration 25143, loss = 4.57576469\n",
      "Iteration 25144, loss = 4.66368537\n",
      "Iteration 25145, loss = 5.56637662\n",
      "Iteration 25146, loss = 4.89532724\n",
      "Iteration 25147, loss = 4.27427008\n",
      "Iteration 25148, loss = 3.86068852\n",
      "Iteration 25149, loss = 3.97364355\n",
      "Iteration 25150, loss = 4.43811272\n",
      "Iteration 25151, loss = 4.14835732\n",
      "Iteration 25152, loss = 4.08099724\n",
      "Iteration 25153, loss = 4.97031028\n",
      "Iteration 25154, loss = 4.66566534\n",
      "Iteration 25155, loss = 5.02715686\n",
      "Iteration 25156, loss = 4.80364827\n",
      "Iteration 25157, loss = 4.80806567\n",
      "Iteration 25158, loss = 4.66180412\n",
      "Iteration 25159, loss = 4.22229351\n",
      "Iteration 25160, loss = 4.31364833\n",
      "Iteration 25161, loss = 4.47959491\n",
      "Iteration 25162, loss = 4.53790563\n",
      "Iteration 25163, loss = 4.41356610\n",
      "Iteration 25164, loss = 4.40612916\n",
      "Iteration 25165, loss = 4.44689254\n",
      "Iteration 25166, loss = 4.68140369\n",
      "Iteration 25167, loss = 4.26743168\n",
      "Iteration 25168, loss = 3.76584768\n",
      "Iteration 25169, loss = 3.76902709\n",
      "Iteration 25170, loss = 4.02578376\n",
      "Iteration 25171, loss = 4.03444690\n",
      "Iteration 25172, loss = 4.31139356\n",
      "Iteration 25173, loss = 4.07463621\n",
      "Iteration 25174, loss = 4.51854970\n",
      "Iteration 25175, loss = 4.77040283\n",
      "Iteration 25176, loss = 4.48584201\n",
      "Iteration 25177, loss = 5.39325477\n",
      "Iteration 25178, loss = 5.95089490\n",
      "Iteration 25179, loss = 5.60928359\n",
      "Iteration 25180, loss = 4.51645240\n",
      "Iteration 25181, loss = 4.14088521\n",
      "Iteration 25182, loss = 5.08020653\n",
      "Iteration 25183, loss = 4.67727556\n",
      "Iteration 25184, loss = 5.15086752\n",
      "Iteration 25185, loss = 5.41399143\n",
      "Iteration 25186, loss = 5.89734564\n",
      "Iteration 25187, loss = 5.74432493\n",
      "Iteration 25188, loss = 5.92546898\n",
      "Iteration 25189, loss = 5.20806337\n",
      "Iteration 25190, loss = 4.46692529\n",
      "Iteration 25191, loss = 4.18744895\n",
      "Iteration 25192, loss = 3.89818404\n",
      "Iteration 25193, loss = 3.98794235\n",
      "Iteration 25194, loss = 4.14972293\n",
      "Iteration 25195, loss = 4.66788538\n",
      "Iteration 25196, loss = 4.33929258\n",
      "Iteration 25197, loss = 4.65246977\n",
      "Iteration 25198, loss = 5.05237563\n",
      "Iteration 25199, loss = 4.78391804\n",
      "Iteration 25200, loss = 4.99675330\n",
      "Iteration 25201, loss = 4.16705229\n",
      "Iteration 25202, loss = 4.11315432\n",
      "Iteration 25203, loss = 4.25001901\n",
      "Iteration 25204, loss = 4.04911099\n",
      "Iteration 25205, loss = 4.11129146\n",
      "Iteration 25206, loss = 4.23576992\n",
      "Iteration 25207, loss = 4.22199996\n",
      "Iteration 25208, loss = 4.26748011\n",
      "Iteration 25209, loss = 4.19267326\n",
      "Iteration 25210, loss = 4.02825399\n",
      "Iteration 25211, loss = 4.17903254\n",
      "Iteration 25212, loss = 4.43995466\n",
      "Iteration 25213, loss = 4.98366979\n",
      "Iteration 25214, loss = 7.10428087\n",
      "Iteration 25215, loss = 6.22353346\n",
      "Iteration 25216, loss = 5.76591199\n",
      "Iteration 25217, loss = 4.69245248\n",
      "Iteration 25218, loss = 4.71483334\n",
      "Iteration 25219, loss = 5.00400680\n",
      "Iteration 25220, loss = 4.35653216\n",
      "Iteration 25221, loss = 4.69614755\n",
      "Iteration 25222, loss = 4.70374103\n",
      "Iteration 25223, loss = 4.62753651\n",
      "Iteration 25224, loss = 4.93801102\n",
      "Iteration 25225, loss = 4.30380176\n",
      "Iteration 25226, loss = 5.48101188\n",
      "Iteration 25227, loss = 5.02679515\n",
      "Iteration 25228, loss = 5.39369081\n",
      "Iteration 25229, loss = 4.55993398\n",
      "Iteration 25230, loss = 4.30450886\n",
      "Iteration 25231, loss = 4.44583193\n",
      "Iteration 25232, loss = 4.75414742\n",
      "Iteration 25233, loss = 4.70752989\n",
      "Iteration 25234, loss = 4.56234460\n",
      "Iteration 25235, loss = 4.16328596\n",
      "Iteration 25236, loss = 4.04749188\n",
      "Iteration 25237, loss = 3.87543399\n",
      "Iteration 25238, loss = 4.50625144\n",
      "Iteration 25239, loss = 5.42221753\n",
      "Iteration 25240, loss = 4.89014538\n",
      "Iteration 25241, loss = 5.61442357\n",
      "Iteration 25242, loss = 4.78852694\n",
      "Iteration 25243, loss = 6.65618760\n",
      "Iteration 25244, loss = 5.79803666\n",
      "Iteration 25245, loss = 4.89652853\n",
      "Iteration 25246, loss = 4.73050633\n",
      "Iteration 25247, loss = 5.58130025\n",
      "Iteration 25248, loss = 4.62444140\n",
      "Iteration 25249, loss = 4.79920047\n",
      "Iteration 25250, loss = 4.13800039\n",
      "Iteration 25251, loss = 4.32774269\n",
      "Iteration 25252, loss = 4.57198758\n",
      "Iteration 25253, loss = 4.82568312\n",
      "Iteration 25254, loss = 5.53855707\n",
      "Iteration 25255, loss = 5.09709672\n",
      "Iteration 25256, loss = 5.03546216\n",
      "Iteration 25257, loss = 4.98850171\n",
      "Iteration 25258, loss = 6.29549664\n",
      "Iteration 25259, loss = 6.09869078\n",
      "Iteration 25260, loss = 4.81187385\n",
      "Iteration 25261, loss = 4.58382755\n",
      "Iteration 25262, loss = 4.61293902\n",
      "Iteration 25263, loss = 4.29357937\n",
      "Iteration 25264, loss = 4.43332822\n",
      "Iteration 25265, loss = 3.95520094\n",
      "Iteration 25266, loss = 4.55452488\n",
      "Iteration 25267, loss = 3.76251684\n",
      "Iteration 25268, loss = 4.14580546\n",
      "Iteration 25269, loss = 4.67924147\n",
      "Iteration 25270, loss = 4.44814259\n",
      "Iteration 25271, loss = 4.24865297\n",
      "Iteration 25272, loss = 4.26959142\n",
      "Iteration 25273, loss = 4.31540592\n",
      "Iteration 25274, loss = 4.06706601\n",
      "Iteration 25275, loss = 4.02275697\n",
      "Iteration 25276, loss = 4.00046706\n",
      "Iteration 25277, loss = 3.81928811\n",
      "Iteration 25278, loss = 3.84492378\n",
      "Iteration 25279, loss = 4.13734967\n",
      "Iteration 25280, loss = 4.13736055\n",
      "Iteration 25281, loss = 4.46783689\n",
      "Iteration 25282, loss = 4.57472605\n",
      "Iteration 25283, loss = 4.28830652\n",
      "Iteration 25284, loss = 5.18310264\n",
      "Iteration 25285, loss = 4.18656201\n",
      "Iteration 25286, loss = 4.96171091\n",
      "Iteration 25287, loss = 4.61217141\n",
      "Iteration 25288, loss = 4.73003816\n",
      "Iteration 25289, loss = 4.40165993\n",
      "Iteration 25290, loss = 4.47419235\n",
      "Iteration 25291, loss = 4.62228634\n",
      "Iteration 25292, loss = 4.57626162\n",
      "Iteration 25293, loss = 5.09578397\n",
      "Iteration 25294, loss = 5.55299806\n",
      "Iteration 25295, loss = 5.38640262\n",
      "Iteration 25296, loss = 4.44809261\n",
      "Iteration 25297, loss = 4.72888471\n",
      "Iteration 25298, loss = 4.60712353\n",
      "Iteration 25299, loss = 3.95901120\n",
      "Iteration 25300, loss = 3.80486778\n",
      "Iteration 25301, loss = 3.80030292\n",
      "Iteration 25302, loss = 3.86212726\n",
      "Iteration 25303, loss = 3.79532274\n",
      "Iteration 25304, loss = 4.07130546\n",
      "Iteration 25305, loss = 3.97755657\n",
      "Iteration 25306, loss = 4.62276815\n",
      "Iteration 25307, loss = 3.97493054\n",
      "Iteration 25308, loss = 4.40189112\n",
      "Iteration 25309, loss = 4.56835120\n",
      "Iteration 25310, loss = 4.47065183\n",
      "Iteration 25311, loss = 3.87013356\n",
      "Iteration 25312, loss = 4.48768512\n",
      "Iteration 25313, loss = 5.24407678\n",
      "Iteration 25314, loss = 4.47241034\n",
      "Iteration 25315, loss = 5.64404103\n",
      "Iteration 25316, loss = 5.23208376\n",
      "Iteration 25317, loss = 6.12844152\n",
      "Iteration 25318, loss = 4.52192846\n",
      "Iteration 25319, loss = 5.44539141\n",
      "Iteration 25320, loss = 4.43523908\n",
      "Iteration 25321, loss = 4.07833461\n",
      "Iteration 25322, loss = 4.45530854\n",
      "Iteration 25323, loss = 5.38809505\n",
      "Iteration 25324, loss = 5.38313755\n",
      "Iteration 25325, loss = 5.99484772\n",
      "Iteration 25326, loss = 5.23878777\n",
      "Iteration 25327, loss = 5.08528339\n",
      "Iteration 25328, loss = 5.09989319\n",
      "Iteration 25329, loss = 4.73257685\n",
      "Iteration 25330, loss = 4.49524030\n",
      "Iteration 25331, loss = 4.74408204\n",
      "Iteration 25332, loss = 4.17553627\n",
      "Iteration 25333, loss = 3.97623926\n",
      "Iteration 25334, loss = 4.01278086\n",
      "Iteration 25335, loss = 4.27716311\n",
      "Iteration 25336, loss = 4.26185822\n",
      "Iteration 25337, loss = 4.24601762\n",
      "Iteration 25338, loss = 3.95607271\n",
      "Iteration 25339, loss = 3.65599788\n",
      "Iteration 25340, loss = 3.97997505\n",
      "Iteration 25341, loss = 3.92408435\n",
      "Iteration 25342, loss = 4.23418764\n",
      "Iteration 25343, loss = 4.30821905\n",
      "Iteration 25344, loss = 3.69036915\n",
      "Iteration 25345, loss = 3.87006196\n",
      "Iteration 25346, loss = 3.91536341\n",
      "Iteration 25347, loss = 4.03639884\n",
      "Iteration 25348, loss = 3.94877234\n",
      "Iteration 25349, loss = 4.49831208\n",
      "Iteration 25350, loss = 3.96246977\n",
      "Iteration 25351, loss = 4.08062888\n",
      "Iteration 25352, loss = 4.36290479\n",
      "Iteration 25353, loss = 4.28217961\n",
      "Iteration 25354, loss = 4.47478393\n",
      "Iteration 25355, loss = 4.64469397\n",
      "Iteration 25356, loss = 4.94677618\n",
      "Iteration 25357, loss = 4.25564072\n",
      "Iteration 25358, loss = 5.04963929\n",
      "Iteration 25359, loss = 5.01695513\n",
      "Iteration 25360, loss = 4.76424645\n",
      "Iteration 25361, loss = 3.84158380\n",
      "Iteration 25362, loss = 4.22972669\n",
      "Iteration 25363, loss = 4.31800305\n",
      "Iteration 25364, loss = 5.11301750\n",
      "Iteration 25365, loss = 5.39349775\n",
      "Iteration 25366, loss = 5.38634465\n",
      "Iteration 25367, loss = 3.87308407\n",
      "Iteration 25368, loss = 3.94707285\n",
      "Iteration 25369, loss = 3.76975275\n",
      "Iteration 25370, loss = 3.87289222\n",
      "Iteration 25371, loss = 3.73412884\n",
      "Iteration 25372, loss = 3.73154977\n",
      "Iteration 25373, loss = 4.00816923\n",
      "Iteration 25374, loss = 3.98970814\n",
      "Iteration 25375, loss = 3.84281178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25376, loss = 3.78044212\n",
      "Iteration 25377, loss = 3.87856541\n",
      "Iteration 25378, loss = 4.11563693\n",
      "Iteration 25379, loss = 4.00484892\n",
      "Iteration 25380, loss = 4.19564539\n",
      "Iteration 25381, loss = 4.60252880\n",
      "Iteration 25382, loss = 4.00473355\n",
      "Iteration 25383, loss = 4.18336920\n",
      "Iteration 25384, loss = 4.33767874\n",
      "Iteration 25385, loss = 4.09914157\n",
      "Iteration 25386, loss = 4.16068713\n",
      "Iteration 25387, loss = 4.49511952\n",
      "Iteration 25388, loss = 4.44605355\n",
      "Iteration 25389, loss = 4.69769051\n",
      "Iteration 25390, loss = 4.22342885\n",
      "Iteration 25391, loss = 3.94476248\n",
      "Iteration 25392, loss = 4.54008851\n",
      "Iteration 25393, loss = 4.73487334\n",
      "Iteration 25394, loss = 3.99027457\n",
      "Iteration 25395, loss = 4.01676509\n",
      "Iteration 25396, loss = 3.87011442\n",
      "Iteration 25397, loss = 4.77725511\n",
      "Iteration 25398, loss = 4.41344069\n",
      "Iteration 25399, loss = 4.57208667\n",
      "Iteration 25400, loss = 4.94242991\n",
      "Iteration 25401, loss = 4.62010606\n",
      "Iteration 25402, loss = 4.23512873\n",
      "Iteration 25403, loss = 4.08499714\n",
      "Iteration 25404, loss = 3.80938084\n",
      "Iteration 25405, loss = 3.79497640\n",
      "Iteration 25406, loss = 3.95888548\n",
      "Iteration 25407, loss = 3.75655950\n",
      "Iteration 25408, loss = 4.41939036\n",
      "Iteration 25409, loss = 3.78664493\n",
      "Iteration 25410, loss = 4.04950347\n",
      "Iteration 25411, loss = 4.57542404\n",
      "Iteration 25412, loss = 4.58795141\n",
      "Iteration 25413, loss = 4.39242225\n",
      "Iteration 25414, loss = 4.93162380\n",
      "Iteration 25415, loss = 4.48342450\n",
      "Iteration 25416, loss = 5.13293398\n",
      "Iteration 25417, loss = 4.20364966\n",
      "Iteration 25418, loss = 4.19462550\n",
      "Iteration 25419, loss = 4.01355942\n",
      "Iteration 25420, loss = 4.59954569\n",
      "Iteration 25421, loss = 5.39903888\n",
      "Iteration 25422, loss = 5.45273665\n",
      "Iteration 25423, loss = 4.46101811\n",
      "Iteration 25424, loss = 4.66058689\n",
      "Iteration 25425, loss = 5.13410755\n",
      "Iteration 25426, loss = 4.60795872\n",
      "Iteration 25427, loss = 4.17540883\n",
      "Iteration 25428, loss = 4.32792601\n",
      "Iteration 25429, loss = 5.37781069\n",
      "Iteration 25430, loss = 4.94290076\n",
      "Iteration 25431, loss = 4.50157454\n",
      "Iteration 25432, loss = 4.14750048\n",
      "Iteration 25433, loss = 4.59761252\n",
      "Iteration 25434, loss = 4.64104842\n",
      "Iteration 25435, loss = 4.64965752\n",
      "Iteration 25436, loss = 4.16729220\n",
      "Iteration 25437, loss = 4.47796784\n",
      "Iteration 25438, loss = 4.35701382\n",
      "Iteration 25439, loss = 4.14881204\n",
      "Iteration 25440, loss = 3.94863894\n",
      "Iteration 25441, loss = 4.61355552\n",
      "Iteration 25442, loss = 5.61176434\n",
      "Iteration 25443, loss = 5.06737112\n",
      "Iteration 25444, loss = 5.50239341\n",
      "Iteration 25445, loss = 4.72202871\n",
      "Iteration 25446, loss = 4.60064890\n",
      "Iteration 25447, loss = 4.02984356\n",
      "Iteration 25448, loss = 4.48809891\n",
      "Iteration 25449, loss = 4.86230602\n",
      "Iteration 25450, loss = 5.16024407\n",
      "Iteration 25451, loss = 4.62756081\n",
      "Iteration 25452, loss = 4.36706824\n",
      "Iteration 25453, loss = 3.89414749\n",
      "Iteration 25454, loss = 4.39750971\n",
      "Iteration 25455, loss = 4.21440821\n",
      "Iteration 25456, loss = 4.12797579\n",
      "Iteration 25457, loss = 4.00157882\n",
      "Iteration 25458, loss = 4.94432998\n",
      "Iteration 25459, loss = 5.27433026\n",
      "Iteration 25460, loss = 5.95218434\n",
      "Iteration 25461, loss = 5.87959417\n",
      "Iteration 25462, loss = 4.65288529\n",
      "Iteration 25463, loss = 4.58274382\n",
      "Iteration 25464, loss = 4.95671305\n",
      "Iteration 25465, loss = 4.92071254\n",
      "Iteration 25466, loss = 5.24711262\n",
      "Iteration 25467, loss = 4.66111215\n",
      "Iteration 25468, loss = 4.13343715\n",
      "Iteration 25469, loss = 4.20257057\n",
      "Iteration 25470, loss = 4.68048033\n",
      "Iteration 25471, loss = 4.60765837\n",
      "Iteration 25472, loss = 4.00890088\n",
      "Iteration 25473, loss = 4.52717644\n",
      "Iteration 25474, loss = 4.35208882\n",
      "Iteration 25475, loss = 4.28177406\n",
      "Iteration 25476, loss = 4.16661596\n",
      "Iteration 25477, loss = 4.08039727\n",
      "Iteration 25478, loss = 3.73203823\n",
      "Iteration 25479, loss = 4.07459687\n",
      "Iteration 25480, loss = 3.80494216\n",
      "Iteration 25481, loss = 3.77403340\n",
      "Iteration 25482, loss = 4.12972969\n",
      "Iteration 25483, loss = 4.68136342\n",
      "Iteration 25484, loss = 4.49202853\n",
      "Iteration 25485, loss = 3.80514126\n",
      "Iteration 25486, loss = 3.70472115\n",
      "Iteration 25487, loss = 3.68128555\n",
      "Iteration 25488, loss = 3.70973054\n",
      "Iteration 25489, loss = 4.04230849\n",
      "Iteration 25490, loss = 4.26547927\n",
      "Iteration 25491, loss = 4.19605921\n",
      "Iteration 25492, loss = 4.18519493\n",
      "Iteration 25493, loss = 4.53252360\n",
      "Iteration 25494, loss = 4.37651540\n",
      "Iteration 25495, loss = 4.31700633\n",
      "Iteration 25496, loss = 3.98966811\n",
      "Iteration 25497, loss = 4.20227741\n",
      "Iteration 25498, loss = 4.55938555\n",
      "Iteration 25499, loss = 5.13683746\n",
      "Iteration 25500, loss = 5.68010775\n",
      "Iteration 25501, loss = 4.23790041\n",
      "Iteration 25502, loss = 4.23859988\n",
      "Iteration 25503, loss = 3.70471794\n",
      "Iteration 25504, loss = 4.31652261\n",
      "Iteration 25505, loss = 5.45492708\n",
      "Iteration 25506, loss = 5.09613980\n",
      "Iteration 25507, loss = 5.06416902\n",
      "Iteration 25508, loss = 6.18679538\n",
      "Iteration 25509, loss = 4.42073730\n",
      "Iteration 25510, loss = 4.17640680\n",
      "Iteration 25511, loss = 4.19022476\n",
      "Iteration 25512, loss = 4.18851397\n",
      "Iteration 25513, loss = 4.14194432\n",
      "Iteration 25514, loss = 3.81671043\n",
      "Iteration 25515, loss = 3.74009475\n",
      "Iteration 25516, loss = 4.06989009\n",
      "Iteration 25517, loss = 3.94833160\n",
      "Iteration 25518, loss = 4.55488057\n",
      "Iteration 25519, loss = 4.36973382\n",
      "Iteration 25520, loss = 3.83177525\n",
      "Iteration 25521, loss = 3.78680065\n",
      "Iteration 25522, loss = 4.21990676\n",
      "Iteration 25523, loss = 4.60900725\n",
      "Iteration 25524, loss = 5.59855098\n",
      "Iteration 25525, loss = 5.49479403\n",
      "Iteration 25526, loss = 5.73710313\n",
      "Iteration 25527, loss = 5.80852538\n",
      "Iteration 25528, loss = 6.02516592\n",
      "Iteration 25529, loss = 5.86729787\n",
      "Iteration 25530, loss = 5.64242671\n",
      "Iteration 25531, loss = 5.27151059\n",
      "Iteration 25532, loss = 6.08074480\n",
      "Iteration 25533, loss = 5.44243586\n",
      "Iteration 25534, loss = 5.52561958\n",
      "Iteration 25535, loss = 4.44128549\n",
      "Iteration 25536, loss = 4.65913982\n",
      "Iteration 25537, loss = 4.26834322\n",
      "Iteration 25538, loss = 4.61811218\n",
      "Iteration 25539, loss = 4.74784360\n",
      "Iteration 25540, loss = 4.31144211\n",
      "Iteration 25541, loss = 3.89810070\n",
      "Iteration 25542, loss = 3.87500582\n",
      "Iteration 25543, loss = 3.88815925\n",
      "Iteration 25544, loss = 3.80471255\n",
      "Iteration 25545, loss = 4.63443395\n",
      "Iteration 25546, loss = 4.51693960\n",
      "Iteration 25547, loss = 3.80211272\n",
      "Iteration 25548, loss = 4.00893227\n",
      "Iteration 25549, loss = 4.03083891\n",
      "Iteration 25550, loss = 3.78645747\n",
      "Iteration 25551, loss = 4.14239221\n",
      "Iteration 25552, loss = 4.30694705\n",
      "Iteration 25553, loss = 4.83486865\n",
      "Iteration 25554, loss = 4.30883594\n",
      "Iteration 25555, loss = 4.54920287\n",
      "Iteration 25556, loss = 4.39209783\n",
      "Iteration 25557, loss = 3.90927694\n",
      "Iteration 25558, loss = 4.07796691\n",
      "Iteration 25559, loss = 4.26631351\n",
      "Iteration 25560, loss = 4.05900351\n",
      "Iteration 25561, loss = 4.02919969\n",
      "Iteration 25562, loss = 4.28075555\n",
      "Iteration 25563, loss = 4.07351999\n",
      "Iteration 25564, loss = 3.99266765\n",
      "Iteration 25565, loss = 4.13329661\n",
      "Iteration 25566, loss = 4.12162722\n",
      "Iteration 25567, loss = 4.07933083\n",
      "Iteration 25568, loss = 4.25211699\n",
      "Iteration 25569, loss = 4.66786691\n",
      "Iteration 25570, loss = 4.47418235\n",
      "Iteration 25571, loss = 4.02916350\n",
      "Iteration 25572, loss = 4.51079349\n",
      "Iteration 25573, loss = 4.05892620\n",
      "Iteration 25574, loss = 3.96951734\n",
      "Iteration 25575, loss = 3.81321094\n",
      "Iteration 25576, loss = 3.75647607\n",
      "Iteration 25577, loss = 4.00979014\n",
      "Iteration 25578, loss = 3.96132511\n",
      "Iteration 25579, loss = 4.15196348\n",
      "Iteration 25580, loss = 3.95352759\n",
      "Iteration 25581, loss = 3.83163947\n",
      "Iteration 25582, loss = 3.87600508\n",
      "Iteration 25583, loss = 4.10139019\n",
      "Iteration 25584, loss = 4.00584135\n",
      "Iteration 25585, loss = 3.73027714\n",
      "Iteration 25586, loss = 3.80378847\n",
      "Iteration 25587, loss = 3.69330322\n",
      "Iteration 25588, loss = 3.59670474\n",
      "Iteration 25589, loss = 4.58411766\n",
      "Iteration 25590, loss = 4.93415591\n",
      "Iteration 25591, loss = 5.43031010\n",
      "Iteration 25592, loss = 5.93765402\n",
      "Iteration 25593, loss = 4.26886526\n",
      "Iteration 25594, loss = 4.52009737\n",
      "Iteration 25595, loss = 4.12677685\n",
      "Iteration 25596, loss = 4.19376993\n",
      "Iteration 25597, loss = 3.93891293\n",
      "Iteration 25598, loss = 4.22001136\n",
      "Iteration 25599, loss = 5.27407035\n",
      "Iteration 25600, loss = 4.64910817\n",
      "Iteration 25601, loss = 4.95263696\n",
      "Iteration 25602, loss = 4.86661240\n",
      "Iteration 25603, loss = 5.02898717\n",
      "Iteration 25604, loss = 4.45761299\n",
      "Iteration 25605, loss = 3.88899178\n",
      "Iteration 25606, loss = 4.28060623\n",
      "Iteration 25607, loss = 4.48429773\n",
      "Iteration 25608, loss = 3.93287333\n",
      "Iteration 25609, loss = 4.23691512\n",
      "Iteration 25610, loss = 4.91191743\n",
      "Iteration 25611, loss = 4.13210909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25612, loss = 4.39040119\n",
      "Iteration 25613, loss = 4.09215781\n",
      "Iteration 25614, loss = 3.70737212\n",
      "Iteration 25615, loss = 3.95342821\n",
      "Iteration 25616, loss = 4.14977542\n",
      "Iteration 25617, loss = 3.76411865\n",
      "Iteration 25618, loss = 3.94461274\n",
      "Iteration 25619, loss = 3.75846598\n",
      "Iteration 25620, loss = 3.85030727\n",
      "Iteration 25621, loss = 4.05193789\n",
      "Iteration 25622, loss = 3.98777665\n",
      "Iteration 25623, loss = 3.92674511\n",
      "Iteration 25624, loss = 4.22514005\n",
      "Iteration 25625, loss = 4.58529847\n",
      "Iteration 25626, loss = 4.38243787\n",
      "Iteration 25627, loss = 4.11688789\n",
      "Iteration 25628, loss = 3.88524137\n",
      "Iteration 25629, loss = 4.85322063\n",
      "Iteration 25630, loss = 4.76863833\n",
      "Iteration 25631, loss = 4.96578027\n",
      "Iteration 25632, loss = 5.29938599\n",
      "Iteration 25633, loss = 4.28490730\n",
      "Iteration 25634, loss = 4.69786860\n",
      "Iteration 25635, loss = 4.48756826\n",
      "Iteration 25636, loss = 5.26742204\n",
      "Iteration 25637, loss = 3.90376637\n",
      "Iteration 25638, loss = 5.47291621\n",
      "Iteration 25639, loss = 4.19942312\n",
      "Iteration 25640, loss = 4.67996230\n",
      "Iteration 25641, loss = 4.12929192\n",
      "Iteration 25642, loss = 4.08156159\n",
      "Iteration 25643, loss = 4.63148294\n",
      "Iteration 25644, loss = 3.88890154\n",
      "Iteration 25645, loss = 3.95995974\n",
      "Iteration 25646, loss = 3.74427337\n",
      "Iteration 25647, loss = 4.39714455\n",
      "Iteration 25648, loss = 4.96932869\n",
      "Iteration 25649, loss = 4.81608722\n",
      "Iteration 25650, loss = 4.73959045\n",
      "Iteration 25651, loss = 5.55826457\n",
      "Iteration 25652, loss = 4.99027683\n",
      "Iteration 25653, loss = 4.78867419\n",
      "Iteration 25654, loss = 4.44330395\n",
      "Iteration 25655, loss = 4.26926249\n",
      "Iteration 25656, loss = 5.02906351\n",
      "Iteration 25657, loss = 4.25059222\n",
      "Iteration 25658, loss = 4.03324525\n",
      "Iteration 25659, loss = 4.57599372\n",
      "Iteration 25660, loss = 4.84856128\n",
      "Iteration 25661, loss = 4.02477647\n",
      "Iteration 25662, loss = 4.54388050\n",
      "Iteration 25663, loss = 4.27292632\n",
      "Iteration 25664, loss = 4.25304167\n",
      "Iteration 25665, loss = 4.14522147\n",
      "Iteration 25666, loss = 3.98666117\n",
      "Iteration 25667, loss = 3.99651680\n",
      "Iteration 25668, loss = 4.03898380\n",
      "Iteration 25669, loss = 4.98633957\n",
      "Iteration 25670, loss = 5.34867628\n",
      "Iteration 25671, loss = 5.51947633\n",
      "Iteration 25672, loss = 4.06209727\n",
      "Iteration 25673, loss = 4.62772645\n",
      "Iteration 25674, loss = 5.46187834\n",
      "Iteration 25675, loss = 5.27551916\n",
      "Iteration 25676, loss = 5.60925928\n",
      "Iteration 25677, loss = 4.72784333\n",
      "Iteration 25678, loss = 4.59613957\n",
      "Iteration 25679, loss = 6.29878867\n",
      "Iteration 25680, loss = 7.45868071\n",
      "Iteration 25681, loss = 6.15802558\n",
      "Iteration 25682, loss = 5.91388129\n",
      "Iteration 25683, loss = 5.70610939\n",
      "Iteration 25684, loss = 4.53881524\n",
      "Iteration 25685, loss = 4.79724163\n",
      "Iteration 25686, loss = 3.85265660\n",
      "Iteration 25687, loss = 3.89470298\n",
      "Iteration 25688, loss = 4.96928231\n",
      "Iteration 25689, loss = 6.22074742\n",
      "Iteration 25690, loss = 5.95501535\n",
      "Iteration 25691, loss = 6.14498986\n",
      "Iteration 25692, loss = 4.93687358\n",
      "Iteration 25693, loss = 3.88097237\n",
      "Iteration 25694, loss = 5.13222926\n",
      "Iteration 25695, loss = 4.34752782\n",
      "Iteration 25696, loss = 3.89809624\n",
      "Iteration 25697, loss = 4.08768294\n",
      "Iteration 25698, loss = 4.16183227\n",
      "Iteration 25699, loss = 4.80282550\n",
      "Iteration 25700, loss = 4.38311805\n",
      "Iteration 25701, loss = 3.66153618\n",
      "Iteration 25702, loss = 4.68724039\n",
      "Iteration 25703, loss = 4.12371647\n",
      "Iteration 25704, loss = 3.71193599\n",
      "Iteration 25705, loss = 3.79514591\n",
      "Iteration 25706, loss = 4.17914322\n",
      "Iteration 25707, loss = 4.39267168\n",
      "Iteration 25708, loss = 4.71281170\n",
      "Iteration 25709, loss = 4.06113299\n",
      "Iteration 25710, loss = 4.07370393\n",
      "Iteration 25711, loss = 4.05928168\n",
      "Iteration 25712, loss = 3.91193700\n",
      "Iteration 25713, loss = 3.89500435\n",
      "Iteration 25714, loss = 4.14138438\n",
      "Iteration 25715, loss = 3.80130577\n",
      "Iteration 25716, loss = 3.62446273\n",
      "Iteration 25717, loss = 3.79119457\n",
      "Iteration 25718, loss = 3.87760074\n",
      "Iteration 25719, loss = 3.83419796\n",
      "Iteration 25720, loss = 4.67173812\n",
      "Iteration 25721, loss = 4.60991679\n",
      "Iteration 25722, loss = 5.08827330\n",
      "Iteration 25723, loss = 4.57511547\n",
      "Iteration 25724, loss = 3.80756530\n",
      "Iteration 25725, loss = 4.14264000\n",
      "Iteration 25726, loss = 4.08106876\n",
      "Iteration 25727, loss = 3.57368600\n",
      "Iteration 25728, loss = 3.84435643\n",
      "Iteration 25729, loss = 4.31673536\n",
      "Iteration 25730, loss = 3.83795596\n",
      "Iteration 25731, loss = 4.10940425\n",
      "Iteration 25732, loss = 4.14463580\n",
      "Iteration 25733, loss = 4.35860514\n",
      "Iteration 25734, loss = 3.99924425\n",
      "Iteration 25735, loss = 3.97215214\n",
      "Iteration 25736, loss = 4.21164141\n",
      "Iteration 25737, loss = 3.75529182\n",
      "Iteration 25738, loss = 4.17920508\n",
      "Iteration 25739, loss = 4.03422137\n",
      "Iteration 25740, loss = 4.01877674\n",
      "Iteration 25741, loss = 3.71439585\n",
      "Iteration 25742, loss = 3.76947239\n",
      "Iteration 25743, loss = 3.87279360\n",
      "Iteration 25744, loss = 4.47078626\n",
      "Iteration 25745, loss = 4.49023282\n",
      "Iteration 25746, loss = 4.62866210\n",
      "Iteration 25747, loss = 4.34409327\n",
      "Iteration 25748, loss = 4.82410576\n",
      "Iteration 25749, loss = 4.78552818\n",
      "Iteration 25750, loss = 4.81928133\n",
      "Iteration 25751, loss = 4.85562835\n",
      "Iteration 25752, loss = 4.69845298\n",
      "Iteration 25753, loss = 4.62411948\n",
      "Iteration 25754, loss = 4.58062379\n",
      "Iteration 25755, loss = 4.90195950\n",
      "Iteration 25756, loss = 4.36436565\n",
      "Iteration 25757, loss = 4.64797809\n",
      "Iteration 25758, loss = 4.60837692\n",
      "Iteration 25759, loss = 5.00292062\n",
      "Iteration 25760, loss = 4.70082678\n",
      "Iteration 25761, loss = 4.82683094\n",
      "Iteration 25762, loss = 4.26269361\n",
      "Iteration 25763, loss = 4.45549990\n",
      "Iteration 25764, loss = 4.00421056\n",
      "Iteration 25765, loss = 3.92646145\n",
      "Iteration 25766, loss = 3.77380563\n",
      "Iteration 25767, loss = 4.02959969\n",
      "Iteration 25768, loss = 3.94265414\n",
      "Iteration 25769, loss = 4.08959108\n",
      "Iteration 25770, loss = 4.82707559\n",
      "Iteration 25771, loss = 5.01584153\n",
      "Iteration 25772, loss = 4.06246841\n",
      "Iteration 25773, loss = 3.55188398\n",
      "Iteration 25774, loss = 4.97011523\n",
      "Iteration 25775, loss = 4.09766145\n",
      "Iteration 25776, loss = 4.08171015\n",
      "Iteration 25777, loss = 4.50648383\n",
      "Iteration 25778, loss = 4.47271686\n",
      "Iteration 25779, loss = 5.09123793\n",
      "Iteration 25780, loss = 5.11545505\n",
      "Iteration 25781, loss = 6.58762036\n",
      "Iteration 25782, loss = 5.69677091\n",
      "Iteration 25783, loss = 6.30702792\n",
      "Iteration 25784, loss = 5.83254947\n",
      "Iteration 25785, loss = 7.37965797\n",
      "Iteration 25786, loss = 6.67740226\n",
      "Iteration 25787, loss = 4.84850429\n",
      "Iteration 25788, loss = 4.31171029\n",
      "Iteration 25789, loss = 4.28540631\n",
      "Iteration 25790, loss = 4.74058818\n",
      "Iteration 25791, loss = 5.39996025\n",
      "Iteration 25792, loss = 5.32976846\n",
      "Iteration 25793, loss = 4.91590298\n",
      "Iteration 25794, loss = 4.27646117\n",
      "Iteration 25795, loss = 4.13412446\n",
      "Iteration 25796, loss = 3.74532988\n",
      "Iteration 25797, loss = 3.93188909\n",
      "Iteration 25798, loss = 3.92860194\n",
      "Iteration 25799, loss = 4.14341170\n",
      "Iteration 25800, loss = 4.33291005\n",
      "Iteration 25801, loss = 4.81143788\n",
      "Iteration 25802, loss = 4.95792132\n",
      "Iteration 25803, loss = 4.34000334\n",
      "Iteration 25804, loss = 4.09284470\n",
      "Iteration 25805, loss = 3.62366654\n",
      "Iteration 25806, loss = 4.09443481\n",
      "Iteration 25807, loss = 4.19144502\n",
      "Iteration 25808, loss = 3.89479609\n",
      "Iteration 25809, loss = 3.77195168\n",
      "Iteration 25810, loss = 3.91761669\n",
      "Iteration 25811, loss = 4.41749528\n",
      "Iteration 25812, loss = 4.03419611\n",
      "Iteration 25813, loss = 3.90248887\n",
      "Iteration 25814, loss = 3.75793929\n",
      "Iteration 25815, loss = 3.55705071\n",
      "Iteration 25816, loss = 3.84004467\n",
      "Iteration 25817, loss = 4.10073188\n",
      "Iteration 25818, loss = 4.12187701\n",
      "Iteration 25819, loss = 4.30828001\n",
      "Iteration 25820, loss = 3.58985006\n",
      "Iteration 25821, loss = 4.09026906\n",
      "Iteration 25822, loss = 3.98137204\n",
      "Iteration 25823, loss = 3.78969768\n",
      "Iteration 25824, loss = 4.42005069\n",
      "Iteration 25825, loss = 4.31282630\n",
      "Iteration 25826, loss = 4.40020414\n",
      "Iteration 25827, loss = 4.58185724\n",
      "Iteration 25828, loss = 5.33405603\n",
      "Iteration 25829, loss = 7.36581457\n",
      "Iteration 25830, loss = 6.31860089\n",
      "Iteration 25831, loss = 5.78770881\n",
      "Iteration 25832, loss = 5.19634992\n",
      "Iteration 25833, loss = 4.77199934\n",
      "Iteration 25834, loss = 5.66676767\n",
      "Iteration 25835, loss = 5.20001739\n",
      "Iteration 25836, loss = 7.28252058\n",
      "Iteration 25837, loss = 7.38988187\n",
      "Iteration 25838, loss = 6.04174977\n",
      "Iteration 25839, loss = 7.02070641\n",
      "Iteration 25840, loss = 4.36701995\n",
      "Iteration 25841, loss = 4.79192481\n",
      "Iteration 25842, loss = 4.83117082\n",
      "Iteration 25843, loss = 4.29880653\n",
      "Iteration 25844, loss = 4.16001288\n",
      "Iteration 25845, loss = 4.37744128\n",
      "Iteration 25846, loss = 4.53054141\n",
      "Iteration 25847, loss = 4.51969740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25848, loss = 4.94756631\n",
      "Iteration 25849, loss = 4.05908998\n",
      "Iteration 25850, loss = 4.49642022\n",
      "Iteration 25851, loss = 4.51908794\n",
      "Iteration 25852, loss = 4.32065765\n",
      "Iteration 25853, loss = 4.41819496\n",
      "Iteration 25854, loss = 4.24786502\n",
      "Iteration 25855, loss = 4.54474115\n",
      "Iteration 25856, loss = 5.37965997\n",
      "Iteration 25857, loss = 6.67472253\n",
      "Iteration 25858, loss = 5.38514054\n",
      "Iteration 25859, loss = 4.87695257\n",
      "Iteration 25860, loss = 4.06050251\n",
      "Iteration 25861, loss = 4.55660360\n",
      "Iteration 25862, loss = 4.65974637\n",
      "Iteration 25863, loss = 4.20379066\n",
      "Iteration 25864, loss = 4.87359327\n",
      "Iteration 25865, loss = 4.18621122\n",
      "Iteration 25866, loss = 4.77018211\n",
      "Iteration 25867, loss = 4.34647218\n",
      "Iteration 25868, loss = 4.06734848\n",
      "Iteration 25869, loss = 4.29647538\n",
      "Iteration 25870, loss = 4.14027092\n",
      "Iteration 25871, loss = 4.00206824\n",
      "Iteration 25872, loss = 3.92853591\n",
      "Iteration 25873, loss = 4.43429748\n",
      "Iteration 25874, loss = 4.07476971\n",
      "Iteration 25875, loss = 4.07230666\n",
      "Iteration 25876, loss = 3.71812040\n",
      "Iteration 25877, loss = 3.68010089\n",
      "Iteration 25878, loss = 3.58213799\n",
      "Iteration 25879, loss = 3.56004714\n",
      "Iteration 25880, loss = 3.86988280\n",
      "Iteration 25881, loss = 3.69345978\n",
      "Iteration 25882, loss = 3.78104609\n",
      "Iteration 25883, loss = 3.79540385\n",
      "Iteration 25884, loss = 3.87040799\n",
      "Iteration 25885, loss = 3.86726077\n",
      "Iteration 25886, loss = 4.46599678\n",
      "Iteration 25887, loss = 4.06855929\n",
      "Iteration 25888, loss = 4.08270435\n",
      "Iteration 25889, loss = 4.44691707\n",
      "Iteration 25890, loss = 4.32539136\n",
      "Iteration 25891, loss = 4.25036435\n",
      "Iteration 25892, loss = 4.25931061\n",
      "Iteration 25893, loss = 4.14453227\n",
      "Iteration 25894, loss = 4.43280745\n",
      "Iteration 25895, loss = 4.31077410\n",
      "Iteration 25896, loss = 4.47886468\n",
      "Iteration 25897, loss = 4.49736437\n",
      "Iteration 25898, loss = 4.25301012\n",
      "Iteration 25899, loss = 3.95590788\n",
      "Iteration 25900, loss = 3.73963960\n",
      "Iteration 25901, loss = 4.14997226\n",
      "Iteration 25902, loss = 3.94122451\n",
      "Iteration 25903, loss = 4.41214476\n",
      "Iteration 25904, loss = 3.74280629\n",
      "Iteration 25905, loss = 3.82460272\n",
      "Iteration 25906, loss = 4.14746736\n",
      "Iteration 25907, loss = 4.53460032\n",
      "Iteration 25908, loss = 4.44730146\n",
      "Iteration 25909, loss = 4.20196693\n",
      "Iteration 25910, loss = 3.85014355\n",
      "Iteration 25911, loss = 3.79602496\n",
      "Iteration 25912, loss = 3.83916659\n",
      "Iteration 25913, loss = 4.34769991\n",
      "Iteration 25914, loss = 4.09847377\n",
      "Iteration 25915, loss = 3.98023633\n",
      "Iteration 25916, loss = 4.18378476\n",
      "Iteration 25917, loss = 4.72567642\n",
      "Iteration 25918, loss = 4.83137482\n",
      "Iteration 25919, loss = 4.71682043\n",
      "Iteration 25920, loss = 4.61834110\n",
      "Iteration 25921, loss = 4.50505372\n",
      "Iteration 25922, loss = 4.66965989\n",
      "Iteration 25923, loss = 3.70360513\n",
      "Iteration 25924, loss = 5.01048774\n",
      "Iteration 25925, loss = 5.26902080\n",
      "Iteration 25926, loss = 4.63202877\n",
      "Iteration 25927, loss = 4.23289269\n",
      "Iteration 25928, loss = 4.88127952\n",
      "Iteration 25929, loss = 5.09967994\n",
      "Iteration 25930, loss = 5.77893921\n",
      "Iteration 25931, loss = 4.69788031\n",
      "Iteration 25932, loss = 4.44596167\n",
      "Iteration 25933, loss = 4.31194611\n",
      "Iteration 25934, loss = 3.85374015\n",
      "Iteration 25935, loss = 4.11495329\n",
      "Iteration 25936, loss = 4.59907364\n",
      "Iteration 25937, loss = 3.81379908\n",
      "Iteration 25938, loss = 4.06925841\n",
      "Iteration 25939, loss = 3.94817188\n",
      "Iteration 25940, loss = 3.84169762\n",
      "Iteration 25941, loss = 3.80590108\n",
      "Iteration 25942, loss = 3.58302456\n",
      "Iteration 25943, loss = 3.96874101\n",
      "Iteration 25944, loss = 3.98192256\n",
      "Iteration 25945, loss = 3.72024645\n",
      "Iteration 25946, loss = 3.71787748\n",
      "Iteration 25947, loss = 4.06040049\n",
      "Iteration 25948, loss = 3.74092481\n",
      "Iteration 25949, loss = 3.65447079\n",
      "Iteration 25950, loss = 3.61135561\n",
      "Iteration 25951, loss = 3.72645294\n",
      "Iteration 25952, loss = 4.01524164\n",
      "Iteration 25953, loss = 3.87663729\n",
      "Iteration 25954, loss = 4.17410931\n",
      "Iteration 25955, loss = 4.20062819\n",
      "Iteration 25956, loss = 3.52312750\n",
      "Iteration 25957, loss = 3.96151922\n",
      "Iteration 25958, loss = 3.80025829\n",
      "Iteration 25959, loss = 3.84524632\n",
      "Iteration 25960, loss = 3.90358974\n",
      "Iteration 25961, loss = 3.77592644\n",
      "Iteration 25962, loss = 3.77473189\n",
      "Iteration 25963, loss = 3.82088058\n",
      "Iteration 25964, loss = 3.57063365\n",
      "Iteration 25965, loss = 3.67251557\n",
      "Iteration 25966, loss = 3.65629533\n",
      "Iteration 25967, loss = 3.85975917\n",
      "Iteration 25968, loss = 4.33529889\n",
      "Iteration 25969, loss = 3.97722245\n",
      "Iteration 25970, loss = 4.24168034\n",
      "Iteration 25971, loss = 3.88889035\n",
      "Iteration 25972, loss = 3.77787302\n",
      "Iteration 25973, loss = 3.75377509\n",
      "Iteration 25974, loss = 4.43579290\n",
      "Iteration 25975, loss = 5.91880506\n",
      "Iteration 25976, loss = 5.94018227\n",
      "Iteration 25977, loss = 6.74380937\n",
      "Iteration 25978, loss = 5.28396087\n",
      "Iteration 25979, loss = 4.68373148\n",
      "Iteration 25980, loss = 5.37844384\n",
      "Iteration 25981, loss = 4.02162196\n",
      "Iteration 25982, loss = 5.67274701\n",
      "Iteration 25983, loss = 5.56013444\n",
      "Iteration 25984, loss = 5.15733259\n",
      "Iteration 25985, loss = 4.26398260\n",
      "Iteration 25986, loss = 4.68747831\n",
      "Iteration 25987, loss = 5.64569435\n",
      "Iteration 25988, loss = 5.02045824\n",
      "Iteration 25989, loss = 4.09222020\n",
      "Iteration 25990, loss = 3.66667268\n",
      "Iteration 25991, loss = 4.41100185\n",
      "Iteration 25992, loss = 5.12744385\n",
      "Iteration 25993, loss = 4.43716459\n",
      "Iteration 25994, loss = 3.65584495\n",
      "Iteration 25995, loss = 3.86042357\n",
      "Iteration 25996, loss = 3.60770033\n",
      "Iteration 25997, loss = 3.56358945\n",
      "Iteration 25998, loss = 4.07724573\n",
      "Iteration 25999, loss = 4.78863180\n",
      "Iteration 26000, loss = 5.23557201\n",
      "Iteration 26001, loss = 4.86504703\n",
      "Iteration 26002, loss = 4.16010750\n",
      "Iteration 26003, loss = 4.13461614\n",
      "Iteration 26004, loss = 4.26059063\n",
      "Iteration 26005, loss = 4.31506933\n",
      "Iteration 26006, loss = 3.94846163\n",
      "Iteration 26007, loss = 3.78056130\n",
      "Iteration 26008, loss = 3.90305645\n",
      "Iteration 26009, loss = 4.62100083\n",
      "Iteration 26010, loss = 3.86400232\n",
      "Iteration 26011, loss = 3.86170182\n",
      "Iteration 26012, loss = 3.78473014\n",
      "Iteration 26013, loss = 3.64648196\n",
      "Iteration 26014, loss = 3.56598996\n",
      "Iteration 26015, loss = 3.94619950\n",
      "Iteration 26016, loss = 3.49063674\n",
      "Iteration 26017, loss = 3.97615709\n",
      "Iteration 26018, loss = 3.57663637\n",
      "Iteration 26019, loss = 3.60302582\n",
      "Iteration 26020, loss = 3.89307352\n",
      "Iteration 26021, loss = 4.84125167\n",
      "Iteration 26022, loss = 3.84475016\n",
      "Iteration 26023, loss = 3.76400592\n",
      "Iteration 26024, loss = 4.37929092\n",
      "Iteration 26025, loss = 4.75335327\n",
      "Iteration 26026, loss = 4.04714988\n",
      "Iteration 26027, loss = 3.73719032\n",
      "Iteration 26028, loss = 4.34597707\n",
      "Iteration 26029, loss = 4.12873878\n",
      "Iteration 26030, loss = 4.02146369\n",
      "Iteration 26031, loss = 4.36862175\n",
      "Iteration 26032, loss = 4.04134206\n",
      "Iteration 26033, loss = 4.97979852\n",
      "Iteration 26034, loss = 3.96293277\n",
      "Iteration 26035, loss = 4.81164052\n",
      "Iteration 26036, loss = 4.72902206\n",
      "Iteration 26037, loss = 4.46524265\n",
      "Iteration 26038, loss = 4.51341981\n",
      "Iteration 26039, loss = 4.59034949\n",
      "Iteration 26040, loss = 4.90255821\n",
      "Iteration 26041, loss = 4.88710509\n",
      "Iteration 26042, loss = 4.96504479\n",
      "Iteration 26043, loss = 4.54788170\n",
      "Iteration 26044, loss = 4.11835326\n",
      "Iteration 26045, loss = 4.09752402\n",
      "Iteration 26046, loss = 3.71635280\n",
      "Iteration 26047, loss = 3.69773524\n",
      "Iteration 26048, loss = 4.12120072\n",
      "Iteration 26049, loss = 3.65684978\n",
      "Iteration 26050, loss = 3.70782379\n",
      "Iteration 26051, loss = 4.17535084\n",
      "Iteration 26052, loss = 3.82662046\n",
      "Iteration 26053, loss = 4.20360959\n",
      "Iteration 26054, loss = 4.35767920\n",
      "Iteration 26055, loss = 6.21255982\n",
      "Iteration 26056, loss = 8.65809217\n",
      "Iteration 26057, loss = 6.30066936\n",
      "Iteration 26058, loss = 5.06070302\n",
      "Iteration 26059, loss = 4.56951962\n",
      "Iteration 26060, loss = 4.24088531\n",
      "Iteration 26061, loss = 3.86939779\n",
      "Iteration 26062, loss = 3.97836148\n",
      "Iteration 26063, loss = 5.44143944\n",
      "Iteration 26064, loss = 5.05257423\n",
      "Iteration 26065, loss = 4.60500435\n",
      "Iteration 26066, loss = 4.07062796\n",
      "Iteration 26067, loss = 3.92546195\n",
      "Iteration 26068, loss = 4.01927104\n",
      "Iteration 26069, loss = 4.10520845\n",
      "Iteration 26070, loss = 4.29674668\n",
      "Iteration 26071, loss = 4.65487514\n",
      "Iteration 26072, loss = 4.17520832\n",
      "Iteration 26073, loss = 4.32532499\n",
      "Iteration 26074, loss = 3.89052740\n",
      "Iteration 26075, loss = 3.90396673\n",
      "Iteration 26076, loss = 3.53540865\n",
      "Iteration 26077, loss = 3.56554459\n",
      "Iteration 26078, loss = 3.59388454\n",
      "Iteration 26079, loss = 4.01955823\n",
      "Iteration 26080, loss = 3.97200172\n",
      "Iteration 26081, loss = 3.70710605\n",
      "Iteration 26082, loss = 4.42738702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 26083, loss = 4.02437604\n",
      "Iteration 26084, loss = 4.29508289\n",
      "Iteration 26085, loss = 3.92755467\n",
      "Iteration 26086, loss = 3.50175206\n",
      "Iteration 26087, loss = 4.00571369\n",
      "Iteration 26088, loss = 3.90690029\n",
      "Iteration 26089, loss = 4.64397475\n",
      "Iteration 26090, loss = 4.62875930\n",
      "Iteration 26091, loss = 4.29471636\n",
      "Iteration 26092, loss = 4.79795777\n",
      "Iteration 26093, loss = 4.73723431\n",
      "Iteration 26094, loss = 4.04201928\n",
      "Iteration 26095, loss = 3.72765191\n",
      "Iteration 26096, loss = 4.10989055\n",
      "Iteration 26097, loss = 3.94255766\n",
      "Iteration 26098, loss = 4.07532751\n",
      "Iteration 26099, loss = 3.67201093\n",
      "Iteration 26100, loss = 3.78598400\n",
      "Iteration 26101, loss = 3.66979126\n",
      "Iteration 26102, loss = 3.91826538\n",
      "Iteration 26103, loss = 4.10023606\n",
      "Iteration 26104, loss = 4.81041446\n",
      "Iteration 26105, loss = 4.77329064\n",
      "Iteration 26106, loss = 4.60045430\n",
      "Iteration 26107, loss = 3.95187464\n",
      "Iteration 26108, loss = 4.43618612\n",
      "Iteration 26109, loss = 3.95996855\n",
      "Iteration 26110, loss = 3.61182468\n",
      "Iteration 26111, loss = 3.71301298\n",
      "Iteration 26112, loss = 3.82653654\n",
      "Iteration 26113, loss = 3.60479727\n",
      "Iteration 26114, loss = 3.66986133\n",
      "Iteration 26115, loss = 4.79181789\n",
      "Iteration 26116, loss = 5.30289406\n",
      "Iteration 26117, loss = 5.15489717\n",
      "Iteration 26118, loss = 4.51941934\n",
      "Iteration 26119, loss = 4.07811322\n",
      "Iteration 26120, loss = 3.93346480\n",
      "Iteration 26121, loss = 3.82346957\n",
      "Iteration 26122, loss = 3.73105826\n",
      "Iteration 26123, loss = 3.82018919\n",
      "Iteration 26124, loss = 3.74080895\n",
      "Iteration 26125, loss = 3.85577571\n",
      "Iteration 26126, loss = 4.10648290\n",
      "Iteration 26127, loss = 4.36802527\n",
      "Iteration 26128, loss = 5.79364028\n",
      "Iteration 26129, loss = 6.16610227\n",
      "Iteration 26130, loss = 5.21519405\n",
      "Iteration 26131, loss = 4.57461907\n",
      "Iteration 26132, loss = 4.24002575\n",
      "Iteration 26133, loss = 5.04891063\n",
      "Iteration 26134, loss = 5.60126437\n",
      "Iteration 26135, loss = 4.34240361\n",
      "Iteration 26136, loss = 5.08492288\n",
      "Iteration 26137, loss = 4.87863012\n",
      "Iteration 26138, loss = 5.34425026\n",
      "Iteration 26139, loss = 4.72272588\n",
      "Iteration 26140, loss = 4.50838121\n",
      "Iteration 26141, loss = 4.13315570\n",
      "Iteration 26142, loss = 3.98987752\n",
      "Iteration 26143, loss = 3.53439935\n",
      "Iteration 26144, loss = 3.51719101\n",
      "Iteration 26145, loss = 3.59280586\n",
      "Iteration 26146, loss = 3.63795104\n",
      "Iteration 26147, loss = 3.95897888\n",
      "Iteration 26148, loss = 4.03167235\n",
      "Iteration 26149, loss = 4.07458608\n",
      "Iteration 26150, loss = 4.51015415\n",
      "Iteration 26151, loss = 4.08584528\n",
      "Iteration 26152, loss = 3.56945341\n",
      "Iteration 26153, loss = 4.53124012\n",
      "Iteration 26154, loss = 4.37564003\n",
      "Iteration 26155, loss = 4.88205273\n",
      "Iteration 26156, loss = 4.07041934\n",
      "Iteration 26157, loss = 3.63297386\n",
      "Iteration 26158, loss = 4.37948932\n",
      "Iteration 26159, loss = 4.68925080\n",
      "Iteration 26160, loss = 5.77888061\n",
      "Iteration 26161, loss = 5.71097889\n",
      "Iteration 26162, loss = 5.92980243\n",
      "Iteration 26163, loss = 4.07563489\n",
      "Iteration 26164, loss = 3.88108392\n",
      "Iteration 26165, loss = 3.85113425\n",
      "Iteration 26166, loss = 4.27341079\n",
      "Iteration 26167, loss = 4.52462927\n",
      "Iteration 26168, loss = 4.19010520\n",
      "Iteration 26169, loss = 3.78205312\n",
      "Iteration 26170, loss = 4.86572242\n",
      "Iteration 26171, loss = 3.85610839\n",
      "Iteration 26172, loss = 3.92633446\n",
      "Iteration 26173, loss = 3.90293324\n",
      "Iteration 26174, loss = 3.77682515\n",
      "Iteration 26175, loss = 3.77832331\n",
      "Iteration 26176, loss = 3.70427477\n",
      "Iteration 26177, loss = 3.80313292\n",
      "Iteration 26178, loss = 4.03160054\n",
      "Iteration 26179, loss = 4.21129034\n",
      "Iteration 26180, loss = 4.65369684\n",
      "Iteration 26181, loss = 3.87970563\n",
      "Iteration 26182, loss = 3.82440498\n",
      "Iteration 26183, loss = 4.78172995\n",
      "Iteration 26184, loss = 5.26805786\n",
      "Iteration 26185, loss = 4.86970470\n",
      "Iteration 26186, loss = 4.53513795\n",
      "Iteration 26187, loss = 4.69715974\n",
      "Iteration 26188, loss = 4.68156439\n",
      "Iteration 26189, loss = 4.85431596\n",
      "Iteration 26190, loss = 3.64898803\n",
      "Iteration 26191, loss = 4.25415483\n",
      "Iteration 26192, loss = 5.20885963\n",
      "Iteration 26193, loss = 5.09985313\n",
      "Iteration 26194, loss = 4.08772949\n",
      "Iteration 26195, loss = 4.11746283\n",
      "Iteration 26196, loss = 4.17702767\n",
      "Iteration 26197, loss = 4.37338555\n",
      "Iteration 26198, loss = 4.45391755\n",
      "Iteration 26199, loss = 4.08766765\n",
      "Iteration 26200, loss = 4.55063364\n",
      "Iteration 26201, loss = 4.68253952\n",
      "Iteration 26202, loss = 3.90845945\n",
      "Iteration 26203, loss = 3.90289784\n",
      "Iteration 26204, loss = 3.46888042\n",
      "Iteration 26205, loss = 3.97148056\n",
      "Iteration 26206, loss = 4.00821058\n",
      "Iteration 26207, loss = 3.88901762\n",
      "Iteration 26208, loss = 4.14880867\n",
      "Iteration 26209, loss = 4.32576473\n",
      "Iteration 26210, loss = 3.85921526\n",
      "Iteration 26211, loss = 4.29718805\n",
      "Iteration 26212, loss = 3.71773199\n",
      "Iteration 26213, loss = 3.93488532\n",
      "Iteration 26214, loss = 3.87413399\n",
      "Iteration 26215, loss = 3.78160126\n",
      "Iteration 26216, loss = 3.76052557\n",
      "Iteration 26217, loss = 4.87920885\n",
      "Iteration 26218, loss = 4.83651061\n",
      "Iteration 26219, loss = 5.34938853\n",
      "Iteration 26220, loss = 4.25026181\n",
      "Iteration 26221, loss = 4.02357606\n",
      "Iteration 26222, loss = 3.94398789\n",
      "Iteration 26223, loss = 3.88659253\n",
      "Iteration 26224, loss = 3.62950520\n",
      "Iteration 26225, loss = 3.89658326\n",
      "Iteration 26226, loss = 3.82470825\n",
      "Iteration 26227, loss = 3.86879728\n",
      "Iteration 26228, loss = 4.02038872\n",
      "Iteration 26229, loss = 4.41547532\n",
      "Iteration 26230, loss = 4.94915561\n",
      "Iteration 26231, loss = 5.45299832\n",
      "Iteration 26232, loss = 4.34870381\n",
      "Iteration 26233, loss = 4.29545864\n",
      "Iteration 26234, loss = 3.85626117\n",
      "Iteration 26235, loss = 4.17505506\n",
      "Iteration 26236, loss = 3.80376699\n",
      "Iteration 26237, loss = 3.94556449\n",
      "Iteration 26238, loss = 3.87099570\n",
      "Iteration 26239, loss = 4.13700029\n",
      "Iteration 26240, loss = 4.43458049\n",
      "Iteration 26241, loss = 4.53357936\n",
      "Iteration 26242, loss = 4.02446067\n",
      "Iteration 26243, loss = 4.37617817\n",
      "Iteration 26244, loss = 6.25559254\n",
      "Iteration 26245, loss = 4.98482231\n",
      "Iteration 26246, loss = 4.32217505\n",
      "Iteration 26247, loss = 4.36413546\n",
      "Iteration 26248, loss = 4.52066611\n",
      "Iteration 26249, loss = 4.92511090\n",
      "Iteration 26250, loss = 3.81387815\n",
      "Iteration 26251, loss = 3.69840777\n",
      "Iteration 26252, loss = 3.79517174\n",
      "Iteration 26253, loss = 3.99301470\n",
      "Iteration 26254, loss = 4.10732927\n",
      "Iteration 26255, loss = 3.88956019\n",
      "Iteration 26256, loss = 3.88902092\n",
      "Iteration 26257, loss = 4.02901899\n",
      "Iteration 26258, loss = 4.71276533\n",
      "Iteration 26259, loss = 4.60658705\n",
      "Iteration 26260, loss = 5.01775713\n",
      "Iteration 26261, loss = 4.31929126\n",
      "Iteration 26262, loss = 4.04963924\n",
      "Iteration 26263, loss = 4.56492863\n",
      "Iteration 26264, loss = 4.20403118\n",
      "Iteration 26265, loss = 4.24861165\n",
      "Iteration 26266, loss = 3.76011282\n",
      "Iteration 26267, loss = 3.56536315\n",
      "Iteration 26268, loss = 3.84143207\n",
      "Iteration 26269, loss = 4.06656037\n",
      "Iteration 26270, loss = 3.99881062\n",
      "Iteration 26271, loss = 3.55853577\n",
      "Iteration 26272, loss = 3.78976515\n",
      "Iteration 26273, loss = 3.84208699\n",
      "Iteration 26274, loss = 4.53985257\n",
      "Iteration 26275, loss = 4.47999625\n",
      "Iteration 26276, loss = 4.07434510\n",
      "Iteration 26277, loss = 4.70106737\n",
      "Iteration 26278, loss = 4.77961030\n",
      "Iteration 26279, loss = 4.75510809\n",
      "Iteration 26280, loss = 5.65041252\n",
      "Iteration 26281, loss = 5.47238597\n",
      "Iteration 26282, loss = 5.36298514\n",
      "Iteration 26283, loss = 6.78490748\n",
      "Iteration 26284, loss = 5.40753032\n",
      "Iteration 26285, loss = 4.23107878\n",
      "Iteration 26286, loss = 4.69277562\n",
      "Iteration 26287, loss = 3.93505081\n",
      "Iteration 26288, loss = 4.05079354\n",
      "Iteration 26289, loss = 4.00372871\n",
      "Iteration 26290, loss = 4.43031064\n",
      "Iteration 26291, loss = 3.76262358\n",
      "Iteration 26292, loss = 3.84642186\n",
      "Iteration 26293, loss = 3.84061370\n",
      "Iteration 26294, loss = 4.05819138\n",
      "Iteration 26295, loss = 4.25712927\n",
      "Iteration 26296, loss = 4.19970147\n",
      "Iteration 26297, loss = 3.67494078\n",
      "Iteration 26298, loss = 3.66237245\n",
      "Iteration 26299, loss = 3.65193624\n",
      "Iteration 26300, loss = 3.55382699\n",
      "Iteration 26301, loss = 3.54905020\n",
      "Iteration 26302, loss = 4.78870711\n",
      "Iteration 26303, loss = 4.10686082\n",
      "Iteration 26304, loss = 4.44079168\n",
      "Iteration 26305, loss = 4.59005706\n",
      "Iteration 26306, loss = 3.85251628\n",
      "Iteration 26307, loss = 3.95649501\n",
      "Iteration 26308, loss = 3.84768672\n",
      "Iteration 26309, loss = 3.87366135\n",
      "Iteration 26310, loss = 3.97380671\n",
      "Iteration 26311, loss = 3.84103027\n",
      "Iteration 26312, loss = 4.31727377\n",
      "Iteration 26313, loss = 4.07206609\n",
      "Iteration 26314, loss = 3.89006824\n",
      "Iteration 26315, loss = 5.02703929\n",
      "Iteration 26316, loss = 4.06699908\n",
      "Iteration 26317, loss = 3.63139077\n",
      "Iteration 26318, loss = 3.61158199\n",
      "Iteration 26319, loss = 4.61477763\n",
      "Iteration 26320, loss = 3.97756935\n",
      "Iteration 26321, loss = 3.94461674\n",
      "Iteration 26322, loss = 4.10411545\n",
      "Iteration 26323, loss = 4.56802749\n",
      "Iteration 26324, loss = 4.73630089\n",
      "Iteration 26325, loss = 5.82540419\n",
      "Iteration 26326, loss = 8.85804630\n",
      "Iteration 26327, loss = 6.41584092\n",
      "Iteration 26328, loss = 6.77182513\n",
      "Iteration 26329, loss = 5.69976771\n",
      "Iteration 26330, loss = 4.84076822\n",
      "Iteration 26331, loss = 4.52468970\n",
      "Iteration 26332, loss = 4.55401412\n",
      "Iteration 26333, loss = 4.27176511\n",
      "Iteration 26334, loss = 3.97423362\n",
      "Iteration 26335, loss = 3.92361338\n",
      "Iteration 26336, loss = 3.87860714\n",
      "Iteration 26337, loss = 3.91664650\n",
      "Iteration 26338, loss = 4.41795400\n",
      "Iteration 26339, loss = 4.41872312\n",
      "Iteration 26340, loss = 3.96047525\n",
      "Iteration 26341, loss = 4.34861943\n",
      "Iteration 26342, loss = 4.47785117\n",
      "Iteration 26343, loss = 4.60158297\n",
      "Iteration 26344, loss = 4.90173498\n",
      "Iteration 26345, loss = 4.68531692\n",
      "Iteration 26346, loss = 4.04748934\n",
      "Iteration 26347, loss = 4.19368050\n",
      "Iteration 26348, loss = 4.33884635\n",
      "Iteration 26349, loss = 4.82413704\n",
      "Iteration 26350, loss = 3.85586784\n",
      "Iteration 26351, loss = 3.59013319\n",
      "Iteration 26352, loss = 4.34054218\n",
      "Iteration 26353, loss = 4.26595563\n",
      "Iteration 26354, loss = 4.30338751\n",
      "Iteration 26355, loss = 4.46188846\n",
      "Iteration 26356, loss = 4.33588147\n",
      "Iteration 26357, loss = 4.31237764\n",
      "Iteration 26358, loss = 4.79772667\n",
      "Iteration 26359, loss = 4.59444920\n",
      "Iteration 26360, loss = 5.02025907\n",
      "Iteration 26361, loss = 4.23357957\n",
      "Iteration 26362, loss = 4.26025181\n",
      "Iteration 26363, loss = 3.64835608\n",
      "Iteration 26364, loss = 4.07232590\n",
      "Iteration 26365, loss = 3.93882431\n",
      "Iteration 26366, loss = 4.12849635\n",
      "Iteration 26367, loss = 5.02794313\n",
      "Iteration 26368, loss = 4.41615433\n",
      "Iteration 26369, loss = 4.19048430\n",
      "Iteration 26370, loss = 4.38604138\n",
      "Iteration 26371, loss = 4.50804253\n",
      "Iteration 26372, loss = 4.54064411\n",
      "Iteration 26373, loss = 4.37575229\n",
      "Iteration 26374, loss = 4.38192566\n",
      "Iteration 26375, loss = 3.66776492\n",
      "Iteration 26376, loss = 3.76543909\n",
      "Iteration 26377, loss = 3.53875943\n",
      "Iteration 26378, loss = 3.50684489\n",
      "Iteration 26379, loss = 4.04193007\n",
      "Iteration 26380, loss = 3.81955590\n",
      "Iteration 26381, loss = 3.47434479\n",
      "Iteration 26382, loss = 3.71545019\n",
      "Iteration 26383, loss = 4.55223081\n",
      "Iteration 26384, loss = 4.67907716\n",
      "Iteration 26385, loss = 4.15891301\n",
      "Iteration 26386, loss = 3.98518847\n",
      "Iteration 26387, loss = 3.97130777\n",
      "Iteration 26388, loss = 3.59684624\n",
      "Iteration 26389, loss = 3.53380224\n",
      "Iteration 26390, loss = 3.49193828\n",
      "Iteration 26391, loss = 3.69876991\n",
      "Iteration 26392, loss = 3.64558033\n",
      "Iteration 26393, loss = 4.06815409\n",
      "Iteration 26394, loss = 3.89779212\n",
      "Iteration 26395, loss = 5.06119937\n",
      "Iteration 26396, loss = 6.68039881\n",
      "Iteration 26397, loss = 6.13832448\n",
      "Iteration 26398, loss = 5.58227608\n",
      "Iteration 26399, loss = 5.40434376\n",
      "Iteration 26400, loss = 4.98169561\n",
      "Iteration 26401, loss = 4.31596383\n",
      "Iteration 26402, loss = 4.17343482\n",
      "Iteration 26403, loss = 3.76750673\n",
      "Iteration 26404, loss = 3.46549325\n",
      "Iteration 26405, loss = 3.66393978\n",
      "Iteration 26406, loss = 3.70450865\n",
      "Iteration 26407, loss = 3.79062883\n",
      "Iteration 26408, loss = 3.65426470\n",
      "Iteration 26409, loss = 4.32324923\n",
      "Iteration 26410, loss = 3.84420840\n",
      "Iteration 26411, loss = 3.55545414\n",
      "Iteration 26412, loss = 3.66695534\n",
      "Iteration 26413, loss = 3.91234863\n",
      "Iteration 26414, loss = 4.76336408\n",
      "Iteration 26415, loss = 3.96555705\n",
      "Iteration 26416, loss = 3.48317653\n",
      "Iteration 26417, loss = 3.39816945\n",
      "Iteration 26418, loss = 3.56297883\n",
      "Iteration 26419, loss = 3.39906366\n",
      "Iteration 26420, loss = 3.78292340\n",
      "Iteration 26421, loss = 4.15213539\n",
      "Iteration 26422, loss = 3.93044068\n",
      "Iteration 26423, loss = 3.87826085\n",
      "Iteration 26424, loss = 3.52368051\n",
      "Iteration 26425, loss = 3.83038357\n",
      "Iteration 26426, loss = 3.69413661\n",
      "Iteration 26427, loss = 4.24447729\n",
      "Iteration 26428, loss = 4.51892475\n",
      "Iteration 26429, loss = 4.45591680\n",
      "Iteration 26430, loss = 4.79634078\n",
      "Iteration 26431, loss = 4.29850577\n",
      "Iteration 26432, loss = 4.82758507\n",
      "Iteration 26433, loss = 4.04298709\n",
      "Iteration 26434, loss = 4.24364835\n",
      "Iteration 26435, loss = 3.89406970\n",
      "Iteration 26436, loss = 3.67617376\n",
      "Iteration 26437, loss = 3.88102408\n",
      "Iteration 26438, loss = 4.04973045\n",
      "Iteration 26439, loss = 4.39023397\n",
      "Iteration 26440, loss = 4.18467733\n",
      "Iteration 26441, loss = 4.03373617\n",
      "Iteration 26442, loss = 3.75426495\n",
      "Iteration 26443, loss = 3.52319190\n",
      "Iteration 26444, loss = 3.53700810\n",
      "Iteration 26445, loss = 3.56996595\n",
      "Iteration 26446, loss = 4.42784222\n",
      "Iteration 26447, loss = 4.36638046\n",
      "Iteration 26448, loss = 3.97159604\n",
      "Iteration 26449, loss = 4.36210769\n",
      "Iteration 26450, loss = 4.28311928\n",
      "Iteration 26451, loss = 3.78238903\n",
      "Iteration 26452, loss = 3.69279979\n",
      "Iteration 26453, loss = 4.09787110\n",
      "Iteration 26454, loss = 4.48340738\n",
      "Iteration 26455, loss = 4.58638785\n",
      "Iteration 26456, loss = 4.89462417\n",
      "Iteration 26457, loss = 4.92223319\n",
      "Iteration 26458, loss = 4.36913768\n",
      "Iteration 26459, loss = 3.60751915\n",
      "Iteration 26460, loss = 3.93521865\n",
      "Iteration 26461, loss = 3.85404646\n",
      "Iteration 26462, loss = 3.91673613\n",
      "Iteration 26463, loss = 3.77173787\n",
      "Iteration 26464, loss = 3.98776866\n",
      "Iteration 26465, loss = 3.85054653\n",
      "Iteration 26466, loss = 3.86500286\n",
      "Iteration 26467, loss = 3.72837952\n",
      "Iteration 26468, loss = 3.87342384\n",
      "Iteration 26469, loss = 3.68839981\n",
      "Iteration 26470, loss = 3.57784102\n",
      "Iteration 26471, loss = 3.59582249\n",
      "Iteration 26472, loss = 3.68803323\n",
      "Iteration 26473, loss = 4.23129961\n",
      "Iteration 26474, loss = 3.91248040\n",
      "Iteration 26475, loss = 3.77109195\n",
      "Iteration 26476, loss = 4.37264408\n",
      "Iteration 26477, loss = 4.26440097\n",
      "Iteration 26478, loss = 4.39335073\n",
      "Iteration 26479, loss = 4.27930811\n",
      "Iteration 26480, loss = 3.80920674\n",
      "Iteration 26481, loss = 3.61648252\n",
      "Iteration 26482, loss = 3.61917561\n",
      "Iteration 26483, loss = 3.60888610\n",
      "Iteration 26484, loss = 4.41862522\n",
      "Iteration 26485, loss = 4.54864244\n",
      "Iteration 26486, loss = 4.14172127\n",
      "Iteration 26487, loss = 4.10456167\n",
      "Iteration 26488, loss = 4.39382692\n",
      "Iteration 26489, loss = 4.13439051\n",
      "Iteration 26490, loss = 4.71513208\n",
      "Iteration 26491, loss = 5.67972638\n",
      "Iteration 26492, loss = 5.25215714\n",
      "Iteration 26493, loss = 4.50341356\n",
      "Iteration 26494, loss = 4.80804221\n",
      "Iteration 26495, loss = 6.13492237\n",
      "Iteration 26496, loss = 7.35843427\n",
      "Iteration 26497, loss = 9.63800060\n",
      "Iteration 26498, loss = 8.39836743\n",
      "Iteration 26499, loss = 7.24663775\n",
      "Iteration 26500, loss = 6.35611659\n",
      "Iteration 26501, loss = 5.06400819\n",
      "Iteration 26502, loss = 3.99087887\n",
      "Iteration 26503, loss = 3.76822673\n",
      "Iteration 26504, loss = 3.93736211\n",
      "Iteration 26505, loss = 3.91744917\n",
      "Iteration 26506, loss = 4.08026470\n",
      "Iteration 26507, loss = 3.84348652\n",
      "Iteration 26508, loss = 3.87100280\n",
      "Iteration 26509, loss = 3.83102164\n",
      "Iteration 26510, loss = 4.15925068\n",
      "Iteration 26511, loss = 3.83772538\n",
      "Iteration 26512, loss = 3.90882002\n",
      "Iteration 26513, loss = 4.09917010\n",
      "Iteration 26514, loss = 5.01428828\n",
      "Iteration 26515, loss = 5.19974771\n",
      "Iteration 26516, loss = 5.33256933\n",
      "Iteration 26517, loss = 6.06286342\n",
      "Iteration 26518, loss = 5.26727968\n",
      "Iteration 26519, loss = 5.22353298\n",
      "Iteration 26520, loss = 4.45836788\n",
      "Iteration 26521, loss = 6.12241721\n",
      "Iteration 26522, loss = 5.11145563\n",
      "Iteration 26523, loss = 4.39312078\n",
      "Iteration 26524, loss = 4.06363199\n",
      "Iteration 26525, loss = 4.54257854\n",
      "Iteration 26526, loss = 3.87578207\n",
      "Iteration 26527, loss = 4.01702119\n",
      "Iteration 26528, loss = 3.64493543\n",
      "Iteration 26529, loss = 3.63983218\n",
      "Iteration 26530, loss = 3.64642395\n",
      "Iteration 26531, loss = 3.69308589\n",
      "Iteration 26532, loss = 4.27657412\n",
      "Iteration 26533, loss = 4.08618861\n",
      "Iteration 26534, loss = 3.82568426\n",
      "Iteration 26535, loss = 4.10744378\n",
      "Iteration 26536, loss = 4.44210247\n",
      "Iteration 26537, loss = 4.74552099\n",
      "Iteration 26538, loss = 4.36517077\n",
      "Iteration 26539, loss = 3.73907731\n",
      "Iteration 26540, loss = 3.64806923\n",
      "Iteration 26541, loss = 3.62590985\n",
      "Iteration 26542, loss = 3.62304879\n",
      "Iteration 26543, loss = 4.76845916\n",
      "Iteration 26544, loss = 4.32026276\n",
      "Iteration 26545, loss = 3.76895705\n",
      "Iteration 26546, loss = 3.55058006\n",
      "Iteration 26547, loss = 3.57010646\n",
      "Iteration 26548, loss = 3.46197710\n",
      "Iteration 26549, loss = 4.12534399\n",
      "Iteration 26550, loss = 3.80863937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 26551, loss = 3.45942806\n",
      "Iteration 26552, loss = 3.45045364\n",
      "Iteration 26553, loss = 3.54475066\n",
      "Iteration 26554, loss = 3.88761089\n",
      "Iteration 26555, loss = 4.08609556\n",
      "Iteration 26556, loss = 3.97756346\n",
      "Iteration 26557, loss = 3.44478275\n",
      "Iteration 26558, loss = 3.91348123\n",
      "Iteration 26559, loss = 4.05597407\n",
      "Iteration 26560, loss = 4.14471848\n",
      "Iteration 26561, loss = 3.45522260\n",
      "Iteration 26562, loss = 3.43923812\n",
      "Iteration 26563, loss = 3.72922427\n",
      "Iteration 26564, loss = 4.28121631\n",
      "Iteration 26565, loss = 4.56782193\n",
      "Iteration 26566, loss = 4.20356002\n",
      "Iteration 26567, loss = 4.42597255\n",
      "Iteration 26568, loss = 4.59966881\n",
      "Iteration 26569, loss = 4.66562732\n",
      "Iteration 26570, loss = 4.20807164\n",
      "Iteration 26571, loss = 3.97991277\n",
      "Iteration 26572, loss = 3.97814255\n",
      "Iteration 26573, loss = 3.89309048\n",
      "Iteration 26574, loss = 4.56685038\n",
      "Iteration 26575, loss = 4.31917375\n",
      "Iteration 26576, loss = 4.25197182\n",
      "Iteration 26577, loss = 4.29481704\n",
      "Iteration 26578, loss = 4.24303150\n",
      "Iteration 26579, loss = 3.86712859\n",
      "Iteration 26580, loss = 3.79484926\n",
      "Iteration 26581, loss = 3.49388078\n",
      "Iteration 26582, loss = 3.49556986\n",
      "Iteration 26583, loss = 3.87744714\n",
      "Iteration 26584, loss = 3.87576943\n",
      "Iteration 26585, loss = 3.85381109\n",
      "Iteration 26586, loss = 3.41504255\n",
      "Iteration 26587, loss = 3.77747681\n",
      "Iteration 26588, loss = 3.61984668\n",
      "Iteration 26589, loss = 3.50651905\n",
      "Iteration 26590, loss = 3.73995628\n",
      "Iteration 26591, loss = 3.38157922\n",
      "Iteration 26592, loss = 4.04674798\n",
      "Iteration 26593, loss = 3.74265366\n",
      "Iteration 26594, loss = 4.28915941\n",
      "Iteration 26595, loss = 5.59605655\n",
      "Iteration 26596, loss = 5.57631483\n",
      "Iteration 26597, loss = 6.34943892\n",
      "Iteration 26598, loss = 4.76279611\n",
      "Iteration 26599, loss = 3.75874525\n",
      "Iteration 26600, loss = 4.48689457\n",
      "Iteration 26601, loss = 5.36956682\n",
      "Iteration 26602, loss = 5.34326506\n",
      "Iteration 26603, loss = 5.01041204\n",
      "Iteration 26604, loss = 4.77992712\n",
      "Iteration 26605, loss = 5.66057331\n",
      "Iteration 26606, loss = 4.37778231\n",
      "Iteration 26607, loss = 3.84593197\n",
      "Iteration 26608, loss = 4.02654746\n",
      "Iteration 26609, loss = 3.60915592\n",
      "Iteration 26610, loss = 3.63203024\n",
      "Iteration 26611, loss = 3.46986699\n",
      "Iteration 26612, loss = 3.39522003\n",
      "Iteration 26613, loss = 3.77318449\n",
      "Iteration 26614, loss = 3.53780995\n",
      "Iteration 26615, loss = 4.57322369\n",
      "Iteration 26616, loss = 4.27356515\n",
      "Iteration 26617, loss = 3.91431401\n",
      "Iteration 26618, loss = 4.39766543\n",
      "Iteration 26619, loss = 3.48871045\n",
      "Iteration 26620, loss = 3.51422271\n",
      "Iteration 26621, loss = 3.62664716\n",
      "Iteration 26622, loss = 4.03784417\n",
      "Iteration 26623, loss = 4.11037909\n",
      "Iteration 26624, loss = 4.36906589\n",
      "Iteration 26625, loss = 3.99065432\n",
      "Iteration 26626, loss = 3.72473362\n",
      "Iteration 26627, loss = 3.76535742\n",
      "Iteration 26628, loss = 3.60713263\n",
      "Iteration 26629, loss = 3.61057840\n",
      "Iteration 26630, loss = 3.69868806\n",
      "Iteration 26631, loss = 3.71151301\n",
      "Iteration 26632, loss = 3.72175943\n",
      "Iteration 26633, loss = 3.47179269\n",
      "Iteration 26634, loss = 3.43075545\n",
      "Iteration 26635, loss = 3.60127450\n",
      "Iteration 26636, loss = 4.19458283\n",
      "Iteration 26637, loss = 4.32349262\n",
      "Iteration 26638, loss = 4.32211130\n",
      "Iteration 26639, loss = 3.46336811\n",
      "Iteration 26640, loss = 3.63048669\n",
      "Iteration 26641, loss = 3.69426726\n",
      "Iteration 26642, loss = 3.56730428\n",
      "Iteration 26643, loss = 3.81857439\n",
      "Iteration 26644, loss = 4.04677570\n",
      "Iteration 26645, loss = 3.95481726\n",
      "Iteration 26646, loss = 5.22335050\n",
      "Iteration 26647, loss = 5.56921739\n",
      "Iteration 26648, loss = 4.26730965\n",
      "Iteration 26649, loss = 5.89718787\n",
      "Iteration 26650, loss = 5.32996335\n",
      "Iteration 26651, loss = 6.98492968\n",
      "Iteration 26652, loss = 7.62465901\n",
      "Iteration 26653, loss = 5.82627355\n",
      "Iteration 26654, loss = 5.28022744\n",
      "Iteration 26655, loss = 3.67467551\n",
      "Iteration 26656, loss = 3.98035792\n",
      "Iteration 26657, loss = 3.51057592\n",
      "Iteration 26658, loss = 3.65358412\n",
      "Iteration 26659, loss = 4.26796989\n",
      "Iteration 26660, loss = 4.17545306\n",
      "Iteration 26661, loss = 3.76306825\n",
      "Iteration 26662, loss = 3.58052117\n",
      "Iteration 26663, loss = 3.63509790\n",
      "Iteration 26664, loss = 4.06012014\n",
      "Iteration 26665, loss = 3.76506428\n",
      "Iteration 26666, loss = 3.79178631\n",
      "Iteration 26667, loss = 3.56354618\n",
      "Iteration 26668, loss = 3.54240648\n",
      "Iteration 26669, loss = 3.40769012\n",
      "Iteration 26670, loss = 3.45005947\n",
      "Iteration 26671, loss = 3.93040721\n",
      "Iteration 26672, loss = 4.38139576\n",
      "Iteration 26673, loss = 4.26615240\n",
      "Iteration 26674, loss = 4.53187593\n",
      "Iteration 26675, loss = 3.97533789\n",
      "Iteration 26676, loss = 3.95293564\n",
      "Iteration 26677, loss = 3.93100149\n",
      "Iteration 26678, loss = 3.94555744\n",
      "Iteration 26679, loss = 3.82018959\n",
      "Iteration 26680, loss = 4.02144790\n",
      "Iteration 26681, loss = 4.06012487\n",
      "Iteration 26682, loss = 3.58462405\n",
      "Iteration 26683, loss = 4.25749583\n",
      "Iteration 26684, loss = 5.10344184\n",
      "Iteration 26685, loss = 5.10145965\n",
      "Iteration 26686, loss = 5.20364597\n",
      "Iteration 26687, loss = 4.35787616\n",
      "Iteration 26688, loss = 4.21465525\n",
      "Iteration 26689, loss = 4.01145735\n",
      "Iteration 26690, loss = 3.73592210\n",
      "Iteration 26691, loss = 4.15756820\n",
      "Iteration 26692, loss = 3.58861067\n",
      "Iteration 26693, loss = 3.99165623\n",
      "Iteration 26694, loss = 3.90393014\n",
      "Iteration 26695, loss = 3.77189523\n",
      "Iteration 26696, loss = 4.27100276\n",
      "Iteration 26697, loss = 4.74385386\n",
      "Iteration 26698, loss = 4.24608853\n",
      "Iteration 26699, loss = 4.46929620\n",
      "Iteration 26700, loss = 3.79896611\n",
      "Iteration 26701, loss = 3.92016297\n",
      "Iteration 26702, loss = 3.55733308\n",
      "Iteration 26703, loss = 3.56753711\n",
      "Iteration 26704, loss = 3.63744007\n",
      "Iteration 26705, loss = 3.71843913\n",
      "Iteration 26706, loss = 3.89435401\n",
      "Iteration 26707, loss = 4.16591195\n",
      "Iteration 26708, loss = 4.64256016\n",
      "Iteration 26709, loss = 3.68278637\n",
      "Iteration 26710, loss = 3.68010453\n",
      "Iteration 26711, loss = 3.63782226\n",
      "Iteration 26712, loss = 3.99767357\n",
      "Iteration 26713, loss = 4.65880486\n",
      "Iteration 26714, loss = 3.67641946\n",
      "Iteration 26715, loss = 4.06612229\n",
      "Iteration 26716, loss = 4.20159840\n",
      "Iteration 26717, loss = 4.41990027\n",
      "Iteration 26718, loss = 4.16811429\n",
      "Iteration 26719, loss = 4.79974229\n",
      "Iteration 26720, loss = 3.91248334\n",
      "Iteration 26721, loss = 3.99420944\n",
      "Iteration 26722, loss = 3.47323182\n",
      "Iteration 26723, loss = 3.68444914\n",
      "Iteration 26724, loss = 4.04760606\n",
      "Iteration 26725, loss = 3.91350915\n",
      "Iteration 26726, loss = 3.84027365\n",
      "Iteration 26727, loss = 4.37326885\n",
      "Iteration 26728, loss = 4.04731162\n",
      "Iteration 26729, loss = 4.47058791\n",
      "Iteration 26730, loss = 4.41005429\n",
      "Iteration 26731, loss = 4.57945995\n",
      "Iteration 26732, loss = 4.24801690\n",
      "Iteration 26733, loss = 3.90713534\n",
      "Iteration 26734, loss = 3.93508833\n",
      "Iteration 26735, loss = 4.13225667\n",
      "Iteration 26736, loss = 3.77513743\n",
      "Iteration 26737, loss = 3.66069846\n",
      "Iteration 26738, loss = 3.61003388\n",
      "Iteration 26739, loss = 3.56720923\n",
      "Iteration 26740, loss = 3.60605549\n",
      "Iteration 26741, loss = 3.39876560\n",
      "Iteration 26742, loss = 4.50908796\n",
      "Iteration 26743, loss = 4.17336492\n",
      "Iteration 26744, loss = 3.79966831\n",
      "Iteration 26745, loss = 4.40351281\n",
      "Iteration 26746, loss = 4.02787086\n",
      "Iteration 26747, loss = 3.50045262\n",
      "Iteration 26748, loss = 3.60877305\n",
      "Iteration 26749, loss = 3.43498967\n",
      "Iteration 26750, loss = 3.34385688\n",
      "Iteration 26751, loss = 3.77794467\n",
      "Iteration 26752, loss = 4.14298932\n",
      "Iteration 26753, loss = 4.05312855\n",
      "Iteration 26754, loss = 3.28761782\n",
      "Iteration 26755, loss = 3.71984626\n",
      "Iteration 26756, loss = 4.64190952\n",
      "Iteration 26757, loss = 4.74863069\n",
      "Iteration 26758, loss = 4.40587184\n",
      "Iteration 26759, loss = 3.74998570\n",
      "Iteration 26760, loss = 4.04262581\n",
      "Iteration 26761, loss = 3.93495421\n",
      "Iteration 26762, loss = 4.12833281\n",
      "Iteration 26763, loss = 3.95061308\n",
      "Iteration 26764, loss = 5.15567617\n",
      "Iteration 26765, loss = 4.93968674\n",
      "Iteration 26766, loss = 4.80620682\n",
      "Iteration 26767, loss = 4.40391270\n",
      "Iteration 26768, loss = 3.90233696\n",
      "Iteration 26769, loss = 3.81767428\n",
      "Iteration 26770, loss = 3.37798653\n",
      "Iteration 26771, loss = 4.19739731\n",
      "Iteration 26772, loss = 4.59858659\n",
      "Iteration 26773, loss = 4.07713837\n",
      "Iteration 26774, loss = 4.33827114\n",
      "Iteration 26775, loss = 4.30739381\n",
      "Iteration 26776, loss = 4.34944083\n",
      "Iteration 26777, loss = 3.69231966\n",
      "Iteration 26778, loss = 3.93750758\n",
      "Iteration 26779, loss = 4.04599355\n",
      "Iteration 26780, loss = 3.70990941\n",
      "Iteration 26781, loss = 4.03670023\n",
      "Iteration 26782, loss = 3.93761802\n",
      "Iteration 26783, loss = 3.88758291\n",
      "Iteration 26784, loss = 3.74790733\n",
      "Iteration 26785, loss = 4.16424597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 26786, loss = 6.15025174\n",
      "Iteration 26787, loss = 4.99335538\n",
      "Iteration 26788, loss = 4.37187204\n",
      "Iteration 26789, loss = 3.99937988\n",
      "Iteration 26790, loss = 3.77083543\n",
      "Iteration 26791, loss = 3.74264503\n",
      "Iteration 26792, loss = 3.85625346\n",
      "Iteration 26793, loss = 3.86938244\n",
      "Iteration 26794, loss = 4.14793668\n",
      "Iteration 26795, loss = 4.01692481\n",
      "Iteration 26796, loss = 4.20887777\n",
      "Iteration 26797, loss = 3.84235276\n",
      "Iteration 26798, loss = 4.19102733\n",
      "Iteration 26799, loss = 5.14684721\n",
      "Iteration 26800, loss = 4.62238469\n",
      "Iteration 26801, loss = 4.60720522\n",
      "Iteration 26802, loss = 5.28389404\n",
      "Iteration 26803, loss = 5.27851919\n",
      "Iteration 26804, loss = 4.23331958\n",
      "Iteration 26805, loss = 4.02359913\n",
      "Iteration 26806, loss = 3.98066121\n",
      "Iteration 26807, loss = 4.25335832\n",
      "Iteration 26808, loss = 4.40024050\n",
      "Iteration 26809, loss = 3.99246006\n",
      "Iteration 26810, loss = 4.40219736\n",
      "Iteration 26811, loss = 4.25993313\n",
      "Iteration 26812, loss = 4.36686002\n",
      "Iteration 26813, loss = 3.75109576\n",
      "Iteration 26814, loss = 4.39776900\n",
      "Iteration 26815, loss = 4.43727065\n",
      "Iteration 26816, loss = 5.30416287\n",
      "Iteration 26817, loss = 4.98394095\n",
      "Iteration 26818, loss = 4.67066820\n",
      "Iteration 26819, loss = 3.77437680\n",
      "Iteration 26820, loss = 4.14789714\n",
      "Iteration 26821, loss = 3.76851577\n",
      "Iteration 26822, loss = 5.15030698\n",
      "Iteration 26823, loss = 6.02287272\n",
      "Iteration 26824, loss = 4.57861275\n",
      "Iteration 26825, loss = 3.71419656\n",
      "Iteration 26826, loss = 4.12018385\n",
      "Iteration 26827, loss = 4.59535300\n",
      "Iteration 26828, loss = 4.16317828\n",
      "Iteration 26829, loss = 4.41418080\n",
      "Iteration 26830, loss = 4.05586181\n",
      "Iteration 26831, loss = 3.85988350\n",
      "Iteration 26832, loss = 3.98672785\n",
      "Iteration 26833, loss = 4.23079203\n",
      "Iteration 26834, loss = 4.60737312\n",
      "Iteration 26835, loss = 4.18341285\n",
      "Iteration 26836, loss = 3.61557545\n",
      "Iteration 26837, loss = 3.88166442\n",
      "Iteration 26838, loss = 3.78537421\n",
      "Iteration 26839, loss = 4.68219000\n",
      "Iteration 26840, loss = 4.60968264\n",
      "Iteration 26841, loss = 4.23855413\n",
      "Iteration 26842, loss = 3.94345676\n",
      "Iteration 26843, loss = 4.22473991\n",
      "Iteration 26844, loss = 3.78326970\n",
      "Iteration 26845, loss = 4.05482014\n",
      "Iteration 26846, loss = 4.12221502\n",
      "Iteration 26847, loss = 3.95748811\n",
      "Iteration 26848, loss = 4.23500889\n",
      "Iteration 26849, loss = 3.77664755\n",
      "Iteration 26850, loss = 4.05658026\n",
      "Iteration 26851, loss = 3.68479284\n",
      "Iteration 26852, loss = 3.75104171\n",
      "Iteration 26853, loss = 3.60875469\n",
      "Iteration 26854, loss = 4.72324327\n",
      "Iteration 26855, loss = 4.91361403\n",
      "Iteration 26856, loss = 6.07164903\n",
      "Iteration 26857, loss = 6.19252050\n",
      "Iteration 26858, loss = 5.34746298\n",
      "Iteration 26859, loss = 5.30386437\n",
      "Iteration 26860, loss = 5.26442679\n",
      "Iteration 26861, loss = 4.27768910\n",
      "Iteration 26862, loss = 3.57284539\n",
      "Iteration 26863, loss = 3.94009533\n",
      "Iteration 26864, loss = 3.97051644\n",
      "Iteration 26865, loss = 3.60209141\n",
      "Iteration 26866, loss = 4.94791740\n",
      "Iteration 26867, loss = 4.99235153\n",
      "Iteration 26868, loss = 4.97540673\n",
      "Iteration 26869, loss = 4.39343394\n",
      "Iteration 26870, loss = 5.03603664\n",
      "Iteration 26871, loss = 5.17649661\n",
      "Iteration 26872, loss = 4.32190224\n",
      "Iteration 26873, loss = 4.12487729\n",
      "Iteration 26874, loss = 4.36010295\n",
      "Iteration 26875, loss = 4.22943389\n",
      "Iteration 26876, loss = 3.99964378\n",
      "Iteration 26877, loss = 3.73659879\n",
      "Iteration 26878, loss = 3.58121131\n",
      "Iteration 26879, loss = 3.67040384\n",
      "Iteration 26880, loss = 3.75385696\n",
      "Iteration 26881, loss = 3.96809996\n",
      "Iteration 26882, loss = 3.99266419\n",
      "Iteration 26883, loss = 3.92112782\n",
      "Iteration 26884, loss = 3.49164012\n",
      "Iteration 26885, loss = 3.47606757\n",
      "Iteration 26886, loss = 3.51445279\n",
      "Iteration 26887, loss = 3.57198655\n",
      "Iteration 26888, loss = 3.42814956\n",
      "Iteration 26889, loss = 3.53376950\n",
      "Iteration 26890, loss = 3.33493961\n",
      "Iteration 26891, loss = 3.52489727\n",
      "Iteration 26892, loss = 3.82409680\n",
      "Iteration 26893, loss = 3.72558544\n",
      "Iteration 26894, loss = 3.49192404\n",
      "Iteration 26895, loss = 3.50953948\n",
      "Iteration 26896, loss = 4.23070296\n",
      "Iteration 26897, loss = 3.92468163\n",
      "Iteration 26898, loss = 3.47539087\n",
      "Iteration 26899, loss = 3.57095443\n",
      "Iteration 26900, loss = 4.23339665\n",
      "Iteration 26901, loss = 4.96528189\n",
      "Iteration 26902, loss = 4.84380484\n",
      "Iteration 26903, loss = 4.47246227\n",
      "Iteration 26904, loss = 4.19392834\n",
      "Iteration 26905, loss = 4.52436470\n",
      "Iteration 26906, loss = 5.03540261\n",
      "Iteration 26907, loss = 5.11131718\n",
      "Iteration 26908, loss = 4.38897790\n",
      "Iteration 26909, loss = 3.89299116\n",
      "Iteration 26910, loss = 3.43110863\n",
      "Iteration 26911, loss = 3.68636589\n",
      "Iteration 26912, loss = 3.99015770\n",
      "Iteration 26913, loss = 4.56064226\n",
      "Iteration 26914, loss = 4.08903995\n",
      "Iteration 26915, loss = 3.90513910\n",
      "Iteration 26916, loss = 3.66378438\n",
      "Iteration 26917, loss = 3.50676957\n",
      "Iteration 26918, loss = 3.69647171\n",
      "Iteration 26919, loss = 3.16060653\n",
      "Iteration 26920, loss = 3.58682923\n",
      "Iteration 26921, loss = 3.20819968\n",
      "Iteration 26922, loss = 4.45298032\n",
      "Iteration 26923, loss = 4.95707908\n",
      "Iteration 26924, loss = 5.12961466\n",
      "Iteration 26925, loss = 5.71100359\n",
      "Iteration 26926, loss = 4.85241832\n",
      "Iteration 26927, loss = 4.18906154\n",
      "Iteration 26928, loss = 4.89064765\n",
      "Iteration 26929, loss = 3.92338330\n",
      "Iteration 26930, loss = 4.13792741\n",
      "Iteration 26931, loss = 4.24697854\n",
      "Iteration 26932, loss = 4.41672859\n",
      "Iteration 26933, loss = 4.29388002\n",
      "Iteration 26934, loss = 4.24418779\n",
      "Iteration 26935, loss = 3.90359987\n",
      "Iteration 26936, loss = 3.40594338\n",
      "Iteration 26937, loss = 3.78066677\n",
      "Iteration 26938, loss = 4.55173635\n",
      "Iteration 26939, loss = 5.41266035\n",
      "Iteration 26940, loss = 4.60307555\n",
      "Iteration 26941, loss = 4.58167021\n",
      "Iteration 26942, loss = 4.44141846\n",
      "Iteration 26943, loss = 4.51052116\n",
      "Iteration 26944, loss = 3.99613700\n",
      "Iteration 26945, loss = 4.08144835\n",
      "Iteration 26946, loss = 4.06243438\n",
      "Iteration 26947, loss = 5.25384640\n",
      "Iteration 26948, loss = 4.11922675\n",
      "Iteration 26949, loss = 4.41183894\n",
      "Iteration 26950, loss = 3.85136836\n",
      "Iteration 26951, loss = 3.89247232\n",
      "Iteration 26952, loss = 3.56769832\n",
      "Iteration 26953, loss = 3.80174807\n",
      "Iteration 26954, loss = 3.76524236\n",
      "Iteration 26955, loss = 3.95065601\n",
      "Iteration 26956, loss = 4.16012247\n",
      "Iteration 26957, loss = 5.22737854\n",
      "Iteration 26958, loss = 4.22075183\n",
      "Iteration 26959, loss = 4.00353998\n",
      "Iteration 26960, loss = 4.57172692\n",
      "Iteration 26961, loss = 3.62719413\n",
      "Iteration 26962, loss = 3.46344268\n",
      "Iteration 26963, loss = 3.45271106\n",
      "Iteration 26964, loss = 3.52401206\n",
      "Iteration 26965, loss = 3.82003339\n",
      "Iteration 26966, loss = 3.87398590\n",
      "Iteration 26967, loss = 3.57290844\n",
      "Iteration 26968, loss = 3.49127525\n",
      "Iteration 26969, loss = 3.64203017\n",
      "Iteration 26970, loss = 5.62381127\n",
      "Iteration 26971, loss = 7.44247171\n",
      "Iteration 26972, loss = 6.24948649\n",
      "Iteration 26973, loss = 5.54540718\n",
      "Iteration 26974, loss = 6.17647649\n",
      "Iteration 26975, loss = 4.58707087\n",
      "Iteration 26976, loss = 4.22344818\n",
      "Iteration 26977, loss = 3.59845803\n",
      "Iteration 26978, loss = 3.39104423\n",
      "Iteration 26979, loss = 4.17155366\n",
      "Iteration 26980, loss = 4.17095545\n",
      "Iteration 26981, loss = 3.84056834\n",
      "Iteration 26982, loss = 3.62942430\n",
      "Iteration 26983, loss = 4.86220872\n",
      "Iteration 26984, loss = 4.27862084\n",
      "Iteration 26985, loss = 3.85385728\n",
      "Iteration 26986, loss = 3.76034267\n",
      "Iteration 26987, loss = 3.63809152\n",
      "Iteration 26988, loss = 3.58528525\n",
      "Iteration 26989, loss = 3.74336556\n",
      "Iteration 26990, loss = 4.18982179\n",
      "Iteration 26991, loss = 4.16375226\n",
      "Iteration 26992, loss = 5.26885580\n",
      "Iteration 26993, loss = 3.88495693\n",
      "Iteration 26994, loss = 4.14002891\n",
      "Iteration 26995, loss = 4.29490667\n",
      "Iteration 26996, loss = 4.04587065\n",
      "Iteration 26997, loss = 3.83707550\n",
      "Iteration 26998, loss = 3.44867223\n",
      "Iteration 26999, loss = 3.69208453\n",
      "Iteration 27000, loss = 3.40824825\n",
      "Iteration 27001, loss = 3.36622718\n",
      "Iteration 27002, loss = 3.49211312\n",
      "Iteration 27003, loss = 3.41925378\n",
      "Iteration 27004, loss = 3.48163135\n",
      "Iteration 27005, loss = 4.92368297\n",
      "Iteration 27006, loss = 4.25567055\n",
      "Iteration 27007, loss = 3.94558869\n",
      "Iteration 27008, loss = 4.94718539\n",
      "Iteration 27009, loss = 5.61122187\n",
      "Iteration 27010, loss = 4.71978512\n",
      "Iteration 27011, loss = 5.35168369\n",
      "Iteration 27012, loss = 5.08285937\n",
      "Iteration 27013, loss = 5.51631028\n",
      "Iteration 27014, loss = 6.08300068\n",
      "Iteration 27015, loss = 6.83992368\n",
      "Iteration 27016, loss = 6.17933283\n",
      "Iteration 27017, loss = 6.30756594\n",
      "Iteration 27018, loss = 4.75211144\n",
      "Iteration 27019, loss = 3.95971761\n",
      "Iteration 27020, loss = 4.21174697\n",
      "Iteration 27021, loss = 4.25519081\n",
      "Iteration 27022, loss = 4.02886501\n",
      "Iteration 27023, loss = 4.29688486\n",
      "Iteration 27024, loss = 3.63287332\n",
      "Iteration 27025, loss = 3.35950183\n",
      "Iteration 27026, loss = 3.53145598\n",
      "Iteration 27027, loss = 3.30111916\n",
      "Iteration 27028, loss = 3.70189836\n",
      "Iteration 27029, loss = 4.41368148\n",
      "Iteration 27030, loss = 4.45483006\n",
      "Iteration 27031, loss = 4.80627203\n",
      "Iteration 27032, loss = 5.18217309\n",
      "Iteration 27033, loss = 6.22908946\n",
      "Iteration 27034, loss = 6.07307612\n",
      "Iteration 27035, loss = 4.76858546\n",
      "Iteration 27036, loss = 5.71904869\n",
      "Iteration 27037, loss = 5.07473275\n",
      "Iteration 27038, loss = 4.69640358\n",
      "Iteration 27039, loss = 4.25666854\n",
      "Iteration 27040, loss = 4.05305227\n",
      "Iteration 27041, loss = 4.96810554\n",
      "Iteration 27042, loss = 5.11623234\n",
      "Iteration 27043, loss = 4.60418975\n",
      "Iteration 27044, loss = 4.62523321\n",
      "Iteration 27045, loss = 4.21968241\n",
      "Iteration 27046, loss = 3.71462012\n",
      "Iteration 27047, loss = 3.53057569\n",
      "Iteration 27048, loss = 4.06336503\n",
      "Iteration 27049, loss = 3.78716206\n",
      "Iteration 27050, loss = 3.43717240\n",
      "Iteration 27051, loss = 4.21818748\n",
      "Iteration 27052, loss = 3.77037064\n",
      "Iteration 27053, loss = 3.38789167\n",
      "Iteration 27054, loss = 3.65316537\n",
      "Iteration 27055, loss = 4.03504968\n",
      "Iteration 27056, loss = 4.07160027\n",
      "Iteration 27057, loss = 5.05687690\n",
      "Iteration 27058, loss = 3.38227762\n",
      "Iteration 27059, loss = 3.71728432\n",
      "Iteration 27060, loss = 3.45454433\n",
      "Iteration 27061, loss = 3.44216251\n",
      "Iteration 27062, loss = 3.79145623\n",
      "Iteration 27063, loss = 3.65390205\n",
      "Iteration 27064, loss = 4.04665707\n",
      "Iteration 27065, loss = 3.58633673\n",
      "Iteration 27066, loss = 3.85219355\n",
      "Iteration 27067, loss = 3.78245331\n",
      "Iteration 27068, loss = 3.81553140\n",
      "Iteration 27069, loss = 3.63908132\n",
      "Iteration 27070, loss = 3.44869127\n",
      "Iteration 27071, loss = 3.83073079\n",
      "Iteration 27072, loss = 4.38313703\n",
      "Iteration 27073, loss = 4.45570687\n",
      "Iteration 27074, loss = 4.02756822\n",
      "Iteration 27075, loss = 3.82980551\n",
      "Iteration 27076, loss = 3.72450237\n",
      "Iteration 27077, loss = 3.37327999\n",
      "Iteration 27078, loss = 3.33722336\n",
      "Iteration 27079, loss = 3.39908316\n",
      "Iteration 27080, loss = 3.30611772\n",
      "Iteration 27081, loss = 3.40009318\n",
      "Iteration 27082, loss = 3.58418437\n",
      "Iteration 27083, loss = 3.63599820\n",
      "Iteration 27084, loss = 3.38823339\n",
      "Iteration 27085, loss = 3.84264533\n",
      "Iteration 27086, loss = 4.25566762\n",
      "Iteration 27087, loss = 3.75631613\n",
      "Iteration 27088, loss = 3.89520956\n",
      "Iteration 27089, loss = 3.72032810\n",
      "Iteration 27090, loss = 4.28469614\n",
      "Iteration 27091, loss = 4.02473142\n",
      "Iteration 27092, loss = 3.74622277\n",
      "Iteration 27093, loss = 3.79933038\n",
      "Iteration 27094, loss = 4.08440140\n",
      "Iteration 27095, loss = 4.14580401\n",
      "Iteration 27096, loss = 3.96781375\n",
      "Iteration 27097, loss = 4.35484506\n",
      "Iteration 27098, loss = 4.67484785\n",
      "Iteration 27099, loss = 3.73607476\n",
      "Iteration 27100, loss = 3.96023878\n",
      "Iteration 27101, loss = 3.40305584\n",
      "Iteration 27102, loss = 3.54440757\n",
      "Iteration 27103, loss = 3.79065348\n",
      "Iteration 27104, loss = 3.70558731\n",
      "Iteration 27105, loss = 4.03696799\n",
      "Iteration 27106, loss = 3.59755073\n",
      "Iteration 27107, loss = 4.04165233\n",
      "Iteration 27108, loss = 3.53827816\n",
      "Iteration 27109, loss = 3.62803531\n",
      "Iteration 27110, loss = 3.41222449\n",
      "Iteration 27111, loss = 3.54762468\n",
      "Iteration 27112, loss = 4.01565032\n",
      "Iteration 27113, loss = 4.56517587\n",
      "Iteration 27114, loss = 3.76189020\n",
      "Iteration 27115, loss = 3.56296415\n",
      "Iteration 27116, loss = 3.43410459\n",
      "Iteration 27117, loss = 3.42133521\n",
      "Iteration 27118, loss = 3.52066852\n",
      "Iteration 27119, loss = 3.46553510\n",
      "Iteration 27120, loss = 3.70873156\n",
      "Iteration 27121, loss = 3.66877067\n",
      "Iteration 27122, loss = 3.74427370\n",
      "Iteration 27123, loss = 4.23779496\n",
      "Iteration 27124, loss = 3.30856472\n",
      "Iteration 27125, loss = 3.80642134\n",
      "Iteration 27126, loss = 3.77771208\n",
      "Iteration 27127, loss = 3.56549533\n",
      "Iteration 27128, loss = 3.58524043\n",
      "Iteration 27129, loss = 3.14565948\n",
      "Iteration 27130, loss = 3.68622048\n",
      "Iteration 27131, loss = 3.92636275\n",
      "Iteration 27132, loss = 3.86401799\n",
      "Iteration 27133, loss = 4.46640758\n",
      "Iteration 27134, loss = 5.56707194\n",
      "Iteration 27135, loss = 4.60728849\n",
      "Iteration 27136, loss = 3.89299298\n",
      "Iteration 27137, loss = 3.55036194\n",
      "Iteration 27138, loss = 3.91548996\n",
      "Iteration 27139, loss = 3.89136588\n",
      "Iteration 27140, loss = 4.04045539\n",
      "Iteration 27141, loss = 3.83166930\n",
      "Iteration 27142, loss = 3.88683638\n",
      "Iteration 27143, loss = 4.09505808\n",
      "Iteration 27144, loss = 4.26962627\n",
      "Iteration 27145, loss = 3.96752489\n",
      "Iteration 27146, loss = 3.75189566\n",
      "Iteration 27147, loss = 3.95707376\n",
      "Iteration 27148, loss = 3.92881894\n",
      "Iteration 27149, loss = 4.34164143\n",
      "Iteration 27150, loss = 4.11485258\n",
      "Iteration 27151, loss = 4.06119374\n",
      "Iteration 27152, loss = 4.03479133\n",
      "Iteration 27153, loss = 3.51336753\n",
      "Iteration 27154, loss = 3.87554933\n",
      "Iteration 27155, loss = 3.63751137\n",
      "Iteration 27156, loss = 3.81516586\n",
      "Iteration 27157, loss = 4.01782117\n",
      "Iteration 27158, loss = 4.30598543\n",
      "Iteration 27159, loss = 4.07582667\n",
      "Iteration 27160, loss = 4.06633354\n",
      "Iteration 27161, loss = 4.48761445\n",
      "Iteration 27162, loss = 4.43464444\n",
      "Iteration 27163, loss = 4.26015538\n",
      "Iteration 27164, loss = 4.09074143\n",
      "Iteration 27165, loss = 5.45048739\n",
      "Iteration 27166, loss = 4.76633486\n",
      "Iteration 27167, loss = 4.45216842\n",
      "Iteration 27168, loss = 3.82772104\n",
      "Iteration 27169, loss = 4.07225110\n",
      "Iteration 27170, loss = 5.20683263\n",
      "Iteration 27171, loss = 3.86202245\n",
      "Iteration 27172, loss = 4.15225659\n",
      "Iteration 27173, loss = 3.79807878\n",
      "Iteration 27174, loss = 3.84538443\n",
      "Iteration 27175, loss = 3.76223879\n",
      "Iteration 27176, loss = 3.57886229\n",
      "Iteration 27177, loss = 3.53281722\n",
      "Iteration 27178, loss = 3.59386821\n",
      "Iteration 27179, loss = 4.05539904\n",
      "Iteration 27180, loss = 3.74696841\n",
      "Iteration 27181, loss = 3.67721072\n",
      "Iteration 27182, loss = 3.71622786\n",
      "Iteration 27183, loss = 3.63729334\n",
      "Iteration 27184, loss = 3.59522576\n",
      "Iteration 27185, loss = 3.59351690\n",
      "Iteration 27186, loss = 3.73649360\n",
      "Iteration 27187, loss = 3.61906512\n",
      "Iteration 27188, loss = 3.89703861\n",
      "Iteration 27189, loss = 3.95657310\n",
      "Iteration 27190, loss = 3.78219604\n",
      "Iteration 27191, loss = 4.14863005\n",
      "Iteration 27192, loss = 4.43088816\n",
      "Iteration 27193, loss = 3.88523312\n",
      "Iteration 27194, loss = 3.60871853\n",
      "Iteration 27195, loss = 3.57308035\n",
      "Iteration 27196, loss = 3.54572466\n",
      "Iteration 27197, loss = 4.14690985\n",
      "Iteration 27198, loss = 4.54673409\n",
      "Iteration 27199, loss = 4.85662308\n",
      "Iteration 27200, loss = 3.96001369\n",
      "Iteration 27201, loss = 5.09724889\n",
      "Iteration 27202, loss = 3.82308295\n",
      "Iteration 27203, loss = 3.77494257\n",
      "Iteration 27204, loss = 3.71121130\n",
      "Iteration 27205, loss = 3.98099803\n",
      "Iteration 27206, loss = 3.71444424\n",
      "Iteration 27207, loss = 4.55597715\n",
      "Iteration 27208, loss = 4.65693143\n",
      "Iteration 27209, loss = 4.15833362\n",
      "Iteration 27210, loss = 4.24379648\n",
      "Iteration 27211, loss = 4.85970007\n",
      "Iteration 27212, loss = 4.39611786\n",
      "Iteration 27213, loss = 4.53186149\n",
      "Iteration 27214, loss = 3.98383985\n",
      "Iteration 27215, loss = 3.68734072\n",
      "Iteration 27216, loss = 3.43465614\n",
      "Iteration 27217, loss = 3.63052500\n",
      "Iteration 27218, loss = 3.77267930\n",
      "Iteration 27219, loss = 3.95235005\n",
      "Iteration 27220, loss = 3.62453496\n",
      "Iteration 27221, loss = 3.50061147\n",
      "Iteration 27222, loss = 5.16571059\n",
      "Iteration 27223, loss = 3.73710063\n",
      "Iteration 27224, loss = 3.66581393\n",
      "Iteration 27225, loss = 3.79516778\n",
      "Iteration 27226, loss = 3.58617785\n",
      "Iteration 27227, loss = 3.67872807\n",
      "Iteration 27228, loss = 3.53294635\n",
      "Iteration 27229, loss = 3.40747274\n",
      "Iteration 27230, loss = 3.82806590\n",
      "Iteration 27231, loss = 4.06679672\n",
      "Iteration 27232, loss = 3.69383529\n",
      "Iteration 27233, loss = 3.45983940\n",
      "Iteration 27234, loss = 3.53678463\n",
      "Iteration 27235, loss = 3.68922042\n",
      "Iteration 27236, loss = 4.32917446\n",
      "Iteration 27237, loss = 4.03007123\n",
      "Iteration 27238, loss = 4.87119248\n",
      "Iteration 27239, loss = 4.06817799\n",
      "Iteration 27240, loss = 3.98354301\n",
      "Iteration 27241, loss = 3.93459745\n",
      "Iteration 27242, loss = 3.41859771\n",
      "Iteration 27243, loss = 3.51445465\n",
      "Iteration 27244, loss = 4.11969461\n",
      "Iteration 27245, loss = 4.12608201\n",
      "Iteration 27246, loss = 3.90822666\n",
      "Iteration 27247, loss = 3.71347077\n",
      "Iteration 27248, loss = 3.97910546\n",
      "Iteration 27249, loss = 4.49602060\n",
      "Iteration 27250, loss = 5.00779049\n",
      "Iteration 27251, loss = 4.38179257\n",
      "Iteration 27252, loss = 4.52511797\n",
      "Iteration 27253, loss = 5.76470907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 27254, loss = 6.62760422\n",
      "Iteration 27255, loss = 6.13906215\n",
      "Iteration 27256, loss = 4.22464798\n",
      "Iteration 27257, loss = 4.33937164\n",
      "Iteration 27258, loss = 3.69453425\n",
      "Iteration 27259, loss = 4.71858575\n",
      "Iteration 27260, loss = 4.82346049\n",
      "Iteration 27261, loss = 4.20467602\n",
      "Iteration 27262, loss = 4.12695636\n",
      "Iteration 27263, loss = 3.83431111\n",
      "Iteration 27264, loss = 4.06062155\n",
      "Iteration 27265, loss = 4.16046357\n",
      "Iteration 27266, loss = 4.02607314\n",
      "Iteration 27267, loss = 3.62430711\n",
      "Iteration 27268, loss = 3.64988130\n",
      "Iteration 27269, loss = 4.01156324\n",
      "Iteration 27270, loss = 3.70660199\n",
      "Iteration 27271, loss = 3.82405173\n",
      "Iteration 27272, loss = 3.59049477\n",
      "Iteration 27273, loss = 3.62242496\n",
      "Iteration 27274, loss = 4.11326680\n",
      "Iteration 27275, loss = 4.20850936\n",
      "Iteration 27276, loss = 3.35664416\n",
      "Iteration 27277, loss = 3.99681990\n",
      "Iteration 27278, loss = 3.65111726\n",
      "Iteration 27279, loss = 3.90482257\n",
      "Iteration 27280, loss = 3.45345811\n",
      "Iteration 27281, loss = 3.27355937\n",
      "Iteration 27282, loss = 3.18393775\n",
      "Iteration 27283, loss = 3.73396652\n",
      "Iteration 27284, loss = 3.40527039\n",
      "Iteration 27285, loss = 4.01122529\n",
      "Iteration 27286, loss = 3.96134966\n",
      "Iteration 27287, loss = 4.61025340\n",
      "Iteration 27288, loss = 3.88571750\n",
      "Iteration 27289, loss = 3.33351650\n",
      "Iteration 27290, loss = 3.24844629\n",
      "Iteration 27291, loss = 3.44065354\n",
      "Iteration 27292, loss = 3.66068550\n",
      "Iteration 27293, loss = 3.62087945\n",
      "Iteration 27294, loss = 3.82862640\n",
      "Iteration 27295, loss = 3.55833904\n",
      "Iteration 27296, loss = 3.56109192\n",
      "Iteration 27297, loss = 3.56957994\n",
      "Iteration 27298, loss = 4.44204372\n",
      "Iteration 27299, loss = 3.71731374\n",
      "Iteration 27300, loss = 3.48311784\n",
      "Iteration 27301, loss = 3.40854853\n",
      "Iteration 27302, loss = 3.46456808\n",
      "Iteration 27303, loss = 3.61542039\n",
      "Iteration 27304, loss = 3.77029445\n",
      "Iteration 27305, loss = 4.05405127\n",
      "Iteration 27306, loss = 4.18519725\n",
      "Iteration 27307, loss = 3.86546948\n",
      "Iteration 27308, loss = 4.15366086\n",
      "Iteration 27309, loss = 3.94973906\n",
      "Iteration 27310, loss = 3.75362968\n",
      "Iteration 27311, loss = 4.04478301\n",
      "Iteration 27312, loss = 3.75205570\n",
      "Iteration 27313, loss = 4.21170330\n",
      "Iteration 27314, loss = 3.81534136\n",
      "Iteration 27315, loss = 3.87233376\n",
      "Iteration 27316, loss = 3.96192041\n",
      "Iteration 27317, loss = 4.05811862\n",
      "Iteration 27318, loss = 3.57145155\n",
      "Iteration 27319, loss = 3.44995530\n",
      "Iteration 27320, loss = 3.71678747\n",
      "Iteration 27321, loss = 3.68189319\n",
      "Iteration 27322, loss = 3.37766930\n",
      "Iteration 27323, loss = 3.67675827\n",
      "Iteration 27324, loss = 4.48590435\n",
      "Iteration 27325, loss = 3.97965859\n",
      "Iteration 27326, loss = 4.21719521\n",
      "Iteration 27327, loss = 3.93731890\n",
      "Iteration 27328, loss = 3.69793762\n",
      "Iteration 27329, loss = 3.39457783\n",
      "Iteration 27330, loss = 3.78001592\n",
      "Iteration 27331, loss = 3.44030515\n",
      "Iteration 27332, loss = 4.17090211\n",
      "Iteration 27333, loss = 3.44410285\n",
      "Iteration 27334, loss = 3.53161630\n",
      "Iteration 27335, loss = 3.30725357\n",
      "Iteration 27336, loss = 3.85074908\n",
      "Iteration 27337, loss = 3.40461747\n",
      "Iteration 27338, loss = 3.27122197\n",
      "Iteration 27339, loss = 3.90552624\n",
      "Iteration 27340, loss = 3.78492732\n",
      "Iteration 27341, loss = 3.84301874\n",
      "Iteration 27342, loss = 3.60056889\n",
      "Iteration 27343, loss = 3.54743857\n",
      "Iteration 27344, loss = 3.37024345\n",
      "Iteration 27345, loss = 3.34391125\n",
      "Iteration 27346, loss = 3.57904697\n",
      "Iteration 27347, loss = 3.44562442\n",
      "Iteration 27348, loss = 3.54000969\n",
      "Iteration 27349, loss = 3.59252356\n",
      "Iteration 27350, loss = 3.74902142\n",
      "Iteration 27351, loss = 4.41703366\n",
      "Iteration 27352, loss = 4.42807010\n",
      "Iteration 27353, loss = 3.42303238\n",
      "Iteration 27354, loss = 3.41327285\n",
      "Iteration 27355, loss = 3.61493436\n",
      "Iteration 27356, loss = 4.49166419\n",
      "Iteration 27357, loss = 4.28784695\n",
      "Iteration 27358, loss = 3.63107506\n",
      "Iteration 27359, loss = 3.84195706\n",
      "Iteration 27360, loss = 3.68813894\n",
      "Iteration 27361, loss = 4.02106030\n",
      "Iteration 27362, loss = 4.50709799\n",
      "Iteration 27363, loss = 5.03136539\n",
      "Iteration 27364, loss = 4.31726683\n",
      "Iteration 27365, loss = 3.85515586\n",
      "Iteration 27366, loss = 3.53183641\n",
      "Iteration 27367, loss = 3.68951534\n",
      "Iteration 27368, loss = 3.53185264\n",
      "Iteration 27369, loss = 3.55995032\n",
      "Iteration 27370, loss = 3.56084775\n",
      "Iteration 27371, loss = 3.79213207\n",
      "Iteration 27372, loss = 3.99135970\n",
      "Iteration 27373, loss = 4.33515736\n",
      "Iteration 27374, loss = 4.82341814\n",
      "Iteration 27375, loss = 5.47025314\n",
      "Iteration 27376, loss = 4.67410384\n",
      "Iteration 27377, loss = 4.17419160\n",
      "Iteration 27378, loss = 4.27489662\n",
      "Iteration 27379, loss = 4.52906378\n",
      "Iteration 27380, loss = 4.06273450\n",
      "Iteration 27381, loss = 5.77510041\n",
      "Iteration 27382, loss = 4.49435851\n",
      "Iteration 27383, loss = 3.40388768\n",
      "Iteration 27384, loss = 3.69457801\n",
      "Iteration 27385, loss = 3.55902678\n",
      "Iteration 27386, loss = 3.89893090\n",
      "Iteration 27387, loss = 4.22996597\n",
      "Iteration 27388, loss = 3.60953302\n",
      "Iteration 27389, loss = 3.59015121\n",
      "Iteration 27390, loss = 3.41115760\n",
      "Iteration 27391, loss = 3.97575751\n",
      "Iteration 27392, loss = 3.86178206\n",
      "Iteration 27393, loss = 5.04635688\n",
      "Iteration 27394, loss = 4.64638077\n",
      "Iteration 27395, loss = 4.09224971\n",
      "Iteration 27396, loss = 4.01379471\n",
      "Iteration 27397, loss = 3.96163343\n",
      "Iteration 27398, loss = 4.39816740\n",
      "Iteration 27399, loss = 5.37286465\n",
      "Iteration 27400, loss = 3.81316699\n",
      "Iteration 27401, loss = 3.53986994\n",
      "Iteration 27402, loss = 3.98929666\n",
      "Iteration 27403, loss = 3.73492907\n",
      "Iteration 27404, loss = 3.64183339\n",
      "Iteration 27405, loss = 3.83369383\n",
      "Iteration 27406, loss = 3.37017062\n",
      "Iteration 27407, loss = 3.33774348\n",
      "Iteration 27408, loss = 3.44924210\n",
      "Iteration 27409, loss = 3.80668061\n",
      "Iteration 27410, loss = 3.40212817\n",
      "Iteration 27411, loss = 3.63532541\n",
      "Iteration 27412, loss = 3.30013351\n",
      "Iteration 27413, loss = 3.32806778\n",
      "Iteration 27414, loss = 4.12536968\n",
      "Iteration 27415, loss = 3.67227264\n",
      "Iteration 27416, loss = 3.89996079\n",
      "Iteration 27417, loss = 4.29173250\n",
      "Iteration 27418, loss = 4.09792277\n",
      "Iteration 27419, loss = 5.58416408\n",
      "Iteration 27420, loss = 4.55660315\n",
      "Iteration 27421, loss = 3.73594156\n",
      "Iteration 27422, loss = 3.51063347\n",
      "Iteration 27423, loss = 3.98475530\n",
      "Iteration 27424, loss = 3.77930120\n",
      "Iteration 27425, loss = 4.54840978\n",
      "Iteration 27426, loss = 4.86440493\n",
      "Iteration 27427, loss = 4.51583398\n",
      "Iteration 27428, loss = 3.66507752\n",
      "Iteration 27429, loss = 3.30252214\n",
      "Iteration 27430, loss = 3.50861401\n",
      "Iteration 27431, loss = 3.56141716\n",
      "Iteration 27432, loss = 3.99131359\n",
      "Iteration 27433, loss = 3.51231921\n",
      "Iteration 27434, loss = 3.46232725\n",
      "Iteration 27435, loss = 3.35390776\n",
      "Iteration 27436, loss = 3.66532571\n",
      "Iteration 27437, loss = 3.96053503\n",
      "Iteration 27438, loss = 4.13370290\n",
      "Iteration 27439, loss = 4.21992881\n",
      "Iteration 27440, loss = 3.84552257\n",
      "Iteration 27441, loss = 3.41972905\n",
      "Iteration 27442, loss = 3.31824263\n",
      "Iteration 27443, loss = 3.46612679\n",
      "Iteration 27444, loss = 3.86921167\n",
      "Iteration 27445, loss = 3.71209128\n",
      "Iteration 27446, loss = 4.33287030\n",
      "Iteration 27447, loss = 3.60388912\n",
      "Iteration 27448, loss = 3.93591983\n",
      "Iteration 27449, loss = 4.65830127\n",
      "Iteration 27450, loss = 4.90202619\n",
      "Iteration 27451, loss = 4.49378749\n",
      "Iteration 27452, loss = 3.63022475\n",
      "Iteration 27453, loss = 3.89488704\n",
      "Iteration 27454, loss = 3.84765317\n",
      "Iteration 27455, loss = 3.85667865\n",
      "Iteration 27456, loss = 3.66188832\n",
      "Iteration 27457, loss = 3.54870228\n",
      "Iteration 27458, loss = 3.61304946\n",
      "Iteration 27459, loss = 3.89989577\n",
      "Iteration 27460, loss = 4.37340368\n",
      "Iteration 27461, loss = 3.39727956\n",
      "Iteration 27462, loss = 3.25846295\n",
      "Iteration 27463, loss = 3.21263568\n",
      "Iteration 27464, loss = 3.61040445\n",
      "Iteration 27465, loss = 3.36746608\n",
      "Iteration 27466, loss = 3.36111762\n",
      "Iteration 27467, loss = 3.52723427\n",
      "Iteration 27468, loss = 3.26076476\n",
      "Iteration 27469, loss = 3.78545754\n",
      "Iteration 27470, loss = 4.07417307\n",
      "Iteration 27471, loss = 4.18123346\n",
      "Iteration 27472, loss = 4.47169612\n",
      "Iteration 27473, loss = 4.14844773\n",
      "Iteration 27474, loss = 4.12209313\n",
      "Iteration 27475, loss = 4.71045015\n",
      "Iteration 27476, loss = 5.16147929\n",
      "Iteration 27477, loss = 5.65249134\n",
      "Iteration 27478, loss = 6.10585540\n",
      "Iteration 27479, loss = 5.91824067\n",
      "Iteration 27480, loss = 5.20335612\n",
      "Iteration 27481, loss = 4.16493715\n",
      "Iteration 27482, loss = 3.72324003\n",
      "Iteration 27483, loss = 3.66876313\n",
      "Iteration 27484, loss = 4.45947797\n",
      "Iteration 27485, loss = 4.22294752\n",
      "Iteration 27486, loss = 4.44245834\n",
      "Iteration 27487, loss = 3.76665358\n",
      "Iteration 27488, loss = 3.84994107\n",
      "Iteration 27489, loss = 3.72844968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 27490, loss = 3.50431786\n",
      "Iteration 27491, loss = 3.96617465\n",
      "Iteration 27492, loss = 4.51283663\n",
      "Iteration 27493, loss = 5.33646108\n",
      "Iteration 27494, loss = 5.03148292\n",
      "Iteration 27495, loss = 4.76672126\n",
      "Iteration 27496, loss = 4.96003865\n",
      "Iteration 27497, loss = 4.59330330\n",
      "Iteration 27498, loss = 3.80865651\n",
      "Iteration 27499, loss = 3.73239988\n",
      "Iteration 27500, loss = 3.71159746\n",
      "Iteration 27501, loss = 3.61594248\n",
      "Iteration 27502, loss = 5.48869404\n",
      "Iteration 27503, loss = 6.55556681\n",
      "Iteration 27504, loss = 5.32180249\n",
      "Iteration 27505, loss = 6.47389402\n",
      "Iteration 27506, loss = 5.97204815\n",
      "Iteration 27507, loss = 5.37794576\n",
      "Iteration 27508, loss = 5.98664046\n",
      "Iteration 27509, loss = 3.87019647\n",
      "Iteration 27510, loss = 4.28435057\n",
      "Iteration 27511, loss = 4.51672140\n",
      "Iteration 27512, loss = 4.26157169\n",
      "Iteration 27513, loss = 4.15304557\n",
      "Iteration 27514, loss = 3.22452475\n",
      "Iteration 27515, loss = 4.12203637\n",
      "Iteration 27516, loss = 7.08510574\n",
      "Iteration 27517, loss = 6.02812050\n",
      "Iteration 27518, loss = 5.76773528\n",
      "Iteration 27519, loss = 5.64548841\n",
      "Iteration 27520, loss = 5.42569351\n",
      "Iteration 27521, loss = 5.56493659\n",
      "Iteration 27522, loss = 5.87028078\n",
      "Iteration 27523, loss = 5.43894732\n",
      "Iteration 27524, loss = 5.02117115\n",
      "Iteration 27525, loss = 3.63958019\n",
      "Iteration 27526, loss = 4.84498579\n",
      "Iteration 27527, loss = 4.49570724\n",
      "Iteration 27528, loss = 4.85164405\n",
      "Iteration 27529, loss = 4.98654490\n",
      "Iteration 27530, loss = 5.17498673\n",
      "Iteration 27531, loss = 4.22399769\n",
      "Iteration 27532, loss = 4.23442906\n",
      "Iteration 27533, loss = 4.11639885\n",
      "Iteration 27534, loss = 3.43942407\n",
      "Iteration 27535, loss = 4.16388914\n",
      "Iteration 27536, loss = 4.41274068\n",
      "Iteration 27537, loss = 3.63009960\n",
      "Iteration 27538, loss = 3.31724344\n",
      "Iteration 27539, loss = 3.36366469\n",
      "Iteration 27540, loss = 3.23115662\n",
      "Iteration 27541, loss = 3.27589342\n",
      "Iteration 27542, loss = 3.42090013\n",
      "Iteration 27543, loss = 3.63522253\n",
      "Iteration 27544, loss = 4.00832412\n",
      "Iteration 27545, loss = 3.74596176\n",
      "Iteration 27546, loss = 4.11166563\n",
      "Iteration 27547, loss = 3.53603606\n",
      "Iteration 27548, loss = 3.71507392\n",
      "Iteration 27549, loss = 3.53621015\n",
      "Iteration 27550, loss = 3.79558725\n",
      "Iteration 27551, loss = 4.12645200\n",
      "Iteration 27552, loss = 3.54312044\n",
      "Iteration 27553, loss = 3.72518461\n",
      "Iteration 27554, loss = 3.70118886\n",
      "Iteration 27555, loss = 3.98559422\n",
      "Iteration 27556, loss = 4.37138144\n",
      "Iteration 27557, loss = 5.00339547\n",
      "Iteration 27558, loss = 4.06357042\n",
      "Iteration 27559, loss = 4.01838939\n",
      "Iteration 27560, loss = 3.49872252\n",
      "Iteration 27561, loss = 3.56775107\n",
      "Iteration 27562, loss = 3.86081616\n",
      "Iteration 27563, loss = 3.86650811\n",
      "Iteration 27564, loss = 3.54062429\n",
      "Iteration 27565, loss = 3.51782584\n",
      "Iteration 27566, loss = 3.87140691\n",
      "Iteration 27567, loss = 3.55320009\n",
      "Iteration 27568, loss = 3.53898657\n",
      "Iteration 27569, loss = 3.62105185\n",
      "Iteration 27570, loss = 3.27081240\n",
      "Iteration 27571, loss = 3.38033584\n",
      "Iteration 27572, loss = 3.48711531\n",
      "Iteration 27573, loss = 3.29605424\n",
      "Iteration 27574, loss = 3.60746556\n",
      "Iteration 27575, loss = 3.40542900\n",
      "Iteration 27576, loss = 3.47155363\n",
      "Iteration 27577, loss = 3.75626620\n",
      "Iteration 27578, loss = 3.94822247\n",
      "Iteration 27579, loss = 4.18075423\n",
      "Iteration 27580, loss = 3.89936380\n",
      "Iteration 27581, loss = 3.51985372\n",
      "Iteration 27582, loss = 3.56979160\n",
      "Iteration 27583, loss = 3.76755398\n",
      "Iteration 27584, loss = 3.35594321\n",
      "Iteration 27585, loss = 3.90867810\n",
      "Iteration 27586, loss = 4.26411718\n",
      "Iteration 27587, loss = 3.97170434\n",
      "Iteration 27588, loss = 3.92242058\n",
      "Iteration 27589, loss = 4.15359249\n",
      "Iteration 27590, loss = 3.65916634\n",
      "Iteration 27591, loss = 3.59851666\n",
      "Iteration 27592, loss = 3.45185031\n",
      "Iteration 27593, loss = 3.60427118\n",
      "Iteration 27594, loss = 3.77680174\n",
      "Iteration 27595, loss = 4.11345493\n",
      "Iteration 27596, loss = 5.75949531\n",
      "Iteration 27597, loss = 5.64916529\n",
      "Iteration 27598, loss = 3.64016718\n",
      "Iteration 27599, loss = 4.68931761\n",
      "Iteration 27600, loss = 4.53489047\n",
      "Iteration 27601, loss = 4.56902801\n",
      "Iteration 27602, loss = 3.49625793\n",
      "Iteration 27603, loss = 3.71274902\n",
      "Iteration 27604, loss = 3.77629530\n",
      "Iteration 27605, loss = 3.73528348\n",
      "Iteration 27606, loss = 3.47624925\n",
      "Iteration 27607, loss = 4.28404521\n",
      "Iteration 27608, loss = 4.52102982\n",
      "Iteration 27609, loss = 4.01487109\n",
      "Iteration 27610, loss = 3.99627583\n",
      "Iteration 27611, loss = 4.00226738\n",
      "Iteration 27612, loss = 4.18785384\n",
      "Iteration 27613, loss = 3.43696357\n",
      "Iteration 27614, loss = 3.48060514\n",
      "Iteration 27615, loss = 3.24699840\n",
      "Iteration 27616, loss = 3.20026700\n",
      "Iteration 27617, loss = 3.72744223\n",
      "Iteration 27618, loss = 4.01164047\n",
      "Iteration 27619, loss = 4.54763276\n",
      "Iteration 27620, loss = 4.46989808\n",
      "Iteration 27621, loss = 3.56726601\n",
      "Iteration 27622, loss = 3.63246252\n",
      "Iteration 27623, loss = 3.38488017\n",
      "Iteration 27624, loss = 3.45872917\n",
      "Iteration 27625, loss = 4.26046497\n",
      "Iteration 27626, loss = 3.86369014\n",
      "Iteration 27627, loss = 4.48048789\n",
      "Iteration 27628, loss = 5.18808083\n",
      "Iteration 27629, loss = 3.70596479\n",
      "Iteration 27630, loss = 3.79964226\n",
      "Iteration 27631, loss = 3.81770654\n",
      "Iteration 27632, loss = 3.89014791\n",
      "Iteration 27633, loss = 3.53710348\n",
      "Iteration 27634, loss = 4.12368701\n",
      "Iteration 27635, loss = 4.87567415\n",
      "Iteration 27636, loss = 5.35156769\n",
      "Iteration 27637, loss = 4.01109822\n",
      "Iteration 27638, loss = 4.43552093\n",
      "Iteration 27639, loss = 4.28289638\n",
      "Iteration 27640, loss = 4.85962095\n",
      "Iteration 27641, loss = 4.55707373\n",
      "Iteration 27642, loss = 4.44162084\n",
      "Iteration 27643, loss = 3.91920222\n",
      "Iteration 27644, loss = 4.11893699\n",
      "Iteration 27645, loss = 3.91929352\n",
      "Iteration 27646, loss = 3.56342927\n",
      "Iteration 27647, loss = 3.36307636\n",
      "Iteration 27648, loss = 3.71761789\n",
      "Iteration 27649, loss = 3.64806934\n",
      "Iteration 27650, loss = 3.16443249\n",
      "Iteration 27651, loss = 3.28617645\n",
      "Iteration 27652, loss = 3.31209766\n",
      "Iteration 27653, loss = 3.42023084\n",
      "Iteration 27654, loss = 3.34414615\n",
      "Iteration 27655, loss = 3.75860768\n",
      "Iteration 27656, loss = 3.57602915\n",
      "Iteration 27657, loss = 3.66626723\n",
      "Iteration 27658, loss = 3.74829546\n",
      "Iteration 27659, loss = 3.47077719\n",
      "Iteration 27660, loss = 3.83525796\n",
      "Iteration 27661, loss = 4.34060858\n",
      "Iteration 27662, loss = 4.80381746\n",
      "Iteration 27663, loss = 3.91360325\n",
      "Iteration 27664, loss = 3.73852955\n",
      "Iteration 27665, loss = 4.30047113\n",
      "Iteration 27666, loss = 3.70178988\n",
      "Iteration 27667, loss = 3.90690270\n",
      "Iteration 27668, loss = 3.94106245\n",
      "Iteration 27669, loss = 4.13238693\n",
      "Iteration 27670, loss = 3.78446833\n",
      "Iteration 27671, loss = 4.60992717\n",
      "Iteration 27672, loss = 3.58431165\n",
      "Iteration 27673, loss = 3.93279720\n",
      "Iteration 27674, loss = 3.95246706\n",
      "Iteration 27675, loss = 3.64447785\n",
      "Iteration 27676, loss = 3.65350193\n",
      "Iteration 27677, loss = 3.36326490\n",
      "Iteration 27678, loss = 4.15479996\n",
      "Iteration 27679, loss = 4.13160183\n",
      "Iteration 27680, loss = 4.20320544\n",
      "Iteration 27681, loss = 3.59166014\n",
      "Iteration 27682, loss = 3.61884866\n",
      "Iteration 27683, loss = 3.56128422\n",
      "Iteration 27684, loss = 3.34022486\n",
      "Iteration 27685, loss = 3.24879887\n",
      "Iteration 27686, loss = 3.46208873\n",
      "Iteration 27687, loss = 4.18082628\n",
      "Iteration 27688, loss = 3.79253660\n",
      "Iteration 27689, loss = 3.82730418\n",
      "Iteration 27690, loss = 3.80985877\n",
      "Iteration 27691, loss = 4.74954652\n",
      "Iteration 27692, loss = 4.40092225\n",
      "Iteration 27693, loss = 4.01778539\n",
      "Iteration 27694, loss = 4.33773934\n",
      "Iteration 27695, loss = 3.62429371\n",
      "Iteration 27696, loss = 3.42314432\n",
      "Iteration 27697, loss = 3.46721159\n",
      "Iteration 27698, loss = 3.69700293\n",
      "Iteration 27699, loss = 3.37419963\n",
      "Iteration 27700, loss = 3.62606077\n",
      "Iteration 27701, loss = 3.62382020\n",
      "Iteration 27702, loss = 3.21364633\n",
      "Iteration 27703, loss = 3.46829397\n",
      "Iteration 27704, loss = 3.62282019\n",
      "Iteration 27705, loss = 3.20306477\n",
      "Iteration 27706, loss = 3.35621405\n",
      "Iteration 27707, loss = 3.98786135\n",
      "Iteration 27708, loss = 3.82540648\n",
      "Iteration 27709, loss = 3.75291888\n",
      "Iteration 27710, loss = 3.47742548\n",
      "Iteration 27711, loss = 3.63451154\n",
      "Iteration 27712, loss = 3.53486437\n",
      "Iteration 27713, loss = 3.28269901\n",
      "Iteration 27714, loss = 3.19661624\n",
      "Iteration 27715, loss = 3.08915220\n",
      "Iteration 27716, loss = 3.79790182\n",
      "Iteration 27717, loss = 4.51305313\n",
      "Iteration 27718, loss = 5.19965919\n",
      "Iteration 27719, loss = 5.56276277\n",
      "Iteration 27720, loss = 6.86146059\n",
      "Iteration 27721, loss = 5.96811296\n",
      "Iteration 27722, loss = 6.91193368\n",
      "Iteration 27723, loss = 5.65850697\n",
      "Iteration 27724, loss = 6.05846091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 27725, loss = 5.04416703\n",
      "Iteration 27726, loss = 6.15204974\n",
      "Iteration 27727, loss = 4.95513359\n",
      "Iteration 27728, loss = 4.18339591\n",
      "Iteration 27729, loss = 4.09696780\n",
      "Iteration 27730, loss = 4.12053134\n",
      "Iteration 27731, loss = 3.68985823\n",
      "Iteration 27732, loss = 3.28460714\n",
      "Iteration 27733, loss = 4.01242796\n",
      "Iteration 27734, loss = 3.92385081\n",
      "Iteration 27735, loss = 3.65394193\n",
      "Iteration 27736, loss = 3.51181908\n",
      "Iteration 27737, loss = 3.49502324\n",
      "Iteration 27738, loss = 3.69634506\n",
      "Iteration 27739, loss = 4.00514388\n",
      "Iteration 27740, loss = 3.97240926\n",
      "Iteration 27741, loss = 3.66891731\n",
      "Iteration 27742, loss = 3.61387464\n",
      "Iteration 27743, loss = 3.93552894\n",
      "Iteration 27744, loss = 3.78784609\n",
      "Iteration 27745, loss = 3.66936611\n",
      "Iteration 27746, loss = 4.09137121\n",
      "Iteration 27747, loss = 3.97146418\n",
      "Iteration 27748, loss = 4.61533857\n",
      "Iteration 27749, loss = 3.40458546\n",
      "Iteration 27750, loss = 3.60285471\n",
      "Iteration 27751, loss = 3.55650067\n",
      "Iteration 27752, loss = 3.73476430\n",
      "Iteration 27753, loss = 3.76874819\n",
      "Iteration 27754, loss = 4.43155029\n",
      "Iteration 27755, loss = 3.82207910\n",
      "Iteration 27756, loss = 4.02707747\n",
      "Iteration 27757, loss = 3.54487876\n",
      "Iteration 27758, loss = 4.04401773\n",
      "Iteration 27759, loss = 4.20887524\n",
      "Iteration 27760, loss = 3.67935249\n",
      "Iteration 27761, loss = 3.58992284\n",
      "Iteration 27762, loss = 3.79595691\n",
      "Iteration 27763, loss = 3.93868818\n",
      "Iteration 27764, loss = 3.45047799\n",
      "Iteration 27765, loss = 3.80343932\n",
      "Iteration 27766, loss = 3.50661380\n",
      "Iteration 27767, loss = 3.31596798\n",
      "Iteration 27768, loss = 4.07233000\n",
      "Iteration 27769, loss = 4.29200700\n",
      "Iteration 27770, loss = 4.05089036\n",
      "Iteration 27771, loss = 3.39468057\n",
      "Iteration 27772, loss = 3.46776226\n",
      "Iteration 27773, loss = 3.28018699\n",
      "Iteration 27774, loss = 3.53573859\n",
      "Iteration 27775, loss = 3.47900764\n",
      "Iteration 27776, loss = 3.74619136\n",
      "Iteration 27777, loss = 3.74183211\n",
      "Iteration 27778, loss = 3.81252052\n",
      "Iteration 27779, loss = 3.61362906\n",
      "Iteration 27780, loss = 3.52128908\n",
      "Iteration 27781, loss = 4.00699681\n",
      "Iteration 27782, loss = 3.48322076\n",
      "Iteration 27783, loss = 3.55403451\n",
      "Iteration 27784, loss = 4.16504712\n",
      "Iteration 27785, loss = 3.75509854\n",
      "Iteration 27786, loss = 5.45072314\n",
      "Iteration 27787, loss = 4.55428563\n",
      "Iteration 27788, loss = 3.72259767\n",
      "Iteration 27789, loss = 4.27263814\n",
      "Iteration 27790, loss = 3.73341509\n",
      "Iteration 27791, loss = 3.70320169\n",
      "Iteration 27792, loss = 3.39365307\n",
      "Iteration 27793, loss = 3.41283066\n",
      "Iteration 27794, loss = 3.43744637\n",
      "Iteration 27795, loss = 3.23016985\n",
      "Iteration 27796, loss = 3.52221178\n",
      "Iteration 27797, loss = 3.68426127\n",
      "Iteration 27798, loss = 3.53399731\n",
      "Iteration 27799, loss = 3.84014052\n",
      "Iteration 27800, loss = 3.89762515\n",
      "Iteration 27801, loss = 4.06234259\n",
      "Iteration 27802, loss = 3.32505461\n",
      "Iteration 27803, loss = 3.45573192\n",
      "Iteration 27804, loss = 3.47648054\n",
      "Iteration 27805, loss = 4.03324858\n",
      "Iteration 27806, loss = 4.10902694\n",
      "Iteration 27807, loss = 3.99257090\n",
      "Iteration 27808, loss = 3.74894165\n",
      "Iteration 27809, loss = 3.29653062\n",
      "Iteration 27810, loss = 3.62939179\n",
      "Iteration 27811, loss = 3.54958000\n",
      "Iteration 27812, loss = 3.52164985\n",
      "Iteration 27813, loss = 3.63119638\n",
      "Iteration 27814, loss = 3.39227406\n",
      "Iteration 27815, loss = 3.95087746\n",
      "Iteration 27816, loss = 3.82474281\n",
      "Iteration 27817, loss = 3.67931585\n",
      "Iteration 27818, loss = 3.70927987\n",
      "Iteration 27819, loss = 4.49035922\n",
      "Iteration 27820, loss = 3.97503070\n",
      "Iteration 27821, loss = 3.62204732\n",
      "Iteration 27822, loss = 3.81973413\n",
      "Iteration 27823, loss = 3.57301317\n",
      "Iteration 27824, loss = 4.13001423\n",
      "Iteration 27825, loss = 3.66145223\n",
      "Iteration 27826, loss = 3.61361576\n",
      "Iteration 27827, loss = 3.51122494\n",
      "Iteration 27828, loss = 3.33928435\n",
      "Iteration 27829, loss = 3.65470418\n",
      "Iteration 27830, loss = 4.31615253\n",
      "Iteration 27831, loss = 3.56725062\n",
      "Iteration 27832, loss = 3.76992475\n",
      "Iteration 27833, loss = 3.66217751\n",
      "Iteration 27834, loss = 4.47502603\n",
      "Iteration 27835, loss = 7.00463661\n",
      "Iteration 27836, loss = 5.86318528\n",
      "Iteration 27837, loss = 6.19169490\n",
      "Iteration 27838, loss = 5.99243364\n",
      "Iteration 27839, loss = 5.45988649\n",
      "Iteration 27840, loss = 6.26688107\n",
      "Iteration 27841, loss = 4.64207219\n",
      "Iteration 27842, loss = 4.01209786\n",
      "Iteration 27843, loss = 4.57088303\n",
      "Iteration 27844, loss = 4.39279230\n",
      "Iteration 27845, loss = 4.28287314\n",
      "Iteration 27846, loss = 3.72059040\n",
      "Iteration 27847, loss = 3.41796232\n",
      "Iteration 27848, loss = 3.29495379\n",
      "Iteration 27849, loss = 3.70272638\n",
      "Iteration 27850, loss = 4.40083137\n",
      "Iteration 27851, loss = 4.10834272\n",
      "Iteration 27852, loss = 4.06191415\n",
      "Iteration 27853, loss = 4.14249158\n",
      "Iteration 27854, loss = 4.77477052\n",
      "Iteration 27855, loss = 4.43439353\n",
      "Iteration 27856, loss = 4.02466393\n",
      "Iteration 27857, loss = 3.84103805\n",
      "Iteration 27858, loss = 4.68730099\n",
      "Iteration 27859, loss = 4.07905219\n",
      "Iteration 27860, loss = 4.38270092\n",
      "Iteration 27861, loss = 4.94490189\n",
      "Iteration 27862, loss = 3.89065397\n",
      "Iteration 27863, loss = 3.96784925\n",
      "Iteration 27864, loss = 3.34019601\n",
      "Iteration 27865, loss = 3.55743372\n",
      "Iteration 27866, loss = 3.33591697\n",
      "Iteration 27867, loss = 3.31371694\n",
      "Iteration 27868, loss = 4.09118146\n",
      "Iteration 27869, loss = 3.86226864\n",
      "Iteration 27870, loss = 3.67200178\n",
      "Iteration 27871, loss = 3.27314992\n",
      "Iteration 27872, loss = 3.28796840\n",
      "Iteration 27873, loss = 4.22725071\n",
      "Iteration 27874, loss = 4.41836433\n",
      "Iteration 27875, loss = 4.90745132\n",
      "Iteration 27876, loss = 4.45714977\n",
      "Iteration 27877, loss = 3.48826348\n",
      "Iteration 27878, loss = 3.30537968\n",
      "Iteration 27879, loss = 3.41917977\n",
      "Iteration 27880, loss = 4.09884129\n",
      "Iteration 27881, loss = 3.81003862\n",
      "Iteration 27882, loss = 3.96736799\n",
      "Iteration 27883, loss = 3.18059575\n",
      "Iteration 27884, loss = 3.77974279\n",
      "Iteration 27885, loss = 4.10120720\n",
      "Iteration 27886, loss = 3.97547475\n",
      "Iteration 27887, loss = 3.64773082\n",
      "Iteration 27888, loss = 3.44372115\n",
      "Iteration 27889, loss = 3.49595189\n",
      "Iteration 27890, loss = 3.69740600\n",
      "Iteration 27891, loss = 3.75261292\n",
      "Iteration 27892, loss = 3.93511085\n",
      "Iteration 27893, loss = 4.61320130\n",
      "Iteration 27894, loss = 3.84267207\n",
      "Iteration 27895, loss = 4.73718030\n",
      "Iteration 27896, loss = 3.28182969\n",
      "Iteration 27897, loss = 3.74918770\n",
      "Iteration 27898, loss = 4.33598114\n",
      "Iteration 27899, loss = 3.82520228\n",
      "Iteration 27900, loss = 4.09703358\n",
      "Iteration 27901, loss = 4.07535605\n",
      "Iteration 27902, loss = 4.56866606\n",
      "Iteration 27903, loss = 3.92033911\n",
      "Iteration 27904, loss = 3.86801964\n",
      "Iteration 27905, loss = 3.48086738\n",
      "Iteration 27906, loss = 3.19666637\n",
      "Iteration 27907, loss = 3.50982615\n",
      "Iteration 27908, loss = 4.51319353\n",
      "Iteration 27909, loss = 4.18965039\n",
      "Iteration 27910, loss = 4.27364185\n",
      "Iteration 27911, loss = 3.79595418\n",
      "Iteration 27912, loss = 4.23005374\n",
      "Iteration 27913, loss = 3.90052479\n",
      "Iteration 27914, loss = 3.55434899\n",
      "Iteration 27915, loss = 3.14413732\n",
      "Iteration 27916, loss = 3.38751649\n",
      "Iteration 27917, loss = 4.09495109\n",
      "Iteration 27918, loss = 3.91952274\n",
      "Iteration 27919, loss = 3.64823106\n",
      "Iteration 27920, loss = 3.59762413\n",
      "Iteration 27921, loss = 3.61429050\n",
      "Iteration 27922, loss = 3.75486653\n",
      "Iteration 27923, loss = 3.39122223\n",
      "Iteration 27924, loss = 4.09468274\n",
      "Iteration 27925, loss = 4.69144938\n",
      "Iteration 27926, loss = 4.94548430\n",
      "Iteration 27927, loss = 5.03979321\n",
      "Iteration 27928, loss = 4.32110018\n",
      "Iteration 27929, loss = 3.79434193\n",
      "Iteration 27930, loss = 3.36766267\n",
      "Iteration 27931, loss = 3.41011352\n",
      "Iteration 27932, loss = 3.36623464\n",
      "Iteration 27933, loss = 3.34215921\n",
      "Iteration 27934, loss = 4.12844602\n",
      "Iteration 27935, loss = 4.47338640\n",
      "Iteration 27936, loss = 4.03405513\n",
      "Iteration 27937, loss = 3.75785010\n",
      "Iteration 27938, loss = 3.53575167\n",
      "Iteration 27939, loss = 4.08736895\n",
      "Iteration 27940, loss = 4.00317454\n",
      "Iteration 27941, loss = 3.96162378\n",
      "Iteration 27942, loss = 4.36228981\n",
      "Iteration 27943, loss = 3.66835326\n",
      "Iteration 27944, loss = 3.80611364\n",
      "Iteration 27945, loss = 3.53553704\n",
      "Iteration 27946, loss = 3.78119841\n",
      "Iteration 27947, loss = 3.62318973\n",
      "Iteration 27948, loss = 3.86985600\n",
      "Iteration 27949, loss = 4.31267900\n",
      "Iteration 27950, loss = 5.18071829\n",
      "Iteration 27951, loss = 5.78961649\n",
      "Iteration 27952, loss = 5.28253738\n",
      "Iteration 27953, loss = 4.98052111\n",
      "Iteration 27954, loss = 3.99627989\n",
      "Iteration 27955, loss = 3.61607871\n",
      "Iteration 27956, loss = 4.31718229\n",
      "Iteration 27957, loss = 5.72118273\n",
      "Iteration 27958, loss = 4.86375245\n",
      "Iteration 27959, loss = 5.71095611\n",
      "Iteration 27960, loss = 4.68800512\n",
      "Iteration 27961, loss = 5.34924863\n",
      "Iteration 27962, loss = 4.23121713\n",
      "Iteration 27963, loss = 4.38738463\n",
      "Iteration 27964, loss = 4.21028504\n",
      "Iteration 27965, loss = 4.81868702\n",
      "Iteration 27966, loss = 4.18591836\n",
      "Iteration 27967, loss = 4.32820517\n",
      "Iteration 27968, loss = 4.12142823\n",
      "Iteration 27969, loss = 3.94392885\n",
      "Iteration 27970, loss = 4.55271441\n",
      "Iteration 27971, loss = 4.36528242\n",
      "Iteration 27972, loss = 3.97604941\n",
      "Iteration 27973, loss = 3.71295056\n",
      "Iteration 27974, loss = 3.85252698\n",
      "Iteration 27975, loss = 4.29032684\n",
      "Iteration 27976, loss = 4.63715176\n",
      "Iteration 27977, loss = 4.85487553\n",
      "Iteration 27978, loss = 3.68088931\n",
      "Iteration 27979, loss = 3.76665683\n",
      "Iteration 27980, loss = 3.76781616\n",
      "Iteration 27981, loss = 3.15359413\n",
      "Iteration 27982, loss = 3.31918358\n",
      "Iteration 27983, loss = 3.52089657\n",
      "Iteration 27984, loss = 3.37102566\n",
      "Iteration 27985, loss = 3.35177002\n",
      "Iteration 27986, loss = 3.91969666\n",
      "Iteration 27987, loss = 3.22162682\n",
      "Iteration 27988, loss = 3.46719083\n",
      "Iteration 27989, loss = 3.86984434\n",
      "Iteration 27990, loss = 3.75213443\n",
      "Iteration 27991, loss = 4.00965244\n",
      "Iteration 27992, loss = 4.56485394\n",
      "Iteration 27993, loss = 5.15559622\n",
      "Iteration 27994, loss = 4.25745632\n",
      "Iteration 27995, loss = 3.44401236\n",
      "Iteration 27996, loss = 3.34347068\n",
      "Iteration 27997, loss = 3.81675151\n",
      "Iteration 27998, loss = 4.08897444\n",
      "Iteration 27999, loss = 3.78185399\n",
      "Iteration 28000, loss = 3.55845330\n",
      "Iteration 28001, loss = 3.38201650\n",
      "Iteration 28002, loss = 3.36305420\n",
      "Iteration 28003, loss = 3.58893526\n",
      "Iteration 28004, loss = 3.79973589\n",
      "Iteration 28005, loss = 3.39509496\n",
      "Iteration 28006, loss = 3.48117810\n",
      "Iteration 28007, loss = 3.44348622\n",
      "Iteration 28008, loss = 3.73050818\n",
      "Iteration 28009, loss = 3.35400187\n",
      "Iteration 28010, loss = 3.09571757\n",
      "Iteration 28011, loss = 3.33655487\n",
      "Iteration 28012, loss = 3.43318740\n",
      "Iteration 28013, loss = 3.48936500\n",
      "Iteration 28014, loss = 3.42210559\n",
      "Iteration 28015, loss = 3.13370566\n",
      "Iteration 28016, loss = 3.73060853\n",
      "Iteration 28017, loss = 4.12147292\n",
      "Iteration 28018, loss = 4.03050258\n",
      "Iteration 28019, loss = 4.73760880\n",
      "Iteration 28020, loss = 3.33575970\n",
      "Iteration 28021, loss = 3.82419331\n",
      "Iteration 28022, loss = 3.56237966\n",
      "Iteration 28023, loss = 3.69805647\n",
      "Iteration 28024, loss = 3.75490646\n",
      "Iteration 28025, loss = 3.49467467\n",
      "Iteration 28026, loss = 3.87176800\n",
      "Iteration 28027, loss = 3.69137248\n",
      "Iteration 28028, loss = 4.73321256\n",
      "Iteration 28029, loss = 3.88179853\n",
      "Iteration 28030, loss = 4.05345991\n",
      "Iteration 28031, loss = 3.76287840\n",
      "Iteration 28032, loss = 3.47596461\n",
      "Iteration 28033, loss = 3.64661360\n",
      "Iteration 28034, loss = 3.21793536\n",
      "Iteration 28035, loss = 3.04118026\n",
      "Iteration 28036, loss = 3.56785448\n",
      "Iteration 28037, loss = 3.87703823\n",
      "Iteration 28038, loss = 3.87005547\n",
      "Iteration 28039, loss = 4.30435320\n",
      "Iteration 28040, loss = 4.78240334\n",
      "Iteration 28041, loss = 4.24043882\n",
      "Iteration 28042, loss = 3.95106458\n",
      "Iteration 28043, loss = 3.39532332\n",
      "Iteration 28044, loss = 3.68916496\n",
      "Iteration 28045, loss = 4.39655902\n",
      "Iteration 28046, loss = 4.29239569\n",
      "Iteration 28047, loss = 4.73383397\n",
      "Iteration 28048, loss = 4.16608703\n",
      "Iteration 28049, loss = 4.41727225\n",
      "Iteration 28050, loss = 3.75134421\n",
      "Iteration 28051, loss = 3.33187270\n",
      "Iteration 28052, loss = 3.35231389\n",
      "Iteration 28053, loss = 3.42508959\n",
      "Iteration 28054, loss = 3.38078221\n",
      "Iteration 28055, loss = 3.59796324\n",
      "Iteration 28056, loss = 3.41919899\n",
      "Iteration 28057, loss = 3.18393462\n",
      "Iteration 28058, loss = 3.62582191\n",
      "Iteration 28059, loss = 3.58617582\n",
      "Iteration 28060, loss = 3.74538431\n",
      "Iteration 28061, loss = 3.34534061\n",
      "Iteration 28062, loss = 3.56351956\n",
      "Iteration 28063, loss = 3.59064975\n",
      "Iteration 28064, loss = 3.93778565\n",
      "Iteration 28065, loss = 4.92860782\n",
      "Iteration 28066, loss = 4.24531437\n",
      "Iteration 28067, loss = 3.70189170\n",
      "Iteration 28068, loss = 3.61407960\n",
      "Iteration 28069, loss = 3.66907325\n",
      "Iteration 28070, loss = 3.86043854\n",
      "Iteration 28071, loss = 3.31253747\n",
      "Iteration 28072, loss = 3.63729001\n",
      "Iteration 28073, loss = 4.09594450\n",
      "Iteration 28074, loss = 4.05559107\n",
      "Iteration 28075, loss = 3.60940200\n",
      "Iteration 28076, loss = 4.13614227\n",
      "Iteration 28077, loss = 5.23006140\n",
      "Iteration 28078, loss = 4.80751497\n",
      "Iteration 28079, loss = 3.47507389\n",
      "Iteration 28080, loss = 3.32220040\n",
      "Iteration 28081, loss = 3.70378167\n",
      "Iteration 28082, loss = 3.61504555\n",
      "Iteration 28083, loss = 4.03522476\n",
      "Iteration 28084, loss = 4.56945380\n",
      "Iteration 28085, loss = 3.89861082\n",
      "Iteration 28086, loss = 4.62022344\n",
      "Iteration 28087, loss = 4.05670080\n",
      "Iteration 28088, loss = 4.00720325\n",
      "Iteration 28089, loss = 3.60181418\n",
      "Iteration 28090, loss = 3.16701254\n",
      "Iteration 28091, loss = 3.30235874\n",
      "Iteration 28092, loss = 5.05536051\n",
      "Iteration 28093, loss = 5.18191128\n",
      "Iteration 28094, loss = 4.73299767\n",
      "Iteration 28095, loss = 4.80105747\n",
      "Iteration 28096, loss = 4.96965530\n",
      "Iteration 28097, loss = 5.26446747\n",
      "Iteration 28098, loss = 4.55269946\n",
      "Iteration 28099, loss = 3.68424262\n",
      "Iteration 28100, loss = 3.44832555\n",
      "Iteration 28101, loss = 3.21363064\n",
      "Iteration 28102, loss = 3.26068568\n",
      "Iteration 28103, loss = 3.13076841\n",
      "Iteration 28104, loss = 3.26494204\n",
      "Iteration 28105, loss = 3.21348159\n",
      "Iteration 28106, loss = 3.20650566\n",
      "Iteration 28107, loss = 3.21788392\n",
      "Iteration 28108, loss = 3.12422657\n",
      "Iteration 28109, loss = 3.35728196\n",
      "Iteration 28110, loss = 3.60351935\n",
      "Iteration 28111, loss = 3.62085074\n",
      "Iteration 28112, loss = 3.57225536\n",
      "Iteration 28113, loss = 3.78581164\n",
      "Iteration 28114, loss = 4.05094709\n",
      "Iteration 28115, loss = 4.04307966\n",
      "Iteration 28116, loss = 4.61625877\n",
      "Iteration 28117, loss = 6.10240777\n",
      "Iteration 28118, loss = 5.45379232\n",
      "Iteration 28119, loss = 4.48435634\n",
      "Iteration 28120, loss = 4.76436640\n",
      "Iteration 28121, loss = 4.56248106\n",
      "Iteration 28122, loss = 5.08458903\n",
      "Iteration 28123, loss = 4.87967416\n",
      "Iteration 28124, loss = 4.35401539\n",
      "Iteration 28125, loss = 4.74661111\n",
      "Iteration 28126, loss = 4.00365036\n",
      "Iteration 28127, loss = 3.41028066\n",
      "Iteration 28128, loss = 3.61010697\n",
      "Iteration 28129, loss = 3.45524965\n",
      "Iteration 28130, loss = 4.12372450\n",
      "Iteration 28131, loss = 3.72371067\n",
      "Iteration 28132, loss = 3.54094453\n",
      "Iteration 28133, loss = 3.08748113\n",
      "Iteration 28134, loss = 3.41428045\n",
      "Iteration 28135, loss = 4.45556430\n",
      "Iteration 28136, loss = 4.21934962\n",
      "Iteration 28137, loss = 3.74167671\n",
      "Iteration 28138, loss = 3.98154248\n",
      "Iteration 28139, loss = 3.90951269\n",
      "Iteration 28140, loss = 3.63360868\n",
      "Iteration 28141, loss = 3.45657180\n",
      "Iteration 28142, loss = 3.60912971\n",
      "Iteration 28143, loss = 3.76678333\n",
      "Iteration 28144, loss = 3.37612608\n",
      "Iteration 28145, loss = 3.72954276\n",
      "Iteration 28146, loss = 4.23538944\n",
      "Iteration 28147, loss = 3.67615401\n",
      "Iteration 28148, loss = 4.22104402\n",
      "Iteration 28149, loss = 3.86291944\n",
      "Iteration 28150, loss = 4.14070523\n",
      "Iteration 28151, loss = 3.87349884\n",
      "Iteration 28152, loss = 3.29887816\n",
      "Iteration 28153, loss = 3.52044957\n",
      "Iteration 28154, loss = 4.01346734\n",
      "Iteration 28155, loss = 3.38084807\n",
      "Iteration 28156, loss = 3.37464685\n",
      "Iteration 28157, loss = 3.53847088\n",
      "Iteration 28158, loss = 3.48117311\n",
      "Iteration 28159, loss = 3.89955346\n",
      "Iteration 28160, loss = 3.44833397\n",
      "Iteration 28161, loss = 3.74975079\n",
      "Iteration 28162, loss = 3.69704730\n",
      "Iteration 28163, loss = 4.07222161\n",
      "Iteration 28164, loss = 3.79606143\n",
      "Iteration 28165, loss = 3.99396812\n",
      "Iteration 28166, loss = 4.55277418\n",
      "Iteration 28167, loss = 4.88196563\n",
      "Iteration 28168, loss = 6.10970597\n",
      "Iteration 28169, loss = 5.35322937\n",
      "Iteration 28170, loss = 4.85785665\n",
      "Iteration 28171, loss = 4.49514913\n",
      "Iteration 28172, loss = 3.60715594\n",
      "Iteration 28173, loss = 4.15278002\n",
      "Iteration 28174, loss = 3.50838641\n",
      "Iteration 28175, loss = 3.47720131\n",
      "Iteration 28176, loss = 3.29512057\n",
      "Iteration 28177, loss = 3.21020995\n",
      "Iteration 28178, loss = 3.34734549\n",
      "Iteration 28179, loss = 3.41858536\n",
      "Iteration 28180, loss = 3.49254558\n",
      "Iteration 28181, loss = 3.28896123\n",
      "Iteration 28182, loss = 3.08799256\n",
      "Iteration 28183, loss = 3.51175591\n",
      "Iteration 28184, loss = 3.45947139\n",
      "Iteration 28185, loss = 3.55519113\n",
      "Iteration 28186, loss = 3.76384847\n",
      "Iteration 28187, loss = 3.81951168\n",
      "Iteration 28188, loss = 3.36763545\n",
      "Iteration 28189, loss = 3.21990147\n",
      "Iteration 28190, loss = 3.13529827\n",
      "Iteration 28191, loss = 3.25500209\n",
      "Iteration 28192, loss = 3.28242841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 28193, loss = 3.18338239\n",
      "Iteration 28194, loss = 3.83601525\n",
      "Iteration 28195, loss = 4.06951225\n",
      "Iteration 28196, loss = 4.02671229\n",
      "Iteration 28197, loss = 3.90186933\n",
      "Iteration 28198, loss = 3.54014299\n",
      "Iteration 28199, loss = 3.29185704\n",
      "Iteration 28200, loss = 3.60092664\n",
      "Iteration 28201, loss = 3.53727314\n",
      "Iteration 28202, loss = 3.59299736\n",
      "Iteration 28203, loss = 3.87478061\n",
      "Iteration 28204, loss = 3.37711911\n",
      "Iteration 28205, loss = 3.77871899\n",
      "Iteration 28206, loss = 3.25036507\n",
      "Iteration 28207, loss = 3.46873226\n",
      "Iteration 28208, loss = 3.29293370\n",
      "Iteration 28209, loss = 3.42358988\n",
      "Iteration 28210, loss = 3.80957110\n",
      "Iteration 28211, loss = 4.06489128\n",
      "Iteration 28212, loss = 4.75193346\n",
      "Iteration 28213, loss = 4.46305768\n",
      "Iteration 28214, loss = 3.89010306\n",
      "Iteration 28215, loss = 3.69440826\n",
      "Iteration 28216, loss = 3.84286753\n",
      "Iteration 28217, loss = 4.02516769\n",
      "Iteration 28218, loss = 4.08117698\n",
      "Iteration 28219, loss = 3.49023470\n",
      "Iteration 28220, loss = 3.24966269\n",
      "Iteration 28221, loss = 4.08177334\n",
      "Iteration 28222, loss = 5.49625784\n",
      "Iteration 28223, loss = 5.34127602\n",
      "Iteration 28224, loss = 8.56144575\n",
      "Iteration 28225, loss = 4.96543929\n",
      "Iteration 28226, loss = 5.60883967\n",
      "Iteration 28227, loss = 5.30232548\n",
      "Iteration 28228, loss = 5.52008716\n",
      "Iteration 28229, loss = 5.17074761\n",
      "Iteration 28230, loss = 3.61830697\n",
      "Iteration 28231, loss = 3.43986263\n",
      "Iteration 28232, loss = 4.08726162\n",
      "Iteration 28233, loss = 3.76231253\n",
      "Iteration 28234, loss = 3.30112655\n",
      "Iteration 28235, loss = 3.96310070\n",
      "Iteration 28236, loss = 3.40758420\n",
      "Iteration 28237, loss = 3.42792808\n",
      "Iteration 28238, loss = 3.72151929\n",
      "Iteration 28239, loss = 3.48893146\n",
      "Iteration 28240, loss = 3.19259831\n",
      "Iteration 28241, loss = 3.23572346\n",
      "Iteration 28242, loss = 3.39952592\n",
      "Iteration 28243, loss = 3.53829480\n",
      "Iteration 28244, loss = 3.89141430\n",
      "Iteration 28245, loss = 4.87158698\n",
      "Iteration 28246, loss = 4.49657002\n",
      "Iteration 28247, loss = 4.21851134\n",
      "Iteration 28248, loss = 3.64669964\n",
      "Iteration 28249, loss = 3.92214568\n",
      "Iteration 28250, loss = 3.50444354\n",
      "Iteration 28251, loss = 3.44461099\n",
      "Iteration 28252, loss = 3.24337195\n",
      "Iteration 28253, loss = 3.56732305\n",
      "Iteration 28254, loss = 3.58305782\n",
      "Iteration 28255, loss = 3.49110006\n",
      "Iteration 28256, loss = 3.30792616\n",
      "Iteration 28257, loss = 3.38542635\n",
      "Iteration 28258, loss = 3.36414513\n",
      "Iteration 28259, loss = 3.74800178\n",
      "Iteration 28260, loss = 3.58227268\n",
      "Iteration 28261, loss = 3.60087981\n",
      "Iteration 28262, loss = 4.21909698\n",
      "Iteration 28263, loss = 4.18897741\n",
      "Iteration 28264, loss = 3.86251935\n",
      "Iteration 28265, loss = 4.17917976\n",
      "Iteration 28266, loss = 3.74304283\n",
      "Iteration 28267, loss = 3.89764960\n",
      "Iteration 28268, loss = 3.66512169\n",
      "Iteration 28269, loss = 3.58695808\n",
      "Iteration 28270, loss = 3.43313989\n",
      "Iteration 28271, loss = 3.27668477\n",
      "Iteration 28272, loss = 3.43329569\n",
      "Iteration 28273, loss = 3.42437115\n",
      "Iteration 28274, loss = 3.61380447\n",
      "Iteration 28275, loss = 3.97934260\n",
      "Iteration 28276, loss = 3.51637590\n",
      "Iteration 28277, loss = 4.21267935\n",
      "Iteration 28278, loss = 4.07822884\n",
      "Iteration 28279, loss = 4.14878429\n",
      "Iteration 28280, loss = 4.42341698\n",
      "Iteration 28281, loss = 4.35769353\n",
      "Iteration 28282, loss = 4.72735605\n",
      "Iteration 28283, loss = 5.54826187\n",
      "Iteration 28284, loss = 5.46186571\n",
      "Iteration 28285, loss = 4.63589015\n",
      "Iteration 28286, loss = 4.54066576\n",
      "Iteration 28287, loss = 3.66659456\n",
      "Iteration 28288, loss = 4.25473463\n",
      "Iteration 28289, loss = 4.08614307\n",
      "Iteration 28290, loss = 3.90737415\n",
      "Iteration 28291, loss = 3.73904545\n",
      "Iteration 28292, loss = 3.73072031\n",
      "Iteration 28293, loss = 4.48184860\n",
      "Iteration 28294, loss = 3.56021387\n",
      "Iteration 28295, loss = 4.26299177\n",
      "Iteration 28296, loss = 3.79471015\n",
      "Iteration 28297, loss = 3.94401186\n",
      "Iteration 28298, loss = 3.39789960\n",
      "Iteration 28299, loss = 3.39296321\n",
      "Iteration 28300, loss = 3.74665475\n",
      "Iteration 28301, loss = 3.43856465\n",
      "Iteration 28302, loss = 3.54867547\n",
      "Iteration 28303, loss = 3.80710431\n",
      "Iteration 28304, loss = 3.92659476\n",
      "Iteration 28305, loss = 3.98501376\n",
      "Iteration 28306, loss = 3.78343819\n",
      "Iteration 28307, loss = 3.42071971\n",
      "Iteration 28308, loss = 3.68566899\n",
      "Iteration 28309, loss = 3.31730090\n",
      "Iteration 28310, loss = 3.40299110\n",
      "Iteration 28311, loss = 3.32982070\n",
      "Iteration 28312, loss = 4.03877683\n",
      "Iteration 28313, loss = 4.01295131\n",
      "Iteration 28314, loss = 3.94982242\n",
      "Iteration 28315, loss = 3.63871735\n",
      "Iteration 28316, loss = 3.38038427\n",
      "Iteration 28317, loss = 3.43775621\n",
      "Iteration 28318, loss = 3.39763854\n",
      "Iteration 28319, loss = 3.84569122\n",
      "Iteration 28320, loss = 3.51275657\n",
      "Iteration 28321, loss = 3.59060288\n",
      "Iteration 28322, loss = 3.14530435\n",
      "Iteration 28323, loss = 3.41013502\n",
      "Iteration 28324, loss = 3.61822617\n",
      "Iteration 28325, loss = 3.31619181\n",
      "Iteration 28326, loss = 3.86506785\n",
      "Iteration 28327, loss = 3.49729234\n",
      "Iteration 28328, loss = 3.50129322\n",
      "Iteration 28329, loss = 3.62067204\n",
      "Iteration 28330, loss = 3.47320224\n",
      "Iteration 28331, loss = 3.23420761\n",
      "Iteration 28332, loss = 3.35209410\n",
      "Iteration 28333, loss = 3.66831961\n",
      "Iteration 28334, loss = 4.15566668\n",
      "Iteration 28335, loss = 3.60974356\n",
      "Iteration 28336, loss = 3.38212510\n",
      "Iteration 28337, loss = 3.46758317\n",
      "Iteration 28338, loss = 3.24769445\n",
      "Iteration 28339, loss = 3.34577017\n",
      "Iteration 28340, loss = 3.33210830\n",
      "Iteration 28341, loss = 3.90617071\n",
      "Iteration 28342, loss = 3.38911590\n",
      "Iteration 28343, loss = 3.08105315\n",
      "Iteration 28344, loss = 3.20957014\n",
      "Iteration 28345, loss = 3.08698190\n",
      "Iteration 28346, loss = 3.51796335\n",
      "Iteration 28347, loss = 3.36754207\n",
      "Iteration 28348, loss = 3.59436380\n",
      "Iteration 28349, loss = 3.48035793\n",
      "Iteration 28350, loss = 3.14731928\n",
      "Iteration 28351, loss = 3.76386401\n",
      "Iteration 28352, loss = 4.42264757\n",
      "Iteration 28353, loss = 4.60995320\n",
      "Iteration 28354, loss = 4.63876852\n",
      "Iteration 28355, loss = 5.71766225\n",
      "Iteration 28356, loss = 6.32114165\n",
      "Iteration 28357, loss = 4.67367677\n",
      "Iteration 28358, loss = 5.16993134\n",
      "Iteration 28359, loss = 4.14678531\n",
      "Iteration 28360, loss = 3.90580532\n",
      "Iteration 28361, loss = 3.39414169\n",
      "Iteration 28362, loss = 3.47888157\n",
      "Iteration 28363, loss = 3.52409845\n",
      "Iteration 28364, loss = 3.57979164\n",
      "Iteration 28365, loss = 4.04772359\n",
      "Iteration 28366, loss = 3.95576713\n",
      "Iteration 28367, loss = 4.78008512\n",
      "Iteration 28368, loss = 4.81188125\n",
      "Iteration 28369, loss = 5.04750488\n",
      "Iteration 28370, loss = 4.60861518\n",
      "Iteration 28371, loss = 5.46904622\n",
      "Iteration 28372, loss = 4.97556855\n",
      "Iteration 28373, loss = 3.48198743\n",
      "Iteration 28374, loss = 3.74908916\n",
      "Iteration 28375, loss = 3.55438758\n",
      "Iteration 28376, loss = 3.94690213\n",
      "Iteration 28377, loss = 3.37696145\n",
      "Iteration 28378, loss = 3.33049004\n",
      "Iteration 28379, loss = 3.30938593\n",
      "Iteration 28380, loss = 3.70186029\n",
      "Iteration 28381, loss = 3.57207202\n",
      "Iteration 28382, loss = 3.64167883\n",
      "Iteration 28383, loss = 3.64074254\n",
      "Iteration 28384, loss = 3.14122213\n",
      "Iteration 28385, loss = 3.47737496\n",
      "Iteration 28386, loss = 3.02677528\n",
      "Iteration 28387, loss = 3.04939577\n",
      "Iteration 28388, loss = 3.41377333\n",
      "Iteration 28389, loss = 3.32416572\n",
      "Iteration 28390, loss = 3.57969448\n",
      "Iteration 28391, loss = 3.67817861\n",
      "Iteration 28392, loss = 3.20356721\n",
      "Iteration 28393, loss = 3.08320169\n",
      "Iteration 28394, loss = 3.25288687\n",
      "Iteration 28395, loss = 3.39034538\n",
      "Iteration 28396, loss = 3.57095138\n",
      "Iteration 28397, loss = 3.53057413\n",
      "Iteration 28398, loss = 3.52278887\n",
      "Iteration 28399, loss = 3.71280902\n",
      "Iteration 28400, loss = 3.93999878\n",
      "Iteration 28401, loss = 3.98723779\n",
      "Iteration 28402, loss = 4.08989851\n",
      "Iteration 28403, loss = 4.67051754\n",
      "Iteration 28404, loss = 4.80362215\n",
      "Iteration 28405, loss = 3.78221641\n",
      "Iteration 28406, loss = 3.88880312\n",
      "Iteration 28407, loss = 3.49686304\n",
      "Iteration 28408, loss = 3.39601518\n",
      "Iteration 28409, loss = 3.12916396\n",
      "Iteration 28410, loss = 3.25237843\n",
      "Iteration 28411, loss = 3.47409031\n",
      "Iteration 28412, loss = 3.72625613\n",
      "Iteration 28413, loss = 6.11307030\n",
      "Iteration 28414, loss = 5.38161178\n",
      "Iteration 28415, loss = 4.52955068\n",
      "Iteration 28416, loss = 4.96494655\n",
      "Iteration 28417, loss = 4.05396592\n",
      "Iteration 28418, loss = 3.54428547\n",
      "Iteration 28419, loss = 4.02647458\n",
      "Iteration 28420, loss = 3.44631135\n",
      "Iteration 28421, loss = 4.21780150\n",
      "Iteration 28422, loss = 4.37563274\n",
      "Iteration 28423, loss = 3.74770350\n",
      "Iteration 28424, loss = 4.10707423\n",
      "Iteration 28425, loss = 4.20201181\n",
      "Iteration 28426, loss = 3.90504939\n",
      "Iteration 28427, loss = 3.56344455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 28428, loss = 4.04358720\n",
      "Iteration 28429, loss = 3.94097556\n",
      "Iteration 28430, loss = 3.96036373\n",
      "Iteration 28431, loss = 3.34436815\n",
      "Iteration 28432, loss = 3.69041781\n",
      "Iteration 28433, loss = 4.46640219\n",
      "Iteration 28434, loss = 4.03008608\n",
      "Iteration 28435, loss = 3.97183749\n",
      "Iteration 28436, loss = 3.89295798\n",
      "Iteration 28437, loss = 3.38102623\n",
      "Iteration 28438, loss = 3.41271442\n",
      "Iteration 28439, loss = 3.70090048\n",
      "Iteration 28440, loss = 4.02221645\n",
      "Iteration 28441, loss = 3.72357052\n",
      "Iteration 28442, loss = 3.31815350\n",
      "Iteration 28443, loss = 3.24591748\n",
      "Iteration 28444, loss = 3.04342293\n",
      "Iteration 28445, loss = 3.25207603\n",
      "Iteration 28446, loss = 3.47513306\n",
      "Iteration 28447, loss = 3.55005563\n",
      "Iteration 28448, loss = 3.74582550\n",
      "Iteration 28449, loss = 3.69956417\n",
      "Iteration 28450, loss = 4.04501691\n",
      "Iteration 28451, loss = 3.57385033\n",
      "Iteration 28452, loss = 4.27124075\n",
      "Iteration 28453, loss = 3.61480680\n",
      "Iteration 28454, loss = 3.54503679\n",
      "Iteration 28455, loss = 3.59573878\n",
      "Iteration 28456, loss = 4.76692150\n",
      "Iteration 28457, loss = 4.75683447\n",
      "Iteration 28458, loss = 4.87832765\n",
      "Iteration 28459, loss = 4.37031262\n",
      "Iteration 28460, loss = 3.77746000\n",
      "Iteration 28461, loss = 3.61320540\n",
      "Iteration 28462, loss = 4.41795321\n",
      "Iteration 28463, loss = 4.68047271\n",
      "Iteration 28464, loss = 5.11141003\n",
      "Iteration 28465, loss = 4.84614512\n",
      "Iteration 28466, loss = 3.82609463\n",
      "Iteration 28467, loss = 3.84226530\n",
      "Iteration 28468, loss = 3.68670915\n",
      "Iteration 28469, loss = 3.56856827\n",
      "Iteration 28470, loss = 4.40794394\n",
      "Iteration 28471, loss = 4.40431286\n",
      "Iteration 28472, loss = 3.77353975\n",
      "Iteration 28473, loss = 3.48708620\n",
      "Iteration 28474, loss = 4.09836720\n",
      "Iteration 28475, loss = 4.33796239\n",
      "Iteration 28476, loss = 3.64315236\n",
      "Iteration 28477, loss = 3.39307386\n",
      "Iteration 28478, loss = 3.50916715\n",
      "Iteration 28479, loss = 3.06213451\n",
      "Iteration 28480, loss = 3.28196899\n",
      "Iteration 28481, loss = 3.42850251\n",
      "Iteration 28482, loss = 3.50815555\n",
      "Iteration 28483, loss = 3.41241204\n",
      "Iteration 28484, loss = 3.19798557\n",
      "Iteration 28485, loss = 3.19156685\n",
      "Iteration 28486, loss = 3.12511622\n",
      "Iteration 28487, loss = 3.06756703\n",
      "Iteration 28488, loss = 3.15109360\n",
      "Iteration 28489, loss = 3.19036221\n",
      "Iteration 28490, loss = 3.29132147\n",
      "Iteration 28491, loss = 3.31182740\n",
      "Iteration 28492, loss = 3.47229242\n",
      "Iteration 28493, loss = 3.45455523\n",
      "Iteration 28494, loss = 3.68168419\n",
      "Iteration 28495, loss = 3.85437974\n",
      "Iteration 28496, loss = 4.03292794\n",
      "Iteration 28497, loss = 3.70294491\n",
      "Iteration 28498, loss = 3.72791656\n",
      "Iteration 28499, loss = 3.13990447\n",
      "Iteration 28500, loss = 3.85503346\n",
      "Iteration 28501, loss = 3.94189131\n",
      "Iteration 28502, loss = 3.76954796\n",
      "Iteration 28503, loss = 3.89486086\n",
      "Iteration 28504, loss = 4.15335245\n",
      "Iteration 28505, loss = 3.57410634\n",
      "Iteration 28506, loss = 3.80236696\n",
      "Iteration 28507, loss = 3.61543831\n",
      "Iteration 28508, loss = 3.26376712\n",
      "Iteration 28509, loss = 3.53191954\n",
      "Iteration 28510, loss = 4.34620288\n",
      "Iteration 28511, loss = 3.27287738\n",
      "Iteration 28512, loss = 3.29617645\n",
      "Iteration 28513, loss = 3.53690211\n",
      "Iteration 28514, loss = 3.44149103\n",
      "Iteration 28515, loss = 3.45708253\n",
      "Iteration 28516, loss = 3.52229035\n",
      "Iteration 28517, loss = 3.09098676\n",
      "Iteration 28518, loss = 3.06210956\n",
      "Iteration 28519, loss = 3.29869257\n",
      "Iteration 28520, loss = 3.65995599\n",
      "Iteration 28521, loss = 3.21752981\n",
      "Iteration 28522, loss = 3.45145069\n",
      "Iteration 28523, loss = 3.81791801\n",
      "Iteration 28524, loss = 3.67005570\n",
      "Iteration 28525, loss = 3.52330056\n",
      "Iteration 28526, loss = 3.48047159\n",
      "Iteration 28527, loss = 3.53014768\n",
      "Iteration 28528, loss = 3.37175615\n",
      "Iteration 28529, loss = 4.01917895\n",
      "Iteration 28530, loss = 3.92180513\n",
      "Iteration 28531, loss = 5.32983154\n",
      "Iteration 28532, loss = 5.47840997\n",
      "Iteration 28533, loss = 4.53137250\n",
      "Iteration 28534, loss = 3.72217924\n",
      "Iteration 28535, loss = 3.76722269\n",
      "Iteration 28536, loss = 3.84172461\n",
      "Iteration 28537, loss = 3.60107622\n",
      "Iteration 28538, loss = 3.82183580\n",
      "Iteration 28539, loss = 4.82533675\n",
      "Iteration 28540, loss = 4.20390418\n",
      "Iteration 28541, loss = 3.58414174\n",
      "Iteration 28542, loss = 3.52326235\n",
      "Iteration 28543, loss = 3.43960897\n",
      "Iteration 28544, loss = 3.86241755\n",
      "Iteration 28545, loss = 4.12925875\n",
      "Iteration 28546, loss = 3.34497258\n",
      "Iteration 28547, loss = 3.19820449\n",
      "Iteration 28548, loss = 3.24888979\n",
      "Iteration 28549, loss = 3.25511935\n",
      "Iteration 28550, loss = 3.52069964\n",
      "Iteration 28551, loss = 3.36886058\n",
      "Iteration 28552, loss = 4.02723516\n",
      "Iteration 28553, loss = 3.80252151\n",
      "Iteration 28554, loss = 3.66961090\n",
      "Iteration 28555, loss = 3.69345244\n",
      "Iteration 28556, loss = 4.88655370\n",
      "Iteration 28557, loss = 5.69621692\n",
      "Iteration 28558, loss = 4.78737071\n",
      "Iteration 28559, loss = 5.11389950\n",
      "Iteration 28560, loss = 5.15387682\n",
      "Iteration 28561, loss = 6.50786573\n",
      "Iteration 28562, loss = 7.04407988\n",
      "Iteration 28563, loss = 7.11288683\n",
      "Iteration 28564, loss = 5.69603634\n",
      "Iteration 28565, loss = 5.49450060\n",
      "Iteration 28566, loss = 4.76595245\n",
      "Iteration 28567, loss = 4.46262684\n",
      "Iteration 28568, loss = 3.83678819\n",
      "Iteration 28569, loss = 3.36668069\n",
      "Iteration 28570, loss = 3.70763727\n",
      "Iteration 28571, loss = 4.66044008\n",
      "Iteration 28572, loss = 5.32194924\n",
      "Iteration 28573, loss = 5.40206235\n",
      "Iteration 28574, loss = 3.85780117\n",
      "Iteration 28575, loss = 3.77116391\n",
      "Iteration 28576, loss = 3.37575402\n",
      "Iteration 28577, loss = 4.06933320\n",
      "Iteration 28578, loss = 5.13195601\n",
      "Iteration 28579, loss = 3.81735963\n",
      "Iteration 28580, loss = 3.61648215\n",
      "Iteration 28581, loss = 3.30455521\n",
      "Iteration 28582, loss = 3.81257635\n",
      "Iteration 28583, loss = 3.53081806\n",
      "Iteration 28584, loss = 3.91158006\n",
      "Iteration 28585, loss = 4.01427560\n",
      "Iteration 28586, loss = 3.37181000\n",
      "Iteration 28587, loss = 3.09490159\n",
      "Iteration 28588, loss = 3.03499078\n",
      "Iteration 28589, loss = 3.69930267\n",
      "Iteration 28590, loss = 5.41894151\n",
      "Iteration 28591, loss = 5.33994022\n",
      "Iteration 28592, loss = 3.99603081\n",
      "Iteration 28593, loss = 3.76545602\n",
      "Iteration 28594, loss = 3.52495393\n",
      "Iteration 28595, loss = 3.32445985\n",
      "Iteration 28596, loss = 3.25095755\n",
      "Iteration 28597, loss = 3.24345675\n",
      "Iteration 28598, loss = 3.21716531\n",
      "Iteration 28599, loss = 3.60919430\n",
      "Iteration 28600, loss = 3.86011415\n",
      "Iteration 28601, loss = 3.72760092\n",
      "Iteration 28602, loss = 3.79936499\n",
      "Iteration 28603, loss = 3.60530230\n",
      "Iteration 28604, loss = 3.43238533\n",
      "Iteration 28605, loss = 3.95684717\n",
      "Iteration 28606, loss = 4.04480947\n",
      "Iteration 28607, loss = 4.28662271\n",
      "Iteration 28608, loss = 3.53119918\n",
      "Iteration 28609, loss = 3.52782166\n",
      "Iteration 28610, loss = 3.88412342\n",
      "Iteration 28611, loss = 4.37376481\n",
      "Iteration 28612, loss = 4.94668304\n",
      "Iteration 28613, loss = 4.05637816\n",
      "Iteration 28614, loss = 3.97800665\n",
      "Iteration 28615, loss = 3.90247648\n",
      "Iteration 28616, loss = 3.70915908\n",
      "Iteration 28617, loss = 3.81057372\n",
      "Iteration 28618, loss = 3.60690906\n",
      "Iteration 28619, loss = 4.31688977\n",
      "Iteration 28620, loss = 3.92437025\n",
      "Iteration 28621, loss = 4.02551477\n",
      "Iteration 28622, loss = 3.49713020\n",
      "Iteration 28623, loss = 3.35165609\n",
      "Iteration 28624, loss = 3.30973715\n",
      "Iteration 28625, loss = 3.34090380\n",
      "Iteration 28626, loss = 3.34009530\n",
      "Iteration 28627, loss = 3.86328516\n",
      "Iteration 28628, loss = 3.44245781\n",
      "Iteration 28629, loss = 3.43705915\n",
      "Iteration 28630, loss = 3.72204193\n",
      "Iteration 28631, loss = 3.53646933\n",
      "Iteration 28632, loss = 4.20424489\n",
      "Iteration 28633, loss = 4.22115568\n",
      "Iteration 28634, loss = 3.71497229\n",
      "Iteration 28635, loss = 3.25593339\n",
      "Iteration 28636, loss = 3.08619467\n",
      "Iteration 28637, loss = 3.21023018\n",
      "Iteration 28638, loss = 3.44831302\n",
      "Iteration 28639, loss = 3.93417442\n",
      "Iteration 28640, loss = 3.74142681\n",
      "Iteration 28641, loss = 4.19645924\n",
      "Iteration 28642, loss = 3.51833860\n",
      "Iteration 28643, loss = 3.41704125\n",
      "Iteration 28644, loss = 3.44101578\n",
      "Iteration 28645, loss = 3.78677651\n",
      "Iteration 28646, loss = 3.81267948\n",
      "Iteration 28647, loss = 3.53417190\n",
      "Iteration 28648, loss = 3.42499405\n",
      "Iteration 28649, loss = 3.32435233\n",
      "Iteration 28650, loss = 3.12585430\n",
      "Iteration 28651, loss = 3.49624075\n",
      "Iteration 28652, loss = 3.48889461\n",
      "Iteration 28653, loss = 3.19539922\n",
      "Iteration 28654, loss = 3.19543167\n",
      "Iteration 28655, loss = 3.09717131\n",
      "Iteration 28656, loss = 3.16824852\n",
      "Iteration 28657, loss = 3.34286278\n",
      "Iteration 28658, loss = 4.13424740\n",
      "Iteration 28659, loss = 4.19442854\n",
      "Iteration 28660, loss = 4.32414383\n",
      "Iteration 28661, loss = 3.81861478\n",
      "Iteration 28662, loss = 3.69548934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 28663, loss = 4.73353225\n",
      "Iteration 28664, loss = 5.49914534\n",
      "Iteration 28665, loss = 4.71417165\n",
      "Iteration 28666, loss = 5.00441329\n",
      "Iteration 28667, loss = 4.76637453\n",
      "Iteration 28668, loss = 4.55909153\n",
      "Iteration 28669, loss = 4.97735232\n",
      "Iteration 28670, loss = 5.22522366\n",
      "Iteration 28671, loss = 4.98321472\n",
      "Iteration 28672, loss = 4.89898336\n",
      "Iteration 28673, loss = 4.19889092\n",
      "Iteration 28674, loss = 3.92414836\n",
      "Iteration 28675, loss = 4.72613494\n",
      "Iteration 28676, loss = 4.52498319\n",
      "Iteration 28677, loss = 3.87695396\n",
      "Iteration 28678, loss = 4.24445460\n",
      "Iteration 28679, loss = 4.29399233\n",
      "Iteration 28680, loss = 4.04445618\n",
      "Iteration 28681, loss = 3.57581127\n",
      "Iteration 28682, loss = 3.42654059\n",
      "Iteration 28683, loss = 3.30067970\n",
      "Iteration 28684, loss = 3.29068354\n",
      "Iteration 28685, loss = 3.09046590\n",
      "Iteration 28686, loss = 3.46295620\n",
      "Iteration 28687, loss = 4.18743798\n",
      "Iteration 28688, loss = 4.28054612\n",
      "Iteration 28689, loss = 4.24608457\n",
      "Iteration 28690, loss = 3.32496623\n",
      "Iteration 28691, loss = 3.36760823\n",
      "Iteration 28692, loss = 3.88495075\n",
      "Iteration 28693, loss = 3.61439052\n",
      "Iteration 28694, loss = 3.62808154\n",
      "Iteration 28695, loss = 4.09214038\n",
      "Iteration 28696, loss = 4.18901684\n",
      "Iteration 28697, loss = 3.70210105\n",
      "Iteration 28698, loss = 3.41468384\n",
      "Iteration 28699, loss = 3.32367766\n",
      "Iteration 28700, loss = 4.19502579\n",
      "Iteration 28701, loss = 3.46757167\n",
      "Iteration 28702, loss = 3.92920646\n",
      "Iteration 28703, loss = 3.35816381\n",
      "Iteration 28704, loss = 3.55359778\n",
      "Iteration 28705, loss = 3.98936238\n",
      "Iteration 28706, loss = 4.15289756\n",
      "Iteration 28707, loss = 5.59006377\n",
      "Iteration 28708, loss = 4.24496553\n",
      "Iteration 28709, loss = 3.63399000\n",
      "Iteration 28710, loss = 3.44441581\n",
      "Iteration 28711, loss = 3.55277353\n",
      "Iteration 28712, loss = 3.75073681\n",
      "Iteration 28713, loss = 3.49966728\n",
      "Iteration 28714, loss = 3.62068576\n",
      "Iteration 28715, loss = 4.06721801\n",
      "Iteration 28716, loss = 4.25625056\n",
      "Iteration 28717, loss = 3.55419450\n",
      "Iteration 28718, loss = 3.26412410\n",
      "Iteration 28719, loss = 3.12776473\n",
      "Iteration 28720, loss = 3.50431787\n",
      "Iteration 28721, loss = 3.23202159\n",
      "Iteration 28722, loss = 3.49780597\n",
      "Iteration 28723, loss = 3.36200101\n",
      "Iteration 28724, loss = 4.05400929\n",
      "Iteration 28725, loss = 3.67464742\n",
      "Iteration 28726, loss = 4.53838964\n",
      "Iteration 28727, loss = 3.48316932\n",
      "Iteration 28728, loss = 3.81147773\n",
      "Iteration 28729, loss = 3.65924987\n",
      "Iteration 28730, loss = 3.72100870\n",
      "Iteration 28731, loss = 3.73048326\n",
      "Iteration 28732, loss = 3.59047557\n",
      "Iteration 28733, loss = 4.36533290\n",
      "Iteration 28734, loss = 3.49119373\n",
      "Iteration 28735, loss = 3.87257529\n",
      "Iteration 28736, loss = 3.32090699\n",
      "Iteration 28737, loss = 3.56337804\n",
      "Iteration 28738, loss = 3.21320552\n",
      "Iteration 28739, loss = 3.28701732\n",
      "Iteration 28740, loss = 3.51864274\n",
      "Iteration 28741, loss = 3.52143038\n",
      "Iteration 28742, loss = 3.48536094\n",
      "Iteration 28743, loss = 3.85218887\n",
      "Iteration 28744, loss = 3.84417788\n",
      "Iteration 28745, loss = 3.78595283\n",
      "Iteration 28746, loss = 3.88839210\n",
      "Iteration 28747, loss = 4.23844632\n",
      "Iteration 28748, loss = 4.93183409\n",
      "Iteration 28749, loss = 4.70005273\n",
      "Iteration 28750, loss = 4.67079946\n",
      "Iteration 28751, loss = 5.04997065\n",
      "Iteration 28752, loss = 4.18293023\n",
      "Iteration 28753, loss = 3.42943859\n",
      "Iteration 28754, loss = 3.47820394\n",
      "Iteration 28755, loss = 3.51584062\n",
      "Iteration 28756, loss = 3.56447066\n",
      "Iteration 28757, loss = 3.18223580\n",
      "Iteration 28758, loss = 3.20637039\n",
      "Iteration 28759, loss = 3.11384945\n",
      "Iteration 28760, loss = 3.29389246\n",
      "Iteration 28761, loss = 3.17568741\n",
      "Iteration 28762, loss = 3.76705419\n",
      "Iteration 28763, loss = 3.61975395\n",
      "Iteration 28764, loss = 3.35477910\n",
      "Iteration 28765, loss = 3.86399731\n",
      "Iteration 28766, loss = 3.15025132\n",
      "Iteration 28767, loss = 3.68288393\n",
      "Iteration 28768, loss = 3.96854045\n",
      "Iteration 28769, loss = 3.12700586\n",
      "Iteration 28770, loss = 3.13147951\n",
      "Iteration 28771, loss = 3.19205099\n",
      "Iteration 28772, loss = 3.24512586\n",
      "Iteration 28773, loss = 3.25751975\n",
      "Iteration 28774, loss = 3.45539448\n",
      "Iteration 28775, loss = 3.07592432\n",
      "Iteration 28776, loss = 3.73776730\n",
      "Iteration 28777, loss = 5.49940477\n",
      "Iteration 28778, loss = 5.57038631\n",
      "Iteration 28779, loss = 4.97910041\n",
      "Iteration 28780, loss = 3.52595290\n",
      "Iteration 28781, loss = 4.17978291\n",
      "Iteration 28782, loss = 3.57192842\n",
      "Iteration 28783, loss = 3.81239964\n",
      "Iteration 28784, loss = 3.83230161\n",
      "Iteration 28785, loss = 4.14561323\n",
      "Iteration 28786, loss = 4.46143146\n",
      "Iteration 28787, loss = 3.50140914\n",
      "Iteration 28788, loss = 3.45948008\n",
      "Iteration 28789, loss = 4.00236155\n",
      "Iteration 28790, loss = 4.24069782\n",
      "Iteration 28791, loss = 6.51324309\n",
      "Iteration 28792, loss = 7.06637603\n",
      "Iteration 28793, loss = 5.95899392\n",
      "Iteration 28794, loss = 5.37327909\n",
      "Iteration 28795, loss = 3.82433355\n",
      "Iteration 28796, loss = 4.53809757\n",
      "Iteration 28797, loss = 3.92857476\n",
      "Iteration 28798, loss = 3.55579590\n",
      "Iteration 28799, loss = 3.22163239\n",
      "Iteration 28800, loss = 3.03564417\n",
      "Iteration 28801, loss = 3.29877844\n",
      "Iteration 28802, loss = 2.89715991\n",
      "Iteration 28803, loss = 3.51732979\n",
      "Iteration 28804, loss = 3.26564097\n",
      "Iteration 28805, loss = 3.81645990\n",
      "Iteration 28806, loss = 3.27858608\n",
      "Iteration 28807, loss = 3.58301062\n",
      "Iteration 28808, loss = 3.19026932\n",
      "Iteration 28809, loss = 3.62714354\n",
      "Iteration 28810, loss = 4.06583078\n",
      "Iteration 28811, loss = 3.40066717\n",
      "Iteration 28812, loss = 3.87445509\n",
      "Iteration 28813, loss = 3.52603802\n",
      "Iteration 28814, loss = 3.12173046\n",
      "Iteration 28815, loss = 3.20040027\n",
      "Iteration 28816, loss = 3.25067841\n",
      "Iteration 28817, loss = 3.20958985\n",
      "Iteration 28818, loss = 3.06965318\n",
      "Iteration 28819, loss = 3.26906232\n",
      "Iteration 28820, loss = 3.50485825\n",
      "Iteration 28821, loss = 3.47734662\n",
      "Iteration 28822, loss = 3.08644722\n",
      "Iteration 28823, loss = 3.17579491\n",
      "Iteration 28824, loss = 3.24781293\n",
      "Iteration 28825, loss = 4.17839950\n",
      "Iteration 28826, loss = 3.89545498\n",
      "Iteration 28827, loss = 4.47849498\n",
      "Iteration 28828, loss = 4.34184687\n",
      "Iteration 28829, loss = 4.04468364\n",
      "Iteration 28830, loss = 3.91430394\n",
      "Iteration 28831, loss = 3.68531515\n",
      "Iteration 28832, loss = 3.41754014\n",
      "Iteration 28833, loss = 4.03182312\n",
      "Iteration 28834, loss = 4.42168340\n",
      "Iteration 28835, loss = 4.27236757\n",
      "Iteration 28836, loss = 3.37311071\n",
      "Iteration 28837, loss = 3.46275291\n",
      "Iteration 28838, loss = 3.74277589\n",
      "Iteration 28839, loss = 3.60634387\n",
      "Iteration 28840, loss = 3.45454922\n",
      "Iteration 28841, loss = 3.38065703\n",
      "Iteration 28842, loss = 2.99619520\n",
      "Iteration 28843, loss = 3.64560824\n",
      "Iteration 28844, loss = 3.45213057\n",
      "Iteration 28845, loss = 3.76275481\n",
      "Iteration 28846, loss = 3.33573483\n",
      "Iteration 28847, loss = 3.53300725\n",
      "Iteration 28848, loss = 3.28747228\n",
      "Iteration 28849, loss = 3.22920297\n",
      "Iteration 28850, loss = 3.19133232\n",
      "Iteration 28851, loss = 3.08089927\n",
      "Iteration 28852, loss = 3.37026498\n",
      "Iteration 28853, loss = 3.14888450\n",
      "Iteration 28854, loss = 3.13370405\n",
      "Iteration 28855, loss = 3.46684788\n",
      "Iteration 28856, loss = 3.27744979\n",
      "Iteration 28857, loss = 3.28562866\n",
      "Iteration 28858, loss = 3.12793272\n",
      "Iteration 28859, loss = 3.17001051\n",
      "Iteration 28860, loss = 3.49565059\n",
      "Iteration 28861, loss = 3.23285838\n",
      "Iteration 28862, loss = 3.55511730\n",
      "Iteration 28863, loss = 3.17757476\n",
      "Iteration 28864, loss = 3.32875653\n",
      "Iteration 28865, loss = 3.54088796\n",
      "Iteration 28866, loss = 3.37875280\n",
      "Iteration 28867, loss = 3.04892577\n",
      "Iteration 28868, loss = 3.31461443\n",
      "Iteration 28869, loss = 3.45392777\n",
      "Iteration 28870, loss = 3.96498064\n",
      "Iteration 28871, loss = 4.23148811\n",
      "Iteration 28872, loss = 3.71098719\n",
      "Iteration 28873, loss = 3.53725375\n",
      "Iteration 28874, loss = 3.51134957\n",
      "Iteration 28875, loss = 3.74600293\n",
      "Iteration 28876, loss = 4.47648168\n",
      "Iteration 28877, loss = 4.80272581\n",
      "Iteration 28878, loss = 5.45331341\n",
      "Iteration 28879, loss = 4.69923169\n",
      "Iteration 28880, loss = 4.31364572\n",
      "Iteration 28881, loss = 3.72951355\n",
      "Iteration 28882, loss = 3.16719067\n",
      "Iteration 28883, loss = 3.05097557\n",
      "Iteration 28884, loss = 3.34102729\n",
      "Iteration 28885, loss = 3.67095692\n",
      "Iteration 28886, loss = 3.22438709\n",
      "Iteration 28887, loss = 3.42228035\n",
      "Iteration 28888, loss = 3.78379299\n",
      "Iteration 28889, loss = 3.45385489\n",
      "Iteration 28890, loss = 3.37771042\n",
      "Iteration 28891, loss = 3.52758086\n",
      "Iteration 28892, loss = 3.36944168\n",
      "Iteration 28893, loss = 3.87235846\n",
      "Iteration 28894, loss = 3.23783172\n",
      "Iteration 28895, loss = 3.47662221\n",
      "Iteration 28896, loss = 3.80563851\n",
      "Iteration 28897, loss = 4.26280677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 28898, loss = 5.35876629\n",
      "Iteration 28899, loss = 4.17161528\n",
      "Iteration 28900, loss = 3.66433226\n",
      "Iteration 28901, loss = 3.72765219\n",
      "Iteration 28902, loss = 4.08076344\n",
      "Iteration 28903, loss = 3.66715456\n",
      "Iteration 28904, loss = 3.87676088\n",
      "Iteration 28905, loss = 3.19065144\n",
      "Iteration 28906, loss = 4.21872406\n",
      "Iteration 28907, loss = 3.76235553\n",
      "Iteration 28908, loss = 3.57341553\n",
      "Iteration 28909, loss = 4.29660713\n",
      "Iteration 28910, loss = 5.03396379\n",
      "Iteration 28911, loss = 4.39086620\n",
      "Iteration 28912, loss = 3.44355919\n",
      "Iteration 28913, loss = 4.10824886\n",
      "Iteration 28914, loss = 4.56508737\n",
      "Iteration 28915, loss = 4.49138150\n",
      "Iteration 28916, loss = 4.01738769\n",
      "Iteration 28917, loss = 5.07281326\n",
      "Iteration 28918, loss = 5.71496663\n",
      "Iteration 28919, loss = 6.00154666\n",
      "Iteration 28920, loss = 6.21727229\n",
      "Iteration 28921, loss = 4.34264880\n",
      "Iteration 28922, loss = 4.37352357\n",
      "Iteration 28923, loss = 5.21746349\n",
      "Iteration 28924, loss = 4.61080262\n",
      "Iteration 28925, loss = 4.52202465\n",
      "Iteration 28926, loss = 3.68733293\n",
      "Iteration 28927, loss = 3.79376012\n",
      "Iteration 28928, loss = 3.73424826\n",
      "Iteration 28929, loss = 3.65430611\n",
      "Iteration 28930, loss = 3.10615371\n",
      "Iteration 28931, loss = 3.41459479\n",
      "Iteration 28932, loss = 3.61129304\n",
      "Iteration 28933, loss = 3.96424372\n",
      "Iteration 28934, loss = 4.10269908\n",
      "Iteration 28935, loss = 4.71421295\n",
      "Iteration 28936, loss = 4.58110455\n",
      "Iteration 28937, loss = 4.83384358\n",
      "Iteration 28938, loss = 4.57684255\n",
      "Iteration 28939, loss = 3.50960836\n",
      "Iteration 28940, loss = 3.61801003\n",
      "Iteration 28941, loss = 4.06056381\n",
      "Iteration 28942, loss = 4.39631120\n",
      "Iteration 28943, loss = 4.62179439\n",
      "Iteration 28944, loss = 3.88479304\n",
      "Iteration 28945, loss = 3.48461387\n",
      "Iteration 28946, loss = 3.72313707\n",
      "Iteration 28947, loss = 3.26365487\n",
      "Iteration 28948, loss = 3.27705825\n",
      "Iteration 28949, loss = 3.46957995\n",
      "Iteration 28950, loss = 3.21679363\n",
      "Iteration 28951, loss = 3.04108750\n",
      "Iteration 28952, loss = 3.18518162\n",
      "Iteration 28953, loss = 3.39397708\n",
      "Iteration 28954, loss = 3.51966623\n",
      "Iteration 28955, loss = 3.52230696\n",
      "Iteration 28956, loss = 3.49823948\n",
      "Iteration 28957, loss = 3.66546213\n",
      "Iteration 28958, loss = 4.28015827\n",
      "Iteration 28959, loss = 4.42516025\n",
      "Iteration 28960, loss = 3.50189517\n",
      "Iteration 28961, loss = 3.80266128\n",
      "Iteration 28962, loss = 3.93373530\n",
      "Iteration 28963, loss = 3.29194914\n",
      "Iteration 28964, loss = 3.38519152\n",
      "Iteration 28965, loss = 3.84350283\n",
      "Iteration 28966, loss = 3.57297461\n",
      "Iteration 28967, loss = 3.29517383\n",
      "Iteration 28968, loss = 3.16220792\n",
      "Iteration 28969, loss = 3.28433360\n",
      "Iteration 28970, loss = 3.58781462\n",
      "Iteration 28971, loss = 3.82798922\n",
      "Iteration 28972, loss = 3.70373131\n",
      "Iteration 28973, loss = 3.20103485\n",
      "Iteration 28974, loss = 3.27873264\n",
      "Iteration 28975, loss = 3.21310013\n",
      "Iteration 28976, loss = 3.10656182\n",
      "Iteration 28977, loss = 3.48531267\n",
      "Iteration 28978, loss = 3.29834039\n",
      "Iteration 28979, loss = 3.86486399\n",
      "Iteration 28980, loss = 3.24903039\n",
      "Iteration 28981, loss = 3.46993394\n",
      "Iteration 28982, loss = 3.56162329\n",
      "Iteration 28983, loss = 3.46796111\n",
      "Iteration 28984, loss = 3.27120851\n",
      "Iteration 28985, loss = 3.42055639\n",
      "Iteration 28986, loss = 3.20847655\n",
      "Iteration 28987, loss = 3.25844184\n",
      "Iteration 28988, loss = 3.79041234\n",
      "Iteration 28989, loss = 3.30863240\n",
      "Iteration 28990, loss = 3.38488228\n",
      "Iteration 28991, loss = 3.61079854\n",
      "Iteration 28992, loss = 3.40416414\n",
      "Iteration 28993, loss = 3.67501939\n",
      "Iteration 28994, loss = 3.10424320\n",
      "Iteration 28995, loss = 3.18325409\n",
      "Iteration 28996, loss = 3.85130996\n",
      "Iteration 28997, loss = 3.82641131\n",
      "Iteration 28998, loss = 3.84494967\n",
      "Iteration 28999, loss = 3.92888142\n",
      "Iteration 29000, loss = 4.47448500\n",
      "Iteration 29001, loss = 4.76670241\n",
      "Iteration 29002, loss = 3.79590259\n",
      "Iteration 29003, loss = 3.74048327\n",
      "Iteration 29004, loss = 4.01920828\n",
      "Iteration 29005, loss = 3.84571186\n",
      "Iteration 29006, loss = 3.49349675\n",
      "Iteration 29007, loss = 3.55363228\n",
      "Iteration 29008, loss = 3.95473720\n",
      "Iteration 29009, loss = 4.22441399\n",
      "Iteration 29010, loss = 3.71682497\n",
      "Iteration 29011, loss = 4.18002968\n",
      "Iteration 29012, loss = 4.46771799\n",
      "Iteration 29013, loss = 3.79457391\n",
      "Iteration 29014, loss = 4.90605212\n",
      "Iteration 29015, loss = 6.46312322\n",
      "Iteration 29016, loss = 6.83224611\n",
      "Iteration 29017, loss = 5.12617601\n",
      "Iteration 29018, loss = 4.85057766\n",
      "Iteration 29019, loss = 4.00573200\n",
      "Iteration 29020, loss = 4.72579429\n",
      "Iteration 29021, loss = 4.24002742\n",
      "Iteration 29022, loss = 3.65788981\n",
      "Iteration 29023, loss = 3.26918705\n",
      "Iteration 29024, loss = 3.76635888\n",
      "Iteration 29025, loss = 3.85022726\n",
      "Iteration 29026, loss = 3.89387936\n",
      "Iteration 29027, loss = 3.67396279\n",
      "Iteration 29028, loss = 3.45144189\n",
      "Iteration 29029, loss = 3.27995520\n",
      "Iteration 29030, loss = 3.58029460\n",
      "Iteration 29031, loss = 3.51670329\n",
      "Iteration 29032, loss = 3.57299291\n",
      "Iteration 29033, loss = 3.83222601\n",
      "Iteration 29034, loss = 4.15899007\n",
      "Iteration 29035, loss = 4.10162338\n",
      "Iteration 29036, loss = 3.94082350\n",
      "Iteration 29037, loss = 3.47827254\n",
      "Iteration 29038, loss = 3.26174718\n",
      "Iteration 29039, loss = 3.47301167\n",
      "Iteration 29040, loss = 3.25333649\n",
      "Iteration 29041, loss = 3.59362560\n",
      "Iteration 29042, loss = 3.32644432\n",
      "Iteration 29043, loss = 3.45204186\n",
      "Iteration 29044, loss = 3.40600756\n",
      "Iteration 29045, loss = 3.66023974\n",
      "Iteration 29046, loss = 3.29438395\n",
      "Iteration 29047, loss = 3.71330095\n",
      "Iteration 29048, loss = 3.94028216\n",
      "Iteration 29049, loss = 3.74157445\n",
      "Iteration 29050, loss = 3.65024150\n",
      "Iteration 29051, loss = 4.02320477\n",
      "Iteration 29052, loss = 3.75006107\n",
      "Iteration 29053, loss = 3.41293719\n",
      "Iteration 29054, loss = 4.02696231\n",
      "Iteration 29055, loss = 3.72530609\n",
      "Iteration 29056, loss = 3.52386799\n",
      "Iteration 29057, loss = 3.60129853\n",
      "Iteration 29058, loss = 3.50225157\n",
      "Iteration 29059, loss = 3.42504701\n",
      "Iteration 29060, loss = 3.41671208\n",
      "Iteration 29061, loss = 3.45542714\n",
      "Iteration 29062, loss = 3.22016572\n",
      "Iteration 29063, loss = 3.94389246\n",
      "Iteration 29064, loss = 3.56837384\n",
      "Iteration 29065, loss = 3.21280143\n",
      "Iteration 29066, loss = 3.26133446\n",
      "Iteration 29067, loss = 3.01750799\n",
      "Iteration 29068, loss = 3.13172277\n",
      "Iteration 29069, loss = 3.38259491\n",
      "Iteration 29070, loss = 3.30229313\n",
      "Iteration 29071, loss = 3.43200916\n",
      "Iteration 29072, loss = 4.60965789\n",
      "Iteration 29073, loss = 5.37661534\n",
      "Iteration 29074, loss = 3.82338993\n",
      "Iteration 29075, loss = 3.56681924\n",
      "Iteration 29076, loss = 3.41553659\n",
      "Iteration 29077, loss = 4.02468487\n",
      "Iteration 29078, loss = 3.49334914\n",
      "Iteration 29079, loss = 3.78854269\n",
      "Iteration 29080, loss = 5.19102571\n",
      "Iteration 29081, loss = 4.41263856\n",
      "Iteration 29082, loss = 4.03829347\n",
      "Iteration 29083, loss = 3.59488582\n",
      "Iteration 29084, loss = 3.22859773\n",
      "Iteration 29085, loss = 3.20686864\n",
      "Iteration 29086, loss = 3.72702035\n",
      "Iteration 29087, loss = 3.64517168\n",
      "Iteration 29088, loss = 3.59479135\n",
      "Iteration 29089, loss = 4.40235115\n",
      "Iteration 29090, loss = 4.05407948\n",
      "Iteration 29091, loss = 4.57800244\n",
      "Iteration 29092, loss = 4.12914126\n",
      "Iteration 29093, loss = 3.86860588\n",
      "Iteration 29094, loss = 3.59475848\n",
      "Iteration 29095, loss = 3.79610679\n",
      "Iteration 29096, loss = 3.49005018\n",
      "Iteration 29097, loss = 3.42574531\n",
      "Iteration 29098, loss = 3.23822871\n",
      "Iteration 29099, loss = 3.22306406\n",
      "Iteration 29100, loss = 3.76953027\n",
      "Iteration 29101, loss = 3.90611794\n",
      "Iteration 29102, loss = 3.56270524\n",
      "Iteration 29103, loss = 3.16346773\n",
      "Iteration 29104, loss = 3.40301769\n",
      "Iteration 29105, loss = 4.80311975\n",
      "Iteration 29106, loss = 4.85921099\n",
      "Iteration 29107, loss = 5.05994427\n",
      "Iteration 29108, loss = 4.16958486\n",
      "Iteration 29109, loss = 3.51303187\n",
      "Iteration 29110, loss = 3.59713796\n",
      "Iteration 29111, loss = 3.84346496\n",
      "Iteration 29112, loss = 4.46941273\n",
      "Iteration 29113, loss = 3.85367359\n",
      "Iteration 29114, loss = 4.31600473\n",
      "Iteration 29115, loss = 3.26089624\n",
      "Iteration 29116, loss = 3.93885319\n",
      "Iteration 29117, loss = 4.09469370\n",
      "Iteration 29118, loss = 4.05552177\n",
      "Iteration 29119, loss = 3.75900444\n",
      "Iteration 29120, loss = 3.23055164\n",
      "Iteration 29121, loss = 3.60437400\n",
      "Iteration 29122, loss = 3.79102996\n",
      "Iteration 29123, loss = 3.50690330\n",
      "Iteration 29124, loss = 3.52400268\n",
      "Iteration 29125, loss = 3.27512161\n",
      "Iteration 29126, loss = 3.73168185\n",
      "Iteration 29127, loss = 3.10889684\n",
      "Iteration 29128, loss = 3.09247071\n",
      "Iteration 29129, loss = 3.26953687\n",
      "Iteration 29130, loss = 3.52353864\n",
      "Iteration 29131, loss = 4.01337826\n",
      "Iteration 29132, loss = 3.46333560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29133, loss = 3.13459529\n",
      "Iteration 29134, loss = 3.66008168\n",
      "Iteration 29135, loss = 3.65304520\n",
      "Iteration 29136, loss = 3.66450979\n",
      "Iteration 29137, loss = 3.33869328\n",
      "Iteration 29138, loss = 4.04009224\n",
      "Iteration 29139, loss = 3.47271191\n",
      "Iteration 29140, loss = 4.23430389\n",
      "Iteration 29141, loss = 3.83872611\n",
      "Iteration 29142, loss = 3.57123115\n",
      "Iteration 29143, loss = 3.67252782\n",
      "Iteration 29144, loss = 3.27121417\n",
      "Iteration 29145, loss = 3.27556666\n",
      "Iteration 29146, loss = 3.43956203\n",
      "Iteration 29147, loss = 3.48148729\n",
      "Iteration 29148, loss = 3.67904075\n",
      "Iteration 29149, loss = 3.72681727\n",
      "Iteration 29150, loss = 5.49284446\n",
      "Iteration 29151, loss = 5.01667794\n",
      "Iteration 29152, loss = 3.79430669\n",
      "Iteration 29153, loss = 3.60306223\n",
      "Iteration 29154, loss = 3.36119011\n",
      "Iteration 29155, loss = 3.60759860\n",
      "Iteration 29156, loss = 3.97574950\n",
      "Iteration 29157, loss = 4.81888927\n",
      "Iteration 29158, loss = 5.52599162\n",
      "Iteration 29159, loss = 5.71689882\n",
      "Iteration 29160, loss = 4.80818030\n",
      "Iteration 29161, loss = 4.07414628\n",
      "Iteration 29162, loss = 4.65417833\n",
      "Iteration 29163, loss = 3.32710213\n",
      "Iteration 29164, loss = 3.75049916\n",
      "Iteration 29165, loss = 3.46289231\n",
      "Iteration 29166, loss = 3.46613860\n",
      "Iteration 29167, loss = 3.58324395\n",
      "Iteration 29168, loss = 3.81271752\n",
      "Iteration 29169, loss = 3.66500959\n",
      "Iteration 29170, loss = 3.68772157\n",
      "Iteration 29171, loss = 3.52325855\n",
      "Iteration 29172, loss = 3.47153728\n",
      "Iteration 29173, loss = 4.00504486\n",
      "Iteration 29174, loss = 3.80902881\n",
      "Iteration 29175, loss = 4.36310396\n",
      "Iteration 29176, loss = 3.97790843\n",
      "Iteration 29177, loss = 3.50684705\n",
      "Iteration 29178, loss = 3.55811588\n",
      "Iteration 29179, loss = 4.03172944\n",
      "Iteration 29180, loss = 3.55058590\n",
      "Iteration 29181, loss = 3.93392441\n",
      "Iteration 29182, loss = 4.14950076\n",
      "Iteration 29183, loss = 3.67144099\n",
      "Iteration 29184, loss = 3.94479155\n",
      "Iteration 29185, loss = 3.88634663\n",
      "Iteration 29186, loss = 3.28503726\n",
      "Iteration 29187, loss = 3.85901069\n",
      "Iteration 29188, loss = 3.02056994\n",
      "Iteration 29189, loss = 3.36346619\n",
      "Iteration 29190, loss = 3.04629309\n",
      "Iteration 29191, loss = 3.12093220\n",
      "Iteration 29192, loss = 3.44781271\n",
      "Iteration 29193, loss = 3.66645814\n",
      "Iteration 29194, loss = 3.49990679\n",
      "Iteration 29195, loss = 3.30534912\n",
      "Iteration 29196, loss = 3.36015291\n",
      "Iteration 29197, loss = 3.58567391\n",
      "Iteration 29198, loss = 3.44071815\n",
      "Iteration 29199, loss = 3.63452839\n",
      "Iteration 29200, loss = 3.95966202\n",
      "Iteration 29201, loss = 4.19709455\n",
      "Iteration 29202, loss = 4.30064612\n",
      "Iteration 29203, loss = 3.92996150\n",
      "Iteration 29204, loss = 3.88975109\n",
      "Iteration 29205, loss = 3.28984679\n",
      "Iteration 29206, loss = 3.12342385\n",
      "Iteration 29207, loss = 3.29987900\n",
      "Iteration 29208, loss = 3.19631712\n",
      "Iteration 29209, loss = 3.41552741\n",
      "Iteration 29210, loss = 3.40847603\n",
      "Iteration 29211, loss = 3.39113402\n",
      "Iteration 29212, loss = 3.09118250\n",
      "Iteration 29213, loss = 3.26795743\n",
      "Iteration 29214, loss = 3.25515270\n",
      "Iteration 29215, loss = 4.40441577\n",
      "Iteration 29216, loss = 4.48028059\n",
      "Iteration 29217, loss = 4.45400191\n",
      "Iteration 29218, loss = 4.09202523\n",
      "Iteration 29219, loss = 4.24008406\n",
      "Iteration 29220, loss = 4.13645066\n",
      "Iteration 29221, loss = 3.40184367\n",
      "Iteration 29222, loss = 3.83927607\n",
      "Iteration 29223, loss = 3.28711049\n",
      "Iteration 29224, loss = 3.61101486\n",
      "Iteration 29225, loss = 3.65329269\n",
      "Iteration 29226, loss = 3.24124381\n",
      "Iteration 29227, loss = 3.61540289\n",
      "Iteration 29228, loss = 3.71116513\n",
      "Iteration 29229, loss = 3.61664915\n",
      "Iteration 29230, loss = 3.59793056\n",
      "Iteration 29231, loss = 3.11005655\n",
      "Iteration 29232, loss = 3.67796073\n",
      "Iteration 29233, loss = 3.91034548\n",
      "Iteration 29234, loss = 3.38095677\n",
      "Iteration 29235, loss = 3.71647178\n",
      "Iteration 29236, loss = 3.34048279\n",
      "Iteration 29237, loss = 3.77409454\n",
      "Iteration 29238, loss = 5.36585374\n",
      "Iteration 29239, loss = 5.39986447\n",
      "Iteration 29240, loss = 5.09769879\n",
      "Iteration 29241, loss = 4.31914695\n",
      "Iteration 29242, loss = 4.09285922\n",
      "Iteration 29243, loss = 3.95411651\n",
      "Iteration 29244, loss = 3.69450430\n",
      "Iteration 29245, loss = 3.24708763\n",
      "Iteration 29246, loss = 3.30746378\n",
      "Iteration 29247, loss = 3.48354713\n",
      "Iteration 29248, loss = 3.32354159\n",
      "Iteration 29249, loss = 3.86180246\n",
      "Iteration 29250, loss = 4.09309635\n",
      "Iteration 29251, loss = 3.46714857\n",
      "Iteration 29252, loss = 4.00726450\n",
      "Iteration 29253, loss = 3.76351013\n",
      "Iteration 29254, loss = 2.99224077\n",
      "Iteration 29255, loss = 3.25380592\n",
      "Iteration 29256, loss = 3.61809440\n",
      "Iteration 29257, loss = 3.38128480\n",
      "Iteration 29258, loss = 3.50893903\n",
      "Iteration 29259, loss = 3.15826153\n",
      "Iteration 29260, loss = 3.36973898\n",
      "Iteration 29261, loss = 3.30481812\n",
      "Iteration 29262, loss = 3.72487902\n",
      "Iteration 29263, loss = 3.28187084\n",
      "Iteration 29264, loss = 3.27926166\n",
      "Iteration 29265, loss = 3.91206471\n",
      "Iteration 29266, loss = 3.24609429\n",
      "Iteration 29267, loss = 3.74903328\n",
      "Iteration 29268, loss = 3.27860857\n",
      "Iteration 29269, loss = 3.73389745\n",
      "Iteration 29270, loss = 3.92495837\n",
      "Iteration 29271, loss = 4.01927159\n",
      "Iteration 29272, loss = 3.56151808\n",
      "Iteration 29273, loss = 4.28797768\n",
      "Iteration 29274, loss = 5.14973131\n",
      "Iteration 29275, loss = 6.65359655\n",
      "Iteration 29276, loss = 5.12904633\n",
      "Iteration 29277, loss = 3.71320201\n",
      "Iteration 29278, loss = 3.33278103\n",
      "Iteration 29279, loss = 4.05306765\n",
      "Iteration 29280, loss = 3.62262438\n",
      "Iteration 29281, loss = 3.49611308\n",
      "Iteration 29282, loss = 3.19489711\n",
      "Iteration 29283, loss = 3.18642664\n",
      "Iteration 29284, loss = 3.35862108\n",
      "Iteration 29285, loss = 3.35094993\n",
      "Iteration 29286, loss = 3.41349165\n",
      "Iteration 29287, loss = 3.52624708\n",
      "Iteration 29288, loss = 3.37243768\n",
      "Iteration 29289, loss = 3.23503876\n",
      "Iteration 29290, loss = 3.19342267\n",
      "Iteration 29291, loss = 4.28353579\n",
      "Iteration 29292, loss = 5.11204540\n",
      "Iteration 29293, loss = 4.38237429\n",
      "Iteration 29294, loss = 4.63496905\n",
      "Iteration 29295, loss = 4.21596753\n",
      "Iteration 29296, loss = 4.06933393\n",
      "Iteration 29297, loss = 3.23225717\n",
      "Iteration 29298, loss = 3.25985661\n",
      "Iteration 29299, loss = 3.43060322\n",
      "Iteration 29300, loss = 3.43667998\n",
      "Iteration 29301, loss = 3.63086111\n",
      "Iteration 29302, loss = 4.13904427\n",
      "Iteration 29303, loss = 7.15001192\n",
      "Iteration 29304, loss = 6.39429599\n",
      "Iteration 29305, loss = 5.30642839\n",
      "Iteration 29306, loss = 3.50290594\n",
      "Iteration 29307, loss = 3.38507464\n",
      "Iteration 29308, loss = 3.23351402\n",
      "Iteration 29309, loss = 3.40972311\n",
      "Iteration 29310, loss = 4.08875683\n",
      "Iteration 29311, loss = 3.67172766\n",
      "Iteration 29312, loss = 3.87251450\n",
      "Iteration 29313, loss = 3.44914218\n",
      "Iteration 29314, loss = 3.78544100\n",
      "Iteration 29315, loss = 3.54610501\n",
      "Iteration 29316, loss = 3.80114768\n",
      "Iteration 29317, loss = 3.76886952\n",
      "Iteration 29318, loss = 3.34690122\n",
      "Iteration 29319, loss = 3.61430669\n",
      "Iteration 29320, loss = 3.67891463\n",
      "Iteration 29321, loss = 3.57251684\n",
      "Iteration 29322, loss = 3.78092158\n",
      "Iteration 29323, loss = 3.48419703\n",
      "Iteration 29324, loss = 3.22615194\n",
      "Iteration 29325, loss = 3.25047496\n",
      "Iteration 29326, loss = 3.92555359\n",
      "Iteration 29327, loss = 3.38794766\n",
      "Iteration 29328, loss = 3.45135916\n",
      "Iteration 29329, loss = 3.48559811\n",
      "Iteration 29330, loss = 3.08849578\n",
      "Iteration 29331, loss = 4.00964179\n",
      "Iteration 29332, loss = 6.05203528\n",
      "Iteration 29333, loss = 5.53818952\n",
      "Iteration 29334, loss = 4.96159497\n",
      "Iteration 29335, loss = 4.36812010\n",
      "Iteration 29336, loss = 4.19750386\n",
      "Iteration 29337, loss = 3.37934638\n",
      "Iteration 29338, loss = 3.49703314\n",
      "Iteration 29339, loss = 3.18497446\n",
      "Iteration 29340, loss = 3.97679076\n",
      "Iteration 29341, loss = 4.51403731\n",
      "Iteration 29342, loss = 4.45054381\n",
      "Iteration 29343, loss = 3.51731898\n",
      "Iteration 29344, loss = 3.49008504\n",
      "Iteration 29345, loss = 4.21890057\n",
      "Iteration 29346, loss = 3.41175771\n",
      "Iteration 29347, loss = 3.49932033\n",
      "Iteration 29348, loss = 3.27265786\n",
      "Iteration 29349, loss = 3.04585707\n",
      "Iteration 29350, loss = 3.17617706\n",
      "Iteration 29351, loss = 3.00782881\n",
      "Iteration 29352, loss = 3.21125954\n",
      "Iteration 29353, loss = 3.10335573\n",
      "Iteration 29354, loss = 3.18914686\n",
      "Iteration 29355, loss = 3.20665007\n",
      "Iteration 29356, loss = 3.50625681\n",
      "Iteration 29357, loss = 3.28559967\n",
      "Iteration 29358, loss = 3.32806464\n",
      "Iteration 29359, loss = 3.05351029\n",
      "Iteration 29360, loss = 3.53517211\n",
      "Iteration 29361, loss = 3.26787845\n",
      "Iteration 29362, loss = 3.33867893\n",
      "Iteration 29363, loss = 3.67830424\n",
      "Iteration 29364, loss = 3.24224027\n",
      "Iteration 29365, loss = 3.25344768\n",
      "Iteration 29366, loss = 3.57353382\n",
      "Iteration 29367, loss = 3.28955346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29368, loss = 3.59099955\n",
      "Iteration 29369, loss = 4.67465598\n",
      "Iteration 29370, loss = 3.77656205\n",
      "Iteration 29371, loss = 3.62846669\n",
      "Iteration 29372, loss = 4.40437572\n",
      "Iteration 29373, loss = 4.32969890\n",
      "Iteration 29374, loss = 3.56873553\n",
      "Iteration 29375, loss = 3.82847063\n",
      "Iteration 29376, loss = 3.70150049\n",
      "Iteration 29377, loss = 3.05490084\n",
      "Iteration 29378, loss = 3.29283590\n",
      "Iteration 29379, loss = 2.99212735\n",
      "Iteration 29380, loss = 3.05336411\n",
      "Iteration 29381, loss = 2.93769093\n",
      "Iteration 29382, loss = 3.65166829\n",
      "Iteration 29383, loss = 3.52231339\n",
      "Iteration 29384, loss = 3.91108269\n",
      "Iteration 29385, loss = 3.47894470\n",
      "Iteration 29386, loss = 3.14758328\n",
      "Iteration 29387, loss = 3.32307093\n",
      "Iteration 29388, loss = 3.33125997\n",
      "Iteration 29389, loss = 3.31982682\n",
      "Iteration 29390, loss = 3.68336940\n",
      "Iteration 29391, loss = 4.96458826\n",
      "Iteration 29392, loss = 4.69028570\n",
      "Iteration 29393, loss = 5.33875120\n",
      "Iteration 29394, loss = 4.47656753\n",
      "Iteration 29395, loss = 3.91654969\n",
      "Iteration 29396, loss = 3.55344090\n",
      "Iteration 29397, loss = 4.19080672\n",
      "Iteration 29398, loss = 4.75397739\n",
      "Iteration 29399, loss = 4.96379566\n",
      "Iteration 29400, loss = 4.99319317\n",
      "Iteration 29401, loss = 4.91393620\n",
      "Iteration 29402, loss = 5.00257384\n",
      "Iteration 29403, loss = 4.71267834\n",
      "Iteration 29404, loss = 4.20863272\n",
      "Iteration 29405, loss = 4.13704758\n",
      "Iteration 29406, loss = 4.09937746\n",
      "Iteration 29407, loss = 3.82618062\n",
      "Iteration 29408, loss = 4.63943169\n",
      "Iteration 29409, loss = 3.56618795\n",
      "Iteration 29410, loss = 3.95395123\n",
      "Iteration 29411, loss = 3.18812030\n",
      "Iteration 29412, loss = 3.12281875\n",
      "Iteration 29413, loss = 3.31910364\n",
      "Iteration 29414, loss = 3.16135941\n",
      "Iteration 29415, loss = 3.79238344\n",
      "Iteration 29416, loss = 3.41839425\n",
      "Iteration 29417, loss = 3.24940336\n",
      "Iteration 29418, loss = 3.22543347\n",
      "Iteration 29419, loss = 3.16022766\n",
      "Iteration 29420, loss = 2.94437301\n",
      "Iteration 29421, loss = 2.92013219\n",
      "Iteration 29422, loss = 3.11001685\n",
      "Iteration 29423, loss = 3.27776184\n",
      "Iteration 29424, loss = 3.27847370\n",
      "Iteration 29425, loss = 3.23546968\n",
      "Iteration 29426, loss = 3.12347579\n",
      "Iteration 29427, loss = 4.66019140\n",
      "Iteration 29428, loss = 4.47556416\n",
      "Iteration 29429, loss = 3.98167462\n",
      "Iteration 29430, loss = 4.02891320\n",
      "Iteration 29431, loss = 3.55994764\n",
      "Iteration 29432, loss = 3.47247078\n",
      "Iteration 29433, loss = 3.58459839\n",
      "Iteration 29434, loss = 3.24381198\n",
      "Iteration 29435, loss = 3.31893020\n",
      "Iteration 29436, loss = 3.77539992\n",
      "Iteration 29437, loss = 3.31154188\n",
      "Iteration 29438, loss = 3.63267003\n",
      "Iteration 29439, loss = 3.29822018\n",
      "Iteration 29440, loss = 3.66774005\n",
      "Iteration 29441, loss = 3.35395737\n",
      "Iteration 29442, loss = 3.59550031\n",
      "Iteration 29443, loss = 4.26939112\n",
      "Iteration 29444, loss = 4.62678077\n",
      "Iteration 29445, loss = 4.39456049\n",
      "Iteration 29446, loss = 4.20911261\n",
      "Iteration 29447, loss = 4.73882137\n",
      "Iteration 29448, loss = 3.40636454\n",
      "Iteration 29449, loss = 3.57987923\n",
      "Iteration 29450, loss = 3.23247173\n",
      "Iteration 29451, loss = 3.43124947\n",
      "Iteration 29452, loss = 3.55478066\n",
      "Iteration 29453, loss = 3.85716890\n",
      "Iteration 29454, loss = 4.06993898\n",
      "Iteration 29455, loss = 3.77797758\n",
      "Iteration 29456, loss = 3.45244952\n",
      "Iteration 29457, loss = 3.35215246\n",
      "Iteration 29458, loss = 3.93270583\n",
      "Iteration 29459, loss = 6.38497820\n",
      "Iteration 29460, loss = 4.90301748\n",
      "Iteration 29461, loss = 5.93711431\n",
      "Iteration 29462, loss = 5.07297906\n",
      "Iteration 29463, loss = 4.57059001\n",
      "Iteration 29464, loss = 4.21338761\n",
      "Iteration 29465, loss = 5.24719730\n",
      "Iteration 29466, loss = 5.90689602\n",
      "Iteration 29467, loss = 5.50654558\n",
      "Iteration 29468, loss = 5.81719994\n",
      "Iteration 29469, loss = 6.73498551\n",
      "Iteration 29470, loss = 6.22083724\n",
      "Iteration 29471, loss = 5.54902796\n",
      "Iteration 29472, loss = 6.17037937\n",
      "Iteration 29473, loss = 5.73401474\n",
      "Iteration 29474, loss = 5.10339233\n",
      "Iteration 29475, loss = 4.43706623\n",
      "Iteration 29476, loss = 3.73304231\n",
      "Iteration 29477, loss = 3.57262774\n",
      "Iteration 29478, loss = 3.15093986\n",
      "Iteration 29479, loss = 3.23718603\n",
      "Iteration 29480, loss = 3.27105509\n",
      "Iteration 29481, loss = 3.33426151\n",
      "Iteration 29482, loss = 3.16684584\n",
      "Iteration 29483, loss = 3.30309007\n",
      "Iteration 29484, loss = 3.17367947\n",
      "Iteration 29485, loss = 3.27505911\n",
      "Iteration 29486, loss = 3.12859136\n",
      "Iteration 29487, loss = 3.41710733\n",
      "Iteration 29488, loss = 3.21029324\n",
      "Iteration 29489, loss = 3.27667245\n",
      "Iteration 29490, loss = 3.86207428\n",
      "Iteration 29491, loss = 3.93078036\n",
      "Iteration 29492, loss = 3.79097014\n",
      "Iteration 29493, loss = 4.32098295\n",
      "Iteration 29494, loss = 4.30331639\n",
      "Iteration 29495, loss = 3.65514690\n",
      "Iteration 29496, loss = 4.14207361\n",
      "Iteration 29497, loss = 4.08769642\n",
      "Iteration 29498, loss = 4.01946682\n",
      "Iteration 29499, loss = 4.01680536\n",
      "Iteration 29500, loss = 4.07444279\n",
      "Iteration 29501, loss = 3.29656812\n",
      "Iteration 29502, loss = 3.15650945\n",
      "Iteration 29503, loss = 3.20859973\n",
      "Iteration 29504, loss = 3.24397353\n",
      "Iteration 29505, loss = 3.64915199\n",
      "Iteration 29506, loss = 3.66027783\n",
      "Iteration 29507, loss = 3.10237316\n",
      "Iteration 29508, loss = 3.39455125\n",
      "Iteration 29509, loss = 3.39018937\n",
      "Iteration 29510, loss = 4.23791956\n",
      "Iteration 29511, loss = 3.90109362\n",
      "Iteration 29512, loss = 3.80723377\n",
      "Iteration 29513, loss = 3.33092593\n",
      "Iteration 29514, loss = 3.15508997\n",
      "Iteration 29515, loss = 3.08757814\n",
      "Iteration 29516, loss = 3.40020967\n",
      "Iteration 29517, loss = 4.57228205\n",
      "Iteration 29518, loss = 3.95121511\n",
      "Iteration 29519, loss = 3.86129837\n",
      "Iteration 29520, loss = 3.36255503\n",
      "Iteration 29521, loss = 3.32691404\n",
      "Iteration 29522, loss = 3.16008935\n",
      "Iteration 29523, loss = 3.35591843\n",
      "Iteration 29524, loss = 3.39283941\n",
      "Iteration 29525, loss = 4.22603664\n",
      "Iteration 29526, loss = 3.55123124\n",
      "Iteration 29527, loss = 3.68180314\n",
      "Iteration 29528, loss = 3.54560670\n",
      "Iteration 29529, loss = 3.25919833\n",
      "Iteration 29530, loss = 3.89755680\n",
      "Iteration 29531, loss = 3.76830759\n",
      "Iteration 29532, loss = 6.38890025\n",
      "Iteration 29533, loss = 5.01375262\n",
      "Iteration 29534, loss = 5.49229642\n",
      "Iteration 29535, loss = 6.96193653\n",
      "Iteration 29536, loss = 6.29510125\n",
      "Iteration 29537, loss = 5.25130638\n",
      "Iteration 29538, loss = 4.18132564\n",
      "Iteration 29539, loss = 4.28815544\n",
      "Iteration 29540, loss = 3.93532814\n",
      "Iteration 29541, loss = 4.66788400\n",
      "Iteration 29542, loss = 3.36425167\n",
      "Iteration 29543, loss = 3.31917003\n",
      "Iteration 29544, loss = 3.46949560\n",
      "Iteration 29545, loss = 3.25272134\n",
      "Iteration 29546, loss = 3.14264720\n",
      "Iteration 29547, loss = 3.23162354\n",
      "Iteration 29548, loss = 3.43257847\n",
      "Iteration 29549, loss = 3.08838702\n",
      "Iteration 29550, loss = 3.99957897\n",
      "Iteration 29551, loss = 4.10575733\n",
      "Iteration 29552, loss = 5.35148336\n",
      "Iteration 29553, loss = 4.05362395\n",
      "Iteration 29554, loss = 3.69303147\n",
      "Iteration 29555, loss = 3.80026090\n",
      "Iteration 29556, loss = 3.55827658\n",
      "Iteration 29557, loss = 3.57761750\n",
      "Iteration 29558, loss = 3.29043573\n",
      "Iteration 29559, loss = 5.30544332\n",
      "Iteration 29560, loss = 4.18661941\n",
      "Iteration 29561, loss = 4.01839244\n",
      "Iteration 29562, loss = 3.85653645\n",
      "Iteration 29563, loss = 3.75376815\n",
      "Iteration 29564, loss = 3.69480924\n",
      "Iteration 29565, loss = 3.56612738\n",
      "Iteration 29566, loss = 3.72854686\n",
      "Iteration 29567, loss = 4.08958365\n",
      "Iteration 29568, loss = 3.56204912\n",
      "Iteration 29569, loss = 3.07348169\n",
      "Iteration 29570, loss = 3.28963027\n",
      "Iteration 29571, loss = 3.06555122\n",
      "Iteration 29572, loss = 2.92302767\n",
      "Iteration 29573, loss = 3.10398389\n",
      "Iteration 29574, loss = 3.03986181\n",
      "Iteration 29575, loss = 3.51042667\n",
      "Iteration 29576, loss = 4.09641804\n",
      "Iteration 29577, loss = 3.75039953\n",
      "Iteration 29578, loss = 3.53982596\n",
      "Iteration 29579, loss = 3.58209278\n",
      "Iteration 29580, loss = 3.82463958\n",
      "Iteration 29581, loss = 3.90083082\n",
      "Iteration 29582, loss = 3.75836884\n",
      "Iteration 29583, loss = 3.70282655\n",
      "Iteration 29584, loss = 3.52055867\n",
      "Iteration 29585, loss = 4.08973038\n",
      "Iteration 29586, loss = 3.43891441\n",
      "Iteration 29587, loss = 3.09454041\n",
      "Iteration 29588, loss = 3.25000856\n",
      "Iteration 29589, loss = 3.33623872\n",
      "Iteration 29590, loss = 3.31304852\n",
      "Iteration 29591, loss = 3.40502216\n",
      "Iteration 29592, loss = 3.19130692\n",
      "Iteration 29593, loss = 3.02958576\n",
      "Iteration 29594, loss = 3.11000667\n",
      "Iteration 29595, loss = 3.07847676\n",
      "Iteration 29596, loss = 3.05924888\n",
      "Iteration 29597, loss = 3.09594513\n",
      "Iteration 29598, loss = 3.03443662\n",
      "Iteration 29599, loss = 4.31421423\n",
      "Iteration 29600, loss = 3.72861573\n",
      "Iteration 29601, loss = 3.33657288\n",
      "Iteration 29602, loss = 3.52350336\n",
      "Iteration 29603, loss = 3.25364019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29604, loss = 3.32533892\n",
      "Iteration 29605, loss = 4.04422140\n",
      "Iteration 29606, loss = 3.50726286\n",
      "Iteration 29607, loss = 3.43645945\n",
      "Iteration 29608, loss = 3.62470834\n",
      "Iteration 29609, loss = 3.32968430\n",
      "Iteration 29610, loss = 3.72505698\n",
      "Iteration 29611, loss = 3.95309648\n",
      "Iteration 29612, loss = 3.57491594\n",
      "Iteration 29613, loss = 3.37150941\n",
      "Iteration 29614, loss = 2.96366646\n",
      "Iteration 29615, loss = 3.05825400\n",
      "Iteration 29616, loss = 3.02764424\n",
      "Iteration 29617, loss = 3.09229440\n",
      "Iteration 29618, loss = 3.18770484\n",
      "Iteration 29619, loss = 3.00955324\n",
      "Iteration 29620, loss = 2.93625801\n",
      "Iteration 29621, loss = 3.15178972\n",
      "Iteration 29622, loss = 3.83423341\n",
      "Iteration 29623, loss = 4.16666072\n",
      "Iteration 29624, loss = 3.71930226\n",
      "Iteration 29625, loss = 3.68104066\n",
      "Iteration 29626, loss = 3.49614330\n",
      "Iteration 29627, loss = 3.61971409\n",
      "Iteration 29628, loss = 3.51287937\n",
      "Iteration 29629, loss = 3.88487766\n",
      "Iteration 29630, loss = 3.63661043\n",
      "Iteration 29631, loss = 4.28492229\n",
      "Iteration 29632, loss = 3.60935395\n",
      "Iteration 29633, loss = 3.28867294\n",
      "Iteration 29634, loss = 3.44112666\n",
      "Iteration 29635, loss = 3.09279382\n",
      "Iteration 29636, loss = 3.13475196\n",
      "Iteration 29637, loss = 3.53724912\n",
      "Iteration 29638, loss = 3.42578916\n",
      "Iteration 29639, loss = 3.84106911\n",
      "Iteration 29640, loss = 3.71040395\n",
      "Iteration 29641, loss = 3.59685256\n",
      "Iteration 29642, loss = 3.67197423\n",
      "Iteration 29643, loss = 3.48700611\n",
      "Iteration 29644, loss = 3.71251878\n",
      "Iteration 29645, loss = 4.04368443\n",
      "Iteration 29646, loss = 4.03829124\n",
      "Iteration 29647, loss = 4.71078078\n",
      "Iteration 29648, loss = 4.19233385\n",
      "Iteration 29649, loss = 4.60525061\n",
      "Iteration 29650, loss = 3.84065040\n",
      "Iteration 29651, loss = 3.05807476\n",
      "Iteration 29652, loss = 3.39849924\n",
      "Iteration 29653, loss = 3.26040459\n",
      "Iteration 29654, loss = 3.02771153\n",
      "Iteration 29655, loss = 3.08908857\n",
      "Iteration 29656, loss = 5.12727277\n",
      "Iteration 29657, loss = 4.03836864\n",
      "Iteration 29658, loss = 4.12552138\n",
      "Iteration 29659, loss = 3.76351858\n",
      "Iteration 29660, loss = 4.10964416\n",
      "Iteration 29661, loss = 3.70490526\n",
      "Iteration 29662, loss = 5.44283297\n",
      "Iteration 29663, loss = 4.61961608\n",
      "Iteration 29664, loss = 4.76993068\n",
      "Iteration 29665, loss = 4.33269096\n",
      "Iteration 29666, loss = 4.95229185\n",
      "Iteration 29667, loss = 4.34529754\n",
      "Iteration 29668, loss = 4.15231000\n",
      "Iteration 29669, loss = 3.16199307\n",
      "Iteration 29670, loss = 3.47607525\n",
      "Iteration 29671, loss = 3.46221429\n",
      "Iteration 29672, loss = 4.56530196\n",
      "Iteration 29673, loss = 4.33776518\n",
      "Iteration 29674, loss = 4.68903425\n",
      "Iteration 29675, loss = 5.43781075\n",
      "Iteration 29676, loss = 4.69161008\n",
      "Iteration 29677, loss = 4.73165343\n",
      "Iteration 29678, loss = 4.97166185\n",
      "Iteration 29679, loss = 3.64113132\n",
      "Iteration 29680, loss = 3.94566396\n",
      "Iteration 29681, loss = 4.44049605\n",
      "Iteration 29682, loss = 4.09807047\n",
      "Iteration 29683, loss = 3.84030510\n",
      "Iteration 29684, loss = 3.45324467\n",
      "Iteration 29685, loss = 3.15159805\n",
      "Iteration 29686, loss = 3.02046744\n",
      "Iteration 29687, loss = 3.21399599\n",
      "Iteration 29688, loss = 3.66238037\n",
      "Iteration 29689, loss = 3.38878107\n",
      "Iteration 29690, loss = 3.50634019\n",
      "Iteration 29691, loss = 3.39477639\n",
      "Iteration 29692, loss = 3.37301684\n",
      "Iteration 29693, loss = 3.10886618\n",
      "Iteration 29694, loss = 3.32909291\n",
      "Iteration 29695, loss = 3.08847540\n",
      "Iteration 29696, loss = 3.03892988\n",
      "Iteration 29697, loss = 3.24248680\n",
      "Iteration 29698, loss = 3.03211592\n",
      "Iteration 29699, loss = 3.59631869\n",
      "Iteration 29700, loss = 3.77833977\n",
      "Iteration 29701, loss = 4.09158695\n",
      "Iteration 29702, loss = 4.30255703\n",
      "Iteration 29703, loss = 3.96654723\n",
      "Iteration 29704, loss = 4.03838688\n",
      "Iteration 29705, loss = 3.87379878\n",
      "Iteration 29706, loss = 3.96152967\n",
      "Iteration 29707, loss = 3.81839988\n",
      "Iteration 29708, loss = 3.79406967\n",
      "Iteration 29709, loss = 4.47629569\n",
      "Iteration 29710, loss = 3.48673468\n",
      "Iteration 29711, loss = 3.50159311\n",
      "Iteration 29712, loss = 4.39587590\n",
      "Iteration 29713, loss = 4.52188323\n",
      "Iteration 29714, loss = 3.40933617\n",
      "Iteration 29715, loss = 3.40913662\n",
      "Iteration 29716, loss = 2.98216270\n",
      "Iteration 29717, loss = 3.38370086\n",
      "Iteration 29718, loss = 3.62264644\n",
      "Iteration 29719, loss = 3.27766876\n",
      "Iteration 29720, loss = 3.48330970\n",
      "Iteration 29721, loss = 4.08940027\n",
      "Iteration 29722, loss = 4.29179520\n",
      "Iteration 29723, loss = 3.75361742\n",
      "Iteration 29724, loss = 3.40178197\n",
      "Iteration 29725, loss = 3.24837021\n",
      "Iteration 29726, loss = 3.47816460\n",
      "Iteration 29727, loss = 4.80520342\n",
      "Iteration 29728, loss = 5.21790090\n",
      "Iteration 29729, loss = 4.17940133\n",
      "Iteration 29730, loss = 4.14013863\n",
      "Iteration 29731, loss = 3.81056030\n",
      "Iteration 29732, loss = 4.04522569\n",
      "Iteration 29733, loss = 4.98134001\n",
      "Iteration 29734, loss = 4.56579994\n",
      "Iteration 29735, loss = 4.61984536\n",
      "Iteration 29736, loss = 3.42510434\n",
      "Iteration 29737, loss = 3.17059479\n",
      "Iteration 29738, loss = 3.25981958\n",
      "Iteration 29739, loss = 3.18794571\n",
      "Iteration 29740, loss = 3.15096494\n",
      "Iteration 29741, loss = 3.30568639\n",
      "Iteration 29742, loss = 3.75625787\n",
      "Iteration 29743, loss = 3.65459389\n",
      "Iteration 29744, loss = 3.42144684\n",
      "Iteration 29745, loss = 3.81668656\n",
      "Iteration 29746, loss = 3.16730141\n",
      "Iteration 29747, loss = 2.95859119\n",
      "Iteration 29748, loss = 3.13018288\n",
      "Iteration 29749, loss = 3.52377159\n",
      "Iteration 29750, loss = 3.84244340\n",
      "Iteration 29751, loss = 3.85619407\n",
      "Iteration 29752, loss = 3.13609564\n",
      "Iteration 29753, loss = 3.15749681\n",
      "Iteration 29754, loss = 3.24411866\n",
      "Iteration 29755, loss = 3.44680013\n",
      "Iteration 29756, loss = 3.59371770\n",
      "Iteration 29757, loss = 3.73210343\n",
      "Iteration 29758, loss = 4.22413396\n",
      "Iteration 29759, loss = 3.77934268\n",
      "Iteration 29760, loss = 3.22783344\n",
      "Iteration 29761, loss = 3.62903616\n",
      "Iteration 29762, loss = 3.38216815\n",
      "Iteration 29763, loss = 3.22558314\n",
      "Iteration 29764, loss = 3.17652697\n",
      "Iteration 29765, loss = 4.04165013\n",
      "Iteration 29766, loss = 3.86881056\n",
      "Iteration 29767, loss = 4.01252280\n",
      "Iteration 29768, loss = 3.45153041\n",
      "Iteration 29769, loss = 3.32534931\n",
      "Iteration 29770, loss = 3.27698811\n",
      "Iteration 29771, loss = 3.40534586\n",
      "Iteration 29772, loss = 3.22806870\n",
      "Iteration 29773, loss = 3.54439201\n",
      "Iteration 29774, loss = 3.66714300\n",
      "Iteration 29775, loss = 3.56401272\n",
      "Iteration 29776, loss = 3.36455940\n",
      "Iteration 29777, loss = 3.11327890\n",
      "Iteration 29778, loss = 3.58828110\n",
      "Iteration 29779, loss = 3.93801328\n",
      "Iteration 29780, loss = 3.71565229\n",
      "Iteration 29781, loss = 3.80165104\n",
      "Iteration 29782, loss = 3.50205399\n",
      "Iteration 29783, loss = 3.13557063\n",
      "Iteration 29784, loss = 3.24946931\n",
      "Iteration 29785, loss = 3.76757340\n",
      "Iteration 29786, loss = 3.23320813\n",
      "Iteration 29787, loss = 3.19378989\n",
      "Iteration 29788, loss = 3.69051221\n",
      "Iteration 29789, loss = 3.52216413\n",
      "Iteration 29790, loss = 3.86124456\n",
      "Iteration 29791, loss = 4.69574557\n",
      "Iteration 29792, loss = 3.81841523\n",
      "Iteration 29793, loss = 3.08663333\n",
      "Iteration 29794, loss = 2.92511961\n",
      "Iteration 29795, loss = 3.21415350\n",
      "Iteration 29796, loss = 3.29146876\n",
      "Iteration 29797, loss = 3.94287078\n",
      "Iteration 29798, loss = 4.12654367\n",
      "Iteration 29799, loss = 3.12682503\n",
      "Iteration 29800, loss = 3.08366030\n",
      "Iteration 29801, loss = 3.29065060\n",
      "Iteration 29802, loss = 3.43318150\n",
      "Iteration 29803, loss = 3.75653093\n",
      "Iteration 29804, loss = 3.36095063\n",
      "Iteration 29805, loss = 3.29220553\n",
      "Iteration 29806, loss = 3.34456701\n",
      "Iteration 29807, loss = 3.16867571\n",
      "Iteration 29808, loss = 3.20666737\n",
      "Iteration 29809, loss = 3.76947185\n",
      "Iteration 29810, loss = 3.44597831\n",
      "Iteration 29811, loss = 3.65360903\n",
      "Iteration 29812, loss = 3.38022420\n",
      "Iteration 29813, loss = 3.80310120\n",
      "Iteration 29814, loss = 3.40732681\n",
      "Iteration 29815, loss = 3.18926992\n",
      "Iteration 29816, loss = 2.86492609\n",
      "Iteration 29817, loss = 3.09624040\n",
      "Iteration 29818, loss = 3.79745022\n",
      "Iteration 29819, loss = 4.52011946\n",
      "Iteration 29820, loss = 5.54714884\n",
      "Iteration 29821, loss = 6.87127777\n",
      "Iteration 29822, loss = 5.00374270\n",
      "Iteration 29823, loss = 4.80925239\n",
      "Iteration 29824, loss = 4.14183157\n",
      "Iteration 29825, loss = 3.74644876\n",
      "Iteration 29826, loss = 3.31029185\n",
      "Iteration 29827, loss = 3.45458484\n",
      "Iteration 29828, loss = 3.46804075\n",
      "Iteration 29829, loss = 3.14662406\n",
      "Iteration 29830, loss = 3.15381531\n",
      "Iteration 29831, loss = 2.90157680\n",
      "Iteration 29832, loss = 3.05420204\n",
      "Iteration 29833, loss = 3.23117947\n",
      "Iteration 29834, loss = 3.86720737\n",
      "Iteration 29835, loss = 3.54163728\n",
      "Iteration 29836, loss = 3.45894184\n",
      "Iteration 29837, loss = 3.25610811\n",
      "Iteration 29838, loss = 3.07968672\n",
      "Iteration 29839, loss = 3.01040017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29840, loss = 2.89459488\n",
      "Iteration 29841, loss = 3.26037111\n",
      "Iteration 29842, loss = 3.32178704\n",
      "Iteration 29843, loss = 3.41434619\n",
      "Iteration 29844, loss = 3.37591160\n",
      "Iteration 29845, loss = 3.27474461\n",
      "Iteration 29846, loss = 3.48752306\n",
      "Iteration 29847, loss = 6.72856189\n",
      "Iteration 29848, loss = 6.46935213\n",
      "Iteration 29849, loss = 6.20900927\n",
      "Iteration 29850, loss = 7.51725783\n",
      "Iteration 29851, loss = 7.15996322\n",
      "Iteration 29852, loss = 7.71209482\n",
      "Iteration 29853, loss = 6.57056772\n",
      "Iteration 29854, loss = 5.16104594\n",
      "Iteration 29855, loss = 4.49339268\n",
      "Iteration 29856, loss = 3.77231121\n",
      "Iteration 29857, loss = 3.37058733\n",
      "Iteration 29858, loss = 3.24703234\n",
      "Iteration 29859, loss = 3.44815304\n",
      "Iteration 29860, loss = 3.37132560\n",
      "Iteration 29861, loss = 3.55716539\n",
      "Iteration 29862, loss = 3.84121729\n",
      "Iteration 29863, loss = 3.93804742\n",
      "Iteration 29864, loss = 3.55718970\n",
      "Iteration 29865, loss = 3.73628614\n",
      "Iteration 29866, loss = 3.45168222\n",
      "Iteration 29867, loss = 3.53411985\n",
      "Iteration 29868, loss = 3.35275074\n",
      "Iteration 29869, loss = 3.54400234\n",
      "Iteration 29870, loss = 3.40810602\n",
      "Iteration 29871, loss = 3.70607933\n",
      "Iteration 29872, loss = 4.06341412\n",
      "Iteration 29873, loss = 3.76037324\n",
      "Iteration 29874, loss = 3.41901897\n",
      "Iteration 29875, loss = 3.50622578\n",
      "Iteration 29876, loss = 3.50393032\n",
      "Iteration 29877, loss = 3.39185150\n",
      "Iteration 29878, loss = 3.62240941\n",
      "Iteration 29879, loss = 4.24788708\n",
      "Iteration 29880, loss = 3.52185495\n",
      "Iteration 29881, loss = 3.52446383\n",
      "Iteration 29882, loss = 3.59946200\n",
      "Iteration 29883, loss = 3.76680401\n",
      "Iteration 29884, loss = 3.44382669\n",
      "Iteration 29885, loss = 3.39337822\n",
      "Iteration 29886, loss = 3.23478043\n",
      "Iteration 29887, loss = 3.49602819\n",
      "Iteration 29888, loss = 3.56991336\n",
      "Iteration 29889, loss = 4.30032976\n",
      "Iteration 29890, loss = 4.20631995\n",
      "Iteration 29891, loss = 3.48650173\n",
      "Iteration 29892, loss = 3.61817642\n",
      "Iteration 29893, loss = 3.45442905\n",
      "Iteration 29894, loss = 3.55145879\n",
      "Iteration 29895, loss = 3.26582689\n",
      "Iteration 29896, loss = 3.15515149\n",
      "Iteration 29897, loss = 3.11937090\n",
      "Iteration 29898, loss = 3.07935021\n",
      "Iteration 29899, loss = 3.24886827\n",
      "Iteration 29900, loss = 3.09054928\n",
      "Iteration 29901, loss = 3.10334307\n",
      "Iteration 29902, loss = 3.03033626\n",
      "Iteration 29903, loss = 3.09960177\n",
      "Iteration 29904, loss = 3.20305896\n",
      "Iteration 29905, loss = 2.96608973\n",
      "Iteration 29906, loss = 3.15811057\n",
      "Iteration 29907, loss = 3.77978631\n",
      "Iteration 29908, loss = 3.98918961\n",
      "Iteration 29909, loss = 3.69988885\n",
      "Iteration 29910, loss = 3.90560505\n",
      "Iteration 29911, loss = 4.19946308\n",
      "Iteration 29912, loss = 3.13737574\n",
      "Iteration 29913, loss = 3.38400868\n",
      "Iteration 29914, loss = 3.39485588\n",
      "Iteration 29915, loss = 3.69357354\n",
      "Iteration 29916, loss = 3.60961511\n",
      "Iteration 29917, loss = 3.52190810\n",
      "Iteration 29918, loss = 3.56825372\n",
      "Iteration 29919, loss = 3.27993461\n",
      "Iteration 29920, loss = 3.09330711\n",
      "Iteration 29921, loss = 3.09631296\n",
      "Iteration 29922, loss = 2.95516473\n",
      "Iteration 29923, loss = 3.11088165\n",
      "Iteration 29924, loss = 3.07349398\n",
      "Iteration 29925, loss = 3.07843967\n",
      "Iteration 29926, loss = 3.50398064\n",
      "Iteration 29927, loss = 3.27932020\n",
      "Iteration 29928, loss = 3.11350413\n",
      "Iteration 29929, loss = 3.19716411\n",
      "Iteration 29930, loss = 3.17415506\n",
      "Iteration 29931, loss = 3.33403567\n",
      "Iteration 29932, loss = 3.80520859\n",
      "Iteration 29933, loss = 2.99305581\n",
      "Iteration 29934, loss = 3.19594222\n",
      "Iteration 29935, loss = 3.16977752\n",
      "Iteration 29936, loss = 3.25741042\n",
      "Iteration 29937, loss = 3.40535156\n",
      "Iteration 29938, loss = 3.43873801\n",
      "Iteration 29939, loss = 3.23375792\n",
      "Iteration 29940, loss = 3.22097586\n",
      "Iteration 29941, loss = 3.18935117\n",
      "Iteration 29942, loss = 3.25580984\n",
      "Iteration 29943, loss = 3.19588694\n",
      "Iteration 29944, loss = 3.48661821\n",
      "Iteration 29945, loss = 3.68477443\n",
      "Iteration 29946, loss = 3.51725603\n",
      "Iteration 29947, loss = 3.13449403\n",
      "Iteration 29948, loss = 3.51360206\n",
      "Iteration 29949, loss = 3.33078215\n",
      "Iteration 29950, loss = 3.17396495\n",
      "Iteration 29951, loss = 3.85845221\n",
      "Iteration 29952, loss = 3.87614590\n",
      "Iteration 29953, loss = 4.16599845\n",
      "Iteration 29954, loss = 3.96719399\n",
      "Iteration 29955, loss = 3.60325580\n",
      "Iteration 29956, loss = 3.80911042\n",
      "Iteration 29957, loss = 3.93729524\n",
      "Iteration 29958, loss = 4.11316936\n",
      "Iteration 29959, loss = 4.02028760\n",
      "Iteration 29960, loss = 4.54937712\n",
      "Iteration 29961, loss = 5.12530788\n",
      "Iteration 29962, loss = 4.73552363\n",
      "Iteration 29963, loss = 4.30962735\n",
      "Iteration 29964, loss = 3.93212048\n",
      "Iteration 29965, loss = 2.93859898\n",
      "Iteration 29966, loss = 3.20673146\n",
      "Iteration 29967, loss = 4.63008083\n",
      "Iteration 29968, loss = 5.16042134\n",
      "Iteration 29969, loss = 4.77141875\n",
      "Iteration 29970, loss = 3.69285343\n",
      "Iteration 29971, loss = 3.92727370\n",
      "Iteration 29972, loss = 3.25672264\n",
      "Iteration 29973, loss = 3.80420704\n",
      "Iteration 29974, loss = 4.45064299\n",
      "Iteration 29975, loss = 3.31857050\n",
      "Iteration 29976, loss = 3.18908218\n",
      "Iteration 29977, loss = 4.41388016\n",
      "Iteration 29978, loss = 3.98546853\n",
      "Iteration 29979, loss = 3.64574271\n",
      "Iteration 29980, loss = 3.24539685\n",
      "Iteration 29981, loss = 3.06663734\n",
      "Iteration 29982, loss = 3.04956622\n",
      "Iteration 29983, loss = 3.50047956\n",
      "Iteration 29984, loss = 3.99865742\n",
      "Iteration 29985, loss = 3.71050356\n",
      "Iteration 29986, loss = 3.59683969\n",
      "Iteration 29987, loss = 3.74404844\n",
      "Iteration 29988, loss = 3.47583057\n",
      "Iteration 29989, loss = 4.18458433\n",
      "Iteration 29990, loss = 3.81927534\n",
      "Iteration 29991, loss = 3.35506439\n",
      "Iteration 29992, loss = 3.66883899\n",
      "Iteration 29993, loss = 3.78335800\n",
      "Iteration 29994, loss = 3.94025532\n",
      "Iteration 29995, loss = 4.26048596\n",
      "Iteration 29996, loss = 3.73213061\n",
      "Iteration 29997, loss = 3.67255366\n",
      "Iteration 29998, loss = 3.53996602\n",
      "Iteration 29999, loss = 3.69472744\n",
      "Iteration 30000, loss = 3.68701992\n",
      "Iteration 30001, loss = 3.34075689\n",
      "Iteration 30002, loss = 3.17396361\n",
      "Iteration 30003, loss = 3.21523776\n",
      "Iteration 30004, loss = 3.20086439\n",
      "Iteration 30005, loss = 3.02196483\n",
      "Iteration 30006, loss = 2.92460971\n",
      "Iteration 30007, loss = 3.14653825\n",
      "Iteration 30008, loss = 3.44798879\n",
      "Iteration 30009, loss = 3.39186879\n",
      "Iteration 30010, loss = 2.80429359\n",
      "Iteration 30011, loss = 3.12692375\n",
      "Iteration 30012, loss = 3.31852238\n",
      "Iteration 30013, loss = 3.14638809\n",
      "Iteration 30014, loss = 3.28616996\n",
      "Iteration 30015, loss = 3.35731869\n",
      "Iteration 30016, loss = 3.47631184\n",
      "Iteration 30017, loss = 3.78346928\n",
      "Iteration 30018, loss = 3.32793819\n",
      "Iteration 30019, loss = 3.30615725\n",
      "Iteration 30020, loss = 3.30452654\n",
      "Iteration 30021, loss = 3.02169615\n",
      "Iteration 30022, loss = 3.12421737\n",
      "Iteration 30023, loss = 2.99057078\n",
      "Iteration 30024, loss = 3.13221349\n",
      "Iteration 30025, loss = 3.38452141\n",
      "Iteration 30026, loss = 3.18241828\n",
      "Iteration 30027, loss = 3.21600668\n",
      "Iteration 30028, loss = 3.16345977\n",
      "Iteration 30029, loss = 3.75156693\n",
      "Iteration 30030, loss = 3.52480916\n",
      "Iteration 30031, loss = 3.22185770\n",
      "Iteration 30032, loss = 3.08026466\n",
      "Iteration 30033, loss = 3.04171244\n",
      "Iteration 30034, loss = 3.14494648\n",
      "Iteration 30035, loss = 3.34600850\n",
      "Iteration 30036, loss = 3.84137553\n",
      "Iteration 30037, loss = 3.56010792\n",
      "Iteration 30038, loss = 3.67893064\n",
      "Iteration 30039, loss = 4.46682544\n",
      "Iteration 30040, loss = 3.52048305\n",
      "Iteration 30041, loss = 3.35489079\n",
      "Iteration 30042, loss = 3.00883691\n",
      "Iteration 30043, loss = 3.27676778\n",
      "Iteration 30044, loss = 3.17347878\n",
      "Iteration 30045, loss = 3.35712236\n",
      "Iteration 30046, loss = 3.59403776\n",
      "Iteration 30047, loss = 3.65039488\n",
      "Iteration 30048, loss = 3.86094074\n",
      "Iteration 30049, loss = 4.11037871\n",
      "Iteration 30050, loss = 3.89837864\n",
      "Iteration 30051, loss = 3.62345170\n",
      "Iteration 30052, loss = 3.20963666\n",
      "Iteration 30053, loss = 3.79721563\n",
      "Iteration 30054, loss = 3.43686587\n",
      "Iteration 30055, loss = 2.93031336\n",
      "Iteration 30056, loss = 3.44289262\n",
      "Iteration 30057, loss = 3.92881467\n",
      "Iteration 30058, loss = 4.64419971\n",
      "Iteration 30059, loss = 3.85956669\n",
      "Iteration 30060, loss = 3.67468347\n",
      "Iteration 30061, loss = 3.31959338\n",
      "Iteration 30062, loss = 3.39530916\n",
      "Iteration 30063, loss = 4.19401637\n",
      "Iteration 30064, loss = 4.73531998\n",
      "Iteration 30065, loss = 4.14088172\n",
      "Iteration 30066, loss = 3.28009975\n",
      "Iteration 30067, loss = 3.21452997\n",
      "Iteration 30068, loss = 3.58555474\n",
      "Iteration 30069, loss = 3.61629430\n",
      "Iteration 30070, loss = 3.14788735\n",
      "Iteration 30071, loss = 3.18487072\n",
      "Iteration 30072, loss = 3.36896031\n",
      "Iteration 30073, loss = 3.26142388\n",
      "Iteration 30074, loss = 3.37291281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30075, loss = 3.18865309\n",
      "Iteration 30076, loss = 3.00233149\n",
      "Iteration 30077, loss = 3.06254260\n",
      "Iteration 30078, loss = 3.51489065\n",
      "Iteration 30079, loss = 3.33862348\n",
      "Iteration 30080, loss = 3.85095931\n",
      "Iteration 30081, loss = 3.23587370\n",
      "Iteration 30082, loss = 3.58755984\n",
      "Iteration 30083, loss = 4.24140311\n",
      "Iteration 30084, loss = 3.45737213\n",
      "Iteration 30085, loss = 4.76065750\n",
      "Iteration 30086, loss = 5.97170687\n",
      "Iteration 30087, loss = 5.30120232\n",
      "Iteration 30088, loss = 4.28403773\n",
      "Iteration 30089, loss = 4.42161288\n",
      "Iteration 30090, loss = 3.44571536\n",
      "Iteration 30091, loss = 3.68428435\n",
      "Iteration 30092, loss = 3.51471636\n",
      "Iteration 30093, loss = 4.69413843\n",
      "Iteration 30094, loss = 5.90625022\n",
      "Iteration 30095, loss = 4.27727217\n",
      "Iteration 30096, loss = 4.03838834\n",
      "Iteration 30097, loss = 4.28024959\n",
      "Iteration 30098, loss = 4.90760187\n",
      "Iteration 30099, loss = 6.21965550\n",
      "Iteration 30100, loss = 6.58848243\n",
      "Iteration 30101, loss = 5.99352687\n",
      "Iteration 30102, loss = 6.30726825\n",
      "Iteration 30103, loss = 4.40781751\n",
      "Iteration 30104, loss = 3.59739916\n",
      "Iteration 30105, loss = 3.30127179\n",
      "Iteration 30106, loss = 4.51635769\n",
      "Iteration 30107, loss = 4.47304080\n",
      "Iteration 30108, loss = 4.05095167\n",
      "Iteration 30109, loss = 3.58367375\n",
      "Iteration 30110, loss = 3.84110271\n",
      "Iteration 30111, loss = 3.93337925\n",
      "Iteration 30112, loss = 3.57792789\n",
      "Iteration 30113, loss = 3.52699127\n",
      "Iteration 30114, loss = 3.26627538\n",
      "Iteration 30115, loss = 3.45703878\n",
      "Iteration 30116, loss = 3.54547369\n",
      "Iteration 30117, loss = 3.53319269\n",
      "Iteration 30118, loss = 3.07766630\n",
      "Iteration 30119, loss = 3.20758039\n",
      "Iteration 30120, loss = 3.36170785\n",
      "Iteration 30121, loss = 2.85691551\n",
      "Iteration 30122, loss = 3.29652899\n",
      "Iteration 30123, loss = 4.21768637\n",
      "Iteration 30124, loss = 5.44399759\n",
      "Iteration 30125, loss = 5.66852837\n",
      "Iteration 30126, loss = 5.60202346\n",
      "Iteration 30127, loss = 6.08291120\n",
      "Iteration 30128, loss = 7.91506171\n",
      "Iteration 30129, loss = 7.23717396\n",
      "Iteration 30130, loss = 6.01969432\n",
      "Iteration 30131, loss = 5.16095877\n",
      "Iteration 30132, loss = 4.01181399\n",
      "Iteration 30133, loss = 4.11114965\n",
      "Iteration 30134, loss = 3.40420317\n",
      "Iteration 30135, loss = 3.55351188\n",
      "Iteration 30136, loss = 3.29101368\n",
      "Iteration 30137, loss = 3.23458716\n",
      "Iteration 30138, loss = 3.10967121\n",
      "Iteration 30139, loss = 3.06333600\n",
      "Iteration 30140, loss = 3.20393538\n",
      "Iteration 30141, loss = 3.53978140\n",
      "Iteration 30142, loss = 4.10694983\n",
      "Iteration 30143, loss = 4.18083428\n",
      "Iteration 30144, loss = 3.73938256\n",
      "Iteration 30145, loss = 3.94078889\n",
      "Iteration 30146, loss = 3.93005902\n",
      "Iteration 30147, loss = 3.87389170\n",
      "Iteration 30148, loss = 4.18026245\n",
      "Iteration 30149, loss = 3.29859791\n",
      "Iteration 30150, loss = 3.21757569\n",
      "Iteration 30151, loss = 3.80633463\n",
      "Iteration 30152, loss = 3.63933444\n",
      "Iteration 30153, loss = 3.52153691\n",
      "Iteration 30154, loss = 3.63299212\n",
      "Iteration 30155, loss = 3.23325967\n",
      "Iteration 30156, loss = 3.19500878\n",
      "Iteration 30157, loss = 3.26612608\n",
      "Iteration 30158, loss = 3.10211082\n",
      "Iteration 30159, loss = 3.34470911\n",
      "Iteration 30160, loss = 3.04365387\n",
      "Iteration 30161, loss = 3.25307785\n",
      "Iteration 30162, loss = 3.08919530\n",
      "Iteration 30163, loss = 3.08944517\n",
      "Iteration 30164, loss = 3.06041277\n",
      "Iteration 30165, loss = 2.86879816\n",
      "Iteration 30166, loss = 3.35910069\n",
      "Iteration 30167, loss = 3.41681457\n",
      "Iteration 30168, loss = 3.81968464\n",
      "Iteration 30169, loss = 3.48392217\n",
      "Iteration 30170, loss = 3.77034828\n",
      "Iteration 30171, loss = 3.68999642\n",
      "Iteration 30172, loss = 3.63022344\n",
      "Iteration 30173, loss = 3.65640347\n",
      "Iteration 30174, loss = 4.04330959\n",
      "Iteration 30175, loss = 3.96221951\n",
      "Iteration 30176, loss = 3.51337888\n",
      "Iteration 30177, loss = 3.93358389\n",
      "Iteration 30178, loss = 4.07070033\n",
      "Iteration 30179, loss = 3.72482224\n",
      "Iteration 30180, loss = 3.70408210\n",
      "Iteration 30181, loss = 3.11829257\n",
      "Iteration 30182, loss = 3.29895323\n",
      "Iteration 30183, loss = 3.50003944\n",
      "Iteration 30184, loss = 2.79463487\n",
      "Iteration 30185, loss = 3.01665506\n",
      "Iteration 30186, loss = 4.40928162\n",
      "Iteration 30187, loss = 3.65947188\n",
      "Iteration 30188, loss = 3.57513106\n",
      "Iteration 30189, loss = 3.24924076\n",
      "Iteration 30190, loss = 3.44675623\n",
      "Iteration 30191, loss = 3.49304981\n",
      "Iteration 30192, loss = 3.36918137\n",
      "Iteration 30193, loss = 3.41392711\n",
      "Iteration 30194, loss = 3.27140411\n",
      "Iteration 30195, loss = 3.52731668\n",
      "Iteration 30196, loss = 3.55089489\n",
      "Iteration 30197, loss = 3.29626013\n",
      "Iteration 30198, loss = 3.35854894\n",
      "Iteration 30199, loss = 3.10463850\n",
      "Iteration 30200, loss = 3.62815402\n",
      "Iteration 30201, loss = 3.21748279\n",
      "Iteration 30202, loss = 3.10261940\n",
      "Iteration 30203, loss = 3.18096006\n",
      "Iteration 30204, loss = 2.90776359\n",
      "Iteration 30205, loss = 2.93075451\n",
      "Iteration 30206, loss = 2.99038589\n",
      "Iteration 30207, loss = 2.80209499\n",
      "Iteration 30208, loss = 2.81843101\n",
      "Iteration 30209, loss = 3.19033395\n",
      "Iteration 30210, loss = 2.99394129\n",
      "Iteration 30211, loss = 3.24165284\n",
      "Iteration 30212, loss = 3.29071107\n",
      "Iteration 30213, loss = 3.04439691\n",
      "Iteration 30214, loss = 3.19582088\n",
      "Iteration 30215, loss = 3.19270660\n",
      "Iteration 30216, loss = 3.04716415\n",
      "Iteration 30217, loss = 3.09086363\n",
      "Iteration 30218, loss = 3.01992671\n",
      "Iteration 30219, loss = 3.30716456\n",
      "Iteration 30220, loss = 3.50855672\n",
      "Iteration 30221, loss = 4.43183817\n",
      "Iteration 30222, loss = 4.16664734\n",
      "Iteration 30223, loss = 3.56443047\n",
      "Iteration 30224, loss = 4.52661540\n",
      "Iteration 30225, loss = 4.63856265\n",
      "Iteration 30226, loss = 3.84778396\n",
      "Iteration 30227, loss = 4.45452885\n",
      "Iteration 30228, loss = 3.16822245\n",
      "Iteration 30229, loss = 3.36490985\n",
      "Iteration 30230, loss = 3.46580312\n",
      "Iteration 30231, loss = 3.28146863\n",
      "Iteration 30232, loss = 3.40453835\n",
      "Iteration 30233, loss = 3.62412187\n",
      "Iteration 30234, loss = 3.68544571\n",
      "Iteration 30235, loss = 3.47736317\n",
      "Iteration 30236, loss = 3.31039726\n",
      "Iteration 30237, loss = 3.79900948\n",
      "Iteration 30238, loss = 3.61198281\n",
      "Iteration 30239, loss = 4.67695429\n",
      "Iteration 30240, loss = 4.26628080\n",
      "Iteration 30241, loss = 3.61335625\n",
      "Iteration 30242, loss = 3.84947379\n",
      "Iteration 30243, loss = 3.49623340\n",
      "Iteration 30244, loss = 3.53641221\n",
      "Iteration 30245, loss = 3.13466360\n",
      "Iteration 30246, loss = 4.06746662\n",
      "Iteration 30247, loss = 4.09417298\n",
      "Iteration 30248, loss = 3.34870654\n",
      "Iteration 30249, loss = 3.22413251\n",
      "Iteration 30250, loss = 3.64495693\n",
      "Iteration 30251, loss = 4.11386448\n",
      "Iteration 30252, loss = 4.63744603\n",
      "Iteration 30253, loss = 4.80851570\n",
      "Iteration 30254, loss = 3.94047865\n",
      "Iteration 30255, loss = 3.57382064\n",
      "Iteration 30256, loss = 3.79747210\n",
      "Iteration 30257, loss = 3.95159264\n",
      "Iteration 30258, loss = 3.50153169\n",
      "Iteration 30259, loss = 3.40291130\n",
      "Iteration 30260, loss = 3.51553250\n",
      "Iteration 30261, loss = 3.91858357\n",
      "Iteration 30262, loss = 3.49925796\n",
      "Iteration 30263, loss = 3.93454760\n",
      "Iteration 30264, loss = 3.46062650\n",
      "Iteration 30265, loss = 3.10281901\n",
      "Iteration 30266, loss = 3.17080996\n",
      "Iteration 30267, loss = 3.73618575\n",
      "Iteration 30268, loss = 3.51946163\n",
      "Iteration 30269, loss = 3.15222532\n",
      "Iteration 30270, loss = 3.13839419\n",
      "Iteration 30271, loss = 3.50859989\n",
      "Iteration 30272, loss = 3.16783440\n",
      "Iteration 30273, loss = 3.04990533\n",
      "Iteration 30274, loss = 3.07008953\n",
      "Iteration 30275, loss = 3.46561704\n",
      "Iteration 30276, loss = 3.32653906\n",
      "Iteration 30277, loss = 3.16992631\n",
      "Iteration 30278, loss = 3.65210297\n",
      "Iteration 30279, loss = 3.46084622\n",
      "Iteration 30280, loss = 3.31614949\n",
      "Iteration 30281, loss = 3.66998797\n",
      "Iteration 30282, loss = 3.43395489\n",
      "Iteration 30283, loss = 3.34939256\n",
      "Iteration 30284, loss = 3.52742094\n",
      "Iteration 30285, loss = 5.22261728\n",
      "Iteration 30286, loss = 4.78040300\n",
      "Iteration 30287, loss = 4.68858856\n",
      "Iteration 30288, loss = 4.51996891\n",
      "Iteration 30289, loss = 5.72654118\n",
      "Iteration 30290, loss = 4.16483169\n",
      "Iteration 30291, loss = 3.83773063\n",
      "Iteration 30292, loss = 3.91874718\n",
      "Iteration 30293, loss = 3.76214697\n",
      "Iteration 30294, loss = 3.12321202\n",
      "Iteration 30295, loss = 2.95595919\n",
      "Iteration 30296, loss = 3.26198617\n",
      "Iteration 30297, loss = 3.90254702\n",
      "Iteration 30298, loss = 3.02432823\n",
      "Iteration 30299, loss = 3.56426968\n",
      "Iteration 30300, loss = 3.52075617\n",
      "Iteration 30301, loss = 3.48712971\n",
      "Iteration 30302, loss = 3.27778020\n",
      "Iteration 30303, loss = 3.05027940\n",
      "Iteration 30304, loss = 3.21440606\n",
      "Iteration 30305, loss = 3.88985632\n",
      "Iteration 30306, loss = 4.17848535\n",
      "Iteration 30307, loss = 3.77861421\n",
      "Iteration 30308, loss = 3.58132860\n",
      "Iteration 30309, loss = 3.47327463\n",
      "Iteration 30310, loss = 3.06168551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30311, loss = 3.51355135\n",
      "Iteration 30312, loss = 3.44624666\n",
      "Iteration 30313, loss = 3.84212949\n",
      "Iteration 30314, loss = 3.88870469\n",
      "Iteration 30315, loss = 3.61504298\n",
      "Iteration 30316, loss = 4.21575223\n",
      "Iteration 30317, loss = 3.58596434\n",
      "Iteration 30318, loss = 3.83156394\n",
      "Iteration 30319, loss = 4.29803795\n",
      "Iteration 30320, loss = 4.16160500\n",
      "Iteration 30321, loss = 3.76074905\n",
      "Iteration 30322, loss = 3.14645018\n",
      "Iteration 30323, loss = 3.37752901\n",
      "Iteration 30324, loss = 3.20119478\n",
      "Iteration 30325, loss = 3.09278488\n",
      "Iteration 30326, loss = 3.35667249\n",
      "Iteration 30327, loss = 3.85521037\n",
      "Iteration 30328, loss = 3.14109605\n",
      "Iteration 30329, loss = 3.00533837\n",
      "Iteration 30330, loss = 3.38754354\n",
      "Iteration 30331, loss = 3.15279261\n",
      "Iteration 30332, loss = 3.23662078\n",
      "Iteration 30333, loss = 2.95976173\n",
      "Iteration 30334, loss = 3.14437645\n",
      "Iteration 30335, loss = 3.38630714\n",
      "Iteration 30336, loss = 3.32668242\n",
      "Iteration 30337, loss = 3.82223233\n",
      "Iteration 30338, loss = 3.63393950\n",
      "Iteration 30339, loss = 3.71821871\n",
      "Iteration 30340, loss = 3.31555823\n",
      "Iteration 30341, loss = 3.23012098\n",
      "Iteration 30342, loss = 3.08385344\n",
      "Iteration 30343, loss = 3.13452280\n",
      "Iteration 30344, loss = 3.17369841\n",
      "Iteration 30345, loss = 3.29404587\n",
      "Iteration 30346, loss = 3.00812274\n",
      "Iteration 30347, loss = 3.45004910\n",
      "Iteration 30348, loss = 3.43505753\n",
      "Iteration 30349, loss = 4.16562731\n",
      "Iteration 30350, loss = 3.53053570\n",
      "Iteration 30351, loss = 3.09018977\n",
      "Iteration 30352, loss = 3.73450356\n",
      "Iteration 30353, loss = 3.33623724\n",
      "Iteration 30354, loss = 3.34674103\n",
      "Iteration 30355, loss = 3.80416958\n",
      "Iteration 30356, loss = 3.69477566\n",
      "Iteration 30357, loss = 4.76600317\n",
      "Iteration 30358, loss = 4.28812476\n",
      "Iteration 30359, loss = 4.34925539\n",
      "Iteration 30360, loss = 3.91715209\n",
      "Iteration 30361, loss = 3.80409895\n",
      "Iteration 30362, loss = 3.46663411\n",
      "Iteration 30363, loss = 3.61731859\n",
      "Iteration 30364, loss = 3.11185997\n",
      "Iteration 30365, loss = 2.88221081\n",
      "Iteration 30366, loss = 3.13697195\n",
      "Iteration 30367, loss = 3.21283037\n",
      "Iteration 30368, loss = 3.37577108\n",
      "Iteration 30369, loss = 3.90183987\n",
      "Iteration 30370, loss = 4.35134741\n",
      "Iteration 30371, loss = 4.57787377\n",
      "Iteration 30372, loss = 3.42014823\n",
      "Iteration 30373, loss = 3.90581482\n",
      "Iteration 30374, loss = 3.18543289\n",
      "Iteration 30375, loss = 3.20104828\n",
      "Iteration 30376, loss = 3.03579709\n",
      "Iteration 30377, loss = 3.20974849\n",
      "Iteration 30378, loss = 3.22097117\n",
      "Iteration 30379, loss = 3.36254738\n",
      "Iteration 30380, loss = 3.26083913\n",
      "Iteration 30381, loss = 3.17085489\n",
      "Iteration 30382, loss = 3.17271342\n",
      "Iteration 30383, loss = 3.10136955\n",
      "Iteration 30384, loss = 3.78880923\n",
      "Iteration 30385, loss = 5.34528924\n",
      "Iteration 30386, loss = 4.42772639\n",
      "Iteration 30387, loss = 4.95939557\n",
      "Iteration 30388, loss = 3.86638635\n",
      "Iteration 30389, loss = 4.82042308\n",
      "Iteration 30390, loss = 4.03788128\n",
      "Iteration 30391, loss = 4.39084248\n",
      "Iteration 30392, loss = 4.50372054\n",
      "Iteration 30393, loss = 4.28557781\n",
      "Iteration 30394, loss = 3.67794374\n",
      "Iteration 30395, loss = 3.41783251\n",
      "Iteration 30396, loss = 3.42058604\n",
      "Iteration 30397, loss = 3.13456163\n",
      "Iteration 30398, loss = 3.43269729\n",
      "Iteration 30399, loss = 3.33626321\n",
      "Iteration 30400, loss = 3.42743631\n",
      "Iteration 30401, loss = 3.42976736\n",
      "Iteration 30402, loss = 4.20344888\n",
      "Iteration 30403, loss = 3.30260356\n",
      "Iteration 30404, loss = 3.35942742\n",
      "Iteration 30405, loss = 3.44969201\n",
      "Iteration 30406, loss = 3.53201175\n",
      "Iteration 30407, loss = 3.84187125\n",
      "Iteration 30408, loss = 3.91198382\n",
      "Iteration 30409, loss = 3.42012781\n",
      "Iteration 30410, loss = 3.53146244\n",
      "Iteration 30411, loss = 4.28250754\n",
      "Iteration 30412, loss = 3.21638813\n",
      "Iteration 30413, loss = 4.07854763\n",
      "Iteration 30414, loss = 3.58838772\n",
      "Iteration 30415, loss = 3.86513097\n",
      "Iteration 30416, loss = 4.26192904\n",
      "Iteration 30417, loss = 4.38269619\n",
      "Iteration 30418, loss = 3.64492042\n",
      "Iteration 30419, loss = 3.32939949\n",
      "Iteration 30420, loss = 3.39375247\n",
      "Iteration 30421, loss = 3.37083909\n",
      "Iteration 30422, loss = 3.38309600\n",
      "Iteration 30423, loss = 3.47272962\n",
      "Iteration 30424, loss = 3.38134426\n",
      "Iteration 30425, loss = 3.14774298\n",
      "Iteration 30426, loss = 3.35866746\n",
      "Iteration 30427, loss = 3.78005770\n",
      "Iteration 30428, loss = 3.77192472\n",
      "Iteration 30429, loss = 3.63529976\n",
      "Iteration 30430, loss = 4.20417145\n",
      "Iteration 30431, loss = 3.60672913\n",
      "Iteration 30432, loss = 3.65646782\n",
      "Iteration 30433, loss = 3.67382272\n",
      "Iteration 30434, loss = 3.51964084\n",
      "Iteration 30435, loss = 3.17296447\n",
      "Iteration 30436, loss = 3.19726866\n",
      "Iteration 30437, loss = 3.11880295\n",
      "Iteration 30438, loss = 3.01773015\n",
      "Iteration 30439, loss = 3.09314763\n",
      "Iteration 30440, loss = 3.32194425\n",
      "Iteration 30441, loss = 3.04618905\n",
      "Iteration 30442, loss = 3.30732284\n",
      "Iteration 30443, loss = 3.65324385\n",
      "Iteration 30444, loss = 4.06267473\n",
      "Iteration 30445, loss = 4.18324459\n",
      "Iteration 30446, loss = 4.37675386\n",
      "Iteration 30447, loss = 4.19161274\n",
      "Iteration 30448, loss = 4.57665683\n",
      "Iteration 30449, loss = 3.52053106\n",
      "Iteration 30450, loss = 4.40924633\n",
      "Iteration 30451, loss = 3.23470929\n",
      "Iteration 30452, loss = 3.34845721\n",
      "Iteration 30453, loss = 3.26821423\n",
      "Iteration 30454, loss = 4.29183149\n",
      "Iteration 30455, loss = 3.55333688\n",
      "Iteration 30456, loss = 3.35931679\n",
      "Iteration 30457, loss = 3.28465776\n",
      "Iteration 30458, loss = 3.45593733\n",
      "Iteration 30459, loss = 3.66587440\n",
      "Iteration 30460, loss = 3.65384257\n",
      "Iteration 30461, loss = 4.69524551\n",
      "Iteration 30462, loss = 3.80948721\n",
      "Iteration 30463, loss = 3.30652986\n",
      "Iteration 30464, loss = 4.08301261\n",
      "Iteration 30465, loss = 4.82793330\n",
      "Iteration 30466, loss = 3.89679578\n",
      "Iteration 30467, loss = 3.49502345\n",
      "Iteration 30468, loss = 3.19398534\n",
      "Iteration 30469, loss = 3.33023060\n",
      "Iteration 30470, loss = 3.33906315\n",
      "Iteration 30471, loss = 3.73806258\n",
      "Iteration 30472, loss = 3.71604711\n",
      "Iteration 30473, loss = 3.43670652\n",
      "Iteration 30474, loss = 3.26456133\n",
      "Iteration 30475, loss = 3.31584462\n",
      "Iteration 30476, loss = 3.77233279\n",
      "Iteration 30477, loss = 5.41714238\n",
      "Iteration 30478, loss = 4.04369008\n",
      "Iteration 30479, loss = 4.75759874\n",
      "Iteration 30480, loss = 3.66014306\n",
      "Iteration 30481, loss = 3.28951241\n",
      "Iteration 30482, loss = 3.35131867\n",
      "Iteration 30483, loss = 3.52389506\n",
      "Iteration 30484, loss = 3.92158314\n",
      "Iteration 30485, loss = 3.24727876\n",
      "Iteration 30486, loss = 3.18947569\n",
      "Iteration 30487, loss = 3.36707719\n",
      "Iteration 30488, loss = 3.66569510\n",
      "Iteration 30489, loss = 3.65860703\n",
      "Iteration 30490, loss = 4.42098520\n",
      "Iteration 30491, loss = 4.08371386\n",
      "Iteration 30492, loss = 4.57802240\n",
      "Iteration 30493, loss = 5.56047085\n",
      "Iteration 30494, loss = 3.70563547\n",
      "Iteration 30495, loss = 4.11341775\n",
      "Iteration 30496, loss = 4.67804185\n",
      "Iteration 30497, loss = 4.92112581\n",
      "Iteration 30498, loss = 4.21980777\n",
      "Iteration 30499, loss = 3.74447832\n",
      "Iteration 30500, loss = 3.23618106\n",
      "Iteration 30501, loss = 3.24289785\n",
      "Iteration 30502, loss = 3.95498241\n",
      "Iteration 30503, loss = 3.09882380\n",
      "Iteration 30504, loss = 3.12577533\n",
      "Iteration 30505, loss = 3.06086558\n",
      "Iteration 30506, loss = 3.01691821\n",
      "Iteration 30507, loss = 3.22089918\n",
      "Iteration 30508, loss = 3.06498294\n",
      "Iteration 30509, loss = 2.97993257\n",
      "Iteration 30510, loss = 3.19473889\n",
      "Iteration 30511, loss = 3.83695261\n",
      "Iteration 30512, loss = 3.80704522\n",
      "Iteration 30513, loss = 3.30320611\n",
      "Iteration 30514, loss = 3.17537731\n",
      "Iteration 30515, loss = 3.60613357\n",
      "Iteration 30516, loss = 3.74309844\n",
      "Iteration 30517, loss = 3.68516703\n",
      "Iteration 30518, loss = 4.11110926\n",
      "Iteration 30519, loss = 3.33753932\n",
      "Iteration 30520, loss = 3.41680752\n",
      "Iteration 30521, loss = 3.32090799\n",
      "Iteration 30522, loss = 3.44713422\n",
      "Iteration 30523, loss = 3.41882818\n",
      "Iteration 30524, loss = 3.41342675\n",
      "Iteration 30525, loss = 2.87311891\n",
      "Iteration 30526, loss = 3.41597422\n",
      "Iteration 30527, loss = 3.16275830\n",
      "Iteration 30528, loss = 3.50407888\n",
      "Iteration 30529, loss = 3.00638894\n",
      "Iteration 30530, loss = 2.99717329\n",
      "Iteration 30531, loss = 3.00358568\n",
      "Iteration 30532, loss = 3.12987997\n",
      "Iteration 30533, loss = 3.12639463\n",
      "Iteration 30534, loss = 3.47173737\n",
      "Iteration 30535, loss = 3.35968634\n",
      "Iteration 30536, loss = 3.55556347\n",
      "Iteration 30537, loss = 2.86812360\n",
      "Iteration 30538, loss = 3.10810693\n",
      "Iteration 30539, loss = 3.51841979\n",
      "Iteration 30540, loss = 3.87255636\n",
      "Iteration 30541, loss = 4.72437252\n",
      "Iteration 30542, loss = 4.25100939\n",
      "Iteration 30543, loss = 3.92871954\n",
      "Iteration 30544, loss = 5.64606528\n",
      "Iteration 30545, loss = 4.21674730\n",
      "Iteration 30546, loss = 4.04692997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30547, loss = 3.43807696\n",
      "Iteration 30548, loss = 4.11897949\n",
      "Iteration 30549, loss = 4.08229594\n",
      "Iteration 30550, loss = 3.35566223\n",
      "Iteration 30551, loss = 3.20857708\n",
      "Iteration 30552, loss = 3.29928135\n",
      "Iteration 30553, loss = 2.99277878\n",
      "Iteration 30554, loss = 3.46005654\n",
      "Iteration 30555, loss = 3.41460909\n",
      "Iteration 30556, loss = 3.39355762\n",
      "Iteration 30557, loss = 4.16649412\n",
      "Iteration 30558, loss = 3.38150959\n",
      "Iteration 30559, loss = 3.05832166\n",
      "Iteration 30560, loss = 2.89263100\n",
      "Iteration 30561, loss = 3.29940223\n",
      "Iteration 30562, loss = 2.98886925\n",
      "Iteration 30563, loss = 3.00708677\n",
      "Iteration 30564, loss = 3.18701874\n",
      "Iteration 30565, loss = 3.96860185\n",
      "Iteration 30566, loss = 3.52113337\n",
      "Iteration 30567, loss = 2.98764864\n",
      "Iteration 30568, loss = 2.93707410\n",
      "Iteration 30569, loss = 2.90004537\n",
      "Iteration 30570, loss = 3.19924823\n",
      "Iteration 30571, loss = 3.22623353\n",
      "Iteration 30572, loss = 3.40406877\n",
      "Iteration 30573, loss = 3.08596617\n",
      "Iteration 30574, loss = 3.09520407\n",
      "Iteration 30575, loss = 3.15882434\n",
      "Iteration 30576, loss = 3.16192562\n",
      "Iteration 30577, loss = 3.64469536\n",
      "Iteration 30578, loss = 5.86846779\n",
      "Iteration 30579, loss = 5.33638934\n",
      "Iteration 30580, loss = 5.59583542\n",
      "Iteration 30581, loss = 4.16632157\n",
      "Iteration 30582, loss = 5.06737353\n",
      "Iteration 30583, loss = 4.87209577\n",
      "Iteration 30584, loss = 4.23640845\n",
      "Iteration 30585, loss = 4.09674548\n",
      "Iteration 30586, loss = 4.04169223\n",
      "Iteration 30587, loss = 3.54616986\n",
      "Iteration 30588, loss = 3.54951271\n",
      "Iteration 30589, loss = 3.41363042\n",
      "Iteration 30590, loss = 3.13138689\n",
      "Iteration 30591, loss = 2.95416792\n",
      "Iteration 30592, loss = 4.02599860\n",
      "Iteration 30593, loss = 3.89741047\n",
      "Iteration 30594, loss = 3.72963211\n",
      "Iteration 30595, loss = 3.12889831\n",
      "Iteration 30596, loss = 3.10796057\n",
      "Iteration 30597, loss = 3.27573040\n",
      "Iteration 30598, loss = 3.48045630\n",
      "Iteration 30599, loss = 3.39257010\n",
      "Iteration 30600, loss = 3.37440584\n",
      "Iteration 30601, loss = 4.19090707\n",
      "Iteration 30602, loss = 3.79357218\n",
      "Iteration 30603, loss = 4.39987394\n",
      "Iteration 30604, loss = 3.94982111\n",
      "Iteration 30605, loss = 3.87047886\n",
      "Iteration 30606, loss = 3.83237301\n",
      "Iteration 30607, loss = 3.67450258\n",
      "Iteration 30608, loss = 3.20178727\n",
      "Iteration 30609, loss = 3.36959731\n",
      "Iteration 30610, loss = 3.35255517\n",
      "Iteration 30611, loss = 3.53152814\n",
      "Iteration 30612, loss = 3.79148472\n",
      "Iteration 30613, loss = 3.82594880\n",
      "Iteration 30614, loss = 3.38324322\n",
      "Iteration 30615, loss = 3.33147056\n",
      "Iteration 30616, loss = 3.02638287\n",
      "Iteration 30617, loss = 2.99997451\n",
      "Iteration 30618, loss = 3.20977126\n",
      "Iteration 30619, loss = 3.75216772\n",
      "Iteration 30620, loss = 2.98338818\n",
      "Iteration 30621, loss = 3.33931324\n",
      "Iteration 30622, loss = 3.99243909\n",
      "Iteration 30623, loss = 3.47208597\n",
      "Iteration 30624, loss = 3.62572679\n",
      "Iteration 30625, loss = 3.51075198\n",
      "Iteration 30626, loss = 3.65010493\n",
      "Iteration 30627, loss = 3.13867093\n",
      "Iteration 30628, loss = 3.28101732\n",
      "Iteration 30629, loss = 3.66726097\n",
      "Iteration 30630, loss = 3.49789710\n",
      "Iteration 30631, loss = 3.35680100\n",
      "Iteration 30632, loss = 3.21262110\n",
      "Iteration 30633, loss = 3.79114044\n",
      "Iteration 30634, loss = 3.79662306\n",
      "Iteration 30635, loss = 3.77138279\n",
      "Iteration 30636, loss = 3.18734942\n",
      "Iteration 30637, loss = 3.23322570\n",
      "Iteration 30638, loss = 3.00177001\n",
      "Iteration 30639, loss = 2.98961167\n",
      "Iteration 30640, loss = 3.61488798\n",
      "Iteration 30641, loss = 3.65229211\n",
      "Iteration 30642, loss = 3.60279870\n",
      "Iteration 30643, loss = 3.26857189\n",
      "Iteration 30644, loss = 3.23244862\n",
      "Iteration 30645, loss = 5.02615702\n",
      "Iteration 30646, loss = 4.72149773\n",
      "Iteration 30647, loss = 4.80357836\n",
      "Iteration 30648, loss = 4.81188396\n",
      "Iteration 30649, loss = 4.11338048\n",
      "Iteration 30650, loss = 3.63649604\n",
      "Iteration 30651, loss = 3.03368921\n",
      "Iteration 30652, loss = 3.32435714\n",
      "Iteration 30653, loss = 4.44501845\n",
      "Iteration 30654, loss = 4.45941433\n",
      "Iteration 30655, loss = 4.00229861\n",
      "Iteration 30656, loss = 3.33278026\n",
      "Iteration 30657, loss = 3.11549423\n",
      "Iteration 30658, loss = 3.36337319\n",
      "Iteration 30659, loss = 3.18145116\n",
      "Iteration 30660, loss = 3.10180211\n",
      "Iteration 30661, loss = 3.51691957\n",
      "Iteration 30662, loss = 4.41634737\n",
      "Iteration 30663, loss = 3.95058014\n",
      "Iteration 30664, loss = 5.20788771\n",
      "Iteration 30665, loss = 4.46477243\n",
      "Iteration 30666, loss = 5.73343452\n",
      "Iteration 30667, loss = 4.01719232\n",
      "Iteration 30668, loss = 4.64730995\n",
      "Iteration 30669, loss = 4.88149516\n",
      "Iteration 30670, loss = 4.96445247\n",
      "Iteration 30671, loss = 4.10377089\n",
      "Iteration 30672, loss = 3.77458186\n",
      "Iteration 30673, loss = 3.98639797\n",
      "Iteration 30674, loss = 4.59625174\n",
      "Iteration 30675, loss = 3.75731353\n",
      "Iteration 30676, loss = 4.04499053\n",
      "Iteration 30677, loss = 4.05206892\n",
      "Iteration 30678, loss = 3.77187809\n",
      "Iteration 30679, loss = 3.23249205\n",
      "Iteration 30680, loss = 4.25121932\n",
      "Iteration 30681, loss = 3.50178819\n",
      "Iteration 30682, loss = 3.05671434\n",
      "Iteration 30683, loss = 3.39272249\n",
      "Iteration 30684, loss = 3.59752821\n",
      "Iteration 30685, loss = 3.45491494\n",
      "Iteration 30686, loss = 3.66391257\n",
      "Iteration 30687, loss = 3.95109052\n",
      "Iteration 30688, loss = 3.77595212\n",
      "Iteration 30689, loss = 3.17083619\n",
      "Iteration 30690, loss = 2.87811385\n",
      "Iteration 30691, loss = 2.82717797\n",
      "Iteration 30692, loss = 3.07778254\n",
      "Iteration 30693, loss = 3.04319750\n",
      "Iteration 30694, loss = 2.99484835\n",
      "Iteration 30695, loss = 3.31454645\n",
      "Iteration 30696, loss = 3.39274932\n",
      "Iteration 30697, loss = 3.55200663\n",
      "Iteration 30698, loss = 3.15480930\n",
      "Iteration 30699, loss = 3.16297851\n",
      "Iteration 30700, loss = 3.08667806\n",
      "Iteration 30701, loss = 3.37795548\n",
      "Iteration 30702, loss = 3.28553774\n",
      "Iteration 30703, loss = 2.94622645\n",
      "Iteration 30704, loss = 3.09902460\n",
      "Iteration 30705, loss = 3.08588968\n",
      "Iteration 30706, loss = 3.20631259\n",
      "Iteration 30707, loss = 3.03996991\n",
      "Iteration 30708, loss = 2.93077329\n",
      "Iteration 30709, loss = 3.68629188\n",
      "Iteration 30710, loss = 3.82445871\n",
      "Iteration 30711, loss = 3.29176833\n",
      "Iteration 30712, loss = 3.06090059\n",
      "Iteration 30713, loss = 3.39391070\n",
      "Iteration 30714, loss = 3.29612612\n",
      "Iteration 30715, loss = 3.26070813\n",
      "Iteration 30716, loss = 4.22693820\n",
      "Iteration 30717, loss = 3.90053609\n",
      "Iteration 30718, loss = 3.87813745\n",
      "Iteration 30719, loss = 3.31955868\n",
      "Iteration 30720, loss = 3.60221079\n",
      "Iteration 30721, loss = 3.39872926\n",
      "Iteration 30722, loss = 3.30769690\n",
      "Iteration 30723, loss = 3.00582304\n",
      "Iteration 30724, loss = 3.20066046\n",
      "Iteration 30725, loss = 3.44548901\n",
      "Iteration 30726, loss = 3.72793485\n",
      "Iteration 30727, loss = 3.46815732\n",
      "Iteration 30728, loss = 3.43444760\n",
      "Iteration 30729, loss = 3.79889313\n",
      "Iteration 30730, loss = 4.01656374\n",
      "Iteration 30731, loss = 4.13812998\n",
      "Iteration 30732, loss = 4.00972500\n",
      "Iteration 30733, loss = 4.17931860\n",
      "Iteration 30734, loss = 4.17381731\n",
      "Iteration 30735, loss = 4.06155183\n",
      "Iteration 30736, loss = 3.70301570\n",
      "Iteration 30737, loss = 3.24467267\n",
      "Iteration 30738, loss = 3.06946196\n",
      "Iteration 30739, loss = 3.04824199\n",
      "Iteration 30740, loss = 3.53023854\n",
      "Iteration 30741, loss = 3.63946367\n",
      "Iteration 30742, loss = 3.62339858\n",
      "Iteration 30743, loss = 3.17926660\n",
      "Iteration 30744, loss = 2.93946183\n",
      "Iteration 30745, loss = 3.04320572\n",
      "Iteration 30746, loss = 3.14319584\n",
      "Iteration 30747, loss = 3.15961594\n",
      "Iteration 30748, loss = 3.14294821\n",
      "Iteration 30749, loss = 3.07879744\n",
      "Iteration 30750, loss = 3.15178050\n",
      "Iteration 30751, loss = 3.10562352\n",
      "Iteration 30752, loss = 3.34818531\n",
      "Iteration 30753, loss = 2.96880488\n",
      "Iteration 30754, loss = 3.29577775\n",
      "Iteration 30755, loss = 3.23406442\n",
      "Iteration 30756, loss = 2.89899688\n",
      "Iteration 30757, loss = 3.21000290\n",
      "Iteration 30758, loss = 3.69330898\n",
      "Iteration 30759, loss = 4.21377449\n",
      "Iteration 30760, loss = 3.71132009\n",
      "Iteration 30761, loss = 3.77804500\n",
      "Iteration 30762, loss = 3.28528428\n",
      "Iteration 30763, loss = 3.10989150\n",
      "Iteration 30764, loss = 3.11782170\n",
      "Iteration 30765, loss = 3.04224928\n",
      "Iteration 30766, loss = 2.75691850\n",
      "Iteration 30767, loss = 3.44044678\n",
      "Iteration 30768, loss = 3.31188753\n",
      "Iteration 30769, loss = 3.32566363\n",
      "Iteration 30770, loss = 3.71053483\n",
      "Iteration 30771, loss = 4.12648703\n",
      "Iteration 30772, loss = 3.66256246\n",
      "Iteration 30773, loss = 3.80483813\n",
      "Iteration 30774, loss = 4.34272520\n",
      "Iteration 30775, loss = 3.75817430\n",
      "Iteration 30776, loss = 4.36842563\n",
      "Iteration 30777, loss = 3.92728741\n",
      "Iteration 30778, loss = 4.76545788\n",
      "Iteration 30779, loss = 3.61183548\n",
      "Iteration 30780, loss = 4.22322700\n",
      "Iteration 30781, loss = 4.82776209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30782, loss = 3.91461798\n",
      "Iteration 30783, loss = 3.68713258\n",
      "Iteration 30784, loss = 3.64829400\n",
      "Iteration 30785, loss = 3.74603235\n",
      "Iteration 30786, loss = 3.17974918\n",
      "Iteration 30787, loss = 4.14915655\n",
      "Iteration 30788, loss = 4.98585480\n",
      "Iteration 30789, loss = 4.88269053\n",
      "Iteration 30790, loss = 4.39474152\n",
      "Iteration 30791, loss = 3.33676496\n",
      "Iteration 30792, loss = 3.40497758\n",
      "Iteration 30793, loss = 3.19937987\n",
      "Iteration 30794, loss = 3.10071375\n",
      "Iteration 30795, loss = 2.96796966\n",
      "Iteration 30796, loss = 2.88770677\n",
      "Iteration 30797, loss = 3.28068946\n",
      "Iteration 30798, loss = 3.14240657\n",
      "Iteration 30799, loss = 3.52999649\n",
      "Iteration 30800, loss = 4.55728691\n",
      "Iteration 30801, loss = 4.38174180\n",
      "Iteration 30802, loss = 3.96967622\n",
      "Iteration 30803, loss = 3.79089017\n",
      "Iteration 30804, loss = 3.88510199\n",
      "Iteration 30805, loss = 3.36510404\n",
      "Iteration 30806, loss = 4.10414844\n",
      "Iteration 30807, loss = 3.97196464\n",
      "Iteration 30808, loss = 4.28333088\n",
      "Iteration 30809, loss = 3.56020348\n",
      "Iteration 30810, loss = 3.67203371\n",
      "Iteration 30811, loss = 3.92380163\n",
      "Iteration 30812, loss = 3.41832138\n",
      "Iteration 30813, loss = 4.08864239\n",
      "Iteration 30814, loss = 3.64599252\n",
      "Iteration 30815, loss = 3.97445689\n",
      "Iteration 30816, loss = 3.52958994\n",
      "Iteration 30817, loss = 3.08442982\n",
      "Iteration 30818, loss = 3.14206264\n",
      "Iteration 30819, loss = 3.20916080\n",
      "Iteration 30820, loss = 3.27595005\n",
      "Iteration 30821, loss = 3.09854312\n",
      "Iteration 30822, loss = 3.00080977\n",
      "Iteration 30823, loss = 3.36505846\n",
      "Iteration 30824, loss = 3.38950616\n",
      "Iteration 30825, loss = 3.04801281\n",
      "Iteration 30826, loss = 2.97474107\n",
      "Iteration 30827, loss = 2.95538837\n",
      "Iteration 30828, loss = 3.26102747\n",
      "Iteration 30829, loss = 3.42150664\n",
      "Iteration 30830, loss = 3.03325021\n",
      "Iteration 30831, loss = 3.25624868\n",
      "Iteration 30832, loss = 2.92274934\n",
      "Iteration 30833, loss = 3.33445403\n",
      "Iteration 30834, loss = 3.54384027\n",
      "Iteration 30835, loss = 4.02080204\n",
      "Iteration 30836, loss = 3.95013868\n",
      "Iteration 30837, loss = 4.16096555\n",
      "Iteration 30838, loss = 4.47876103\n",
      "Iteration 30839, loss = 3.94850687\n",
      "Iteration 30840, loss = 3.50033172\n",
      "Iteration 30841, loss = 3.21820474\n",
      "Iteration 30842, loss = 3.20232793\n",
      "Iteration 30843, loss = 3.25797316\n",
      "Iteration 30844, loss = 3.56315155\n",
      "Iteration 30845, loss = 3.58285525\n",
      "Iteration 30846, loss = 3.92549728\n",
      "Iteration 30847, loss = 3.14246916\n",
      "Iteration 30848, loss = 3.32154603\n",
      "Iteration 30849, loss = 2.84690724\n",
      "Iteration 30850, loss = 3.00339047\n",
      "Iteration 30851, loss = 3.10580653\n",
      "Iteration 30852, loss = 2.98949736\n",
      "Iteration 30853, loss = 2.99101706\n",
      "Iteration 30854, loss = 3.10515854\n",
      "Iteration 30855, loss = 3.18667813\n",
      "Iteration 30856, loss = 3.11611666\n",
      "Iteration 30857, loss = 3.00828029\n",
      "Iteration 30858, loss = 3.46295556\n",
      "Iteration 30859, loss = 3.75388692\n",
      "Iteration 30860, loss = 3.95743575\n",
      "Iteration 30861, loss = 3.72730801\n",
      "Iteration 30862, loss = 4.13437490\n",
      "Iteration 30863, loss = 3.91315798\n",
      "Iteration 30864, loss = 4.20211870\n",
      "Iteration 30865, loss = 4.11746437\n",
      "Iteration 30866, loss = 5.40067300\n",
      "Iteration 30867, loss = 4.45429729\n",
      "Iteration 30868, loss = 4.03454866\n",
      "Iteration 30869, loss = 3.24045323\n",
      "Iteration 30870, loss = 2.97255256\n",
      "Iteration 30871, loss = 2.94077740\n",
      "Iteration 30872, loss = 3.14719575\n",
      "Iteration 30873, loss = 3.14383680\n",
      "Iteration 30874, loss = 3.21490698\n",
      "Iteration 30875, loss = 3.63241391\n",
      "Iteration 30876, loss = 3.61161437\n",
      "Iteration 30877, loss = 3.27376440\n",
      "Iteration 30878, loss = 3.60662217\n",
      "Iteration 30879, loss = 3.63191467\n",
      "Iteration 30880, loss = 3.76243867\n",
      "Iteration 30881, loss = 3.18156156\n",
      "Iteration 30882, loss = 3.08046105\n",
      "Iteration 30883, loss = 3.39949429\n",
      "Iteration 30884, loss = 3.00650260\n",
      "Iteration 30885, loss = 2.95552891\n",
      "Iteration 30886, loss = 2.92067156\n",
      "Iteration 30887, loss = 3.48320834\n",
      "Iteration 30888, loss = 3.77414528\n",
      "Iteration 30889, loss = 2.94444140\n",
      "Iteration 30890, loss = 2.75328047\n",
      "Iteration 30891, loss = 2.78584229\n",
      "Iteration 30892, loss = 3.81589906\n",
      "Iteration 30893, loss = 3.77671761\n",
      "Iteration 30894, loss = 3.48742814\n",
      "Iteration 30895, loss = 3.21896200\n",
      "Iteration 30896, loss = 3.01244964\n",
      "Iteration 30897, loss = 3.13460210\n",
      "Iteration 30898, loss = 3.12991854\n",
      "Iteration 30899, loss = 3.41714112\n",
      "Iteration 30900, loss = 3.33926245\n",
      "Iteration 30901, loss = 2.97242373\n",
      "Iteration 30902, loss = 3.22284772\n",
      "Iteration 30903, loss = 3.22379475\n",
      "Iteration 30904, loss = 3.12767776\n",
      "Iteration 30905, loss = 3.30013984\n",
      "Iteration 30906, loss = 3.49551726\n",
      "Iteration 30907, loss = 3.37313681\n",
      "Iteration 30908, loss = 2.99828438\n",
      "Iteration 30909, loss = 3.63960178\n",
      "Iteration 30910, loss = 4.64682143\n",
      "Iteration 30911, loss = 4.18499612\n",
      "Iteration 30912, loss = 4.41781999\n",
      "Iteration 30913, loss = 3.93673061\n",
      "Iteration 30914, loss = 3.43825555\n",
      "Iteration 30915, loss = 3.77705048\n",
      "Iteration 30916, loss = 4.22184243\n",
      "Iteration 30917, loss = 3.62191071\n",
      "Iteration 30918, loss = 4.43442328\n",
      "Iteration 30919, loss = 3.14535856\n",
      "Iteration 30920, loss = 4.13974777\n",
      "Iteration 30921, loss = 3.79125034\n",
      "Iteration 30922, loss = 3.29294324\n",
      "Iteration 30923, loss = 3.00469424\n",
      "Iteration 30924, loss = 3.81099394\n",
      "Iteration 30925, loss = 3.58415860\n",
      "Iteration 30926, loss = 4.53509893\n",
      "Iteration 30927, loss = 4.09347267\n",
      "Iteration 30928, loss = 4.47724641\n",
      "Iteration 30929, loss = 4.14002145\n",
      "Iteration 30930, loss = 3.69271166\n",
      "Iteration 30931, loss = 3.82112176\n",
      "Iteration 30932, loss = 3.55554449\n",
      "Iteration 30933, loss = 3.61564125\n",
      "Iteration 30934, loss = 3.87667104\n",
      "Iteration 30935, loss = 3.36543212\n",
      "Iteration 30936, loss = 2.91167296\n",
      "Iteration 30937, loss = 3.00007798\n",
      "Iteration 30938, loss = 3.72848475\n",
      "Iteration 30939, loss = 3.25043918\n",
      "Iteration 30940, loss = 3.21989609\n",
      "Iteration 30941, loss = 2.97192768\n",
      "Iteration 30942, loss = 3.20043341\n",
      "Iteration 30943, loss = 2.99997324\n",
      "Iteration 30944, loss = 2.91805417\n",
      "Iteration 30945, loss = 3.18055872\n",
      "Iteration 30946, loss = 3.22660571\n",
      "Iteration 30947, loss = 3.09634958\n",
      "Iteration 30948, loss = 3.39956790\n",
      "Iteration 30949, loss = 3.75966907\n",
      "Iteration 30950, loss = 3.61987669\n",
      "Iteration 30951, loss = 3.31075293\n",
      "Iteration 30952, loss = 3.29656323\n",
      "Iteration 30953, loss = 3.29719568\n",
      "Iteration 30954, loss = 3.58913445\n",
      "Iteration 30955, loss = 3.45074845\n",
      "Iteration 30956, loss = 3.63663192\n",
      "Iteration 30957, loss = 3.16256130\n",
      "Iteration 30958, loss = 3.52630153\n",
      "Iteration 30959, loss = 3.74992529\n",
      "Iteration 30960, loss = 3.41645241\n",
      "Iteration 30961, loss = 2.87144946\n",
      "Iteration 30962, loss = 3.77777014\n",
      "Iteration 30963, loss = 3.77374171\n",
      "Iteration 30964, loss = 3.24171376\n",
      "Iteration 30965, loss = 3.48801783\n",
      "Iteration 30966, loss = 3.29101718\n",
      "Iteration 30967, loss = 3.51243694\n",
      "Iteration 30968, loss = 3.28319259\n",
      "Iteration 30969, loss = 3.84746014\n",
      "Iteration 30970, loss = 4.48347228\n",
      "Iteration 30971, loss = 3.58629041\n",
      "Iteration 30972, loss = 4.24224691\n",
      "Iteration 30973, loss = 4.04670276\n",
      "Iteration 30974, loss = 3.38907896\n",
      "Iteration 30975, loss = 4.02589819\n",
      "Iteration 30976, loss = 4.47963198\n",
      "Iteration 30977, loss = 5.10920802\n",
      "Iteration 30978, loss = 4.07782940\n",
      "Iteration 30979, loss = 4.01281613\n",
      "Iteration 30980, loss = 3.91573330\n",
      "Iteration 30981, loss = 4.09640209\n",
      "Iteration 30982, loss = 5.04895429\n",
      "Iteration 30983, loss = 4.97564091\n",
      "Iteration 30984, loss = 4.76248493\n",
      "Iteration 30985, loss = 3.97928312\n",
      "Iteration 30986, loss = 3.34123504\n",
      "Iteration 30987, loss = 3.55816907\n",
      "Iteration 30988, loss = 3.87845998\n",
      "Iteration 30989, loss = 3.77092818\n",
      "Iteration 30990, loss = 3.43191561\n",
      "Iteration 30991, loss = 3.91962227\n",
      "Iteration 30992, loss = 3.15768671\n",
      "Iteration 30993, loss = 2.84670287\n",
      "Iteration 30994, loss = 3.48358650\n",
      "Iteration 30995, loss = 3.19624966\n",
      "Iteration 30996, loss = 2.98950455\n",
      "Iteration 30997, loss = 3.11075246\n",
      "Iteration 30998, loss = 3.07297475\n",
      "Iteration 30999, loss = 3.81509503\n",
      "Iteration 31000, loss = 3.48384605\n",
      "Iteration 31001, loss = 3.09949993\n",
      "Iteration 31002, loss = 3.07290076\n",
      "Iteration 31003, loss = 4.07978901\n",
      "Iteration 31004, loss = 3.31400395\n",
      "Iteration 31005, loss = 3.68176403\n",
      "Iteration 31006, loss = 3.62851197\n",
      "Iteration 31007, loss = 3.88158922\n",
      "Iteration 31008, loss = 3.57473174\n",
      "Iteration 31009, loss = 3.16477452\n",
      "Iteration 31010, loss = 3.67451155\n",
      "Iteration 31011, loss = 3.40820656\n",
      "Iteration 31012, loss = 2.98148603\n",
      "Iteration 31013, loss = 2.89737729\n",
      "Iteration 31014, loss = 2.95425442\n",
      "Iteration 31015, loss = 3.34479405\n",
      "Iteration 31016, loss = 3.44299894\n",
      "Iteration 31017, loss = 4.00551268\n",
      "Iteration 31018, loss = 3.77097527\n",
      "Iteration 31019, loss = 4.17243395\n",
      "Iteration 31020, loss = 4.04759714\n",
      "Iteration 31021, loss = 3.57402537\n",
      "Iteration 31022, loss = 3.15219616\n",
      "Iteration 31023, loss = 3.07304351\n",
      "Iteration 31024, loss = 2.99092667\n",
      "Iteration 31025, loss = 3.84362455\n",
      "Iteration 31026, loss = 3.61276762\n",
      "Iteration 31027, loss = 3.85948736\n",
      "Iteration 31028, loss = 3.81307988\n",
      "Iteration 31029, loss = 4.14725422\n",
      "Iteration 31030, loss = 3.87086779\n",
      "Iteration 31031, loss = 3.11931696\n",
      "Iteration 31032, loss = 3.57886299\n",
      "Iteration 31033, loss = 3.73754833\n",
      "Iteration 31034, loss = 3.15680383\n",
      "Iteration 31035, loss = 3.09198220\n",
      "Iteration 31036, loss = 3.11371220\n",
      "Iteration 31037, loss = 3.00937011\n",
      "Iteration 31038, loss = 3.52145861\n",
      "Iteration 31039, loss = 3.14682809\n",
      "Iteration 31040, loss = 3.11005381\n",
      "Iteration 31041, loss = 3.01155158\n",
      "Iteration 31042, loss = 3.14867414\n",
      "Iteration 31043, loss = 3.08065905\n",
      "Iteration 31044, loss = 3.33818196\n",
      "Iteration 31045, loss = 3.88862820\n",
      "Iteration 31046, loss = 4.02356341\n",
      "Iteration 31047, loss = 3.59733625\n",
      "Iteration 31048, loss = 4.88244523\n",
      "Iteration 31049, loss = 5.24239292\n",
      "Iteration 31050, loss = 5.44310260\n",
      "Iteration 31051, loss = 6.14680489\n",
      "Iteration 31052, loss = 4.97290456\n",
      "Iteration 31053, loss = 3.40738849\n",
      "Iteration 31054, loss = 3.85241597\n",
      "Iteration 31055, loss = 3.93072619\n",
      "Iteration 31056, loss = 4.38084258\n",
      "Iteration 31057, loss = 3.56753430\n",
      "Iteration 31058, loss = 3.52346483\n",
      "Iteration 31059, loss = 3.33025743\n",
      "Iteration 31060, loss = 2.98682285\n",
      "Iteration 31061, loss = 2.98988635\n",
      "Iteration 31062, loss = 3.31948973\n",
      "Iteration 31063, loss = 3.55306955\n",
      "Iteration 31064, loss = 3.47610853\n",
      "Iteration 31065, loss = 3.53723110\n",
      "Iteration 31066, loss = 3.59928135\n",
      "Iteration 31067, loss = 3.49203533\n",
      "Iteration 31068, loss = 3.63935882\n",
      "Iteration 31069, loss = 3.19472672\n",
      "Iteration 31070, loss = 3.17733567\n",
      "Iteration 31071, loss = 3.18278593\n",
      "Iteration 31072, loss = 3.33084182\n",
      "Iteration 31073, loss = 3.82879302\n",
      "Iteration 31074, loss = 3.50227128\n",
      "Iteration 31075, loss = 4.27872908\n",
      "Iteration 31076, loss = 4.08672825\n",
      "Iteration 31077, loss = 3.64121013\n",
      "Iteration 31078, loss = 3.17592872\n",
      "Iteration 31079, loss = 3.28978669\n",
      "Iteration 31080, loss = 3.06492283\n",
      "Iteration 31081, loss = 3.16206942\n",
      "Iteration 31082, loss = 3.27854398\n",
      "Iteration 31083, loss = 2.92452136\n",
      "Iteration 31084, loss = 3.00267890\n",
      "Iteration 31085, loss = 3.01275360\n",
      "Iteration 31086, loss = 2.82632802\n",
      "Iteration 31087, loss = 2.93644926\n",
      "Iteration 31088, loss = 3.33727059\n",
      "Iteration 31089, loss = 3.74017195\n",
      "Iteration 31090, loss = 5.24613260\n",
      "Iteration 31091, loss = 4.60008824\n",
      "Iteration 31092, loss = 4.24292818\n",
      "Iteration 31093, loss = 4.04806678\n",
      "Iteration 31094, loss = 4.06680948\n",
      "Iteration 31095, loss = 3.27172163\n",
      "Iteration 31096, loss = 4.08740554\n",
      "Iteration 31097, loss = 3.40862838\n",
      "Iteration 31098, loss = 3.74659822\n",
      "Iteration 31099, loss = 3.90139079\n",
      "Iteration 31100, loss = 3.83990510\n",
      "Iteration 31101, loss = 4.28120862\n",
      "Iteration 31102, loss = 3.82954490\n",
      "Iteration 31103, loss = 3.34167377\n",
      "Iteration 31104, loss = 3.77007592\n",
      "Iteration 31105, loss = 3.55840029\n",
      "Iteration 31106, loss = 3.14192340\n",
      "Iteration 31107, loss = 2.99332252\n",
      "Iteration 31108, loss = 3.01337775\n",
      "Iteration 31109, loss = 3.52604519\n",
      "Iteration 31110, loss = 3.44449627\n",
      "Iteration 31111, loss = 3.01354669\n",
      "Iteration 31112, loss = 2.88597515\n",
      "Iteration 31113, loss = 3.10724731\n",
      "Iteration 31114, loss = 2.78547276\n",
      "Iteration 31115, loss = 3.19385690\n",
      "Iteration 31116, loss = 3.06375800\n",
      "Iteration 31117, loss = 3.01825094\n",
      "Iteration 31118, loss = 2.98571963\n",
      "Iteration 31119, loss = 3.05001042\n",
      "Iteration 31120, loss = 3.41291101\n",
      "Iteration 31121, loss = 3.66952243\n",
      "Iteration 31122, loss = 3.29352007\n",
      "Iteration 31123, loss = 3.48848604\n",
      "Iteration 31124, loss = 3.13295273\n",
      "Iteration 31125, loss = 3.29166985\n",
      "Iteration 31126, loss = 3.66630501\n",
      "Iteration 31127, loss = 3.25801937\n",
      "Iteration 31128, loss = 2.93504640\n",
      "Iteration 31129, loss = 3.15557593\n",
      "Iteration 31130, loss = 2.81545152\n",
      "Iteration 31131, loss = 3.63903465\n",
      "Iteration 31132, loss = 3.52785955\n",
      "Iteration 31133, loss = 3.68402958\n",
      "Iteration 31134, loss = 3.29838928\n",
      "Iteration 31135, loss = 3.71665471\n",
      "Iteration 31136, loss = 3.77522333\n",
      "Iteration 31137, loss = 3.09385614\n",
      "Iteration 31138, loss = 3.19165279\n",
      "Iteration 31139, loss = 3.60058622\n",
      "Iteration 31140, loss = 4.06145615\n",
      "Iteration 31141, loss = 3.57566880\n",
      "Iteration 31142, loss = 3.35550250\n",
      "Iteration 31143, loss = 3.13524689\n",
      "Iteration 31144, loss = 3.25100634\n",
      "Iteration 31145, loss = 3.71322207\n",
      "Iteration 31146, loss = 3.82963796\n",
      "Iteration 31147, loss = 3.75629875\n",
      "Iteration 31148, loss = 3.63431298\n",
      "Iteration 31149, loss = 3.34329285\n",
      "Iteration 31150, loss = 3.12664311\n",
      "Iteration 31151, loss = 3.00783249\n",
      "Iteration 31152, loss = 3.01323629\n",
      "Iteration 31153, loss = 3.11222162\n",
      "Iteration 31154, loss = 3.19770033\n",
      "Iteration 31155, loss = 3.40837473\n",
      "Iteration 31156, loss = 3.27413808\n",
      "Iteration 31157, loss = 3.84322689\n",
      "Iteration 31158, loss = 3.75035493\n",
      "Iteration 31159, loss = 3.27767973\n",
      "Iteration 31160, loss = 4.22812755\n",
      "Iteration 31161, loss = 4.05851156\n",
      "Iteration 31162, loss = 3.92655321\n",
      "Iteration 31163, loss = 3.77291331\n",
      "Iteration 31164, loss = 5.45795224\n",
      "Iteration 31165, loss = 4.69045967\n",
      "Iteration 31166, loss = 3.74102048\n",
      "Iteration 31167, loss = 3.94130599\n",
      "Iteration 31168, loss = 4.52566427\n",
      "Iteration 31169, loss = 4.19700088\n",
      "Iteration 31170, loss = 3.69295813\n",
      "Iteration 31171, loss = 3.68684907\n",
      "Iteration 31172, loss = 3.98239441\n",
      "Iteration 31173, loss = 4.90203520\n",
      "Iteration 31174, loss = 4.87253542\n",
      "Iteration 31175, loss = 4.62026533\n",
      "Iteration 31176, loss = 3.59881211\n",
      "Iteration 31177, loss = 3.27839915\n",
      "Iteration 31178, loss = 3.68618918\n",
      "Iteration 31179, loss = 4.13115962\n",
      "Iteration 31180, loss = 3.32509125\n",
      "Iteration 31181, loss = 3.29581916\n",
      "Iteration 31182, loss = 3.10417195\n",
      "Iteration 31183, loss = 2.96743678\n",
      "Iteration 31184, loss = 2.98382351\n",
      "Iteration 31185, loss = 2.88536364\n",
      "Iteration 31186, loss = 3.66235179\n",
      "Iteration 31187, loss = 3.48581407\n",
      "Iteration 31188, loss = 3.76350039\n",
      "Iteration 31189, loss = 3.56328289\n",
      "Iteration 31190, loss = 3.58170880\n",
      "Iteration 31191, loss = 3.70490240\n",
      "Iteration 31192, loss = 4.09190638\n",
      "Iteration 31193, loss = 4.59777852\n",
      "Iteration 31194, loss = 4.01204233\n",
      "Iteration 31195, loss = 4.55056577\n",
      "Iteration 31196, loss = 6.61904384\n",
      "Iteration 31197, loss = 4.38888469\n",
      "Iteration 31198, loss = 3.26605520\n",
      "Iteration 31199, loss = 3.62646474\n",
      "Iteration 31200, loss = 3.84806562\n",
      "Iteration 31201, loss = 3.59740820\n",
      "Iteration 31202, loss = 3.48991989\n",
      "Iteration 31203, loss = 3.07478849\n",
      "Iteration 31204, loss = 3.13474426\n",
      "Iteration 31205, loss = 2.85238063\n",
      "Iteration 31206, loss = 3.37813969\n",
      "Iteration 31207, loss = 3.19918232\n",
      "Iteration 31208, loss = 3.46598229\n",
      "Iteration 31209, loss = 2.97778517\n",
      "Iteration 31210, loss = 3.22501171\n",
      "Iteration 31211, loss = 3.14223989\n",
      "Iteration 31212, loss = 3.01843022\n",
      "Iteration 31213, loss = 2.95754553\n",
      "Iteration 31214, loss = 3.25827628\n",
      "Iteration 31215, loss = 3.11579761\n",
      "Iteration 31216, loss = 3.24926901\n",
      "Iteration 31217, loss = 3.08102094\n",
      "Iteration 31218, loss = 4.44482268\n",
      "Iteration 31219, loss = 4.04203207\n",
      "Iteration 31220, loss = 4.04942269\n",
      "Iteration 31221, loss = 3.38436669\n",
      "Iteration 31222, loss = 4.02632808\n",
      "Iteration 31223, loss = 4.75143569\n",
      "Iteration 31224, loss = 3.76145453\n",
      "Iteration 31225, loss = 3.88441956\n",
      "Iteration 31226, loss = 3.32541239\n",
      "Iteration 31227, loss = 3.01877291\n",
      "Iteration 31228, loss = 3.26181493\n",
      "Iteration 31229, loss = 3.26886110\n",
      "Iteration 31230, loss = 2.96735335\n",
      "Iteration 31231, loss = 2.97798932\n",
      "Iteration 31232, loss = 3.08590960\n",
      "Iteration 31233, loss = 3.47618306\n",
      "Iteration 31234, loss = 4.77727657\n",
      "Iteration 31235, loss = 4.05052709\n",
      "Iteration 31236, loss = 3.24692018\n",
      "Iteration 31237, loss = 3.48884598\n",
      "Iteration 31238, loss = 3.23169012\n",
      "Iteration 31239, loss = 3.43667371\n",
      "Iteration 31240, loss = 4.12843209\n",
      "Iteration 31241, loss = 3.43259604\n",
      "Iteration 31242, loss = 3.46506631\n",
      "Iteration 31243, loss = 3.39890168\n",
      "Iteration 31244, loss = 3.31371593\n",
      "Iteration 31245, loss = 3.23429774\n",
      "Iteration 31246, loss = 2.75109014\n",
      "Iteration 31247, loss = 3.22763205\n",
      "Iteration 31248, loss = 4.12592737\n",
      "Iteration 31249, loss = 4.71289797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 31250, loss = 4.14513072\n",
      "Iteration 31251, loss = 3.90478117\n",
      "Iteration 31252, loss = 4.00511041\n",
      "Iteration 31253, loss = 4.11733003\n",
      "Iteration 31254, loss = 4.14459955\n",
      "Iteration 31255, loss = 4.24028598\n",
      "Iteration 31256, loss = 4.70526692\n",
      "Iteration 31257, loss = 4.63616319\n",
      "Iteration 31258, loss = 5.71870914\n",
      "Iteration 31259, loss = 7.81105997\n",
      "Iteration 31260, loss = 6.01589411\n",
      "Iteration 31261, loss = 5.85098454\n",
      "Iteration 31262, loss = 4.28153995\n",
      "Iteration 31263, loss = 4.42353931\n",
      "Iteration 31264, loss = 3.46093021\n",
      "Iteration 31265, loss = 3.27631292\n",
      "Iteration 31266, loss = 3.00949356\n",
      "Iteration 31267, loss = 3.44805584\n",
      "Iteration 31268, loss = 3.66736567\n",
      "Iteration 31269, loss = 3.82409680\n",
      "Iteration 31270, loss = 3.71202420\n",
      "Iteration 31271, loss = 3.79070530\n",
      "Iteration 31272, loss = 3.18143525\n",
      "Iteration 31273, loss = 3.02638513\n",
      "Iteration 31274, loss = 3.13321223\n",
      "Iteration 31275, loss = 3.08852211\n",
      "Iteration 31276, loss = 3.04022387\n",
      "Iteration 31277, loss = 3.27519437\n",
      "Iteration 31278, loss = 3.79025940\n",
      "Iteration 31279, loss = 3.43554007\n",
      "Iteration 31280, loss = 3.29631964\n",
      "Iteration 31281, loss = 3.30812823\n",
      "Iteration 31282, loss = 3.63404201\n",
      "Iteration 31283, loss = 5.19529789\n",
      "Iteration 31284, loss = 6.12689920\n",
      "Iteration 31285, loss = 6.09320869\n",
      "Iteration 31286, loss = 5.41813156\n",
      "Iteration 31287, loss = 5.18566436\n",
      "Iteration 31288, loss = 4.40978092\n",
      "Iteration 31289, loss = 5.53639459\n",
      "Iteration 31290, loss = 4.76093442\n",
      "Iteration 31291, loss = 5.78402490\n",
      "Iteration 31292, loss = 4.46601189\n",
      "Iteration 31293, loss = 3.85305636\n",
      "Iteration 31294, loss = 3.24315772\n",
      "Iteration 31295, loss = 4.06524067\n",
      "Iteration 31296, loss = 3.75866181\n",
      "Iteration 31297, loss = 4.46315824\n",
      "Iteration 31298, loss = 4.96837830\n",
      "Iteration 31299, loss = 4.79187310\n",
      "Iteration 31300, loss = 4.54719488\n",
      "Iteration 31301, loss = 4.80418418\n",
      "Iteration 31302, loss = 4.54581999\n",
      "Iteration 31303, loss = 4.20258995\n",
      "Iteration 31304, loss = 3.92635097\n",
      "Iteration 31305, loss = 3.71904659\n",
      "Iteration 31306, loss = 3.39642789\n",
      "Iteration 31307, loss = 3.06465790\n",
      "Iteration 31308, loss = 2.87091909\n",
      "Iteration 31309, loss = 2.84767394\n",
      "Iteration 31310, loss = 3.02739971\n",
      "Iteration 31311, loss = 3.18081068\n",
      "Iteration 31312, loss = 3.18303888\n",
      "Iteration 31313, loss = 2.90919666\n",
      "Iteration 31314, loss = 2.82413330\n",
      "Iteration 31315, loss = 2.74953049\n",
      "Iteration 31316, loss = 2.91141411\n",
      "Iteration 31317, loss = 3.23697602\n",
      "Iteration 31318, loss = 3.35818707\n",
      "Iteration 31319, loss = 3.93057054\n",
      "Iteration 31320, loss = 4.27641809\n",
      "Iteration 31321, loss = 3.87159818\n",
      "Iteration 31322, loss = 5.45248899\n",
      "Iteration 31323, loss = 4.50420049\n",
      "Iteration 31324, loss = 5.02838246\n",
      "Iteration 31325, loss = 4.51193916\n",
      "Iteration 31326, loss = 4.31586346\n",
      "Iteration 31327, loss = 4.18139211\n",
      "Iteration 31328, loss = 3.87931603\n",
      "Iteration 31329, loss = 3.92320762\n",
      "Iteration 31330, loss = 3.29622012\n",
      "Iteration 31331, loss = 3.32344471\n",
      "Iteration 31332, loss = 3.93408473\n",
      "Iteration 31333, loss = 3.65889116\n",
      "Iteration 31334, loss = 3.15022492\n",
      "Iteration 31335, loss = 3.23227420\n",
      "Iteration 31336, loss = 3.11519906\n",
      "Iteration 31337, loss = 3.31418105\n",
      "Iteration 31338, loss = 2.90290997\n",
      "Iteration 31339, loss = 2.97120873\n",
      "Iteration 31340, loss = 3.24415177\n",
      "Iteration 31341, loss = 3.48772833\n",
      "Iteration 31342, loss = 3.99834668\n",
      "Iteration 31343, loss = 4.14878926\n",
      "Iteration 31344, loss = 4.67130191\n",
      "Iteration 31345, loss = 3.95936468\n",
      "Iteration 31346, loss = 4.78276987\n",
      "Iteration 31347, loss = 3.06935991\n",
      "Iteration 31348, loss = 3.35155885\n",
      "Iteration 31349, loss = 3.45754766\n",
      "Iteration 31350, loss = 3.83671286\n",
      "Iteration 31351, loss = 3.22446669\n",
      "Iteration 31352, loss = 2.87779565\n",
      "Iteration 31353, loss = 2.87957834\n",
      "Iteration 31354, loss = 3.96860619\n",
      "Iteration 31355, loss = 3.54293384\n",
      "Iteration 31356, loss = 3.35438835\n",
      "Iteration 31357, loss = 3.06624771\n",
      "Iteration 31358, loss = 3.45462506\n",
      "Iteration 31359, loss = 3.24644684\n",
      "Iteration 31360, loss = 3.48859178\n",
      "Iteration 31361, loss = 3.42066824\n",
      "Iteration 31362, loss = 3.51012184\n",
      "Iteration 31363, loss = 3.91788789\n",
      "Iteration 31364, loss = 4.62922306\n",
      "Iteration 31365, loss = 3.90204045\n",
      "Iteration 31366, loss = 4.80298162\n",
      "Iteration 31367, loss = 4.40046859\n",
      "Iteration 31368, loss = 3.99042919\n",
      "Iteration 31369, loss = 3.43512174\n",
      "Iteration 31370, loss = 3.74043513\n",
      "Iteration 31371, loss = 3.16791907\n",
      "Iteration 31372, loss = 3.60566949\n",
      "Iteration 31373, loss = 3.63996733\n",
      "Iteration 31374, loss = 3.32219295\n",
      "Iteration 31375, loss = 3.19122960\n",
      "Iteration 31376, loss = 3.44005660\n",
      "Iteration 31377, loss = 2.87970861\n",
      "Iteration 31378, loss = 3.08201359\n",
      "Iteration 31379, loss = 2.83321959\n",
      "Iteration 31380, loss = 3.13146890\n",
      "Iteration 31381, loss = 3.53387806\n",
      "Iteration 31382, loss = 3.46298698\n",
      "Iteration 31383, loss = 4.04989440\n",
      "Iteration 31384, loss = 3.85828728\n",
      "Iteration 31385, loss = 3.70161633\n",
      "Iteration 31386, loss = 3.32141964\n",
      "Iteration 31387, loss = 4.24469356\n",
      "Iteration 31388, loss = 3.22334059\n",
      "Iteration 31389, loss = 3.36376015\n",
      "Iteration 31390, loss = 4.12103048\n",
      "Iteration 31391, loss = 4.38737466\n",
      "Iteration 31392, loss = 3.98999542\n",
      "Iteration 31393, loss = 3.51100496\n",
      "Iteration 31394, loss = 3.40771991\n",
      "Iteration 31395, loss = 3.46355470\n",
      "Iteration 31396, loss = 3.57009619\n",
      "Iteration 31397, loss = 3.48918639\n",
      "Iteration 31398, loss = 3.15444855\n",
      "Iteration 31399, loss = 3.68056886\n",
      "Iteration 31400, loss = 2.82731865\n",
      "Iteration 31401, loss = 2.84372383\n",
      "Iteration 31402, loss = 2.94928455\n",
      "Iteration 31403, loss = 3.25495510\n",
      "Iteration 31404, loss = 3.70603506\n",
      "Iteration 31405, loss = 4.32715195\n",
      "Iteration 31406, loss = 3.45553688\n",
      "Iteration 31407, loss = 3.10499673\n",
      "Iteration 31408, loss = 2.92828413\n",
      "Iteration 31409, loss = 3.57487031\n",
      "Iteration 31410, loss = 3.47189012\n",
      "Iteration 31411, loss = 3.70156671\n",
      "Iteration 31412, loss = 3.65222256\n",
      "Iteration 31413, loss = 3.67258226\n",
      "Iteration 31414, loss = 3.12464142\n",
      "Iteration 31415, loss = 3.05494739\n",
      "Iteration 31416, loss = 3.30771210\n",
      "Iteration 31417, loss = 3.26756881\n",
      "Iteration 31418, loss = 3.09153729\n",
      "Iteration 31419, loss = 3.05888910\n",
      "Iteration 31420, loss = 2.91662039\n",
      "Iteration 31421, loss = 3.37929960\n",
      "Iteration 31422, loss = 3.05751634\n",
      "Iteration 31423, loss = 2.86283604\n",
      "Iteration 31424, loss = 2.74874277\n",
      "Iteration 31425, loss = 3.02105724\n",
      "Iteration 31426, loss = 3.55688954\n",
      "Iteration 31427, loss = 3.55641450\n",
      "Iteration 31428, loss = 5.05494100\n",
      "Iteration 31429, loss = 4.32938417\n",
      "Iteration 31430, loss = 3.78948479\n",
      "Iteration 31431, loss = 3.52649083\n",
      "Iteration 31432, loss = 3.11519154\n",
      "Iteration 31433, loss = 3.11129836\n",
      "Iteration 31434, loss = 3.49554603\n",
      "Iteration 31435, loss = 4.10138424\n",
      "Iteration 31436, loss = 4.03867703\n",
      "Iteration 31437, loss = 3.59206528\n",
      "Iteration 31438, loss = 3.27308958\n",
      "Iteration 31439, loss = 3.18123381\n",
      "Iteration 31440, loss = 3.56834986\n",
      "Iteration 31441, loss = 3.69828092\n",
      "Iteration 31442, loss = 4.06046296\n",
      "Iteration 31443, loss = 3.37842051\n",
      "Iteration 31444, loss = 3.23986274\n",
      "Iteration 31445, loss = 3.52417484\n",
      "Iteration 31446, loss = 3.96517916\n",
      "Iteration 31447, loss = 3.66047952\n",
      "Iteration 31448, loss = 3.59294823\n",
      "Iteration 31449, loss = 3.44627599\n",
      "Iteration 31450, loss = 3.01233070\n",
      "Iteration 31451, loss = 3.40127550\n",
      "Iteration 31452, loss = 3.37605663\n",
      "Iteration 31453, loss = 3.23705984\n",
      "Iteration 31454, loss = 3.70511011\n",
      "Iteration 31455, loss = 2.89756505\n",
      "Iteration 31456, loss = 3.11036595\n",
      "Iteration 31457, loss = 3.30169175\n",
      "Iteration 31458, loss = 3.35163514\n",
      "Iteration 31459, loss = 3.22898582\n",
      "Iteration 31460, loss = 2.96578194\n",
      "Iteration 31461, loss = 2.90101833\n",
      "Iteration 31462, loss = 2.75379071\n",
      "Iteration 31463, loss = 3.26731498\n",
      "Iteration 31464, loss = 3.06734677\n",
      "Iteration 31465, loss = 2.90342397\n",
      "Iteration 31466, loss = 3.00007622\n",
      "Iteration 31467, loss = 3.15636459\n",
      "Iteration 31468, loss = 3.01456413\n",
      "Iteration 31469, loss = 2.89011294\n",
      "Iteration 31470, loss = 2.85763141\n",
      "Iteration 31471, loss = 3.73655102\n",
      "Iteration 31472, loss = 3.66658921\n",
      "Iteration 31473, loss = 3.19239894\n",
      "Iteration 31474, loss = 3.40263525\n",
      "Iteration 31475, loss = 3.18725892\n",
      "Iteration 31476, loss = 3.19615472\n",
      "Iteration 31477, loss = 3.05507404\n",
      "Iteration 31478, loss = 3.53490411\n",
      "Iteration 31479, loss = 3.19388677\n",
      "Iteration 31480, loss = 3.23548821\n",
      "Iteration 31481, loss = 3.61366900\n",
      "Iteration 31482, loss = 3.41383383\n",
      "Iteration 31483, loss = 3.71084992\n",
      "Iteration 31484, loss = 3.03720299\n",
      "Iteration 31485, loss = 2.91676040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 31486, loss = 2.97766404\n",
      "Iteration 31487, loss = 3.02325304\n",
      "Iteration 31488, loss = 3.00025168\n",
      "Iteration 31489, loss = 2.70559262\n",
      "Iteration 31490, loss = 3.32365110\n",
      "Iteration 31491, loss = 2.90674085\n",
      "Iteration 31492, loss = 3.09241350\n",
      "Iteration 31493, loss = 3.04070751\n",
      "Iteration 31494, loss = 3.20278280\n",
      "Iteration 31495, loss = 3.48991834\n",
      "Iteration 31496, loss = 3.33151189\n",
      "Iteration 31497, loss = 2.98281109\n",
      "Iteration 31498, loss = 3.01249212\n",
      "Iteration 31499, loss = 2.76940140\n",
      "Iteration 31500, loss = 3.03563127\n",
      "Iteration 31501, loss = 2.87618728\n",
      "Iteration 31502, loss = 2.89150292\n",
      "Iteration 31503, loss = 3.12322784\n",
      "Iteration 31504, loss = 2.91592332\n",
      "Iteration 31505, loss = 3.03739506\n",
      "Iteration 31506, loss = 3.42064584\n",
      "Iteration 31507, loss = 3.35071684\n",
      "Iteration 31508, loss = 4.15243103\n",
      "Iteration 31509, loss = 4.30287172\n",
      "Iteration 31510, loss = 4.48862017\n",
      "Iteration 31511, loss = 5.49021816\n",
      "Iteration 31512, loss = 5.73502944\n",
      "Iteration 31513, loss = 6.72012870\n",
      "Iteration 31514, loss = 4.65963798\n",
      "Iteration 31515, loss = 4.43720832\n",
      "Iteration 31516, loss = 3.41256950\n",
      "Iteration 31517, loss = 3.02394831\n",
      "Iteration 31518, loss = 2.74310763\n",
      "Iteration 31519, loss = 2.85926991\n",
      "Iteration 31520, loss = 3.01933726\n",
      "Iteration 31521, loss = 4.17971317\n",
      "Iteration 31522, loss = 3.68105721\n",
      "Iteration 31523, loss = 4.00015846\n",
      "Iteration 31524, loss = 3.35736894\n",
      "Iteration 31525, loss = 3.85770320\n",
      "Iteration 31526, loss = 3.52968899\n",
      "Iteration 31527, loss = 3.05015215\n",
      "Iteration 31528, loss = 3.11229077\n",
      "Iteration 31529, loss = 2.84710637\n",
      "Iteration 31530, loss = 2.84878920\n",
      "Iteration 31531, loss = 2.82599399\n",
      "Iteration 31532, loss = 2.75282189\n",
      "Iteration 31533, loss = 3.18790069\n",
      "Iteration 31534, loss = 2.75831747\n",
      "Iteration 31535, loss = 2.76556161\n",
      "Iteration 31536, loss = 3.01535741\n",
      "Iteration 31537, loss = 3.20456107\n",
      "Iteration 31538, loss = 3.20932542\n",
      "Iteration 31539, loss = 3.01335053\n",
      "Iteration 31540, loss = 3.35570715\n",
      "Iteration 31541, loss = 3.56397121\n",
      "Iteration 31542, loss = 6.06172100\n",
      "Iteration 31543, loss = 5.89106007\n",
      "Iteration 31544, loss = 7.38192743\n",
      "Iteration 31545, loss = 5.91001579\n",
      "Iteration 31546, loss = 4.24562663\n",
      "Iteration 31547, loss = 3.85163170\n",
      "Iteration 31548, loss = 3.50013159\n",
      "Iteration 31549, loss = 3.55668013\n",
      "Iteration 31550, loss = 3.08874231\n",
      "Iteration 31551, loss = 3.34071024\n",
      "Iteration 31552, loss = 3.26072828\n",
      "Iteration 31553, loss = 4.43024670\n",
      "Iteration 31554, loss = 3.82752596\n",
      "Iteration 31555, loss = 3.99292092\n",
      "Iteration 31556, loss = 3.83303867\n",
      "Iteration 31557, loss = 3.38944562\n",
      "Iteration 31558, loss = 2.83158311\n",
      "Iteration 31559, loss = 3.24492317\n",
      "Iteration 31560, loss = 3.03653344\n",
      "Iteration 31561, loss = 3.55521441\n",
      "Iteration 31562, loss = 3.46196009\n",
      "Iteration 31563, loss = 3.33431393\n",
      "Iteration 31564, loss = 3.34997554\n",
      "Iteration 31565, loss = 2.83768474\n",
      "Iteration 31566, loss = 3.09581322\n",
      "Iteration 31567, loss = 2.95452768\n",
      "Iteration 31568, loss = 2.88342632\n",
      "Iteration 31569, loss = 2.73975199\n",
      "Iteration 31570, loss = 3.30596667\n",
      "Iteration 31571, loss = 3.36991535\n",
      "Iteration 31572, loss = 3.28014223\n",
      "Iteration 31573, loss = 2.97988009\n",
      "Iteration 31574, loss = 2.78658474\n",
      "Iteration 31575, loss = 3.20034168\n",
      "Iteration 31576, loss = 4.20996327\n",
      "Iteration 31577, loss = 4.29819977\n",
      "Iteration 31578, loss = 3.25789304\n",
      "Iteration 31579, loss = 3.07409661\n",
      "Iteration 31580, loss = 3.11914771\n",
      "Iteration 31581, loss = 3.16591248\n",
      "Iteration 31582, loss = 3.46260171\n",
      "Iteration 31583, loss = 3.76555485\n",
      "Iteration 31584, loss = 3.92789641\n",
      "Iteration 31585, loss = 3.15420137\n",
      "Iteration 31586, loss = 2.98521426\n",
      "Iteration 31587, loss = 3.02610624\n",
      "Iteration 31588, loss = 2.71317945\n",
      "Iteration 31589, loss = 2.92187233\n",
      "Iteration 31590, loss = 3.38865126\n",
      "Iteration 31591, loss = 3.10269513\n",
      "Iteration 31592, loss = 3.16181315\n",
      "Iteration 31593, loss = 3.39258953\n",
      "Iteration 31594, loss = 3.45672041\n",
      "Iteration 31595, loss = 3.54110465\n",
      "Iteration 31596, loss = 2.89834817\n",
      "Iteration 31597, loss = 3.02530878\n",
      "Iteration 31598, loss = 2.90309209\n",
      "Iteration 31599, loss = 3.67294881\n",
      "Iteration 31600, loss = 3.28193668\n",
      "Iteration 31601, loss = 3.85116208\n",
      "Iteration 31602, loss = 3.21616916\n",
      "Iteration 31603, loss = 3.44349645\n",
      "Iteration 31604, loss = 3.07847854\n",
      "Iteration 31605, loss = 3.49302800\n",
      "Iteration 31606, loss = 3.52193272\n",
      "Iteration 31607, loss = 3.20953377\n",
      "Iteration 31608, loss = 3.24205782\n",
      "Iteration 31609, loss = 2.99787307\n",
      "Iteration 31610, loss = 3.34220729\n",
      "Iteration 31611, loss = 4.50836631\n",
      "Iteration 31612, loss = 5.39978650\n",
      "Iteration 31613, loss = 5.75841551\n",
      "Iteration 31614, loss = 6.13105563\n",
      "Iteration 31615, loss = 4.20042404\n",
      "Iteration 31616, loss = 3.57812421\n",
      "Iteration 31617, loss = 3.08121834\n",
      "Iteration 31618, loss = 2.92974207\n",
      "Iteration 31619, loss = 2.88838903\n",
      "Iteration 31620, loss = 3.27620694\n",
      "Iteration 31621, loss = 2.93475302\n",
      "Iteration 31622, loss = 2.95223764\n",
      "Iteration 31623, loss = 3.00319208\n",
      "Iteration 31624, loss = 2.99479363\n",
      "Iteration 31625, loss = 3.32674226\n",
      "Iteration 31626, loss = 3.15906250\n",
      "Iteration 31627, loss = 3.98801583\n",
      "Iteration 31628, loss = 3.49575040\n",
      "Iteration 31629, loss = 3.58849718\n",
      "Iteration 31630, loss = 3.48118224\n",
      "Iteration 31631, loss = 3.84329101\n",
      "Iteration 31632, loss = 4.98887765\n",
      "Iteration 31633, loss = 4.94798904\n",
      "Iteration 31634, loss = 4.26521755\n",
      "Iteration 31635, loss = 3.21454165\n",
      "Iteration 31636, loss = 3.77175329\n",
      "Iteration 31637, loss = 4.14156413\n",
      "Iteration 31638, loss = 4.54994828\n",
      "Iteration 31639, loss = 3.87811023\n",
      "Iteration 31640, loss = 3.07228732\n",
      "Iteration 31641, loss = 2.90724780\n",
      "Iteration 31642, loss = 2.84137981\n",
      "Iteration 31643, loss = 2.82278942\n",
      "Iteration 31644, loss = 2.83403199\n",
      "Iteration 31645, loss = 2.97026303\n",
      "Iteration 31646, loss = 2.99359822\n",
      "Iteration 31647, loss = 3.08235482\n",
      "Iteration 31648, loss = 3.22176893\n",
      "Iteration 31649, loss = 2.95035855\n",
      "Iteration 31650, loss = 3.27305258\n",
      "Iteration 31651, loss = 3.20714928\n",
      "Iteration 31652, loss = 3.58198955\n",
      "Iteration 31653, loss = 3.40289252\n",
      "Iteration 31654, loss = 3.53809346\n",
      "Iteration 31655, loss = 3.78357498\n",
      "Iteration 31656, loss = 3.95542193\n",
      "Iteration 31657, loss = 4.37049346\n",
      "Iteration 31658, loss = 3.55248918\n",
      "Iteration 31659, loss = 3.26231382\n",
      "Iteration 31660, loss = 3.78429947\n",
      "Iteration 31661, loss = 5.25768820\n",
      "Iteration 31662, loss = 5.23966063\n",
      "Iteration 31663, loss = 3.90181445\n",
      "Iteration 31664, loss = 4.17266911\n",
      "Iteration 31665, loss = 3.44851475\n",
      "Iteration 31666, loss = 3.35430876\n",
      "Iteration 31667, loss = 3.10850436\n",
      "Iteration 31668, loss = 3.14613022\n",
      "Iteration 31669, loss = 3.52254750\n",
      "Iteration 31670, loss = 4.90957928\n",
      "Iteration 31671, loss = 4.37776267\n",
      "Iteration 31672, loss = 4.41515203\n",
      "Iteration 31673, loss = 3.66058845\n",
      "Iteration 31674, loss = 3.31895013\n",
      "Iteration 31675, loss = 3.62874111\n",
      "Iteration 31676, loss = 3.34790743\n",
      "Iteration 31677, loss = 3.05897583\n",
      "Iteration 31678, loss = 3.13669462\n",
      "Iteration 31679, loss = 2.81572936\n",
      "Iteration 31680, loss = 2.79315526\n",
      "Iteration 31681, loss = 2.99831864\n",
      "Iteration 31682, loss = 3.19164104\n",
      "Iteration 31683, loss = 3.60262506\n",
      "Iteration 31684, loss = 3.01777366\n",
      "Iteration 31685, loss = 3.32728857\n",
      "Iteration 31686, loss = 3.19836637\n",
      "Iteration 31687, loss = 3.63009158\n",
      "Iteration 31688, loss = 3.46072046\n",
      "Iteration 31689, loss = 3.40435786\n",
      "Iteration 31690, loss = 3.17855601\n",
      "Iteration 31691, loss = 3.57653768\n",
      "Iteration 31692, loss = 4.37923532\n",
      "Iteration 31693, loss = 4.70702289\n",
      "Iteration 31694, loss = 6.13681828\n",
      "Iteration 31695, loss = 4.99517672\n",
      "Iteration 31696, loss = 4.20846197\n",
      "Iteration 31697, loss = 3.64887108\n",
      "Iteration 31698, loss = 3.42805775\n",
      "Iteration 31699, loss = 3.25191590\n",
      "Iteration 31700, loss = 2.95913302\n",
      "Iteration 31701, loss = 2.91796866\n",
      "Iteration 31702, loss = 2.89144608\n",
      "Iteration 31703, loss = 2.79185300\n",
      "Iteration 31704, loss = 3.01788940\n",
      "Iteration 31705, loss = 3.41850226\n",
      "Iteration 31706, loss = 3.78602929\n",
      "Iteration 31707, loss = 3.81261364\n",
      "Iteration 31708, loss = 3.53987231\n",
      "Iteration 31709, loss = 3.29732941\n",
      "Iteration 31710, loss = 2.66490709\n",
      "Iteration 31711, loss = 3.25370486\n",
      "Iteration 31712, loss = 3.12578797\n",
      "Iteration 31713, loss = 3.85455710\n",
      "Iteration 31714, loss = 4.61713841\n",
      "Iteration 31715, loss = 3.62419921\n",
      "Iteration 31716, loss = 3.32501673\n",
      "Iteration 31717, loss = 3.44626079\n",
      "Iteration 31718, loss = 3.34978767\n",
      "Iteration 31719, loss = 3.35798390\n",
      "Iteration 31720, loss = 3.40761884\n",
      "Iteration 31721, loss = 3.82081546\n",
      "Iteration 31722, loss = 3.68542413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 31723, loss = 3.59570903\n",
      "Iteration 31724, loss = 3.20491264\n",
      "Iteration 31725, loss = 3.17298494\n",
      "Iteration 31726, loss = 3.21810810\n",
      "Iteration 31727, loss = 3.09123606\n",
      "Iteration 31728, loss = 2.95939262\n",
      "Iteration 31729, loss = 2.69188915\n",
      "Iteration 31730, loss = 2.85183017\n",
      "Iteration 31731, loss = 2.76355170\n",
      "Iteration 31732, loss = 2.80597761\n",
      "Iteration 31733, loss = 2.81507269\n",
      "Iteration 31734, loss = 3.17296763\n",
      "Iteration 31735, loss = 4.16461064\n",
      "Iteration 31736, loss = 3.12869348\n",
      "Iteration 31737, loss = 2.95774228\n",
      "Iteration 31738, loss = 2.87097580\n",
      "Iteration 31739, loss = 2.85111428\n",
      "Iteration 31740, loss = 2.89704011\n",
      "Iteration 31741, loss = 3.19514368\n",
      "Iteration 31742, loss = 2.85906942\n",
      "Iteration 31743, loss = 2.84172055\n",
      "Iteration 31744, loss = 2.93591335\n",
      "Iteration 31745, loss = 3.01910482\n",
      "Iteration 31746, loss = 3.00065892\n",
      "Iteration 31747, loss = 2.82732094\n",
      "Iteration 31748, loss = 3.07062593\n",
      "Iteration 31749, loss = 3.67420621\n",
      "Iteration 31750, loss = 3.70321524\n",
      "Iteration 31751, loss = 3.80912122\n",
      "Iteration 31752, loss = 3.88142101\n",
      "Iteration 31753, loss = 3.73929355\n",
      "Iteration 31754, loss = 2.99031575\n",
      "Iteration 31755, loss = 3.56915736\n",
      "Iteration 31756, loss = 3.71639422\n",
      "Iteration 31757, loss = 3.29916533\n",
      "Iteration 31758, loss = 3.40440811\n",
      "Iteration 31759, loss = 2.82697219\n",
      "Iteration 31760, loss = 3.06976193\n",
      "Iteration 31761, loss = 3.92935796\n",
      "Iteration 31762, loss = 3.59043378\n",
      "Iteration 31763, loss = 3.50192188\n",
      "Iteration 31764, loss = 3.32460896\n",
      "Iteration 31765, loss = 2.99302198\n",
      "Iteration 31766, loss = 2.86059193\n",
      "Iteration 31767, loss = 2.81974946\n",
      "Iteration 31768, loss = 2.93036370\n",
      "Iteration 31769, loss = 3.21684062\n",
      "Iteration 31770, loss = 3.32518948\n",
      "Iteration 31771, loss = 3.03274309\n",
      "Iteration 31772, loss = 2.70703089\n",
      "Iteration 31773, loss = 3.36412988\n",
      "Iteration 31774, loss = 3.41894753\n",
      "Iteration 31775, loss = 3.09383768\n",
      "Iteration 31776, loss = 3.07617791\n",
      "Iteration 31777, loss = 3.86669320\n",
      "Iteration 31778, loss = 4.57357566\n",
      "Iteration 31779, loss = 4.17306756\n",
      "Iteration 31780, loss = 4.24437029\n",
      "Iteration 31781, loss = 3.60460952\n",
      "Iteration 31782, loss = 3.44049351\n",
      "Iteration 31783, loss = 4.03759986\n",
      "Iteration 31784, loss = 4.05766355\n",
      "Iteration 31785, loss = 3.09955270\n",
      "Iteration 31786, loss = 3.05598818\n",
      "Iteration 31787, loss = 2.89557466\n",
      "Iteration 31788, loss = 3.06133760\n",
      "Iteration 31789, loss = 3.11076473\n",
      "Iteration 31790, loss = 2.86782240\n",
      "Iteration 31791, loss = 3.31343887\n",
      "Iteration 31792, loss = 2.88117282\n",
      "Iteration 31793, loss = 2.92466028\n",
      "Iteration 31794, loss = 3.69048940\n",
      "Iteration 31795, loss = 4.35407651\n",
      "Iteration 31796, loss = 4.30990322\n",
      "Iteration 31797, loss = 5.72016717\n",
      "Iteration 31798, loss = 4.25565962\n",
      "Iteration 31799, loss = 4.40257892\n",
      "Iteration 31800, loss = 3.22869943\n",
      "Iteration 31801, loss = 3.11959711\n",
      "Iteration 31802, loss = 3.12386484\n",
      "Iteration 31803, loss = 2.99788783\n",
      "Iteration 31804, loss = 2.84485739\n",
      "Iteration 31805, loss = 3.50689254\n",
      "Iteration 31806, loss = 4.17708469\n",
      "Iteration 31807, loss = 3.29973209\n",
      "Iteration 31808, loss = 4.09041485\n",
      "Iteration 31809, loss = 3.96919922\n",
      "Iteration 31810, loss = 3.93089520\n",
      "Iteration 31811, loss = 3.55891970\n",
      "Iteration 31812, loss = 4.91820034\n",
      "Iteration 31813, loss = 4.39604535\n",
      "Iteration 31814, loss = 4.21129960\n",
      "Iteration 31815, loss = 6.25103623\n",
      "Iteration 31816, loss = 8.46727331\n",
      "Iteration 31817, loss = 7.46684502\n",
      "Iteration 31818, loss = 10.27445186\n",
      "Iteration 31819, loss = 7.86511558\n",
      "Iteration 31820, loss = 5.92074597\n",
      "Iteration 31821, loss = 5.05684240\n",
      "Iteration 31822, loss = 4.60554464\n",
      "Iteration 31823, loss = 3.99897943\n",
      "Iteration 31824, loss = 3.04567942\n",
      "Iteration 31825, loss = 3.30580458\n",
      "Iteration 31826, loss = 2.99618488\n",
      "Iteration 31827, loss = 3.32370018\n",
      "Iteration 31828, loss = 3.75179758\n",
      "Iteration 31829, loss = 4.37240911\n",
      "Iteration 31830, loss = 4.54392542\n",
      "Iteration 31831, loss = 4.01447002\n",
      "Iteration 31832, loss = 4.10543691\n",
      "Iteration 31833, loss = 3.39865129\n",
      "Iteration 31834, loss = 3.34715128\n",
      "Iteration 31835, loss = 3.00372255\n",
      "Iteration 31836, loss = 2.92847305\n",
      "Iteration 31837, loss = 3.31706752\n",
      "Iteration 31838, loss = 2.97254546\n",
      "Iteration 31839, loss = 2.83013620\n",
      "Iteration 31840, loss = 3.28547315\n",
      "Iteration 31841, loss = 3.97933778\n",
      "Iteration 31842, loss = 3.37693714\n",
      "Iteration 31843, loss = 3.56718474\n",
      "Iteration 31844, loss = 3.37183984\n",
      "Iteration 31845, loss = 3.26763254\n",
      "Iteration 31846, loss = 3.14172982\n",
      "Iteration 31847, loss = 2.96730996\n",
      "Iteration 31848, loss = 3.22408938\n",
      "Iteration 31849, loss = 3.15826374\n",
      "Iteration 31850, loss = 3.02930584\n",
      "Iteration 31851, loss = 3.28566275\n",
      "Iteration 31852, loss = 3.51424398\n",
      "Iteration 31853, loss = 3.24950457\n",
      "Iteration 31854, loss = 2.99083052\n",
      "Iteration 31855, loss = 2.88282941\n",
      "Iteration 31856, loss = 3.05026721\n",
      "Iteration 31857, loss = 3.24635524\n",
      "Iteration 31858, loss = 3.48162449\n",
      "Iteration 31859, loss = 3.14099899\n",
      "Iteration 31860, loss = 3.43213785\n",
      "Iteration 31861, loss = 3.82112428\n",
      "Iteration 31862, loss = 4.17881887\n",
      "Iteration 31863, loss = 3.31507848\n",
      "Iteration 31864, loss = 3.19049931\n",
      "Iteration 31865, loss = 2.97147604\n",
      "Iteration 31866, loss = 3.12736943\n",
      "Iteration 31867, loss = 3.50374921\n",
      "Iteration 31868, loss = 3.64687136\n",
      "Iteration 31869, loss = 3.19085320\n",
      "Iteration 31870, loss = 2.84746834\n",
      "Iteration 31871, loss = 2.70213000\n",
      "Iteration 31872, loss = 2.88161714\n",
      "Iteration 31873, loss = 2.94404486\n",
      "Iteration 31874, loss = 3.17065560\n",
      "Iteration 31875, loss = 3.65736770\n",
      "Iteration 31876, loss = 3.57539841\n",
      "Iteration 31877, loss = 3.06355471\n",
      "Iteration 31878, loss = 3.40353338\n",
      "Iteration 31879, loss = 3.68608038\n",
      "Iteration 31880, loss = 4.04828751\n",
      "Iteration 31881, loss = 3.79653761\n",
      "Iteration 31882, loss = 3.87559240\n",
      "Iteration 31883, loss = 4.07603504\n",
      "Iteration 31884, loss = 3.20743429\n",
      "Iteration 31885, loss = 3.36118397\n",
      "Iteration 31886, loss = 3.30173382\n",
      "Iteration 31887, loss = 3.18513152\n",
      "Iteration 31888, loss = 3.03421080\n",
      "Iteration 31889, loss = 2.99932632\n",
      "Iteration 31890, loss = 2.96181135\n",
      "Iteration 31891, loss = 3.11413041\n",
      "Iteration 31892, loss = 3.11949579\n",
      "Iteration 31893, loss = 3.80371511\n",
      "Iteration 31894, loss = 3.37841803\n",
      "Iteration 31895, loss = 3.82207208\n",
      "Iteration 31896, loss = 3.80099608\n",
      "Iteration 31897, loss = 3.97520010\n",
      "Iteration 31898, loss = 4.07708453\n",
      "Iteration 31899, loss = 3.43118519\n",
      "Iteration 31900, loss = 4.13039001\n",
      "Iteration 31901, loss = 3.52260745\n",
      "Iteration 31902, loss = 3.30600134\n",
      "Iteration 31903, loss = 3.46941916\n",
      "Iteration 31904, loss = 2.92517303\n",
      "Iteration 31905, loss = 3.28582252\n",
      "Iteration 31906, loss = 3.00708675\n",
      "Iteration 31907, loss = 2.84586195\n",
      "Iteration 31908, loss = 2.94699188\n",
      "Iteration 31909, loss = 3.21562006\n",
      "Iteration 31910, loss = 2.93110952\n",
      "Iteration 31911, loss = 3.94960679\n",
      "Iteration 31912, loss = 3.85869650\n",
      "Iteration 31913, loss = 3.46926667\n",
      "Iteration 31914, loss = 3.29111810\n",
      "Iteration 31915, loss = 4.86503815\n",
      "Iteration 31916, loss = 3.48392429\n",
      "Iteration 31917, loss = 3.24990596\n",
      "Iteration 31918, loss = 3.10201305\n",
      "Iteration 31919, loss = 3.27028094\n",
      "Iteration 31920, loss = 3.30138743\n",
      "Iteration 31921, loss = 3.52437016\n",
      "Iteration 31922, loss = 3.43626592\n",
      "Iteration 31923, loss = 4.16174271\n",
      "Iteration 31924, loss = 3.19194992\n",
      "Iteration 31925, loss = 3.35274700\n",
      "Iteration 31926, loss = 3.52592385\n",
      "Iteration 31927, loss = 3.15487337\n",
      "Iteration 31928, loss = 3.10612827\n",
      "Iteration 31929, loss = 3.00683989\n",
      "Iteration 31930, loss = 2.87039321\n",
      "Iteration 31931, loss = 2.73717570\n",
      "Iteration 31932, loss = 2.95451883\n",
      "Iteration 31933, loss = 2.97031396\n",
      "Iteration 31934, loss = 3.02081122\n",
      "Iteration 31935, loss = 3.23289755\n",
      "Iteration 31936, loss = 3.07471543\n",
      "Iteration 31937, loss = 3.22287134\n",
      "Iteration 31938, loss = 3.41040674\n",
      "Iteration 31939, loss = 2.99534294\n",
      "Iteration 31940, loss = 2.97614397\n",
      "Iteration 31941, loss = 2.98470697\n",
      "Iteration 31942, loss = 3.02568225\n",
      "Iteration 31943, loss = 3.57326234\n",
      "Iteration 31944, loss = 3.49159539\n",
      "Iteration 31945, loss = 3.24812880\n",
      "Iteration 31946, loss = 3.78500664\n",
      "Iteration 31947, loss = 5.00303438\n",
      "Iteration 31948, loss = 6.86912259\n",
      "Iteration 31949, loss = 4.22954478\n",
      "Iteration 31950, loss = 3.88737113\n",
      "Iteration 31951, loss = 2.97893145\n",
      "Iteration 31952, loss = 3.54038732\n",
      "Iteration 31953, loss = 3.61753704\n",
      "Iteration 31954, loss = 3.64529729\n",
      "Iteration 31955, loss = 4.23666529\n",
      "Iteration 31956, loss = 3.72973199\n",
      "Iteration 31957, loss = 3.29132450\n",
      "Iteration 31958, loss = 3.04810696\n",
      "Iteration 31959, loss = 3.17116561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 31960, loss = 2.76566220\n",
      "Iteration 31961, loss = 3.09851612\n",
      "Iteration 31962, loss = 2.89173668\n",
      "Iteration 31963, loss = 3.12502374\n",
      "Iteration 31964, loss = 2.99819776\n",
      "Iteration 31965, loss = 2.95383753\n",
      "Iteration 31966, loss = 3.22244641\n",
      "Iteration 31967, loss = 3.06619439\n",
      "Iteration 31968, loss = 2.72908667\n",
      "Iteration 31969, loss = 3.14462964\n",
      "Iteration 31970, loss = 3.70713222\n",
      "Iteration 31971, loss = 4.23977434\n",
      "Iteration 31972, loss = 4.28428714\n",
      "Iteration 31973, loss = 3.84371216\n",
      "Iteration 31974, loss = 3.49720104\n",
      "Iteration 31975, loss = 2.83392433\n",
      "Iteration 31976, loss = 3.35085504\n",
      "Iteration 31977, loss = 3.03644464\n",
      "Iteration 31978, loss = 3.20070006\n",
      "Iteration 31979, loss = 3.95799561\n",
      "Iteration 31980, loss = 4.66141549\n",
      "Iteration 31981, loss = 4.46102961\n",
      "Iteration 31982, loss = 4.11355007\n",
      "Iteration 31983, loss = 3.58900994\n",
      "Iteration 31984, loss = 3.54610293\n",
      "Iteration 31985, loss = 3.63706629\n",
      "Iteration 31986, loss = 3.76061043\n",
      "Iteration 31987, loss = 3.17211998\n",
      "Iteration 31988, loss = 2.84522225\n",
      "Iteration 31989, loss = 2.77305984\n",
      "Iteration 31990, loss = 2.97098525\n",
      "Iteration 31991, loss = 3.06379704\n",
      "Iteration 31992, loss = 3.08560787\n",
      "Iteration 31993, loss = 3.15154194\n",
      "Iteration 31994, loss = 3.01252489\n",
      "Iteration 31995, loss = 3.26706453\n",
      "Iteration 31996, loss = 2.94700836\n",
      "Iteration 31997, loss = 2.82637899\n",
      "Iteration 31998, loss = 3.17163286\n",
      "Iteration 31999, loss = 3.80121842\n",
      "Iteration 32000, loss = 4.24395470\n",
      "Iteration 32001, loss = 3.58428811\n",
      "Iteration 32002, loss = 3.57044764\n",
      "Iteration 32003, loss = 3.05208626\n",
      "Iteration 32004, loss = 3.59628005\n",
      "Iteration 32005, loss = 2.91618055\n",
      "Iteration 32006, loss = 2.83577215\n",
      "Iteration 32007, loss = 2.68123997\n",
      "Iteration 32008, loss = 2.97399397\n",
      "Iteration 32009, loss = 3.36085905\n",
      "Iteration 32010, loss = 3.05527634\n",
      "Iteration 32011, loss = 3.50920958\n",
      "Iteration 32012, loss = 3.27741874\n",
      "Iteration 32013, loss = 3.03787553\n",
      "Iteration 32014, loss = 3.19969828\n",
      "Iteration 32015, loss = 3.27789554\n",
      "Iteration 32016, loss = 2.88319027\n",
      "Iteration 32017, loss = 2.77442743\n",
      "Iteration 32018, loss = 2.80595460\n",
      "Iteration 32019, loss = 3.07709495\n",
      "Iteration 32020, loss = 3.74595336\n",
      "Iteration 32021, loss = 5.01950368\n",
      "Iteration 32022, loss = 4.51169561\n",
      "Iteration 32023, loss = 3.03969955\n",
      "Iteration 32024, loss = 2.94534078\n",
      "Iteration 32025, loss = 3.00800681\n",
      "Iteration 32026, loss = 3.58854122\n",
      "Iteration 32027, loss = 3.20436854\n",
      "Iteration 32028, loss = 3.33732705\n",
      "Iteration 32029, loss = 3.12931445\n",
      "Iteration 32030, loss = 3.73028903\n",
      "Iteration 32031, loss = 3.82040759\n",
      "Iteration 32032, loss = 3.20522751\n",
      "Iteration 32033, loss = 3.45203481\n",
      "Iteration 32034, loss = 3.10011259\n",
      "Iteration 32035, loss = 2.76608896\n",
      "Iteration 32036, loss = 3.14163535\n",
      "Iteration 32037, loss = 3.02409059\n",
      "Iteration 32038, loss = 3.14680744\n",
      "Iteration 32039, loss = 3.40414714\n",
      "Iteration 32040, loss = 3.63631925\n",
      "Iteration 32041, loss = 3.50050939\n",
      "Iteration 32042, loss = 3.34178927\n",
      "Iteration 32043, loss = 4.04943539\n",
      "Iteration 32044, loss = 4.09968607\n",
      "Iteration 32045, loss = 3.46125984\n",
      "Iteration 32046, loss = 3.16508834\n",
      "Iteration 32047, loss = 3.05415188\n",
      "Iteration 32048, loss = 2.90457390\n",
      "Iteration 32049, loss = 2.94726801\n",
      "Iteration 32050, loss = 3.14116199\n",
      "Iteration 32051, loss = 2.67153267\n",
      "Iteration 32052, loss = 3.03912447\n",
      "Iteration 32053, loss = 3.64803494\n",
      "Iteration 32054, loss = 3.63830636\n",
      "Iteration 32055, loss = 2.86720327\n",
      "Iteration 32056, loss = 2.98557811\n",
      "Iteration 32057, loss = 2.63995605\n",
      "Iteration 32058, loss = 2.90463807\n",
      "Iteration 32059, loss = 3.38030876\n",
      "Iteration 32060, loss = 3.62168316\n",
      "Iteration 32061, loss = 3.91299956\n",
      "Iteration 32062, loss = 2.91590879\n",
      "Iteration 32063, loss = 3.13101368\n",
      "Iteration 32064, loss = 2.91083059\n",
      "Iteration 32065, loss = 3.56119906\n",
      "Iteration 32066, loss = 3.42243743\n",
      "Iteration 32067, loss = 2.82264033\n",
      "Iteration 32068, loss = 3.84160871\n",
      "Iteration 32069, loss = 3.49773111\n",
      "Iteration 32070, loss = 3.83288390\n",
      "Iteration 32071, loss = 4.18107272\n",
      "Iteration 32072, loss = 3.48902360\n",
      "Iteration 32073, loss = 4.99935030\n",
      "Iteration 32074, loss = 4.29597373\n",
      "Iteration 32075, loss = 3.33862774\n",
      "Iteration 32076, loss = 3.66075600\n",
      "Iteration 32077, loss = 3.99468352\n",
      "Iteration 32078, loss = 3.37033357\n",
      "Iteration 32079, loss = 3.67382370\n",
      "Iteration 32080, loss = 3.43368888\n",
      "Iteration 32081, loss = 3.82461773\n",
      "Iteration 32082, loss = 3.56830052\n",
      "Iteration 32083, loss = 3.44508965\n",
      "Iteration 32084, loss = 2.86853045\n",
      "Iteration 32085, loss = 2.78425794\n",
      "Iteration 32086, loss = 2.87071744\n",
      "Iteration 32087, loss = 3.11334567\n",
      "Iteration 32088, loss = 3.44881940\n",
      "Iteration 32089, loss = 3.56817107\n",
      "Iteration 32090, loss = 3.90974006\n",
      "Iteration 32091, loss = 3.52970921\n",
      "Iteration 32092, loss = 4.64633421\n",
      "Iteration 32093, loss = 4.91198664\n",
      "Iteration 32094, loss = 5.58410232\n",
      "Iteration 32095, loss = 4.46306032\n",
      "Iteration 32096, loss = 3.98140511\n",
      "Iteration 32097, loss = 3.35893872\n",
      "Iteration 32098, loss = 4.58107954\n",
      "Iteration 32099, loss = 3.93923802\n",
      "Iteration 32100, loss = 4.12971519\n",
      "Iteration 32101, loss = 3.59249574\n",
      "Iteration 32102, loss = 3.45686024\n",
      "Iteration 32103, loss = 3.48622432\n",
      "Iteration 32104, loss = 3.28473288\n",
      "Iteration 32105, loss = 3.21219172\n",
      "Iteration 32106, loss = 2.96526149\n",
      "Iteration 32107, loss = 3.05113156\n",
      "Iteration 32108, loss = 2.95126372\n",
      "Iteration 32109, loss = 3.27979367\n",
      "Iteration 32110, loss = 3.31803910\n",
      "Iteration 32111, loss = 3.92404385\n",
      "Iteration 32112, loss = 3.73791774\n",
      "Iteration 32113, loss = 3.57517220\n",
      "Iteration 32114, loss = 2.91909000\n",
      "Iteration 32115, loss = 2.70086000\n",
      "Iteration 32116, loss = 2.84094740\n",
      "Iteration 32117, loss = 2.91651991\n",
      "Iteration 32118, loss = 2.76788261\n",
      "Iteration 32119, loss = 3.22586084\n",
      "Iteration 32120, loss = 3.23983920\n",
      "Iteration 32121, loss = 3.07950173\n",
      "Iteration 32122, loss = 2.97213742\n",
      "Iteration 32123, loss = 2.88443449\n",
      "Iteration 32124, loss = 3.02386671\n",
      "Iteration 32125, loss = 3.68214691\n",
      "Iteration 32126, loss = 3.37353052\n",
      "Iteration 32127, loss = 3.31659861\n",
      "Iteration 32128, loss = 3.42239159\n",
      "Iteration 32129, loss = 3.37672108\n",
      "Iteration 32130, loss = 2.94755530\n",
      "Iteration 32131, loss = 3.07390853\n",
      "Iteration 32132, loss = 2.91917947\n",
      "Iteration 32133, loss = 2.96961346\n",
      "Iteration 32134, loss = 2.72811419\n",
      "Iteration 32135, loss = 2.90132358\n",
      "Iteration 32136, loss = 3.02764116\n",
      "Iteration 32137, loss = 3.19453648\n",
      "Iteration 32138, loss = 3.23467076\n",
      "Iteration 32139, loss = 3.29466023\n",
      "Iteration 32140, loss = 3.15423969\n",
      "Iteration 32141, loss = 3.04172767\n",
      "Iteration 32142, loss = 3.55291585\n",
      "Iteration 32143, loss = 3.09722197\n",
      "Iteration 32144, loss = 3.49361448\n",
      "Iteration 32145, loss = 4.65116068\n",
      "Iteration 32146, loss = 4.09616660\n",
      "Iteration 32147, loss = 3.23326946\n",
      "Iteration 32148, loss = 3.28582535\n",
      "Iteration 32149, loss = 4.13666637\n",
      "Iteration 32150, loss = 5.65296551\n",
      "Iteration 32151, loss = 5.18303046\n",
      "Iteration 32152, loss = 5.14801037\n",
      "Iteration 32153, loss = 5.05433982\n",
      "Iteration 32154, loss = 3.90545408\n",
      "Iteration 32155, loss = 3.70510173\n",
      "Iteration 32156, loss = 3.18610983\n",
      "Iteration 32157, loss = 3.05432582\n",
      "Iteration 32158, loss = 3.06012724\n",
      "Iteration 32159, loss = 3.05648813\n",
      "Iteration 32160, loss = 3.09418224\n",
      "Iteration 32161, loss = 3.33876096\n",
      "Iteration 32162, loss = 3.09654289\n",
      "Iteration 32163, loss = 3.71041557\n",
      "Iteration 32164, loss = 3.62666860\n",
      "Iteration 32165, loss = 3.24048117\n",
      "Iteration 32166, loss = 3.09493020\n",
      "Iteration 32167, loss = 2.83480768\n",
      "Iteration 32168, loss = 2.94174409\n",
      "Iteration 32169, loss = 3.39070260\n",
      "Iteration 32170, loss = 3.70589505\n",
      "Iteration 32171, loss = 3.38068857\n",
      "Iteration 32172, loss = 3.25280065\n",
      "Iteration 32173, loss = 3.39247949\n",
      "Iteration 32174, loss = 3.89540436\n",
      "Iteration 32175, loss = 4.30254650\n",
      "Iteration 32176, loss = 4.84091547\n",
      "Iteration 32177, loss = 4.64371059\n",
      "Iteration 32178, loss = 3.30113195\n",
      "Iteration 32179, loss = 3.15762326\n",
      "Iteration 32180, loss = 3.01919656\n",
      "Iteration 32181, loss = 3.03882425\n",
      "Iteration 32182, loss = 3.55439622\n",
      "Iteration 32183, loss = 3.48022709\n",
      "Iteration 32184, loss = 3.44489442\n",
      "Iteration 32185, loss = 3.19421969\n",
      "Iteration 32186, loss = 2.81893456\n",
      "Iteration 32187, loss = 2.88835622\n",
      "Iteration 32188, loss = 2.85555925\n",
      "Iteration 32189, loss = 2.94130347\n",
      "Iteration 32190, loss = 3.13566918\n",
      "Iteration 32191, loss = 2.76928166\n",
      "Iteration 32192, loss = 2.71572544\n",
      "Iteration 32193, loss = 2.73227715\n",
      "Iteration 32194, loss = 3.18496904\n",
      "Iteration 32195, loss = 3.36288171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32196, loss = 3.28308106\n",
      "Iteration 32197, loss = 3.53298266\n",
      "Iteration 32198, loss = 3.18393186\n",
      "Iteration 32199, loss = 3.18206721\n",
      "Iteration 32200, loss = 2.97226236\n",
      "Iteration 32201, loss = 3.24864132\n",
      "Iteration 32202, loss = 3.50060785\n",
      "Iteration 32203, loss = 2.89148527\n",
      "Iteration 32204, loss = 3.94841469\n",
      "Iteration 32205, loss = 4.26163741\n",
      "Iteration 32206, loss = 3.27208442\n",
      "Iteration 32207, loss = 3.28711295\n",
      "Iteration 32208, loss = 2.84747197\n",
      "Iteration 32209, loss = 3.06417811\n",
      "Iteration 32210, loss = 2.86326147\n",
      "Iteration 32211, loss = 2.71466041\n",
      "Iteration 32212, loss = 3.11822123\n",
      "Iteration 32213, loss = 3.32729046\n",
      "Iteration 32214, loss = 2.82398350\n",
      "Iteration 32215, loss = 3.22866347\n",
      "Iteration 32216, loss = 3.71570491\n",
      "Iteration 32217, loss = 3.05466826\n",
      "Iteration 32218, loss = 3.14383676\n",
      "Iteration 32219, loss = 3.18511486\n",
      "Iteration 32220, loss = 3.20684459\n",
      "Iteration 32221, loss = 3.15598891\n",
      "Iteration 32222, loss = 3.89821839\n",
      "Iteration 32223, loss = 3.83745188\n",
      "Iteration 32224, loss = 3.98707372\n",
      "Iteration 32225, loss = 3.42217019\n",
      "Iteration 32226, loss = 3.30180995\n",
      "Iteration 32227, loss = 4.44134272\n",
      "Iteration 32228, loss = 3.76316612\n",
      "Iteration 32229, loss = 3.24095331\n",
      "Iteration 32230, loss = 3.46223942\n",
      "Iteration 32231, loss = 5.69794428\n",
      "Iteration 32232, loss = 7.31654659\n",
      "Iteration 32233, loss = 6.94218834\n",
      "Iteration 32234, loss = 8.03031389\n",
      "Iteration 32235, loss = 8.38156645\n",
      "Iteration 32236, loss = 7.89979328\n",
      "Iteration 32237, loss = 6.16752806\n",
      "Iteration 32238, loss = 6.45907093\n",
      "Iteration 32239, loss = 5.89537998\n",
      "Iteration 32240, loss = 4.39284511\n",
      "Iteration 32241, loss = 3.45779077\n",
      "Iteration 32242, loss = 4.45733446\n",
      "Iteration 32243, loss = 4.12826062\n",
      "Iteration 32244, loss = 3.57053169\n",
      "Iteration 32245, loss = 3.38436826\n",
      "Iteration 32246, loss = 3.22696572\n",
      "Iteration 32247, loss = 2.87048596\n",
      "Iteration 32248, loss = 3.44192162\n",
      "Iteration 32249, loss = 3.09710207\n",
      "Iteration 32250, loss = 2.98578447\n",
      "Iteration 32251, loss = 2.83646877\n",
      "Iteration 32252, loss = 2.95506022\n",
      "Iteration 32253, loss = 3.22591532\n",
      "Iteration 32254, loss = 3.49697157\n",
      "Iteration 32255, loss = 3.43910452\n",
      "Iteration 32256, loss = 2.91747477\n",
      "Iteration 32257, loss = 2.77737384\n",
      "Iteration 32258, loss = 2.82190783\n",
      "Iteration 32259, loss = 2.84627452\n",
      "Iteration 32260, loss = 3.18999264\n",
      "Iteration 32261, loss = 3.23438755\n",
      "Iteration 32262, loss = 4.03413446\n",
      "Iteration 32263, loss = 2.95210704\n",
      "Iteration 32264, loss = 2.85860295\n",
      "Iteration 32265, loss = 3.17678146\n",
      "Iteration 32266, loss = 3.09842246\n",
      "Iteration 32267, loss = 2.62102123\n",
      "Iteration 32268, loss = 2.90253338\n",
      "Iteration 32269, loss = 3.21418725\n",
      "Iteration 32270, loss = 2.66544755\n",
      "Iteration 32271, loss = 2.84720649\n",
      "Iteration 32272, loss = 2.96616115\n",
      "Iteration 32273, loss = 3.19575112\n",
      "Iteration 32274, loss = 3.04977192\n",
      "Iteration 32275, loss = 3.29622931\n",
      "Iteration 32276, loss = 3.37598843\n",
      "Iteration 32277, loss = 3.49055897\n",
      "Iteration 32278, loss = 3.38681413\n",
      "Iteration 32279, loss = 3.98859862\n",
      "Iteration 32280, loss = 5.33135887\n",
      "Iteration 32281, loss = 4.92373552\n",
      "Iteration 32282, loss = 4.74024727\n",
      "Iteration 32283, loss = 3.68375398\n",
      "Iteration 32284, loss = 3.16626020\n",
      "Iteration 32285, loss = 2.88716870\n",
      "Iteration 32286, loss = 2.84585981\n",
      "Iteration 32287, loss = 3.29789145\n",
      "Iteration 32288, loss = 2.96923865\n",
      "Iteration 32289, loss = 2.85997780\n",
      "Iteration 32290, loss = 2.92546109\n",
      "Iteration 32291, loss = 3.20250830\n",
      "Iteration 32292, loss = 3.29008493\n",
      "Iteration 32293, loss = 2.90878145\n",
      "Iteration 32294, loss = 3.31152650\n",
      "Iteration 32295, loss = 3.64547769\n",
      "Iteration 32296, loss = 3.55385563\n",
      "Iteration 32297, loss = 4.38061717\n",
      "Iteration 32298, loss = 4.28632641\n",
      "Iteration 32299, loss = 3.83399280\n",
      "Iteration 32300, loss = 4.07914655\n",
      "Iteration 32301, loss = 3.24117014\n",
      "Iteration 32302, loss = 3.02294548\n",
      "Iteration 32303, loss = 3.52940888\n",
      "Iteration 32304, loss = 3.88220247\n",
      "Iteration 32305, loss = 3.75703745\n",
      "Iteration 32306, loss = 4.67498398\n",
      "Iteration 32307, loss = 3.66684038\n",
      "Iteration 32308, loss = 5.29886876\n",
      "Iteration 32309, loss = 7.77222353\n",
      "Iteration 32310, loss = 8.48923258\n",
      "Iteration 32311, loss = 5.45376534\n",
      "Iteration 32312, loss = 5.00959940\n",
      "Iteration 32313, loss = 3.81232069\n",
      "Iteration 32314, loss = 3.58890886\n",
      "Iteration 32315, loss = 3.53204027\n",
      "Iteration 32316, loss = 3.34016482\n",
      "Iteration 32317, loss = 3.82209200\n",
      "Iteration 32318, loss = 3.45090061\n",
      "Iteration 32319, loss = 3.53869045\n",
      "Iteration 32320, loss = 3.14068351\n",
      "Iteration 32321, loss = 3.09231386\n",
      "Iteration 32322, loss = 3.29698904\n",
      "Iteration 32323, loss = 2.80980441\n",
      "Iteration 32324, loss = 3.09423258\n",
      "Iteration 32325, loss = 3.01592719\n",
      "Iteration 32326, loss = 3.24814202\n",
      "Iteration 32327, loss = 3.12067475\n",
      "Iteration 32328, loss = 3.12322105\n",
      "Iteration 32329, loss = 3.27637920\n",
      "Iteration 32330, loss = 4.00475397\n",
      "Iteration 32331, loss = 3.34775691\n",
      "Iteration 32332, loss = 4.37331841\n",
      "Iteration 32333, loss = 3.88921402\n",
      "Iteration 32334, loss = 3.99155252\n",
      "Iteration 32335, loss = 3.38736876\n",
      "Iteration 32336, loss = 3.00715117\n",
      "Iteration 32337, loss = 2.88902609\n",
      "Iteration 32338, loss = 3.66240626\n",
      "Iteration 32339, loss = 3.73252087\n",
      "Iteration 32340, loss = 3.43125962\n",
      "Iteration 32341, loss = 3.62397445\n",
      "Iteration 32342, loss = 3.59060644\n",
      "Iteration 32343, loss = 3.91723697\n",
      "Iteration 32344, loss = 3.93643633\n",
      "Iteration 32345, loss = 4.14569412\n",
      "Iteration 32346, loss = 4.97055594\n",
      "Iteration 32347, loss = 4.22367849\n",
      "Iteration 32348, loss = 3.56506675\n",
      "Iteration 32349, loss = 4.08380241\n",
      "Iteration 32350, loss = 4.78535185\n",
      "Iteration 32351, loss = 4.99748079\n",
      "Iteration 32352, loss = 4.52898255\n",
      "Iteration 32353, loss = 4.26349881\n",
      "Iteration 32354, loss = 4.35524528\n",
      "Iteration 32355, loss = 4.21115925\n",
      "Iteration 32356, loss = 3.82524970\n",
      "Iteration 32357, loss = 4.89078022\n",
      "Iteration 32358, loss = 4.37596854\n",
      "Iteration 32359, loss = 4.09000844\n",
      "Iteration 32360, loss = 3.60243286\n",
      "Iteration 32361, loss = 2.87743808\n",
      "Iteration 32362, loss = 3.05074038\n",
      "Iteration 32363, loss = 3.85578423\n",
      "Iteration 32364, loss = 3.75782502\n",
      "Iteration 32365, loss = 3.41728832\n",
      "Iteration 32366, loss = 3.72380718\n",
      "Iteration 32367, loss = 3.19465271\n",
      "Iteration 32368, loss = 3.61013325\n",
      "Iteration 32369, loss = 3.40744064\n",
      "Iteration 32370, loss = 3.23412333\n",
      "Iteration 32371, loss = 3.66777727\n",
      "Iteration 32372, loss = 3.88539751\n",
      "Iteration 32373, loss = 3.06795968\n",
      "Iteration 32374, loss = 3.20325691\n",
      "Iteration 32375, loss = 3.25258163\n",
      "Iteration 32376, loss = 3.49820194\n",
      "Iteration 32377, loss = 2.89471351\n",
      "Iteration 32378, loss = 3.25247091\n",
      "Iteration 32379, loss = 3.00642018\n",
      "Iteration 32380, loss = 3.33451628\n",
      "Iteration 32381, loss = 3.72674339\n",
      "Iteration 32382, loss = 3.92707164\n",
      "Iteration 32383, loss = 4.60011878\n",
      "Iteration 32384, loss = 3.81307199\n",
      "Iteration 32385, loss = 4.19283761\n",
      "Iteration 32386, loss = 4.07570707\n",
      "Iteration 32387, loss = 3.25150990\n",
      "Iteration 32388, loss = 3.51378010\n",
      "Iteration 32389, loss = 3.22917517\n",
      "Iteration 32390, loss = 3.18425722\n",
      "Iteration 32391, loss = 2.93873337\n",
      "Iteration 32392, loss = 3.21235392\n",
      "Iteration 32393, loss = 3.53861567\n",
      "Iteration 32394, loss = 4.19566615\n",
      "Iteration 32395, loss = 3.54134949\n",
      "Iteration 32396, loss = 3.03004108\n",
      "Iteration 32397, loss = 2.89200775\n",
      "Iteration 32398, loss = 3.64466708\n",
      "Iteration 32399, loss = 3.92206714\n",
      "Iteration 32400, loss = 4.23199746\n",
      "Iteration 32401, loss = 3.42441370\n",
      "Iteration 32402, loss = 3.33682796\n",
      "Iteration 32403, loss = 3.00947333\n",
      "Iteration 32404, loss = 3.28676119\n",
      "Iteration 32405, loss = 2.78974102\n",
      "Iteration 32406, loss = 2.95813080\n",
      "Iteration 32407, loss = 2.87474947\n",
      "Iteration 32408, loss = 2.82593262\n",
      "Iteration 32409, loss = 2.95662313\n",
      "Iteration 32410, loss = 3.03809644\n",
      "Iteration 32411, loss = 2.73432383\n",
      "Iteration 32412, loss = 3.06611457\n",
      "Iteration 32413, loss = 3.08960536\n",
      "Iteration 32414, loss = 3.94649707\n",
      "Iteration 32415, loss = 3.31619452\n",
      "Iteration 32416, loss = 3.76186089\n",
      "Iteration 32417, loss = 4.82707071\n",
      "Iteration 32418, loss = 3.84318634\n",
      "Iteration 32419, loss = 3.81620499\n",
      "Iteration 32420, loss = 4.24504961\n",
      "Iteration 32421, loss = 3.60281178\n",
      "Iteration 32422, loss = 4.34425524\n",
      "Iteration 32423, loss = 3.76425447\n",
      "Iteration 32424, loss = 3.37710742\n",
      "Iteration 32425, loss = 3.24567358\n",
      "Iteration 32426, loss = 3.17943173\n",
      "Iteration 32427, loss = 3.13322434\n",
      "Iteration 32428, loss = 4.17961361\n",
      "Iteration 32429, loss = 3.65016811\n",
      "Iteration 32430, loss = 3.44834123\n",
      "Iteration 32431, loss = 3.58511395\n",
      "Iteration 32432, loss = 4.09856846\n",
      "Iteration 32433, loss = 3.39068039\n",
      "Iteration 32434, loss = 3.12998448\n",
      "Iteration 32435, loss = 3.96139949\n",
      "Iteration 32436, loss = 4.21588390\n",
      "Iteration 32437, loss = 3.30462945\n",
      "Iteration 32438, loss = 3.05422400\n",
      "Iteration 32439, loss = 3.01716262\n",
      "Iteration 32440, loss = 2.75393357\n",
      "Iteration 32441, loss = 3.41096562\n",
      "Iteration 32442, loss = 2.98486994\n",
      "Iteration 32443, loss = 2.76332024\n",
      "Iteration 32444, loss = 3.02550997\n",
      "Iteration 32445, loss = 3.18625711\n",
      "Iteration 32446, loss = 3.69801800\n",
      "Iteration 32447, loss = 3.10569023\n",
      "Iteration 32448, loss = 3.58579495\n",
      "Iteration 32449, loss = 3.38969776\n",
      "Iteration 32450, loss = 3.51583666\n",
      "Iteration 32451, loss = 2.94546596\n",
      "Iteration 32452, loss = 2.96708956\n",
      "Iteration 32453, loss = 3.08038588\n",
      "Iteration 32454, loss = 2.76024060\n",
      "Iteration 32455, loss = 3.14423380\n",
      "Iteration 32456, loss = 3.03501007\n",
      "Iteration 32457, loss = 3.24126707\n",
      "Iteration 32458, loss = 3.39105075\n",
      "Iteration 32459, loss = 3.02640363\n",
      "Iteration 32460, loss = 3.19908293\n",
      "Iteration 32461, loss = 3.26601453\n",
      "Iteration 32462, loss = 4.19476279\n",
      "Iteration 32463, loss = 3.87872757\n",
      "Iteration 32464, loss = 3.80825791\n",
      "Iteration 32465, loss = 3.27449645\n",
      "Iteration 32466, loss = 3.26016435\n",
      "Iteration 32467, loss = 3.06872513\n",
      "Iteration 32468, loss = 2.88100956\n",
      "Iteration 32469, loss = 2.91863946\n",
      "Iteration 32470, loss = 3.31583432\n",
      "Iteration 32471, loss = 3.55134603\n",
      "Iteration 32472, loss = 3.78149599\n",
      "Iteration 32473, loss = 3.38421114\n",
      "Iteration 32474, loss = 3.17627379\n",
      "Iteration 32475, loss = 3.91263163\n",
      "Iteration 32476, loss = 3.58213354\n",
      "Iteration 32477, loss = 3.06390699\n",
      "Iteration 32478, loss = 2.83621294\n",
      "Iteration 32479, loss = 2.69730152\n",
      "Iteration 32480, loss = 2.60597043\n",
      "Iteration 32481, loss = 2.79049545\n",
      "Iteration 32482, loss = 3.01137838\n",
      "Iteration 32483, loss = 3.90960473\n",
      "Iteration 32484, loss = 4.52700656\n",
      "Iteration 32485, loss = 4.82840194\n",
      "Iteration 32486, loss = 4.16053635\n",
      "Iteration 32487, loss = 4.47621441\n",
      "Iteration 32488, loss = 4.01508638\n",
      "Iteration 32489, loss = 5.07622647\n",
      "Iteration 32490, loss = 3.60204343\n",
      "Iteration 32491, loss = 3.49468725\n",
      "Iteration 32492, loss = 3.16750587\n",
      "Iteration 32493, loss = 3.64612530\n",
      "Iteration 32494, loss = 3.52158336\n",
      "Iteration 32495, loss = 2.88651735\n",
      "Iteration 32496, loss = 3.23636305\n",
      "Iteration 32497, loss = 3.65199237\n",
      "Iteration 32498, loss = 3.78077942\n",
      "Iteration 32499, loss = 3.87960263\n",
      "Iteration 32500, loss = 2.92739056\n",
      "Iteration 32501, loss = 2.78033769\n",
      "Iteration 32502, loss = 3.15141594\n",
      "Iteration 32503, loss = 3.00925209\n",
      "Iteration 32504, loss = 3.01037389\n",
      "Iteration 32505, loss = 3.08361331\n",
      "Iteration 32506, loss = 2.76924723\n",
      "Iteration 32507, loss = 2.79582970\n",
      "Iteration 32508, loss = 3.05716113\n",
      "Iteration 32509, loss = 3.49899869\n",
      "Iteration 32510, loss = 2.95006641\n",
      "Iteration 32511, loss = 3.08600005\n",
      "Iteration 32512, loss = 3.06960852\n",
      "Iteration 32513, loss = 2.95402013\n",
      "Iteration 32514, loss = 3.07259470\n",
      "Iteration 32515, loss = 3.53100300\n",
      "Iteration 32516, loss = 3.51943084\n",
      "Iteration 32517, loss = 4.08575455\n",
      "Iteration 32518, loss = 3.14552106\n",
      "Iteration 32519, loss = 3.23999732\n",
      "Iteration 32520, loss = 4.23709256\n",
      "Iteration 32521, loss = 3.87639589\n",
      "Iteration 32522, loss = 3.74082636\n",
      "Iteration 32523, loss = 3.32917530\n",
      "Iteration 32524, loss = 3.72795916\n",
      "Iteration 32525, loss = 4.38198766\n",
      "Iteration 32526, loss = 4.31891925\n",
      "Iteration 32527, loss = 3.46244903\n",
      "Iteration 32528, loss = 2.85924983\n",
      "Iteration 32529, loss = 2.87868145\n",
      "Iteration 32530, loss = 3.00063023\n",
      "Iteration 32531, loss = 3.08272546\n",
      "Iteration 32532, loss = 3.04326029\n",
      "Iteration 32533, loss = 2.79643071\n",
      "Iteration 32534, loss = 2.79607624\n",
      "Iteration 32535, loss = 3.07187441\n",
      "Iteration 32536, loss = 2.84485900\n",
      "Iteration 32537, loss = 3.09326038\n",
      "Iteration 32538, loss = 2.74664496\n",
      "Iteration 32539, loss = 3.11031877\n",
      "Iteration 32540, loss = 3.02181723\n",
      "Iteration 32541, loss = 3.06272487\n",
      "Iteration 32542, loss = 2.86795775\n",
      "Iteration 32543, loss = 3.04552906\n",
      "Iteration 32544, loss = 3.41504429\n",
      "Iteration 32545, loss = 3.11946987\n",
      "Iteration 32546, loss = 3.22506333\n",
      "Iteration 32547, loss = 3.08477048\n",
      "Iteration 32548, loss = 2.68619002\n",
      "Iteration 32549, loss = 2.99954518\n",
      "Iteration 32550, loss = 3.01333007\n",
      "Iteration 32551, loss = 3.04905089\n",
      "Iteration 32552, loss = 2.67243022\n",
      "Iteration 32553, loss = 2.60583015\n",
      "Iteration 32554, loss = 2.68404666\n",
      "Iteration 32555, loss = 2.88371124\n",
      "Iteration 32556, loss = 3.18616765\n",
      "Iteration 32557, loss = 3.17241091\n",
      "Iteration 32558, loss = 3.60783679\n",
      "Iteration 32559, loss = 2.91450800\n",
      "Iteration 32560, loss = 3.52792843\n",
      "Iteration 32561, loss = 3.27252280\n",
      "Iteration 32562, loss = 3.46513939\n",
      "Iteration 32563, loss = 3.94591959\n",
      "Iteration 32564, loss = 3.51437801\n",
      "Iteration 32565, loss = 2.83266011\n",
      "Iteration 32566, loss = 4.49596931\n",
      "Iteration 32567, loss = 4.04978621\n",
      "Iteration 32568, loss = 3.38776762\n",
      "Iteration 32569, loss = 2.99574910\n",
      "Iteration 32570, loss = 3.08208774\n",
      "Iteration 32571, loss = 4.32767912\n",
      "Iteration 32572, loss = 6.52049255\n",
      "Iteration 32573, loss = 5.14440495\n",
      "Iteration 32574, loss = 4.27761883\n",
      "Iteration 32575, loss = 4.19128275\n",
      "Iteration 32576, loss = 3.51666666\n",
      "Iteration 32577, loss = 3.01140566\n",
      "Iteration 32578, loss = 3.39556823\n",
      "Iteration 32579, loss = 3.36721269\n",
      "Iteration 32580, loss = 3.18692071\n",
      "Iteration 32581, loss = 3.38284619\n",
      "Iteration 32582, loss = 2.85787187\n",
      "Iteration 32583, loss = 4.23575781\n",
      "Iteration 32584, loss = 3.98122538\n",
      "Iteration 32585, loss = 3.88150829\n",
      "Iteration 32586, loss = 2.95859542\n",
      "Iteration 32587, loss = 3.28934788\n",
      "Iteration 32588, loss = 3.35566898\n",
      "Iteration 32589, loss = 2.77905909\n",
      "Iteration 32590, loss = 3.34898880\n",
      "Iteration 32591, loss = 3.47141470\n",
      "Iteration 32592, loss = 3.19324334\n",
      "Iteration 32593, loss = 2.81980269\n",
      "Iteration 32594, loss = 3.27515676\n",
      "Iteration 32595, loss = 2.76610596\n",
      "Iteration 32596, loss = 3.01077844\n",
      "Iteration 32597, loss = 2.79936397\n",
      "Iteration 32598, loss = 2.96235357\n",
      "Iteration 32599, loss = 2.93913345\n",
      "Iteration 32600, loss = 2.84971737\n",
      "Iteration 32601, loss = 2.72221008\n",
      "Iteration 32602, loss = 3.11925126\n",
      "Iteration 32603, loss = 3.04279122\n",
      "Iteration 32604, loss = 2.96153440\n",
      "Iteration 32605, loss = 2.74738709\n",
      "Iteration 32606, loss = 3.09544553\n",
      "Iteration 32607, loss = 3.17437566\n",
      "Iteration 32608, loss = 3.44922620\n",
      "Iteration 32609, loss = 2.84814011\n",
      "Iteration 32610, loss = 2.69426568\n",
      "Iteration 32611, loss = 3.11788123\n",
      "Iteration 32612, loss = 2.53841741\n",
      "Iteration 32613, loss = 2.71446151\n",
      "Iteration 32614, loss = 2.79697508\n",
      "Iteration 32615, loss = 2.98378269\n",
      "Iteration 32616, loss = 3.26000337\n",
      "Iteration 32617, loss = 3.92314184\n",
      "Iteration 32618, loss = 3.73865323\n",
      "Iteration 32619, loss = 3.27924909\n",
      "Iteration 32620, loss = 2.90842951\n",
      "Iteration 32621, loss = 3.21038695\n",
      "Iteration 32622, loss = 2.93041582\n",
      "Iteration 32623, loss = 3.61991005\n",
      "Iteration 32624, loss = 4.29414526\n",
      "Iteration 32625, loss = 3.71292699\n",
      "Iteration 32626, loss = 3.19174616\n",
      "Iteration 32627, loss = 3.05437558\n",
      "Iteration 32628, loss = 3.29831384\n",
      "Iteration 32629, loss = 3.19664738\n",
      "Iteration 32630, loss = 2.86001887\n",
      "Iteration 32631, loss = 3.40088497\n",
      "Iteration 32632, loss = 3.21477977\n",
      "Iteration 32633, loss = 3.50572973\n",
      "Iteration 32634, loss = 3.73593405\n",
      "Iteration 32635, loss = 3.36125512\n",
      "Iteration 32636, loss = 2.77381939\n",
      "Iteration 32637, loss = 2.82313550\n",
      "Iteration 32638, loss = 3.20417051\n",
      "Iteration 32639, loss = 3.02352040\n",
      "Iteration 32640, loss = 2.85486256\n",
      "Iteration 32641, loss = 3.06254397\n",
      "Iteration 32642, loss = 3.20790894\n",
      "Iteration 32643, loss = 3.73476132\n",
      "Iteration 32644, loss = 3.69162481\n",
      "Iteration 32645, loss = 3.68364897\n",
      "Iteration 32646, loss = 3.53302039\n",
      "Iteration 32647, loss = 3.89871142\n",
      "Iteration 32648, loss = 3.34507305\n",
      "Iteration 32649, loss = 3.21546329\n",
      "Iteration 32650, loss = 3.30064952\n",
      "Iteration 32651, loss = 3.33905706\n",
      "Iteration 32652, loss = 3.06901940\n",
      "Iteration 32653, loss = 3.41457950\n",
      "Iteration 32654, loss = 2.84606835\n",
      "Iteration 32655, loss = 3.34008661\n",
      "Iteration 32656, loss = 5.08830659\n",
      "Iteration 32657, loss = 4.68666520\n",
      "Iteration 32658, loss = 4.13015992\n",
      "Iteration 32659, loss = 4.15730291\n",
      "Iteration 32660, loss = 4.70147565\n",
      "Iteration 32661, loss = 3.97293368\n",
      "Iteration 32662, loss = 3.75571063\n",
      "Iteration 32663, loss = 2.93959954\n",
      "Iteration 32664, loss = 2.94606833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32665, loss = 2.99034639\n",
      "Iteration 32666, loss = 2.86917659\n",
      "Iteration 32667, loss = 3.79447922\n",
      "Iteration 32668, loss = 3.55565278\n",
      "Iteration 32669, loss = 3.28656064\n",
      "Iteration 32670, loss = 2.93585288\n",
      "Iteration 32671, loss = 4.12373485\n",
      "Iteration 32672, loss = 4.24463552\n",
      "Iteration 32673, loss = 3.84421276\n",
      "Iteration 32674, loss = 3.59606610\n",
      "Iteration 32675, loss = 3.20168566\n",
      "Iteration 32676, loss = 3.11053816\n",
      "Iteration 32677, loss = 3.64945930\n",
      "Iteration 32678, loss = 3.19002584\n",
      "Iteration 32679, loss = 2.70022372\n",
      "Iteration 32680, loss = 2.72913644\n",
      "Iteration 32681, loss = 2.68335423\n",
      "Iteration 32682, loss = 2.77394521\n",
      "Iteration 32683, loss = 3.50773548\n",
      "Iteration 32684, loss = 4.13319589\n",
      "Iteration 32685, loss = 3.67434800\n",
      "Iteration 32686, loss = 3.19371429\n",
      "Iteration 32687, loss = 2.81266005\n",
      "Iteration 32688, loss = 3.09645139\n",
      "Iteration 32689, loss = 3.32426490\n",
      "Iteration 32690, loss = 3.12368091\n",
      "Iteration 32691, loss = 3.15206401\n",
      "Iteration 32692, loss = 2.93217647\n",
      "Iteration 32693, loss = 2.68242685\n",
      "Iteration 32694, loss = 2.81808194\n",
      "Iteration 32695, loss = 3.19221162\n",
      "Iteration 32696, loss = 4.06062759\n",
      "Iteration 32697, loss = 4.09983037\n",
      "Iteration 32698, loss = 3.70751639\n",
      "Iteration 32699, loss = 3.24139261\n",
      "Iteration 32700, loss = 3.17934191\n",
      "Iteration 32701, loss = 2.70311338\n",
      "Iteration 32702, loss = 3.19678572\n",
      "Iteration 32703, loss = 3.39938839\n",
      "Iteration 32704, loss = 3.41689850\n",
      "Iteration 32705, loss = 3.27551644\n",
      "Iteration 32706, loss = 3.53501214\n",
      "Iteration 32707, loss = 3.04554658\n",
      "Iteration 32708, loss = 2.80421665\n",
      "Iteration 32709, loss = 2.74925331\n",
      "Iteration 32710, loss = 3.10292176\n",
      "Iteration 32711, loss = 3.75295087\n",
      "Iteration 32712, loss = 3.11977566\n",
      "Iteration 32713, loss = 2.88487889\n",
      "Iteration 32714, loss = 3.40344827\n",
      "Iteration 32715, loss = 2.85447084\n",
      "Iteration 32716, loss = 2.76403524\n",
      "Iteration 32717, loss = 2.72496532\n",
      "Iteration 32718, loss = 2.91757106\n",
      "Iteration 32719, loss = 3.16228778\n",
      "Iteration 32720, loss = 3.14652868\n",
      "Iteration 32721, loss = 2.75512936\n",
      "Iteration 32722, loss = 3.60422853\n",
      "Iteration 32723, loss = 6.93188654\n",
      "Iteration 32724, loss = 6.08497311\n",
      "Iteration 32725, loss = 4.42725010\n",
      "Iteration 32726, loss = 3.85066151\n",
      "Iteration 32727, loss = 3.61702167\n",
      "Iteration 32728, loss = 3.65021030\n",
      "Iteration 32729, loss = 3.70188513\n",
      "Iteration 32730, loss = 3.24240312\n",
      "Iteration 32731, loss = 2.99672979\n",
      "Iteration 32732, loss = 3.63093626\n",
      "Iteration 32733, loss = 3.76899760\n",
      "Iteration 32734, loss = 3.26875268\n",
      "Iteration 32735, loss = 3.29023263\n",
      "Iteration 32736, loss = 3.32521306\n",
      "Iteration 32737, loss = 3.11644191\n",
      "Iteration 32738, loss = 2.86853161\n",
      "Iteration 32739, loss = 3.39406641\n",
      "Iteration 32740, loss = 3.29549051\n",
      "Iteration 32741, loss = 3.77097075\n",
      "Iteration 32742, loss = 3.46079856\n",
      "Iteration 32743, loss = 3.63808147\n",
      "Iteration 32744, loss = 2.90451000\n",
      "Iteration 32745, loss = 2.94174204\n",
      "Iteration 32746, loss = 2.70768947\n",
      "Iteration 32747, loss = 3.00359725\n",
      "Iteration 32748, loss = 3.48000497\n",
      "Iteration 32749, loss = 3.66243046\n",
      "Iteration 32750, loss = 3.48310150\n",
      "Iteration 32751, loss = 2.92408826\n",
      "Iteration 32752, loss = 3.22041307\n",
      "Iteration 32753, loss = 3.54189557\n",
      "Iteration 32754, loss = 3.22490095\n",
      "Iteration 32755, loss = 2.65955363\n",
      "Iteration 32756, loss = 2.88638129\n",
      "Iteration 32757, loss = 2.93536079\n",
      "Iteration 32758, loss = 3.19120167\n",
      "Iteration 32759, loss = 3.27389739\n",
      "Iteration 32760, loss = 3.04429496\n",
      "Iteration 32761, loss = 3.68206445\n",
      "Iteration 32762, loss = 3.62678472\n",
      "Iteration 32763, loss = 4.88544809\n",
      "Iteration 32764, loss = 3.64992077\n",
      "Iteration 32765, loss = 3.87294298\n",
      "Iteration 32766, loss = 3.33834910\n",
      "Iteration 32767, loss = 3.36734330\n",
      "Iteration 32768, loss = 3.49856502\n",
      "Iteration 32769, loss = 2.97299890\n",
      "Iteration 32770, loss = 3.13918422\n",
      "Iteration 32771, loss = 3.94610464\n",
      "Iteration 32772, loss = 3.61837371\n",
      "Iteration 32773, loss = 3.22269101\n",
      "Iteration 32774, loss = 2.83943446\n",
      "Iteration 32775, loss = 2.85637954\n",
      "Iteration 32776, loss = 3.37984823\n",
      "Iteration 32777, loss = 3.38059023\n",
      "Iteration 32778, loss = 4.33264046\n",
      "Iteration 32779, loss = 3.91545965\n",
      "Iteration 32780, loss = 3.40291432\n",
      "Iteration 32781, loss = 3.71278527\n",
      "Iteration 32782, loss = 2.93569417\n",
      "Iteration 32783, loss = 2.85045575\n",
      "Iteration 32784, loss = 2.76404716\n",
      "Iteration 32785, loss = 3.06885688\n",
      "Iteration 32786, loss = 2.99157473\n",
      "Iteration 32787, loss = 2.98824223\n",
      "Iteration 32788, loss = 3.05029140\n",
      "Iteration 32789, loss = 3.21442838\n",
      "Iteration 32790, loss = 3.33967981\n",
      "Iteration 32791, loss = 3.21791387\n",
      "Iteration 32792, loss = 4.24171472\n",
      "Iteration 32793, loss = 4.71136839\n",
      "Iteration 32794, loss = 7.34225794\n",
      "Iteration 32795, loss = 6.78651157\n",
      "Iteration 32796, loss = 4.59061658\n",
      "Iteration 32797, loss = 4.68884413\n",
      "Iteration 32798, loss = 4.05518159\n",
      "Iteration 32799, loss = 3.24832229\n",
      "Iteration 32800, loss = 3.58545744\n",
      "Iteration 32801, loss = 4.37919462\n",
      "Iteration 32802, loss = 4.14181362\n",
      "Iteration 32803, loss = 5.23955770\n",
      "Iteration 32804, loss = 4.82841388\n",
      "Iteration 32805, loss = 3.54032419\n",
      "Iteration 32806, loss = 3.17119829\n",
      "Iteration 32807, loss = 3.12640609\n",
      "Iteration 32808, loss = 2.88295851\n",
      "Iteration 32809, loss = 2.66416020\n",
      "Iteration 32810, loss = 3.08174977\n",
      "Iteration 32811, loss = 3.02395554\n",
      "Iteration 32812, loss = 2.73610223\n",
      "Iteration 32813, loss = 2.58887848\n",
      "Iteration 32814, loss = 2.76794988\n",
      "Iteration 32815, loss = 2.75555962\n",
      "Iteration 32816, loss = 2.53548377\n",
      "Iteration 32817, loss = 2.88560682\n",
      "Iteration 32818, loss = 2.73719801\n",
      "Iteration 32819, loss = 3.13534109\n",
      "Iteration 32820, loss = 3.21008823\n",
      "Iteration 32821, loss = 3.31703352\n",
      "Iteration 32822, loss = 4.75712273\n",
      "Iteration 32823, loss = 4.72788982\n",
      "Iteration 32824, loss = 4.78713831\n",
      "Iteration 32825, loss = 3.38890079\n",
      "Iteration 32826, loss = 3.69830238\n",
      "Iteration 32827, loss = 3.53787004\n",
      "Iteration 32828, loss = 3.55210083\n",
      "Iteration 32829, loss = 3.16942924\n",
      "Iteration 32830, loss = 3.56340983\n",
      "Iteration 32831, loss = 3.84680736\n",
      "Iteration 32832, loss = 3.50398829\n",
      "Iteration 32833, loss = 3.47977278\n",
      "Iteration 32834, loss = 3.26223273\n",
      "Iteration 32835, loss = 3.26523724\n",
      "Iteration 32836, loss = 3.04627887\n",
      "Iteration 32837, loss = 2.87844579\n",
      "Iteration 32838, loss = 2.61357142\n",
      "Iteration 32839, loss = 2.93523927\n",
      "Iteration 32840, loss = 2.94520252\n",
      "Iteration 32841, loss = 3.11060359\n",
      "Iteration 32842, loss = 3.07242170\n",
      "Iteration 32843, loss = 2.96850233\n",
      "Iteration 32844, loss = 2.99955450\n",
      "Iteration 32845, loss = 2.68452406\n",
      "Iteration 32846, loss = 3.10405428\n",
      "Iteration 32847, loss = 2.75540160\n",
      "Iteration 32848, loss = 3.02798631\n",
      "Iteration 32849, loss = 3.41687130\n",
      "Iteration 32850, loss = 2.96275035\n",
      "Iteration 32851, loss = 2.76461212\n",
      "Iteration 32852, loss = 2.60249524\n",
      "Iteration 32853, loss = 3.26539400\n",
      "Iteration 32854, loss = 3.24919404\n",
      "Iteration 32855, loss = 3.16797937\n",
      "Iteration 32856, loss = 2.96102077\n",
      "Iteration 32857, loss = 3.24907052\n",
      "Iteration 32858, loss = 3.41310629\n",
      "Iteration 32859, loss = 3.17444263\n",
      "Iteration 32860, loss = 3.72804000\n",
      "Iteration 32861, loss = 3.46261853\n",
      "Iteration 32862, loss = 2.79622424\n",
      "Iteration 32863, loss = 3.44411449\n",
      "Iteration 32864, loss = 4.68851859\n",
      "Iteration 32865, loss = 4.93324366\n",
      "Iteration 32866, loss = 6.31512350\n",
      "Iteration 32867, loss = 4.33011109\n",
      "Iteration 32868, loss = 6.04482074\n",
      "Iteration 32869, loss = 5.04397818\n",
      "Iteration 32870, loss = 3.01419351\n",
      "Iteration 32871, loss = 3.07679247\n",
      "Iteration 32872, loss = 3.40493945\n",
      "Iteration 32873, loss = 3.19946957\n",
      "Iteration 32874, loss = 3.38011326\n",
      "Iteration 32875, loss = 3.05123115\n",
      "Iteration 32876, loss = 2.98899922\n",
      "Iteration 32877, loss = 3.35803059\n",
      "Iteration 32878, loss = 2.99538743\n",
      "Iteration 32879, loss = 3.30806103\n",
      "Iteration 32880, loss = 3.14028009\n",
      "Iteration 32881, loss = 2.76357160\n",
      "Iteration 32882, loss = 2.72325138\n",
      "Iteration 32883, loss = 2.59973519\n",
      "Iteration 32884, loss = 2.72566627\n",
      "Iteration 32885, loss = 2.83839361\n",
      "Iteration 32886, loss = 2.76870488\n",
      "Iteration 32887, loss = 3.14109381\n",
      "Iteration 32888, loss = 4.04553162\n",
      "Iteration 32889, loss = 4.00711943\n",
      "Iteration 32890, loss = 4.36799360\n",
      "Iteration 32891, loss = 3.74122127\n",
      "Iteration 32892, loss = 3.66434901\n",
      "Iteration 32893, loss = 3.44368358\n",
      "Iteration 32894, loss = 4.40056258\n",
      "Iteration 32895, loss = 3.35183775\n",
      "Iteration 32896, loss = 3.72229539\n",
      "Iteration 32897, loss = 5.48405079\n",
      "Iteration 32898, loss = 6.80619875\n",
      "Iteration 32899, loss = 7.65206269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32900, loss = 4.99018727\n",
      "Iteration 32901, loss = 4.34555316\n",
      "Iteration 32902, loss = 3.20167577\n",
      "Iteration 32903, loss = 3.36469635\n",
      "Iteration 32904, loss = 3.19334149\n",
      "Iteration 32905, loss = 3.32965682\n",
      "Iteration 32906, loss = 3.18758333\n",
      "Iteration 32907, loss = 2.87762241\n",
      "Iteration 32908, loss = 2.91885877\n",
      "Iteration 32909, loss = 2.68367096\n",
      "Iteration 32910, loss = 2.97933360\n",
      "Iteration 32911, loss = 4.10689587\n",
      "Iteration 32912, loss = 3.20963356\n",
      "Iteration 32913, loss = 3.13727608\n",
      "Iteration 32914, loss = 2.95970211\n",
      "Iteration 32915, loss = 2.99747658\n",
      "Iteration 32916, loss = 3.10034341\n",
      "Iteration 32917, loss = 3.21364518\n",
      "Iteration 32918, loss = 3.04643917\n",
      "Iteration 32919, loss = 2.85828551\n",
      "Iteration 32920, loss = 2.96668359\n",
      "Iteration 32921, loss = 3.06962678\n",
      "Iteration 32922, loss = 2.65388695\n",
      "Iteration 32923, loss = 3.51625438\n",
      "Iteration 32924, loss = 3.74221829\n",
      "Iteration 32925, loss = 3.01920646\n",
      "Iteration 32926, loss = 3.61987027\n",
      "Iteration 32927, loss = 3.52445976\n",
      "Iteration 32928, loss = 3.29759576\n",
      "Iteration 32929, loss = 2.77693688\n",
      "Iteration 32930, loss = 2.60200629\n",
      "Iteration 32931, loss = 3.02936582\n",
      "Iteration 32932, loss = 2.93464263\n",
      "Iteration 32933, loss = 2.89770225\n",
      "Iteration 32934, loss = 3.23675646\n",
      "Iteration 32935, loss = 3.85878782\n",
      "Iteration 32936, loss = 3.47342187\n",
      "Iteration 32937, loss = 3.93688711\n",
      "Iteration 32938, loss = 3.70111541\n",
      "Iteration 32939, loss = 3.56035856\n",
      "Iteration 32940, loss = 6.68409955\n",
      "Iteration 32941, loss = 4.06899661\n",
      "Iteration 32942, loss = 3.59331848\n",
      "Iteration 32943, loss = 3.01474314\n",
      "Iteration 32944, loss = 3.46548470\n",
      "Iteration 32945, loss = 3.72596616\n",
      "Iteration 32946, loss = 4.41093718\n",
      "Iteration 32947, loss = 2.90072309\n",
      "Iteration 32948, loss = 2.86655680\n",
      "Iteration 32949, loss = 2.89460267\n",
      "Iteration 32950, loss = 2.98830356\n",
      "Iteration 32951, loss = 3.18413681\n",
      "Iteration 32952, loss = 2.97546117\n",
      "Iteration 32953, loss = 3.08522347\n",
      "Iteration 32954, loss = 2.89567147\n",
      "Iteration 32955, loss = 2.74403570\n",
      "Iteration 32956, loss = 3.04333842\n",
      "Iteration 32957, loss = 2.96396240\n",
      "Iteration 32958, loss = 3.12102754\n",
      "Iteration 32959, loss = 2.98634506\n",
      "Iteration 32960, loss = 3.05469638\n",
      "Iteration 32961, loss = 3.01234707\n",
      "Iteration 32962, loss = 3.84839205\n",
      "Iteration 32963, loss = 3.76035269\n",
      "Iteration 32964, loss = 2.95404764\n",
      "Iteration 32965, loss = 3.40182969\n",
      "Iteration 32966, loss = 3.34762494\n",
      "Iteration 32967, loss = 3.76927074\n",
      "Iteration 32968, loss = 3.15259577\n",
      "Iteration 32969, loss = 2.99752615\n",
      "Iteration 32970, loss = 2.79809870\n",
      "Iteration 32971, loss = 2.93961920\n",
      "Iteration 32972, loss = 2.70332137\n",
      "Iteration 32973, loss = 2.76487603\n",
      "Iteration 32974, loss = 2.64618885\n",
      "Iteration 32975, loss = 2.76332450\n",
      "Iteration 32976, loss = 3.04610313\n",
      "Iteration 32977, loss = 2.96516228\n",
      "Iteration 32978, loss = 3.42648888\n",
      "Iteration 32979, loss = 3.29056548\n",
      "Iteration 32980, loss = 2.95535027\n",
      "Iteration 32981, loss = 3.32773374\n",
      "Iteration 32982, loss = 3.02562980\n",
      "Iteration 32983, loss = 3.04878891\n",
      "Iteration 32984, loss = 3.31773335\n",
      "Iteration 32985, loss = 3.86892880\n",
      "Iteration 32986, loss = 3.12306447\n",
      "Iteration 32987, loss = 3.18462559\n",
      "Iteration 32988, loss = 3.37072223\n",
      "Iteration 32989, loss = 3.27612622\n",
      "Iteration 32990, loss = 3.74051059\n",
      "Iteration 32991, loss = 2.89778416\n",
      "Iteration 32992, loss = 3.32412488\n",
      "Iteration 32993, loss = 3.81444085\n",
      "Iteration 32994, loss = 3.20845719\n",
      "Iteration 32995, loss = 2.80818381\n",
      "Iteration 32996, loss = 2.82503660\n",
      "Iteration 32997, loss = 2.93017703\n",
      "Iteration 32998, loss = 3.01035844\n",
      "Iteration 32999, loss = 2.96311998\n",
      "Iteration 33000, loss = 3.14264703\n",
      "Iteration 33001, loss = 3.58321538\n",
      "Iteration 33002, loss = 3.09529868\n",
      "Iteration 33003, loss = 3.04395587\n",
      "Iteration 33004, loss = 2.84653228\n",
      "Iteration 33005, loss = 2.89886802\n",
      "Iteration 33006, loss = 2.82117423\n",
      "Iteration 33007, loss = 2.80048248\n",
      "Iteration 33008, loss = 3.16202708\n",
      "Iteration 33009, loss = 3.30589220\n",
      "Iteration 33010, loss = 3.42291152\n",
      "Iteration 33011, loss = 3.66419618\n",
      "Iteration 33012, loss = 3.63568064\n",
      "Iteration 33013, loss = 3.26482095\n",
      "Iteration 33014, loss = 2.82221080\n",
      "Iteration 33015, loss = 3.31818478\n",
      "Iteration 33016, loss = 2.65355333\n",
      "Iteration 33017, loss = 3.28617158\n",
      "Iteration 33018, loss = 3.49192511\n",
      "Iteration 33019, loss = 3.25816811\n",
      "Iteration 33020, loss = 3.15776426\n",
      "Iteration 33021, loss = 2.96874693\n",
      "Iteration 33022, loss = 2.91163443\n",
      "Iteration 33023, loss = 3.12768494\n",
      "Iteration 33024, loss = 3.05698948\n",
      "Iteration 33025, loss = 3.36057138\n",
      "Iteration 33026, loss = 3.17408463\n",
      "Iteration 33027, loss = 3.33549619\n",
      "Iteration 33028, loss = 3.35332098\n",
      "Iteration 33029, loss = 3.06074241\n",
      "Iteration 33030, loss = 3.66220826\n",
      "Iteration 33031, loss = 3.58805710\n",
      "Iteration 33032, loss = 3.43761884\n",
      "Iteration 33033, loss = 3.27103176\n",
      "Iteration 33034, loss = 3.78332405\n",
      "Iteration 33035, loss = 3.35502900\n",
      "Iteration 33036, loss = 3.63019824\n",
      "Iteration 33037, loss = 2.94977738\n",
      "Iteration 33038, loss = 2.94994683\n",
      "Iteration 33039, loss = 2.69536880\n",
      "Iteration 33040, loss = 2.73143231\n",
      "Iteration 33041, loss = 2.98810252\n",
      "Iteration 33042, loss = 2.81510604\n",
      "Iteration 33043, loss = 3.17316371\n",
      "Iteration 33044, loss = 3.98868515\n",
      "Iteration 33045, loss = 4.06880525\n",
      "Iteration 33046, loss = 3.93139908\n",
      "Iteration 33047, loss = 4.16878413\n",
      "Iteration 33048, loss = 3.33936611\n",
      "Iteration 33049, loss = 2.73789744\n",
      "Iteration 33050, loss = 2.67734781\n",
      "Iteration 33051, loss = 2.67594699\n",
      "Iteration 33052, loss = 2.85591129\n",
      "Iteration 33053, loss = 2.70902767\n",
      "Iteration 33054, loss = 3.11814663\n",
      "Iteration 33055, loss = 3.01058133\n",
      "Iteration 33056, loss = 3.43576179\n",
      "Iteration 33057, loss = 3.72696983\n",
      "Iteration 33058, loss = 2.89516463\n",
      "Iteration 33059, loss = 3.01786246\n",
      "Iteration 33060, loss = 2.69169496\n",
      "Iteration 33061, loss = 3.00087430\n",
      "Iteration 33062, loss = 3.50477525\n",
      "Iteration 33063, loss = 3.74773660\n",
      "Iteration 33064, loss = 3.58191390\n",
      "Iteration 33065, loss = 3.17595884\n",
      "Iteration 33066, loss = 2.99018512\n",
      "Iteration 33067, loss = 3.19881831\n",
      "Iteration 33068, loss = 4.00732696\n",
      "Iteration 33069, loss = 3.87300796\n",
      "Iteration 33070, loss = 4.16996095\n",
      "Iteration 33071, loss = 3.58629390\n",
      "Iteration 33072, loss = 3.42448122\n",
      "Iteration 33073, loss = 3.97535468\n",
      "Iteration 33074, loss = 3.71567124\n",
      "Iteration 33075, loss = 2.89813360\n",
      "Iteration 33076, loss = 2.99224164\n",
      "Iteration 33077, loss = 2.76739207\n",
      "Iteration 33078, loss = 3.11609759\n",
      "Iteration 33079, loss = 2.94800552\n",
      "Iteration 33080, loss = 2.95740337\n",
      "Iteration 33081, loss = 2.72360890\n",
      "Iteration 33082, loss = 2.81208439\n",
      "Iteration 33083, loss = 2.62594971\n",
      "Iteration 33084, loss = 3.41771952\n",
      "Iteration 33085, loss = 3.05678646\n",
      "Iteration 33086, loss = 3.67199712\n",
      "Iteration 33087, loss = 3.99167496\n",
      "Iteration 33088, loss = 5.24631906\n",
      "Iteration 33089, loss = 4.37023365\n",
      "Iteration 33090, loss = 3.25406087\n",
      "Iteration 33091, loss = 2.95590514\n",
      "Iteration 33092, loss = 2.89667320\n",
      "Iteration 33093, loss = 2.73948418\n",
      "Iteration 33094, loss = 3.06209122\n",
      "Iteration 33095, loss = 2.85304990\n",
      "Iteration 33096, loss = 3.61784454\n",
      "Iteration 33097, loss = 3.20958358\n",
      "Iteration 33098, loss = 3.48752505\n",
      "Iteration 33099, loss = 3.02594142\n",
      "Iteration 33100, loss = 3.33214838\n",
      "Iteration 33101, loss = 2.76827365\n",
      "Iteration 33102, loss = 2.56244702\n",
      "Iteration 33103, loss = 2.64098338\n",
      "Iteration 33104, loss = 2.88881316\n",
      "Iteration 33105, loss = 2.94258422\n",
      "Iteration 33106, loss = 3.19915185\n",
      "Iteration 33107, loss = 2.89593698\n",
      "Iteration 33108, loss = 3.27509878\n",
      "Iteration 33109, loss = 2.58124052\n",
      "Iteration 33110, loss = 2.88652372\n",
      "Iteration 33111, loss = 3.15950792\n",
      "Iteration 33112, loss = 3.05225936\n",
      "Iteration 33113, loss = 2.90815374\n",
      "Iteration 33114, loss = 3.28400970\n",
      "Iteration 33115, loss = 3.04788254\n",
      "Iteration 33116, loss = 3.27337450\n",
      "Iteration 33117, loss = 3.52534197\n",
      "Iteration 33118, loss = 4.06887231\n",
      "Iteration 33119, loss = 3.79485841\n",
      "Iteration 33120, loss = 3.58259084\n",
      "Iteration 33121, loss = 3.80168957\n",
      "Iteration 33122, loss = 4.82435278\n",
      "Iteration 33123, loss = 4.35533618\n",
      "Iteration 33124, loss = 3.95212718\n",
      "Iteration 33125, loss = 3.26212155\n",
      "Iteration 33126, loss = 3.01415637\n",
      "Iteration 33127, loss = 2.97376637\n",
      "Iteration 33128, loss = 4.43191713\n",
      "Iteration 33129, loss = 3.51921968\n",
      "Iteration 33130, loss = 3.74632201\n",
      "Iteration 33131, loss = 3.54532767\n",
      "Iteration 33132, loss = 3.38191830\n",
      "Iteration 33133, loss = 3.42221154\n",
      "Iteration 33134, loss = 3.38660251\n",
      "Iteration 33135, loss = 4.04671057\n",
      "Iteration 33136, loss = 4.21048758\n",
      "Iteration 33137, loss = 3.56961197\n",
      "Iteration 33138, loss = 3.50197099\n",
      "Iteration 33139, loss = 3.64641299\n",
      "Iteration 33140, loss = 3.03345454\n",
      "Iteration 33141, loss = 3.11726766\n",
      "Iteration 33142, loss = 3.82873558\n",
      "Iteration 33143, loss = 3.01526205\n",
      "Iteration 33144, loss = 3.54915138\n",
      "Iteration 33145, loss = 3.44074891\n",
      "Iteration 33146, loss = 3.19433923\n",
      "Iteration 33147, loss = 2.56291876\n",
      "Iteration 33148, loss = 2.65866252\n",
      "Iteration 33149, loss = 2.68190729\n",
      "Iteration 33150, loss = 2.98943110\n",
      "Iteration 33151, loss = 3.07968929\n",
      "Iteration 33152, loss = 2.98702430\n",
      "Iteration 33153, loss = 3.45010020\n",
      "Iteration 33154, loss = 3.61565246\n",
      "Iteration 33155, loss = 3.77668012\n",
      "Iteration 33156, loss = 3.43980366\n",
      "Iteration 33157, loss = 3.38563752\n",
      "Iteration 33158, loss = 3.20312882\n",
      "Iteration 33159, loss = 3.11449987\n",
      "Iteration 33160, loss = 3.09931056\n",
      "Iteration 33161, loss = 3.58489290\n",
      "Iteration 33162, loss = 3.30154801\n",
      "Iteration 33163, loss = 3.41237114\n",
      "Iteration 33164, loss = 2.94479541\n",
      "Iteration 33165, loss = 3.31570217\n",
      "Iteration 33166, loss = 2.94418184\n",
      "Iteration 33167, loss = 3.02490894\n",
      "Iteration 33168, loss = 3.01975227\n",
      "Iteration 33169, loss = 2.71506161\n",
      "Iteration 33170, loss = 2.89381408\n",
      "Iteration 33171, loss = 3.81936190\n",
      "Iteration 33172, loss = 4.32913747\n",
      "Iteration 33173, loss = 5.37600183\n",
      "Iteration 33174, loss = 4.22079968\n",
      "Iteration 33175, loss = 3.96142300\n",
      "Iteration 33176, loss = 3.29373610\n",
      "Iteration 33177, loss = 3.13382019\n",
      "Iteration 33178, loss = 3.44467496\n",
      "Iteration 33179, loss = 2.97019268\n",
      "Iteration 33180, loss = 3.46236463\n",
      "Iteration 33181, loss = 3.89561036\n",
      "Iteration 33182, loss = 3.34912098\n",
      "Iteration 33183, loss = 3.04701252\n",
      "Iteration 33184, loss = 3.71925794\n",
      "Iteration 33185, loss = 3.65763161\n",
      "Iteration 33186, loss = 4.35371473\n",
      "Iteration 33187, loss = 5.27520789\n",
      "Iteration 33188, loss = 5.12023730\n",
      "Iteration 33189, loss = 3.74959140\n",
      "Iteration 33190, loss = 3.37954652\n",
      "Iteration 33191, loss = 3.04664065\n",
      "Iteration 33192, loss = 3.20955992\n",
      "Iteration 33193, loss = 3.19250034\n",
      "Iteration 33194, loss = 2.68242399\n",
      "Iteration 33195, loss = 2.79554245\n",
      "Iteration 33196, loss = 3.00776071\n",
      "Iteration 33197, loss = 3.00031685\n",
      "Iteration 33198, loss = 3.12225352\n",
      "Iteration 33199, loss = 2.93833109\n",
      "Iteration 33200, loss = 3.14023894\n",
      "Iteration 33201, loss = 3.43602471\n",
      "Iteration 33202, loss = 3.03712059\n",
      "Iteration 33203, loss = 3.56662579\n",
      "Iteration 33204, loss = 3.09395177\n",
      "Iteration 33205, loss = 3.23578937\n",
      "Iteration 33206, loss = 3.29025591\n",
      "Iteration 33207, loss = 2.56085551\n",
      "Iteration 33208, loss = 2.60639156\n",
      "Iteration 33209, loss = 2.91075654\n",
      "Iteration 33210, loss = 2.58960681\n",
      "Iteration 33211, loss = 2.81100854\n",
      "Iteration 33212, loss = 2.85823148\n",
      "Iteration 33213, loss = 3.74730721\n",
      "Iteration 33214, loss = 3.58602361\n",
      "Iteration 33215, loss = 3.21899926\n",
      "Iteration 33216, loss = 4.68667920\n",
      "Iteration 33217, loss = 3.96759814\n",
      "Iteration 33218, loss = 4.07946687\n",
      "Iteration 33219, loss = 3.31103796\n",
      "Iteration 33220, loss = 2.58162677\n",
      "Iteration 33221, loss = 2.74595081\n",
      "Iteration 33222, loss = 2.99181732\n",
      "Iteration 33223, loss = 3.01747039\n",
      "Iteration 33224, loss = 3.11255594\n",
      "Iteration 33225, loss = 2.58423250\n",
      "Iteration 33226, loss = 2.64898511\n",
      "Iteration 33227, loss = 2.75407550\n",
      "Iteration 33228, loss = 2.60026033\n",
      "Iteration 33229, loss = 2.83210246\n",
      "Iteration 33230, loss = 3.05707974\n",
      "Iteration 33231, loss = 2.90106331\n",
      "Iteration 33232, loss = 2.97743727\n",
      "Iteration 33233, loss = 2.90504628\n",
      "Iteration 33234, loss = 3.17131614\n",
      "Iteration 33235, loss = 3.22388172\n",
      "Iteration 33236, loss = 2.62294623\n",
      "Iteration 33237, loss = 2.52070912\n",
      "Iteration 33238, loss = 2.65399929\n",
      "Iteration 33239, loss = 2.83872103\n",
      "Iteration 33240, loss = 3.32100262\n",
      "Iteration 33241, loss = 4.73285224\n",
      "Iteration 33242, loss = 3.65325633\n",
      "Iteration 33243, loss = 3.22908220\n",
      "Iteration 33244, loss = 2.92674473\n",
      "Iteration 33245, loss = 3.19353153\n",
      "Iteration 33246, loss = 2.91147687\n",
      "Iteration 33247, loss = 2.65036488\n",
      "Iteration 33248, loss = 2.92770973\n",
      "Iteration 33249, loss = 2.81202017\n",
      "Iteration 33250, loss = 4.18678837\n",
      "Iteration 33251, loss = 3.24210268\n",
      "Iteration 33252, loss = 3.19605972\n",
      "Iteration 33253, loss = 3.63427000\n",
      "Iteration 33254, loss = 4.59150355\n",
      "Iteration 33255, loss = 5.17688576\n",
      "Iteration 33256, loss = 8.27596806\n",
      "Iteration 33257, loss = 7.70607954\n",
      "Iteration 33258, loss = 8.12121363\n",
      "Iteration 33259, loss = 8.66357820\n",
      "Iteration 33260, loss = 7.53163658\n",
      "Iteration 33261, loss = 8.48649913\n",
      "Iteration 33262, loss = 6.46096803\n",
      "Iteration 33263, loss = 4.15889035\n",
      "Iteration 33264, loss = 3.87351026\n",
      "Iteration 33265, loss = 3.31698601\n",
      "Iteration 33266, loss = 3.33364685\n",
      "Iteration 33267, loss = 3.46606298\n",
      "Iteration 33268, loss = 3.95490881\n",
      "Iteration 33269, loss = 3.45533286\n",
      "Iteration 33270, loss = 3.18473908\n",
      "Iteration 33271, loss = 4.56156025\n",
      "Iteration 33272, loss = 4.21671132\n",
      "Iteration 33273, loss = 3.57762075\n",
      "Iteration 33274, loss = 3.04877691\n",
      "Iteration 33275, loss = 2.90480290\n",
      "Iteration 33276, loss = 3.14188127\n",
      "Iteration 33277, loss = 3.79207938\n",
      "Iteration 33278, loss = 3.50310997\n",
      "Iteration 33279, loss = 3.57134266\n",
      "Iteration 33280, loss = 3.55526668\n",
      "Iteration 33281, loss = 3.46012124\n",
      "Iteration 33282, loss = 3.16403602\n",
      "Iteration 33283, loss = 3.02115415\n",
      "Iteration 33284, loss = 2.63818054\n",
      "Iteration 33285, loss = 2.86585639\n",
      "Iteration 33286, loss = 2.97917126\n",
      "Iteration 33287, loss = 3.20634594\n",
      "Iteration 33288, loss = 3.19666900\n",
      "Iteration 33289, loss = 3.44769244\n",
      "Iteration 33290, loss = 3.03625980\n",
      "Iteration 33291, loss = 3.56095854\n",
      "Iteration 33292, loss = 3.46188749\n",
      "Iteration 33293, loss = 3.37714692\n",
      "Iteration 33294, loss = 2.72579321\n",
      "Iteration 33295, loss = 3.08034211\n",
      "Iteration 33296, loss = 3.85389453\n",
      "Iteration 33297, loss = 3.55069051\n",
      "Iteration 33298, loss = 3.12931558\n",
      "Iteration 33299, loss = 3.08128037\n",
      "Iteration 33300, loss = 3.17310316\n",
      "Iteration 33301, loss = 2.75090203\n",
      "Iteration 33302, loss = 3.65037051\n",
      "Iteration 33303, loss = 3.31731198\n",
      "Iteration 33304, loss = 3.36679771\n",
      "Iteration 33305, loss = 3.05089931\n",
      "Iteration 33306, loss = 2.95202039\n",
      "Iteration 33307, loss = 2.91739486\n",
      "Iteration 33308, loss = 3.12921214\n",
      "Iteration 33309, loss = 3.00557509\n",
      "Iteration 33310, loss = 2.56355133\n",
      "Iteration 33311, loss = 2.69564528\n",
      "Iteration 33312, loss = 2.70795982\n",
      "Iteration 33313, loss = 3.10506356\n",
      "Iteration 33314, loss = 3.22770290\n",
      "Iteration 33315, loss = 2.76496877\n",
      "Iteration 33316, loss = 2.93697496\n",
      "Iteration 33317, loss = 2.64740089\n",
      "Iteration 33318, loss = 3.61972363\n",
      "Iteration 33319, loss = 4.04445457\n",
      "Iteration 33320, loss = 5.06074203\n",
      "Iteration 33321, loss = 4.75897721\n",
      "Iteration 33322, loss = 5.14487027\n",
      "Iteration 33323, loss = 5.64714928\n",
      "Iteration 33324, loss = 8.89406839\n",
      "Iteration 33325, loss = 5.81836561\n",
      "Iteration 33326, loss = 5.31504816\n",
      "Iteration 33327, loss = 4.79968264\n",
      "Iteration 33328, loss = 4.20786561\n",
      "Iteration 33329, loss = 3.20774532\n",
      "Iteration 33330, loss = 2.98733376\n",
      "Iteration 33331, loss = 2.96692436\n",
      "Iteration 33332, loss = 2.70990534\n",
      "Iteration 33333, loss = 2.87135787\n",
      "Iteration 33334, loss = 2.92384470\n",
      "Iteration 33335, loss = 3.13249618\n",
      "Iteration 33336, loss = 3.22349412\n",
      "Iteration 33337, loss = 3.37206612\n",
      "Iteration 33338, loss = 3.21362641\n",
      "Iteration 33339, loss = 3.31649430\n",
      "Iteration 33340, loss = 2.94086999\n",
      "Iteration 33341, loss = 3.38578687\n",
      "Iteration 33342, loss = 3.57298588\n",
      "Iteration 33343, loss = 3.10971994\n",
      "Iteration 33344, loss = 3.66828964\n",
      "Iteration 33345, loss = 3.80654627\n",
      "Iteration 33346, loss = 3.84628466\n",
      "Iteration 33347, loss = 3.27766101\n",
      "Iteration 33348, loss = 2.92475702\n",
      "Iteration 33349, loss = 3.10783879\n",
      "Iteration 33350, loss = 3.27214257\n",
      "Iteration 33351, loss = 3.53119026\n",
      "Iteration 33352, loss = 3.41332412\n",
      "Iteration 33353, loss = 3.10929737\n",
      "Iteration 33354, loss = 3.22544395\n",
      "Iteration 33355, loss = 2.78469746\n",
      "Iteration 33356, loss = 2.81848437\n",
      "Iteration 33357, loss = 2.82051292\n",
      "Iteration 33358, loss = 3.23446372\n",
      "Iteration 33359, loss = 3.28881283\n",
      "Iteration 33360, loss = 3.61034372\n",
      "Iteration 33361, loss = 2.99789187\n",
      "Iteration 33362, loss = 2.82580099\n",
      "Iteration 33363, loss = 2.84036164\n",
      "Iteration 33364, loss = 2.81861759\n",
      "Iteration 33365, loss = 3.57416108\n",
      "Iteration 33366, loss = 4.48015877\n",
      "Iteration 33367, loss = 3.71322658\n",
      "Iteration 33368, loss = 3.55430146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 33369, loss = 2.92083113\n",
      "Iteration 33370, loss = 4.34443404\n",
      "Iteration 33371, loss = 4.13133601\n",
      "Iteration 33372, loss = 4.97527130\n",
      "Iteration 33373, loss = 3.81765130\n",
      "Iteration 33374, loss = 3.47244651\n",
      "Iteration 33375, loss = 3.31619101\n",
      "Iteration 33376, loss = 4.19611301\n",
      "Iteration 33377, loss = 4.13107403\n",
      "Iteration 33378, loss = 4.07328459\n",
      "Iteration 33379, loss = 3.37571124\n",
      "Iteration 33380, loss = 2.95803773\n",
      "Iteration 33381, loss = 2.66151440\n",
      "Iteration 33382, loss = 2.77961743\n",
      "Iteration 33383, loss = 2.79557899\n",
      "Iteration 33384, loss = 2.65613823\n",
      "Iteration 33385, loss = 2.79606204\n",
      "Iteration 33386, loss = 3.08317499\n",
      "Iteration 33387, loss = 2.79717006\n",
      "Iteration 33388, loss = 2.80801245\n",
      "Iteration 33389, loss = 2.74353427\n",
      "Iteration 33390, loss = 2.94079864\n",
      "Iteration 33391, loss = 2.86259925\n",
      "Iteration 33392, loss = 2.71843157\n",
      "Iteration 33393, loss = 2.73211417\n",
      "Iteration 33394, loss = 3.19131484\n",
      "Iteration 33395, loss = 3.10433863\n",
      "Iteration 33396, loss = 2.89249557\n",
      "Iteration 33397, loss = 3.10395491\n",
      "Iteration 33398, loss = 3.57718182\n",
      "Iteration 33399, loss = 2.97165827\n",
      "Iteration 33400, loss = 3.11073965\n",
      "Iteration 33401, loss = 2.74898933\n",
      "Iteration 33402, loss = 3.13779793\n",
      "Iteration 33403, loss = 3.10163286\n",
      "Iteration 33404, loss = 2.86000775\n",
      "Iteration 33405, loss = 2.86985794\n",
      "Iteration 33406, loss = 2.53056848\n",
      "Iteration 33407, loss = 2.80894324\n",
      "Iteration 33408, loss = 2.77704802\n",
      "Iteration 33409, loss = 2.81967826\n",
      "Iteration 33410, loss = 2.61399822\n",
      "Iteration 33411, loss = 2.94942556\n",
      "Iteration 33412, loss = 2.80945034\n",
      "Iteration 33413, loss = 2.94980684\n",
      "Iteration 33414, loss = 2.85248203\n",
      "Iteration 33415, loss = 2.83006272\n",
      "Iteration 33416, loss = 2.87147401\n",
      "Iteration 33417, loss = 2.85905500\n",
      "Iteration 33418, loss = 2.74633891\n",
      "Iteration 33419, loss = 2.61712399\n",
      "Iteration 33420, loss = 2.66780930\n",
      "Iteration 33421, loss = 2.89068732\n",
      "Iteration 33422, loss = 3.12409624\n",
      "Iteration 33423, loss = 3.02369437\n",
      "Iteration 33424, loss = 3.24089950\n",
      "Iteration 33425, loss = 3.40882338\n",
      "Iteration 33426, loss = 3.56271940\n",
      "Iteration 33427, loss = 3.62813280\n",
      "Iteration 33428, loss = 3.54243930\n",
      "Iteration 33429, loss = 3.59192681\n",
      "Iteration 33430, loss = 3.56930516\n",
      "Iteration 33431, loss = 3.51807148\n",
      "Iteration 33432, loss = 3.16553908\n",
      "Iteration 33433, loss = 3.17970204\n",
      "Iteration 33434, loss = 3.35388172\n",
      "Iteration 33435, loss = 2.70501793\n",
      "Iteration 33436, loss = 4.14020298\n",
      "Iteration 33437, loss = 3.45671450\n",
      "Iteration 33438, loss = 3.70869226\n",
      "Iteration 33439, loss = 3.94331632\n",
      "Iteration 33440, loss = 3.98625462\n",
      "Iteration 33441, loss = 3.57716681\n",
      "Iteration 33442, loss = 3.70335086\n",
      "Iteration 33443, loss = 4.68805668\n",
      "Iteration 33444, loss = 4.92930440\n",
      "Iteration 33445, loss = 3.99356124\n",
      "Iteration 33446, loss = 3.82788636\n",
      "Iteration 33447, loss = 4.56573530\n",
      "Iteration 33448, loss = 4.35501672\n",
      "Iteration 33449, loss = 3.44407705\n",
      "Iteration 33450, loss = 4.26209090\n",
      "Iteration 33451, loss = 4.64402747\n",
      "Iteration 33452, loss = 3.62862375\n",
      "Iteration 33453, loss = 3.79472557\n",
      "Iteration 33454, loss = 4.06087857\n",
      "Iteration 33455, loss = 5.46420302\n",
      "Iteration 33456, loss = 6.53513182\n",
      "Iteration 33457, loss = 7.92909065\n",
      "Iteration 33458, loss = 6.69060849\n",
      "Iteration 33459, loss = 4.85800005\n",
      "Iteration 33460, loss = 4.30724015\n",
      "Iteration 33461, loss = 4.30674883\n",
      "Iteration 33462, loss = 3.97819634\n",
      "Iteration 33463, loss = 3.88484366\n",
      "Iteration 33464, loss = 4.03366585\n",
      "Iteration 33465, loss = 3.33606027\n",
      "Iteration 33466, loss = 3.15166629\n",
      "Iteration 33467, loss = 2.61458645\n",
      "Iteration 33468, loss = 2.85682058\n",
      "Iteration 33469, loss = 2.91581416\n",
      "Iteration 33470, loss = 3.63372213\n",
      "Iteration 33471, loss = 3.20720873\n",
      "Iteration 33472, loss = 3.05695902\n",
      "Iteration 33473, loss = 2.82553509\n",
      "Iteration 33474, loss = 3.56223097\n",
      "Iteration 33475, loss = 3.45028529\n",
      "Iteration 33476, loss = 2.93199863\n",
      "Iteration 33477, loss = 3.22397437\n",
      "Iteration 33478, loss = 3.42665728\n",
      "Iteration 33479, loss = 3.39925772\n",
      "Iteration 33480, loss = 3.14410689\n",
      "Iteration 33481, loss = 2.94087162\n",
      "Iteration 33482, loss = 3.26277360\n",
      "Iteration 33483, loss = 2.99834948\n",
      "Iteration 33484, loss = 2.97331483\n",
      "Iteration 33485, loss = 2.78364922\n",
      "Iteration 33486, loss = 2.45285819\n",
      "Iteration 33487, loss = 2.70408907\n",
      "Iteration 33488, loss = 2.78003667\n",
      "Iteration 33489, loss = 2.85459350\n",
      "Iteration 33490, loss = 2.63376805\n",
      "Iteration 33491, loss = 3.57504511\n",
      "Iteration 33492, loss = 3.14056131\n",
      "Iteration 33493, loss = 3.18415223\n",
      "Iteration 33494, loss = 3.14462565\n",
      "Iteration 33495, loss = 2.76298474\n",
      "Iteration 33496, loss = 2.76521080\n",
      "Iteration 33497, loss = 3.10149112\n",
      "Iteration 33498, loss = 4.43518964\n",
      "Iteration 33499, loss = 3.31862780\n",
      "Iteration 33500, loss = 4.49265323\n",
      "Iteration 33501, loss = 3.39086369\n",
      "Iteration 33502, loss = 4.05047878\n",
      "Iteration 33503, loss = 4.03108390\n",
      "Iteration 33504, loss = 3.39725292\n",
      "Iteration 33505, loss = 3.39315412\n",
      "Iteration 33506, loss = 4.59408473\n",
      "Iteration 33507, loss = 4.27107005\n",
      "Iteration 33508, loss = 4.07716066\n",
      "Iteration 33509, loss = 4.36338810\n",
      "Iteration 33510, loss = 4.30235244\n",
      "Iteration 33511, loss = 3.58814051\n",
      "Iteration 33512, loss = 3.00945694\n",
      "Iteration 33513, loss = 2.86482655\n",
      "Iteration 33514, loss = 2.73144620\n",
      "Iteration 33515, loss = 2.73399668\n",
      "Iteration 33516, loss = 2.75334628\n",
      "Iteration 33517, loss = 3.35877778\n",
      "Iteration 33518, loss = 3.52269076\n",
      "Iteration 33519, loss = 3.32597104\n",
      "Iteration 33520, loss = 2.77961860\n",
      "Iteration 33521, loss = 3.02038959\n",
      "Iteration 33522, loss = 2.72628091\n",
      "Iteration 33523, loss = 3.04213597\n",
      "Iteration 33524, loss = 2.92082191\n",
      "Iteration 33525, loss = 3.41741626\n",
      "Iteration 33526, loss = 3.64008676\n",
      "Iteration 33527, loss = 3.64188851\n",
      "Iteration 33528, loss = 3.44559839\n",
      "Iteration 33529, loss = 3.94126452\n",
      "Iteration 33530, loss = 3.17663332\n",
      "Iteration 33531, loss = 3.36173011\n",
      "Iteration 33532, loss = 3.41887463\n",
      "Iteration 33533, loss = 3.06950714\n",
      "Iteration 33534, loss = 2.84844959\n",
      "Iteration 33535, loss = 2.94724757\n",
      "Iteration 33536, loss = 3.14115364\n",
      "Iteration 33537, loss = 3.08216220\n",
      "Iteration 33538, loss = 3.30291092\n",
      "Iteration 33539, loss = 3.46622291\n",
      "Iteration 33540, loss = 3.34259474\n",
      "Iteration 33541, loss = 3.08573323\n",
      "Iteration 33542, loss = 2.94891910\n",
      "Iteration 33543, loss = 3.30827800\n",
      "Iteration 33544, loss = 3.14812162\n",
      "Iteration 33545, loss = 3.94532739\n",
      "Iteration 33546, loss = 3.78832959\n",
      "Iteration 33547, loss = 3.43173573\n",
      "Iteration 33548, loss = 3.25596844\n",
      "Iteration 33549, loss = 3.15380872\n",
      "Iteration 33550, loss = 2.82881309\n",
      "Iteration 33551, loss = 3.12554551\n",
      "Iteration 33552, loss = 2.96887458\n",
      "Iteration 33553, loss = 2.77124877\n",
      "Iteration 33554, loss = 3.03625016\n",
      "Iteration 33555, loss = 4.27749938\n",
      "Iteration 33556, loss = 4.32279241\n",
      "Iteration 33557, loss = 4.08084232\n",
      "Iteration 33558, loss = 3.75145309\n",
      "Iteration 33559, loss = 3.62482632\n",
      "Iteration 33560, loss = 3.55965144\n",
      "Iteration 33561, loss = 3.14010911\n",
      "Iteration 33562, loss = 2.98916148\n",
      "Iteration 33563, loss = 2.92749273\n",
      "Iteration 33564, loss = 2.60722473\n",
      "Iteration 33565, loss = 2.72784598\n",
      "Iteration 33566, loss = 3.58429106\n",
      "Iteration 33567, loss = 3.05592699\n",
      "Iteration 33568, loss = 2.99632161\n",
      "Iteration 33569, loss = 2.84256167\n",
      "Iteration 33570, loss = 3.00604889\n",
      "Iteration 33571, loss = 2.78402005\n",
      "Iteration 33572, loss = 2.79864767\n",
      "Iteration 33573, loss = 2.65381665\n",
      "Iteration 33574, loss = 2.81228718\n",
      "Iteration 33575, loss = 3.25503661\n",
      "Iteration 33576, loss = 3.58247198\n",
      "Iteration 33577, loss = 3.10320407\n",
      "Iteration 33578, loss = 3.27938974\n",
      "Iteration 33579, loss = 3.20655902\n",
      "Iteration 33580, loss = 2.86806972\n",
      "Iteration 33581, loss = 2.98545480\n",
      "Iteration 33582, loss = 2.83237496\n",
      "Iteration 33583, loss = 2.90648368\n",
      "Iteration 33584, loss = 2.79944405\n",
      "Iteration 33585, loss = 2.98646561\n",
      "Iteration 33586, loss = 2.99477777\n",
      "Iteration 33587, loss = 3.07107101\n",
      "Iteration 33588, loss = 3.07463975\n",
      "Iteration 33589, loss = 3.19348545\n",
      "Iteration 33590, loss = 3.80150723\n",
      "Iteration 33591, loss = 3.27514659\n",
      "Iteration 33592, loss = 3.11167599\n",
      "Iteration 33593, loss = 2.72571263\n",
      "Iteration 33594, loss = 3.16783265\n",
      "Iteration 33595, loss = 2.82562355\n",
      "Iteration 33596, loss = 2.88690854\n",
      "Iteration 33597, loss = 3.24071389\n",
      "Iteration 33598, loss = 3.22208703\n",
      "Iteration 33599, loss = 3.02426510\n",
      "Iteration 33600, loss = 2.66753432\n",
      "Iteration 33601, loss = 2.87562008\n",
      "Iteration 33602, loss = 2.78023025\n",
      "Iteration 33603, loss = 2.77897669\n",
      "Iteration 33604, loss = 2.68157522\n",
      "Iteration 33605, loss = 2.58564170\n",
      "Iteration 33606, loss = 2.71033322\n",
      "Iteration 33607, loss = 3.64867905\n",
      "Iteration 33608, loss = 3.52840152\n",
      "Iteration 33609, loss = 3.13487654\n",
      "Iteration 33610, loss = 2.88466184\n",
      "Iteration 33611, loss = 2.84369928\n",
      "Iteration 33612, loss = 2.90960580\n",
      "Iteration 33613, loss = 3.52785902\n",
      "Iteration 33614, loss = 3.15795978\n",
      "Iteration 33615, loss = 3.42513856\n",
      "Iteration 33616, loss = 3.14694971\n",
      "Iteration 33617, loss = 3.06656070\n",
      "Iteration 33618, loss = 2.65684991\n",
      "Iteration 33619, loss = 2.77635221\n",
      "Iteration 33620, loss = 2.71440524\n",
      "Iteration 33621, loss = 2.86327970\n",
      "Iteration 33622, loss = 3.13806147\n",
      "Iteration 33623, loss = 2.66953449\n",
      "Iteration 33624, loss = 2.75807429\n",
      "Iteration 33625, loss = 2.83388129\n",
      "Iteration 33626, loss = 3.46843420\n",
      "Iteration 33627, loss = 3.62240798\n",
      "Iteration 33628, loss = 2.98253122\n",
      "Iteration 33629, loss = 2.70814152\n",
      "Iteration 33630, loss = 2.60394818\n",
      "Iteration 33631, loss = 3.28742159\n",
      "Iteration 33632, loss = 3.89837784\n",
      "Iteration 33633, loss = 4.80725224\n",
      "Iteration 33634, loss = 4.35158287\n",
      "Iteration 33635, loss = 4.01160967\n",
      "Iteration 33636, loss = 3.09747537\n",
      "Iteration 33637, loss = 3.20215389\n",
      "Iteration 33638, loss = 3.20546177\n",
      "Iteration 33639, loss = 3.04175687\n",
      "Iteration 33640, loss = 3.15878829\n",
      "Iteration 33641, loss = 3.13676732\n",
      "Iteration 33642, loss = 2.83566721\n",
      "Iteration 33643, loss = 2.84222362\n",
      "Iteration 33644, loss = 3.07484960\n",
      "Iteration 33645, loss = 2.57264165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 33646, loss = 2.81236748\n",
      "Iteration 33647, loss = 2.77524833\n",
      "Iteration 33648, loss = 2.59158456\n",
      "Iteration 33649, loss = 2.78810712\n",
      "Iteration 33650, loss = 3.00036254\n",
      "Iteration 33651, loss = 3.41276165\n",
      "Iteration 33652, loss = 2.71406475\n",
      "Iteration 33653, loss = 3.53692881\n",
      "Iteration 33654, loss = 3.04006385\n",
      "Iteration 33655, loss = 3.78509089\n",
      "Iteration 33656, loss = 3.67578821\n",
      "Iteration 33657, loss = 3.41997411\n",
      "Iteration 33658, loss = 2.83132747\n",
      "Iteration 33659, loss = 3.05659606\n",
      "Iteration 33660, loss = 3.09851179\n",
      "Iteration 33661, loss = 3.28128760\n",
      "Iteration 33662, loss = 3.13409701\n",
      "Iteration 33663, loss = 3.65087182\n",
      "Iteration 33664, loss = 3.96270701\n",
      "Iteration 33665, loss = 3.09316383\n",
      "Iteration 33666, loss = 2.93913066\n",
      "Iteration 33667, loss = 2.86862389\n",
      "Iteration 33668, loss = 2.77328573\n",
      "Iteration 33669, loss = 2.66420520\n",
      "Iteration 33670, loss = 2.82872015\n",
      "Iteration 33671, loss = 3.23661629\n",
      "Iteration 33672, loss = 3.20511546\n",
      "Iteration 33673, loss = 3.82975500\n",
      "Iteration 33674, loss = 5.14963796\n",
      "Iteration 33675, loss = 4.88636445\n",
      "Iteration 33676, loss = 6.59304346\n",
      "Iteration 33677, loss = 5.12545656\n",
      "Iteration 33678, loss = 4.48056993\n",
      "Iteration 33679, loss = 3.31003697\n",
      "Iteration 33680, loss = 3.37798213\n",
      "Iteration 33681, loss = 3.39250567\n",
      "Iteration 33682, loss = 3.96735995\n",
      "Iteration 33683, loss = 3.09276208\n",
      "Iteration 33684, loss = 3.06969520\n",
      "Iteration 33685, loss = 2.78977242\n",
      "Iteration 33686, loss = 2.68641472\n",
      "Iteration 33687, loss = 2.72218601\n",
      "Iteration 33688, loss = 2.87530012\n",
      "Iteration 33689, loss = 2.86289487\n",
      "Iteration 33690, loss = 2.83565459\n",
      "Iteration 33691, loss = 2.64904010\n",
      "Iteration 33692, loss = 2.71826660\n",
      "Iteration 33693, loss = 2.76823943\n",
      "Iteration 33694, loss = 3.08865673\n",
      "Iteration 33695, loss = 3.19550485\n",
      "Iteration 33696, loss = 2.80516416\n",
      "Iteration 33697, loss = 2.95088016\n",
      "Iteration 33698, loss = 3.86888115\n",
      "Iteration 33699, loss = 3.94543324\n",
      "Iteration 33700, loss = 3.38438495\n",
      "Iteration 33701, loss = 4.48131162\n",
      "Iteration 33702, loss = 3.33254111\n",
      "Iteration 33703, loss = 3.36832966\n",
      "Iteration 33704, loss = 3.23774531\n",
      "Iteration 33705, loss = 2.93661225\n",
      "Iteration 33706, loss = 2.93964322\n",
      "Iteration 33707, loss = 2.62263922\n",
      "Iteration 33708, loss = 2.70168714\n",
      "Iteration 33709, loss = 3.02248531\n",
      "Iteration 33710, loss = 2.95587393\n",
      "Iteration 33711, loss = 3.11573046\n",
      "Iteration 33712, loss = 3.13472867\n",
      "Iteration 33713, loss = 3.03462381\n",
      "Iteration 33714, loss = 2.87053156\n",
      "Iteration 33715, loss = 3.22721322\n",
      "Iteration 33716, loss = 3.40621551\n",
      "Iteration 33717, loss = 4.02658247\n",
      "Iteration 33718, loss = 3.93215426\n",
      "Iteration 33719, loss = 4.92754333\n",
      "Iteration 33720, loss = 5.36915981\n",
      "Iteration 33721, loss = 3.27846788\n",
      "Iteration 33722, loss = 4.01355952\n",
      "Iteration 33723, loss = 3.68230701\n",
      "Iteration 33724, loss = 3.06867840\n",
      "Iteration 33725, loss = 3.11630984\n",
      "Iteration 33726, loss = 3.60433828\n",
      "Iteration 33727, loss = 4.57942110\n",
      "Iteration 33728, loss = 3.64910992\n",
      "Iteration 33729, loss = 3.84093888\n",
      "Iteration 33730, loss = 3.63039376\n",
      "Iteration 33731, loss = 4.18087340\n",
      "Iteration 33732, loss = 4.55566121\n",
      "Iteration 33733, loss = 4.03258416\n",
      "Iteration 33734, loss = 3.81262973\n",
      "Iteration 33735, loss = 3.37168140\n",
      "Iteration 33736, loss = 2.74240721\n",
      "Iteration 33737, loss = 2.75060308\n",
      "Iteration 33738, loss = 2.78994640\n",
      "Iteration 33739, loss = 3.09573341\n",
      "Iteration 33740, loss = 3.20496995\n",
      "Iteration 33741, loss = 3.07952143\n",
      "Iteration 33742, loss = 3.14942547\n",
      "Iteration 33743, loss = 2.67904240\n",
      "Iteration 33744, loss = 3.20123452\n",
      "Iteration 33745, loss = 2.96657826\n",
      "Iteration 33746, loss = 2.92430370\n",
      "Iteration 33747, loss = 2.75971476\n",
      "Iteration 33748, loss = 2.75260642\n",
      "Iteration 33749, loss = 2.70230013\n",
      "Iteration 33750, loss = 2.90198612\n",
      "Iteration 33751, loss = 3.38835926\n",
      "Iteration 33752, loss = 4.01083171\n",
      "Iteration 33753, loss = 4.67366768\n",
      "Iteration 33754, loss = 5.05274942\n",
      "Iteration 33755, loss = 3.96731968\n",
      "Iteration 33756, loss = 3.89190224\n",
      "Iteration 33757, loss = 4.03879944\n",
      "Iteration 33758, loss = 4.50908851\n",
      "Iteration 33759, loss = 4.17075378\n",
      "Iteration 33760, loss = 3.06181665\n",
      "Iteration 33761, loss = 3.15200490\n",
      "Iteration 33762, loss = 3.30977473\n",
      "Iteration 33763, loss = 2.83684905\n",
      "Iteration 33764, loss = 2.90136177\n",
      "Iteration 33765, loss = 3.33126711\n",
      "Iteration 33766, loss = 2.86562140\n",
      "Iteration 33767, loss = 3.04225637\n",
      "Iteration 33768, loss = 2.99034597\n",
      "Iteration 33769, loss = 2.64154130\n",
      "Iteration 33770, loss = 2.88463084\n",
      "Iteration 33771, loss = 2.87914230\n",
      "Iteration 33772, loss = 3.41697413\n",
      "Iteration 33773, loss = 3.29098550\n",
      "Iteration 33774, loss = 3.16365047\n",
      "Iteration 33775, loss = 3.06482205\n",
      "Iteration 33776, loss = 3.15540210\n",
      "Iteration 33777, loss = 2.78656167\n",
      "Iteration 33778, loss = 2.51159134\n",
      "Iteration 33779, loss = 2.51681308\n",
      "Iteration 33780, loss = 2.60517525\n",
      "Iteration 33781, loss = 2.83709472\n",
      "Iteration 33782, loss = 2.90281437\n",
      "Iteration 33783, loss = 3.30677994\n",
      "Iteration 33784, loss = 2.79015013\n",
      "Iteration 33785, loss = 3.27296468\n",
      "Iteration 33786, loss = 3.55134466\n",
      "Iteration 33787, loss = 3.23076539\n",
      "Iteration 33788, loss = 3.68667889\n",
      "Iteration 33789, loss = 3.22004610\n",
      "Iteration 33790, loss = 3.54795522\n",
      "Iteration 33791, loss = 3.37135355\n",
      "Iteration 33792, loss = 3.47688902\n",
      "Iteration 33793, loss = 3.23768997\n",
      "Iteration 33794, loss = 3.37881759\n",
      "Iteration 33795, loss = 2.99926141\n",
      "Iteration 33796, loss = 3.63206753\n",
      "Iteration 33797, loss = 3.12549808\n",
      "Iteration 33798, loss = 2.94309498\n",
      "Iteration 33799, loss = 3.28044710\n",
      "Iteration 33800, loss = 3.75960651\n",
      "Iteration 33801, loss = 6.95554153\n",
      "Iteration 33802, loss = 5.11393604\n",
      "Iteration 33803, loss = 5.59466175\n",
      "Iteration 33804, loss = 4.51309658\n",
      "Iteration 33805, loss = 4.03747372\n",
      "Iteration 33806, loss = 3.42777145\n",
      "Iteration 33807, loss = 3.32356563\n",
      "Iteration 33808, loss = 2.84204326\n",
      "Iteration 33809, loss = 3.16953169\n",
      "Iteration 33810, loss = 3.54994471\n",
      "Iteration 33811, loss = 3.16943237\n",
      "Iteration 33812, loss = 3.36202336\n",
      "Iteration 33813, loss = 3.05699295\n",
      "Iteration 33814, loss = 3.12347107\n",
      "Iteration 33815, loss = 3.29535934\n",
      "Iteration 33816, loss = 2.74651943\n",
      "Iteration 33817, loss = 2.64527171\n",
      "Iteration 33818, loss = 2.83717573\n",
      "Iteration 33819, loss = 2.73931999\n",
      "Iteration 33820, loss = 2.70148462\n",
      "Iteration 33821, loss = 2.76930895\n",
      "Iteration 33822, loss = 3.06257676\n",
      "Iteration 33823, loss = 2.81018749\n",
      "Iteration 33824, loss = 2.67265541\n",
      "Iteration 33825, loss = 3.64861111\n",
      "Iteration 33826, loss = 3.04574235\n",
      "Iteration 33827, loss = 2.60879495\n",
      "Iteration 33828, loss = 3.00530560\n",
      "Iteration 33829, loss = 2.86894068\n",
      "Iteration 33830, loss = 2.61776375\n",
      "Iteration 33831, loss = 3.77083875\n",
      "Iteration 33832, loss = 2.67885123\n",
      "Iteration 33833, loss = 2.71459876\n",
      "Iteration 33834, loss = 2.90226682\n",
      "Iteration 33835, loss = 3.27924600\n",
      "Iteration 33836, loss = 3.16535509\n",
      "Iteration 33837, loss = 3.30612612\n",
      "Iteration 33838, loss = 3.33474381\n",
      "Iteration 33839, loss = 2.92847367\n",
      "Iteration 33840, loss = 3.33019452\n",
      "Iteration 33841, loss = 3.72989223\n",
      "Iteration 33842, loss = 3.25775660\n",
      "Iteration 33843, loss = 3.53551216\n",
      "Iteration 33844, loss = 3.40146420\n",
      "Iteration 33845, loss = 3.85068668\n",
      "Iteration 33846, loss = 3.78152478\n",
      "Iteration 33847, loss = 5.30941096\n",
      "Iteration 33848, loss = 4.90626381\n",
      "Iteration 33849, loss = 4.33546043\n",
      "Iteration 33850, loss = 3.46170646\n",
      "Iteration 33851, loss = 4.19480509\n",
      "Iteration 33852, loss = 3.52875408\n",
      "Iteration 33853, loss = 3.37196728\n",
      "Iteration 33854, loss = 3.34288072\n",
      "Iteration 33855, loss = 2.83974158\n",
      "Iteration 33856, loss = 2.92014448\n",
      "Iteration 33857, loss = 2.95766509\n",
      "Iteration 33858, loss = 3.31323962\n",
      "Iteration 33859, loss = 3.10217640\n",
      "Iteration 33860, loss = 3.18151507\n",
      "Iteration 33861, loss = 2.73152108\n",
      "Iteration 33862, loss = 2.58084867\n",
      "Iteration 33863, loss = 2.68001588\n",
      "Iteration 33864, loss = 3.05560520\n",
      "Iteration 33865, loss = 2.91330550\n",
      "Iteration 33866, loss = 3.22784171\n",
      "Iteration 33867, loss = 2.89514957\n",
      "Iteration 33868, loss = 2.70495059\n",
      "Iteration 33869, loss = 2.66740303\n",
      "Iteration 33870, loss = 2.78781844\n",
      "Iteration 33871, loss = 3.89072133\n",
      "Iteration 33872, loss = 3.19331101\n",
      "Iteration 33873, loss = 3.37102747\n",
      "Iteration 33874, loss = 3.62794314\n",
      "Iteration 33875, loss = 3.56773286\n",
      "Iteration 33876, loss = 4.52668546\n",
      "Iteration 33877, loss = 3.99268476\n",
      "Iteration 33878, loss = 4.81684952\n",
      "Iteration 33879, loss = 3.75886402\n",
      "Iteration 33880, loss = 3.21951449\n",
      "Iteration 33881, loss = 3.36716708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 33882, loss = 3.25911065\n",
      "Iteration 33883, loss = 3.92129175\n",
      "Iteration 33884, loss = 3.68589812\n",
      "Iteration 33885, loss = 3.27797151\n",
      "Iteration 33886, loss = 2.92632739\n",
      "Iteration 33887, loss = 3.04083323\n",
      "Iteration 33888, loss = 2.80879934\n",
      "Iteration 33889, loss = 2.65891474\n",
      "Iteration 33890, loss = 2.70073704\n",
      "Iteration 33891, loss = 2.82616328\n",
      "Iteration 33892, loss = 2.73630529\n",
      "Iteration 33893, loss = 3.13708814\n",
      "Iteration 33894, loss = 3.04196844\n",
      "Iteration 33895, loss = 3.52853708\n",
      "Iteration 33896, loss = 3.25124322\n",
      "Iteration 33897, loss = 2.86173524\n",
      "Iteration 33898, loss = 3.48215936\n",
      "Iteration 33899, loss = 3.42412683\n",
      "Iteration 33900, loss = 3.01274345\n",
      "Iteration 33901, loss = 2.94472371\n",
      "Iteration 33902, loss = 3.15880836\n",
      "Iteration 33903, loss = 2.87222733\n",
      "Iteration 33904, loss = 2.64315594\n",
      "Iteration 33905, loss = 3.42684677\n",
      "Iteration 33906, loss = 3.52200744\n",
      "Iteration 33907, loss = 3.93449193\n",
      "Iteration 33908, loss = 4.00098803\n",
      "Iteration 33909, loss = 4.11351610\n",
      "Iteration 33910, loss = 3.21293551\n",
      "Iteration 33911, loss = 3.02536319\n",
      "Iteration 33912, loss = 3.37625908\n",
      "Iteration 33913, loss = 2.78502812\n",
      "Iteration 33914, loss = 2.46318652\n",
      "Iteration 33915, loss = 3.04356156\n",
      "Iteration 33916, loss = 3.68147777\n",
      "Iteration 33917, loss = 3.34381890\n",
      "Iteration 33918, loss = 3.62652441\n",
      "Iteration 33919, loss = 3.36928686\n",
      "Iteration 33920, loss = 4.60642114\n",
      "Iteration 33921, loss = 3.86475104\n",
      "Iteration 33922, loss = 3.88992474\n",
      "Iteration 33923, loss = 3.64748299\n",
      "Iteration 33924, loss = 4.04009411\n",
      "Iteration 33925, loss = 3.64970163\n",
      "Iteration 33926, loss = 5.03007960\n",
      "Iteration 33927, loss = 4.67868390\n",
      "Iteration 33928, loss = 5.61570289\n",
      "Iteration 33929, loss = 4.54021195\n",
      "Iteration 33930, loss = 4.50472052\n",
      "Iteration 33931, loss = 3.66743660\n",
      "Iteration 33932, loss = 3.11576064\n",
      "Iteration 33933, loss = 3.41607342\n",
      "Iteration 33934, loss = 3.62910362\n",
      "Iteration 33935, loss = 3.16362953\n",
      "Iteration 33936, loss = 3.49259005\n",
      "Iteration 33937, loss = 3.35624928\n",
      "Iteration 33938, loss = 3.62672552\n",
      "Iteration 33939, loss = 3.84637896\n",
      "Iteration 33940, loss = 3.68419112\n",
      "Iteration 33941, loss = 5.23362250\n",
      "Iteration 33942, loss = 4.17494182\n",
      "Iteration 33943, loss = 4.08257644\n",
      "Iteration 33944, loss = 3.15902938\n",
      "Iteration 33945, loss = 3.11956424\n",
      "Iteration 33946, loss = 2.68834038\n",
      "Iteration 33947, loss = 2.68094027\n",
      "Iteration 33948, loss = 3.31269584\n",
      "Iteration 33949, loss = 3.14320898\n",
      "Iteration 33950, loss = 3.45707382\n",
      "Iteration 33951, loss = 3.31497993\n",
      "Iteration 33952, loss = 3.28278555\n",
      "Iteration 33953, loss = 3.32475575\n",
      "Iteration 33954, loss = 3.07960051\n",
      "Iteration 33955, loss = 2.83588175\n",
      "Iteration 33956, loss = 2.90994434\n",
      "Iteration 33957, loss = 2.89368521\n",
      "Iteration 33958, loss = 2.98982234\n",
      "Iteration 33959, loss = 3.02719640\n",
      "Iteration 33960, loss = 3.30007515\n",
      "Iteration 33961, loss = 3.18713999\n",
      "Iteration 33962, loss = 3.14783808\n",
      "Iteration 33963, loss = 3.01750540\n",
      "Iteration 33964, loss = 3.39509719\n",
      "Iteration 33965, loss = 3.28364294\n",
      "Iteration 33966, loss = 4.05792499\n",
      "Iteration 33967, loss = 3.15628107\n",
      "Iteration 33968, loss = 3.02215522\n",
      "Iteration 33969, loss = 2.57642534\n",
      "Iteration 33970, loss = 2.88533254\n",
      "Iteration 33971, loss = 3.15156632\n",
      "Iteration 33972, loss = 3.15414235\n",
      "Iteration 33973, loss = 3.41929748\n",
      "Iteration 33974, loss = 2.65842649\n",
      "Iteration 33975, loss = 2.62400061\n",
      "Iteration 33976, loss = 2.65110205\n",
      "Iteration 33977, loss = 2.85964664\n",
      "Iteration 33978, loss = 3.27959075\n",
      "Iteration 33979, loss = 3.11290377\n",
      "Iteration 33980, loss = 2.99370306\n",
      "Iteration 33981, loss = 2.91510554\n",
      "Iteration 33982, loss = 3.21772610\n",
      "Iteration 33983, loss = 3.64771107\n",
      "Iteration 33984, loss = 3.90530229\n",
      "Iteration 33985, loss = 4.34240780\n",
      "Iteration 33986, loss = 3.33132400\n",
      "Iteration 33987, loss = 2.68418679\n",
      "Iteration 33988, loss = 3.11901398\n",
      "Iteration 33989, loss = 3.63331700\n",
      "Iteration 33990, loss = 3.60361440\n",
      "Iteration 33991, loss = 4.01544323\n",
      "Iteration 33992, loss = 4.17562611\n",
      "Iteration 33993, loss = 3.87912788\n",
      "Iteration 33994, loss = 3.31817479\n",
      "Iteration 33995, loss = 3.56890618\n",
      "Iteration 33996, loss = 3.74672648\n",
      "Iteration 33997, loss = 3.01934933\n",
      "Iteration 33998, loss = 2.91246489\n",
      "Iteration 33999, loss = 2.75122393\n",
      "Iteration 34000, loss = 3.24890440\n",
      "Iteration 34001, loss = 2.88239471\n",
      "Iteration 34002, loss = 2.98377851\n",
      "Iteration 34003, loss = 4.08237656\n",
      "Iteration 34004, loss = 3.53450725\n",
      "Iteration 34005, loss = 3.84344925\n",
      "Iteration 34006, loss = 4.69122035\n",
      "Iteration 34007, loss = 3.63983608\n",
      "Iteration 34008, loss = 3.92450914\n",
      "Iteration 34009, loss = 4.27703132\n",
      "Iteration 34010, loss = 3.35456596\n",
      "Iteration 34011, loss = 4.24404581\n",
      "Iteration 34012, loss = 3.06174406\n",
      "Iteration 34013, loss = 2.60718369\n",
      "Iteration 34014, loss = 2.59134330\n",
      "Iteration 34015, loss = 2.78797023\n",
      "Iteration 34016, loss = 2.78267727\n",
      "Iteration 34017, loss = 2.91426009\n",
      "Iteration 34018, loss = 3.15807460\n",
      "Iteration 34019, loss = 3.22087851\n",
      "Iteration 34020, loss = 3.46326252\n",
      "Iteration 34021, loss = 3.17172900\n",
      "Iteration 34022, loss = 3.73201958\n",
      "Iteration 34023, loss = 3.40805262\n",
      "Iteration 34024, loss = 3.31048926\n",
      "Iteration 34025, loss = 3.03624960\n",
      "Iteration 34026, loss = 3.17759922\n",
      "Iteration 34027, loss = 3.23973496\n",
      "Iteration 34028, loss = 2.70989480\n",
      "Iteration 34029, loss = 2.85108651\n",
      "Iteration 34030, loss = 3.18275196\n",
      "Iteration 34031, loss = 3.93051732\n",
      "Iteration 34032, loss = 3.87627183\n",
      "Iteration 34033, loss = 3.87303471\n",
      "Iteration 34034, loss = 3.58278835\n",
      "Iteration 34035, loss = 3.13670136\n",
      "Iteration 34036, loss = 2.90813432\n",
      "Iteration 34037, loss = 3.15836511\n",
      "Iteration 34038, loss = 3.04461252\n",
      "Iteration 34039, loss = 3.05665207\n",
      "Iteration 34040, loss = 2.86111577\n",
      "Iteration 34041, loss = 2.75350041\n",
      "Iteration 34042, loss = 2.55117202\n",
      "Iteration 34043, loss = 2.56773396\n",
      "Iteration 34044, loss = 2.58791883\n",
      "Iteration 34045, loss = 3.16020090\n",
      "Iteration 34046, loss = 3.56174652\n",
      "Iteration 34047, loss = 2.90279574\n",
      "Iteration 34048, loss = 3.25251173\n",
      "Iteration 34049, loss = 3.52702568\n",
      "Iteration 34050, loss = 3.27458035\n",
      "Iteration 34051, loss = 3.34912898\n",
      "Iteration 34052, loss = 3.49959783\n",
      "Iteration 34053, loss = 3.48315208\n",
      "Iteration 34054, loss = 3.01226685\n",
      "Iteration 34055, loss = 2.81888851\n",
      "Iteration 34056, loss = 2.84682820\n",
      "Iteration 34057, loss = 2.58461185\n",
      "Iteration 34058, loss = 2.82469834\n",
      "Iteration 34059, loss = 3.07383770\n",
      "Iteration 34060, loss = 3.30757022\n",
      "Iteration 34061, loss = 2.94463813\n",
      "Iteration 34062, loss = 3.44066573\n",
      "Iteration 34063, loss = 3.14536863\n",
      "Iteration 34064, loss = 2.83715339\n",
      "Iteration 34065, loss = 2.49544351\n",
      "Iteration 34066, loss = 2.71788343\n",
      "Iteration 34067, loss = 2.87759777\n",
      "Iteration 34068, loss = 2.72520538\n",
      "Iteration 34069, loss = 3.00614026\n",
      "Iteration 34070, loss = 2.60955835\n",
      "Iteration 34071, loss = 2.68563887\n",
      "Iteration 34072, loss = 2.46341560\n",
      "Iteration 34073, loss = 2.55141951\n",
      "Iteration 34074, loss = 3.14291416\n",
      "Iteration 34075, loss = 3.57892307\n",
      "Iteration 34076, loss = 3.29041536\n",
      "Iteration 34077, loss = 4.13945815\n",
      "Iteration 34078, loss = 3.03800721\n",
      "Iteration 34079, loss = 3.70193994\n",
      "Iteration 34080, loss = 3.46691569\n",
      "Iteration 34081, loss = 3.19167160\n",
      "Iteration 34082, loss = 3.33166495\n",
      "Iteration 34083, loss = 2.91291179\n",
      "Iteration 34084, loss = 2.62910882\n",
      "Iteration 34085, loss = 3.22813466\n",
      "Iteration 34086, loss = 2.66589186\n",
      "Iteration 34087, loss = 2.95411736\n",
      "Iteration 34088, loss = 2.98286452\n",
      "Iteration 34089, loss = 3.33002617\n",
      "Iteration 34090, loss = 2.88572414\n",
      "Iteration 34091, loss = 2.69900473\n",
      "Iteration 34092, loss = 2.89376412\n",
      "Iteration 34093, loss = 4.80949396\n",
      "Iteration 34094, loss = 4.93546647\n",
      "Iteration 34095, loss = 4.13093912\n",
      "Iteration 34096, loss = 3.41039009\n",
      "Iteration 34097, loss = 3.25849817\n",
      "Iteration 34098, loss = 3.58143959\n",
      "Iteration 34099, loss = 2.62174319\n",
      "Iteration 34100, loss = 2.94585384\n",
      "Iteration 34101, loss = 3.03078677\n",
      "Iteration 34102, loss = 3.20053769\n",
      "Iteration 34103, loss = 3.45227774\n",
      "Iteration 34104, loss = 2.83708124\n",
      "Iteration 34105, loss = 3.36354943\n",
      "Iteration 34106, loss = 3.34352582\n",
      "Iteration 34107, loss = 3.35259191\n",
      "Iteration 34108, loss = 3.18723140\n",
      "Iteration 34109, loss = 2.94265891\n",
      "Iteration 34110, loss = 2.59486988\n",
      "Iteration 34111, loss = 2.50575601\n",
      "Iteration 34112, loss = 2.58351970\n",
      "Iteration 34113, loss = 3.17494301\n",
      "Iteration 34114, loss = 3.36096734\n",
      "Iteration 34115, loss = 3.60177205\n",
      "Iteration 34116, loss = 2.92656022\n",
      "Iteration 34117, loss = 2.83613998\n",
      "Iteration 34118, loss = 2.99807129\n",
      "Iteration 34119, loss = 3.04306554\n",
      "Iteration 34120, loss = 3.16731343\n",
      "Iteration 34121, loss = 2.92391942\n",
      "Iteration 34122, loss = 2.88207835\n",
      "Iteration 34123, loss = 3.57526384\n",
      "Iteration 34124, loss = 2.84337554\n",
      "Iteration 34125, loss = 3.06328289\n",
      "Iteration 34126, loss = 2.73731706\n",
      "Iteration 34127, loss = 2.88935140\n",
      "Iteration 34128, loss = 3.03194384\n",
      "Iteration 34129, loss = 3.34244005\n",
      "Iteration 34130, loss = 3.24102260\n",
      "Iteration 34131, loss = 3.08816790\n",
      "Iteration 34132, loss = 2.92014898\n",
      "Iteration 34133, loss = 3.34170042\n",
      "Iteration 34134, loss = 3.99953312\n",
      "Iteration 34135, loss = 4.06111934\n",
      "Iteration 34136, loss = 3.29875925\n",
      "Iteration 34137, loss = 3.82000314\n",
      "Iteration 34138, loss = 3.01985242\n",
      "Iteration 34139, loss = 2.68223003\n",
      "Iteration 34140, loss = 2.48984408\n",
      "Iteration 34141, loss = 2.61657585\n",
      "Iteration 34142, loss = 3.09528680\n",
      "Iteration 34143, loss = 3.26657525\n",
      "Iteration 34144, loss = 2.48347598\n",
      "Iteration 34145, loss = 2.97792112\n",
      "Iteration 34146, loss = 3.44486157\n",
      "Iteration 34147, loss = 3.05625738\n",
      "Iteration 34148, loss = 2.83871920\n",
      "Iteration 34149, loss = 3.11019357\n",
      "Iteration 34150, loss = 3.35819943\n",
      "Iteration 34151, loss = 2.94194598\n",
      "Iteration 34152, loss = 3.00108853\n",
      "Iteration 34153, loss = 3.17950547\n",
      "Iteration 34154, loss = 2.92284559\n",
      "Iteration 34155, loss = 2.66510618\n",
      "Iteration 34156, loss = 2.61323758\n",
      "Iteration 34157, loss = 3.40427326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 34158, loss = 3.89119991\n",
      "Iteration 34159, loss = 4.35791674\n",
      "Iteration 34160, loss = 4.92385397\n",
      "Iteration 34161, loss = 5.39298437\n",
      "Iteration 34162, loss = 4.71798082\n",
      "Iteration 34163, loss = 4.37211757\n",
      "Iteration 34164, loss = 3.87646268\n",
      "Iteration 34165, loss = 4.97525132\n",
      "Iteration 34166, loss = 4.81529053\n",
      "Iteration 34167, loss = 4.92562329\n",
      "Iteration 34168, loss = 4.35654513\n",
      "Iteration 34169, loss = 3.73105130\n",
      "Iteration 34170, loss = 3.11148000\n",
      "Iteration 34171, loss = 2.72442662\n",
      "Iteration 34172, loss = 2.77902493\n",
      "Iteration 34173, loss = 2.89104865\n",
      "Iteration 34174, loss = 3.46976442\n",
      "Iteration 34175, loss = 3.22336499\n",
      "Iteration 34176, loss = 3.29429842\n",
      "Iteration 34177, loss = 3.18607927\n",
      "Iteration 34178, loss = 3.87237558\n",
      "Iteration 34179, loss = 4.13541214\n",
      "Iteration 34180, loss = 3.62156821\n",
      "Iteration 34181, loss = 2.96074585\n",
      "Iteration 34182, loss = 2.71260746\n",
      "Iteration 34183, loss = 2.71807266\n",
      "Iteration 34184, loss = 2.71194997\n",
      "Iteration 34185, loss = 2.74253105\n",
      "Iteration 34186, loss = 2.71430296\n",
      "Iteration 34187, loss = 2.48347838\n",
      "Iteration 34188, loss = 2.78558855\n",
      "Iteration 34189, loss = 2.85043101\n",
      "Iteration 34190, loss = 2.87461993\n",
      "Iteration 34191, loss = 2.91113823\n",
      "Iteration 34192, loss = 3.08593599\n",
      "Iteration 34193, loss = 3.30837105\n",
      "Iteration 34194, loss = 3.25839237\n",
      "Iteration 34195, loss = 3.35021666\n",
      "Iteration 34196, loss = 2.75916264\n",
      "Iteration 34197, loss = 3.27651136\n",
      "Iteration 34198, loss = 4.27283413\n",
      "Iteration 34199, loss = 5.05653407\n",
      "Iteration 34200, loss = 3.52989054\n",
      "Iteration 34201, loss = 3.29941285\n",
      "Iteration 34202, loss = 3.28163333\n",
      "Iteration 34203, loss = 2.89834834\n",
      "Iteration 34204, loss = 2.71949803\n",
      "Iteration 34205, loss = 2.77927712\n",
      "Iteration 34206, loss = 2.84586475\n",
      "Iteration 34207, loss = 3.19618700\n",
      "Iteration 34208, loss = 3.10796957\n",
      "Iteration 34209, loss = 3.99039551\n",
      "Iteration 34210, loss = 4.04702770\n",
      "Iteration 34211, loss = 3.52695981\n",
      "Iteration 34212, loss = 3.34005552\n",
      "Iteration 34213, loss = 2.92335088\n",
      "Iteration 34214, loss = 3.20816962\n",
      "Iteration 34215, loss = 3.13938592\n",
      "Iteration 34216, loss = 3.11891221\n",
      "Iteration 34217, loss = 3.66599513\n",
      "Iteration 34218, loss = 3.90512384\n",
      "Iteration 34219, loss = 3.30277248\n",
      "Iteration 34220, loss = 3.01131261\n",
      "Iteration 34221, loss = 3.16908860\n",
      "Iteration 34222, loss = 3.48393488\n",
      "Iteration 34223, loss = 3.47366330\n",
      "Iteration 34224, loss = 2.99848205\n",
      "Iteration 34225, loss = 2.74526141\n",
      "Iteration 34226, loss = 2.94255210\n",
      "Iteration 34227, loss = 2.61843215\n",
      "Iteration 34228, loss = 2.42572645\n",
      "Iteration 34229, loss = 2.43238475\n",
      "Iteration 34230, loss = 2.60904787\n",
      "Iteration 34231, loss = 2.55339798\n",
      "Iteration 34232, loss = 2.45866124\n",
      "Iteration 34233, loss = 2.60117417\n",
      "Iteration 34234, loss = 2.71981251\n",
      "Iteration 34235, loss = 2.47593652\n",
      "Iteration 34236, loss = 2.43745230\n",
      "Iteration 34237, loss = 2.71902540\n",
      "Iteration 34238, loss = 3.15437912\n",
      "Iteration 34239, loss = 2.84608986\n",
      "Iteration 34240, loss = 2.77361540\n",
      "Iteration 34241, loss = 2.84340396\n",
      "Iteration 34242, loss = 2.84557460\n",
      "Iteration 34243, loss = 3.01421494\n",
      "Iteration 34244, loss = 2.62070896\n",
      "Iteration 34245, loss = 2.66897971\n",
      "Iteration 34246, loss = 2.69764940\n",
      "Iteration 34247, loss = 2.61459313\n",
      "Iteration 34248, loss = 2.67194529\n",
      "Iteration 34249, loss = 2.58115167\n",
      "Iteration 34250, loss = 2.83958227\n",
      "Iteration 34251, loss = 3.03483603\n",
      "Iteration 34252, loss = 2.83696572\n",
      "Iteration 34253, loss = 2.84225205\n",
      "Iteration 34254, loss = 3.45004244\n",
      "Iteration 34255, loss = 3.76597442\n",
      "Iteration 34256, loss = 2.99748907\n",
      "Iteration 34257, loss = 2.78965023\n",
      "Iteration 34258, loss = 3.75408515\n",
      "Iteration 34259, loss = 2.79649658\n",
      "Iteration 34260, loss = 2.74405573\n",
      "Iteration 34261, loss = 3.13709486\n",
      "Iteration 34262, loss = 3.35504893\n",
      "Iteration 34263, loss = 3.31300012\n",
      "Iteration 34264, loss = 3.15821666\n",
      "Iteration 34265, loss = 3.51798774\n",
      "Iteration 34266, loss = 2.92881852\n",
      "Iteration 34267, loss = 2.89966570\n",
      "Iteration 34268, loss = 3.30639935\n",
      "Iteration 34269, loss = 2.89934767\n",
      "Iteration 34270, loss = 2.90802762\n",
      "Iteration 34271, loss = 3.66939725\n",
      "Iteration 34272, loss = 3.01650861\n",
      "Iteration 34273, loss = 4.09674934\n",
      "Iteration 34274, loss = 3.79857855\n",
      "Iteration 34275, loss = 5.17479937\n",
      "Iteration 34276, loss = 4.25525317\n",
      "Iteration 34277, loss = 3.64279081\n",
      "Iteration 34278, loss = 3.41497258\n",
      "Iteration 34279, loss = 2.96083498\n",
      "Iteration 34280, loss = 3.28836067\n",
      "Iteration 34281, loss = 3.63844502\n",
      "Iteration 34282, loss = 3.64550718\n",
      "Iteration 34283, loss = 3.59979476\n",
      "Iteration 34284, loss = 3.24867470\n",
      "Iteration 34285, loss = 3.13981910\n",
      "Iteration 34286, loss = 2.85642250\n",
      "Iteration 34287, loss = 3.25038366\n",
      "Iteration 34288, loss = 3.12075267\n",
      "Iteration 34289, loss = 3.17195239\n",
      "Iteration 34290, loss = 3.43717589\n",
      "Iteration 34291, loss = 3.22562236\n",
      "Iteration 34292, loss = 3.32442925\n",
      "Iteration 34293, loss = 5.07821424\n",
      "Iteration 34294, loss = 4.73774839\n",
      "Iteration 34295, loss = 6.44370685\n",
      "Iteration 34296, loss = 5.47383746\n",
      "Iteration 34297, loss = 5.50558690\n",
      "Iteration 34298, loss = 5.16801916\n",
      "Iteration 34299, loss = 3.99242156\n",
      "Iteration 34300, loss = 3.63428813\n",
      "Iteration 34301, loss = 3.62644889\n",
      "Iteration 34302, loss = 3.55867480\n",
      "Iteration 34303, loss = 2.76884812\n",
      "Iteration 34304, loss = 2.69924657\n",
      "Iteration 34305, loss = 2.86804444\n",
      "Iteration 34306, loss = 3.05875478\n",
      "Iteration 34307, loss = 3.51469741\n",
      "Iteration 34308, loss = 3.34655888\n",
      "Iteration 34309, loss = 2.99234520\n",
      "Iteration 34310, loss = 2.80840889\n",
      "Iteration 34311, loss = 3.34903095\n",
      "Iteration 34312, loss = 3.47164619\n",
      "Iteration 34313, loss = 3.44906789\n",
      "Iteration 34314, loss = 3.56043583\n",
      "Iteration 34315, loss = 2.56635766\n",
      "Iteration 34316, loss = 3.14870793\n",
      "Iteration 34317, loss = 2.62756571\n",
      "Iteration 34318, loss = 3.00216942\n",
      "Iteration 34319, loss = 2.75984075\n",
      "Iteration 34320, loss = 2.61028105\n",
      "Iteration 34321, loss = 2.93298235\n",
      "Iteration 34322, loss = 2.59694970\n",
      "Iteration 34323, loss = 2.66161192\n",
      "Iteration 34324, loss = 2.79889237\n",
      "Iteration 34325, loss = 2.50300181\n",
      "Iteration 34326, loss = 3.36639995\n",
      "Iteration 34327, loss = 4.18952641\n",
      "Iteration 34328, loss = 4.55490616\n",
      "Iteration 34329, loss = 4.40240119\n",
      "Iteration 34330, loss = 3.85736733\n",
      "Iteration 34331, loss = 3.96665853\n",
      "Iteration 34332, loss = 3.74196629\n",
      "Iteration 34333, loss = 3.47786737\n",
      "Iteration 34334, loss = 3.55342277\n",
      "Iteration 34335, loss = 2.66649477\n",
      "Iteration 34336, loss = 2.83546731\n",
      "Iteration 34337, loss = 2.76720109\n",
      "Iteration 34338, loss = 2.91276529\n",
      "Iteration 34339, loss = 2.38767980\n",
      "Iteration 34340, loss = 2.81542916\n",
      "Iteration 34341, loss = 3.07109351\n",
      "Iteration 34342, loss = 2.92924031\n",
      "Iteration 34343, loss = 3.31187258\n",
      "Iteration 34344, loss = 2.80784915\n",
      "Iteration 34345, loss = 3.18742101\n",
      "Iteration 34346, loss = 3.12282264\n",
      "Iteration 34347, loss = 3.56438879\n",
      "Iteration 34348, loss = 3.44952112\n",
      "Iteration 34349, loss = 2.83545414\n",
      "Iteration 34350, loss = 2.89844401\n",
      "Iteration 34351, loss = 3.65354618\n",
      "Iteration 34352, loss = 3.42160100\n",
      "Iteration 34353, loss = 3.64069956\n",
      "Iteration 34354, loss = 3.62356915\n",
      "Iteration 34355, loss = 3.55048524\n",
      "Iteration 34356, loss = 2.90320906\n",
      "Iteration 34357, loss = 2.84669247\n",
      "Iteration 34358, loss = 3.23998860\n",
      "Iteration 34359, loss = 3.85821615\n",
      "Iteration 34360, loss = 3.44132185\n",
      "Iteration 34361, loss = 3.41070924\n",
      "Iteration 34362, loss = 3.10212461\n",
      "Iteration 34363, loss = 2.88148537\n",
      "Iteration 34364, loss = 2.72994786\n",
      "Iteration 34365, loss = 2.78301804\n",
      "Iteration 34366, loss = 3.61794986\n",
      "Iteration 34367, loss = 5.26199477\n",
      "Iteration 34368, loss = 4.56985619\n",
      "Iteration 34369, loss = 4.28978556\n",
      "Iteration 34370, loss = 3.23205879\n",
      "Iteration 34371, loss = 3.50691606\n",
      "Iteration 34372, loss = 3.92701619\n",
      "Iteration 34373, loss = 4.07862060\n",
      "Iteration 34374, loss = 2.87111553\n",
      "Iteration 34375, loss = 3.61144477\n",
      "Iteration 34376, loss = 4.27006997\n",
      "Iteration 34377, loss = 3.30112806\n",
      "Iteration 34378, loss = 2.65866146\n",
      "Iteration 34379, loss = 2.91596132\n",
      "Iteration 34380, loss = 3.62451526\n",
      "Iteration 34381, loss = 2.72979625\n",
      "Iteration 34382, loss = 2.80783002\n",
      "Iteration 34383, loss = 2.60918407\n",
      "Iteration 34384, loss = 2.95278090\n",
      "Iteration 34385, loss = 3.57677462\n",
      "Iteration 34386, loss = 3.80364391\n",
      "Iteration 34387, loss = 3.90769414\n",
      "Iteration 34388, loss = 2.87277108\n",
      "Iteration 34389, loss = 2.84154946\n",
      "Iteration 34390, loss = 2.88942214\n",
      "Iteration 34391, loss = 2.70963272\n",
      "Iteration 34392, loss = 2.53745473\n",
      "Iteration 34393, loss = 2.69832497\n",
      "Iteration 34394, loss = 2.74172328\n",
      "Iteration 34395, loss = 2.94121913\n",
      "Iteration 34396, loss = 2.88181695\n",
      "Iteration 34397, loss = 3.09605871\n",
      "Iteration 34398, loss = 3.20347091\n",
      "Iteration 34399, loss = 2.86714564\n",
      "Iteration 34400, loss = 2.89413451\n",
      "Iteration 34401, loss = 3.08981368\n",
      "Iteration 34402, loss = 3.77242088\n",
      "Iteration 34403, loss = 4.32484641\n",
      "Iteration 34404, loss = 4.11604170\n",
      "Iteration 34405, loss = 3.68659712\n",
      "Iteration 34406, loss = 3.35245330\n",
      "Iteration 34407, loss = 2.78774634\n",
      "Iteration 34408, loss = 3.01458993\n",
      "Iteration 34409, loss = 2.65610412\n",
      "Iteration 34410, loss = 2.75133841\n",
      "Iteration 34411, loss = 2.67278222\n",
      "Iteration 34412, loss = 2.58258093\n",
      "Iteration 34413, loss = 2.91866010\n",
      "Iteration 34414, loss = 3.24933544\n",
      "Iteration 34415, loss = 2.68930546\n",
      "Iteration 34416, loss = 2.87967264\n",
      "Iteration 34417, loss = 3.19656831\n",
      "Iteration 34418, loss = 3.19146851\n",
      "Iteration 34419, loss = 2.94843558\n",
      "Iteration 34420, loss = 3.04857800\n",
      "Iteration 34421, loss = 3.16262775\n",
      "Iteration 34422, loss = 2.81786851\n",
      "Iteration 34423, loss = 3.75352322\n",
      "Iteration 34424, loss = 3.37853705\n",
      "Iteration 34425, loss = 3.14171001\n",
      "Iteration 34426, loss = 3.16648831\n",
      "Iteration 34427, loss = 4.32101768\n",
      "Iteration 34428, loss = 3.81909636\n",
      "Iteration 34429, loss = 4.06897252\n",
      "Iteration 34430, loss = 4.27564704\n",
      "Iteration 34431, loss = 3.91799635\n",
      "Iteration 34432, loss = 3.76835163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 34433, loss = 3.86992441\n",
      "Iteration 34434, loss = 3.21565392\n",
      "Iteration 34435, loss = 3.21360445\n",
      "Iteration 34436, loss = 3.67501258\n",
      "Iteration 34437, loss = 3.26271103\n",
      "Iteration 34438, loss = 2.81813493\n",
      "Iteration 34439, loss = 3.15638309\n",
      "Iteration 34440, loss = 3.08083737\n",
      "Iteration 34441, loss = 3.78337323\n",
      "Iteration 34442, loss = 3.12131237\n",
      "Iteration 34443, loss = 3.42621496\n",
      "Iteration 34444, loss = 2.58676298\n",
      "Iteration 34445, loss = 2.72339493\n",
      "Iteration 34446, loss = 3.18510570\n",
      "Iteration 34447, loss = 3.10131236\n",
      "Iteration 34448, loss = 3.15373894\n",
      "Iteration 34449, loss = 3.34145374\n",
      "Iteration 34450, loss = 2.77755635\n",
      "Iteration 34451, loss = 2.69328511\n",
      "Iteration 34452, loss = 3.12692159\n",
      "Iteration 34453, loss = 3.70712888\n",
      "Iteration 34454, loss = 2.88216755\n",
      "Iteration 34455, loss = 3.08616890\n",
      "Iteration 34456, loss = 2.72998852\n",
      "Iteration 34457, loss = 2.93529484\n",
      "Iteration 34458, loss = 2.82103963\n",
      "Iteration 34459, loss = 2.99236744\n",
      "Iteration 34460, loss = 3.25046389\n",
      "Iteration 34461, loss = 2.67361131\n",
      "Iteration 34462, loss = 2.60672902\n",
      "Iteration 34463, loss = 3.89965456\n",
      "Iteration 34464, loss = 3.32458786\n",
      "Iteration 34465, loss = 3.17089741\n",
      "Iteration 34466, loss = 3.64692089\n",
      "Iteration 34467, loss = 2.99231624\n",
      "Iteration 34468, loss = 3.40863257\n",
      "Iteration 34469, loss = 3.27588980\n",
      "Iteration 34470, loss = 3.03411704\n",
      "Iteration 34471, loss = 3.18815756\n",
      "Iteration 34472, loss = 3.31778385\n",
      "Iteration 34473, loss = 3.38042231\n",
      "Iteration 34474, loss = 3.11770582\n",
      "Iteration 34475, loss = 3.23881535\n",
      "Iteration 34476, loss = 3.28550884\n",
      "Iteration 34477, loss = 3.22823777\n",
      "Iteration 34478, loss = 3.32800009\n",
      "Iteration 34479, loss = 3.67064986\n",
      "Iteration 34480, loss = 4.55439558\n",
      "Iteration 34481, loss = 5.53196822\n",
      "Iteration 34482, loss = 4.36416121\n",
      "Iteration 34483, loss = 4.98917618\n",
      "Iteration 34484, loss = 3.85892595\n",
      "Iteration 34485, loss = 3.44425563\n",
      "Iteration 34486, loss = 3.54891733\n",
      "Iteration 34487, loss = 3.20392026\n",
      "Iteration 34488, loss = 2.97942085\n",
      "Iteration 34489, loss = 2.73910581\n",
      "Iteration 34490, loss = 2.58198394\n",
      "Iteration 34491, loss = 3.04148252\n",
      "Iteration 34492, loss = 3.01992662\n",
      "Iteration 34493, loss = 2.99696893\n",
      "Iteration 34494, loss = 2.81075107\n",
      "Iteration 34495, loss = 2.95259034\n",
      "Iteration 34496, loss = 3.18821182\n",
      "Iteration 34497, loss = 3.02225029\n",
      "Iteration 34498, loss = 4.06115096\n",
      "Iteration 34499, loss = 6.99642784\n",
      "Iteration 34500, loss = 6.93621874\n",
      "Iteration 34501, loss = 6.93871942\n",
      "Iteration 34502, loss = 5.58267425\n",
      "Iteration 34503, loss = 4.33709804\n",
      "Iteration 34504, loss = 4.13148062\n",
      "Iteration 34505, loss = 4.70330065\n",
      "Iteration 34506, loss = 3.69814747\n",
      "Iteration 34507, loss = 4.00873942\n",
      "Iteration 34508, loss = 3.82695336\n",
      "Iteration 34509, loss = 3.40609135\n",
      "Iteration 34510, loss = 4.32337034\n",
      "Iteration 34511, loss = 3.87445374\n",
      "Iteration 34512, loss = 3.67399037\n",
      "Iteration 34513, loss = 2.77526748\n",
      "Iteration 34514, loss = 2.58447164\n",
      "Iteration 34515, loss = 2.90606977\n",
      "Iteration 34516, loss = 2.59280157\n",
      "Iteration 34517, loss = 2.66712810\n",
      "Iteration 34518, loss = 2.93359347\n",
      "Iteration 34519, loss = 2.70898170\n",
      "Iteration 34520, loss = 2.83987963\n",
      "Iteration 34521, loss = 2.48696204\n",
      "Iteration 34522, loss = 2.44925167\n",
      "Iteration 34523, loss = 2.49705195\n",
      "Iteration 34524, loss = 2.68030359\n",
      "Iteration 34525, loss = 2.97636839\n",
      "Iteration 34526, loss = 3.65791290\n",
      "Iteration 34527, loss = 3.87087768\n",
      "Iteration 34528, loss = 4.36572592\n",
      "Iteration 34529, loss = 4.77336780\n",
      "Iteration 34530, loss = 3.45415909\n",
      "Iteration 34531, loss = 3.44705472\n",
      "Iteration 34532, loss = 3.24137799\n",
      "Iteration 34533, loss = 3.19315417\n",
      "Iteration 34534, loss = 2.70984735\n",
      "Iteration 34535, loss = 2.85765604\n",
      "Iteration 34536, loss = 3.17015207\n",
      "Iteration 34537, loss = 3.08642928\n",
      "Iteration 34538, loss = 3.86090263\n",
      "Iteration 34539, loss = 3.83202370\n",
      "Iteration 34540, loss = 3.32813825\n",
      "Iteration 34541, loss = 2.83330752\n",
      "Iteration 34542, loss = 2.65417054\n",
      "Iteration 34543, loss = 2.68670032\n",
      "Iteration 34544, loss = 3.94830776\n",
      "Iteration 34545, loss = 4.07952359\n",
      "Iteration 34546, loss = 3.80714122\n",
      "Iteration 34547, loss = 3.55820389\n",
      "Iteration 34548, loss = 3.49770390\n",
      "Iteration 34549, loss = 3.07714546\n",
      "Iteration 34550, loss = 2.90433097\n",
      "Iteration 34551, loss = 2.53728914\n",
      "Iteration 34552, loss = 2.48137913\n",
      "Iteration 34553, loss = 2.85526676\n",
      "Iteration 34554, loss = 3.35297009\n",
      "Iteration 34555, loss = 5.35787381\n",
      "Iteration 34556, loss = 4.92184486\n",
      "Iteration 34557, loss = 3.22190772\n",
      "Iteration 34558, loss = 3.00247218\n",
      "Iteration 34559, loss = 2.90229892\n",
      "Iteration 34560, loss = 2.70006225\n",
      "Iteration 34561, loss = 2.54387628\n",
      "Iteration 34562, loss = 2.71067489\n",
      "Iteration 34563, loss = 2.60964346\n",
      "Iteration 34564, loss = 2.98210787\n",
      "Iteration 34565, loss = 2.83674266\n",
      "Iteration 34566, loss = 3.37013641\n",
      "Iteration 34567, loss = 3.35660171\n",
      "Iteration 34568, loss = 2.97269414\n",
      "Iteration 34569, loss = 2.92017071\n",
      "Iteration 34570, loss = 2.72367574\n",
      "Iteration 34571, loss = 2.78577314\n",
      "Iteration 34572, loss = 3.50569815\n",
      "Iteration 34573, loss = 3.17301939\n",
      "Iteration 34574, loss = 2.85681650\n",
      "Iteration 34575, loss = 2.72881438\n",
      "Iteration 34576, loss = 3.18623015\n",
      "Iteration 34577, loss = 2.80829220\n",
      "Iteration 34578, loss = 3.41220914\n",
      "Iteration 34579, loss = 3.72740237\n",
      "Iteration 34580, loss = 2.88398814\n",
      "Iteration 34581, loss = 3.29143885\n",
      "Iteration 34582, loss = 3.04489632\n",
      "Iteration 34583, loss = 3.04474140\n",
      "Iteration 34584, loss = 3.07685681\n",
      "Iteration 34585, loss = 2.81003526\n",
      "Iteration 34586, loss = 3.18804329\n",
      "Iteration 34587, loss = 3.14732372\n",
      "Iteration 34588, loss = 2.97719013\n",
      "Iteration 34589, loss = 2.73957675\n",
      "Iteration 34590, loss = 3.00815987\n",
      "Iteration 34591, loss = 3.33736051\n",
      "Iteration 34592, loss = 3.01218754\n",
      "Iteration 34593, loss = 3.09020919\n",
      "Iteration 34594, loss = 3.11267279\n",
      "Iteration 34595, loss = 3.38142693\n",
      "Iteration 34596, loss = 3.60547610\n",
      "Iteration 34597, loss = 3.04001920\n",
      "Iteration 34598, loss = 3.26068845\n",
      "Iteration 34599, loss = 3.15142505\n",
      "Iteration 34600, loss = 3.59121040\n",
      "Iteration 34601, loss = 3.16162045\n",
      "Iteration 34602, loss = 3.18317084\n",
      "Iteration 34603, loss = 2.70016705\n",
      "Iteration 34604, loss = 2.75352845\n",
      "Iteration 34605, loss = 2.58889452\n",
      "Iteration 34606, loss = 2.73835811\n",
      "Iteration 34607, loss = 2.62213486\n",
      "Iteration 34608, loss = 2.64111250\n",
      "Iteration 34609, loss = 2.57097315\n",
      "Iteration 34610, loss = 2.75801749\n",
      "Iteration 34611, loss = 2.60965121\n",
      "Iteration 34612, loss = 2.86215864\n",
      "Iteration 34613, loss = 3.24327486\n",
      "Iteration 34614, loss = 2.78254049\n",
      "Iteration 34615, loss = 2.92908418\n",
      "Iteration 34616, loss = 3.32944718\n",
      "Iteration 34617, loss = 2.88630214\n",
      "Iteration 34618, loss = 2.78832949\n",
      "Iteration 34619, loss = 3.89401839\n",
      "Iteration 34620, loss = 3.27592430\n",
      "Iteration 34621, loss = 3.24510558\n",
      "Iteration 34622, loss = 3.08894852\n",
      "Iteration 34623, loss = 3.15793049\n",
      "Iteration 34624, loss = 2.48210724\n",
      "Iteration 34625, loss = 2.89023964\n",
      "Iteration 34626, loss = 2.90294776\n",
      "Iteration 34627, loss = 2.82199842\n",
      "Iteration 34628, loss = 3.27181481\n",
      "Iteration 34629, loss = 2.90177834\n",
      "Iteration 34630, loss = 3.09299076\n",
      "Iteration 34631, loss = 3.43934931\n",
      "Iteration 34632, loss = 4.03826947\n",
      "Iteration 34633, loss = 3.41209851\n",
      "Iteration 34634, loss = 4.05268938\n",
      "Iteration 34635, loss = 3.42619380\n",
      "Iteration 34636, loss = 3.57521798\n",
      "Iteration 34637, loss = 3.13233903\n",
      "Iteration 34638, loss = 2.83993658\n",
      "Iteration 34639, loss = 2.71272929\n",
      "Iteration 34640, loss = 2.66299534\n",
      "Iteration 34641, loss = 2.65018552\n",
      "Iteration 34642, loss = 2.58495134\n",
      "Iteration 34643, loss = 2.77630005\n",
      "Iteration 34644, loss = 2.77656281\n",
      "Iteration 34645, loss = 3.02982363\n",
      "Iteration 34646, loss = 3.21292183\n",
      "Iteration 34647, loss = 2.82002199\n",
      "Iteration 34648, loss = 2.70756191\n",
      "Iteration 34649, loss = 2.59610573\n",
      "Iteration 34650, loss = 2.78288075\n",
      "Iteration 34651, loss = 2.68078554\n",
      "Iteration 34652, loss = 2.59249575\n",
      "Iteration 34653, loss = 2.74484849\n",
      "Iteration 34654, loss = 3.12445718\n",
      "Iteration 34655, loss = 2.62910208\n",
      "Iteration 34656, loss = 3.06661465\n",
      "Iteration 34657, loss = 4.12453776\n",
      "Iteration 34658, loss = 3.27624315\n",
      "Iteration 34659, loss = 3.11936657\n",
      "Iteration 34660, loss = 3.15867462\n",
      "Iteration 34661, loss = 3.25179059\n",
      "Iteration 34662, loss = 3.74036378\n",
      "Iteration 34663, loss = 3.64667040\n",
      "Iteration 34664, loss = 2.99594519\n",
      "Iteration 34665, loss = 3.24130551\n",
      "Iteration 34666, loss = 3.39025586\n",
      "Iteration 34667, loss = 2.98116037\n",
      "Iteration 34668, loss = 3.31647549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 34669, loss = 2.65002834\n",
      "Iteration 34670, loss = 2.66974408\n",
      "Iteration 34671, loss = 2.74419124\n",
      "Iteration 34672, loss = 2.96110129\n",
      "Iteration 34673, loss = 2.83177112\n",
      "Iteration 34674, loss = 3.29917535\n",
      "Iteration 34675, loss = 3.28612407\n",
      "Iteration 34676, loss = 3.18172618\n",
      "Iteration 34677, loss = 2.77076577\n",
      "Iteration 34678, loss = 2.67149747\n",
      "Iteration 34679, loss = 2.94570414\n",
      "Iteration 34680, loss = 3.19034484\n",
      "Iteration 34681, loss = 3.55686759\n",
      "Iteration 34682, loss = 3.72892198\n",
      "Iteration 34683, loss = 3.08757492\n",
      "Iteration 34684, loss = 3.03213322\n",
      "Iteration 34685, loss = 2.82782959\n",
      "Iteration 34686, loss = 2.82616454\n",
      "Iteration 34687, loss = 2.79869079\n",
      "Iteration 34688, loss = 2.67935410\n",
      "Iteration 34689, loss = 2.81621259\n",
      "Iteration 34690, loss = 2.75835125\n",
      "Iteration 34691, loss = 2.65415062\n",
      "Iteration 34692, loss = 2.77380251\n",
      "Iteration 34693, loss = 2.55651928\n",
      "Iteration 34694, loss = 2.71746297\n",
      "Iteration 34695, loss = 2.60763913\n",
      "Iteration 34696, loss = 2.73146093\n",
      "Iteration 34697, loss = 3.18857775\n",
      "Iteration 34698, loss = 2.70751511\n",
      "Iteration 34699, loss = 3.07328658\n",
      "Iteration 34700, loss = 3.13905877\n",
      "Iteration 34701, loss = 3.38187517\n",
      "Iteration 34702, loss = 3.52952504\n",
      "Iteration 34703, loss = 2.88805553\n",
      "Iteration 34704, loss = 2.76441993\n",
      "Iteration 34705, loss = 2.80460096\n",
      "Iteration 34706, loss = 3.10629962\n",
      "Iteration 34707, loss = 3.07590109\n",
      "Iteration 34708, loss = 3.34096075\n",
      "Iteration 34709, loss = 3.47329209\n",
      "Iteration 34710, loss = 3.48552319\n",
      "Iteration 34711, loss = 3.57548615\n",
      "Iteration 34712, loss = 3.47342529\n",
      "Iteration 34713, loss = 3.19339344\n",
      "Iteration 34714, loss = 2.95920833\n",
      "Iteration 34715, loss = 3.17970953\n",
      "Iteration 34716, loss = 3.35793016\n",
      "Iteration 34717, loss = 2.79476209\n",
      "Iteration 34718, loss = 2.96445994\n",
      "Iteration 34719, loss = 3.04322594\n",
      "Iteration 34720, loss = 3.39501269\n",
      "Iteration 34721, loss = 3.47289645\n",
      "Iteration 34722, loss = 4.92209596\n",
      "Iteration 34723, loss = 3.71013867\n",
      "Iteration 34724, loss = 3.95828974\n",
      "Iteration 34725, loss = 3.32476972\n",
      "Iteration 34726, loss = 3.73213072\n",
      "Iteration 34727, loss = 3.86952988\n",
      "Iteration 34728, loss = 4.05850493\n",
      "Iteration 34729, loss = 2.70919812\n",
      "Iteration 34730, loss = 2.89506484\n",
      "Iteration 34731, loss = 3.34059204\n",
      "Iteration 34732, loss = 4.05395892\n",
      "Iteration 34733, loss = 3.86231451\n",
      "Iteration 34734, loss = 4.04933370\n",
      "Iteration 34735, loss = 4.30678684\n",
      "Iteration 34736, loss = 2.91250068\n",
      "Iteration 34737, loss = 2.85330606\n",
      "Iteration 34738, loss = 2.96478001\n",
      "Iteration 34739, loss = 2.90240454\n",
      "Iteration 34740, loss = 3.76868600\n",
      "Iteration 34741, loss = 3.70224181\n",
      "Iteration 34742, loss = 3.62307675\n",
      "Iteration 34743, loss = 2.96716428\n",
      "Iteration 34744, loss = 3.39737074\n",
      "Iteration 34745, loss = 3.30818104\n",
      "Iteration 34746, loss = 3.28588819\n",
      "Iteration 34747, loss = 3.10112057\n",
      "Iteration 34748, loss = 2.82421293\n",
      "Iteration 34749, loss = 3.04243226\n",
      "Iteration 34750, loss = 3.98061384\n",
      "Iteration 34751, loss = 4.42625581\n",
      "Iteration 34752, loss = 2.98169695\n",
      "Iteration 34753, loss = 3.07346854\n",
      "Iteration 34754, loss = 2.46839459\n",
      "Iteration 34755, loss = 2.46359291\n",
      "Iteration 34756, loss = 2.81074182\n",
      "Iteration 34757, loss = 3.15987157\n",
      "Iteration 34758, loss = 2.97407037\n",
      "Iteration 34759, loss = 2.78673595\n",
      "Iteration 34760, loss = 2.81679633\n",
      "Iteration 34761, loss = 2.75841087\n",
      "Iteration 34762, loss = 3.12665574\n",
      "Iteration 34763, loss = 3.34426061\n",
      "Iteration 34764, loss = 2.88700723\n",
      "Iteration 34765, loss = 3.35865258\n",
      "Iteration 34766, loss = 3.29287361\n",
      "Iteration 34767, loss = 3.43383318\n",
      "Iteration 34768, loss = 3.02663096\n",
      "Iteration 34769, loss = 3.06782079\n",
      "Iteration 34770, loss = 3.25371173\n",
      "Iteration 34771, loss = 3.19613237\n",
      "Iteration 34772, loss = 3.02153910\n",
      "Iteration 34773, loss = 3.09187077\n",
      "Iteration 34774, loss = 4.13170784\n",
      "Iteration 34775, loss = 3.18836726\n",
      "Iteration 34776, loss = 3.53182163\n",
      "Iteration 34777, loss = 3.81005273\n",
      "Iteration 34778, loss = 3.43140551\n",
      "Iteration 34779, loss = 2.86267553\n",
      "Iteration 34780, loss = 3.01776652\n",
      "Iteration 34781, loss = 2.58483307\n",
      "Iteration 34782, loss = 2.81957818\n",
      "Iteration 34783, loss = 2.76932637\n",
      "Iteration 34784, loss = 2.78904442\n",
      "Iteration 34785, loss = 2.68521658\n",
      "Iteration 34786, loss = 2.97602545\n",
      "Iteration 34787, loss = 2.86218856\n",
      "Iteration 34788, loss = 2.92469167\n",
      "Iteration 34789, loss = 2.79398437\n",
      "Iteration 34790, loss = 2.64334189\n",
      "Iteration 34791, loss = 2.84512987\n",
      "Iteration 34792, loss = 3.75150539\n",
      "Iteration 34793, loss = 6.22921939\n",
      "Iteration 34794, loss = 3.68929620\n",
      "Iteration 34795, loss = 3.40247611\n",
      "Iteration 34796, loss = 3.72292581\n",
      "Iteration 34797, loss = 3.67018827\n",
      "Iteration 34798, loss = 3.35279084\n",
      "Iteration 34799, loss = 3.32988532\n",
      "Iteration 34800, loss = 3.56898445\n",
      "Iteration 34801, loss = 2.88651123\n",
      "Iteration 34802, loss = 3.04593071\n",
      "Iteration 34803, loss = 3.06230599\n",
      "Iteration 34804, loss = 2.68015423\n",
      "Iteration 34805, loss = 2.80633914\n",
      "Iteration 34806, loss = 3.26489998\n",
      "Iteration 34807, loss = 3.02014987\n",
      "Iteration 34808, loss = 3.00149148\n",
      "Iteration 34809, loss = 2.89993056\n",
      "Iteration 34810, loss = 2.93253081\n",
      "Iteration 34811, loss = 2.63054208\n",
      "Iteration 34812, loss = 3.06470059\n",
      "Iteration 34813, loss = 3.66732618\n",
      "Iteration 34814, loss = 2.88980418\n",
      "Iteration 34815, loss = 2.80584500\n",
      "Iteration 34816, loss = 2.92052142\n",
      "Iteration 34817, loss = 3.19503995\n",
      "Iteration 34818, loss = 2.80809222\n",
      "Iteration 34819, loss = 2.79129488\n",
      "Iteration 34820, loss = 3.32012713\n",
      "Iteration 34821, loss = 3.93677549\n",
      "Iteration 34822, loss = 4.06916139\n",
      "Iteration 34823, loss = 4.49728984\n",
      "Iteration 34824, loss = 4.04065501\n",
      "Iteration 34825, loss = 5.65021631\n",
      "Iteration 34826, loss = 5.48244708\n",
      "Iteration 34827, loss = 4.01814387\n",
      "Iteration 34828, loss = 3.32471449\n",
      "Iteration 34829, loss = 2.84220029\n",
      "Iteration 34830, loss = 2.76178409\n",
      "Iteration 34831, loss = 2.66244748\n",
      "Iteration 34832, loss = 2.88093422\n",
      "Iteration 34833, loss = 2.74829462\n",
      "Iteration 34834, loss = 2.93069487\n",
      "Iteration 34835, loss = 2.65699165\n",
      "Iteration 34836, loss = 2.57098830\n",
      "Iteration 34837, loss = 2.67445237\n",
      "Iteration 34838, loss = 3.21929113\n",
      "Iteration 34839, loss = 2.99742051\n",
      "Iteration 34840, loss = 3.23317557\n",
      "Iteration 34841, loss = 3.13315796\n",
      "Iteration 34842, loss = 2.91198335\n",
      "Iteration 34843, loss = 3.00915758\n",
      "Iteration 34844, loss = 3.25599578\n",
      "Iteration 34845, loss = 3.75294040\n",
      "Iteration 34846, loss = 3.09858949\n",
      "Iteration 34847, loss = 3.24891815\n",
      "Iteration 34848, loss = 3.10036331\n",
      "Iteration 34849, loss = 3.34342353\n",
      "Iteration 34850, loss = 3.19589474\n",
      "Iteration 34851, loss = 4.93751338\n",
      "Iteration 34852, loss = 7.51770624\n",
      "Iteration 34853, loss = 4.70906022\n",
      "Iteration 34854, loss = 3.73873001\n",
      "Iteration 34855, loss = 3.01379318\n",
      "Iteration 34856, loss = 2.74445383\n",
      "Iteration 34857, loss = 2.85110837\n",
      "Iteration 34858, loss = 3.00582324\n",
      "Iteration 34859, loss = 2.64842637\n",
      "Iteration 34860, loss = 2.72512114\n",
      "Iteration 34861, loss = 2.96807062\n",
      "Iteration 34862, loss = 2.97069217\n",
      "Iteration 34863, loss = 2.68794898\n",
      "Iteration 34864, loss = 2.52199302\n",
      "Iteration 34865, loss = 2.73918307\n",
      "Iteration 34866, loss = 2.86128179\n",
      "Iteration 34867, loss = 3.25086871\n",
      "Iteration 34868, loss = 2.58720817\n",
      "Iteration 34869, loss = 3.05312550\n",
      "Iteration 34870, loss = 3.12622943\n",
      "Iteration 34871, loss = 3.86487604\n",
      "Iteration 34872, loss = 3.21332977\n",
      "Iteration 34873, loss = 2.83676013\n",
      "Iteration 34874, loss = 2.53481087\n",
      "Iteration 34875, loss = 2.50135680\n",
      "Iteration 34876, loss = 2.52696792\n",
      "Iteration 34877, loss = 2.79046224\n",
      "Iteration 34878, loss = 2.63640267\n",
      "Iteration 34879, loss = 3.22909819\n",
      "Iteration 34880, loss = 2.93387028\n",
      "Iteration 34881, loss = 2.96885797\n",
      "Iteration 34882, loss = 2.63585639\n",
      "Iteration 34883, loss = 2.65141594\n",
      "Iteration 34884, loss = 3.24189010\n",
      "Iteration 34885, loss = 3.14096324\n",
      "Iteration 34886, loss = 2.76028191\n",
      "Iteration 34887, loss = 2.66464448\n",
      "Iteration 34888, loss = 2.63059667\n",
      "Iteration 34889, loss = 3.02048152\n",
      "Iteration 34890, loss = 3.59621752\n",
      "Iteration 34891, loss = 3.45543877\n",
      "Iteration 34892, loss = 3.37111398\n",
      "Iteration 34893, loss = 2.75573318\n",
      "Iteration 34894, loss = 3.07421126\n",
      "Iteration 34895, loss = 2.97181005\n",
      "Iteration 34896, loss = 2.75297525\n",
      "Iteration 34897, loss = 2.48666545\n",
      "Iteration 34898, loss = 2.84860015\n",
      "Iteration 34899, loss = 2.79203471\n",
      "Iteration 34900, loss = 2.65202414\n",
      "Iteration 34901, loss = 2.74341197\n",
      "Iteration 34902, loss = 2.97013823\n",
      "Iteration 34903, loss = 2.97305506\n",
      "Iteration 34904, loss = 2.99171367\n",
      "Iteration 34905, loss = 3.54225211\n",
      "Iteration 34906, loss = 4.01151385\n",
      "Iteration 34907, loss = 4.44706994\n",
      "Iteration 34908, loss = 4.74343041\n",
      "Iteration 34909, loss = 5.16081134\n",
      "Iteration 34910, loss = 3.67669603\n",
      "Iteration 34911, loss = 3.65996376\n",
      "Iteration 34912, loss = 3.36110235\n",
      "Iteration 34913, loss = 3.74368174\n",
      "Iteration 34914, loss = 3.38187771\n",
      "Iteration 34915, loss = 3.25496891\n",
      "Iteration 34916, loss = 2.95102065\n",
      "Iteration 34917, loss = 2.93745615\n",
      "Iteration 34918, loss = 2.82889515\n",
      "Iteration 34919, loss = 2.76535265\n",
      "Iteration 34920, loss = 2.56131888\n",
      "Iteration 34921, loss = 3.33197302\n",
      "Iteration 34922, loss = 2.95065502\n",
      "Iteration 34923, loss = 2.72085032\n",
      "Iteration 34924, loss = 2.88784838\n",
      "Iteration 34925, loss = 3.03336378\n",
      "Iteration 34926, loss = 2.42736144\n",
      "Iteration 34927, loss = 2.53646146\n",
      "Iteration 34928, loss = 2.47076065\n",
      "Iteration 34929, loss = 2.57659257\n",
      "Iteration 34930, loss = 2.64048468\n",
      "Iteration 34931, loss = 2.96248984\n",
      "Iteration 34932, loss = 2.93896652\n",
      "Iteration 34933, loss = 2.79433711\n",
      "Iteration 34934, loss = 3.67126267\n",
      "Iteration 34935, loss = 5.39224232\n",
      "Iteration 34936, loss = 3.38253813\n",
      "Iteration 34937, loss = 4.32566512\n",
      "Iteration 34938, loss = 3.64698865\n",
      "Iteration 34939, loss = 3.57000787\n",
      "Iteration 34940, loss = 2.33906830\n",
      "Iteration 34941, loss = 2.53429652\n",
      "Iteration 34942, loss = 2.52831453\n",
      "Iteration 34943, loss = 2.41691362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 34944, loss = 2.87959198\n",
      "Iteration 34945, loss = 2.59811321\n",
      "Iteration 34946, loss = 2.68580803\n",
      "Iteration 34947, loss = 3.36242633\n",
      "Iteration 34948, loss = 3.10086217\n",
      "Iteration 34949, loss = 2.92365458\n",
      "Iteration 34950, loss = 2.74991278\n",
      "Iteration 34951, loss = 3.23880794\n",
      "Iteration 34952, loss = 3.49556621\n",
      "Iteration 34953, loss = 2.94636134\n",
      "Iteration 34954, loss = 3.73776571\n",
      "Iteration 34955, loss = 3.19849020\n",
      "Iteration 34956, loss = 2.69497226\n",
      "Iteration 34957, loss = 3.19655230\n",
      "Iteration 34958, loss = 3.90889147\n",
      "Iteration 34959, loss = 3.51102648\n",
      "Iteration 34960, loss = 5.27095916\n",
      "Iteration 34961, loss = 4.04672123\n",
      "Iteration 34962, loss = 4.20682557\n",
      "Iteration 34963, loss = 3.27430564\n",
      "Iteration 34964, loss = 3.84160514\n",
      "Iteration 34965, loss = 3.09936139\n",
      "Iteration 34966, loss = 2.75149692\n",
      "Iteration 34967, loss = 3.09701622\n",
      "Iteration 34968, loss = 3.44957733\n",
      "Iteration 34969, loss = 3.04379866\n",
      "Iteration 34970, loss = 3.18117458\n",
      "Iteration 34971, loss = 3.01369955\n",
      "Iteration 34972, loss = 3.47394760\n",
      "Iteration 34973, loss = 3.24885011\n",
      "Iteration 34974, loss = 2.81598776\n",
      "Iteration 34975, loss = 2.52334143\n",
      "Iteration 34976, loss = 2.69722249\n",
      "Iteration 34977, loss = 2.62625315\n",
      "Iteration 34978, loss = 2.46025046\n",
      "Iteration 34979, loss = 2.71762931\n",
      "Iteration 34980, loss = 2.48655860\n",
      "Iteration 34981, loss = 2.66049416\n",
      "Iteration 34982, loss = 3.19888562\n",
      "Iteration 34983, loss = 4.80833629\n",
      "Iteration 34984, loss = 3.67916109\n",
      "Iteration 34985, loss = 3.91247851\n",
      "Iteration 34986, loss = 4.29110977\n",
      "Iteration 34987, loss = 3.70797153\n",
      "Iteration 34988, loss = 3.55101802\n",
      "Iteration 34989, loss = 2.75281276\n",
      "Iteration 34990, loss = 2.74365047\n",
      "Iteration 34991, loss = 3.45984643\n",
      "Iteration 34992, loss = 4.28430368\n",
      "Iteration 34993, loss = 3.54741354\n",
      "Iteration 34994, loss = 2.39919725\n",
      "Iteration 34995, loss = 3.08412822\n",
      "Iteration 34996, loss = 3.17984237\n",
      "Iteration 34997, loss = 2.84074073\n",
      "Iteration 34998, loss = 3.11318100\n",
      "Iteration 34999, loss = 2.66365129\n",
      "Iteration 35000, loss = 3.03192374\n",
      "Iteration 35001, loss = 3.50978894\n",
      "Iteration 35002, loss = 3.45646031\n",
      "Iteration 35003, loss = 3.47109434\n",
      "Iteration 35004, loss = 3.06989645\n",
      "Iteration 35005, loss = 2.91913167\n",
      "Iteration 35006, loss = 2.55661998\n",
      "Iteration 35007, loss = 2.94434404\n",
      "Iteration 35008, loss = 2.71198611\n",
      "Iteration 35009, loss = 3.04682694\n",
      "Iteration 35010, loss = 3.02264824\n",
      "Iteration 35011, loss = 3.51318368\n",
      "Iteration 35012, loss = 3.10837911\n",
      "Iteration 35013, loss = 3.33714712\n",
      "Iteration 35014, loss = 3.33844919\n",
      "Iteration 35015, loss = 3.08695334\n",
      "Iteration 35016, loss = 2.67236019\n",
      "Iteration 35017, loss = 3.04042913\n",
      "Iteration 35018, loss = 2.92430540\n",
      "Iteration 35019, loss = 3.03660862\n",
      "Iteration 35020, loss = 2.64404546\n",
      "Iteration 35021, loss = 2.86857029\n",
      "Iteration 35022, loss = 2.61366460\n",
      "Iteration 35023, loss = 3.10305225\n",
      "Iteration 35024, loss = 3.40923354\n",
      "Iteration 35025, loss = 2.94929131\n",
      "Iteration 35026, loss = 3.10436057\n",
      "Iteration 35027, loss = 2.81641806\n",
      "Iteration 35028, loss = 3.14188196\n",
      "Iteration 35029, loss = 3.62208875\n",
      "Iteration 35030, loss = 4.82961486\n",
      "Iteration 35031, loss = 4.35008503\n",
      "Iteration 35032, loss = 4.82096792\n",
      "Iteration 35033, loss = 3.88075063\n",
      "Iteration 35034, loss = 4.01230228\n",
      "Iteration 35035, loss = 3.04351246\n",
      "Iteration 35036, loss = 3.26993866\n",
      "Iteration 35037, loss = 3.12114502\n",
      "Iteration 35038, loss = 3.35893288\n",
      "Iteration 35039, loss = 2.84787689\n",
      "Iteration 35040, loss = 2.49836041\n",
      "Iteration 35041, loss = 3.03286889\n",
      "Iteration 35042, loss = 2.50221843\n",
      "Iteration 35043, loss = 2.57541033\n",
      "Iteration 35044, loss = 2.48456963\n",
      "Iteration 35045, loss = 2.78018874\n",
      "Iteration 35046, loss = 2.81526675\n",
      "Iteration 35047, loss = 2.73179904\n",
      "Iteration 35048, loss = 2.88281822\n",
      "Iteration 35049, loss = 2.96703770\n",
      "Iteration 35050, loss = 3.53693357\n",
      "Iteration 35051, loss = 3.64152020\n",
      "Iteration 35052, loss = 3.17552333\n",
      "Iteration 35053, loss = 3.01109132\n",
      "Iteration 35054, loss = 2.86952354\n",
      "Iteration 35055, loss = 2.98380913\n",
      "Iteration 35056, loss = 2.69834693\n",
      "Iteration 35057, loss = 3.15572264\n",
      "Iteration 35058, loss = 3.27426007\n",
      "Iteration 35059, loss = 3.56346467\n",
      "Iteration 35060, loss = 3.08912800\n",
      "Iteration 35061, loss = 2.97459436\n",
      "Iteration 35062, loss = 2.65935320\n",
      "Iteration 35063, loss = 2.58906658\n",
      "Iteration 35064, loss = 2.64226220\n",
      "Iteration 35065, loss = 2.45870177\n",
      "Iteration 35066, loss = 2.84588965\n",
      "Iteration 35067, loss = 3.01645357\n",
      "Iteration 35068, loss = 2.69800167\n",
      "Iteration 35069, loss = 2.71277228\n",
      "Iteration 35070, loss = 2.99910895\n",
      "Iteration 35071, loss = 2.88397119\n",
      "Iteration 35072, loss = 2.55312442\n",
      "Iteration 35073, loss = 2.49841809\n",
      "Iteration 35074, loss = 2.71120930\n",
      "Iteration 35075, loss = 2.52609601\n",
      "Iteration 35076, loss = 2.66916390\n",
      "Iteration 35077, loss = 2.70678482\n",
      "Iteration 35078, loss = 2.80293724\n",
      "Iteration 35079, loss = 2.76092780\n",
      "Iteration 35080, loss = 2.61201503\n",
      "Iteration 35081, loss = 2.96281423\n",
      "Iteration 35082, loss = 2.62737776\n",
      "Iteration 35083, loss = 2.61969472\n",
      "Iteration 35084, loss = 2.47639845\n",
      "Iteration 35085, loss = 2.64284300\n",
      "Iteration 35086, loss = 3.93798122\n",
      "Iteration 35087, loss = 3.21079288\n",
      "Iteration 35088, loss = 2.80593816\n",
      "Iteration 35089, loss = 3.53499750\n",
      "Iteration 35090, loss = 3.42907350\n",
      "Iteration 35091, loss = 2.82586125\n",
      "Iteration 35092, loss = 2.94316592\n",
      "Iteration 35093, loss = 2.72916882\n",
      "Iteration 35094, loss = 2.85199820\n",
      "Iteration 35095, loss = 3.47459990\n",
      "Iteration 35096, loss = 2.61832458\n",
      "Iteration 35097, loss = 2.64916753\n",
      "Iteration 35098, loss = 2.83625527\n",
      "Iteration 35099, loss = 3.08649339\n",
      "Iteration 35100, loss = 3.51140693\n",
      "Iteration 35101, loss = 2.97601380\n",
      "Iteration 35102, loss = 2.79396394\n",
      "Iteration 35103, loss = 3.16299092\n",
      "Iteration 35104, loss = 3.53223844\n",
      "Iteration 35105, loss = 3.53368367\n",
      "Iteration 35106, loss = 3.93781403\n",
      "Iteration 35107, loss = 3.09633518\n",
      "Iteration 35108, loss = 2.85490520\n",
      "Iteration 35109, loss = 3.54485041\n",
      "Iteration 35110, loss = 3.03539050\n",
      "Iteration 35111, loss = 3.30293380\n",
      "Iteration 35112, loss = 3.94826614\n",
      "Iteration 35113, loss = 3.80018404\n",
      "Iteration 35114, loss = 3.44644388\n",
      "Iteration 35115, loss = 3.25252720\n",
      "Iteration 35116, loss = 3.09393605\n",
      "Iteration 35117, loss = 2.73623334\n",
      "Iteration 35118, loss = 2.75464243\n",
      "Iteration 35119, loss = 2.94998327\n",
      "Iteration 35120, loss = 3.15290695\n",
      "Iteration 35121, loss = 2.56548329\n",
      "Iteration 35122, loss = 3.27189113\n",
      "Iteration 35123, loss = 5.01277486\n",
      "Iteration 35124, loss = 4.79973140\n",
      "Iteration 35125, loss = 4.26735978\n",
      "Iteration 35126, loss = 3.91552790\n",
      "Iteration 35127, loss = 4.00097799\n",
      "Iteration 35128, loss = 3.60249081\n",
      "Iteration 35129, loss = 3.40278585\n",
      "Iteration 35130, loss = 4.07169157\n",
      "Iteration 35131, loss = 3.44837795\n",
      "Iteration 35132, loss = 4.01887349\n",
      "Iteration 35133, loss = 4.27445272\n",
      "Iteration 35134, loss = 4.24712198\n",
      "Iteration 35135, loss = 4.19538476\n",
      "Iteration 35136, loss = 4.73247171\n",
      "Iteration 35137, loss = 5.89585578\n",
      "Iteration 35138, loss = 5.07037129\n",
      "Iteration 35139, loss = 6.50041454\n",
      "Iteration 35140, loss = 4.10456953\n",
      "Iteration 35141, loss = 3.93443464\n",
      "Iteration 35142, loss = 2.99307891\n",
      "Iteration 35143, loss = 3.34376205\n",
      "Iteration 35144, loss = 3.08171121\n",
      "Iteration 35145, loss = 3.47955485\n",
      "Iteration 35146, loss = 2.91110576\n",
      "Iteration 35147, loss = 2.62253263\n",
      "Iteration 35148, loss = 2.71318935\n",
      "Iteration 35149, loss = 2.89481561\n",
      "Iteration 35150, loss = 2.88001799\n",
      "Iteration 35151, loss = 3.02439173\n",
      "Iteration 35152, loss = 2.74355625\n",
      "Iteration 35153, loss = 3.35487918\n",
      "Iteration 35154, loss = 3.09209246\n",
      "Iteration 35155, loss = 3.74567126\n",
      "Iteration 35156, loss = 4.15245038\n",
      "Iteration 35157, loss = 4.18500944\n",
      "Iteration 35158, loss = 4.37056507\n",
      "Iteration 35159, loss = 3.52767589\n",
      "Iteration 35160, loss = 3.07170391\n",
      "Iteration 35161, loss = 2.59733457\n",
      "Iteration 35162, loss = 2.84576213\n",
      "Iteration 35163, loss = 3.36954136\n",
      "Iteration 35164, loss = 2.96216476\n",
      "Iteration 35165, loss = 2.61146674\n",
      "Iteration 35166, loss = 2.91171466\n",
      "Iteration 35167, loss = 2.92245122\n",
      "Iteration 35168, loss = 3.69443252\n",
      "Iteration 35169, loss = 3.99073270\n",
      "Iteration 35170, loss = 4.02497982\n",
      "Iteration 35171, loss = 3.56569752\n",
      "Iteration 35172, loss = 4.08713766\n",
      "Iteration 35173, loss = 3.37191887\n",
      "Iteration 35174, loss = 3.02420079\n",
      "Iteration 35175, loss = 2.92425534\n",
      "Iteration 35176, loss = 2.61673236\n",
      "Iteration 35177, loss = 3.16540425\n",
      "Iteration 35178, loss = 3.49848702\n",
      "Iteration 35179, loss = 3.70394771\n",
      "Iteration 35180, loss = 3.17712764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 35181, loss = 2.75677477\n",
      "Iteration 35182, loss = 3.16610773\n",
      "Iteration 35183, loss = 3.33810221\n",
      "Iteration 35184, loss = 3.01090893\n",
      "Iteration 35185, loss = 2.97309735\n",
      "Iteration 35186, loss = 2.66066585\n",
      "Iteration 35187, loss = 2.31525410\n",
      "Iteration 35188, loss = 2.64090681\n",
      "Iteration 35189, loss = 2.38744148\n",
      "Iteration 35190, loss = 2.45350726\n",
      "Iteration 35191, loss = 2.96060990\n",
      "Iteration 35192, loss = 2.92621812\n",
      "Iteration 35193, loss = 2.63396953\n",
      "Iteration 35194, loss = 2.92088849\n",
      "Iteration 35195, loss = 3.21402679\n",
      "Iteration 35196, loss = 3.22542843\n",
      "Iteration 35197, loss = 3.46353585\n",
      "Iteration 35198, loss = 4.69588431\n",
      "Iteration 35199, loss = 3.79448188\n",
      "Iteration 35200, loss = 3.69521090\n",
      "Iteration 35201, loss = 3.66878255\n",
      "Iteration 35202, loss = 3.23047477\n",
      "Iteration 35203, loss = 3.07455455\n",
      "Iteration 35204, loss = 2.64753718\n",
      "Iteration 35205, loss = 2.77625555\n",
      "Iteration 35206, loss = 2.59748076\n",
      "Iteration 35207, loss = 2.57416838\n",
      "Iteration 35208, loss = 3.34516396\n",
      "Iteration 35209, loss = 3.76112647\n",
      "Iteration 35210, loss = 3.46693765\n",
      "Iteration 35211, loss = 3.74105285\n",
      "Iteration 35212, loss = 4.06388551\n",
      "Iteration 35213, loss = 3.21774405\n",
      "Iteration 35214, loss = 3.09496662\n",
      "Iteration 35215, loss = 3.13175677\n",
      "Iteration 35216, loss = 3.04334177\n",
      "Iteration 35217, loss = 2.55246969\n",
      "Iteration 35218, loss = 2.69718053\n",
      "Iteration 35219, loss = 2.51422968\n",
      "Iteration 35220, loss = 2.38150155\n",
      "Iteration 35221, loss = 2.46437188\n",
      "Iteration 35222, loss = 2.50514139\n",
      "Iteration 35223, loss = 2.73136657\n",
      "Iteration 35224, loss = 2.48264438\n",
      "Iteration 35225, loss = 2.30964059\n",
      "Iteration 35226, loss = 3.29199815\n",
      "Iteration 35227, loss = 3.28176283\n",
      "Iteration 35228, loss = 3.14414166\n",
      "Iteration 35229, loss = 3.20993286\n",
      "Iteration 35230, loss = 2.75296640\n",
      "Iteration 35231, loss = 2.42361668\n",
      "Iteration 35232, loss = 2.44360988\n",
      "Iteration 35233, loss = 2.30790164\n",
      "Iteration 35234, loss = 2.61625855\n",
      "Iteration 35235, loss = 3.07424807\n",
      "Iteration 35236, loss = 2.93772580\n",
      "Iteration 35237, loss = 3.08914222\n",
      "Iteration 35238, loss = 2.70628698\n",
      "Iteration 35239, loss = 2.89617610\n",
      "Iteration 35240, loss = 3.04613671\n",
      "Iteration 35241, loss = 2.59796679\n",
      "Iteration 35242, loss = 2.66019749\n",
      "Iteration 35243, loss = 2.62054814\n",
      "Iteration 35244, loss = 2.41975972\n",
      "Iteration 35245, loss = 2.53634987\n",
      "Iteration 35246, loss = 3.20584324\n",
      "Iteration 35247, loss = 4.32110939\n",
      "Iteration 35248, loss = 5.35798065\n",
      "Iteration 35249, loss = 4.70743873\n",
      "Iteration 35250, loss = 3.16544054\n",
      "Iteration 35251, loss = 3.48541477\n",
      "Iteration 35252, loss = 3.09849084\n",
      "Iteration 35253, loss = 4.51248154\n",
      "Iteration 35254, loss = 4.40325373\n",
      "Iteration 35255, loss = 4.24051110\n",
      "Iteration 35256, loss = 3.77278055\n",
      "Iteration 35257, loss = 3.27702073\n",
      "Iteration 35258, loss = 3.40156093\n",
      "Iteration 35259, loss = 3.08961401\n",
      "Iteration 35260, loss = 2.93320965\n",
      "Iteration 35261, loss = 2.92859407\n",
      "Iteration 35262, loss = 2.75059361\n",
      "Iteration 35263, loss = 2.91983745\n",
      "Iteration 35264, loss = 3.06205033\n",
      "Iteration 35265, loss = 2.95915942\n",
      "Iteration 35266, loss = 2.65498972\n",
      "Iteration 35267, loss = 2.60042900\n",
      "Iteration 35268, loss = 2.44517572\n",
      "Iteration 35269, loss = 2.97985892\n",
      "Iteration 35270, loss = 2.62801364\n",
      "Iteration 35271, loss = 2.45779622\n",
      "Iteration 35272, loss = 2.60578452\n",
      "Iteration 35273, loss = 2.60030560\n",
      "Iteration 35274, loss = 2.52615689\n",
      "Iteration 35275, loss = 3.01713446\n",
      "Iteration 35276, loss = 3.59383037\n",
      "Iteration 35277, loss = 2.97966415\n",
      "Iteration 35278, loss = 3.00040292\n",
      "Iteration 35279, loss = 2.87655928\n",
      "Iteration 35280, loss = 2.65187153\n",
      "Iteration 35281, loss = 3.06359374\n",
      "Iteration 35282, loss = 2.75428368\n",
      "Iteration 35283, loss = 2.77582343\n",
      "Iteration 35284, loss = 2.91857479\n",
      "Iteration 35285, loss = 3.34449069\n",
      "Iteration 35286, loss = 3.00127932\n",
      "Iteration 35287, loss = 2.44915633\n",
      "Iteration 35288, loss = 2.50499070\n",
      "Iteration 35289, loss = 2.57879894\n",
      "Iteration 35290, loss = 2.92365571\n",
      "Iteration 35291, loss = 3.65165897\n",
      "Iteration 35292, loss = 2.97664283\n",
      "Iteration 35293, loss = 3.18392407\n",
      "Iteration 35294, loss = 2.62047984\n",
      "Iteration 35295, loss = 2.41822715\n",
      "Iteration 35296, loss = 2.88789775\n",
      "Iteration 35297, loss = 2.51163531\n",
      "Iteration 35298, loss = 3.15004577\n",
      "Iteration 35299, loss = 3.29403326\n",
      "Iteration 35300, loss = 2.79368878\n",
      "Iteration 35301, loss = 2.83366920\n",
      "Iteration 35302, loss = 2.92777053\n",
      "Iteration 35303, loss = 3.47524887\n",
      "Iteration 35304, loss = 3.41554290\n",
      "Iteration 35305, loss = 3.68715632\n",
      "Iteration 35306, loss = 3.96229804\n",
      "Iteration 35307, loss = 4.11381606\n",
      "Iteration 35308, loss = 3.30504317\n",
      "Iteration 35309, loss = 3.56257021\n",
      "Iteration 35310, loss = 2.74424335\n",
      "Iteration 35311, loss = 2.95043422\n",
      "Iteration 35312, loss = 2.97965463\n",
      "Iteration 35313, loss = 3.00278691\n",
      "Iteration 35314, loss = 3.44667532\n",
      "Iteration 35315, loss = 2.71758568\n",
      "Iteration 35316, loss = 3.42899402\n",
      "Iteration 35317, loss = 3.08600712\n",
      "Iteration 35318, loss = 2.40399798\n",
      "Iteration 35319, loss = 2.64247715\n",
      "Iteration 35320, loss = 2.70792391\n",
      "Iteration 35321, loss = 2.94923842\n",
      "Iteration 35322, loss = 3.38451393\n",
      "Iteration 35323, loss = 3.13098632\n",
      "Iteration 35324, loss = 2.52865660\n",
      "Iteration 35325, loss = 2.43996019\n",
      "Iteration 35326, loss = 2.49954076\n",
      "Iteration 35327, loss = 2.55090641\n",
      "Iteration 35328, loss = 2.40707975\n",
      "Iteration 35329, loss = 2.67619167\n",
      "Iteration 35330, loss = 2.39426743\n",
      "Iteration 35331, loss = 2.61438936\n",
      "Iteration 35332, loss = 2.54710683\n",
      "Iteration 35333, loss = 2.81090106\n",
      "Iteration 35334, loss = 2.79245084\n",
      "Iteration 35335, loss = 3.19169650\n",
      "Iteration 35336, loss = 3.74090788\n",
      "Iteration 35337, loss = 5.72435598\n",
      "Iteration 35338, loss = 7.20898266\n",
      "Iteration 35339, loss = 3.99435629\n",
      "Iteration 35340, loss = 4.16811478\n",
      "Iteration 35341, loss = 3.46176257\n",
      "Iteration 35342, loss = 3.80224523\n",
      "Iteration 35343, loss = 3.23558178\n",
      "Iteration 35344, loss = 3.54416935\n",
      "Iteration 35345, loss = 2.99270943\n",
      "Iteration 35346, loss = 2.76124295\n",
      "Iteration 35347, loss = 2.76637640\n",
      "Iteration 35348, loss = 2.89330589\n",
      "Iteration 35349, loss = 2.94612714\n",
      "Iteration 35350, loss = 2.84560167\n",
      "Iteration 35351, loss = 2.68409483\n",
      "Iteration 35352, loss = 2.96007713\n",
      "Iteration 35353, loss = 2.60381212\n",
      "Iteration 35354, loss = 3.38548746\n",
      "Iteration 35355, loss = 3.07795592\n",
      "Iteration 35356, loss = 3.40187709\n",
      "Iteration 35357, loss = 2.89613042\n",
      "Iteration 35358, loss = 3.13932123\n",
      "Iteration 35359, loss = 2.83055192\n",
      "Iteration 35360, loss = 2.86922943\n",
      "Iteration 35361, loss = 2.96935098\n",
      "Iteration 35362, loss = 2.82431602\n",
      "Iteration 35363, loss = 2.75042643\n",
      "Iteration 35364, loss = 2.88142347\n",
      "Iteration 35365, loss = 3.27514410\n",
      "Iteration 35366, loss = 3.30774429\n",
      "Iteration 35367, loss = 3.95054536\n",
      "Iteration 35368, loss = 3.30155599\n",
      "Iteration 35369, loss = 3.65919119\n",
      "Iteration 35370, loss = 3.15997781\n",
      "Iteration 35371, loss = 2.73863258\n",
      "Iteration 35372, loss = 2.55425956\n",
      "Iteration 35373, loss = 3.10639959\n",
      "Iteration 35374, loss = 3.38881452\n",
      "Iteration 35375, loss = 2.90182467\n",
      "Iteration 35376, loss = 3.05290957\n",
      "Iteration 35377, loss = 2.57619995\n",
      "Iteration 35378, loss = 2.78036467\n",
      "Iteration 35379, loss = 2.98171730\n",
      "Iteration 35380, loss = 3.53787705\n",
      "Iteration 35381, loss = 4.04308116\n",
      "Iteration 35382, loss = 3.99862889\n",
      "Iteration 35383, loss = 5.80842388\n",
      "Iteration 35384, loss = 3.53565853\n",
      "Iteration 35385, loss = 4.27253206\n",
      "Iteration 35386, loss = 3.13641548\n",
      "Iteration 35387, loss = 2.57830052\n",
      "Iteration 35388, loss = 3.06641600\n",
      "Iteration 35389, loss = 3.11367709\n",
      "Iteration 35390, loss = 2.72191089\n",
      "Iteration 35391, loss = 2.43761595\n",
      "Iteration 35392, loss = 3.16832330\n",
      "Iteration 35393, loss = 2.56861432\n",
      "Iteration 35394, loss = 2.88680243\n",
      "Iteration 35395, loss = 2.81489480\n",
      "Iteration 35396, loss = 2.91153198\n",
      "Iteration 35397, loss = 3.07530419\n",
      "Iteration 35398, loss = 2.91984032\n",
      "Iteration 35399, loss = 3.27351991\n",
      "Iteration 35400, loss = 3.83779155\n",
      "Iteration 35401, loss = 4.57859141\n",
      "Iteration 35402, loss = 5.86532875\n",
      "Iteration 35403, loss = 4.23591595\n",
      "Iteration 35404, loss = 3.51666189\n",
      "Iteration 35405, loss = 2.72713416\n",
      "Iteration 35406, loss = 3.61572097\n",
      "Iteration 35407, loss = 2.69391973\n",
      "Iteration 35408, loss = 2.90201930\n",
      "Iteration 35409, loss = 3.07468901\n",
      "Iteration 35410, loss = 2.42912415\n",
      "Iteration 35411, loss = 2.44776903\n",
      "Iteration 35412, loss = 2.47658763\n",
      "Iteration 35413, loss = 2.31028762\n",
      "Iteration 35414, loss = 2.61107032\n",
      "Iteration 35415, loss = 3.22256946\n",
      "Iteration 35416, loss = 2.80525199\n",
      "Iteration 35417, loss = 2.88937719\n",
      "Iteration 35418, loss = 3.04847464\n",
      "Iteration 35419, loss = 3.19745541\n",
      "Iteration 35420, loss = 2.87904480\n",
      "Iteration 35421, loss = 3.20799697\n",
      "Iteration 35422, loss = 3.51938783\n",
      "Iteration 35423, loss = 3.73640404\n",
      "Iteration 35424, loss = 4.45791155\n",
      "Iteration 35425, loss = 5.11044639\n",
      "Iteration 35426, loss = 4.46994322\n",
      "Iteration 35427, loss = 3.15307766\n",
      "Iteration 35428, loss = 3.60689654\n",
      "Iteration 35429, loss = 3.17848246\n",
      "Iteration 35430, loss = 3.88183822\n",
      "Iteration 35431, loss = 5.09434366\n",
      "Iteration 35432, loss = 3.35229241\n",
      "Iteration 35433, loss = 3.56633878\n",
      "Iteration 35434, loss = 3.42164984\n",
      "Iteration 35435, loss = 2.80478094\n",
      "Iteration 35436, loss = 2.81169275\n",
      "Iteration 35437, loss = 3.32026959\n",
      "Iteration 35438, loss = 2.95564690\n",
      "Iteration 35439, loss = 3.75708541\n",
      "Iteration 35440, loss = 3.35137426\n",
      "Iteration 35441, loss = 2.62111396\n",
      "Iteration 35442, loss = 2.56725198\n",
      "Iteration 35443, loss = 2.66566110\n",
      "Iteration 35444, loss = 2.31030387\n",
      "Iteration 35445, loss = 2.45194020\n",
      "Iteration 35446, loss = 2.38949600\n",
      "Iteration 35447, loss = 2.56548878\n",
      "Iteration 35448, loss = 2.94068386\n",
      "Iteration 35449, loss = 2.93820511\n",
      "Iteration 35450, loss = 3.01530243\n",
      "Iteration 35451, loss = 3.59763835\n",
      "Iteration 35452, loss = 3.37137557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 35453, loss = 3.21460549\n",
      "Iteration 35454, loss = 3.40041669\n",
      "Iteration 35455, loss = 3.29259499\n",
      "Iteration 35456, loss = 4.99176435\n",
      "Iteration 35457, loss = 4.42028202\n",
      "Iteration 35458, loss = 5.15577926\n",
      "Iteration 35459, loss = 4.70899485\n",
      "Iteration 35460, loss = 3.58892827\n",
      "Iteration 35461, loss = 3.93998750\n",
      "Iteration 35462, loss = 3.41297134\n",
      "Iteration 35463, loss = 2.94579054\n",
      "Iteration 35464, loss = 2.82544656\n",
      "Iteration 35465, loss = 2.59414239\n",
      "Iteration 35466, loss = 2.62790069\n",
      "Iteration 35467, loss = 3.12109588\n",
      "Iteration 35468, loss = 2.67512924\n",
      "Iteration 35469, loss = 2.96157012\n",
      "Iteration 35470, loss = 3.05740509\n",
      "Iteration 35471, loss = 3.31269719\n",
      "Iteration 35472, loss = 3.17158833\n",
      "Iteration 35473, loss = 3.20384329\n",
      "Iteration 35474, loss = 2.65383523\n",
      "Iteration 35475, loss = 2.47172007\n",
      "Iteration 35476, loss = 2.46529643\n",
      "Iteration 35477, loss = 2.67838024\n",
      "Iteration 35478, loss = 3.40379808\n",
      "Iteration 35479, loss = 3.03427166\n",
      "Iteration 35480, loss = 2.67847403\n",
      "Iteration 35481, loss = 2.82928049\n",
      "Iteration 35482, loss = 3.16512532\n",
      "Iteration 35483, loss = 3.07187210\n",
      "Iteration 35484, loss = 2.75712122\n",
      "Iteration 35485, loss = 3.01141171\n",
      "Iteration 35486, loss = 2.68663704\n",
      "Iteration 35487, loss = 2.98463807\n",
      "Iteration 35488, loss = 2.38471446\n",
      "Iteration 35489, loss = 2.89411160\n",
      "Iteration 35490, loss = 2.76750965\n",
      "Iteration 35491, loss = 2.79252478\n",
      "Iteration 35492, loss = 2.80756726\n",
      "Iteration 35493, loss = 2.74299468\n",
      "Iteration 35494, loss = 2.80424338\n",
      "Iteration 35495, loss = 3.74908424\n",
      "Iteration 35496, loss = 3.12396354\n",
      "Iteration 35497, loss = 2.82055090\n",
      "Iteration 35498, loss = 2.93009134\n",
      "Iteration 35499, loss = 2.86734272\n",
      "Iteration 35500, loss = 3.74218171\n",
      "Iteration 35501, loss = 3.36790737\n",
      "Iteration 35502, loss = 2.82181800\n",
      "Iteration 35503, loss = 2.84560620\n",
      "Iteration 35504, loss = 2.81154752\n",
      "Iteration 35505, loss = 2.69928948\n",
      "Iteration 35506, loss = 2.64014525\n",
      "Iteration 35507, loss = 2.77868785\n",
      "Iteration 35508, loss = 3.49563766\n",
      "Iteration 35509, loss = 3.28424860\n",
      "Iteration 35510, loss = 3.17564301\n",
      "Iteration 35511, loss = 2.82848236\n",
      "Iteration 35512, loss = 2.75040007\n",
      "Iteration 35513, loss = 3.02354278\n",
      "Iteration 35514, loss = 2.57099319\n",
      "Iteration 35515, loss = 2.57144336\n",
      "Iteration 35516, loss = 2.48029453\n",
      "Iteration 35517, loss = 2.23896666\n",
      "Iteration 35518, loss = 2.39310761\n",
      "Iteration 35519, loss = 2.46420828\n",
      "Iteration 35520, loss = 2.40791035\n",
      "Iteration 35521, loss = 2.39229841\n",
      "Iteration 35522, loss = 2.80034899\n",
      "Iteration 35523, loss = 2.96267647\n",
      "Iteration 35524, loss = 2.67876559\n",
      "Iteration 35525, loss = 2.85252992\n",
      "Iteration 35526, loss = 2.49900943\n",
      "Iteration 35527, loss = 2.39103089\n",
      "Iteration 35528, loss = 2.48355428\n",
      "Iteration 35529, loss = 2.73466549\n",
      "Iteration 35530, loss = 2.61663419\n",
      "Iteration 35531, loss = 2.44670214\n",
      "Iteration 35532, loss = 3.17625807\n",
      "Iteration 35533, loss = 3.27093459\n",
      "Iteration 35534, loss = 4.01133414\n",
      "Iteration 35535, loss = 4.90131876\n",
      "Iteration 35536, loss = 3.40314436\n",
      "Iteration 35537, loss = 3.52069841\n",
      "Iteration 35538, loss = 3.51715545\n",
      "Iteration 35539, loss = 3.51465790\n",
      "Iteration 35540, loss = 2.78647793\n",
      "Iteration 35541, loss = 2.47801323\n",
      "Iteration 35542, loss = 2.92998219\n",
      "Iteration 35543, loss = 2.71012343\n",
      "Iteration 35544, loss = 2.98366817\n",
      "Iteration 35545, loss = 2.72495654\n",
      "Iteration 35546, loss = 2.78009074\n",
      "Iteration 35547, loss = 3.08601778\n",
      "Iteration 35548, loss = 2.62790472\n",
      "Iteration 35549, loss = 2.39803013\n",
      "Iteration 35550, loss = 2.70494552\n",
      "Iteration 35551, loss = 2.50676869\n",
      "Iteration 35552, loss = 3.02401554\n",
      "Iteration 35553, loss = 3.61911206\n",
      "Iteration 35554, loss = 3.27062380\n",
      "Iteration 35555, loss = 2.76441057\n",
      "Iteration 35556, loss = 2.46082446\n",
      "Iteration 35557, loss = 2.82734833\n",
      "Iteration 35558, loss = 2.70703408\n",
      "Iteration 35559, loss = 2.93783339\n",
      "Iteration 35560, loss = 5.83749094\n",
      "Iteration 35561, loss = 5.81828371\n",
      "Iteration 35562, loss = 5.79786889\n",
      "Iteration 35563, loss = 5.57846871\n",
      "Iteration 35564, loss = 6.92177914\n",
      "Iteration 35565, loss = 8.49088493\n",
      "Iteration 35566, loss = 8.69559623\n",
      "Iteration 35567, loss = 8.98998850\n",
      "Iteration 35568, loss = 6.23916513\n",
      "Iteration 35569, loss = 4.81054714\n",
      "Iteration 35570, loss = 3.50324534\n",
      "Iteration 35571, loss = 3.53042704\n",
      "Iteration 35572, loss = 3.15335375\n",
      "Iteration 35573, loss = 2.56920609\n",
      "Iteration 35574, loss = 2.50512508\n",
      "Iteration 35575, loss = 2.66520651\n",
      "Iteration 35576, loss = 2.49287247\n",
      "Iteration 35577, loss = 2.54443259\n",
      "Iteration 35578, loss = 2.72289628\n",
      "Iteration 35579, loss = 3.05700916\n",
      "Iteration 35580, loss = 3.01378181\n",
      "Iteration 35581, loss = 3.20767692\n",
      "Iteration 35582, loss = 3.80733098\n",
      "Iteration 35583, loss = 3.05129864\n",
      "Iteration 35584, loss = 4.24783861\n",
      "Iteration 35585, loss = 4.26434878\n",
      "Iteration 35586, loss = 3.15144207\n",
      "Iteration 35587, loss = 2.81389129\n",
      "Iteration 35588, loss = 3.09626627\n",
      "Iteration 35589, loss = 2.76121940\n",
      "Iteration 35590, loss = 2.94037042\n",
      "Iteration 35591, loss = 3.30633584\n",
      "Iteration 35592, loss = 4.75061671\n",
      "Iteration 35593, loss = 5.50964346\n",
      "Iteration 35594, loss = 4.38710126\n",
      "Iteration 35595, loss = 3.59180427\n",
      "Iteration 35596, loss = 4.17319346\n",
      "Iteration 35597, loss = 3.36897769\n",
      "Iteration 35598, loss = 3.50229752\n",
      "Iteration 35599, loss = 3.51215338\n",
      "Iteration 35600, loss = 3.57389956\n",
      "Iteration 35601, loss = 3.28876334\n",
      "Iteration 35602, loss = 2.87464004\n",
      "Iteration 35603, loss = 2.70259581\n",
      "Iteration 35604, loss = 2.22472292\n",
      "Iteration 35605, loss = 2.63740390\n",
      "Iteration 35606, loss = 2.78741068\n",
      "Iteration 35607, loss = 3.04293102\n",
      "Iteration 35608, loss = 3.45604067\n",
      "Iteration 35609, loss = 3.41659279\n",
      "Iteration 35610, loss = 3.20838262\n",
      "Iteration 35611, loss = 2.43194271\n",
      "Iteration 35612, loss = 2.32824673\n",
      "Iteration 35613, loss = 2.86063560\n",
      "Iteration 35614, loss = 2.56846151\n",
      "Iteration 35615, loss = 2.43315514\n",
      "Iteration 35616, loss = 2.34068074\n",
      "Iteration 35617, loss = 2.76413622\n",
      "Iteration 35618, loss = 3.32203106\n",
      "Iteration 35619, loss = 3.25419740\n",
      "Iteration 35620, loss = 2.82681940\n",
      "Iteration 35621, loss = 2.82611797\n",
      "Iteration 35622, loss = 2.73108000\n",
      "Iteration 35623, loss = 2.46188136\n",
      "Iteration 35624, loss = 2.60198277\n",
      "Iteration 35625, loss = 2.52195566\n",
      "Iteration 35626, loss = 2.55396763\n",
      "Iteration 35627, loss = 2.80168177\n",
      "Iteration 35628, loss = 2.39628626\n",
      "Iteration 35629, loss = 2.33775423\n",
      "Iteration 35630, loss = 2.53134393\n",
      "Iteration 35631, loss = 2.51812900\n",
      "Iteration 35632, loss = 2.60178370\n",
      "Iteration 35633, loss = 2.99669834\n",
      "Iteration 35634, loss = 2.77233136\n",
      "Iteration 35635, loss = 3.00142206\n",
      "Iteration 35636, loss = 2.63650082\n",
      "Iteration 35637, loss = 2.43813618\n",
      "Iteration 35638, loss = 2.63588835\n",
      "Iteration 35639, loss = 3.21768711\n",
      "Iteration 35640, loss = 2.79319189\n",
      "Iteration 35641, loss = 3.25936379\n",
      "Iteration 35642, loss = 3.32627074\n",
      "Iteration 35643, loss = 3.61800053\n",
      "Iteration 35644, loss = 2.69480627\n",
      "Iteration 35645, loss = 3.55194183\n",
      "Iteration 35646, loss = 3.79852494\n",
      "Iteration 35647, loss = 3.35156126\n",
      "Iteration 35648, loss = 3.25513938\n",
      "Iteration 35649, loss = 2.54524466\n",
      "Iteration 35650, loss = 2.79221452\n",
      "Iteration 35651, loss = 2.46765208\n",
      "Iteration 35652, loss = 2.65316214\n",
      "Iteration 35653, loss = 2.56661971\n",
      "Iteration 35654, loss = 2.91558241\n",
      "Iteration 35655, loss = 2.55753139\n",
      "Iteration 35656, loss = 2.59200170\n",
      "Iteration 35657, loss = 2.61342312\n",
      "Iteration 35658, loss = 3.32047968\n",
      "Iteration 35659, loss = 2.89405072\n",
      "Iteration 35660, loss = 3.33301283\n",
      "Iteration 35661, loss = 3.62628668\n",
      "Iteration 35662, loss = 3.25978975\n",
      "Iteration 35663, loss = 3.42358306\n",
      "Iteration 35664, loss = 3.00084016\n",
      "Iteration 35665, loss = 3.50586217\n",
      "Iteration 35666, loss = 2.66143049\n",
      "Iteration 35667, loss = 2.95497836\n",
      "Iteration 35668, loss = 2.69250007\n",
      "Iteration 35669, loss = 2.60839333\n",
      "Iteration 35670, loss = 2.34558148\n",
      "Iteration 35671, loss = 2.53566463\n",
      "Iteration 35672, loss = 2.52931861\n",
      "Iteration 35673, loss = 2.51255483\n",
      "Iteration 35674, loss = 2.61076900\n",
      "Iteration 35675, loss = 2.41296101\n",
      "Iteration 35676, loss = 2.67425917\n",
      "Iteration 35677, loss = 2.85161647\n",
      "Iteration 35678, loss = 2.87990733\n",
      "Iteration 35679, loss = 3.01219875\n",
      "Iteration 35680, loss = 4.63085283\n",
      "Iteration 35681, loss = 4.55013068\n",
      "Iteration 35682, loss = 7.66664701\n",
      "Iteration 35683, loss = 7.12349167\n",
      "Iteration 35684, loss = 5.88938532\n",
      "Iteration 35685, loss = 4.24306237\n",
      "Iteration 35686, loss = 5.38508509\n",
      "Iteration 35687, loss = 4.30405286\n",
      "Iteration 35688, loss = 3.74613550\n",
      "Iteration 35689, loss = 2.85329281\n",
      "Iteration 35690, loss = 3.10732914\n",
      "Iteration 35691, loss = 3.24211537\n",
      "Iteration 35692, loss = 2.66678405\n",
      "Iteration 35693, loss = 2.62183228\n",
      "Iteration 35694, loss = 2.38125389\n",
      "Iteration 35695, loss = 2.51129625\n",
      "Iteration 35696, loss = 2.52606092\n",
      "Iteration 35697, loss = 2.67902571\n",
      "Iteration 35698, loss = 2.56079572\n",
      "Iteration 35699, loss = 2.73713080\n",
      "Iteration 35700, loss = 2.77236264\n",
      "Iteration 35701, loss = 2.88684519\n",
      "Iteration 35702, loss = 2.92190697\n",
      "Iteration 35703, loss = 2.50995065\n",
      "Iteration 35704, loss = 2.90423133\n",
      "Iteration 35705, loss = 3.13773649\n",
      "Iteration 35706, loss = 2.60980000\n",
      "Iteration 35707, loss = 2.59343145\n",
      "Iteration 35708, loss = 2.46559060\n",
      "Iteration 35709, loss = 2.50703893\n",
      "Iteration 35710, loss = 2.65723645\n",
      "Iteration 35711, loss = 2.93042573\n",
      "Iteration 35712, loss = 2.91637286\n",
      "Iteration 35713, loss = 2.77938597\n",
      "Iteration 35714, loss = 2.94868978\n",
      "Iteration 35715, loss = 2.36831597\n",
      "Iteration 35716, loss = 2.55206205\n",
      "Iteration 35717, loss = 2.58246424\n",
      "Iteration 35718, loss = 2.59448819\n",
      "Iteration 35719, loss = 3.18669486\n",
      "Iteration 35720, loss = 2.74958241\n",
      "Iteration 35721, loss = 2.53932604\n",
      "Iteration 35722, loss = 2.42102818\n",
      "Iteration 35723, loss = 3.02258329\n",
      "Iteration 35724, loss = 3.05463122\n",
      "Iteration 35725, loss = 2.95925227\n",
      "Iteration 35726, loss = 2.91757383\n",
      "Iteration 35727, loss = 3.16671703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 35728, loss = 3.20187573\n",
      "Iteration 35729, loss = 2.90557327\n",
      "Iteration 35730, loss = 2.68077173\n",
      "Iteration 35731, loss = 2.82722722\n",
      "Iteration 35732, loss = 2.61414077\n",
      "Iteration 35733, loss = 2.91467727\n",
      "Iteration 35734, loss = 2.63920690\n",
      "Iteration 35735, loss = 2.78608196\n",
      "Iteration 35736, loss = 2.44364713\n",
      "Iteration 35737, loss = 3.12887790\n",
      "Iteration 35738, loss = 2.97622932\n",
      "Iteration 35739, loss = 3.01561060\n",
      "Iteration 35740, loss = 2.93546450\n",
      "Iteration 35741, loss = 2.70348223\n",
      "Iteration 35742, loss = 2.83594117\n",
      "Iteration 35743, loss = 2.98337500\n",
      "Iteration 35744, loss = 3.78623187\n",
      "Iteration 35745, loss = 3.54860738\n",
      "Iteration 35746, loss = 3.68314982\n",
      "Iteration 35747, loss = 3.59066999\n",
      "Iteration 35748, loss = 3.22221110\n",
      "Iteration 35749, loss = 2.69644454\n",
      "Iteration 35750, loss = 2.57490272\n",
      "Iteration 35751, loss = 2.85249529\n",
      "Iteration 35752, loss = 2.52506314\n",
      "Iteration 35753, loss = 2.51764014\n",
      "Iteration 35754, loss = 2.67457993\n",
      "Iteration 35755, loss = 2.53982327\n",
      "Iteration 35756, loss = 3.35369539\n",
      "Iteration 35757, loss = 2.79384811\n",
      "Iteration 35758, loss = 2.48594267\n",
      "Iteration 35759, loss = 2.35167023\n",
      "Iteration 35760, loss = 2.86463567\n",
      "Iteration 35761, loss = 2.56223607\n",
      "Iteration 35762, loss = 2.46979196\n",
      "Iteration 35763, loss = 2.97320663\n",
      "Iteration 35764, loss = 2.87285924\n",
      "Iteration 35765, loss = 2.90772553\n",
      "Iteration 35766, loss = 3.07576951\n",
      "Iteration 35767, loss = 2.78807637\n",
      "Iteration 35768, loss = 2.92274572\n",
      "Iteration 35769, loss = 3.28897933\n",
      "Iteration 35770, loss = 3.31529256\n",
      "Iteration 35771, loss = 2.69564072\n",
      "Iteration 35772, loss = 3.15110107\n",
      "Iteration 35773, loss = 2.83803808\n",
      "Iteration 35774, loss = 2.98449863\n",
      "Iteration 35775, loss = 2.97674123\n",
      "Iteration 35776, loss = 2.93947720\n",
      "Iteration 35777, loss = 2.74747028\n",
      "Iteration 35778, loss = 2.74369835\n",
      "Iteration 35779, loss = 3.28876072\n",
      "Iteration 35780, loss = 2.71611370\n",
      "Iteration 35781, loss = 3.28125151\n",
      "Iteration 35782, loss = 2.91565509\n",
      "Iteration 35783, loss = 3.64606333\n",
      "Iteration 35784, loss = 2.64447447\n",
      "Iteration 35785, loss = 2.44925893\n",
      "Iteration 35786, loss = 3.02824538\n",
      "Iteration 35787, loss = 3.18972975\n",
      "Iteration 35788, loss = 3.01982592\n",
      "Iteration 35789, loss = 2.62390557\n",
      "Iteration 35790, loss = 2.60566832\n",
      "Iteration 35791, loss = 2.60207298\n",
      "Iteration 35792, loss = 2.88748803\n",
      "Iteration 35793, loss = 2.86167506\n",
      "Iteration 35794, loss = 2.31834951\n",
      "Iteration 35795, loss = 2.80873235\n",
      "Iteration 35796, loss = 3.25290602\n",
      "Iteration 35797, loss = 3.28558900\n",
      "Iteration 35798, loss = 3.59294717\n",
      "Iteration 35799, loss = 3.53659190\n",
      "Iteration 35800, loss = 2.97947734\n",
      "Iteration 35801, loss = 2.49823015\n",
      "Iteration 35802, loss = 2.54085260\n",
      "Iteration 35803, loss = 2.46717264\n",
      "Iteration 35804, loss = 2.53524066\n",
      "Iteration 35805, loss = 3.05575944\n",
      "Iteration 35806, loss = 3.09168093\n",
      "Iteration 35807, loss = 3.22615497\n",
      "Iteration 35808, loss = 2.45613005\n",
      "Iteration 35809, loss = 2.74224968\n",
      "Iteration 35810, loss = 2.40092944\n",
      "Iteration 35811, loss = 2.73618513\n",
      "Iteration 35812, loss = 4.03574716\n",
      "Iteration 35813, loss = 3.40418376\n",
      "Iteration 35814, loss = 3.58643750\n",
      "Iteration 35815, loss = 3.42318948\n",
      "Iteration 35816, loss = 3.17466849\n",
      "Iteration 35817, loss = 3.46188921\n",
      "Iteration 35818, loss = 3.35275309\n",
      "Iteration 35819, loss = 3.15825184\n",
      "Iteration 35820, loss = 2.89578664\n",
      "Iteration 35821, loss = 3.23969262\n",
      "Iteration 35822, loss = 3.01733686\n",
      "Iteration 35823, loss = 3.02852614\n",
      "Iteration 35824, loss = 3.34199723\n",
      "Iteration 35825, loss = 3.86195395\n",
      "Iteration 35826, loss = 2.95611038\n",
      "Iteration 35827, loss = 3.25427114\n",
      "Iteration 35828, loss = 2.88505666\n",
      "Iteration 35829, loss = 2.60588489\n",
      "Iteration 35830, loss = 2.39217373\n",
      "Iteration 35831, loss = 2.74862721\n",
      "Iteration 35832, loss = 2.96316858\n",
      "Iteration 35833, loss = 3.14459104\n",
      "Iteration 35834, loss = 3.11999312\n",
      "Iteration 35835, loss = 3.35136185\n",
      "Iteration 35836, loss = 4.66572742\n",
      "Iteration 35837, loss = 3.76845666\n",
      "Iteration 35838, loss = 3.41792785\n",
      "Iteration 35839, loss = 2.93874153\n",
      "Iteration 35840, loss = 3.01710685\n",
      "Iteration 35841, loss = 2.64082146\n",
      "Iteration 35842, loss = 2.99832204\n",
      "Iteration 35843, loss = 3.14323915\n",
      "Iteration 35844, loss = 2.97854404\n",
      "Iteration 35845, loss = 2.56979372\n",
      "Iteration 35846, loss = 2.39053768\n",
      "Iteration 35847, loss = 2.78667329\n",
      "Iteration 35848, loss = 3.40678686\n",
      "Iteration 35849, loss = 2.61771483\n",
      "Iteration 35850, loss = 2.90731129\n",
      "Iteration 35851, loss = 3.09601678\n",
      "Iteration 35852, loss = 2.62572478\n",
      "Iteration 35853, loss = 2.59705963\n",
      "Iteration 35854, loss = 2.63667603\n",
      "Iteration 35855, loss = 2.53125693\n",
      "Iteration 35856, loss = 3.03212108\n",
      "Iteration 35857, loss = 2.68903087\n",
      "Iteration 35858, loss = 2.68136076\n",
      "Iteration 35859, loss = 3.11988250\n",
      "Iteration 35860, loss = 2.88887366\n",
      "Iteration 35861, loss = 2.90648849\n",
      "Iteration 35862, loss = 3.30052172\n",
      "Iteration 35863, loss = 4.89130050\n",
      "Iteration 35864, loss = 4.00736331\n",
      "Iteration 35865, loss = 3.20136146\n",
      "Iteration 35866, loss = 3.80541690\n",
      "Iteration 35867, loss = 3.21400735\n",
      "Iteration 35868, loss = 3.16851940\n",
      "Iteration 35869, loss = 3.45372289\n",
      "Iteration 35870, loss = 3.10699358\n",
      "Iteration 35871, loss = 3.11602501\n",
      "Iteration 35872, loss = 2.80096510\n",
      "Iteration 35873, loss = 2.99067457\n",
      "Iteration 35874, loss = 2.54976545\n",
      "Iteration 35875, loss = 2.77941633\n",
      "Iteration 35876, loss = 3.65840662\n",
      "Iteration 35877, loss = 2.76914145\n",
      "Iteration 35878, loss = 3.29602498\n",
      "Iteration 35879, loss = 2.83150403\n",
      "Iteration 35880, loss = 3.18334200\n",
      "Iteration 35881, loss = 2.86125050\n",
      "Iteration 35882, loss = 4.44653610\n",
      "Iteration 35883, loss = 3.92977732\n",
      "Iteration 35884, loss = 3.38329945\n",
      "Iteration 35885, loss = 2.70907753\n",
      "Iteration 35886, loss = 3.00855199\n",
      "Iteration 35887, loss = 2.69047557\n",
      "Iteration 35888, loss = 2.46316680\n",
      "Iteration 35889, loss = 2.37562107\n",
      "Iteration 35890, loss = 2.64996300\n",
      "Iteration 35891, loss = 3.01647869\n",
      "Iteration 35892, loss = 2.76998764\n",
      "Iteration 35893, loss = 2.59207195\n",
      "Iteration 35894, loss = 2.45473945\n",
      "Iteration 35895, loss = 2.78599645\n",
      "Iteration 35896, loss = 2.45058262\n",
      "Iteration 35897, loss = 2.97790510\n",
      "Iteration 35898, loss = 3.76624078\n",
      "Iteration 35899, loss = 3.07916996\n",
      "Iteration 35900, loss = 3.50622395\n",
      "Iteration 35901, loss = 4.27270237\n",
      "Iteration 35902, loss = 2.93435982\n",
      "Iteration 35903, loss = 2.70832861\n",
      "Iteration 35904, loss = 2.65410118\n",
      "Iteration 35905, loss = 2.60400122\n",
      "Iteration 35906, loss = 2.70058961\n",
      "Iteration 35907, loss = 2.57549759\n",
      "Iteration 35908, loss = 2.52257218\n",
      "Iteration 35909, loss = 3.03134528\n",
      "Iteration 35910, loss = 3.08295817\n",
      "Iteration 35911, loss = 2.91873688\n",
      "Iteration 35912, loss = 2.77639502\n",
      "Iteration 35913, loss = 2.45289658\n",
      "Iteration 35914, loss = 2.36196904\n",
      "Iteration 35915, loss = 2.52860391\n",
      "Iteration 35916, loss = 2.27100150\n",
      "Iteration 35917, loss = 2.29380854\n",
      "Iteration 35918, loss = 2.76967215\n",
      "Iteration 35919, loss = 3.06126366\n",
      "Iteration 35920, loss = 2.75833520\n",
      "Iteration 35921, loss = 2.33887102\n",
      "Iteration 35922, loss = 2.87135729\n",
      "Iteration 35923, loss = 2.66768001\n",
      "Iteration 35924, loss = 2.73995795\n",
      "Iteration 35925, loss = 2.41092068\n",
      "Iteration 35926, loss = 2.54152931\n",
      "Iteration 35927, loss = 2.67202451\n",
      "Iteration 35928, loss = 3.07627237\n",
      "Iteration 35929, loss = 3.17127639\n",
      "Iteration 35930, loss = 3.93724050\n",
      "Iteration 35931, loss = 4.45388645\n",
      "Iteration 35932, loss = 3.12301039\n",
      "Iteration 35933, loss = 2.84259891\n",
      "Iteration 35934, loss = 3.53249331\n",
      "Iteration 35935, loss = 3.57107142\n",
      "Iteration 35936, loss = 2.86755155\n",
      "Iteration 35937, loss = 2.72313831\n",
      "Iteration 35938, loss = 2.83617041\n",
      "Iteration 35939, loss = 2.91760238\n",
      "Iteration 35940, loss = 3.09752889\n",
      "Iteration 35941, loss = 2.91947435\n",
      "Iteration 35942, loss = 4.05665579\n",
      "Iteration 35943, loss = 3.88484885\n",
      "Iteration 35944, loss = 6.49618427\n",
      "Iteration 35945, loss = 5.37685428\n",
      "Iteration 35946, loss = 4.29143629\n",
      "Iteration 35947, loss = 4.03582013\n",
      "Iteration 35948, loss = 2.65594970\n",
      "Iteration 35949, loss = 3.51469368\n",
      "Iteration 35950, loss = 3.48238689\n",
      "Iteration 35951, loss = 3.22007751\n",
      "Iteration 35952, loss = 3.44025337\n",
      "Iteration 35953, loss = 4.08081461\n",
      "Iteration 35954, loss = 4.99235766\n",
      "Iteration 35955, loss = 5.46297158\n",
      "Iteration 35956, loss = 4.03580185\n",
      "Iteration 35957, loss = 3.57996032\n",
      "Iteration 35958, loss = 3.44285493\n",
      "Iteration 35959, loss = 3.15241387\n",
      "Iteration 35960, loss = 3.77544674\n",
      "Iteration 35961, loss = 4.22073371\n",
      "Iteration 35962, loss = 3.39308380\n",
      "Iteration 35963, loss = 3.30259843\n",
      "Iteration 35964, loss = 2.68453385\n",
      "Iteration 35965, loss = 2.74041636\n",
      "Iteration 35966, loss = 2.81290709\n",
      "Iteration 35967, loss = 2.44351021\n",
      "Iteration 35968, loss = 3.37823632\n",
      "Iteration 35969, loss = 2.50265899\n",
      "Iteration 35970, loss = 2.28728052\n",
      "Iteration 35971, loss = 2.45874275\n",
      "Iteration 35972, loss = 2.44274814\n",
      "Iteration 35973, loss = 2.24800416\n",
      "Iteration 35974, loss = 2.33210648\n",
      "Iteration 35975, loss = 2.33880919\n",
      "Iteration 35976, loss = 2.38065737\n",
      "Iteration 35977, loss = 2.29682640\n",
      "Iteration 35978, loss = 2.45378327\n",
      "Iteration 35979, loss = 2.64854217\n",
      "Iteration 35980, loss = 2.87348082\n",
      "Iteration 35981, loss = 3.56200371\n",
      "Iteration 35982, loss = 3.07904356\n",
      "Iteration 35983, loss = 2.64825236\n",
      "Iteration 35984, loss = 2.66398397\n",
      "Iteration 35985, loss = 2.61010850\n",
      "Iteration 35986, loss = 2.67844286\n",
      "Iteration 35987, loss = 2.76820873\n",
      "Iteration 35988, loss = 2.44948916\n",
      "Iteration 35989, loss = 2.54502816\n",
      "Iteration 35990, loss = 2.47476203\n",
      "Iteration 35991, loss = 2.74677080\n",
      "Iteration 35992, loss = 3.12414391\n",
      "Iteration 35993, loss = 2.72061406\n",
      "Iteration 35994, loss = 2.48653745\n",
      "Iteration 35995, loss = 2.93306916\n",
      "Iteration 35996, loss = 3.25138470\n",
      "Iteration 35997, loss = 3.93942528\n",
      "Iteration 35998, loss = 3.24926488\n",
      "Iteration 35999, loss = 3.06407187\n",
      "Iteration 36000, loss = 3.12317387\n",
      "Iteration 36001, loss = 2.50746401\n",
      "Iteration 36002, loss = 2.49866671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 36003, loss = 2.41945746\n",
      "Iteration 36004, loss = 2.63479252\n",
      "Iteration 36005, loss = 2.35865827\n",
      "Iteration 36006, loss = 2.56353315\n",
      "Iteration 36007, loss = 2.24049259\n",
      "Iteration 36008, loss = 2.47619928\n",
      "Iteration 36009, loss = 2.73937979\n",
      "Iteration 36010, loss = 3.32858526\n",
      "Iteration 36011, loss = 3.18707214\n",
      "Iteration 36012, loss = 3.13909489\n",
      "Iteration 36013, loss = 2.97929721\n",
      "Iteration 36014, loss = 2.92742761\n",
      "Iteration 36015, loss = 3.08597037\n",
      "Iteration 36016, loss = 2.92085990\n",
      "Iteration 36017, loss = 2.67767561\n",
      "Iteration 36018, loss = 2.72182088\n",
      "Iteration 36019, loss = 2.36111205\n",
      "Iteration 36020, loss = 2.75424666\n",
      "Iteration 36021, loss = 2.77141707\n",
      "Iteration 36022, loss = 3.38632040\n",
      "Iteration 36023, loss = 3.40397320\n",
      "Iteration 36024, loss = 2.76612984\n",
      "Iteration 36025, loss = 2.77023990\n",
      "Iteration 36026, loss = 2.72214625\n",
      "Iteration 36027, loss = 2.40790688\n",
      "Iteration 36028, loss = 2.63517650\n",
      "Iteration 36029, loss = 2.69673959\n",
      "Iteration 36030, loss = 2.47395090\n",
      "Iteration 36031, loss = 2.94284692\n",
      "Iteration 36032, loss = 2.57593019\n",
      "Iteration 36033, loss = 2.73137958\n",
      "Iteration 36034, loss = 3.63436758\n",
      "Iteration 36035, loss = 3.34027155\n",
      "Iteration 36036, loss = 3.29886983\n",
      "Iteration 36037, loss = 2.86869797\n",
      "Iteration 36038, loss = 2.78350646\n",
      "Iteration 36039, loss = 2.81893080\n",
      "Iteration 36040, loss = 2.64684502\n",
      "Iteration 36041, loss = 2.46765960\n",
      "Iteration 36042, loss = 2.61444084\n",
      "Iteration 36043, loss = 2.70179092\n",
      "Iteration 36044, loss = 2.90647783\n",
      "Iteration 36045, loss = 2.75174233\n",
      "Iteration 36046, loss = 3.32056962\n",
      "Iteration 36047, loss = 3.34113324\n",
      "Iteration 36048, loss = 2.64824720\n",
      "Iteration 36049, loss = 2.60172696\n",
      "Iteration 36050, loss = 2.53093927\n",
      "Iteration 36051, loss = 2.79907424\n",
      "Iteration 36052, loss = 2.52169599\n",
      "Iteration 36053, loss = 2.52666436\n",
      "Iteration 36054, loss = 2.92645833\n",
      "Iteration 36055, loss = 3.14867705\n",
      "Iteration 36056, loss = 3.43957693\n",
      "Iteration 36057, loss = 3.70730301\n",
      "Iteration 36058, loss = 3.52419563\n",
      "Iteration 36059, loss = 5.34910915\n",
      "Iteration 36060, loss = 5.74118034\n",
      "Iteration 36061, loss = 5.51728462\n",
      "Iteration 36062, loss = 4.16900603\n",
      "Iteration 36063, loss = 3.28810452\n",
      "Iteration 36064, loss = 2.67298957\n",
      "Iteration 36065, loss = 3.02169510\n",
      "Iteration 36066, loss = 3.07274522\n",
      "Iteration 36067, loss = 3.12898879\n",
      "Iteration 36068, loss = 2.86517081\n",
      "Iteration 36069, loss = 2.80278820\n",
      "Iteration 36070, loss = 2.81841003\n",
      "Iteration 36071, loss = 2.41692925\n",
      "Iteration 36072, loss = 2.77809296\n",
      "Iteration 36073, loss = 2.89941064\n",
      "Iteration 36074, loss = 2.97793666\n",
      "Iteration 36075, loss = 2.49606300\n",
      "Iteration 36076, loss = 2.47413550\n",
      "Iteration 36077, loss = 3.14688915\n",
      "Iteration 36078, loss = 3.17588625\n",
      "Iteration 36079, loss = 3.03553098\n",
      "Iteration 36080, loss = 2.91442627\n",
      "Iteration 36081, loss = 2.92777033\n",
      "Iteration 36082, loss = 2.72244742\n",
      "Iteration 36083, loss = 2.65372135\n",
      "Iteration 36084, loss = 2.42429583\n",
      "Iteration 36085, loss = 2.75492351\n",
      "Iteration 36086, loss = 2.52319038\n",
      "Iteration 36087, loss = 2.25308076\n",
      "Iteration 36088, loss = 2.51613340\n",
      "Iteration 36089, loss = 2.65996984\n",
      "Iteration 36090, loss = 2.69604556\n",
      "Iteration 36091, loss = 2.41379693\n",
      "Iteration 36092, loss = 2.59404601\n",
      "Iteration 36093, loss = 2.48567234\n",
      "Iteration 36094, loss = 2.60776975\n",
      "Iteration 36095, loss = 2.57663475\n",
      "Iteration 36096, loss = 2.95983963\n",
      "Iteration 36097, loss = 2.70869947\n",
      "Iteration 36098, loss = 3.41167814\n",
      "Iteration 36099, loss = 2.86415577\n",
      "Iteration 36100, loss = 3.43970237\n",
      "Iteration 36101, loss = 3.08364384\n",
      "Iteration 36102, loss = 3.35796134\n",
      "Iteration 36103, loss = 3.62358077\n",
      "Iteration 36104, loss = 2.80035870\n",
      "Iteration 36105, loss = 2.36777497\n",
      "Iteration 36106, loss = 2.44211696\n",
      "Iteration 36107, loss = 2.75397982\n",
      "Iteration 36108, loss = 2.85450015\n",
      "Iteration 36109, loss = 2.89661407\n",
      "Iteration 36110, loss = 3.61003025\n",
      "Iteration 36111, loss = 3.07974251\n",
      "Iteration 36112, loss = 2.72746666\n",
      "Iteration 36113, loss = 2.63281698\n",
      "Iteration 36114, loss = 2.30059277\n",
      "Iteration 36115, loss = 2.37902457\n",
      "Iteration 36116, loss = 2.51738269\n",
      "Iteration 36117, loss = 2.81245553\n",
      "Iteration 36118, loss = 3.16922240\n",
      "Iteration 36119, loss = 2.52312885\n",
      "Iteration 36120, loss = 2.46531920\n",
      "Iteration 36121, loss = 2.62841826\n",
      "Iteration 36122, loss = 3.66168688\n",
      "Iteration 36123, loss = 3.24741684\n",
      "Iteration 36124, loss = 3.23362256\n",
      "Iteration 36125, loss = 2.79846823\n",
      "Iteration 36126, loss = 2.85246542\n",
      "Iteration 36127, loss = 3.37317928\n",
      "Iteration 36128, loss = 2.73267236\n",
      "Iteration 36129, loss = 2.97257592\n",
      "Iteration 36130, loss = 3.03605422\n",
      "Iteration 36131, loss = 3.24289028\n",
      "Iteration 36132, loss = 2.59079672\n",
      "Iteration 36133, loss = 2.57275915\n",
      "Iteration 36134, loss = 2.89704359\n",
      "Iteration 36135, loss = 3.10791631\n",
      "Iteration 36136, loss = 2.77425624\n",
      "Iteration 36137, loss = 2.65296523\n",
      "Iteration 36138, loss = 2.49447541\n",
      "Iteration 36139, loss = 2.62698917\n",
      "Iteration 36140, loss = 2.63462497\n",
      "Iteration 36141, loss = 2.34234092\n",
      "Iteration 36142, loss = 2.66777356\n",
      "Iteration 36143, loss = 2.35705625\n",
      "Iteration 36144, loss = 2.36888182\n",
      "Iteration 36145, loss = 2.26546118\n",
      "Iteration 36146, loss = 2.34071141\n",
      "Iteration 36147, loss = 2.42923808\n",
      "Iteration 36148, loss = 2.89748953\n",
      "Iteration 36149, loss = 2.73337081\n",
      "Iteration 36150, loss = 2.73984168\n",
      "Iteration 36151, loss = 3.02233450\n",
      "Iteration 36152, loss = 3.02679089\n",
      "Iteration 36153, loss = 3.02680760\n",
      "Iteration 36154, loss = 3.02938865\n",
      "Iteration 36155, loss = 3.14945485\n",
      "Iteration 36156, loss = 3.03072911\n",
      "Iteration 36157, loss = 3.03977337\n",
      "Iteration 36158, loss = 2.73688819\n",
      "Iteration 36159, loss = 2.96718729\n",
      "Iteration 36160, loss = 3.75221218\n",
      "Iteration 36161, loss = 7.39273447\n",
      "Iteration 36162, loss = 6.84035227\n",
      "Iteration 36163, loss = 4.96675485\n",
      "Iteration 36164, loss = 4.50499879\n",
      "Iteration 36165, loss = 4.19502441\n",
      "Iteration 36166, loss = 3.52727627\n",
      "Iteration 36167, loss = 3.51474075\n",
      "Iteration 36168, loss = 2.56296666\n",
      "Iteration 36169, loss = 2.87909244\n",
      "Iteration 36170, loss = 3.28777614\n",
      "Iteration 36171, loss = 2.82008263\n",
      "Iteration 36172, loss = 2.53626893\n",
      "Iteration 36173, loss = 2.80828544\n",
      "Iteration 36174, loss = 2.74785845\n",
      "Iteration 36175, loss = 2.63813460\n",
      "Iteration 36176, loss = 2.65972162\n",
      "Iteration 36177, loss = 2.77356108\n",
      "Iteration 36178, loss = 2.39281915\n",
      "Iteration 36179, loss = 2.79089904\n",
      "Iteration 36180, loss = 2.45137144\n",
      "Iteration 36181, loss = 2.70019466\n",
      "Iteration 36182, loss = 2.83637754\n",
      "Iteration 36183, loss = 4.02222783\n",
      "Iteration 36184, loss = 3.75122938\n",
      "Iteration 36185, loss = 4.17580279\n",
      "Iteration 36186, loss = 6.57954333\n",
      "Iteration 36187, loss = 6.36984390\n",
      "Iteration 36188, loss = 4.24698317\n",
      "Iteration 36189, loss = 3.22391711\n",
      "Iteration 36190, loss = 3.22146991\n",
      "Iteration 36191, loss = 3.23471619\n",
      "Iteration 36192, loss = 3.00828504\n",
      "Iteration 36193, loss = 2.64287990\n",
      "Iteration 36194, loss = 2.99379706\n",
      "Iteration 36195, loss = 2.84413904\n",
      "Iteration 36196, loss = 3.22178763\n",
      "Iteration 36197, loss = 2.34943904\n",
      "Iteration 36198, loss = 2.82615852\n",
      "Iteration 36199, loss = 2.75969160\n",
      "Iteration 36200, loss = 2.58799926\n",
      "Iteration 36201, loss = 2.69098839\n",
      "Iteration 36202, loss = 2.40614611\n",
      "Iteration 36203, loss = 2.28894604\n",
      "Iteration 36204, loss = 2.49314327\n",
      "Iteration 36205, loss = 2.20368385\n",
      "Iteration 36206, loss = 2.54652985\n",
      "Iteration 36207, loss = 3.06638277\n",
      "Iteration 36208, loss = 3.29699735\n",
      "Iteration 36209, loss = 3.70164267\n",
      "Iteration 36210, loss = 3.20293013\n",
      "Iteration 36211, loss = 2.54122836\n",
      "Iteration 36212, loss = 2.47704558\n",
      "Iteration 36213, loss = 3.52699073\n",
      "Iteration 36214, loss = 3.58451621\n",
      "Iteration 36215, loss = 3.31389611\n",
      "Iteration 36216, loss = 3.05175722\n",
      "Iteration 36217, loss = 3.08041181\n",
      "Iteration 36218, loss = 3.20880082\n",
      "Iteration 36219, loss = 3.85103565\n",
      "Iteration 36220, loss = 2.90891317\n",
      "Iteration 36221, loss = 2.96399978\n",
      "Iteration 36222, loss = 2.78638664\n",
      "Iteration 36223, loss = 2.87874826\n",
      "Iteration 36224, loss = 2.83093334\n",
      "Iteration 36225, loss = 3.23868559\n",
      "Iteration 36226, loss = 3.67362473\n",
      "Iteration 36227, loss = 3.59495099\n",
      "Iteration 36228, loss = 2.66582834\n",
      "Iteration 36229, loss = 3.06421854\n",
      "Iteration 36230, loss = 3.51886818\n",
      "Iteration 36231, loss = 3.27391699\n",
      "Iteration 36232, loss = 3.13046520\n",
      "Iteration 36233, loss = 3.33950425\n",
      "Iteration 36234, loss = 2.88857629\n",
      "Iteration 36235, loss = 2.61728262\n",
      "Iteration 36236, loss = 2.51120089\n",
      "Iteration 36237, loss = 2.74332608\n",
      "Iteration 36238, loss = 2.36375683\n",
      "Iteration 36239, loss = 2.58646854\n",
      "Iteration 36240, loss = 2.95767306\n",
      "Iteration 36241, loss = 2.95704575\n",
      "Iteration 36242, loss = 2.78056694\n",
      "Iteration 36243, loss = 2.49318971\n",
      "Iteration 36244, loss = 2.45134250\n",
      "Iteration 36245, loss = 2.66221530\n",
      "Iteration 36246, loss = 3.14916812\n",
      "Iteration 36247, loss = 2.76274975\n",
      "Iteration 36248, loss = 2.57521986\n",
      "Iteration 36249, loss = 3.49537578\n",
      "Iteration 36250, loss = 2.48687939\n",
      "Iteration 36251, loss = 2.88843531\n",
      "Iteration 36252, loss = 2.98328054\n",
      "Iteration 36253, loss = 2.95677110\n",
      "Iteration 36254, loss = 2.90036600\n",
      "Iteration 36255, loss = 2.85586783\n",
      "Iteration 36256, loss = 3.21704994\n",
      "Iteration 36257, loss = 3.25671678\n",
      "Iteration 36258, loss = 3.42612355\n",
      "Iteration 36259, loss = 3.05749019\n",
      "Iteration 36260, loss = 2.91108947\n",
      "Iteration 36261, loss = 2.74520495\n",
      "Iteration 36262, loss = 2.55964071\n",
      "Iteration 36263, loss = 2.50359280\n",
      "Iteration 36264, loss = 2.53910470\n",
      "Iteration 36265, loss = 3.19293672\n",
      "Iteration 36266, loss = 2.74081699\n",
      "Iteration 36267, loss = 2.43176651\n",
      "Iteration 36268, loss = 3.26873096\n",
      "Iteration 36269, loss = 2.58683958\n",
      "Iteration 36270, loss = 2.60619677\n",
      "Iteration 36271, loss = 2.66851816\n",
      "Iteration 36272, loss = 2.89447140\n",
      "Iteration 36273, loss = 2.37950009\n",
      "Iteration 36274, loss = 2.99601857\n",
      "Iteration 36275, loss = 3.00131655\n",
      "Iteration 36276, loss = 3.49462296\n",
      "Iteration 36277, loss = 2.86461851\n",
      "Iteration 36278, loss = 2.98345640\n",
      "Iteration 36279, loss = 3.21198411\n",
      "Iteration 36280, loss = 2.62818458\n",
      "Iteration 36281, loss = 2.37326063\n",
      "Iteration 36282, loss = 2.16827042\n",
      "Iteration 36283, loss = 2.23001541\n",
      "Iteration 36284, loss = 2.17572242\n",
      "Iteration 36285, loss = 2.46612167\n",
      "Iteration 36286, loss = 2.54568573\n",
      "Iteration 36287, loss = 3.05383991\n",
      "Iteration 36288, loss = 3.00712990\n",
      "Iteration 36289, loss = 3.82615922\n",
      "Iteration 36290, loss = 3.03207201\n",
      "Iteration 36291, loss = 2.98114904\n",
      "Iteration 36292, loss = 3.77952265\n",
      "Iteration 36293, loss = 3.09579491\n",
      "Iteration 36294, loss = 2.42269705\n",
      "Iteration 36295, loss = 2.94422288\n",
      "Iteration 36296, loss = 2.78763140\n",
      "Iteration 36297, loss = 3.32976733\n",
      "Iteration 36298, loss = 3.57853268\n",
      "Iteration 36299, loss = 3.04943829\n",
      "Iteration 36300, loss = 3.07487596\n",
      "Iteration 36301, loss = 2.73405081\n",
      "Iteration 36302, loss = 3.02227346\n",
      "Iteration 36303, loss = 3.14280168\n",
      "Iteration 36304, loss = 2.49964597\n",
      "Iteration 36305, loss = 2.71322980\n",
      "Iteration 36306, loss = 2.74596260\n",
      "Iteration 36307, loss = 3.89815507\n",
      "Iteration 36308, loss = 3.10588941\n",
      "Iteration 36309, loss = 2.99874359\n",
      "Iteration 36310, loss = 3.46073635\n",
      "Iteration 36311, loss = 3.36058507\n",
      "Iteration 36312, loss = 3.66588564\n",
      "Iteration 36313, loss = 2.83881678\n",
      "Iteration 36314, loss = 3.02794124\n",
      "Iteration 36315, loss = 2.31989830\n",
      "Iteration 36316, loss = 2.61326645\n",
      "Iteration 36317, loss = 2.83912929\n",
      "Iteration 36318, loss = 2.75702080\n",
      "Iteration 36319, loss = 3.08273339\n",
      "Iteration 36320, loss = 2.96745710\n",
      "Iteration 36321, loss = 2.67300190\n",
      "Iteration 36322, loss = 2.55376727\n",
      "Iteration 36323, loss = 3.00007143\n",
      "Iteration 36324, loss = 2.52083058\n",
      "Iteration 36325, loss = 2.84029389\n",
      "Iteration 36326, loss = 2.79055613\n",
      "Iteration 36327, loss = 3.11063847\n",
      "Iteration 36328, loss = 3.07511268\n",
      "Iteration 36329, loss = 2.64424015\n",
      "Iteration 36330, loss = 2.76559262\n",
      "Iteration 36331, loss = 2.91025833\n",
      "Iteration 36332, loss = 2.54053990\n",
      "Iteration 36333, loss = 2.25197834\n",
      "Iteration 36334, loss = 2.33327973\n",
      "Iteration 36335, loss = 2.83924871\n",
      "Iteration 36336, loss = 3.30364141\n",
      "Iteration 36337, loss = 3.35285389\n",
      "Iteration 36338, loss = 3.40310792\n",
      "Iteration 36339, loss = 4.05590515\n",
      "Iteration 36340, loss = 3.34694387\n",
      "Iteration 36341, loss = 4.95967871\n",
      "Iteration 36342, loss = 6.06231709\n",
      "Iteration 36343, loss = 4.18818389\n",
      "Iteration 36344, loss = 3.07527667\n",
      "Iteration 36345, loss = 2.96385877\n",
      "Iteration 36346, loss = 3.58080941\n",
      "Iteration 36347, loss = 3.38870714\n",
      "Iteration 36348, loss = 2.69183612\n",
      "Iteration 36349, loss = 3.02522203\n",
      "Iteration 36350, loss = 2.75468921\n",
      "Iteration 36351, loss = 2.65784019\n",
      "Iteration 36352, loss = 2.32723199\n",
      "Iteration 36353, loss = 2.30763360\n",
      "Iteration 36354, loss = 2.32979620\n",
      "Iteration 36355, loss = 2.40421740\n",
      "Iteration 36356, loss = 2.78619405\n",
      "Iteration 36357, loss = 2.98399633\n",
      "Iteration 36358, loss = 2.65949507\n",
      "Iteration 36359, loss = 3.14133077\n",
      "Iteration 36360, loss = 3.39051331\n",
      "Iteration 36361, loss = 3.43164140\n",
      "Iteration 36362, loss = 2.77633811\n",
      "Iteration 36363, loss = 3.36159046\n",
      "Iteration 36364, loss = 3.72146105\n",
      "Iteration 36365, loss = 3.38688088\n",
      "Iteration 36366, loss = 3.63249788\n",
      "Iteration 36367, loss = 3.64847110\n",
      "Iteration 36368, loss = 3.35149790\n",
      "Iteration 36369, loss = 3.06706232\n",
      "Iteration 36370, loss = 3.32334842\n",
      "Iteration 36371, loss = 6.65597767\n",
      "Iteration 36372, loss = 6.49406326\n",
      "Iteration 36373, loss = 6.79295978\n",
      "Iteration 36374, loss = 6.96826715\n",
      "Iteration 36375, loss = 4.34512096\n",
      "Iteration 36376, loss = 4.55067591\n",
      "Iteration 36377, loss = 3.56739819\n",
      "Iteration 36378, loss = 3.45816750\n",
      "Iteration 36379, loss = 3.01977075\n",
      "Iteration 36380, loss = 2.64319780\n",
      "Iteration 36381, loss = 2.62491816\n",
      "Iteration 36382, loss = 2.68576543\n",
      "Iteration 36383, loss = 2.35853359\n",
      "Iteration 36384, loss = 2.69896827\n",
      "Iteration 36385, loss = 2.83228595\n",
      "Iteration 36386, loss = 3.00140799\n",
      "Iteration 36387, loss = 2.89696587\n",
      "Iteration 36388, loss = 2.65304965\n",
      "Iteration 36389, loss = 3.26082146\n",
      "Iteration 36390, loss = 3.32495757\n",
      "Iteration 36391, loss = 3.28870475\n",
      "Iteration 36392, loss = 2.49600793\n",
      "Iteration 36393, loss = 3.00988465\n",
      "Iteration 36394, loss = 3.03959464\n",
      "Iteration 36395, loss = 2.61924622\n",
      "Iteration 36396, loss = 2.97588926\n",
      "Iteration 36397, loss = 2.28151799\n",
      "Iteration 36398, loss = 2.67404919\n",
      "Iteration 36399, loss = 2.39944037\n",
      "Iteration 36400, loss = 2.30954063\n",
      "Iteration 36401, loss = 2.85538926\n",
      "Iteration 36402, loss = 2.53639368\n",
      "Iteration 36403, loss = 2.55475758\n",
      "Iteration 36404, loss = 2.27191434\n",
      "Iteration 36405, loss = 2.40259233\n",
      "Iteration 36406, loss = 2.34910656\n",
      "Iteration 36407, loss = 2.37203610\n",
      "Iteration 36408, loss = 2.57180000\n",
      "Iteration 36409, loss = 2.55776268\n",
      "Iteration 36410, loss = 2.66877101\n",
      "Iteration 36411, loss = 2.37238243\n",
      "Iteration 36412, loss = 2.70190343\n",
      "Iteration 36413, loss = 3.62280081\n",
      "Iteration 36414, loss = 4.64449301\n",
      "Iteration 36415, loss = 2.77962674\n",
      "Iteration 36416, loss = 2.56842837\n",
      "Iteration 36417, loss = 2.85937003\n",
      "Iteration 36418, loss = 2.81986028\n",
      "Iteration 36419, loss = 2.24431079\n",
      "Iteration 36420, loss = 2.23320116\n",
      "Iteration 36421, loss = 2.24757245\n",
      "Iteration 36422, loss = 2.16393364\n",
      "Iteration 36423, loss = 2.28064671\n",
      "Iteration 36424, loss = 2.86011397\n",
      "Iteration 36425, loss = 2.98056318\n",
      "Iteration 36426, loss = 2.84218949\n",
      "Iteration 36427, loss = 2.33410069\n",
      "Iteration 36428, loss = 2.45126226\n",
      "Iteration 36429, loss = 2.76046115\n",
      "Iteration 36430, loss = 3.88841677\n",
      "Iteration 36431, loss = 3.91635135\n",
      "Iteration 36432, loss = 2.94502115\n",
      "Iteration 36433, loss = 3.06457446\n",
      "Iteration 36434, loss = 2.74189960\n",
      "Iteration 36435, loss = 2.96344221\n",
      "Iteration 36436, loss = 2.60396520\n",
      "Iteration 36437, loss = 2.30746670\n",
      "Iteration 36438, loss = 2.46691970\n",
      "Iteration 36439, loss = 2.38798783\n",
      "Iteration 36440, loss = 2.26810174\n",
      "Iteration 36441, loss = 2.26457395\n",
      "Iteration 36442, loss = 2.52929436\n",
      "Iteration 36443, loss = 2.28991210\n",
      "Iteration 36444, loss = 2.33343512\n",
      "Iteration 36445, loss = 2.28369802\n",
      "Iteration 36446, loss = 2.39048241\n",
      "Iteration 36447, loss = 2.44054009\n",
      "Iteration 36448, loss = 2.28175041\n",
      "Iteration 36449, loss = 2.26610212\n",
      "Iteration 36450, loss = 2.37526391\n",
      "Iteration 36451, loss = 2.13202391\n",
      "Iteration 36452, loss = 2.21264717\n",
      "Iteration 36453, loss = 2.39599539\n",
      "Iteration 36454, loss = 2.48210948\n",
      "Iteration 36455, loss = 2.73227624\n",
      "Iteration 36456, loss = 2.59478722\n",
      "Iteration 36457, loss = 2.69644460\n",
      "Iteration 36458, loss = 2.99886469\n",
      "Iteration 36459, loss = 3.25257047\n",
      "Iteration 36460, loss = 3.08830102\n",
      "Iteration 36461, loss = 3.12478494\n",
      "Iteration 36462, loss = 2.54001462\n",
      "Iteration 36463, loss = 2.84329756\n",
      "Iteration 36464, loss = 3.14849744\n",
      "Iteration 36465, loss = 3.08801356\n",
      "Iteration 36466, loss = 2.90875837\n",
      "Iteration 36467, loss = 2.84285604\n",
      "Iteration 36468, loss = 2.80560539\n",
      "Iteration 36469, loss = 2.93658548\n",
      "Iteration 36470, loss = 2.83327210\n",
      "Iteration 36471, loss = 3.10816232\n",
      "Iteration 36472, loss = 3.35670273\n",
      "Iteration 36473, loss = 3.52424765\n",
      "Iteration 36474, loss = 3.53486999\n",
      "Iteration 36475, loss = 3.03461209\n",
      "Iteration 36476, loss = 3.35258615\n",
      "Iteration 36477, loss = 2.92092906\n",
      "Iteration 36478, loss = 3.03661312\n",
      "Iteration 36479, loss = 2.41488059\n",
      "Iteration 36480, loss = 2.64444216\n",
      "Iteration 36481, loss = 2.58439475\n",
      "Iteration 36482, loss = 2.38310351\n",
      "Iteration 36483, loss = 2.39264202\n",
      "Iteration 36484, loss = 2.46860278\n",
      "Iteration 36485, loss = 2.28831489\n",
      "Iteration 36486, loss = 2.23497145\n",
      "Iteration 36487, loss = 2.53794616\n",
      "Iteration 36488, loss = 2.42509988\n",
      "Iteration 36489, loss = 2.29324275\n",
      "Iteration 36490, loss = 2.90046189\n",
      "Iteration 36491, loss = 2.92438578\n",
      "Iteration 36492, loss = 2.55158670\n",
      "Iteration 36493, loss = 3.19931422\n",
      "Iteration 36494, loss = 3.41416165\n",
      "Iteration 36495, loss = 3.16744986\n",
      "Iteration 36496, loss = 2.72586472\n",
      "Iteration 36497, loss = 2.38109349\n",
      "Iteration 36498, loss = 3.15414485\n",
      "Iteration 36499, loss = 3.09882550\n",
      "Iteration 36500, loss = 3.74727359\n",
      "Iteration 36501, loss = 3.94398042\n",
      "Iteration 36502, loss = 2.63173720\n",
      "Iteration 36503, loss = 3.10518857\n",
      "Iteration 36504, loss = 4.85040837\n",
      "Iteration 36505, loss = 5.27270155\n",
      "Iteration 36506, loss = 5.54047785\n",
      "Iteration 36507, loss = 3.52238569\n",
      "Iteration 36508, loss = 3.10329566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 36509, loss = 2.98371896\n",
      "Iteration 36510, loss = 2.95852458\n",
      "Iteration 36511, loss = 2.80334218\n",
      "Iteration 36512, loss = 2.92159923\n",
      "Iteration 36513, loss = 2.71264921\n",
      "Iteration 36514, loss = 2.80379413\n",
      "Iteration 36515, loss = 2.86254890\n",
      "Iteration 36516, loss = 3.06748598\n",
      "Iteration 36517, loss = 2.80948033\n",
      "Iteration 36518, loss = 2.73803252\n",
      "Iteration 36519, loss = 4.15773644\n",
      "Iteration 36520, loss = 3.92576032\n",
      "Iteration 36521, loss = 3.51995730\n",
      "Iteration 36522, loss = 3.40983800\n",
      "Iteration 36523, loss = 2.86949792\n",
      "Iteration 36524, loss = 2.84276731\n",
      "Iteration 36525, loss = 2.66390931\n",
      "Iteration 36526, loss = 2.41311023\n",
      "Iteration 36527, loss = 2.34436978\n",
      "Iteration 36528, loss = 2.23991558\n",
      "Iteration 36529, loss = 2.55135022\n",
      "Iteration 36530, loss = 2.75592746\n",
      "Iteration 36531, loss = 2.87406332\n",
      "Iteration 36532, loss = 2.52917412\n",
      "Iteration 36533, loss = 3.00255404\n",
      "Iteration 36534, loss = 3.15179223\n",
      "Iteration 36535, loss = 2.91059685\n",
      "Iteration 36536, loss = 2.36722893\n",
      "Iteration 36537, loss = 2.38723336\n",
      "Iteration 36538, loss = 2.42513864\n",
      "Iteration 36539, loss = 2.29912229\n",
      "Iteration 36540, loss = 2.23485152\n",
      "Iteration 36541, loss = 2.59915237\n",
      "Iteration 36542, loss = 2.82149472\n",
      "Iteration 36543, loss = 2.88580936\n",
      "Iteration 36544, loss = 2.74620430\n",
      "Iteration 36545, loss = 2.37472158\n",
      "Iteration 36546, loss = 2.44805804\n",
      "Iteration 36547, loss = 2.40667794\n",
      "Iteration 36548, loss = 2.58249409\n",
      "Iteration 36549, loss = 2.86583170\n",
      "Iteration 36550, loss = 2.36240645\n",
      "Iteration 36551, loss = 2.63709071\n",
      "Iteration 36552, loss = 2.70164421\n",
      "Iteration 36553, loss = 2.17178747\n",
      "Iteration 36554, loss = 2.59313426\n",
      "Iteration 36555, loss = 2.69760775\n",
      "Iteration 36556, loss = 2.92831616\n",
      "Iteration 36557, loss = 2.55521620\n",
      "Iteration 36558, loss = 2.42443798\n",
      "Iteration 36559, loss = 2.44648566\n",
      "Iteration 36560, loss = 2.71886119\n",
      "Iteration 36561, loss = 2.94691674\n",
      "Iteration 36562, loss = 3.19118537\n",
      "Iteration 36563, loss = 3.83028355\n",
      "Iteration 36564, loss = 6.45569750\n",
      "Iteration 36565, loss = 6.20381954\n",
      "Iteration 36566, loss = 4.75775513\n",
      "Iteration 36567, loss = 4.29851261\n",
      "Iteration 36568, loss = 3.08619999\n",
      "Iteration 36569, loss = 3.38527810\n",
      "Iteration 36570, loss = 3.32711863\n",
      "Iteration 36571, loss = 3.67341270\n",
      "Iteration 36572, loss = 3.80249341\n",
      "Iteration 36573, loss = 3.22854560\n",
      "Iteration 36574, loss = 2.74273915\n",
      "Iteration 36575, loss = 3.12559177\n",
      "Iteration 36576, loss = 2.95933233\n",
      "Iteration 36577, loss = 3.52468043\n",
      "Iteration 36578, loss = 3.49599457\n",
      "Iteration 36579, loss = 3.36783553\n",
      "Iteration 36580, loss = 2.50618942\n",
      "Iteration 36581, loss = 2.84750771\n",
      "Iteration 36582, loss = 2.89158390\n",
      "Iteration 36583, loss = 2.44852434\n",
      "Iteration 36584, loss = 2.44991765\n",
      "Iteration 36585, loss = 2.28108055\n",
      "Iteration 36586, loss = 2.30703196\n",
      "Iteration 36587, loss = 2.81172583\n",
      "Iteration 36588, loss = 2.74965493\n",
      "Iteration 36589, loss = 2.38575311\n",
      "Iteration 36590, loss = 2.27289881\n",
      "Iteration 36591, loss = 2.22801366\n",
      "Iteration 36592, loss = 2.20429800\n",
      "Iteration 36593, loss = 2.22795725\n",
      "Iteration 36594, loss = 2.58843512\n",
      "Iteration 36595, loss = 2.62926080\n",
      "Iteration 36596, loss = 2.55203671\n",
      "Iteration 36597, loss = 2.79391103\n",
      "Iteration 36598, loss = 2.73826267\n",
      "Iteration 36599, loss = 2.98464704\n",
      "Iteration 36600, loss = 3.03404598\n",
      "Iteration 36601, loss = 2.49525859\n",
      "Iteration 36602, loss = 2.47490420\n",
      "Iteration 36603, loss = 2.59885481\n",
      "Iteration 36604, loss = 2.58727974\n",
      "Iteration 36605, loss = 2.66550887\n",
      "Iteration 36606, loss = 2.52951892\n",
      "Iteration 36607, loss = 2.45602985\n",
      "Iteration 36608, loss = 2.27447445\n",
      "Iteration 36609, loss = 2.91990665\n",
      "Iteration 36610, loss = 3.78786974\n",
      "Iteration 36611, loss = 3.48481431\n",
      "Iteration 36612, loss = 2.75175589\n",
      "Iteration 36613, loss = 4.32991025\n",
      "Iteration 36614, loss = 3.77830114\n",
      "Iteration 36615, loss = 4.93396831\n",
      "Iteration 36616, loss = 5.01983504\n",
      "Iteration 36617, loss = 4.21226723\n",
      "Iteration 36618, loss = 4.40429976\n",
      "Iteration 36619, loss = 2.90443290\n",
      "Iteration 36620, loss = 3.14705288\n",
      "Iteration 36621, loss = 2.18312782\n",
      "Iteration 36622, loss = 2.50571844\n",
      "Iteration 36623, loss = 2.53432728\n",
      "Iteration 36624, loss = 2.70641954\n",
      "Iteration 36625, loss = 4.14814175\n",
      "Iteration 36626, loss = 3.08660193\n",
      "Iteration 36627, loss = 3.44280100\n",
      "Iteration 36628, loss = 2.76731937\n",
      "Iteration 36629, loss = 2.91090325\n",
      "Iteration 36630, loss = 2.62898206\n",
      "Iteration 36631, loss = 2.73693860\n",
      "Iteration 36632, loss = 2.40167270\n",
      "Iteration 36633, loss = 2.58980985\n",
      "Iteration 36634, loss = 3.31711546\n",
      "Iteration 36635, loss = 3.23193096\n",
      "Iteration 36636, loss = 3.76321051\n",
      "Iteration 36637, loss = 3.44295659\n",
      "Iteration 36638, loss = 3.56231642\n",
      "Iteration 36639, loss = 3.33716181\n",
      "Iteration 36640, loss = 3.29022187\n",
      "Iteration 36641, loss = 3.11853220\n",
      "Iteration 36642, loss = 3.64149596\n",
      "Iteration 36643, loss = 3.73333505\n",
      "Iteration 36644, loss = 3.71084118\n",
      "Iteration 36645, loss = 4.38969758\n",
      "Iteration 36646, loss = 4.87168710\n",
      "Iteration 36647, loss = 4.30765469\n",
      "Iteration 36648, loss = 3.90486208\n",
      "Iteration 36649, loss = 3.17534614\n",
      "Iteration 36650, loss = 2.62622908\n",
      "Iteration 36651, loss = 3.09422464\n",
      "Iteration 36652, loss = 2.97958104\n",
      "Iteration 36653, loss = 3.70259331\n",
      "Iteration 36654, loss = 3.17554398\n",
      "Iteration 36655, loss = 4.89979426\n",
      "Iteration 36656, loss = 3.76129857\n",
      "Iteration 36657, loss = 3.15786257\n",
      "Iteration 36658, loss = 4.32961168\n",
      "Iteration 36659, loss = 3.64300295\n",
      "Iteration 36660, loss = 3.23770280\n",
      "Iteration 36661, loss = 2.93132227\n",
      "Iteration 36662, loss = 2.92670393\n",
      "Iteration 36663, loss = 2.50580095\n",
      "Iteration 36664, loss = 3.10624580\n",
      "Iteration 36665, loss = 2.73533446\n",
      "Iteration 36666, loss = 2.47001060\n",
      "Iteration 36667, loss = 2.44401204\n",
      "Iteration 36668, loss = 2.36805178\n",
      "Iteration 36669, loss = 2.31756322\n",
      "Iteration 36670, loss = 2.58951773\n",
      "Iteration 36671, loss = 2.50072920\n",
      "Iteration 36672, loss = 2.52733917\n",
      "Iteration 36673, loss = 2.70836041\n",
      "Iteration 36674, loss = 2.40666756\n",
      "Iteration 36675, loss = 2.30164331\n",
      "Iteration 36676, loss = 2.46878147\n",
      "Iteration 36677, loss = 2.53468044\n",
      "Iteration 36678, loss = 2.39368217\n",
      "Iteration 36679, loss = 2.21542467\n",
      "Iteration 36680, loss = 2.18279653\n",
      "Iteration 36681, loss = 2.32863703\n",
      "Iteration 36682, loss = 2.55993840\n",
      "Iteration 36683, loss = 2.42638332\n",
      "Iteration 36684, loss = 2.16898788\n",
      "Iteration 36685, loss = 2.73965018\n",
      "Iteration 36686, loss = 3.28408601\n",
      "Iteration 36687, loss = 3.22373268\n",
      "Iteration 36688, loss = 3.18775436\n",
      "Iteration 36689, loss = 2.52670842\n",
      "Iteration 36690, loss = 2.52878259\n",
      "Iteration 36691, loss = 2.39445464\n",
      "Iteration 36692, loss = 2.79486019\n",
      "Iteration 36693, loss = 2.33250001\n",
      "Iteration 36694, loss = 2.18562899\n",
      "Iteration 36695, loss = 2.39264979\n",
      "Iteration 36696, loss = 2.38891624\n",
      "Iteration 36697, loss = 2.39477969\n",
      "Iteration 36698, loss = 2.84510792\n",
      "Iteration 36699, loss = 2.22059738\n",
      "Iteration 36700, loss = 2.32523438\n",
      "Iteration 36701, loss = 2.39776292\n",
      "Iteration 36702, loss = 2.54337762\n",
      "Iteration 36703, loss = 2.58317104\n",
      "Iteration 36704, loss = 2.84401209\n",
      "Iteration 36705, loss = 2.78887291\n",
      "Iteration 36706, loss = 2.44691513\n",
      "Iteration 36707, loss = 2.50352524\n",
      "Iteration 36708, loss = 2.32998547\n",
      "Iteration 36709, loss = 2.54612669\n",
      "Iteration 36710, loss = 2.51797699\n",
      "Iteration 36711, loss = 3.55124139\n",
      "Iteration 36712, loss = 3.35629467\n",
      "Iteration 36713, loss = 2.51171338\n",
      "Iteration 36714, loss = 2.72770372\n",
      "Iteration 36715, loss = 2.45922142\n",
      "Iteration 36716, loss = 2.38020877\n",
      "Iteration 36717, loss = 2.33628294\n",
      "Iteration 36718, loss = 2.30615866\n",
      "Iteration 36719, loss = 2.39176848\n",
      "Iteration 36720, loss = 2.34505705\n",
      "Iteration 36721, loss = 3.33612702\n",
      "Iteration 36722, loss = 3.04720715\n",
      "Iteration 36723, loss = 3.49558100\n",
      "Iteration 36724, loss = 3.26074705\n",
      "Iteration 36725, loss = 2.64294168\n",
      "Iteration 36726, loss = 3.23821004\n",
      "Iteration 36727, loss = 3.93297372\n",
      "Iteration 36728, loss = 4.98321558\n",
      "Iteration 36729, loss = 5.32241930\n",
      "Iteration 36730, loss = 4.28563504\n",
      "Iteration 36731, loss = 4.05369302\n",
      "Iteration 36732, loss = 3.65911201\n",
      "Iteration 36733, loss = 3.34868177\n",
      "Iteration 36734, loss = 2.49934268\n",
      "Iteration 36735, loss = 2.74076690\n",
      "Iteration 36736, loss = 2.82411491\n",
      "Iteration 36737, loss = 4.88845208\n",
      "Iteration 36738, loss = 4.63284412\n",
      "Iteration 36739, loss = 3.96892700\n",
      "Iteration 36740, loss = 2.78757171\n",
      "Iteration 36741, loss = 2.89022512\n",
      "Iteration 36742, loss = 2.88087120\n",
      "Iteration 36743, loss = 2.89504914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 36744, loss = 2.55264582\n",
      "Iteration 36745, loss = 2.39996490\n",
      "Iteration 36746, loss = 2.41094029\n",
      "Iteration 36747, loss = 2.50313361\n",
      "Iteration 36748, loss = 2.52850310\n",
      "Iteration 36749, loss = 2.87325587\n",
      "Iteration 36750, loss = 2.51787415\n",
      "Iteration 36751, loss = 2.68086379\n",
      "Iteration 36752, loss = 3.00548869\n",
      "Iteration 36753, loss = 2.63687703\n",
      "Iteration 36754, loss = 2.26022866\n",
      "Iteration 36755, loss = 2.29744696\n",
      "Iteration 36756, loss = 2.36957446\n",
      "Iteration 36757, loss = 3.17177768\n",
      "Iteration 36758, loss = 3.05579033\n",
      "Iteration 36759, loss = 2.94162166\n",
      "Iteration 36760, loss = 2.49847380\n",
      "Iteration 36761, loss = 2.72193859\n",
      "Iteration 36762, loss = 3.69996624\n",
      "Iteration 36763, loss = 3.19078539\n",
      "Iteration 36764, loss = 2.79503599\n",
      "Iteration 36765, loss = 2.76585578\n",
      "Iteration 36766, loss = 2.60356792\n",
      "Iteration 36767, loss = 2.41760812\n",
      "Iteration 36768, loss = 2.38410756\n",
      "Iteration 36769, loss = 2.38408430\n",
      "Iteration 36770, loss = 2.52253112\n",
      "Iteration 36771, loss = 2.64631842\n",
      "Iteration 36772, loss = 2.53048126\n",
      "Iteration 36773, loss = 2.93829652\n",
      "Iteration 36774, loss = 2.83099251\n",
      "Iteration 36775, loss = 2.89901475\n",
      "Iteration 36776, loss = 3.67149708\n",
      "Iteration 36777, loss = 3.40500043\n",
      "Iteration 36778, loss = 2.60243321\n",
      "Iteration 36779, loss = 2.64849089\n",
      "Iteration 36780, loss = 2.39872989\n",
      "Iteration 36781, loss = 2.51432481\n",
      "Iteration 36782, loss = 2.60885423\n",
      "Iteration 36783, loss = 2.66575033\n",
      "Iteration 36784, loss = 2.66476915\n",
      "Iteration 36785, loss = 2.54555261\n",
      "Iteration 36786, loss = 2.38649398\n",
      "Iteration 36787, loss = 2.32511304\n",
      "Iteration 36788, loss = 2.68288922\n",
      "Iteration 36789, loss = 2.58061890\n",
      "Iteration 36790, loss = 2.21464509\n",
      "Iteration 36791, loss = 2.24720830\n",
      "Iteration 36792, loss = 2.28682683\n",
      "Iteration 36793, loss = 2.47076644\n",
      "Iteration 36794, loss = 2.77928332\n",
      "Iteration 36795, loss = 2.49089919\n",
      "Iteration 36796, loss = 2.54203211\n",
      "Iteration 36797, loss = 2.58047509\n",
      "Iteration 36798, loss = 3.31460057\n",
      "Iteration 36799, loss = 2.94303159\n",
      "Iteration 36800, loss = 2.98755211\n",
      "Iteration 36801, loss = 2.87973344\n",
      "Iteration 36802, loss = 3.04351110\n",
      "Iteration 36803, loss = 3.18572490\n",
      "Iteration 36804, loss = 2.71028068\n",
      "Iteration 36805, loss = 2.49232557\n",
      "Iteration 36806, loss = 2.62081574\n",
      "Iteration 36807, loss = 2.46635247\n",
      "Iteration 36808, loss = 3.10339837\n",
      "Iteration 36809, loss = 3.21809168\n",
      "Iteration 36810, loss = 2.98102214\n",
      "Iteration 36811, loss = 2.92054227\n",
      "Iteration 36812, loss = 2.79187613\n",
      "Iteration 36813, loss = 2.86781870\n",
      "Iteration 36814, loss = 2.38326184\n",
      "Iteration 36815, loss = 2.61344509\n",
      "Iteration 36816, loss = 2.65581679\n",
      "Iteration 36817, loss = 2.63669896\n",
      "Iteration 36818, loss = 2.95226742\n",
      "Iteration 36819, loss = 2.79519286\n",
      "Iteration 36820, loss = 2.71755179\n",
      "Iteration 36821, loss = 2.63970410\n",
      "Iteration 36822, loss = 3.02379023\n",
      "Iteration 36823, loss = 3.07188483\n",
      "Iteration 36824, loss = 2.46032311\n",
      "Iteration 36825, loss = 2.32581948\n",
      "Iteration 36826, loss = 2.65331709\n",
      "Iteration 36827, loss = 3.05466973\n",
      "Iteration 36828, loss = 2.54717040\n",
      "Iteration 36829, loss = 2.68118786\n",
      "Iteration 36830, loss = 2.34574297\n",
      "Iteration 36831, loss = 2.53351683\n",
      "Iteration 36832, loss = 2.58570275\n",
      "Iteration 36833, loss = 3.51925157\n",
      "Iteration 36834, loss = 3.27349213\n",
      "Iteration 36835, loss = 3.43043739\n",
      "Iteration 36836, loss = 2.86215552\n",
      "Iteration 36837, loss = 3.34669825\n",
      "Iteration 36838, loss = 3.98864310\n",
      "Iteration 36839, loss = 3.19998677\n",
      "Iteration 36840, loss = 2.73049766\n",
      "Iteration 36841, loss = 2.43202879\n",
      "Iteration 36842, loss = 2.78197605\n",
      "Iteration 36843, loss = 2.39601957\n",
      "Iteration 36844, loss = 3.05453390\n",
      "Iteration 36845, loss = 2.35844488\n",
      "Iteration 36846, loss = 2.54385190\n",
      "Iteration 36847, loss = 2.38472887\n",
      "Iteration 36848, loss = 2.36509767\n",
      "Iteration 36849, loss = 2.30394633\n",
      "Iteration 36850, loss = 2.38647160\n",
      "Iteration 36851, loss = 2.43176524\n",
      "Iteration 36852, loss = 2.26843824\n",
      "Iteration 36853, loss = 2.41220836\n",
      "Iteration 36854, loss = 2.28436029\n",
      "Iteration 36855, loss = 2.28748669\n",
      "Iteration 36856, loss = 2.53196181\n",
      "Iteration 36857, loss = 2.50955680\n",
      "Iteration 36858, loss = 2.55967659\n",
      "Iteration 36859, loss = 2.68712227\n",
      "Iteration 36860, loss = 2.64452329\n",
      "Iteration 36861, loss = 3.22671669\n",
      "Iteration 36862, loss = 3.13567566\n",
      "Iteration 36863, loss = 2.68204497\n",
      "Iteration 36864, loss = 3.46548162\n",
      "Iteration 36865, loss = 2.98195276\n",
      "Iteration 36866, loss = 3.43800325\n",
      "Iteration 36867, loss = 3.51254623\n",
      "Iteration 36868, loss = 3.07531074\n",
      "Iteration 36869, loss = 4.08493396\n",
      "Iteration 36870, loss = 3.93977825\n",
      "Iteration 36871, loss = 3.18513239\n",
      "Iteration 36872, loss = 3.33416317\n",
      "Iteration 36873, loss = 3.19934337\n",
      "Iteration 36874, loss = 4.19295749\n",
      "Iteration 36875, loss = 3.23544289\n",
      "Iteration 36876, loss = 2.48968107\n",
      "Iteration 36877, loss = 2.29435473\n",
      "Iteration 36878, loss = 2.24080215\n",
      "Iteration 36879, loss = 3.27703422\n",
      "Iteration 36880, loss = 3.02361781\n",
      "Iteration 36881, loss = 3.47368680\n",
      "Iteration 36882, loss = 2.92301763\n",
      "Iteration 36883, loss = 2.57448944\n",
      "Iteration 36884, loss = 3.29331585\n",
      "Iteration 36885, loss = 3.10192099\n",
      "Iteration 36886, loss = 2.62813096\n",
      "Iteration 36887, loss = 3.10741970\n",
      "Iteration 36888, loss = 2.31465486\n",
      "Iteration 36889, loss = 3.21409677\n",
      "Iteration 36890, loss = 2.53001988\n",
      "Iteration 36891, loss = 3.42029839\n",
      "Iteration 36892, loss = 2.56678707\n",
      "Iteration 36893, loss = 2.74230916\n",
      "Iteration 36894, loss = 3.62362560\n",
      "Iteration 36895, loss = 4.08952130\n",
      "Iteration 36896, loss = 4.01985002\n",
      "Iteration 36897, loss = 8.80157858\n",
      "Iteration 36898, loss = 7.59804248\n",
      "Iteration 36899, loss = 6.89779799\n",
      "Iteration 36900, loss = 4.26918775\n",
      "Iteration 36901, loss = 2.90630634\n",
      "Iteration 36902, loss = 3.47914830\n",
      "Iteration 36903, loss = 3.31423883\n",
      "Iteration 36904, loss = 3.70176150\n",
      "Iteration 36905, loss = 3.44972185\n",
      "Iteration 36906, loss = 2.89242187\n",
      "Iteration 36907, loss = 3.45191245\n",
      "Iteration 36908, loss = 3.21600712\n",
      "Iteration 36909, loss = 2.97507273\n",
      "Iteration 36910, loss = 2.76274709\n",
      "Iteration 36911, loss = 2.40733551\n",
      "Iteration 36912, loss = 2.73885283\n",
      "Iteration 36913, loss = 2.82563646\n",
      "Iteration 36914, loss = 2.57404255\n",
      "Iteration 36915, loss = 3.06137145\n",
      "Iteration 36916, loss = 3.28078291\n",
      "Iteration 36917, loss = 3.62757740\n",
      "Iteration 36918, loss = 2.92342172\n",
      "Iteration 36919, loss = 3.29186504\n",
      "Iteration 36920, loss = 3.20350267\n",
      "Iteration 36921, loss = 2.96409052\n",
      "Iteration 36922, loss = 3.74242613\n",
      "Iteration 36923, loss = 2.55871407\n",
      "Iteration 36924, loss = 2.78257789\n",
      "Iteration 36925, loss = 2.72114205\n",
      "Iteration 36926, loss = 2.61946789\n",
      "Iteration 36927, loss = 2.72446506\n",
      "Iteration 36928, loss = 2.75644544\n",
      "Iteration 36929, loss = 3.40471346\n",
      "Iteration 36930, loss = 2.80647731\n",
      "Iteration 36931, loss = 3.25778093\n",
      "Iteration 36932, loss = 3.18619945\n",
      "Iteration 36933, loss = 2.20772098\n",
      "Iteration 36934, loss = 2.35198335\n",
      "Iteration 36935, loss = 2.69063790\n",
      "Iteration 36936, loss = 2.42653676\n",
      "Iteration 36937, loss = 2.66736967\n",
      "Iteration 36938, loss = 2.50343692\n",
      "Iteration 36939, loss = 2.42571647\n",
      "Iteration 36940, loss = 3.15057088\n",
      "Iteration 36941, loss = 3.50099600\n",
      "Iteration 36942, loss = 4.57586093\n",
      "Iteration 36943, loss = 4.52269158\n",
      "Iteration 36944, loss = 2.98722202\n",
      "Iteration 36945, loss = 3.11739563\n",
      "Iteration 36946, loss = 2.97806316\n",
      "Iteration 36947, loss = 3.22615279\n",
      "Iteration 36948, loss = 2.76318715\n",
      "Iteration 36949, loss = 3.38680993\n",
      "Iteration 36950, loss = 3.59746288\n",
      "Iteration 36951, loss = 3.63574252\n",
      "Iteration 36952, loss = 3.23512851\n",
      "Iteration 36953, loss = 4.16918734\n",
      "Iteration 36954, loss = 4.87857129\n",
      "Iteration 36955, loss = 6.93226799\n",
      "Iteration 36956, loss = 5.51847339\n",
      "Iteration 36957, loss = 3.70703255\n",
      "Iteration 36958, loss = 3.86302591\n",
      "Iteration 36959, loss = 3.34426682\n",
      "Iteration 36960, loss = 3.01065904\n",
      "Iteration 36961, loss = 2.63841120\n",
      "Iteration 36962, loss = 2.38502758\n",
      "Iteration 36963, loss = 2.19674940\n",
      "Iteration 36964, loss = 2.18215349\n",
      "Iteration 36965, loss = 2.64012223\n",
      "Iteration 36966, loss = 2.22872977\n",
      "Iteration 36967, loss = 2.35685761\n",
      "Iteration 36968, loss = 3.22203091\n",
      "Iteration 36969, loss = 2.84544495\n",
      "Iteration 36970, loss = 2.96664481\n",
      "Iteration 36971, loss = 2.31171897\n",
      "Iteration 36972, loss = 3.01770782\n",
      "Iteration 36973, loss = 3.01996091\n",
      "Iteration 36974, loss = 3.26271799\n",
      "Iteration 36975, loss = 2.48153588\n",
      "Iteration 36976, loss = 2.68176282\n",
      "Iteration 36977, loss = 3.06040502\n",
      "Iteration 36978, loss = 3.01213918\n",
      "Iteration 36979, loss = 3.58383783\n",
      "Iteration 36980, loss = 3.66800653\n",
      "Iteration 36981, loss = 2.46062966\n",
      "Iteration 36982, loss = 2.90800736\n",
      "Iteration 36983, loss = 2.70065395\n",
      "Iteration 36984, loss = 2.43970869\n",
      "Iteration 36985, loss = 2.38273975\n",
      "Iteration 36986, loss = 2.46004247\n",
      "Iteration 36987, loss = 2.15482411\n",
      "Iteration 36988, loss = 2.67859982\n",
      "Iteration 36989, loss = 2.41780628\n",
      "Iteration 36990, loss = 2.53268855\n",
      "Iteration 36991, loss = 2.32975987\n",
      "Iteration 36992, loss = 2.71357537\n",
      "Iteration 36993, loss = 2.69382484\n",
      "Iteration 36994, loss = 2.62132601\n",
      "Iteration 36995, loss = 2.71327366\n",
      "Iteration 36996, loss = 2.44489645\n",
      "Iteration 36997, loss = 2.59939777\n",
      "Iteration 36998, loss = 2.14155890\n",
      "Iteration 36999, loss = 2.09775095\n",
      "Iteration 37000, loss = 2.28964402\n",
      "Iteration 37001, loss = 2.26036328\n",
      "Iteration 37002, loss = 2.46091183\n",
      "Iteration 37003, loss = 2.42046104\n",
      "Iteration 37004, loss = 2.22705802\n",
      "Iteration 37005, loss = 2.38374920\n",
      "Iteration 37006, loss = 2.41790855\n",
      "Iteration 37007, loss = 2.47659188\n",
      "Iteration 37008, loss = 2.26056828\n",
      "Iteration 37009, loss = 2.60285567\n",
      "Iteration 37010, loss = 3.07699005\n",
      "Iteration 37011, loss = 2.77345533\n",
      "Iteration 37012, loss = 3.17671345\n",
      "Iteration 37013, loss = 2.85349992\n",
      "Iteration 37014, loss = 2.59932621\n",
      "Iteration 37015, loss = 2.62878254\n",
      "Iteration 37016, loss = 2.42291366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 37017, loss = 2.60438591\n",
      "Iteration 37018, loss = 2.52998287\n",
      "Iteration 37019, loss = 3.58582155\n",
      "Iteration 37020, loss = 3.69335107\n",
      "Iteration 37021, loss = 2.99757678\n",
      "Iteration 37022, loss = 3.18564534\n",
      "Iteration 37023, loss = 3.03109719\n",
      "Iteration 37024, loss = 2.28438949\n",
      "Iteration 37025, loss = 2.40024622\n",
      "Iteration 37026, loss = 2.29592583\n",
      "Iteration 37027, loss = 2.26086380\n",
      "Iteration 37028, loss = 2.24643406\n",
      "Iteration 37029, loss = 2.38895570\n",
      "Iteration 37030, loss = 2.35537687\n",
      "Iteration 37031, loss = 2.84796296\n",
      "Iteration 37032, loss = 2.44002117\n",
      "Iteration 37033, loss = 2.61821653\n",
      "Iteration 37034, loss = 2.34919491\n",
      "Iteration 37035, loss = 2.31678618\n",
      "Iteration 37036, loss = 2.42058777\n",
      "Iteration 37037, loss = 2.93272345\n",
      "Iteration 37038, loss = 2.81541808\n",
      "Iteration 37039, loss = 2.56911385\n",
      "Iteration 37040, loss = 2.54573959\n",
      "Iteration 37041, loss = 2.79975920\n",
      "Iteration 37042, loss = 2.35744423\n",
      "Iteration 37043, loss = 2.24340319\n",
      "Iteration 37044, loss = 2.34013435\n",
      "Iteration 37045, loss = 2.86968741\n",
      "Iteration 37046, loss = 3.26780698\n",
      "Iteration 37047, loss = 3.95193776\n",
      "Iteration 37048, loss = 2.80630754\n",
      "Iteration 37049, loss = 2.72121647\n",
      "Iteration 37050, loss = 2.60564558\n",
      "Iteration 37051, loss = 2.29004690\n",
      "Iteration 37052, loss = 2.17712623\n",
      "Iteration 37053, loss = 2.40736465\n",
      "Iteration 37054, loss = 2.45436581\n",
      "Iteration 37055, loss = 2.50987285\n",
      "Iteration 37056, loss = 2.53269900\n",
      "Iteration 37057, loss = 2.83808538\n",
      "Iteration 37058, loss = 2.71261160\n",
      "Iteration 37059, loss = 2.54636513\n",
      "Iteration 37060, loss = 3.11146922\n",
      "Iteration 37061, loss = 2.81241342\n",
      "Iteration 37062, loss = 2.66759154\n",
      "Iteration 37063, loss = 2.37304403\n",
      "Iteration 37064, loss = 2.71426774\n",
      "Iteration 37065, loss = 2.60773278\n",
      "Iteration 37066, loss = 2.51803432\n",
      "Iteration 37067, loss = 2.66670962\n",
      "Iteration 37068, loss = 2.60047962\n",
      "Iteration 37069, loss = 2.34940735\n",
      "Iteration 37070, loss = 2.61699706\n",
      "Iteration 37071, loss = 3.30634380\n",
      "Iteration 37072, loss = 4.04311183\n",
      "Iteration 37073, loss = 4.62156861\n",
      "Iteration 37074, loss = 3.93256360\n",
      "Iteration 37075, loss = 4.30100652\n",
      "Iteration 37076, loss = 3.89423929\n",
      "Iteration 37077, loss = 2.77895633\n",
      "Iteration 37078, loss = 3.20754990\n",
      "Iteration 37079, loss = 2.45041298\n",
      "Iteration 37080, loss = 2.42228030\n",
      "Iteration 37081, loss = 2.22862317\n",
      "Iteration 37082, loss = 2.18799698\n",
      "Iteration 37083, loss = 2.28985918\n",
      "Iteration 37084, loss = 2.33801561\n",
      "Iteration 37085, loss = 2.37058691\n",
      "Iteration 37086, loss = 2.32989699\n",
      "Iteration 37087, loss = 2.69256928\n",
      "Iteration 37088, loss = 2.44164885\n",
      "Iteration 37089, loss = 2.48132395\n",
      "Iteration 37090, loss = 3.42475676\n",
      "Iteration 37091, loss = 2.76334491\n",
      "Iteration 37092, loss = 3.05958072\n",
      "Iteration 37093, loss = 3.14717788\n",
      "Iteration 37094, loss = 2.26249897\n",
      "Iteration 37095, loss = 2.46732747\n",
      "Iteration 37096, loss = 2.31789750\n",
      "Iteration 37097, loss = 2.28640249\n",
      "Iteration 37098, loss = 2.79445252\n",
      "Iteration 37099, loss = 2.64512616\n",
      "Iteration 37100, loss = 3.03065892\n",
      "Iteration 37101, loss = 3.10032465\n",
      "Iteration 37102, loss = 2.84278842\n",
      "Iteration 37103, loss = 2.82754215\n",
      "Iteration 37104, loss = 3.48018996\n",
      "Iteration 37105, loss = 2.56517552\n",
      "Iteration 37106, loss = 2.73833907\n",
      "Iteration 37107, loss = 2.87721628\n",
      "Iteration 37108, loss = 2.65289638\n",
      "Iteration 37109, loss = 3.01994557\n",
      "Iteration 37110, loss = 3.92124752\n",
      "Iteration 37111, loss = 4.23908364\n",
      "Iteration 37112, loss = 3.69704157\n",
      "Iteration 37113, loss = 3.08211196\n",
      "Iteration 37114, loss = 3.33277088\n",
      "Iteration 37115, loss = 2.93282143\n",
      "Iteration 37116, loss = 2.93576574\n",
      "Iteration 37117, loss = 3.07549995\n",
      "Iteration 37118, loss = 2.74433760\n",
      "Iteration 37119, loss = 3.41877565\n",
      "Iteration 37120, loss = 2.77395051\n",
      "Iteration 37121, loss = 2.62969761\n",
      "Iteration 37122, loss = 2.88509674\n",
      "Iteration 37123, loss = 2.49900042\n",
      "Iteration 37124, loss = 2.35585426\n",
      "Iteration 37125, loss = 2.22174024\n",
      "Iteration 37126, loss = 2.21977292\n",
      "Iteration 37127, loss = 2.52791496\n",
      "Iteration 37128, loss = 2.49890215\n",
      "Iteration 37129, loss = 2.81444133\n",
      "Iteration 37130, loss = 3.05030849\n",
      "Iteration 37131, loss = 2.57723070\n",
      "Iteration 37132, loss = 2.66166016\n",
      "Iteration 37133, loss = 3.33529255\n",
      "Iteration 37134, loss = 3.01221153\n",
      "Iteration 37135, loss = 2.87351354\n",
      "Iteration 37136, loss = 2.53130752\n",
      "Iteration 37137, loss = 2.36373486\n",
      "Iteration 37138, loss = 2.50086194\n",
      "Iteration 37139, loss = 2.26177271\n",
      "Iteration 37140, loss = 2.35513199\n",
      "Iteration 37141, loss = 2.13532478\n",
      "Iteration 37142, loss = 2.22922615\n",
      "Iteration 37143, loss = 2.36048432\n",
      "Iteration 37144, loss = 2.28140209\n",
      "Iteration 37145, loss = 2.48046226\n",
      "Iteration 37146, loss = 2.29807813\n",
      "Iteration 37147, loss = 2.25245009\n",
      "Iteration 37148, loss = 2.62633649\n",
      "Iteration 37149, loss = 2.59104329\n",
      "Iteration 37150, loss = 2.36674922\n",
      "Iteration 37151, loss = 2.45864566\n",
      "Iteration 37152, loss = 2.61331019\n",
      "Iteration 37153, loss = 2.77719501\n",
      "Iteration 37154, loss = 3.19506801\n",
      "Iteration 37155, loss = 3.08612939\n",
      "Iteration 37156, loss = 3.55210626\n",
      "Iteration 37157, loss = 3.24458297\n",
      "Iteration 37158, loss = 5.07819310\n",
      "Iteration 37159, loss = 4.44234848\n",
      "Iteration 37160, loss = 3.86382675\n",
      "Iteration 37161, loss = 3.37417274\n",
      "Iteration 37162, loss = 2.95212937\n",
      "Iteration 37163, loss = 2.60560715\n",
      "Iteration 37164, loss = 2.92925000\n",
      "Iteration 37165, loss = 3.19429616\n",
      "Iteration 37166, loss = 2.87611510\n",
      "Iteration 37167, loss = 2.98621187\n",
      "Iteration 37168, loss = 2.45972458\n",
      "Iteration 37169, loss = 3.42738397\n",
      "Iteration 37170, loss = 3.25960247\n",
      "Iteration 37171, loss = 3.13099947\n",
      "Iteration 37172, loss = 2.90894059\n",
      "Iteration 37173, loss = 2.28570757\n",
      "Iteration 37174, loss = 2.51654710\n",
      "Iteration 37175, loss = 2.25238706\n",
      "Iteration 37176, loss = 2.37955542\n",
      "Iteration 37177, loss = 2.20862695\n",
      "Iteration 37178, loss = 2.52664363\n",
      "Iteration 37179, loss = 2.39956721\n",
      "Iteration 37180, loss = 2.53058429\n",
      "Iteration 37181, loss = 2.26989249\n",
      "Iteration 37182, loss = 2.72336017\n",
      "Iteration 37183, loss = 2.60897176\n",
      "Iteration 37184, loss = 2.62628626\n",
      "Iteration 37185, loss = 2.48483856\n",
      "Iteration 37186, loss = 2.17914576\n",
      "Iteration 37187, loss = 2.14241504\n",
      "Iteration 37188, loss = 2.20450391\n",
      "Iteration 37189, loss = 2.57251468\n",
      "Iteration 37190, loss = 2.94296943\n",
      "Iteration 37191, loss = 3.63754395\n",
      "Iteration 37192, loss = 4.42882481\n",
      "Iteration 37193, loss = 3.56515564\n",
      "Iteration 37194, loss = 2.57189308\n",
      "Iteration 37195, loss = 2.54669355\n",
      "Iteration 37196, loss = 2.39488698\n",
      "Iteration 37197, loss = 2.56504220\n",
      "Iteration 37198, loss = 2.76772578\n",
      "Iteration 37199, loss = 3.05150282\n",
      "Iteration 37200, loss = 2.81624909\n",
      "Iteration 37201, loss = 2.57018062\n",
      "Iteration 37202, loss = 2.77630467\n",
      "Iteration 37203, loss = 2.65938760\n",
      "Iteration 37204, loss = 2.85170176\n",
      "Iteration 37205, loss = 3.11343542\n",
      "Iteration 37206, loss = 3.26044566\n",
      "Iteration 37207, loss = 2.89668278\n",
      "Iteration 37208, loss = 2.41954761\n",
      "Iteration 37209, loss = 2.40982656\n",
      "Iteration 37210, loss = 2.69162641\n",
      "Iteration 37211, loss = 2.32499255\n",
      "Iteration 37212, loss = 2.32678817\n",
      "Iteration 37213, loss = 2.34402987\n",
      "Iteration 37214, loss = 2.44523098\n",
      "Iteration 37215, loss = 2.63143178\n",
      "Iteration 37216, loss = 2.65500111\n",
      "Iteration 37217, loss = 2.56436467\n",
      "Iteration 37218, loss = 2.44952180\n",
      "Iteration 37219, loss = 2.52300365\n",
      "Iteration 37220, loss = 2.74865599\n",
      "Iteration 37221, loss = 2.55572099\n",
      "Iteration 37222, loss = 2.49087143\n",
      "Iteration 37223, loss = 2.69348392\n",
      "Iteration 37224, loss = 2.41298825\n",
      "Iteration 37225, loss = 3.56059693\n",
      "Iteration 37226, loss = 3.02107154\n",
      "Iteration 37227, loss = 3.29210981\n",
      "Iteration 37228, loss = 2.62479035\n",
      "Iteration 37229, loss = 3.06999392\n",
      "Iteration 37230, loss = 3.05004385\n",
      "Iteration 37231, loss = 3.69723951\n",
      "Iteration 37232, loss = 4.07923132\n",
      "Iteration 37233, loss = 2.95795272\n",
      "Iteration 37234, loss = 2.55731859\n",
      "Iteration 37235, loss = 2.19615255\n",
      "Iteration 37236, loss = 2.22499281\n",
      "Iteration 37237, loss = 2.12073538\n",
      "Iteration 37238, loss = 2.67111291\n",
      "Iteration 37239, loss = 2.66981498\n",
      "Iteration 37240, loss = 2.38598452\n",
      "Iteration 37241, loss = 2.92612988\n",
      "Iteration 37242, loss = 2.86235992\n",
      "Iteration 37243, loss = 2.59485734\n",
      "Iteration 37244, loss = 2.70639090\n",
      "Iteration 37245, loss = 2.63591144\n",
      "Iteration 37246, loss = 2.62903124\n",
      "Iteration 37247, loss = 3.23104375\n",
      "Iteration 37248, loss = 4.07500053\n",
      "Iteration 37249, loss = 5.65742153\n",
      "Iteration 37250, loss = 4.58518226\n",
      "Iteration 37251, loss = 3.79666629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 37252, loss = 3.02675906\n",
      "Iteration 37253, loss = 2.71789401\n",
      "Iteration 37254, loss = 2.50205610\n",
      "Iteration 37255, loss = 2.58748030\n",
      "Iteration 37256, loss = 2.74265211\n",
      "Iteration 37257, loss = 2.70452710\n",
      "Iteration 37258, loss = 3.00394305\n",
      "Iteration 37259, loss = 3.51801293\n",
      "Iteration 37260, loss = 2.92215882\n",
      "Iteration 37261, loss = 2.54572243\n",
      "Iteration 37262, loss = 2.18871284\n",
      "Iteration 37263, loss = 2.19928940\n",
      "Iteration 37264, loss = 2.27100690\n",
      "Iteration 37265, loss = 2.50563377\n",
      "Iteration 37266, loss = 2.13071677\n",
      "Iteration 37267, loss = 3.41080238\n",
      "Iteration 37268, loss = 3.77339050\n",
      "Iteration 37269, loss = 3.01639286\n",
      "Iteration 37270, loss = 3.89940215\n",
      "Iteration 37271, loss = 3.12173709\n",
      "Iteration 37272, loss = 2.72033760\n",
      "Iteration 37273, loss = 2.46333432\n",
      "Iteration 37274, loss = 3.00648174\n",
      "Iteration 37275, loss = 2.36422212\n",
      "Iteration 37276, loss = 2.33790950\n",
      "Iteration 37277, loss = 2.43421577\n",
      "Iteration 37278, loss = 2.19404415\n",
      "Iteration 37279, loss = 2.20291989\n",
      "Iteration 37280, loss = 2.35666196\n",
      "Iteration 37281, loss = 2.69179169\n",
      "Iteration 37282, loss = 2.34601006\n",
      "Iteration 37283, loss = 2.32062733\n",
      "Iteration 37284, loss = 2.19880128\n",
      "Iteration 37285, loss = 2.63960638\n",
      "Iteration 37286, loss = 2.61421017\n",
      "Iteration 37287, loss = 2.26719064\n",
      "Iteration 37288, loss = 2.31460450\n",
      "Iteration 37289, loss = 2.46621722\n",
      "Iteration 37290, loss = 2.12794047\n",
      "Iteration 37291, loss = 2.19728056\n",
      "Iteration 37292, loss = 2.95161324\n",
      "Iteration 37293, loss = 3.11736231\n",
      "Iteration 37294, loss = 2.33800046\n",
      "Iteration 37295, loss = 2.49737738\n",
      "Iteration 37296, loss = 2.23533863\n",
      "Iteration 37297, loss = 2.50205117\n",
      "Iteration 37298, loss = 2.59898040\n",
      "Iteration 37299, loss = 2.91396477\n",
      "Iteration 37300, loss = 2.95852421\n",
      "Iteration 37301, loss = 3.12248839\n",
      "Iteration 37302, loss = 3.58370989\n",
      "Iteration 37303, loss = 3.06599468\n",
      "Iteration 37304, loss = 2.97813773\n",
      "Iteration 37305, loss = 2.71497431\n",
      "Iteration 37306, loss = 2.63321739\n",
      "Iteration 37307, loss = 2.86796577\n",
      "Iteration 37308, loss = 3.10754128\n",
      "Iteration 37309, loss = 4.28607627\n",
      "Iteration 37310, loss = 3.52188611\n",
      "Iteration 37311, loss = 2.79580091\n",
      "Iteration 37312, loss = 2.47719798\n",
      "Iteration 37313, loss = 2.55064056\n",
      "Iteration 37314, loss = 2.89434043\n",
      "Iteration 37315, loss = 2.87007459\n",
      "Iteration 37316, loss = 3.08101554\n",
      "Iteration 37317, loss = 2.54787644\n",
      "Iteration 37318, loss = 3.08541635\n",
      "Iteration 37319, loss = 2.87145450\n",
      "Iteration 37320, loss = 2.53586055\n",
      "Iteration 37321, loss = 2.62296222\n",
      "Iteration 37322, loss = 2.80158333\n",
      "Iteration 37323, loss = 2.58610175\n",
      "Iteration 37324, loss = 2.66104690\n",
      "Iteration 37325, loss = 2.84078291\n",
      "Iteration 37326, loss = 2.66397953\n",
      "Iteration 37327, loss = 2.44789346\n",
      "Iteration 37328, loss = 2.16667803\n",
      "Iteration 37329, loss = 2.27343160\n",
      "Iteration 37330, loss = 2.88780749\n",
      "Iteration 37331, loss = 3.01552867\n",
      "Iteration 37332, loss = 2.59608891\n",
      "Iteration 37333, loss = 2.41869634\n",
      "Iteration 37334, loss = 2.45677569\n",
      "Iteration 37335, loss = 2.25493086\n",
      "Iteration 37336, loss = 2.15521813\n",
      "Iteration 37337, loss = 2.51184946\n",
      "Iteration 37338, loss = 2.31821950\n",
      "Iteration 37339, loss = 2.53879732\n",
      "Iteration 37340, loss = 2.25529252\n",
      "Iteration 37341, loss = 2.46268955\n",
      "Iteration 37342, loss = 2.59601904\n",
      "Iteration 37343, loss = 2.39490429\n",
      "Iteration 37344, loss = 2.31696392\n",
      "Iteration 37345, loss = 2.43639847\n",
      "Iteration 37346, loss = 2.21424333\n",
      "Iteration 37347, loss = 2.35622848\n",
      "Iteration 37348, loss = 2.81546285\n",
      "Iteration 37349, loss = 2.89435091\n",
      "Iteration 37350, loss = 2.49410808\n",
      "Iteration 37351, loss = 2.22676080\n",
      "Iteration 37352, loss = 2.07963976\n",
      "Iteration 37353, loss = 2.46017670\n",
      "Iteration 37354, loss = 2.83488608\n",
      "Iteration 37355, loss = 3.05423925\n",
      "Iteration 37356, loss = 3.53550295\n",
      "Iteration 37357, loss = 3.38152850\n",
      "Iteration 37358, loss = 3.61819379\n",
      "Iteration 37359, loss = 2.70166827\n",
      "Iteration 37360, loss = 3.85719288\n",
      "Iteration 37361, loss = 2.77810193\n",
      "Iteration 37362, loss = 2.53697052\n",
      "Iteration 37363, loss = 2.76958412\n",
      "Iteration 37364, loss = 2.56635758\n",
      "Iteration 37365, loss = 2.82883157\n",
      "Iteration 37366, loss = 3.25417327\n",
      "Iteration 37367, loss = 2.98448619\n",
      "Iteration 37368, loss = 2.77446152\n",
      "Iteration 37369, loss = 2.32348991\n",
      "Iteration 37370, loss = 2.28465246\n",
      "Iteration 37371, loss = 2.78424259\n",
      "Iteration 37372, loss = 2.66395050\n",
      "Iteration 37373, loss = 2.23125216\n",
      "Iteration 37374, loss = 2.20185610\n",
      "Iteration 37375, loss = 2.75674167\n",
      "Iteration 37376, loss = 2.09767817\n",
      "Iteration 37377, loss = 2.75224606\n",
      "Iteration 37378, loss = 2.62906788\n",
      "Iteration 37379, loss = 2.32826411\n",
      "Iteration 37380, loss = 2.29845305\n",
      "Iteration 37381, loss = 2.39840741\n",
      "Iteration 37382, loss = 2.59698366\n",
      "Iteration 37383, loss = 2.40591137\n",
      "Iteration 37384, loss = 2.71920864\n",
      "Iteration 37385, loss = 2.43109669\n",
      "Iteration 37386, loss = 2.73215384\n",
      "Iteration 37387, loss = 3.08190893\n",
      "Iteration 37388, loss = 3.14219103\n",
      "Iteration 37389, loss = 3.80090631\n",
      "Iteration 37390, loss = 4.22409530\n",
      "Iteration 37391, loss = 3.48320857\n",
      "Iteration 37392, loss = 3.65246392\n",
      "Iteration 37393, loss = 2.87124926\n",
      "Iteration 37394, loss = 2.84104011\n",
      "Iteration 37395, loss = 3.17806789\n",
      "Iteration 37396, loss = 2.23497017\n",
      "Iteration 37397, loss = 3.88717601\n",
      "Iteration 37398, loss = 2.94195295\n",
      "Iteration 37399, loss = 2.75396960\n",
      "Iteration 37400, loss = 3.42110553\n",
      "Iteration 37401, loss = 2.87443118\n",
      "Iteration 37402, loss = 2.77217916\n",
      "Iteration 37403, loss = 2.89765319\n",
      "Iteration 37404, loss = 2.71630738\n",
      "Iteration 37405, loss = 2.95650323\n",
      "Iteration 37406, loss = 2.45077131\n",
      "Iteration 37407, loss = 2.20858142\n",
      "Iteration 37408, loss = 2.23359446\n",
      "Iteration 37409, loss = 2.21095639\n",
      "Iteration 37410, loss = 2.19121124\n",
      "Iteration 37411, loss = 2.15113408\n",
      "Iteration 37412, loss = 2.18760527\n",
      "Iteration 37413, loss = 2.30689177\n",
      "Iteration 37414, loss = 2.53440154\n",
      "Iteration 37415, loss = 2.25839499\n",
      "Iteration 37416, loss = 2.51674502\n",
      "Iteration 37417, loss = 2.55980432\n",
      "Iteration 37418, loss = 2.45102470\n",
      "Iteration 37419, loss = 2.34514427\n",
      "Iteration 37420, loss = 2.51213581\n",
      "Iteration 37421, loss = 2.87174547\n",
      "Iteration 37422, loss = 2.63553610\n",
      "Iteration 37423, loss = 3.15118475\n",
      "Iteration 37424, loss = 2.84434512\n",
      "Iteration 37425, loss = 2.51410783\n",
      "Iteration 37426, loss = 2.38697139\n",
      "Iteration 37427, loss = 2.73181961\n",
      "Iteration 37428, loss = 2.48799029\n",
      "Iteration 37429, loss = 2.24475014\n",
      "Iteration 37430, loss = 2.69234385\n",
      "Iteration 37431, loss = 2.47719965\n",
      "Iteration 37432, loss = 2.46755121\n",
      "Iteration 37433, loss = 2.91726965\n",
      "Iteration 37434, loss = 2.59355682\n",
      "Iteration 37435, loss = 2.51595650\n",
      "Iteration 37436, loss = 2.54454571\n",
      "Iteration 37437, loss = 2.23049479\n",
      "Iteration 37438, loss = 2.23632504\n",
      "Iteration 37439, loss = 2.22159856\n",
      "Iteration 37440, loss = 2.23331937\n",
      "Iteration 37441, loss = 2.21983021\n",
      "Iteration 37442, loss = 2.28528007\n",
      "Iteration 37443, loss = 2.46778880\n",
      "Iteration 37444, loss = 2.39423473\n",
      "Iteration 37445, loss = 2.17372452\n",
      "Iteration 37446, loss = 2.24800820\n",
      "Iteration 37447, loss = 2.41865173\n",
      "Iteration 37448, loss = 2.81172462\n",
      "Iteration 37449, loss = 2.38676310\n",
      "Iteration 37450, loss = 2.25458132\n",
      "Iteration 37451, loss = 2.21183881\n",
      "Iteration 37452, loss = 2.18741079\n",
      "Iteration 37453, loss = 2.13656862\n",
      "Iteration 37454, loss = 2.34607399\n",
      "Iteration 37455, loss = 2.21977337\n",
      "Iteration 37456, loss = 2.05128824\n",
      "Iteration 37457, loss = 2.37602966\n",
      "Iteration 37458, loss = 2.44109380\n",
      "Iteration 37459, loss = 2.67142174\n",
      "Iteration 37460, loss = 2.39734336\n",
      "Iteration 37461, loss = 2.42372220\n",
      "Iteration 37462, loss = 2.45459423\n",
      "Iteration 37463, loss = 2.23601763\n",
      "Iteration 37464, loss = 2.53390367\n",
      "Iteration 37465, loss = 2.43288781\n",
      "Iteration 37466, loss = 2.23829053\n",
      "Iteration 37467, loss = 2.42233286\n",
      "Iteration 37468, loss = 2.07037555\n",
      "Iteration 37469, loss = 2.23932368\n",
      "Iteration 37470, loss = 2.69315596\n",
      "Iteration 37471, loss = 2.49606868\n",
      "Iteration 37472, loss = 2.77805272\n",
      "Iteration 37473, loss = 3.67137210\n",
      "Iteration 37474, loss = 3.01981951\n",
      "Iteration 37475, loss = 2.54407452\n",
      "Iteration 37476, loss = 2.46819965\n",
      "Iteration 37477, loss = 2.61044584\n",
      "Iteration 37478, loss = 2.51207004\n",
      "Iteration 37479, loss = 2.91123622\n",
      "Iteration 37480, loss = 2.95971285\n",
      "Iteration 37481, loss = 3.06886536\n",
      "Iteration 37482, loss = 2.57065636\n",
      "Iteration 37483, loss = 4.01578959\n",
      "Iteration 37484, loss = 4.17174961\n",
      "Iteration 37485, loss = 3.77782355\n",
      "Iteration 37486, loss = 3.19880780\n",
      "Iteration 37487, loss = 3.12536751\n",
      "Iteration 37488, loss = 2.94597198\n",
      "Iteration 37489, loss = 2.43816005\n",
      "Iteration 37490, loss = 2.59693804\n",
      "Iteration 37491, loss = 2.44847456\n",
      "Iteration 37492, loss = 2.30596737\n",
      "Iteration 37493, loss = 2.81227784\n",
      "Iteration 37494, loss = 2.40446371\n",
      "Iteration 37495, loss = 2.64033040\n",
      "Iteration 37496, loss = 2.70304590\n",
      "Iteration 37497, loss = 2.35762132\n",
      "Iteration 37498, loss = 2.77164452\n",
      "Iteration 37499, loss = 4.35879852\n",
      "Iteration 37500, loss = 6.53598375\n",
      "Iteration 37501, loss = 4.90945718\n",
      "Iteration 37502, loss = 6.27882563\n",
      "Iteration 37503, loss = 6.07788787\n",
      "Iteration 37504, loss = 3.91622845\n",
      "Iteration 37505, loss = 3.86784861\n",
      "Iteration 37506, loss = 3.78136072\n",
      "Iteration 37507, loss = 2.90511464\n",
      "Iteration 37508, loss = 3.40361622\n",
      "Iteration 37509, loss = 3.80256411\n",
      "Iteration 37510, loss = 3.12435695\n",
      "Iteration 37511, loss = 3.34152731\n",
      "Iteration 37512, loss = 3.38635789\n",
      "Iteration 37513, loss = 3.08909245\n",
      "Iteration 37514, loss = 3.32023384\n",
      "Iteration 37515, loss = 2.87908722\n",
      "Iteration 37516, loss = 2.19449701\n",
      "Iteration 37517, loss = 2.21364674\n",
      "Iteration 37518, loss = 2.36813799\n",
      "Iteration 37519, loss = 2.32481324\n",
      "Iteration 37520, loss = 2.36212754\n",
      "Iteration 37521, loss = 2.34926709\n",
      "Iteration 37522, loss = 2.34841936\n",
      "Iteration 37523, loss = 2.25094993\n",
      "Iteration 37524, loss = 2.22559672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 37525, loss = 2.40525169\n",
      "Iteration 37526, loss = 2.18932103\n",
      "Iteration 37527, loss = 3.10541406\n",
      "Iteration 37528, loss = 2.60109803\n",
      "Iteration 37529, loss = 3.90910510\n",
      "Iteration 37530, loss = 3.50139784\n",
      "Iteration 37531, loss = 2.71567446\n",
      "Iteration 37532, loss = 2.61428894\n",
      "Iteration 37533, loss = 2.51451088\n",
      "Iteration 37534, loss = 2.24704234\n",
      "Iteration 37535, loss = 2.60165432\n",
      "Iteration 37536, loss = 2.85709028\n",
      "Iteration 37537, loss = 2.64244093\n",
      "Iteration 37538, loss = 2.27331269\n",
      "Iteration 37539, loss = 2.11400186\n",
      "Iteration 37540, loss = 2.26581797\n",
      "Iteration 37541, loss = 2.77834859\n",
      "Iteration 37542, loss = 3.44832458\n",
      "Iteration 37543, loss = 3.52907583\n",
      "Iteration 37544, loss = 3.61926056\n",
      "Iteration 37545, loss = 4.13656076\n",
      "Iteration 37546, loss = 3.20625640\n",
      "Iteration 37547, loss = 2.50934139\n",
      "Iteration 37548, loss = 2.46424485\n",
      "Iteration 37549, loss = 2.80320140\n",
      "Iteration 37550, loss = 2.56806479\n",
      "Iteration 37551, loss = 3.11169808\n",
      "Iteration 37552, loss = 2.87994041\n",
      "Iteration 37553, loss = 3.17296861\n",
      "Iteration 37554, loss = 2.67241423\n",
      "Iteration 37555, loss = 4.45911361\n",
      "Iteration 37556, loss = 2.39798558\n",
      "Iteration 37557, loss = 3.23359731\n",
      "Iteration 37558, loss = 3.82344465\n",
      "Iteration 37559, loss = 3.23038671\n",
      "Iteration 37560, loss = 2.51066539\n",
      "Iteration 37561, loss = 2.65115542\n",
      "Iteration 37562, loss = 2.96866756\n",
      "Iteration 37563, loss = 2.49995249\n",
      "Iteration 37564, loss = 2.29718800\n",
      "Iteration 37565, loss = 2.07551622\n",
      "Iteration 37566, loss = 2.30280677\n",
      "Iteration 37567, loss = 2.48050768\n",
      "Iteration 37568, loss = 2.86753908\n",
      "Iteration 37569, loss = 2.08517979\n",
      "Iteration 37570, loss = 2.48638098\n",
      "Iteration 37571, loss = 2.51169185\n",
      "Iteration 37572, loss = 3.45817268\n",
      "Iteration 37573, loss = 4.12794913\n",
      "Iteration 37574, loss = 3.62675277\n",
      "Iteration 37575, loss = 3.82551938\n",
      "Iteration 37576, loss = 4.36990033\n",
      "Iteration 37577, loss = 4.97748905\n",
      "Iteration 37578, loss = 3.83702095\n",
      "Iteration 37579, loss = 4.30573711\n",
      "Iteration 37580, loss = 3.74442537\n",
      "Iteration 37581, loss = 3.39041557\n",
      "Iteration 37582, loss = 3.12536838\n",
      "Iteration 37583, loss = 2.36523163\n",
      "Iteration 37584, loss = 2.76234554\n",
      "Iteration 37585, loss = 2.94378447\n",
      "Iteration 37586, loss = 4.21354915\n",
      "Iteration 37587, loss = 4.75968589\n",
      "Iteration 37588, loss = 3.82516018\n",
      "Iteration 37589, loss = 2.80144838\n",
      "Iteration 37590, loss = 2.92469955\n",
      "Iteration 37591, loss = 2.32588208\n",
      "Iteration 37592, loss = 2.26696781\n",
      "Iteration 37593, loss = 2.58409244\n",
      "Iteration 37594, loss = 2.01995624\n",
      "Iteration 37595, loss = 2.17900777\n",
      "Iteration 37596, loss = 2.25602816\n",
      "Iteration 37597, loss = 2.31033073\n",
      "Iteration 37598, loss = 2.25061254\n",
      "Iteration 37599, loss = 2.34364785\n",
      "Iteration 37600, loss = 2.13610713\n",
      "Iteration 37601, loss = 2.21682348\n",
      "Iteration 37602, loss = 2.54747594\n",
      "Iteration 37603, loss = 2.73237703\n",
      "Iteration 37604, loss = 2.63799336\n",
      "Iteration 37605, loss = 2.67140888\n",
      "Iteration 37606, loss = 2.61431995\n",
      "Iteration 37607, loss = 2.48719239\n",
      "Iteration 37608, loss = 2.45763208\n",
      "Iteration 37609, loss = 3.25079368\n",
      "Iteration 37610, loss = 3.40716745\n",
      "Iteration 37611, loss = 2.60948452\n",
      "Iteration 37612, loss = 2.95109927\n",
      "Iteration 37613, loss = 2.97675818\n",
      "Iteration 37614, loss = 3.95218207\n",
      "Iteration 37615, loss = 4.81842752\n",
      "Iteration 37616, loss = 4.36615787\n",
      "Iteration 37617, loss = 3.87207610\n",
      "Iteration 37618, loss = 2.87886961\n",
      "Iteration 37619, loss = 2.63227594\n",
      "Iteration 37620, loss = 3.01505051\n",
      "Iteration 37621, loss = 3.12392778\n",
      "Iteration 37622, loss = 2.77525923\n",
      "Iteration 37623, loss = 3.07730467\n",
      "Iteration 37624, loss = 2.56602822\n",
      "Iteration 37625, loss = 3.54538807\n",
      "Iteration 37626, loss = 2.70772315\n",
      "Iteration 37627, loss = 2.42549367\n",
      "Iteration 37628, loss = 2.36969308\n",
      "Iteration 37629, loss = 2.32827482\n",
      "Iteration 37630, loss = 2.15403967\n",
      "Iteration 37631, loss = 2.39313424\n",
      "Iteration 37632, loss = 2.88907188\n",
      "Iteration 37633, loss = 2.55783140\n",
      "Iteration 37634, loss = 2.48982391\n",
      "Iteration 37635, loss = 2.21434366\n",
      "Iteration 37636, loss = 2.04226875\n",
      "Iteration 37637, loss = 2.55267880\n",
      "Iteration 37638, loss = 2.53620179\n",
      "Iteration 37639, loss = 2.42077237\n",
      "Iteration 37640, loss = 2.07191314\n",
      "Iteration 37641, loss = 2.24360494\n",
      "Iteration 37642, loss = 2.09211698\n",
      "Iteration 37643, loss = 2.21888255\n",
      "Iteration 37644, loss = 2.10052116\n",
      "Iteration 37645, loss = 2.16813957\n",
      "Iteration 37646, loss = 2.11822465\n",
      "Iteration 37647, loss = 2.27546400\n",
      "Iteration 37648, loss = 2.44154353\n",
      "Iteration 37649, loss = 2.33318625\n",
      "Iteration 37650, loss = 2.71163404\n",
      "Iteration 37651, loss = 3.26556372\n",
      "Iteration 37652, loss = 2.37088423\n",
      "Iteration 37653, loss = 2.45954090\n",
      "Iteration 37654, loss = 2.94783726\n",
      "Iteration 37655, loss = 2.57347123\n",
      "Iteration 37656, loss = 2.17460558\n",
      "Iteration 37657, loss = 2.36737378\n",
      "Iteration 37658, loss = 2.58234943\n",
      "Iteration 37659, loss = 2.54209985\n",
      "Iteration 37660, loss = 2.77608831\n",
      "Iteration 37661, loss = 3.21160207\n",
      "Iteration 37662, loss = 3.51149663\n",
      "Iteration 37663, loss = 3.57735913\n",
      "Iteration 37664, loss = 2.79003072\n",
      "Iteration 37665, loss = 2.55178880\n",
      "Iteration 37666, loss = 2.57868298\n",
      "Iteration 37667, loss = 2.92280217\n",
      "Iteration 37668, loss = 2.62370287\n",
      "Iteration 37669, loss = 2.36782922\n",
      "Iteration 37670, loss = 2.21325419\n",
      "Iteration 37671, loss = 2.53572742\n",
      "Iteration 37672, loss = 2.44114614\n",
      "Iteration 37673, loss = 2.47653596\n",
      "Iteration 37674, loss = 2.44839881\n",
      "Iteration 37675, loss = 2.55355890\n",
      "Iteration 37676, loss = 2.21857465\n",
      "Iteration 37677, loss = 2.12874621\n",
      "Iteration 37678, loss = 2.53664177\n",
      "Iteration 37679, loss = 2.35001140\n",
      "Iteration 37680, loss = 2.82764760\n",
      "Iteration 37681, loss = 2.83667946\n",
      "Iteration 37682, loss = 2.56008067\n",
      "Iteration 37683, loss = 3.42754761\n",
      "Iteration 37684, loss = 3.16820730\n",
      "Iteration 37685, loss = 2.98890977\n",
      "Iteration 37686, loss = 3.44985015\n",
      "Iteration 37687, loss = 3.28028873\n",
      "Iteration 37688, loss = 2.19462987\n",
      "Iteration 37689, loss = 2.74767754\n",
      "Iteration 37690, loss = 2.58151710\n",
      "Iteration 37691, loss = 2.16633776\n",
      "Iteration 37692, loss = 2.37444765\n",
      "Iteration 37693, loss = 2.27516907\n",
      "Iteration 37694, loss = 2.28474545\n",
      "Iteration 37695, loss = 2.39064228\n",
      "Iteration 37696, loss = 2.85736362\n",
      "Iteration 37697, loss = 2.84382370\n",
      "Iteration 37698, loss = 5.28482051\n",
      "Iteration 37699, loss = 4.10212809\n",
      "Iteration 37700, loss = 2.50708714\n",
      "Iteration 37701, loss = 2.61047084\n",
      "Iteration 37702, loss = 2.58692669\n",
      "Iteration 37703, loss = 2.53869925\n",
      "Iteration 37704, loss = 3.15259221\n",
      "Iteration 37705, loss = 3.76132685\n",
      "Iteration 37706, loss = 2.78435492\n",
      "Iteration 37707, loss = 2.56349791\n",
      "Iteration 37708, loss = 2.54454098\n",
      "Iteration 37709, loss = 2.95164295\n",
      "Iteration 37710, loss = 3.09557593\n",
      "Iteration 37711, loss = 3.63344553\n",
      "Iteration 37712, loss = 2.89859363\n",
      "Iteration 37713, loss = 2.72659615\n",
      "Iteration 37714, loss = 2.12892684\n",
      "Iteration 37715, loss = 2.55504115\n",
      "Iteration 37716, loss = 2.11486109\n",
      "Iteration 37717, loss = 2.74841990\n",
      "Iteration 37718, loss = 2.49283752\n",
      "Iteration 37719, loss = 2.83975548\n",
      "Iteration 37720, loss = 3.43656749\n",
      "Iteration 37721, loss = 2.78459742\n",
      "Iteration 37722, loss = 2.39507290\n",
      "Iteration 37723, loss = 2.52638068\n",
      "Iteration 37724, loss = 2.80746970\n",
      "Iteration 37725, loss = 3.75087402\n",
      "Iteration 37726, loss = 2.31407482\n",
      "Iteration 37727, loss = 2.65833452\n",
      "Iteration 37728, loss = 2.58835508\n",
      "Iteration 37729, loss = 2.67285367\n",
      "Iteration 37730, loss = 2.54188842\n",
      "Iteration 37731, loss = 2.67823351\n",
      "Iteration 37732, loss = 3.15002944\n",
      "Iteration 37733, loss = 2.81260753\n",
      "Iteration 37734, loss = 2.78005351\n",
      "Iteration 37735, loss = 3.57335586\n",
      "Iteration 37736, loss = 2.67410991\n",
      "Iteration 37737, loss = 2.92852442\n",
      "Iteration 37738, loss = 2.63458405\n",
      "Iteration 37739, loss = 2.25646430\n",
      "Iteration 37740, loss = 2.50579180\n",
      "Iteration 37741, loss = 2.16485232\n",
      "Iteration 37742, loss = 2.49639012\n",
      "Iteration 37743, loss = 2.96229095\n",
      "Iteration 37744, loss = 2.66583598\n",
      "Iteration 37745, loss = 2.60189162\n",
      "Iteration 37746, loss = 2.59028073\n",
      "Iteration 37747, loss = 3.25091281\n",
      "Iteration 37748, loss = 2.98844221\n",
      "Iteration 37749, loss = 3.26117667\n",
      "Iteration 37750, loss = 3.12409541\n",
      "Iteration 37751, loss = 3.40287365\n",
      "Iteration 37752, loss = 3.09180596\n",
      "Iteration 37753, loss = 2.79806165\n",
      "Iteration 37754, loss = 2.52633914\n",
      "Iteration 37755, loss = 2.38020218\n",
      "Iteration 37756, loss = 2.71911275\n",
      "Iteration 37757, loss = 2.90942302\n",
      "Iteration 37758, loss = 2.65127121\n",
      "Iteration 37759, loss = 2.62689018\n",
      "Iteration 37760, loss = 2.72967246\n",
      "Iteration 37761, loss = 3.61148493\n",
      "Iteration 37762, loss = 3.64034153\n",
      "Iteration 37763, loss = 2.84959094\n",
      "Iteration 37764, loss = 3.51453773\n",
      "Iteration 37765, loss = 4.00911537\n",
      "Iteration 37766, loss = 4.02429841\n",
      "Iteration 37767, loss = 4.01221389\n",
      "Iteration 37768, loss = 5.52310779\n",
      "Iteration 37769, loss = 3.46866974\n",
      "Iteration 37770, loss = 3.28988987\n",
      "Iteration 37771, loss = 2.59142174\n",
      "Iteration 37772, loss = 2.44734647\n",
      "Iteration 37773, loss = 2.25421664\n",
      "Iteration 37774, loss = 2.29135000\n",
      "Iteration 37775, loss = 2.38371229\n",
      "Iteration 37776, loss = 2.11143135\n",
      "Iteration 37777, loss = 2.57316082\n",
      "Iteration 37778, loss = 2.81782958\n",
      "Iteration 37779, loss = 2.74621368\n",
      "Iteration 37780, loss = 2.57157775\n",
      "Iteration 37781, loss = 3.16434499\n",
      "Iteration 37782, loss = 2.81832974\n",
      "Iteration 37783, loss = 2.60899096\n",
      "Iteration 37784, loss = 2.54910886\n",
      "Iteration 37785, loss = 2.71882567\n",
      "Iteration 37786, loss = 2.58534122\n",
      "Iteration 37787, loss = 2.53937505\n",
      "Iteration 37788, loss = 2.26715299\n",
      "Iteration 37789, loss = 2.12078018\n",
      "Iteration 37790, loss = 2.25998474\n",
      "Iteration 37791, loss = 2.60878513\n",
      "Iteration 37792, loss = 2.37033319\n",
      "Iteration 37793, loss = 2.02862563\n",
      "Iteration 37794, loss = 2.24232219\n",
      "Iteration 37795, loss = 2.52313190\n",
      "Iteration 37796, loss = 2.44611697\n",
      "Iteration 37797, loss = 2.16174723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 37798, loss = 2.55898504\n",
      "Iteration 37799, loss = 2.97978804\n",
      "Iteration 37800, loss = 4.32027142\n",
      "Iteration 37801, loss = 4.06736512\n",
      "Iteration 37802, loss = 4.13943518\n",
      "Iteration 37803, loss = 5.18890115\n",
      "Iteration 37804, loss = 3.52394799\n",
      "Iteration 37805, loss = 4.60873418\n",
      "Iteration 37806, loss = 5.08788164\n",
      "Iteration 37807, loss = 4.32111331\n",
      "Iteration 37808, loss = 3.11586844\n",
      "Iteration 37809, loss = 4.32158115\n",
      "Iteration 37810, loss = 3.96992624\n",
      "Iteration 37811, loss = 3.47772589\n",
      "Iteration 37812, loss = 2.96896327\n",
      "Iteration 37813, loss = 2.62658380\n",
      "Iteration 37814, loss = 2.66012581\n",
      "Iteration 37815, loss = 2.54823759\n",
      "Iteration 37816, loss = 2.37650365\n",
      "Iteration 37817, loss = 2.43120524\n",
      "Iteration 37818, loss = 2.66491183\n",
      "Iteration 37819, loss = 2.48664097\n",
      "Iteration 37820, loss = 2.68187167\n",
      "Iteration 37821, loss = 2.67730210\n",
      "Iteration 37822, loss = 2.64747481\n",
      "Iteration 37823, loss = 2.50336757\n",
      "Iteration 37824, loss = 2.38768467\n",
      "Iteration 37825, loss = 2.65773017\n",
      "Iteration 37826, loss = 2.99738207\n",
      "Iteration 37827, loss = 2.79681049\n",
      "Iteration 37828, loss = 2.41130145\n",
      "Iteration 37829, loss = 2.53621520\n",
      "Iteration 37830, loss = 2.15351350\n",
      "Iteration 37831, loss = 2.15805342\n",
      "Iteration 37832, loss = 2.05042308\n",
      "Iteration 37833, loss = 2.10305412\n",
      "Iteration 37834, loss = 2.36472712\n",
      "Iteration 37835, loss = 2.21712608\n",
      "Iteration 37836, loss = 2.27342321\n",
      "Iteration 37837, loss = 2.16953528\n",
      "Iteration 37838, loss = 2.43778034\n",
      "Iteration 37839, loss = 2.36025364\n",
      "Iteration 37840, loss = 2.58673157\n",
      "Iteration 37841, loss = 2.67295711\n",
      "Iteration 37842, loss = 2.32000265\n",
      "Iteration 37843, loss = 2.15367656\n",
      "Iteration 37844, loss = 2.20477364\n",
      "Iteration 37845, loss = 2.65891781\n",
      "Iteration 37846, loss = 2.38775279\n",
      "Iteration 37847, loss = 2.54863877\n",
      "Iteration 37848, loss = 3.13267948\n",
      "Iteration 37849, loss = 2.29213846\n",
      "Iteration 37850, loss = 3.15592817\n",
      "Iteration 37851, loss = 2.75232317\n",
      "Iteration 37852, loss = 2.34774042\n",
      "Iteration 37853, loss = 2.18722467\n",
      "Iteration 37854, loss = 2.00550783\n",
      "Iteration 37855, loss = 2.40974375\n",
      "Iteration 37856, loss = 2.57489982\n",
      "Iteration 37857, loss = 2.39380534\n",
      "Iteration 37858, loss = 2.27750706\n",
      "Iteration 37859, loss = 2.25421987\n",
      "Iteration 37860, loss = 2.08703056\n",
      "Iteration 37861, loss = 2.22076731\n",
      "Iteration 37862, loss = 2.16436753\n",
      "Iteration 37863, loss = 2.33077608\n",
      "Iteration 37864, loss = 2.09206351\n",
      "Iteration 37865, loss = 2.43825495\n",
      "Iteration 37866, loss = 2.44213796\n",
      "Iteration 37867, loss = 2.53303709\n",
      "Iteration 37868, loss = 2.18748039\n",
      "Iteration 37869, loss = 2.20187011\n",
      "Iteration 37870, loss = 2.73612970\n",
      "Iteration 37871, loss = 3.15956869\n",
      "Iteration 37872, loss = 2.90351345\n",
      "Iteration 37873, loss = 2.55746467\n",
      "Iteration 37874, loss = 3.15289382\n",
      "Iteration 37875, loss = 2.90320680\n",
      "Iteration 37876, loss = 2.44248361\n",
      "Iteration 37877, loss = 3.92216475\n",
      "Iteration 37878, loss = 3.51048635\n",
      "Iteration 37879, loss = 2.87836123\n",
      "Iteration 37880, loss = 2.79423422\n",
      "Iteration 37881, loss = 2.47825109\n",
      "Iteration 37882, loss = 2.12081007\n",
      "Iteration 37883, loss = 2.01298906\n",
      "Iteration 37884, loss = 2.54992918\n",
      "Iteration 37885, loss = 2.03466396\n",
      "Iteration 37886, loss = 2.00515092\n",
      "Iteration 37887, loss = 2.29632364\n",
      "Iteration 37888, loss = 2.32651207\n",
      "Iteration 37889, loss = 2.77379656\n",
      "Iteration 37890, loss = 2.69554082\n",
      "Iteration 37891, loss = 2.34476048\n",
      "Iteration 37892, loss = 3.45213826\n",
      "Iteration 37893, loss = 2.49139872\n",
      "Iteration 37894, loss = 3.69205012\n",
      "Iteration 37895, loss = 3.93775730\n",
      "Iteration 37896, loss = 3.30928080\n",
      "Iteration 37897, loss = 2.64604549\n",
      "Iteration 37898, loss = 3.44532724\n",
      "Iteration 37899, loss = 2.86086466\n",
      "Iteration 37900, loss = 3.18257661\n",
      "Iteration 37901, loss = 3.05009246\n",
      "Iteration 37902, loss = 2.87544631\n",
      "Iteration 37903, loss = 2.66932308\n",
      "Iteration 37904, loss = 3.19475184\n",
      "Iteration 37905, loss = 2.68755173\n",
      "Iteration 37906, loss = 2.60170131\n",
      "Iteration 37907, loss = 2.54175511\n",
      "Iteration 37908, loss = 2.13824186\n",
      "Iteration 37909, loss = 2.08422037\n",
      "Iteration 37910, loss = 2.04660659\n",
      "Iteration 37911, loss = 2.54390781\n",
      "Iteration 37912, loss = 2.91436595\n",
      "Iteration 37913, loss = 3.08427678\n",
      "Iteration 37914, loss = 2.83755994\n",
      "Iteration 37915, loss = 2.95368178\n",
      "Iteration 37916, loss = 2.52629312\n",
      "Iteration 37917, loss = 2.78865008\n",
      "Iteration 37918, loss = 2.71339368\n",
      "Iteration 37919, loss = 3.62677499\n",
      "Iteration 37920, loss = 2.52791544\n",
      "Iteration 37921, loss = 2.29490669\n",
      "Iteration 37922, loss = 2.42107599\n",
      "Iteration 37923, loss = 2.95319543\n",
      "Iteration 37924, loss = 3.22573006\n",
      "Iteration 37925, loss = 2.47437034\n",
      "Iteration 37926, loss = 2.05464752\n",
      "Iteration 37927, loss = 2.35237728\n",
      "Iteration 37928, loss = 2.26231542\n",
      "Iteration 37929, loss = 2.48782024\n",
      "Iteration 37930, loss = 2.29805940\n",
      "Iteration 37931, loss = 2.42312559\n",
      "Iteration 37932, loss = 2.92740076\n",
      "Iteration 37933, loss = 2.81389336\n",
      "Iteration 37934, loss = 2.88962489\n",
      "Iteration 37935, loss = 2.97027654\n",
      "Iteration 37936, loss = 2.85319227\n",
      "Iteration 37937, loss = 2.47668843\n",
      "Iteration 37938, loss = 3.48886771\n",
      "Iteration 37939, loss = 2.53730415\n",
      "Iteration 37940, loss = 3.67275098\n",
      "Iteration 37941, loss = 2.87603866\n",
      "Iteration 37942, loss = 2.50253068\n",
      "Iteration 37943, loss = 2.29111805\n",
      "Iteration 37944, loss = 3.24546346\n",
      "Iteration 37945, loss = 2.99337966\n",
      "Iteration 37946, loss = 2.30559778\n",
      "Iteration 37947, loss = 2.46737983\n",
      "Iteration 37948, loss = 2.88623515\n",
      "Iteration 37949, loss = 2.30805207\n",
      "Iteration 37950, loss = 2.55547616\n",
      "Iteration 37951, loss = 2.53634486\n",
      "Iteration 37952, loss = 2.86699678\n",
      "Iteration 37953, loss = 2.67527365\n",
      "Iteration 37954, loss = 2.43026867\n",
      "Iteration 37955, loss = 2.27260340\n",
      "Iteration 37956, loss = 2.61561632\n",
      "Iteration 37957, loss = 2.49111960\n",
      "Iteration 37958, loss = 3.19359170\n",
      "Iteration 37959, loss = 3.06335169\n",
      "Iteration 37960, loss = 3.36447436\n",
      "Iteration 37961, loss = 3.30676069\n",
      "Iteration 37962, loss = 3.32363900\n",
      "Iteration 37963, loss = 2.32799293\n",
      "Iteration 37964, loss = 2.66920338\n",
      "Iteration 37965, loss = 2.56986936\n",
      "Iteration 37966, loss = 2.79424009\n",
      "Iteration 37967, loss = 2.63961835\n",
      "Iteration 37968, loss = 2.54003784\n",
      "Iteration 37969, loss = 2.29862923\n",
      "Iteration 37970, loss = 2.13194594\n",
      "Iteration 37971, loss = 2.39604990\n",
      "Iteration 37972, loss = 2.75676474\n",
      "Iteration 37973, loss = 3.27270752\n",
      "Iteration 37974, loss = 3.48538635\n",
      "Iteration 37975, loss = 2.94919849\n",
      "Iteration 37976, loss = 2.58523052\n",
      "Iteration 37977, loss = 2.71037248\n",
      "Iteration 37978, loss = 2.90323702\n",
      "Iteration 37979, loss = 2.55295769\n",
      "Iteration 37980, loss = 2.58571413\n",
      "Iteration 37981, loss = 2.79778034\n",
      "Iteration 37982, loss = 2.75262609\n",
      "Iteration 37983, loss = 3.15240976\n",
      "Iteration 37984, loss = 3.20113821\n",
      "Iteration 37985, loss = 3.79178910\n",
      "Iteration 37986, loss = 3.48335122\n",
      "Iteration 37987, loss = 3.64441865\n",
      "Iteration 37988, loss = 2.69881932\n",
      "Iteration 37989, loss = 2.47678796\n",
      "Iteration 37990, loss = 2.57393011\n",
      "Iteration 37991, loss = 3.10160863\n",
      "Iteration 37992, loss = 2.53200473\n",
      "Iteration 37993, loss = 2.23570130\n",
      "Iteration 37994, loss = 1.97159643\n",
      "Iteration 37995, loss = 2.05068467\n",
      "Iteration 37996, loss = 2.10180555\n",
      "Iteration 37997, loss = 2.25017017\n",
      "Iteration 37998, loss = 2.17866660\n",
      "Iteration 37999, loss = 2.22947252\n",
      "Iteration 38000, loss = 2.32068423\n",
      "Iteration 38001, loss = 2.06798326\n",
      "Iteration 38002, loss = 2.58975593\n",
      "Iteration 38003, loss = 2.38325017\n",
      "Iteration 38004, loss = 2.63535192\n",
      "Iteration 38005, loss = 2.42000673\n",
      "Iteration 38006, loss = 2.45667337\n",
      "Iteration 38007, loss = 2.55224875\n",
      "Iteration 38008, loss = 2.54863355\n",
      "Iteration 38009, loss = 2.82012973\n",
      "Iteration 38010, loss = 2.80380098\n",
      "Iteration 38011, loss = 2.57336278\n",
      "Iteration 38012, loss = 2.15476178\n",
      "Iteration 38013, loss = 2.28106734\n",
      "Iteration 38014, loss = 2.40712557\n",
      "Iteration 38015, loss = 2.18267635\n",
      "Iteration 38016, loss = 2.02230420\n",
      "Iteration 38017, loss = 2.55191030\n",
      "Iteration 38018, loss = 2.33973018\n",
      "Iteration 38019, loss = 2.10129813\n",
      "Iteration 38020, loss = 2.22834157\n",
      "Iteration 38021, loss = 2.42447934\n",
      "Iteration 38022, loss = 2.47932572\n",
      "Iteration 38023, loss = 2.32159844\n",
      "Iteration 38024, loss = 2.12665506\n",
      "Iteration 38025, loss = 1.96500721\n",
      "Iteration 38026, loss = 2.49643450\n",
      "Iteration 38027, loss = 3.30299717\n",
      "Iteration 38028, loss = 2.78868251\n",
      "Iteration 38029, loss = 3.67553058\n",
      "Iteration 38030, loss = 3.95410999\n",
      "Iteration 38031, loss = 3.71215380\n",
      "Iteration 38032, loss = 3.11130666\n",
      "Iteration 38033, loss = 4.08835236\n",
      "Iteration 38034, loss = 4.23759719\n",
      "Iteration 38035, loss = 3.01960431\n",
      "Iteration 38036, loss = 3.33088614\n",
      "Iteration 38037, loss = 3.12942717\n",
      "Iteration 38038, loss = 2.21400097\n",
      "Iteration 38039, loss = 2.18148714\n",
      "Iteration 38040, loss = 2.95221660\n",
      "Iteration 38041, loss = 2.76440135\n",
      "Iteration 38042, loss = 3.15834866\n",
      "Iteration 38043, loss = 3.01962228\n",
      "Iteration 38044, loss = 2.92285282\n",
      "Iteration 38045, loss = 3.50364837\n",
      "Iteration 38046, loss = 3.67878477\n",
      "Iteration 38047, loss = 3.13862685\n",
      "Iteration 38048, loss = 2.79441517\n",
      "Iteration 38049, loss = 2.25984874\n",
      "Iteration 38050, loss = 2.19895234\n",
      "Iteration 38051, loss = 2.34724707\n",
      "Iteration 38052, loss = 2.30256262\n",
      "Iteration 38053, loss = 2.37301853\n",
      "Iteration 38054, loss = 2.36309667\n",
      "Iteration 38055, loss = 2.29581348\n",
      "Iteration 38056, loss = 2.09486990\n",
      "Iteration 38057, loss = 2.51550562\n",
      "Iteration 38058, loss = 2.81359528\n",
      "Iteration 38059, loss = 2.63586585\n",
      "Iteration 38060, loss = 3.03213039\n",
      "Iteration 38061, loss = 2.83866109\n",
      "Iteration 38062, loss = 2.62727508\n",
      "Iteration 38063, loss = 2.68303905\n",
      "Iteration 38064, loss = 2.53868059\n",
      "Iteration 38065, loss = 2.56514849\n",
      "Iteration 38066, loss = 2.99668407\n",
      "Iteration 38067, loss = 2.93874078\n",
      "Iteration 38068, loss = 2.67273928\n",
      "Iteration 38069, loss = 2.72951823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 38070, loss = 2.45244303\n",
      "Iteration 38071, loss = 2.38983859\n",
      "Iteration 38072, loss = 2.17153667\n",
      "Iteration 38073, loss = 2.34544782\n",
      "Iteration 38074, loss = 2.29244428\n",
      "Iteration 38075, loss = 2.80191429\n",
      "Iteration 38076, loss = 2.43073786\n",
      "Iteration 38077, loss = 2.35799204\n",
      "Iteration 38078, loss = 2.50964704\n",
      "Iteration 38079, loss = 2.48665669\n",
      "Iteration 38080, loss = 2.24982650\n",
      "Iteration 38081, loss = 2.04691119\n",
      "Iteration 38082, loss = 2.47681309\n",
      "Iteration 38083, loss = 2.99247873\n",
      "Iteration 38084, loss = 3.52001183\n",
      "Iteration 38085, loss = 2.66064075\n",
      "Iteration 38086, loss = 2.88731063\n",
      "Iteration 38087, loss = 2.95284775\n",
      "Iteration 38088, loss = 2.70446116\n",
      "Iteration 38089, loss = 2.72311717\n",
      "Iteration 38090, loss = 2.29510779\n",
      "Iteration 38091, loss = 2.34171485\n",
      "Iteration 38092, loss = 2.25553065\n",
      "Iteration 38093, loss = 2.17892133\n",
      "Iteration 38094, loss = 2.00373528\n",
      "Iteration 38095, loss = 2.52515312\n",
      "Iteration 38096, loss = 2.37249857\n",
      "Iteration 38097, loss = 2.37822234\n",
      "Iteration 38098, loss = 3.63225135\n",
      "Iteration 38099, loss = 3.76327437\n",
      "Iteration 38100, loss = 2.72113629\n",
      "Iteration 38101, loss = 2.64333600\n",
      "Iteration 38102, loss = 2.18356994\n",
      "Iteration 38103, loss = 2.42494277\n",
      "Iteration 38104, loss = 2.35071039\n",
      "Iteration 38105, loss = 2.39933310\n",
      "Iteration 38106, loss = 2.14641182\n",
      "Iteration 38107, loss = 2.60831995\n",
      "Iteration 38108, loss = 2.55747926\n",
      "Iteration 38109, loss = 2.83756515\n",
      "Iteration 38110, loss = 2.55138663\n",
      "Iteration 38111, loss = 2.90841467\n",
      "Iteration 38112, loss = 3.05457925\n",
      "Iteration 38113, loss = 3.31152643\n",
      "Iteration 38114, loss = 2.63579944\n",
      "Iteration 38115, loss = 3.19392740\n",
      "Iteration 38116, loss = 2.86434318\n",
      "Iteration 38117, loss = 3.27069133\n",
      "Iteration 38118, loss = 2.61438540\n",
      "Iteration 38119, loss = 2.75894858\n",
      "Iteration 38120, loss = 3.88178248\n",
      "Iteration 38121, loss = 4.47312157\n",
      "Iteration 38122, loss = 3.99458329\n",
      "Iteration 38123, loss = 4.37359207\n",
      "Iteration 38124, loss = 3.51300502\n",
      "Iteration 38125, loss = 3.11310375\n",
      "Iteration 38126, loss = 3.17313422\n",
      "Iteration 38127, loss = 2.63022186\n",
      "Iteration 38128, loss = 2.36657612\n",
      "Iteration 38129, loss = 2.73875314\n",
      "Iteration 38130, loss = 2.27814423\n",
      "Iteration 38131, loss = 2.56941995\n",
      "Iteration 38132, loss = 2.26255922\n",
      "Iteration 38133, loss = 1.95279794\n",
      "Iteration 38134, loss = 2.17036182\n",
      "Iteration 38135, loss = 2.26858084\n",
      "Iteration 38136, loss = 2.20678892\n",
      "Iteration 38137, loss = 2.19312533\n",
      "Iteration 38138, loss = 2.92237364\n",
      "Iteration 38139, loss = 2.58969889\n",
      "Iteration 38140, loss = 2.54403341\n",
      "Iteration 38141, loss = 2.42341216\n",
      "Iteration 38142, loss = 2.62305077\n",
      "Iteration 38143, loss = 2.84573811\n",
      "Iteration 38144, loss = 2.72882680\n",
      "Iteration 38145, loss = 3.25548209\n",
      "Iteration 38146, loss = 2.66594110\n",
      "Iteration 38147, loss = 2.27361866\n",
      "Iteration 38148, loss = 2.33491892\n",
      "Iteration 38149, loss = 2.27897171\n",
      "Iteration 38150, loss = 2.21914034\n",
      "Iteration 38151, loss = 2.22814177\n",
      "Iteration 38152, loss = 2.33574763\n",
      "Iteration 38153, loss = 2.77534672\n",
      "Iteration 38154, loss = 2.48012628\n",
      "Iteration 38155, loss = 2.39437122\n",
      "Iteration 38156, loss = 2.39421410\n",
      "Iteration 38157, loss = 1.96972681\n",
      "Iteration 38158, loss = 2.47100246\n",
      "Iteration 38159, loss = 2.61750662\n",
      "Iteration 38160, loss = 3.12535376\n",
      "Iteration 38161, loss = 2.68937765\n",
      "Iteration 38162, loss = 2.50112638\n",
      "Iteration 38163, loss = 2.47165088\n",
      "Iteration 38164, loss = 2.63501937\n",
      "Iteration 38165, loss = 2.54209990\n",
      "Iteration 38166, loss = 2.29867973\n",
      "Iteration 38167, loss = 2.68342637\n",
      "Iteration 38168, loss = 2.44013603\n",
      "Iteration 38169, loss = 2.37942354\n",
      "Iteration 38170, loss = 2.47438496\n",
      "Iteration 38171, loss = 2.43834082\n",
      "Iteration 38172, loss = 2.14315714\n",
      "Iteration 38173, loss = 2.16706637\n",
      "Iteration 38174, loss = 2.48736461\n",
      "Iteration 38175, loss = 2.63523857\n",
      "Iteration 38176, loss = 2.55814911\n",
      "Iteration 38177, loss = 2.59499152\n",
      "Iteration 38178, loss = 2.97748567\n",
      "Iteration 38179, loss = 4.18289651\n",
      "Iteration 38180, loss = 3.80409974\n",
      "Iteration 38181, loss = 4.35857043\n",
      "Iteration 38182, loss = 3.04671613\n",
      "Iteration 38183, loss = 4.62111481\n",
      "Iteration 38184, loss = 3.24048374\n",
      "Iteration 38185, loss = 4.18282136\n",
      "Iteration 38186, loss = 3.14538909\n",
      "Iteration 38187, loss = 3.13590195\n",
      "Iteration 38188, loss = 3.28891484\n",
      "Iteration 38189, loss = 2.95907460\n",
      "Iteration 38190, loss = 3.51608838\n",
      "Iteration 38191, loss = 4.26698057\n",
      "Iteration 38192, loss = 4.66827449\n",
      "Iteration 38193, loss = 3.78378830\n",
      "Iteration 38194, loss = 3.06615709\n",
      "Iteration 38195, loss = 2.87584756\n",
      "Iteration 38196, loss = 2.79188074\n",
      "Iteration 38197, loss = 2.56796760\n",
      "Iteration 38198, loss = 2.72169073\n",
      "Iteration 38199, loss = 2.86317728\n",
      "Iteration 38200, loss = 3.76283968\n",
      "Iteration 38201, loss = 3.54701146\n",
      "Iteration 38202, loss = 3.09548067\n",
      "Iteration 38203, loss = 3.36729209\n",
      "Iteration 38204, loss = 2.64722387\n",
      "Iteration 38205, loss = 2.38816399\n",
      "Iteration 38206, loss = 2.26109830\n",
      "Iteration 38207, loss = 2.36720674\n",
      "Iteration 38208, loss = 2.08768381\n",
      "Iteration 38209, loss = 2.18618540\n",
      "Iteration 38210, loss = 2.18280943\n",
      "Iteration 38211, loss = 2.25965477\n",
      "Iteration 38212, loss = 2.05769066\n",
      "Iteration 38213, loss = 2.21602960\n",
      "Iteration 38214, loss = 2.63826397\n",
      "Iteration 38215, loss = 2.68692512\n",
      "Iteration 38216, loss = 3.87869649\n",
      "Iteration 38217, loss = 4.60101054\n",
      "Iteration 38218, loss = 4.48253558\n",
      "Iteration 38219, loss = 3.81348542\n",
      "Iteration 38220, loss = 4.64420983\n",
      "Iteration 38221, loss = 3.53396884\n",
      "Iteration 38222, loss = 4.14649431\n",
      "Iteration 38223, loss = 3.34427321\n",
      "Iteration 38224, loss = 2.87752349\n",
      "Iteration 38225, loss = 2.29735695\n",
      "Iteration 38226, loss = 2.49547332\n",
      "Iteration 38227, loss = 2.82019814\n",
      "Iteration 38228, loss = 2.30998581\n",
      "Iteration 38229, loss = 2.59373989\n",
      "Iteration 38230, loss = 2.74973935\n",
      "Iteration 38231, loss = 2.87872537\n",
      "Iteration 38232, loss = 2.06658881\n",
      "Iteration 38233, loss = 2.32891739\n",
      "Iteration 38234, loss = 2.39561483\n",
      "Iteration 38235, loss = 2.43437451\n",
      "Iteration 38236, loss = 2.43850881\n",
      "Iteration 38237, loss = 2.29154633\n",
      "Iteration 38238, loss = 2.49589981\n",
      "Iteration 38239, loss = 2.27904930\n",
      "Iteration 38240, loss = 2.78841694\n",
      "Iteration 38241, loss = 2.84811846\n",
      "Iteration 38242, loss = 2.86490999\n",
      "Iteration 38243, loss = 2.62658393\n",
      "Iteration 38244, loss = 3.01887084\n",
      "Iteration 38245, loss = 2.58726717\n",
      "Iteration 38246, loss = 2.27180582\n",
      "Iteration 38247, loss = 2.09720184\n",
      "Iteration 38248, loss = 2.25737180\n",
      "Iteration 38249, loss = 2.25528698\n",
      "Iteration 38250, loss = 2.08720098\n",
      "Iteration 38251, loss = 2.17753558\n",
      "Iteration 38252, loss = 2.27204975\n",
      "Iteration 38253, loss = 2.54638434\n",
      "Iteration 38254, loss = 2.21225298\n",
      "Iteration 38255, loss = 2.61752396\n",
      "Iteration 38256, loss = 2.77269888\n",
      "Iteration 38257, loss = 2.50650071\n",
      "Iteration 38258, loss = 2.84865693\n",
      "Iteration 38259, loss = 2.16603527\n",
      "Iteration 38260, loss = 2.12014134\n",
      "Iteration 38261, loss = 2.18436531\n",
      "Iteration 38262, loss = 2.31975968\n",
      "Iteration 38263, loss = 2.30852138\n",
      "Iteration 38264, loss = 2.76701136\n",
      "Iteration 38265, loss = 2.39780414\n",
      "Iteration 38266, loss = 2.29853179\n",
      "Iteration 38267, loss = 2.06361192\n",
      "Iteration 38268, loss = 2.21047070\n",
      "Iteration 38269, loss = 2.24887181\n",
      "Iteration 38270, loss = 2.58467316\n",
      "Iteration 38271, loss = 2.60937096\n",
      "Iteration 38272, loss = 2.29331821\n",
      "Iteration 38273, loss = 2.78736071\n",
      "Iteration 38274, loss = 3.02341233\n",
      "Iteration 38275, loss = 2.22661448\n",
      "Iteration 38276, loss = 2.54951141\n",
      "Iteration 38277, loss = 2.10064717\n",
      "Iteration 38278, loss = 2.18133102\n",
      "Iteration 38279, loss = 2.56101989\n",
      "Iteration 38280, loss = 2.69416655\n",
      "Iteration 38281, loss = 3.81244237\n",
      "Iteration 38282, loss = 3.28525198\n",
      "Iteration 38283, loss = 2.93101645\n",
      "Iteration 38284, loss = 2.57486163\n",
      "Iteration 38285, loss = 2.67787785\n",
      "Iteration 38286, loss = 2.69596744\n",
      "Iteration 38287, loss = 3.46065948\n",
      "Iteration 38288, loss = 3.69588257\n",
      "Iteration 38289, loss = 3.70004771\n",
      "Iteration 38290, loss = 4.86462527\n",
      "Iteration 38291, loss = 4.07195366\n",
      "Iteration 38292, loss = 4.09103743\n",
      "Iteration 38293, loss = 4.96110161\n",
      "Iteration 38294, loss = 5.29027872\n",
      "Iteration 38295, loss = 6.17557418\n",
      "Iteration 38296, loss = 4.99292405\n",
      "Iteration 38297, loss = 6.29822689\n",
      "Iteration 38298, loss = 5.01849236\n",
      "Iteration 38299, loss = 5.19139760\n",
      "Iteration 38300, loss = 4.32768099\n",
      "Iteration 38301, loss = 3.86512854\n",
      "Iteration 38302, loss = 4.28533060\n",
      "Iteration 38303, loss = 3.41324074\n",
      "Iteration 38304, loss = 3.49922490\n",
      "Iteration 38305, loss = 3.16256895\n",
      "Iteration 38306, loss = 2.79371769\n",
      "Iteration 38307, loss = 3.19656485\n",
      "Iteration 38308, loss = 3.34905305\n",
      "Iteration 38309, loss = 2.71598853\n",
      "Iteration 38310, loss = 2.66538029\n",
      "Iteration 38311, loss = 3.26327911\n",
      "Iteration 38312, loss = 2.83437947\n",
      "Iteration 38313, loss = 2.86623226\n",
      "Iteration 38314, loss = 3.03385180\n",
      "Iteration 38315, loss = 3.70206756\n",
      "Iteration 38316, loss = 3.08186961\n",
      "Iteration 38317, loss = 2.96707827\n",
      "Iteration 38318, loss = 3.13245585\n",
      "Iteration 38319, loss = 2.80328876\n",
      "Iteration 38320, loss = 2.77084106\n",
      "Iteration 38321, loss = 2.74111967\n",
      "Iteration 38322, loss = 2.43214862\n",
      "Iteration 38323, loss = 2.31889258\n",
      "Iteration 38324, loss = 2.56257898\n",
      "Iteration 38325, loss = 2.50192819\n",
      "Iteration 38326, loss = 2.64408300\n",
      "Iteration 38327, loss = 2.28929724\n",
      "Iteration 38328, loss = 2.10447324\n",
      "Iteration 38329, loss = 2.28337363\n",
      "Iteration 38330, loss = 2.33655186\n",
      "Iteration 38331, loss = 2.35553453\n",
      "Iteration 38332, loss = 2.29584530\n",
      "Iteration 38333, loss = 2.17976305\n",
      "Iteration 38334, loss = 2.28512104\n",
      "Iteration 38335, loss = 2.21968617\n",
      "Iteration 38336, loss = 2.41639214\n",
      "Iteration 38337, loss = 2.31054369\n",
      "Iteration 38338, loss = 2.66101961\n",
      "Iteration 38339, loss = 2.09980376\n",
      "Iteration 38340, loss = 2.03157606\n",
      "Iteration 38341, loss = 2.26099841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 38342, loss = 2.14146286\n",
      "Iteration 38343, loss = 2.20254324\n",
      "Iteration 38344, loss = 2.32802265\n",
      "Iteration 38345, loss = 2.93592712\n",
      "Iteration 38346, loss = 2.71496902\n",
      "Iteration 38347, loss = 3.04322005\n",
      "Iteration 38348, loss = 2.83306192\n",
      "Iteration 38349, loss = 2.59726135\n",
      "Iteration 38350, loss = 2.58392755\n",
      "Iteration 38351, loss = 2.70497929\n",
      "Iteration 38352, loss = 2.87587896\n",
      "Iteration 38353, loss = 2.81097172\n",
      "Iteration 38354, loss = 3.62950661\n",
      "Iteration 38355, loss = 3.46547816\n",
      "Iteration 38356, loss = 2.54979754\n",
      "Iteration 38357, loss = 2.24617062\n",
      "Iteration 38358, loss = 2.67596407\n",
      "Iteration 38359, loss = 2.65832270\n",
      "Iteration 38360, loss = 2.63810953\n",
      "Iteration 38361, loss = 2.66244660\n",
      "Iteration 38362, loss = 2.42830697\n",
      "Iteration 38363, loss = 2.39766063\n",
      "Iteration 38364, loss = 2.23375607\n",
      "Iteration 38365, loss = 2.60916054\n",
      "Iteration 38366, loss = 2.84116439\n",
      "Iteration 38367, loss = 3.31123932\n",
      "Iteration 38368, loss = 3.20835357\n",
      "Iteration 38369, loss = 3.86185602\n",
      "Iteration 38370, loss = 6.42450953\n",
      "Iteration 38371, loss = 7.87001597\n",
      "Iteration 38372, loss = 6.50394829\n",
      "Iteration 38373, loss = 6.63218166\n",
      "Iteration 38374, loss = 4.86536804\n",
      "Iteration 38375, loss = 3.84767594\n",
      "Iteration 38376, loss = 3.15082563\n",
      "Iteration 38377, loss = 3.54394407\n",
      "Iteration 38378, loss = 3.26081049\n",
      "Iteration 38379, loss = 3.33430689\n",
      "Iteration 38380, loss = 3.25033406\n",
      "Iteration 38381, loss = 2.83093098\n",
      "Iteration 38382, loss = 2.66046516\n",
      "Iteration 38383, loss = 2.67044679\n",
      "Iteration 38384, loss = 2.85102034\n",
      "Iteration 38385, loss = 2.83367879\n",
      "Iteration 38386, loss = 2.22221368\n",
      "Iteration 38387, loss = 1.95501278\n",
      "Iteration 38388, loss = 2.26274968\n",
      "Iteration 38389, loss = 2.46463238\n",
      "Iteration 38390, loss = 2.53696391\n",
      "Iteration 38391, loss = 3.33719587\n",
      "Iteration 38392, loss = 3.31208825\n",
      "Iteration 38393, loss = 3.49176576\n",
      "Iteration 38394, loss = 3.35253671\n",
      "Iteration 38395, loss = 2.65194421\n",
      "Iteration 38396, loss = 2.34430800\n",
      "Iteration 38397, loss = 2.11701890\n",
      "Iteration 38398, loss = 2.39779303\n",
      "Iteration 38399, loss = 2.23256134\n",
      "Iteration 38400, loss = 2.03891701\n",
      "Iteration 38401, loss = 2.09265544\n",
      "Iteration 38402, loss = 2.47249719\n",
      "Iteration 38403, loss = 2.21025173\n",
      "Iteration 38404, loss = 2.27309198\n",
      "Iteration 38405, loss = 2.57305037\n",
      "Iteration 38406, loss = 2.70531669\n",
      "Iteration 38407, loss = 2.56545958\n",
      "Iteration 38408, loss = 2.38162868\n",
      "Iteration 38409, loss = 2.39931060\n",
      "Iteration 38410, loss = 2.63848606\n",
      "Iteration 38411, loss = 2.68711832\n",
      "Iteration 38412, loss = 2.98354201\n",
      "Iteration 38413, loss = 2.13487445\n",
      "Iteration 38414, loss = 2.16242951\n",
      "Iteration 38415, loss = 2.10216059\n",
      "Iteration 38416, loss = 2.23876218\n",
      "Iteration 38417, loss = 2.06744117\n",
      "Iteration 38418, loss = 1.99437124\n",
      "Iteration 38419, loss = 2.03767956\n",
      "Iteration 38420, loss = 2.17368389\n",
      "Iteration 38421, loss = 2.44982672\n",
      "Iteration 38422, loss = 2.37244041\n",
      "Iteration 38423, loss = 2.24241574\n",
      "Iteration 38424, loss = 2.29531680\n",
      "Iteration 38425, loss = 1.99992526\n",
      "Iteration 38426, loss = 2.03735789\n",
      "Iteration 38427, loss = 2.21861113\n",
      "Iteration 38428, loss = 2.18989411\n",
      "Iteration 38429, loss = 2.24750212\n",
      "Iteration 38430, loss = 2.10023927\n",
      "Iteration 38431, loss = 2.50162424\n",
      "Iteration 38432, loss = 2.57967574\n",
      "Iteration 38433, loss = 2.18306172\n",
      "Iteration 38434, loss = 3.39522813\n",
      "Iteration 38435, loss = 3.35522449\n",
      "Iteration 38436, loss = 2.51238105\n",
      "Iteration 38437, loss = 2.45799353\n",
      "Iteration 38438, loss = 2.75803324\n",
      "Iteration 38439, loss = 2.44486389\n",
      "Iteration 38440, loss = 2.19046316\n",
      "Iteration 38441, loss = 2.35686374\n",
      "Iteration 38442, loss = 2.43120086\n",
      "Iteration 38443, loss = 2.33899797\n",
      "Iteration 38444, loss = 2.78560348\n",
      "Iteration 38445, loss = 2.64750386\n",
      "Iteration 38446, loss = 2.36656466\n",
      "Iteration 38447, loss = 2.35251778\n",
      "Iteration 38448, loss = 2.33971213\n",
      "Iteration 38449, loss = 2.48573598\n",
      "Iteration 38450, loss = 3.59323232\n",
      "Iteration 38451, loss = 4.23702318\n",
      "Iteration 38452, loss = 2.69942857\n",
      "Iteration 38453, loss = 3.19411203\n",
      "Iteration 38454, loss = 3.04244669\n",
      "Iteration 38455, loss = 3.04654473\n",
      "Iteration 38456, loss = 2.77596821\n",
      "Iteration 38457, loss = 3.39943494\n",
      "Iteration 38458, loss = 2.53899023\n",
      "Iteration 38459, loss = 3.02648015\n",
      "Iteration 38460, loss = 3.72825299\n",
      "Iteration 38461, loss = 4.08416533\n",
      "Iteration 38462, loss = 2.65844894\n",
      "Iteration 38463, loss = 2.74757801\n",
      "Iteration 38464, loss = 2.48985866\n",
      "Iteration 38465, loss = 2.54576201\n",
      "Iteration 38466, loss = 2.68177897\n",
      "Iteration 38467, loss = 2.19881909\n",
      "Iteration 38468, loss = 2.14291463\n",
      "Iteration 38469, loss = 2.31525632\n",
      "Iteration 38470, loss = 2.15648324\n",
      "Iteration 38471, loss = 2.27434314\n",
      "Iteration 38472, loss = 2.58172982\n",
      "Iteration 38473, loss = 2.42939768\n",
      "Iteration 38474, loss = 2.36341055\n",
      "Iteration 38475, loss = 2.29762465\n",
      "Iteration 38476, loss = 2.31244057\n",
      "Iteration 38477, loss = 2.11598751\n",
      "Iteration 38478, loss = 2.18801645\n",
      "Iteration 38479, loss = 2.25558287\n",
      "Iteration 38480, loss = 2.30187710\n",
      "Iteration 38481, loss = 2.06528558\n",
      "Iteration 38482, loss = 2.02816702\n",
      "Iteration 38483, loss = 1.97053216\n",
      "Iteration 38484, loss = 1.97272568\n",
      "Iteration 38485, loss = 2.13272358\n",
      "Iteration 38486, loss = 2.05409297\n",
      "Iteration 38487, loss = 2.06749559\n",
      "Iteration 38488, loss = 2.00514169\n",
      "Iteration 38489, loss = 2.15779120\n",
      "Iteration 38490, loss = 2.31640080\n",
      "Iteration 38491, loss = 3.11423078\n",
      "Iteration 38492, loss = 2.69239776\n",
      "Iteration 38493, loss = 4.19993840\n",
      "Iteration 38494, loss = 3.59068380\n",
      "Iteration 38495, loss = 3.38569941\n",
      "Iteration 38496, loss = 2.97208153\n",
      "Iteration 38497, loss = 2.88970487\n",
      "Iteration 38498, loss = 2.82537177\n",
      "Iteration 38499, loss = 2.79491084\n",
      "Iteration 38500, loss = 2.32623827\n",
      "Iteration 38501, loss = 2.61765163\n",
      "Iteration 38502, loss = 2.23403198\n",
      "Iteration 38503, loss = 2.27727498\n",
      "Iteration 38504, loss = 2.17332532\n",
      "Iteration 38505, loss = 2.28546644\n",
      "Iteration 38506, loss = 2.24380809\n",
      "Iteration 38507, loss = 2.29744875\n",
      "Iteration 38508, loss = 1.93051758\n",
      "Iteration 38509, loss = 2.33225834\n",
      "Iteration 38510, loss = 2.20246667\n",
      "Iteration 38511, loss = 2.32052879\n",
      "Iteration 38512, loss = 2.05274913\n",
      "Iteration 38513, loss = 2.31950101\n",
      "Iteration 38514, loss = 2.67831565\n",
      "Iteration 38515, loss = 2.64886021\n",
      "Iteration 38516, loss = 2.48031860\n",
      "Iteration 38517, loss = 2.30391307\n",
      "Iteration 38518, loss = 2.23111922\n",
      "Iteration 38519, loss = 2.48315251\n",
      "Iteration 38520, loss = 2.18973595\n",
      "Iteration 38521, loss = 2.02893891\n",
      "Iteration 38522, loss = 2.72665491\n",
      "Iteration 38523, loss = 2.29024424\n",
      "Iteration 38524, loss = 2.20188955\n",
      "Iteration 38525, loss = 2.36468848\n",
      "Iteration 38526, loss = 2.21758906\n",
      "Iteration 38527, loss = 2.30093837\n",
      "Iteration 38528, loss = 2.14888345\n",
      "Iteration 38529, loss = 2.46615140\n",
      "Iteration 38530, loss = 2.32418276\n",
      "Iteration 38531, loss = 2.41066281\n",
      "Iteration 38532, loss = 2.45382992\n",
      "Iteration 38533, loss = 2.50318150\n",
      "Iteration 38534, loss = 2.76250471\n",
      "Iteration 38535, loss = 2.53772306\n",
      "Iteration 38536, loss = 3.04286614\n",
      "Iteration 38537, loss = 2.31198861\n",
      "Iteration 38538, loss = 2.34725816\n",
      "Iteration 38539, loss = 2.26725994\n",
      "Iteration 38540, loss = 2.11126410\n",
      "Iteration 38541, loss = 2.55181420\n",
      "Iteration 38542, loss = 2.49535898\n",
      "Iteration 38543, loss = 2.14445735\n",
      "Iteration 38544, loss = 2.29192599\n",
      "Iteration 38545, loss = 2.31279560\n",
      "Iteration 38546, loss = 2.43119972\n",
      "Iteration 38547, loss = 2.75088880\n",
      "Iteration 38548, loss = 2.90412276\n",
      "Iteration 38549, loss = 3.43932374\n",
      "Iteration 38550, loss = 3.16846494\n",
      "Iteration 38551, loss = 2.83021953\n",
      "Iteration 38552, loss = 2.51472037\n",
      "Iteration 38553, loss = 2.54245299\n",
      "Iteration 38554, loss = 2.53039169\n",
      "Iteration 38555, loss = 2.65287334\n",
      "Iteration 38556, loss = 2.55968897\n",
      "Iteration 38557, loss = 2.87230113\n",
      "Iteration 38558, loss = 2.43240606\n",
      "Iteration 38559, loss = 2.22528319\n",
      "Iteration 38560, loss = 2.32877335\n",
      "Iteration 38561, loss = 2.53866186\n",
      "Iteration 38562, loss = 2.32242825\n",
      "Iteration 38563, loss = 2.50621476\n",
      "Iteration 38564, loss = 2.26197629\n",
      "Iteration 38565, loss = 2.51504273\n",
      "Iteration 38566, loss = 2.64244656\n",
      "Iteration 38567, loss = 2.65477821\n",
      "Iteration 38568, loss = 2.32702331\n",
      "Iteration 38569, loss = 2.15319856\n",
      "Iteration 38570, loss = 2.51718962\n",
      "Iteration 38571, loss = 2.58793178\n",
      "Iteration 38572, loss = 2.27929969\n",
      "Iteration 38573, loss = 2.69443442\n",
      "Iteration 38574, loss = 3.00316695\n",
      "Iteration 38575, loss = 2.94714530\n",
      "Iteration 38576, loss = 2.78927682\n",
      "Iteration 38577, loss = 2.35641284\n",
      "Iteration 38578, loss = 3.30025622\n",
      "Iteration 38579, loss = 3.31386811\n",
      "Iteration 38580, loss = 3.13401646\n",
      "Iteration 38581, loss = 2.44041647\n",
      "Iteration 38582, loss = 2.65680370\n",
      "Iteration 38583, loss = 2.69721269\n",
      "Iteration 38584, loss = 2.15224361\n",
      "Iteration 38585, loss = 2.06754435\n",
      "Iteration 38586, loss = 1.99867427\n",
      "Iteration 38587, loss = 2.45122278\n",
      "Iteration 38588, loss = 2.52551008\n",
      "Iteration 38589, loss = 2.38730786\n",
      "Iteration 38590, loss = 2.31155149\n",
      "Iteration 38591, loss = 2.33995899\n",
      "Iteration 38592, loss = 2.27552270\n",
      "Iteration 38593, loss = 2.86701815\n",
      "Iteration 38594, loss = 2.57436116\n",
      "Iteration 38595, loss = 2.18898528\n",
      "Iteration 38596, loss = 2.06067554\n",
      "Iteration 38597, loss = 2.19163849\n",
      "Iteration 38598, loss = 2.04473672\n",
      "Iteration 38599, loss = 2.45174430\n",
      "Iteration 38600, loss = 2.44691347\n",
      "Iteration 38601, loss = 2.14848468\n",
      "Iteration 38602, loss = 2.28863406\n",
      "Iteration 38603, loss = 2.19673543\n",
      "Iteration 38604, loss = 1.93904253\n",
      "Iteration 38605, loss = 2.06652006\n",
      "Iteration 38606, loss = 2.62634162\n",
      "Iteration 38607, loss = 2.69241952\n",
      "Iteration 38608, loss = 2.77885216\n",
      "Iteration 38609, loss = 2.78105981\n",
      "Iteration 38610, loss = 2.67294808\n",
      "Iteration 38611, loss = 2.87903993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 38612, loss = 2.94915031\n",
      "Iteration 38613, loss = 2.56893775\n",
      "Iteration 38614, loss = 2.62803872\n",
      "Iteration 38615, loss = 3.13996396\n",
      "Iteration 38616, loss = 5.52614011\n",
      "Iteration 38617, loss = 3.67350441\n",
      "Iteration 38618, loss = 6.16345132\n",
      "Iteration 38619, loss = 3.26221481\n",
      "Iteration 38620, loss = 2.81534282\n",
      "Iteration 38621, loss = 2.12057954\n",
      "Iteration 38622, loss = 2.22113596\n",
      "Iteration 38623, loss = 2.17544112\n",
      "Iteration 38624, loss = 2.33025937\n",
      "Iteration 38625, loss = 2.35022260\n",
      "Iteration 38626, loss = 2.41407022\n",
      "Iteration 38627, loss = 2.48299799\n",
      "Iteration 38628, loss = 3.61908716\n",
      "Iteration 38629, loss = 3.40300550\n",
      "Iteration 38630, loss = 3.60217615\n",
      "Iteration 38631, loss = 3.48735609\n",
      "Iteration 38632, loss = 3.01758055\n",
      "Iteration 38633, loss = 3.03590817\n",
      "Iteration 38634, loss = 2.69291795\n",
      "Iteration 38635, loss = 2.24736336\n",
      "Iteration 38636, loss = 2.25178091\n",
      "Iteration 38637, loss = 2.10943205\n",
      "Iteration 38638, loss = 1.97981114\n",
      "Iteration 38639, loss = 2.06690812\n",
      "Iteration 38640, loss = 2.18404664\n",
      "Iteration 38641, loss = 2.36938356\n",
      "Iteration 38642, loss = 2.38300819\n",
      "Iteration 38643, loss = 2.25005294\n",
      "Iteration 38644, loss = 2.53646050\n",
      "Iteration 38645, loss = 2.02324624\n",
      "Iteration 38646, loss = 2.17343532\n",
      "Iteration 38647, loss = 2.51019509\n",
      "Iteration 38648, loss = 2.34476627\n",
      "Iteration 38649, loss = 2.84102985\n",
      "Iteration 38650, loss = 3.50981185\n",
      "Iteration 38651, loss = 2.82365007\n",
      "Iteration 38652, loss = 2.44651165\n",
      "Iteration 38653, loss = 2.63362533\n",
      "Iteration 38654, loss = 2.42819651\n",
      "Iteration 38655, loss = 2.44854662\n",
      "Iteration 38656, loss = 2.46529091\n",
      "Iteration 38657, loss = 2.27690062\n",
      "Iteration 38658, loss = 2.48198018\n",
      "Iteration 38659, loss = 2.43303426\n",
      "Iteration 38660, loss = 3.67829279\n",
      "Iteration 38661, loss = 3.44748783\n",
      "Iteration 38662, loss = 4.51192878\n",
      "Iteration 38663, loss = 3.72224950\n",
      "Iteration 38664, loss = 2.94488642\n",
      "Iteration 38665, loss = 3.12939051\n",
      "Iteration 38666, loss = 2.55428940\n",
      "Iteration 38667, loss = 2.47709353\n",
      "Iteration 38668, loss = 2.44516971\n",
      "Iteration 38669, loss = 2.38757274\n",
      "Iteration 38670, loss = 2.21641496\n",
      "Iteration 38671, loss = 2.26368368\n",
      "Iteration 38672, loss = 2.41738581\n",
      "Iteration 38673, loss = 2.75173121\n",
      "Iteration 38674, loss = 2.26718752\n",
      "Iteration 38675, loss = 2.12903683\n",
      "Iteration 38676, loss = 2.11065679\n",
      "Iteration 38677, loss = 2.01742854\n",
      "Iteration 38678, loss = 2.07223000\n",
      "Iteration 38679, loss = 2.22755202\n",
      "Iteration 38680, loss = 2.60211978\n",
      "Iteration 38681, loss = 2.38213216\n",
      "Iteration 38682, loss = 2.47035940\n",
      "Iteration 38683, loss = 2.34281798\n",
      "Iteration 38684, loss = 2.20672512\n",
      "Iteration 38685, loss = 2.34200222\n",
      "Iteration 38686, loss = 2.25942676\n",
      "Iteration 38687, loss = 2.07414609\n",
      "Iteration 38688, loss = 2.18871468\n",
      "Iteration 38689, loss = 2.28752719\n",
      "Iteration 38690, loss = 2.50424083\n",
      "Iteration 38691, loss = 2.90492091\n",
      "Iteration 38692, loss = 3.20414466\n",
      "Iteration 38693, loss = 3.11957311\n",
      "Iteration 38694, loss = 2.36834414\n",
      "Iteration 38695, loss = 2.47653475\n",
      "Iteration 38696, loss = 2.62430364\n",
      "Iteration 38697, loss = 2.62760329\n",
      "Iteration 38698, loss = 3.37040429\n",
      "Iteration 38699, loss = 3.14162334\n",
      "Iteration 38700, loss = 2.83305641\n",
      "Iteration 38701, loss = 3.12001614\n",
      "Iteration 38702, loss = 2.50002670\n",
      "Iteration 38703, loss = 2.54510833\n",
      "Iteration 38704, loss = 2.95619963\n",
      "Iteration 38705, loss = 2.73434341\n",
      "Iteration 38706, loss = 2.18408848\n",
      "Iteration 38707, loss = 2.79281818\n",
      "Iteration 38708, loss = 2.40923240\n",
      "Iteration 38709, loss = 2.23496395\n",
      "Iteration 38710, loss = 2.86502387\n",
      "Iteration 38711, loss = 2.62483834\n",
      "Iteration 38712, loss = 2.92806517\n",
      "Iteration 38713, loss = 3.25125001\n",
      "Iteration 38714, loss = 3.23142542\n",
      "Iteration 38715, loss = 2.69436675\n",
      "Iteration 38716, loss = 2.96439034\n",
      "Iteration 38717, loss = 2.58746977\n",
      "Iteration 38718, loss = 2.61851422\n",
      "Iteration 38719, loss = 2.88602843\n",
      "Iteration 38720, loss = 3.18081187\n",
      "Iteration 38721, loss = 2.29257592\n",
      "Iteration 38722, loss = 2.38364096\n",
      "Iteration 38723, loss = 2.22014488\n",
      "Iteration 38724, loss = 2.72803519\n",
      "Iteration 38725, loss = 2.16127804\n",
      "Iteration 38726, loss = 2.09194837\n",
      "Iteration 38727, loss = 2.11365820\n",
      "Iteration 38728, loss = 2.15873425\n",
      "Iteration 38729, loss = 1.93998314\n",
      "Iteration 38730, loss = 2.22089973\n",
      "Iteration 38731, loss = 2.49531830\n",
      "Iteration 38732, loss = 2.11653349\n",
      "Iteration 38733, loss = 2.71458822\n",
      "Iteration 38734, loss = 3.01867087\n",
      "Iteration 38735, loss = 2.61520039\n",
      "Iteration 38736, loss = 2.43441270\n",
      "Iteration 38737, loss = 2.38810650\n",
      "Iteration 38738, loss = 2.28274960\n",
      "Iteration 38739, loss = 2.01779383\n",
      "Iteration 38740, loss = 2.12976082\n",
      "Iteration 38741, loss = 2.36716225\n",
      "Iteration 38742, loss = 2.86542616\n",
      "Iteration 38743, loss = 2.53056085\n",
      "Iteration 38744, loss = 1.99763720\n",
      "Iteration 38745, loss = 1.97837813\n",
      "Iteration 38746, loss = 2.43534294\n",
      "Iteration 38747, loss = 2.44928728\n",
      "Iteration 38748, loss = 2.39889839\n",
      "Iteration 38749, loss = 2.50659463\n",
      "Iteration 38750, loss = 3.75415748\n",
      "Iteration 38751, loss = 2.96857002\n",
      "Iteration 38752, loss = 2.68116659\n",
      "Iteration 38753, loss = 2.53469074\n",
      "Iteration 38754, loss = 2.27747599\n",
      "Iteration 38755, loss = 2.59622944\n",
      "Iteration 38756, loss = 2.88198727\n",
      "Iteration 38757, loss = 2.78486985\n",
      "Iteration 38758, loss = 3.08665041\n",
      "Iteration 38759, loss = 2.85512444\n",
      "Iteration 38760, loss = 2.23109561\n",
      "Iteration 38761, loss = 2.31348638\n",
      "Iteration 38762, loss = 1.98271390\n",
      "Iteration 38763, loss = 2.08199200\n",
      "Iteration 38764, loss = 1.92052672\n",
      "Iteration 38765, loss = 2.15139917\n",
      "Iteration 38766, loss = 2.90894093\n",
      "Iteration 38767, loss = 3.12766537\n",
      "Iteration 38768, loss = 3.20428529\n",
      "Iteration 38769, loss = 2.90911303\n",
      "Iteration 38770, loss = 2.77633679\n",
      "Iteration 38771, loss = 2.82359862\n",
      "Iteration 38772, loss = 2.58779676\n",
      "Iteration 38773, loss = 2.50970781\n",
      "Iteration 38774, loss = 2.90582311\n",
      "Iteration 38775, loss = 3.35293439\n",
      "Iteration 38776, loss = 3.42103969\n",
      "Iteration 38777, loss = 3.62269495\n",
      "Iteration 38778, loss = 3.25979162\n",
      "Iteration 38779, loss = 2.70962075\n",
      "Iteration 38780, loss = 2.39676989\n",
      "Iteration 38781, loss = 2.60055373\n",
      "Iteration 38782, loss = 1.97024628\n",
      "Iteration 38783, loss = 2.16402097\n",
      "Iteration 38784, loss = 2.05148011\n",
      "Iteration 38785, loss = 2.39326479\n",
      "Iteration 38786, loss = 2.27651909\n",
      "Iteration 38787, loss = 2.18475203\n",
      "Iteration 38788, loss = 2.12012606\n",
      "Iteration 38789, loss = 1.99520176\n",
      "Iteration 38790, loss = 2.26883319\n",
      "Iteration 38791, loss = 2.02934263\n",
      "Iteration 38792, loss = 2.19397766\n",
      "Iteration 38793, loss = 2.22100266\n",
      "Iteration 38794, loss = 1.99012095\n",
      "Iteration 38795, loss = 1.99691109\n",
      "Iteration 38796, loss = 2.17935057\n",
      "Iteration 38797, loss = 2.37475702\n",
      "Iteration 38798, loss = 2.11595683\n",
      "Iteration 38799, loss = 2.13919796\n",
      "Iteration 38800, loss = 2.29767400\n",
      "Iteration 38801, loss = 2.23011033\n",
      "Iteration 38802, loss = 2.28558527\n",
      "Iteration 38803, loss = 2.36720130\n",
      "Iteration 38804, loss = 2.81597872\n",
      "Iteration 38805, loss = 2.42208890\n",
      "Iteration 38806, loss = 2.21432515\n",
      "Iteration 38807, loss = 2.36953501\n",
      "Iteration 38808, loss = 2.77454811\n",
      "Iteration 38809, loss = 3.07571663\n",
      "Iteration 38810, loss = 2.33424076\n",
      "Iteration 38811, loss = 2.55801441\n",
      "Iteration 38812, loss = 2.75843373\n",
      "Iteration 38813, loss = 3.05549632\n",
      "Iteration 38814, loss = 2.77312554\n",
      "Iteration 38815, loss = 3.07596046\n",
      "Iteration 38816, loss = 2.93245951\n",
      "Iteration 38817, loss = 2.62301969\n",
      "Iteration 38818, loss = 2.60869981\n",
      "Iteration 38819, loss = 3.03692403\n",
      "Iteration 38820, loss = 3.07742146\n",
      "Iteration 38821, loss = 2.54127255\n",
      "Iteration 38822, loss = 2.66662871\n",
      "Iteration 38823, loss = 2.80617719\n",
      "Iteration 38824, loss = 2.63138245\n",
      "Iteration 38825, loss = 2.95439014\n",
      "Iteration 38826, loss = 2.21051499\n",
      "Iteration 38827, loss = 2.35296604\n",
      "Iteration 38828, loss = 2.28723731\n",
      "Iteration 38829, loss = 2.71834110\n",
      "Iteration 38830, loss = 2.64695854\n",
      "Iteration 38831, loss = 3.05386008\n",
      "Iteration 38832, loss = 2.80595668\n",
      "Iteration 38833, loss = 2.41023466\n",
      "Iteration 38834, loss = 2.94644493\n",
      "Iteration 38835, loss = 2.64693641\n",
      "Iteration 38836, loss = 3.25624851\n",
      "Iteration 38837, loss = 3.87368022\n",
      "Iteration 38838, loss = 2.90303008\n",
      "Iteration 38839, loss = 2.81547304\n",
      "Iteration 38840, loss = 2.48069181\n",
      "Iteration 38841, loss = 2.29506158\n",
      "Iteration 38842, loss = 3.09557115\n",
      "Iteration 38843, loss = 2.77422218\n",
      "Iteration 38844, loss = 2.69124384\n",
      "Iteration 38845, loss = 2.97688816\n",
      "Iteration 38846, loss = 3.08390790\n",
      "Iteration 38847, loss = 2.45808082\n",
      "Iteration 38848, loss = 2.92838161\n",
      "Iteration 38849, loss = 2.83484661\n",
      "Iteration 38850, loss = 3.03123270\n",
      "Iteration 38851, loss = 3.05030111\n",
      "Iteration 38852, loss = 2.40639588\n",
      "Iteration 38853, loss = 2.57735835\n",
      "Iteration 38854, loss = 2.53360712\n",
      "Iteration 38855, loss = 2.31665587\n",
      "Iteration 38856, loss = 2.73419032\n",
      "Iteration 38857, loss = 2.42477409\n",
      "Iteration 38858, loss = 3.33858659\n",
      "Iteration 38859, loss = 3.95660960\n",
      "Iteration 38860, loss = 4.61000463\n",
      "Iteration 38861, loss = 3.71094446\n",
      "Iteration 38862, loss = 3.27750687\n",
      "Iteration 38863, loss = 4.09785032\n",
      "Iteration 38864, loss = 3.54063479\n",
      "Iteration 38865, loss = 3.15408569\n",
      "Iteration 38866, loss = 2.79316435\n",
      "Iteration 38867, loss = 2.46994655\n",
      "Iteration 38868, loss = 2.48533511\n",
      "Iteration 38869, loss = 2.83877063\n",
      "Iteration 38870, loss = 2.43414139\n",
      "Iteration 38871, loss = 2.18181859\n",
      "Iteration 38872, loss = 2.89886171\n",
      "Iteration 38873, loss = 2.69509786\n",
      "Iteration 38874, loss = 2.79257281\n",
      "Iteration 38875, loss = 2.70053102\n",
      "Iteration 38876, loss = 2.53616828\n",
      "Iteration 38877, loss = 2.88500347\n",
      "Iteration 38878, loss = 2.46047147\n",
      "Iteration 38879, loss = 3.31949599\n",
      "Iteration 38880, loss = 2.69398648\n",
      "Iteration 38881, loss = 3.89291373\n",
      "Iteration 38882, loss = 4.45165228\n",
      "Iteration 38883, loss = 3.48807932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 38884, loss = 3.64878586\n",
      "Iteration 38885, loss = 4.19539169\n",
      "Iteration 38886, loss = 4.01763209\n",
      "Iteration 38887, loss = 3.00822000\n",
      "Iteration 38888, loss = 3.29547196\n",
      "Iteration 38889, loss = 3.59245997\n",
      "Iteration 38890, loss = 2.50090126\n",
      "Iteration 38891, loss = 2.52417890\n",
      "Iteration 38892, loss = 2.57092870\n",
      "Iteration 38893, loss = 2.32393211\n",
      "Iteration 38894, loss = 2.21439607\n",
      "Iteration 38895, loss = 2.59246942\n",
      "Iteration 38896, loss = 2.33787027\n",
      "Iteration 38897, loss = 2.43713340\n",
      "Iteration 38898, loss = 2.44989233\n",
      "Iteration 38899, loss = 2.93462643\n",
      "Iteration 38900, loss = 3.01299548\n",
      "Iteration 38901, loss = 2.36299563\n",
      "Iteration 38902, loss = 2.03361546\n",
      "Iteration 38903, loss = 2.02946067\n",
      "Iteration 38904, loss = 2.09431647\n",
      "Iteration 38905, loss = 2.22175778\n",
      "Iteration 38906, loss = 2.14689061\n",
      "Iteration 38907, loss = 2.21074081\n",
      "Iteration 38908, loss = 2.39694797\n",
      "Iteration 38909, loss = 2.00718740\n",
      "Iteration 38910, loss = 2.11840721\n",
      "Iteration 38911, loss = 2.47499719\n",
      "Iteration 38912, loss = 2.21070833\n",
      "Iteration 38913, loss = 2.42336761\n",
      "Iteration 38914, loss = 2.40332292\n",
      "Iteration 38915, loss = 2.13913159\n",
      "Iteration 38916, loss = 2.20582051\n",
      "Iteration 38917, loss = 2.66643064\n",
      "Iteration 38918, loss = 3.29636052\n",
      "Iteration 38919, loss = 2.97638581\n",
      "Iteration 38920, loss = 2.47820915\n",
      "Iteration 38921, loss = 2.25312456\n",
      "Iteration 38922, loss = 2.01976326\n",
      "Iteration 38923, loss = 2.01689258\n",
      "Iteration 38924, loss = 2.01691233\n",
      "Iteration 38925, loss = 2.04452377\n",
      "Iteration 38926, loss = 2.28240610\n",
      "Iteration 38927, loss = 2.37024398\n",
      "Iteration 38928, loss = 2.71625070\n",
      "Iteration 38929, loss = 2.42800052\n",
      "Iteration 38930, loss = 2.22529378\n",
      "Iteration 38931, loss = 1.98607066\n",
      "Iteration 38932, loss = 1.86554746\n",
      "Iteration 38933, loss = 1.95124004\n",
      "Iteration 38934, loss = 1.93818223\n",
      "Iteration 38935, loss = 2.02676460\n",
      "Iteration 38936, loss = 2.19100410\n",
      "Iteration 38937, loss = 2.87773439\n",
      "Iteration 38938, loss = 2.24099739\n",
      "Iteration 38939, loss = 2.47316563\n",
      "Iteration 38940, loss = 2.43634588\n",
      "Iteration 38941, loss = 2.93586031\n",
      "Iteration 38942, loss = 2.87302931\n",
      "Iteration 38943, loss = 2.79758918\n",
      "Iteration 38944, loss = 2.82492708\n",
      "Iteration 38945, loss = 3.16126599\n",
      "Iteration 38946, loss = 2.06481370\n",
      "Iteration 38947, loss = 2.21706982\n",
      "Iteration 38948, loss = 2.23562312\n",
      "Iteration 38949, loss = 1.96257354\n",
      "Iteration 38950, loss = 2.29433288\n",
      "Iteration 38951, loss = 2.64215852\n",
      "Iteration 38952, loss = 2.36234607\n",
      "Iteration 38953, loss = 2.54017125\n",
      "Iteration 38954, loss = 2.59388002\n",
      "Iteration 38955, loss = 2.35499949\n",
      "Iteration 38956, loss = 2.20488692\n",
      "Iteration 38957, loss = 2.88918031\n",
      "Iteration 38958, loss = 2.80013995\n",
      "Iteration 38959, loss = 2.46692063\n",
      "Iteration 38960, loss = 2.11018311\n",
      "Iteration 38961, loss = 2.22072932\n",
      "Iteration 38962, loss = 2.00798749\n",
      "Iteration 38963, loss = 2.37588088\n",
      "Iteration 38964, loss = 2.04567206\n",
      "Iteration 38965, loss = 2.54403854\n",
      "Iteration 38966, loss = 2.14462122\n",
      "Iteration 38967, loss = 2.21294260\n",
      "Iteration 38968, loss = 1.98715690\n",
      "Iteration 38969, loss = 1.95607113\n",
      "Iteration 38970, loss = 2.70521109\n",
      "Iteration 38971, loss = 2.82389707\n",
      "Iteration 38972, loss = 2.58268610\n",
      "Iteration 38973, loss = 2.82002354\n",
      "Iteration 38974, loss = 1.98776367\n",
      "Iteration 38975, loss = 2.25194150\n",
      "Iteration 38976, loss = 2.50335770\n",
      "Iteration 38977, loss = 2.32099188\n",
      "Iteration 38978, loss = 3.23489717\n",
      "Iteration 38979, loss = 3.22605463\n",
      "Iteration 38980, loss = 3.65276616\n",
      "Iteration 38981, loss = 2.88061289\n",
      "Iteration 38982, loss = 3.20198883\n",
      "Iteration 38983, loss = 2.92249036\n",
      "Iteration 38984, loss = 2.91781943\n",
      "Iteration 38985, loss = 3.55426689\n",
      "Iteration 38986, loss = 2.35284861\n",
      "Iteration 38987, loss = 2.50488855\n",
      "Iteration 38988, loss = 2.41952981\n",
      "Iteration 38989, loss = 2.08497798\n",
      "Iteration 38990, loss = 2.34161821\n",
      "Iteration 38991, loss = 2.67803076\n",
      "Iteration 38992, loss = 2.51976903\n",
      "Iteration 38993, loss = 3.14152569\n",
      "Iteration 38994, loss = 2.81973455\n",
      "Iteration 38995, loss = 2.45989940\n",
      "Iteration 38996, loss = 2.50616740\n",
      "Iteration 38997, loss = 2.62414027\n",
      "Iteration 38998, loss = 2.72489106\n",
      "Iteration 38999, loss = 2.62321811\n",
      "Iteration 39000, loss = 2.59982021\n",
      "Iteration 39001, loss = 2.60354202\n",
      "Iteration 39002, loss = 2.35515629\n",
      "Iteration 39003, loss = 2.26348628\n",
      "Iteration 39004, loss = 2.65688946\n",
      "Iteration 39005, loss = 2.51319010\n",
      "Iteration 39006, loss = 2.68276778\n",
      "Iteration 39007, loss = 2.90173379\n",
      "Iteration 39008, loss = 3.00903235\n",
      "Iteration 39009, loss = 2.32722150\n",
      "Iteration 39010, loss = 2.29357576\n",
      "Iteration 39011, loss = 2.36789436\n",
      "Iteration 39012, loss = 2.71247037\n",
      "Iteration 39013, loss = 2.83515760\n",
      "Iteration 39014, loss = 2.74821271\n",
      "Iteration 39015, loss = 2.26512595\n",
      "Iteration 39016, loss = 2.23412745\n",
      "Iteration 39017, loss = 2.53416450\n",
      "Iteration 39018, loss = 2.49531469\n",
      "Iteration 39019, loss = 2.57874575\n",
      "Iteration 39020, loss = 2.46684786\n",
      "Iteration 39021, loss = 2.28742047\n",
      "Iteration 39022, loss = 2.37419888\n",
      "Iteration 39023, loss = 2.22913447\n",
      "Iteration 39024, loss = 2.64269299\n",
      "Iteration 39025, loss = 2.47221676\n",
      "Iteration 39026, loss = 3.69073887\n",
      "Iteration 39027, loss = 4.06653440\n",
      "Iteration 39028, loss = 2.52036511\n",
      "Iteration 39029, loss = 3.01664636\n",
      "Iteration 39030, loss = 2.91565883\n",
      "Iteration 39031, loss = 3.03760041\n",
      "Iteration 39032, loss = 2.72244428\n",
      "Iteration 39033, loss = 3.43166785\n",
      "Iteration 39034, loss = 2.95712522\n",
      "Iteration 39035, loss = 2.76469479\n",
      "Iteration 39036, loss = 2.38278864\n",
      "Iteration 39037, loss = 2.51364501\n",
      "Iteration 39038, loss = 2.36400409\n",
      "Iteration 39039, loss = 2.35177489\n",
      "Iteration 39040, loss = 2.34727554\n",
      "Iteration 39041, loss = 2.08616572\n",
      "Iteration 39042, loss = 2.35429128\n",
      "Iteration 39043, loss = 2.27763143\n",
      "Iteration 39044, loss = 2.16339942\n",
      "Iteration 39045, loss = 2.13777866\n",
      "Iteration 39046, loss = 2.56249712\n",
      "Iteration 39047, loss = 2.31119902\n",
      "Iteration 39048, loss = 2.56824703\n",
      "Iteration 39049, loss = 2.28419744\n",
      "Iteration 39050, loss = 2.26938764\n",
      "Iteration 39051, loss = 2.22605396\n",
      "Iteration 39052, loss = 2.17754500\n",
      "Iteration 39053, loss = 2.00237873\n",
      "Iteration 39054, loss = 2.12006860\n",
      "Iteration 39055, loss = 2.38149245\n",
      "Iteration 39056, loss = 2.16105172\n",
      "Iteration 39057, loss = 2.37918425\n",
      "Iteration 39058, loss = 2.29711298\n",
      "Iteration 39059, loss = 2.08351886\n",
      "Iteration 39060, loss = 2.30804611\n",
      "Iteration 39061, loss = 2.66261412\n",
      "Iteration 39062, loss = 2.66239730\n",
      "Iteration 39063, loss = 2.82286151\n",
      "Iteration 39064, loss = 2.99477911\n",
      "Iteration 39065, loss = 2.94419944\n",
      "Iteration 39066, loss = 3.00997288\n",
      "Iteration 39067, loss = 3.13475118\n",
      "Iteration 39068, loss = 2.99680887\n",
      "Iteration 39069, loss = 2.38824218\n",
      "Iteration 39070, loss = 2.72425940\n",
      "Iteration 39071, loss = 2.10822990\n",
      "Iteration 39072, loss = 2.86798235\n",
      "Iteration 39073, loss = 3.17683911\n",
      "Iteration 39074, loss = 3.09710336\n",
      "Iteration 39075, loss = 3.26770272\n",
      "Iteration 39076, loss = 2.97870985\n",
      "Iteration 39077, loss = 3.68916253\n",
      "Iteration 39078, loss = 3.36766388\n",
      "Iteration 39079, loss = 3.68947023\n",
      "Iteration 39080, loss = 3.19056728\n",
      "Iteration 39081, loss = 3.36894177\n",
      "Iteration 39082, loss = 2.88250993\n",
      "Iteration 39083, loss = 3.21059854\n",
      "Iteration 39084, loss = 3.17737835\n",
      "Iteration 39085, loss = 2.26171107\n",
      "Iteration 39086, loss = 2.63881405\n",
      "Iteration 39087, loss = 2.74229412\n",
      "Iteration 39088, loss = 2.43083388\n",
      "Iteration 39089, loss = 2.47490916\n",
      "Iteration 39090, loss = 2.37562568\n",
      "Iteration 39091, loss = 2.82922879\n",
      "Iteration 39092, loss = 2.52962717\n",
      "Iteration 39093, loss = 2.76508357\n",
      "Iteration 39094, loss = 2.88471052\n",
      "Iteration 39095, loss = 3.38623788\n",
      "Iteration 39096, loss = 4.15425974\n",
      "Iteration 39097, loss = 4.65724657\n",
      "Iteration 39098, loss = 4.79107221\n",
      "Iteration 39099, loss = 4.86583517\n",
      "Iteration 39100, loss = 3.71707569\n",
      "Iteration 39101, loss = 4.46079798\n",
      "Iteration 39102, loss = 2.58997240\n",
      "Iteration 39103, loss = 3.22127270\n",
      "Iteration 39104, loss = 2.79389090\n",
      "Iteration 39105, loss = 2.99628816\n",
      "Iteration 39106, loss = 3.07154246\n",
      "Iteration 39107, loss = 3.41464639\n",
      "Iteration 39108, loss = 5.21654814\n",
      "Iteration 39109, loss = 5.55900002\n",
      "Iteration 39110, loss = 5.75916332\n",
      "Iteration 39111, loss = 3.80703010\n",
      "Iteration 39112, loss = 4.18066754\n",
      "Iteration 39113, loss = 3.22716059\n",
      "Iteration 39114, loss = 3.27903761\n",
      "Iteration 39115, loss = 3.20065310\n",
      "Iteration 39116, loss = 3.14358539\n",
      "Iteration 39117, loss = 2.88544162\n",
      "Iteration 39118, loss = 2.14329982\n",
      "Iteration 39119, loss = 2.12123221\n",
      "Iteration 39120, loss = 1.93308110\n",
      "Iteration 39121, loss = 1.86927464\n",
      "Iteration 39122, loss = 2.03280477\n",
      "Iteration 39123, loss = 1.95238549\n",
      "Iteration 39124, loss = 1.97491431\n",
      "Iteration 39125, loss = 1.92709922\n",
      "Iteration 39126, loss = 1.98289444\n",
      "Iteration 39127, loss = 2.18140261\n",
      "Iteration 39128, loss = 2.76480069\n",
      "Iteration 39129, loss = 2.41408450\n",
      "Iteration 39130, loss = 2.16298520\n",
      "Iteration 39131, loss = 2.25723889\n",
      "Iteration 39132, loss = 2.39957317\n",
      "Iteration 39133, loss = 2.26351284\n",
      "Iteration 39134, loss = 2.13677721\n",
      "Iteration 39135, loss = 1.94435261\n",
      "Iteration 39136, loss = 2.06890271\n",
      "Iteration 39137, loss = 2.18572534\n",
      "Iteration 39138, loss = 2.30950348\n",
      "Iteration 39139, loss = 2.38272319\n",
      "Iteration 39140, loss = 2.53189195\n",
      "Iteration 39141, loss = 2.55461076\n",
      "Iteration 39142, loss = 2.51579853\n",
      "Iteration 39143, loss = 2.71854159\n",
      "Iteration 39144, loss = 2.47928472\n",
      "Iteration 39145, loss = 2.68973795\n",
      "Iteration 39146, loss = 2.93667661\n",
      "Iteration 39147, loss = 2.68055393\n",
      "Iteration 39148, loss = 2.71211514\n",
      "Iteration 39149, loss = 2.86637971\n",
      "Iteration 39150, loss = 2.48290513\n",
      "Iteration 39151, loss = 2.80419612\n",
      "Iteration 39152, loss = 2.76492668\n",
      "Iteration 39153, loss = 2.92122614\n",
      "Iteration 39154, loss = 2.42757832\n",
      "Iteration 39155, loss = 2.74526756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 39156, loss = 2.52139948\n",
      "Iteration 39157, loss = 2.15631451\n",
      "Iteration 39158, loss = 2.60525674\n",
      "Iteration 39159, loss = 2.19947105\n",
      "Iteration 39160, loss = 2.18587502\n",
      "Iteration 39161, loss = 2.15738721\n",
      "Iteration 39162, loss = 1.98157615\n",
      "Iteration 39163, loss = 2.27980274\n",
      "Iteration 39164, loss = 2.21617012\n",
      "Iteration 39165, loss = 2.30297082\n",
      "Iteration 39166, loss = 2.84574133\n",
      "Iteration 39167, loss = 2.41141562\n",
      "Iteration 39168, loss = 2.21712476\n",
      "Iteration 39169, loss = 2.07088446\n",
      "Iteration 39170, loss = 2.25374956\n",
      "Iteration 39171, loss = 2.44575220\n",
      "Iteration 39172, loss = 1.98119076\n",
      "Iteration 39173, loss = 2.37270364\n",
      "Iteration 39174, loss = 2.25291952\n",
      "Iteration 39175, loss = 2.43904983\n",
      "Iteration 39176, loss = 2.62778266\n",
      "Iteration 39177, loss = 2.69249991\n",
      "Iteration 39178, loss = 2.16445127\n",
      "Iteration 39179, loss = 2.20025997\n",
      "Iteration 39180, loss = 3.06881763\n",
      "Iteration 39181, loss = 2.87852161\n",
      "Iteration 39182, loss = 2.87365079\n",
      "Iteration 39183, loss = 3.30603943\n",
      "Iteration 39184, loss = 3.67067062\n",
      "Iteration 39185, loss = 3.39273173\n",
      "Iteration 39186, loss = 2.98195908\n",
      "Iteration 39187, loss = 3.40764699\n",
      "Iteration 39188, loss = 2.61139718\n",
      "Iteration 39189, loss = 2.18169619\n",
      "Iteration 39190, loss = 1.99403834\n",
      "Iteration 39191, loss = 1.85036347\n",
      "Iteration 39192, loss = 1.87930121\n",
      "Iteration 39193, loss = 1.98951922\n",
      "Iteration 39194, loss = 1.93476662\n",
      "Iteration 39195, loss = 2.08576821\n",
      "Iteration 39196, loss = 1.92841573\n",
      "Iteration 39197, loss = 2.14556908\n",
      "Iteration 39198, loss = 2.18980263\n",
      "Iteration 39199, loss = 1.86924159\n",
      "Iteration 39200, loss = 1.86815597\n",
      "Iteration 39201, loss = 2.29133673\n",
      "Iteration 39202, loss = 1.95238002\n",
      "Iteration 39203, loss = 2.06008229\n",
      "Iteration 39204, loss = 2.04241381\n",
      "Iteration 39205, loss = 2.24752317\n",
      "Iteration 39206, loss = 2.12934791\n",
      "Iteration 39207, loss = 2.14276237\n",
      "Iteration 39208, loss = 2.43777027\n",
      "Iteration 39209, loss = 2.58317110\n",
      "Iteration 39210, loss = 2.73901102\n",
      "Iteration 39211, loss = 3.32227643\n",
      "Iteration 39212, loss = 3.06580416\n",
      "Iteration 39213, loss = 2.54135896\n",
      "Iteration 39214, loss = 2.83420972\n",
      "Iteration 39215, loss = 2.71089800\n",
      "Iteration 39216, loss = 2.51596365\n",
      "Iteration 39217, loss = 2.39891904\n",
      "Iteration 39218, loss = 2.75373250\n",
      "Iteration 39219, loss = 3.03269153\n",
      "Iteration 39220, loss = 3.50819766\n",
      "Iteration 39221, loss = 3.26149124\n",
      "Iteration 39222, loss = 2.78535781\n",
      "Iteration 39223, loss = 2.23927724\n",
      "Iteration 39224, loss = 2.03726360\n",
      "Iteration 39225, loss = 2.28804373\n",
      "Iteration 39226, loss = 2.51227422\n",
      "Iteration 39227, loss = 2.07467023\n",
      "Iteration 39228, loss = 2.23262448\n",
      "Iteration 39229, loss = 2.38514242\n",
      "Iteration 39230, loss = 2.36964089\n",
      "Iteration 39231, loss = 2.37904610\n",
      "Iteration 39232, loss = 2.55862835\n",
      "Iteration 39233, loss = 2.74278381\n",
      "Iteration 39234, loss = 2.58222186\n",
      "Iteration 39235, loss = 2.49431738\n",
      "Iteration 39236, loss = 2.44046855\n",
      "Iteration 39237, loss = 2.02610699\n",
      "Iteration 39238, loss = 2.00536231\n",
      "Iteration 39239, loss = 2.16010339\n",
      "Iteration 39240, loss = 2.28687232\n",
      "Iteration 39241, loss = 2.12626596\n",
      "Iteration 39242, loss = 2.08469651\n",
      "Iteration 39243, loss = 2.39981502\n",
      "Iteration 39244, loss = 3.19217496\n",
      "Iteration 39245, loss = 3.54265318\n",
      "Iteration 39246, loss = 4.27152262\n",
      "Iteration 39247, loss = 4.34510941\n",
      "Iteration 39248, loss = 3.18564670\n",
      "Iteration 39249, loss = 2.92633535\n",
      "Iteration 39250, loss = 2.57604325\n",
      "Iteration 39251, loss = 2.70205764\n",
      "Iteration 39252, loss = 2.21660374\n",
      "Iteration 39253, loss = 2.10057385\n",
      "Iteration 39254, loss = 2.15594865\n",
      "Iteration 39255, loss = 2.01536005\n",
      "Iteration 39256, loss = 2.02251420\n",
      "Iteration 39257, loss = 2.08288279\n",
      "Iteration 39258, loss = 1.90206689\n",
      "Iteration 39259, loss = 2.08270202\n",
      "Iteration 39260, loss = 2.19579471\n",
      "Iteration 39261, loss = 2.14161493\n",
      "Iteration 39262, loss = 2.03135085\n",
      "Iteration 39263, loss = 2.19181285\n",
      "Iteration 39264, loss = 2.14759109\n",
      "Iteration 39265, loss = 2.38874196\n",
      "Iteration 39266, loss = 2.66269044\n",
      "Iteration 39267, loss = 2.43443198\n",
      "Iteration 39268, loss = 2.18849124\n",
      "Iteration 39269, loss = 2.12494821\n",
      "Iteration 39270, loss = 2.03167356\n",
      "Iteration 39271, loss = 2.11052817\n",
      "Iteration 39272, loss = 2.20373002\n",
      "Iteration 39273, loss = 2.08924223\n",
      "Iteration 39274, loss = 2.07514377\n",
      "Iteration 39275, loss = 2.46879071\n",
      "Iteration 39276, loss = 2.50036015\n",
      "Iteration 39277, loss = 2.01422275\n",
      "Iteration 39278, loss = 1.86714154\n",
      "Iteration 39279, loss = 1.93647861\n",
      "Iteration 39280, loss = 1.94929792\n",
      "Iteration 39281, loss = 2.06415132\n",
      "Iteration 39282, loss = 2.20369910\n",
      "Iteration 39283, loss = 2.46881385\n",
      "Iteration 39284, loss = 2.51065651\n",
      "Iteration 39285, loss = 2.41608483\n",
      "Iteration 39286, loss = 2.53548735\n",
      "Iteration 39287, loss = 2.09293445\n",
      "Iteration 39288, loss = 2.22199124\n",
      "Iteration 39289, loss = 2.22098851\n",
      "Iteration 39290, loss = 2.37281081\n",
      "Iteration 39291, loss = 2.43844942\n",
      "Iteration 39292, loss = 2.30036747\n",
      "Iteration 39293, loss = 2.46084171\n",
      "Iteration 39294, loss = 2.43053089\n",
      "Iteration 39295, loss = 2.38352282\n",
      "Iteration 39296, loss = 2.56491676\n",
      "Iteration 39297, loss = 2.74318878\n",
      "Iteration 39298, loss = 2.39411842\n",
      "Iteration 39299, loss = 2.26457015\n",
      "Iteration 39300, loss = 2.44840364\n",
      "Iteration 39301, loss = 2.10628133\n",
      "Iteration 39302, loss = 2.19139040\n",
      "Iteration 39303, loss = 1.93955065\n",
      "Iteration 39304, loss = 1.98932326\n",
      "Iteration 39305, loss = 2.10994862\n",
      "Iteration 39306, loss = 2.45254032\n",
      "Iteration 39307, loss = 2.34925481\n",
      "Iteration 39308, loss = 2.77335841\n",
      "Iteration 39309, loss = 2.51688825\n",
      "Iteration 39310, loss = 3.35269848\n",
      "Iteration 39311, loss = 2.56738726\n",
      "Iteration 39312, loss = 2.39334293\n",
      "Iteration 39313, loss = 2.46500061\n",
      "Iteration 39314, loss = 2.21807704\n",
      "Iteration 39315, loss = 2.40373686\n",
      "Iteration 39316, loss = 2.38983400\n",
      "Iteration 39317, loss = 2.47177940\n",
      "Iteration 39318, loss = 2.91177328\n",
      "Iteration 39319, loss = 2.72464024\n",
      "Iteration 39320, loss = 2.44328802\n",
      "Iteration 39321, loss = 2.12518161\n",
      "Iteration 39322, loss = 1.94006325\n",
      "Iteration 39323, loss = 2.17634875\n",
      "Iteration 39324, loss = 2.27433180\n",
      "Iteration 39325, loss = 2.06773188\n",
      "Iteration 39326, loss = 1.95141088\n",
      "Iteration 39327, loss = 2.11750409\n",
      "Iteration 39328, loss = 2.07902204\n",
      "Iteration 39329, loss = 2.09034686\n",
      "Iteration 39330, loss = 2.16209019\n",
      "Iteration 39331, loss = 2.23368175\n",
      "Iteration 39332, loss = 2.31026854\n",
      "Iteration 39333, loss = 2.02540000\n",
      "Iteration 39334, loss = 2.04675065\n",
      "Iteration 39335, loss = 2.08881036\n",
      "Iteration 39336, loss = 2.21989316\n",
      "Iteration 39337, loss = 1.92861210\n",
      "Iteration 39338, loss = 2.01750473\n",
      "Iteration 39339, loss = 2.29085081\n",
      "Iteration 39340, loss = 2.60167030\n",
      "Iteration 39341, loss = 2.39468519\n",
      "Iteration 39342, loss = 2.79721065\n",
      "Iteration 39343, loss = 2.87253941\n",
      "Iteration 39344, loss = 2.58463695\n",
      "Iteration 39345, loss = 2.77323889\n",
      "Iteration 39346, loss = 2.47711980\n",
      "Iteration 39347, loss = 2.19220984\n",
      "Iteration 39348, loss = 2.14817978\n",
      "Iteration 39349, loss = 2.52211951\n",
      "Iteration 39350, loss = 2.39677062\n",
      "Iteration 39351, loss = 3.11774477\n",
      "Iteration 39352, loss = 2.82300544\n",
      "Iteration 39353, loss = 2.45946543\n",
      "Iteration 39354, loss = 2.30828284\n",
      "Iteration 39355, loss = 2.17842665\n",
      "Iteration 39356, loss = 2.26294701\n",
      "Iteration 39357, loss = 2.91174056\n",
      "Iteration 39358, loss = 2.87466754\n",
      "Iteration 39359, loss = 2.46280083\n",
      "Iteration 39360, loss = 2.21503269\n",
      "Iteration 39361, loss = 2.64383835\n",
      "Iteration 39362, loss = 2.46076449\n",
      "Iteration 39363, loss = 2.37797344\n",
      "Iteration 39364, loss = 1.99842433\n",
      "Iteration 39365, loss = 2.03430103\n",
      "Iteration 39366, loss = 1.98355755\n",
      "Iteration 39367, loss = 1.93252209\n",
      "Iteration 39368, loss = 1.94541306\n",
      "Iteration 39369, loss = 1.95533206\n",
      "Iteration 39370, loss = 1.97296695\n",
      "Iteration 39371, loss = 2.05486556\n",
      "Iteration 39372, loss = 2.53539263\n",
      "Iteration 39373, loss = 2.92101433\n",
      "Iteration 39374, loss = 2.57754502\n",
      "Iteration 39375, loss = 3.53012932\n",
      "Iteration 39376, loss = 2.57148765\n",
      "Iteration 39377, loss = 3.02013807\n",
      "Iteration 39378, loss = 2.78242034\n",
      "Iteration 39379, loss = 2.54912419\n",
      "Iteration 39380, loss = 3.04601841\n",
      "Iteration 39381, loss = 2.77740253\n",
      "Iteration 39382, loss = 2.52959990\n",
      "Iteration 39383, loss = 2.61122470\n",
      "Iteration 39384, loss = 2.05128979\n",
      "Iteration 39385, loss = 2.43400327\n",
      "Iteration 39386, loss = 2.74700863\n",
      "Iteration 39387, loss = 3.39229134\n",
      "Iteration 39388, loss = 3.47301397\n",
      "Iteration 39389, loss = 3.21802821\n",
      "Iteration 39390, loss = 2.80660536\n",
      "Iteration 39391, loss = 2.41467174\n",
      "Iteration 39392, loss = 2.64737386\n",
      "Iteration 39393, loss = 2.88190299\n",
      "Iteration 39394, loss = 5.31200244\n",
      "Iteration 39395, loss = 6.90526224\n",
      "Iteration 39396, loss = 6.10368620\n",
      "Iteration 39397, loss = 7.49042762\n",
      "Iteration 39398, loss = 4.53505027\n",
      "Iteration 39399, loss = 3.07088255\n",
      "Iteration 39400, loss = 3.24264311\n",
      "Iteration 39401, loss = 2.30317529\n",
      "Iteration 39402, loss = 2.87248692\n",
      "Iteration 39403, loss = 2.92785344\n",
      "Iteration 39404, loss = 2.84332308\n",
      "Iteration 39405, loss = 2.60679823\n",
      "Iteration 39406, loss = 2.34230037\n",
      "Iteration 39407, loss = 2.31724230\n",
      "Iteration 39408, loss = 2.15696614\n",
      "Iteration 39409, loss = 2.42965792\n",
      "Iteration 39410, loss = 2.66641023\n",
      "Iteration 39411, loss = 3.20572448\n",
      "Iteration 39412, loss = 2.51003089\n",
      "Iteration 39413, loss = 2.77879827\n",
      "Iteration 39414, loss = 3.13725007\n",
      "Iteration 39415, loss = 2.80093980\n",
      "Iteration 39416, loss = 2.55910843\n",
      "Iteration 39417, loss = 2.70218906\n",
      "Iteration 39418, loss = 3.18229802\n",
      "Iteration 39419, loss = 3.20523565\n",
      "Iteration 39420, loss = 2.90178131\n",
      "Iteration 39421, loss = 2.50483495\n",
      "Iteration 39422, loss = 2.23472734\n",
      "Iteration 39423, loss = 2.29359566\n",
      "Iteration 39424, loss = 2.13546877\n",
      "Iteration 39425, loss = 2.80387295\n",
      "Iteration 39426, loss = 2.45462324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 39427, loss = 2.21223544\n",
      "Iteration 39428, loss = 2.15716126\n",
      "Iteration 39429, loss = 2.30144531\n",
      "Iteration 39430, loss = 1.96603255\n",
      "Iteration 39431, loss = 2.03481524\n",
      "Iteration 39432, loss = 2.44051951\n",
      "Iteration 39433, loss = 2.24094008\n",
      "Iteration 39434, loss = 2.28859965\n",
      "Iteration 39435, loss = 2.08914757\n",
      "Iteration 39436, loss = 2.32505689\n",
      "Iteration 39437, loss = 2.08020071\n",
      "Iteration 39438, loss = 2.25155340\n",
      "Iteration 39439, loss = 2.76480088\n",
      "Iteration 39440, loss = 2.58002825\n",
      "Iteration 39441, loss = 2.18169234\n",
      "Iteration 39442, loss = 2.63145454\n",
      "Iteration 39443, loss = 3.02863478\n",
      "Iteration 39444, loss = 2.31147181\n",
      "Iteration 39445, loss = 2.34764456\n",
      "Iteration 39446, loss = 1.99610855\n",
      "Iteration 39447, loss = 1.96976671\n",
      "Iteration 39448, loss = 2.11143258\n",
      "Iteration 39449, loss = 2.05885171\n",
      "Iteration 39450, loss = 2.13650472\n",
      "Iteration 39451, loss = 3.08195268\n",
      "Iteration 39452, loss = 3.46645444\n",
      "Iteration 39453, loss = 3.39456187\n",
      "Iteration 39454, loss = 2.84270253\n",
      "Iteration 39455, loss = 2.55318243\n",
      "Iteration 39456, loss = 2.30419197\n",
      "Iteration 39457, loss = 2.25063795\n",
      "Iteration 39458, loss = 2.68568651\n",
      "Iteration 39459, loss = 2.59133984\n",
      "Iteration 39460, loss = 2.57286517\n",
      "Iteration 39461, loss = 2.40241228\n",
      "Iteration 39462, loss = 2.54768371\n",
      "Iteration 39463, loss = 3.23224102\n",
      "Iteration 39464, loss = 2.22375746\n",
      "Iteration 39465, loss = 2.21036754\n",
      "Iteration 39466, loss = 2.01138284\n",
      "Iteration 39467, loss = 2.11648077\n",
      "Iteration 39468, loss = 2.19789950\n",
      "Iteration 39469, loss = 2.02683392\n",
      "Iteration 39470, loss = 2.22118397\n",
      "Iteration 39471, loss = 2.71958055\n",
      "Iteration 39472, loss = 2.36900958\n",
      "Iteration 39473, loss = 2.47009492\n",
      "Iteration 39474, loss = 2.30006027\n",
      "Iteration 39475, loss = 2.29496462\n",
      "Iteration 39476, loss = 2.47634512\n",
      "Iteration 39477, loss = 2.56639676\n",
      "Iteration 39478, loss = 2.72994769\n",
      "Iteration 39479, loss = 2.81384702\n",
      "Iteration 39480, loss = 2.70910743\n",
      "Iteration 39481, loss = 2.65807705\n",
      "Iteration 39482, loss = 2.21568617\n",
      "Iteration 39483, loss = 2.30212686\n",
      "Iteration 39484, loss = 2.17178661\n",
      "Iteration 39485, loss = 2.09937393\n",
      "Iteration 39486, loss = 2.15650759\n",
      "Iteration 39487, loss = 2.04661263\n",
      "Iteration 39488, loss = 2.02392668\n",
      "Iteration 39489, loss = 1.93127234\n",
      "Iteration 39490, loss = 2.19978429\n",
      "Iteration 39491, loss = 2.38173554\n",
      "Iteration 39492, loss = 2.21650199\n",
      "Iteration 39493, loss = 2.52873478\n",
      "Iteration 39494, loss = 2.92970079\n",
      "Iteration 39495, loss = 3.21686680\n",
      "Iteration 39496, loss = 3.13622455\n",
      "Iteration 39497, loss = 3.61045970\n",
      "Iteration 39498, loss = 3.50981901\n",
      "Iteration 39499, loss = 3.24975262\n",
      "Iteration 39500, loss = 3.08286970\n",
      "Iteration 39501, loss = 2.97190890\n",
      "Iteration 39502, loss = 2.73308828\n",
      "Iteration 39503, loss = 2.58201010\n",
      "Iteration 39504, loss = 2.32363162\n",
      "Iteration 39505, loss = 2.27700664\n",
      "Iteration 39506, loss = 2.55625078\n",
      "Iteration 39507, loss = 3.05266832\n",
      "Iteration 39508, loss = 3.17374161\n",
      "Iteration 39509, loss = 3.54941818\n",
      "Iteration 39510, loss = 3.49198418\n",
      "Iteration 39511, loss = 3.14175110\n",
      "Iteration 39512, loss = 2.37752800\n",
      "Iteration 39513, loss = 2.39123985\n",
      "Iteration 39514, loss = 2.44082752\n",
      "Iteration 39515, loss = 2.62214091\n",
      "Iteration 39516, loss = 3.66633020\n",
      "Iteration 39517, loss = 3.25091412\n",
      "Iteration 39518, loss = 2.70268869\n",
      "Iteration 39519, loss = 2.41645039\n",
      "Iteration 39520, loss = 2.19054493\n",
      "Iteration 39521, loss = 2.32005397\n",
      "Iteration 39522, loss = 2.22479658\n",
      "Iteration 39523, loss = 2.60449882\n",
      "Iteration 39524, loss = 3.06051561\n",
      "Iteration 39525, loss = 6.08274590\n",
      "Iteration 39526, loss = 4.30089599\n",
      "Iteration 39527, loss = 3.84795716\n",
      "Iteration 39528, loss = 3.25947358\n",
      "Iteration 39529, loss = 2.73850071\n",
      "Iteration 39530, loss = 2.56590891\n",
      "Iteration 39531, loss = 2.93025671\n",
      "Iteration 39532, loss = 2.35992519\n",
      "Iteration 39533, loss = 2.37948861\n",
      "Iteration 39534, loss = 2.57181811\n",
      "Iteration 39535, loss = 2.89186349\n",
      "Iteration 39536, loss = 2.20034434\n",
      "Iteration 39537, loss = 2.52112284\n",
      "Iteration 39538, loss = 2.67148922\n",
      "Iteration 39539, loss = 4.26302923\n",
      "Iteration 39540, loss = 3.06101418\n",
      "Iteration 39541, loss = 2.62063564\n",
      "Iteration 39542, loss = 3.73821262\n",
      "Iteration 39543, loss = 3.21212114\n",
      "Iteration 39544, loss = 3.36064116\n",
      "Iteration 39545, loss = 2.97393964\n",
      "Iteration 39546, loss = 2.56928114\n",
      "Iteration 39547, loss = 2.26419502\n",
      "Iteration 39548, loss = 2.53712003\n",
      "Iteration 39549, loss = 2.43483546\n",
      "Iteration 39550, loss = 2.21796748\n",
      "Iteration 39551, loss = 2.23771911\n",
      "Iteration 39552, loss = 2.07688850\n",
      "Iteration 39553, loss = 2.35335113\n",
      "Iteration 39554, loss = 2.39517618\n",
      "Iteration 39555, loss = 3.00959122\n",
      "Iteration 39556, loss = 2.56932541\n",
      "Iteration 39557, loss = 2.20434801\n",
      "Iteration 39558, loss = 2.15793724\n",
      "Iteration 39559, loss = 1.99154727\n",
      "Iteration 39560, loss = 1.99212017\n",
      "Iteration 39561, loss = 2.15773327\n",
      "Iteration 39562, loss = 2.18001008\n",
      "Iteration 39563, loss = 2.51626458\n",
      "Iteration 39564, loss = 2.16575433\n",
      "Iteration 39565, loss = 2.36010430\n",
      "Iteration 39566, loss = 2.24137591\n",
      "Iteration 39567, loss = 2.32381501\n",
      "Iteration 39568, loss = 2.20939955\n",
      "Iteration 39569, loss = 2.37542265\n",
      "Iteration 39570, loss = 2.17853053\n",
      "Iteration 39571, loss = 1.98995035\n",
      "Iteration 39572, loss = 2.11064450\n",
      "Iteration 39573, loss = 2.36353976\n",
      "Iteration 39574, loss = 1.93933818\n",
      "Iteration 39575, loss = 2.28807318\n",
      "Iteration 39576, loss = 2.01005092\n",
      "Iteration 39577, loss = 2.41972455\n",
      "Iteration 39578, loss = 2.33408113\n",
      "Iteration 39579, loss = 2.15949818\n",
      "Iteration 39580, loss = 2.07486111\n",
      "Iteration 39581, loss = 2.00196393\n",
      "Iteration 39582, loss = 2.29768693\n",
      "Iteration 39583, loss = 2.08764801\n",
      "Iteration 39584, loss = 1.95994949\n",
      "Iteration 39585, loss = 2.20747053\n",
      "Iteration 39586, loss = 2.80474090\n",
      "Iteration 39587, loss = 2.19331658\n",
      "Iteration 39588, loss = 2.16933832\n",
      "Iteration 39589, loss = 1.99919577\n",
      "Iteration 39590, loss = 1.98336953\n",
      "Iteration 39591, loss = 1.96988201\n",
      "Iteration 39592, loss = 2.08021137\n",
      "Iteration 39593, loss = 2.04900455\n",
      "Iteration 39594, loss = 2.23142832\n",
      "Iteration 39595, loss = 2.09953251\n",
      "Iteration 39596, loss = 2.01241531\n",
      "Iteration 39597, loss = 3.39254794\n",
      "Iteration 39598, loss = 3.31583951\n",
      "Iteration 39599, loss = 2.87698883\n",
      "Iteration 39600, loss = 2.70426223\n",
      "Iteration 39601, loss = 2.25602949\n",
      "Iteration 39602, loss = 2.55101086\n",
      "Iteration 39603, loss = 3.21056045\n",
      "Iteration 39604, loss = 2.51098286\n",
      "Iteration 39605, loss = 2.64489179\n",
      "Iteration 39606, loss = 2.80450787\n",
      "Iteration 39607, loss = 2.54854982\n",
      "Iteration 39608, loss = 2.28851657\n",
      "Iteration 39609, loss = 3.30548885\n",
      "Iteration 39610, loss = 2.46158224\n",
      "Iteration 39611, loss = 2.14022504\n",
      "Iteration 39612, loss = 2.00899507\n",
      "Iteration 39613, loss = 1.89993815\n",
      "Iteration 39614, loss = 2.22147270\n",
      "Iteration 39615, loss = 2.35621988\n",
      "Iteration 39616, loss = 2.27851827\n",
      "Iteration 39617, loss = 2.20839511\n",
      "Iteration 39618, loss = 2.00882840\n",
      "Iteration 39619, loss = 1.92342990\n",
      "Iteration 39620, loss = 2.19756619\n",
      "Iteration 39621, loss = 2.32242830\n",
      "Iteration 39622, loss = 2.94379049\n",
      "Iteration 39623, loss = 2.43158718\n",
      "Iteration 39624, loss = 2.77030239\n",
      "Iteration 39625, loss = 2.49239321\n",
      "Iteration 39626, loss = 2.85879643\n",
      "Iteration 39627, loss = 2.86119223\n",
      "Iteration 39628, loss = 2.03415052\n",
      "Iteration 39629, loss = 2.03454110\n",
      "Iteration 39630, loss = 2.55750358\n",
      "Iteration 39631, loss = 2.95530105\n",
      "Iteration 39632, loss = 2.26881450\n",
      "Iteration 39633, loss = 2.52900031\n",
      "Iteration 39634, loss = 2.70919460\n",
      "Iteration 39635, loss = 1.98791270\n",
      "Iteration 39636, loss = 1.98854250\n",
      "Iteration 39637, loss = 2.11469050\n",
      "Iteration 39638, loss = 2.14964130\n",
      "Iteration 39639, loss = 2.02011826\n",
      "Iteration 39640, loss = 2.30609494\n",
      "Iteration 39641, loss = 2.23354920\n",
      "Iteration 39642, loss = 2.76391366\n",
      "Iteration 39643, loss = 2.59970369\n",
      "Iteration 39644, loss = 2.43973766\n",
      "Iteration 39645, loss = 2.12522736\n",
      "Iteration 39646, loss = 2.06424448\n",
      "Iteration 39647, loss = 2.02689402\n",
      "Iteration 39648, loss = 2.24217210\n",
      "Iteration 39649, loss = 2.39925740\n",
      "Iteration 39650, loss = 2.19793589\n",
      "Iteration 39651, loss = 2.76132136\n",
      "Iteration 39652, loss = 2.80903715\n",
      "Iteration 39653, loss = 2.80458145\n",
      "Iteration 39654, loss = 2.54050186\n",
      "Iteration 39655, loss = 3.31276197\n",
      "Iteration 39656, loss = 3.22642873\n",
      "Iteration 39657, loss = 2.31805138\n",
      "Iteration 39658, loss = 2.50970651\n",
      "Iteration 39659, loss = 2.25621607\n",
      "Iteration 39660, loss = 2.31332316\n",
      "Iteration 39661, loss = 2.81870184\n",
      "Iteration 39662, loss = 3.52442051\n",
      "Iteration 39663, loss = 2.58685497\n",
      "Iteration 39664, loss = 2.93966861\n",
      "Iteration 39665, loss = 2.75483627\n",
      "Iteration 39666, loss = 2.60655529\n",
      "Iteration 39667, loss = 2.54518830\n",
      "Iteration 39668, loss = 2.31490638\n",
      "Iteration 39669, loss = 2.67324995\n",
      "Iteration 39670, loss = 3.14265544\n",
      "Iteration 39671, loss = 3.31091511\n",
      "Iteration 39672, loss = 2.43168153\n",
      "Iteration 39673, loss = 2.57496836\n",
      "Iteration 39674, loss = 3.12099203\n",
      "Iteration 39675, loss = 2.67361610\n",
      "Iteration 39676, loss = 3.28356843\n",
      "Iteration 39677, loss = 2.45197077\n",
      "Iteration 39678, loss = 2.14810842\n",
      "Iteration 39679, loss = 2.02743275\n",
      "Iteration 39680, loss = 2.49941888\n",
      "Iteration 39681, loss = 2.45300356\n",
      "Iteration 39682, loss = 2.40839146\n",
      "Iteration 39683, loss = 2.53036148\n",
      "Iteration 39684, loss = 2.34807147\n",
      "Iteration 39685, loss = 2.54199782\n",
      "Iteration 39686, loss = 2.29645044\n",
      "Iteration 39687, loss = 2.62215004\n",
      "Iteration 39688, loss = 2.54212552\n",
      "Iteration 39689, loss = 2.69996511\n",
      "Iteration 39690, loss = 2.14215990\n",
      "Iteration 39691, loss = 2.00358874\n",
      "Iteration 39692, loss = 2.22992841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 39693, loss = 2.27731658\n",
      "Iteration 39694, loss = 2.46802996\n",
      "Iteration 39695, loss = 2.36246469\n",
      "Iteration 39696, loss = 2.91813735\n",
      "Iteration 39697, loss = 3.59683697\n",
      "Iteration 39698, loss = 3.38056577\n",
      "Iteration 39699, loss = 3.33294056\n",
      "Iteration 39700, loss = 4.78170497\n",
      "Iteration 39701, loss = 6.32288699\n",
      "Iteration 39702, loss = 4.06774190\n",
      "Iteration 39703, loss = 3.04801814\n",
      "Iteration 39704, loss = 2.27807676\n",
      "Iteration 39705, loss = 2.31172566\n",
      "Iteration 39706, loss = 2.36516675\n",
      "Iteration 39707, loss = 2.06729119\n",
      "Iteration 39708, loss = 2.07394392\n",
      "Iteration 39709, loss = 1.95495775\n",
      "Iteration 39710, loss = 2.05656446\n",
      "Iteration 39711, loss = 2.61593658\n",
      "Iteration 39712, loss = 2.18478753\n",
      "Iteration 39713, loss = 2.34599604\n",
      "Iteration 39714, loss = 2.40497123\n",
      "Iteration 39715, loss = 2.64778172\n",
      "Iteration 39716, loss = 2.46633914\n",
      "Iteration 39717, loss = 2.49027217\n",
      "Iteration 39718, loss = 2.88491852\n",
      "Iteration 39719, loss = 2.34778608\n",
      "Iteration 39720, loss = 2.73921692\n",
      "Iteration 39721, loss = 2.74238158\n",
      "Iteration 39722, loss = 2.82019111\n",
      "Iteration 39723, loss = 2.97471409\n",
      "Iteration 39724, loss = 2.39784710\n",
      "Iteration 39725, loss = 2.10144915\n",
      "Iteration 39726, loss = 2.25790098\n",
      "Iteration 39727, loss = 2.28586629\n",
      "Iteration 39728, loss = 2.11373948\n",
      "Iteration 39729, loss = 2.26188257\n",
      "Iteration 39730, loss = 2.59744120\n",
      "Iteration 39731, loss = 2.42330901\n",
      "Iteration 39732, loss = 2.52470647\n",
      "Iteration 39733, loss = 2.55839551\n",
      "Iteration 39734, loss = 3.75418617\n",
      "Iteration 39735, loss = 3.67773299\n",
      "Iteration 39736, loss = 3.68018634\n",
      "Iteration 39737, loss = 3.14926401\n",
      "Iteration 39738, loss = 2.21310531\n",
      "Iteration 39739, loss = 3.18204115\n",
      "Iteration 39740, loss = 3.60505052\n",
      "Iteration 39741, loss = 2.67645856\n",
      "Iteration 39742, loss = 2.69973221\n",
      "Iteration 39743, loss = 2.13225747\n",
      "Iteration 39744, loss = 2.09849162\n",
      "Iteration 39745, loss = 1.95258589\n",
      "Iteration 39746, loss = 2.05803183\n",
      "Iteration 39747, loss = 2.12730057\n",
      "Iteration 39748, loss = 2.08112631\n",
      "Iteration 39749, loss = 2.02153009\n",
      "Iteration 39750, loss = 1.97121714\n",
      "Iteration 39751, loss = 2.00282616\n",
      "Iteration 39752, loss = 2.24639689\n",
      "Iteration 39753, loss = 2.19067423\n",
      "Iteration 39754, loss = 2.49974902\n",
      "Iteration 39755, loss = 2.05574079\n",
      "Iteration 39756, loss = 2.26660873\n",
      "Iteration 39757, loss = 2.22710677\n",
      "Iteration 39758, loss = 1.92681183\n",
      "Iteration 39759, loss = 1.97721305\n",
      "Iteration 39760, loss = 2.29680996\n",
      "Iteration 39761, loss = 2.17002812\n",
      "Iteration 39762, loss = 2.20930917\n",
      "Iteration 39763, loss = 2.57190296\n",
      "Iteration 39764, loss = 2.23178165\n",
      "Iteration 39765, loss = 1.99716283\n",
      "Iteration 39766, loss = 2.03118080\n",
      "Iteration 39767, loss = 1.99138617\n",
      "Iteration 39768, loss = 2.26721437\n",
      "Iteration 39769, loss = 2.58831102\n",
      "Iteration 39770, loss = 2.30491321\n",
      "Iteration 39771, loss = 2.51845995\n",
      "Iteration 39772, loss = 2.18487230\n",
      "Iteration 39773, loss = 2.25732601\n",
      "Iteration 39774, loss = 1.89330682\n",
      "Iteration 39775, loss = 2.01129627\n",
      "Iteration 39776, loss = 1.95161743\n",
      "Iteration 39777, loss = 1.88198567\n",
      "Iteration 39778, loss = 2.18031209\n",
      "Iteration 39779, loss = 2.04878298\n",
      "Iteration 39780, loss = 2.22578707\n",
      "Iteration 39781, loss = 2.35286164\n",
      "Iteration 39782, loss = 2.54018775\n",
      "Iteration 39783, loss = 2.10974966\n",
      "Iteration 39784, loss = 1.99574461\n",
      "Iteration 39785, loss = 2.28447629\n",
      "Iteration 39786, loss = 3.11758666\n",
      "Iteration 39787, loss = 2.39179509\n",
      "Iteration 39788, loss = 2.69930880\n",
      "Iteration 39789, loss = 2.58015553\n",
      "Iteration 39790, loss = 2.81927356\n",
      "Iteration 39791, loss = 2.36173801\n",
      "Iteration 39792, loss = 2.23451932\n",
      "Iteration 39793, loss = 2.58468744\n",
      "Iteration 39794, loss = 2.24119429\n",
      "Iteration 39795, loss = 2.32587366\n",
      "Iteration 39796, loss = 2.06250624\n",
      "Iteration 39797, loss = 2.18361035\n",
      "Iteration 39798, loss = 2.53067692\n",
      "Iteration 39799, loss = 2.25923031\n",
      "Iteration 39800, loss = 2.24751373\n",
      "Iteration 39801, loss = 2.06470171\n",
      "Iteration 39802, loss = 2.15104208\n",
      "Iteration 39803, loss = 2.13748133\n",
      "Iteration 39804, loss = 2.04924746\n",
      "Iteration 39805, loss = 2.37795761\n",
      "Iteration 39806, loss = 2.43828936\n",
      "Iteration 39807, loss = 2.04197566\n",
      "Iteration 39808, loss = 1.84810403\n",
      "Iteration 39809, loss = 1.89235244\n",
      "Iteration 39810, loss = 2.05228794\n",
      "Iteration 39811, loss = 1.99177065\n",
      "Iteration 39812, loss = 1.98740940\n",
      "Iteration 39813, loss = 2.41838759\n",
      "Iteration 39814, loss = 2.40621720\n",
      "Iteration 39815, loss = 1.98975687\n",
      "Iteration 39816, loss = 1.96216376\n",
      "Iteration 39817, loss = 2.11991224\n",
      "Iteration 39818, loss = 2.15394915\n",
      "Iteration 39819, loss = 2.50349515\n",
      "Iteration 39820, loss = 2.57865927\n",
      "Iteration 39821, loss = 2.12021494\n",
      "Iteration 39822, loss = 2.54429432\n",
      "Iteration 39823, loss = 2.12404057\n",
      "Iteration 39824, loss = 2.34893952\n",
      "Iteration 39825, loss = 2.16381285\n",
      "Iteration 39826, loss = 2.60293573\n",
      "Iteration 39827, loss = 3.72313663\n",
      "Iteration 39828, loss = 3.65564624\n",
      "Iteration 39829, loss = 3.53074229\n",
      "Iteration 39830, loss = 3.32077252\n",
      "Iteration 39831, loss = 2.48852187\n",
      "Iteration 39832, loss = 2.34688791\n",
      "Iteration 39833, loss = 2.61622787\n",
      "Iteration 39834, loss = 2.65235345\n",
      "Iteration 39835, loss = 2.87862703\n",
      "Iteration 39836, loss = 2.48513871\n",
      "Iteration 39837, loss = 2.54453449\n",
      "Iteration 39838, loss = 2.15621784\n",
      "Iteration 39839, loss = 3.06942658\n",
      "Iteration 39840, loss = 2.54602055\n",
      "Iteration 39841, loss = 3.15636741\n",
      "Iteration 39842, loss = 3.47784883\n",
      "Iteration 39843, loss = 3.23949822\n",
      "Iteration 39844, loss = 2.65638159\n",
      "Iteration 39845, loss = 2.16302871\n",
      "Iteration 39846, loss = 2.14921883\n",
      "Iteration 39847, loss = 2.06264749\n",
      "Iteration 39848, loss = 2.01588266\n",
      "Iteration 39849, loss = 2.14722971\n",
      "Iteration 39850, loss = 2.43179060\n",
      "Iteration 39851, loss = 1.88942866\n",
      "Iteration 39852, loss = 2.00315912\n",
      "Iteration 39853, loss = 1.95479322\n",
      "Iteration 39854, loss = 1.92643911\n",
      "Iteration 39855, loss = 1.91930492\n",
      "Iteration 39856, loss = 2.10334974\n",
      "Iteration 39857, loss = 2.89678856\n",
      "Iteration 39858, loss = 2.39759921\n",
      "Iteration 39859, loss = 2.64431654\n",
      "Iteration 39860, loss = 2.79439983\n",
      "Iteration 39861, loss = 3.25875064\n",
      "Iteration 39862, loss = 3.17824539\n",
      "Iteration 39863, loss = 3.41072769\n",
      "Iteration 39864, loss = 3.42317208\n",
      "Iteration 39865, loss = 3.41439001\n",
      "Iteration 39866, loss = 3.32093175\n",
      "Iteration 39867, loss = 3.51976371\n",
      "Iteration 39868, loss = 2.87210412\n",
      "Iteration 39869, loss = 2.94666065\n",
      "Iteration 39870, loss = 2.60695599\n",
      "Iteration 39871, loss = 1.90371265\n",
      "Iteration 39872, loss = 2.35622987\n",
      "Iteration 39873, loss = 2.18725803\n",
      "Iteration 39874, loss = 2.23370149\n",
      "Iteration 39875, loss = 2.63011298\n",
      "Iteration 39876, loss = 3.07530310\n",
      "Iteration 39877, loss = 2.51652034\n",
      "Iteration 39878, loss = 2.15065424\n",
      "Iteration 39879, loss = 2.06638592\n",
      "Iteration 39880, loss = 2.14806419\n",
      "Iteration 39881, loss = 2.16096231\n",
      "Iteration 39882, loss = 2.13394942\n",
      "Iteration 39883, loss = 2.04635487\n",
      "Iteration 39884, loss = 2.01344775\n",
      "Iteration 39885, loss = 1.93447979\n",
      "Iteration 39886, loss = 1.97638849\n",
      "Iteration 39887, loss = 2.33290921\n",
      "Iteration 39888, loss = 2.46429655\n",
      "Iteration 39889, loss = 2.48791049\n",
      "Iteration 39890, loss = 2.86997009\n",
      "Iteration 39891, loss = 2.81563843\n",
      "Iteration 39892, loss = 2.76383074\n",
      "Iteration 39893, loss = 2.18929938\n",
      "Iteration 39894, loss = 2.07696327\n",
      "Iteration 39895, loss = 2.51804687\n",
      "Iteration 39896, loss = 2.43225569\n",
      "Iteration 39897, loss = 2.08075389\n",
      "Iteration 39898, loss = 2.32980657\n",
      "Iteration 39899, loss = 2.79395525\n",
      "Iteration 39900, loss = 3.24391416\n",
      "Iteration 39901, loss = 3.15200809\n",
      "Iteration 39902, loss = 2.96575846\n",
      "Iteration 39903, loss = 2.65038980\n",
      "Iteration 39904, loss = 2.38254915\n",
      "Iteration 39905, loss = 2.19693066\n",
      "Iteration 39906, loss = 2.04552715\n",
      "Iteration 39907, loss = 2.40298070\n",
      "Iteration 39908, loss = 1.97090538\n",
      "Iteration 39909, loss = 2.15560597\n",
      "Iteration 39910, loss = 2.07419517\n",
      "Iteration 39911, loss = 1.95465285\n",
      "Iteration 39912, loss = 1.87549746\n",
      "Iteration 39913, loss = 1.91717846\n",
      "Iteration 39914, loss = 2.12317317\n",
      "Iteration 39915, loss = 2.03760421\n",
      "Iteration 39916, loss = 1.82980188\n",
      "Iteration 39917, loss = 1.88623114\n",
      "Iteration 39918, loss = 1.96062757\n",
      "Iteration 39919, loss = 1.82187914\n",
      "Iteration 39920, loss = 2.23243029\n",
      "Iteration 39921, loss = 2.35880946\n",
      "Iteration 39922, loss = 2.05255383\n",
      "Iteration 39923, loss = 1.88638332\n",
      "Iteration 39924, loss = 2.29221849\n",
      "Iteration 39925, loss = 2.31923116\n",
      "Iteration 39926, loss = 2.24145259\n",
      "Iteration 39927, loss = 3.05041364\n",
      "Iteration 39928, loss = 2.65964833\n",
      "Iteration 39929, loss = 2.42473459\n",
      "Iteration 39930, loss = 2.08943279\n",
      "Iteration 39931, loss = 2.54548402\n",
      "Iteration 39932, loss = 2.32787992\n",
      "Iteration 39933, loss = 2.61758999\n",
      "Iteration 39934, loss = 2.34380683\n",
      "Iteration 39935, loss = 2.13932310\n",
      "Iteration 39936, loss = 2.18940722\n",
      "Iteration 39937, loss = 2.30386920\n",
      "Iteration 39938, loss = 2.11027846\n",
      "Iteration 39939, loss = 2.06446545\n",
      "Iteration 39940, loss = 2.25189456\n",
      "Iteration 39941, loss = 2.33260480\n",
      "Iteration 39942, loss = 2.34801528\n",
      "Iteration 39943, loss = 2.39465535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 39944, loss = 2.37288506\n",
      "Iteration 39945, loss = 2.56530488\n",
      "Iteration 39946, loss = 2.47241541\n",
      "Iteration 39947, loss = 2.64030872\n",
      "Iteration 39948, loss = 3.43720313\n",
      "Iteration 39949, loss = 4.09020375\n",
      "Iteration 39950, loss = 3.57782919\n",
      "Iteration 39951, loss = 3.34629067\n",
      "Iteration 39952, loss = 3.68289649\n",
      "Iteration 39953, loss = 3.80255457\n",
      "Iteration 39954, loss = 2.81944810\n",
      "Iteration 39955, loss = 3.78352442\n",
      "Iteration 39956, loss = 3.69781962\n",
      "Iteration 39957, loss = 4.32012745\n",
      "Iteration 39958, loss = 3.40414229\n",
      "Iteration 39959, loss = 3.98652203\n",
      "Iteration 39960, loss = 4.28341809\n",
      "Iteration 39961, loss = 3.32557836\n",
      "Iteration 39962, loss = 3.78595218\n",
      "Iteration 39963, loss = 3.53684201\n",
      "Iteration 39964, loss = 4.05414993\n",
      "Iteration 39965, loss = 2.88470212\n",
      "Iteration 39966, loss = 2.29001304\n",
      "Iteration 39967, loss = 2.50219495\n",
      "Iteration 39968, loss = 2.79781751\n",
      "Iteration 39969, loss = 2.38230968\n",
      "Iteration 39970, loss = 2.22785375\n",
      "Iteration 39971, loss = 2.25008696\n",
      "Iteration 39972, loss = 3.02233874\n",
      "Iteration 39973, loss = 3.83129962\n",
      "Iteration 39974, loss = 4.45988417\n",
      "Iteration 39975, loss = 3.06369219\n",
      "Iteration 39976, loss = 3.48640697\n",
      "Iteration 39977, loss = 2.88360029\n",
      "Iteration 39978, loss = 2.82769148\n",
      "Iteration 39979, loss = 2.45957841\n",
      "Iteration 39980, loss = 2.44887320\n",
      "Iteration 39981, loss = 3.29624535\n",
      "Iteration 39982, loss = 2.28057524\n",
      "Iteration 39983, loss = 2.35944439\n",
      "Iteration 39984, loss = 2.51605123\n",
      "Iteration 39985, loss = 2.60681498\n",
      "Iteration 39986, loss = 2.10318017\n",
      "Iteration 39987, loss = 2.46727704\n",
      "Iteration 39988, loss = 2.16350810\n",
      "Iteration 39989, loss = 1.96579684\n",
      "Iteration 39990, loss = 1.85595847\n",
      "Iteration 39991, loss = 1.95918792\n",
      "Iteration 39992, loss = 2.00590602\n",
      "Iteration 39993, loss = 1.99066959\n",
      "Iteration 39994, loss = 1.99541903\n",
      "Iteration 39995, loss = 1.95010024\n",
      "Iteration 39996, loss = 2.14572099\n",
      "Iteration 39997, loss = 2.00513710\n",
      "Iteration 39998, loss = 1.86092295\n",
      "Iteration 39999, loss = 2.02925929\n",
      "Iteration 40000, loss = 1.94590254\n",
      "Iteration 40001, loss = 2.05372574\n",
      "Iteration 40002, loss = 2.00263570\n",
      "Iteration 40003, loss = 2.01934265\n",
      "Iteration 40004, loss = 2.18261806\n",
      "Iteration 40005, loss = 2.46562765\n",
      "Iteration 40006, loss = 2.28023843\n",
      "Iteration 40007, loss = 2.27273780\n",
      "Iteration 40008, loss = 2.45021836\n",
      "Iteration 40009, loss = 2.11073228\n",
      "Iteration 40010, loss = 2.22251598\n",
      "Iteration 40011, loss = 2.19132271\n",
      "Iteration 40012, loss = 1.93955600\n",
      "Iteration 40013, loss = 1.83714533\n",
      "Iteration 40014, loss = 2.25510870\n",
      "Iteration 40015, loss = 1.91073546\n",
      "Iteration 40016, loss = 1.86279382\n",
      "Iteration 40017, loss = 2.17338552\n",
      "Iteration 40018, loss = 1.96908100\n",
      "Iteration 40019, loss = 1.81687076\n",
      "Iteration 40020, loss = 1.98524795\n",
      "Iteration 40021, loss = 2.40887989\n",
      "Iteration 40022, loss = 2.53140119\n",
      "Iteration 40023, loss = 2.40010452\n",
      "Iteration 40024, loss = 2.11859563\n",
      "Iteration 40025, loss = 2.29099996\n",
      "Iteration 40026, loss = 2.89364479\n",
      "Iteration 40027, loss = 2.98271402\n",
      "Iteration 40028, loss = 2.71360199\n",
      "Iteration 40029, loss = 2.82766121\n",
      "Iteration 40030, loss = 3.60527129\n",
      "Iteration 40031, loss = 3.92605650\n",
      "Iteration 40032, loss = 2.70024813\n",
      "Iteration 40033, loss = 3.07485008\n",
      "Iteration 40034, loss = 3.40265081\n",
      "Iteration 40035, loss = 2.57082791\n",
      "Iteration 40036, loss = 3.22818465\n",
      "Iteration 40037, loss = 2.65190842\n",
      "Iteration 40038, loss = 3.10204390\n",
      "Iteration 40039, loss = 3.12785585\n",
      "Iteration 40040, loss = 3.37443795\n",
      "Iteration 40041, loss = 2.76107257\n",
      "Iteration 40042, loss = 2.95589897\n",
      "Iteration 40043, loss = 3.18352645\n",
      "Iteration 40044, loss = 3.05159244\n",
      "Iteration 40045, loss = 3.43571503\n",
      "Iteration 40046, loss = 3.09887499\n",
      "Iteration 40047, loss = 3.45608166\n",
      "Iteration 40048, loss = 2.80865904\n",
      "Iteration 40049, loss = 2.36828154\n",
      "Iteration 40050, loss = 2.17682423\n",
      "Iteration 40051, loss = 1.83748519\n",
      "Iteration 40052, loss = 1.77682255\n",
      "Iteration 40053, loss = 2.12943147\n",
      "Iteration 40054, loss = 2.07231454\n",
      "Iteration 40055, loss = 2.00020697\n",
      "Iteration 40056, loss = 2.11594437\n",
      "Iteration 40057, loss = 1.95414640\n",
      "Iteration 40058, loss = 2.02865388\n",
      "Iteration 40059, loss = 1.97252222\n",
      "Iteration 40060, loss = 2.66655181\n",
      "Iteration 40061, loss = 2.65420651\n",
      "Iteration 40062, loss = 3.37110670\n",
      "Iteration 40063, loss = 3.81365966\n",
      "Iteration 40064, loss = 2.67312100\n",
      "Iteration 40065, loss = 2.76733011\n",
      "Iteration 40066, loss = 2.32838219\n",
      "Iteration 40067, loss = 2.28856714\n",
      "Iteration 40068, loss = 2.22790046\n",
      "Iteration 40069, loss = 2.05161493\n",
      "Iteration 40070, loss = 2.19985882\n",
      "Iteration 40071, loss = 2.03289486\n",
      "Iteration 40072, loss = 1.97279975\n",
      "Iteration 40073, loss = 1.92223603\n",
      "Iteration 40074, loss = 1.98676850\n",
      "Iteration 40075, loss = 2.44689154\n",
      "Iteration 40076, loss = 2.92003236\n",
      "Iteration 40077, loss = 3.06672475\n",
      "Iteration 40078, loss = 2.91682432\n",
      "Iteration 40079, loss = 3.15421936\n",
      "Iteration 40080, loss = 2.30265123\n",
      "Iteration 40081, loss = 2.83391569\n",
      "Iteration 40082, loss = 2.56815887\n",
      "Iteration 40083, loss = 2.31182734\n",
      "Iteration 40084, loss = 1.98898314\n",
      "Iteration 40085, loss = 2.09731322\n",
      "Iteration 40086, loss = 2.04199848\n",
      "Iteration 40087, loss = 1.97942634\n",
      "Iteration 40088, loss = 1.91413456\n",
      "Iteration 40089, loss = 2.26812056\n",
      "Iteration 40090, loss = 2.16297446\n",
      "Iteration 40091, loss = 2.81226486\n",
      "Iteration 40092, loss = 2.01622237\n",
      "Iteration 40093, loss = 2.37652463\n",
      "Iteration 40094, loss = 2.11896895\n",
      "Iteration 40095, loss = 2.11265410\n",
      "Iteration 40096, loss = 2.06300891\n",
      "Iteration 40097, loss = 1.84265363\n",
      "Iteration 40098, loss = 2.28485988\n",
      "Iteration 40099, loss = 2.26453203\n",
      "Iteration 40100, loss = 2.41765359\n",
      "Iteration 40101, loss = 2.87215317\n",
      "Iteration 40102, loss = 2.62892272\n",
      "Iteration 40103, loss = 2.81878923\n",
      "Iteration 40104, loss = 2.90906054\n",
      "Iteration 40105, loss = 3.27111636\n",
      "Iteration 40106, loss = 2.98818935\n",
      "Iteration 40107, loss = 2.08201954\n",
      "Iteration 40108, loss = 2.42703529\n",
      "Iteration 40109, loss = 2.01206623\n",
      "Iteration 40110, loss = 2.61192808\n",
      "Iteration 40111, loss = 2.42901963\n",
      "Iteration 40112, loss = 2.37850645\n",
      "Iteration 40113, loss = 2.11499998\n",
      "Iteration 40114, loss = 2.05768189\n",
      "Iteration 40115, loss = 1.96084235\n",
      "Iteration 40116, loss = 2.29054710\n",
      "Iteration 40117, loss = 2.30335615\n",
      "Iteration 40118, loss = 2.57681172\n",
      "Iteration 40119, loss = 2.53052768\n",
      "Iteration 40120, loss = 2.38528944\n",
      "Iteration 40121, loss = 3.41850261\n",
      "Iteration 40122, loss = 2.28228820\n",
      "Iteration 40123, loss = 2.98881315\n",
      "Iteration 40124, loss = 3.27292393\n",
      "Iteration 40125, loss = 2.55191695\n",
      "Iteration 40126, loss = 2.16120496\n",
      "Iteration 40127, loss = 2.22973076\n",
      "Iteration 40128, loss = 2.13204324\n",
      "Iteration 40129, loss = 2.32331607\n",
      "Iteration 40130, loss = 2.29286537\n",
      "Iteration 40131, loss = 1.89050653\n",
      "Iteration 40132, loss = 2.45840935\n",
      "Iteration 40133, loss = 2.46786935\n",
      "Iteration 40134, loss = 2.33068605\n",
      "Iteration 40135, loss = 2.10334134\n",
      "Iteration 40136, loss = 2.01940013\n",
      "Iteration 40137, loss = 2.18307758\n",
      "Iteration 40138, loss = 2.12827665\n",
      "Iteration 40139, loss = 2.32691739\n",
      "Iteration 40140, loss = 3.39138184\n",
      "Iteration 40141, loss = 3.04414639\n",
      "Iteration 40142, loss = 3.59387307\n",
      "Iteration 40143, loss = 2.52586974\n",
      "Iteration 40144, loss = 2.73178610\n",
      "Iteration 40145, loss = 3.20839829\n",
      "Iteration 40146, loss = 2.24932202\n",
      "Iteration 40147, loss = 2.06650996\n",
      "Iteration 40148, loss = 1.99029991\n",
      "Iteration 40149, loss = 1.90603285\n",
      "Iteration 40150, loss = 1.92701737\n",
      "Iteration 40151, loss = 2.03551423\n",
      "Iteration 40152, loss = 1.96511323\n",
      "Iteration 40153, loss = 1.84266169\n",
      "Iteration 40154, loss = 2.06681369\n",
      "Iteration 40155, loss = 2.20477179\n",
      "Iteration 40156, loss = 2.09031828\n",
      "Iteration 40157, loss = 2.32071191\n",
      "Iteration 40158, loss = 2.57193896\n",
      "Iteration 40159, loss = 2.40387764\n",
      "Iteration 40160, loss = 2.19449376\n",
      "Iteration 40161, loss = 2.24949314\n",
      "Iteration 40162, loss = 1.79014689\n",
      "Iteration 40163, loss = 1.90198468\n",
      "Iteration 40164, loss = 2.03863300\n",
      "Iteration 40165, loss = 1.92733968\n",
      "Iteration 40166, loss = 2.29322403\n",
      "Iteration 40167, loss = 2.68150658\n",
      "Iteration 40168, loss = 2.84006982\n",
      "Iteration 40169, loss = 3.01775703\n",
      "Iteration 40170, loss = 2.57795118\n",
      "Iteration 40171, loss = 3.10582668\n",
      "Iteration 40172, loss = 2.69655360\n",
      "Iteration 40173, loss = 2.46894452\n",
      "Iteration 40174, loss = 2.86404613\n",
      "Iteration 40175, loss = 2.44383157\n",
      "Iteration 40176, loss = 3.12146511\n",
      "Iteration 40177, loss = 2.52619132\n",
      "Iteration 40178, loss = 2.48655804\n",
      "Iteration 40179, loss = 2.53765071\n",
      "Iteration 40180, loss = 2.14756255\n",
      "Iteration 40181, loss = 2.89489657\n",
      "Iteration 40182, loss = 2.79082358\n",
      "Iteration 40183, loss = 3.13428986\n",
      "Iteration 40184, loss = 2.82157055\n",
      "Iteration 40185, loss = 2.45464665\n",
      "Iteration 40186, loss = 2.71083251\n",
      "Iteration 40187, loss = 3.00428814\n",
      "Iteration 40188, loss = 3.16305619\n",
      "Iteration 40189, loss = 2.83958056\n",
      "Iteration 40190, loss = 3.49340313\n",
      "Iteration 40191, loss = 5.65686830\n",
      "Iteration 40192, loss = 7.87385204\n",
      "Iteration 40193, loss = 6.83896555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 40194, loss = 6.45429464\n",
      "Iteration 40195, loss = 5.53446271\n",
      "Iteration 40196, loss = 7.30123567\n",
      "Iteration 40197, loss = 5.53463453\n",
      "Iteration 40198, loss = 4.57237806\n",
      "Iteration 40199, loss = 4.79924765\n",
      "Iteration 40200, loss = 3.56092152\n",
      "Iteration 40201, loss = 2.90214433\n",
      "Iteration 40202, loss = 2.57431466\n",
      "Iteration 40203, loss = 2.16146659\n",
      "Iteration 40204, loss = 2.30300464\n",
      "Iteration 40205, loss = 2.18024253\n",
      "Iteration 40206, loss = 2.32015369\n",
      "Iteration 40207, loss = 2.41594032\n",
      "Iteration 40208, loss = 2.28785692\n",
      "Iteration 40209, loss = 2.53539527\n",
      "Iteration 40210, loss = 2.36335637\n",
      "Iteration 40211, loss = 2.40798647\n",
      "Iteration 40212, loss = 2.20615746\n",
      "Iteration 40213, loss = 2.37656898\n",
      "Iteration 40214, loss = 2.51374378\n",
      "Iteration 40215, loss = 2.38473462\n",
      "Iteration 40216, loss = 2.07927218\n",
      "Iteration 40217, loss = 1.92399554\n",
      "Iteration 40218, loss = 2.18749089\n",
      "Iteration 40219, loss = 1.92344070\n",
      "Iteration 40220, loss = 2.07866981\n",
      "Iteration 40221, loss = 2.07050885\n",
      "Iteration 40222, loss = 1.89338229\n",
      "Iteration 40223, loss = 1.87524259\n",
      "Iteration 40224, loss = 1.78837576\n",
      "Iteration 40225, loss = 2.02243380\n",
      "Iteration 40226, loss = 2.29587923\n",
      "Iteration 40227, loss = 2.60254594\n",
      "Iteration 40228, loss = 2.45205206\n",
      "Iteration 40229, loss = 2.43128420\n",
      "Iteration 40230, loss = 2.33304645\n",
      "Iteration 40231, loss = 1.88404677\n",
      "Iteration 40232, loss = 2.04096754\n",
      "Iteration 40233, loss = 1.96193473\n",
      "Iteration 40234, loss = 1.93230020\n",
      "Iteration 40235, loss = 1.88148590\n",
      "Iteration 40236, loss = 1.85893252\n",
      "Iteration 40237, loss = 1.93387528\n",
      "Iteration 40238, loss = 2.03873878\n",
      "Iteration 40239, loss = 1.86303426\n",
      "Iteration 40240, loss = 2.37863467\n",
      "Iteration 40241, loss = 2.48841311\n",
      "Iteration 40242, loss = 2.17144745\n",
      "Iteration 40243, loss = 1.94412924\n",
      "Iteration 40244, loss = 1.85237556\n",
      "Iteration 40245, loss = 2.11208988\n",
      "Iteration 40246, loss = 2.08247668\n",
      "Iteration 40247, loss = 2.40116090\n",
      "Iteration 40248, loss = 2.18701316\n",
      "Iteration 40249, loss = 3.28568929\n",
      "Iteration 40250, loss = 2.93929038\n",
      "Iteration 40251, loss = 2.89285488\n",
      "Iteration 40252, loss = 3.12300455\n",
      "Iteration 40253, loss = 3.49964367\n",
      "Iteration 40254, loss = 2.91527067\n",
      "Iteration 40255, loss = 2.34782609\n",
      "Iteration 40256, loss = 2.53348533\n",
      "Iteration 40257, loss = 2.24991943\n",
      "Iteration 40258, loss = 2.02195841\n",
      "Iteration 40259, loss = 1.97057082\n",
      "Iteration 40260, loss = 2.30262402\n",
      "Iteration 40261, loss = 2.04069315\n",
      "Iteration 40262, loss = 1.86920197\n",
      "Iteration 40263, loss = 2.07236082\n",
      "Iteration 40264, loss = 1.94219883\n",
      "Iteration 40265, loss = 1.85340474\n",
      "Iteration 40266, loss = 1.93665029\n",
      "Iteration 40267, loss = 2.00658290\n",
      "Iteration 40268, loss = 2.15954211\n",
      "Iteration 40269, loss = 2.12892930\n",
      "Iteration 40270, loss = 2.11784171\n",
      "Iteration 40271, loss = 2.28418607\n",
      "Iteration 40272, loss = 2.30775209\n",
      "Iteration 40273, loss = 1.99337547\n",
      "Iteration 40274, loss = 1.91576274\n",
      "Iteration 40275, loss = 1.93236776\n",
      "Iteration 40276, loss = 1.95253833\n",
      "Iteration 40277, loss = 2.02843329\n",
      "Iteration 40278, loss = 2.39669523\n",
      "Iteration 40279, loss = 2.20381150\n",
      "Iteration 40280, loss = 2.52208048\n",
      "Iteration 40281, loss = 2.08478689\n",
      "Iteration 40282, loss = 2.12628690\n",
      "Iteration 40283, loss = 2.09901343\n",
      "Iteration 40284, loss = 2.33129183\n",
      "Iteration 40285, loss = 1.91721611\n",
      "Iteration 40286, loss = 2.13226630\n",
      "Iteration 40287, loss = 1.96485206\n",
      "Iteration 40288, loss = 1.89728401\n",
      "Iteration 40289, loss = 2.10747062\n",
      "Iteration 40290, loss = 2.44294371\n",
      "Iteration 40291, loss = 2.81486043\n",
      "Iteration 40292, loss = 2.30760755\n",
      "Iteration 40293, loss = 2.54822027\n",
      "Iteration 40294, loss = 2.27648566\n",
      "Iteration 40295, loss = 2.40103841\n",
      "Iteration 40296, loss = 2.67716339\n",
      "Iteration 40297, loss = 2.59346834\n",
      "Iteration 40298, loss = 2.96633035\n",
      "Iteration 40299, loss = 2.27494582\n",
      "Iteration 40300, loss = 2.80811096\n",
      "Iteration 40301, loss = 2.43405433\n",
      "Iteration 40302, loss = 2.47139870\n",
      "Iteration 40303, loss = 2.30272280\n",
      "Iteration 40304, loss = 2.46218451\n",
      "Iteration 40305, loss = 2.12982677\n",
      "Iteration 40306, loss = 2.47734048\n",
      "Iteration 40307, loss = 2.38834776\n",
      "Iteration 40308, loss = 2.45548559\n",
      "Iteration 40309, loss = 2.41052116\n",
      "Iteration 40310, loss = 2.06370787\n",
      "Iteration 40311, loss = 2.16065930\n",
      "Iteration 40312, loss = 1.93803724\n",
      "Iteration 40313, loss = 1.99404429\n",
      "Iteration 40314, loss = 1.91901686\n",
      "Iteration 40315, loss = 1.93046463\n",
      "Iteration 40316, loss = 1.82521173\n",
      "Iteration 40317, loss = 1.92375496\n",
      "Iteration 40318, loss = 1.90332610\n",
      "Iteration 40319, loss = 1.86088153\n",
      "Iteration 40320, loss = 2.26130863\n",
      "Iteration 40321, loss = 1.73860473\n",
      "Iteration 40322, loss = 2.00019238\n",
      "Iteration 40323, loss = 1.99366523\n",
      "Iteration 40324, loss = 2.06415472\n",
      "Iteration 40325, loss = 1.95031992\n",
      "Iteration 40326, loss = 2.04610457\n",
      "Iteration 40327, loss = 2.10145376\n",
      "Iteration 40328, loss = 2.13109472\n",
      "Iteration 40329, loss = 1.90109279\n",
      "Iteration 40330, loss = 2.09953694\n",
      "Iteration 40331, loss = 2.24533012\n",
      "Iteration 40332, loss = 2.07898887\n",
      "Iteration 40333, loss = 2.00884817\n",
      "Iteration 40334, loss = 2.03783703\n",
      "Iteration 40335, loss = 2.80052614\n",
      "Iteration 40336, loss = 2.49587011\n",
      "Iteration 40337, loss = 2.42956970\n",
      "Iteration 40338, loss = 2.09620404\n",
      "Iteration 40339, loss = 2.26272339\n",
      "Iteration 40340, loss = 2.62900883\n",
      "Iteration 40341, loss = 2.61353166\n",
      "Iteration 40342, loss = 3.58834857\n",
      "Iteration 40343, loss = 3.87145319\n",
      "Iteration 40344, loss = 3.32970180\n",
      "Iteration 40345, loss = 2.06539771\n",
      "Iteration 40346, loss = 2.29755167\n",
      "Iteration 40347, loss = 2.51570593\n",
      "Iteration 40348, loss = 2.18387424\n",
      "Iteration 40349, loss = 2.09611361\n",
      "Iteration 40350, loss = 2.17497230\n",
      "Iteration 40351, loss = 2.01862314\n",
      "Iteration 40352, loss = 2.19553902\n",
      "Iteration 40353, loss = 2.07514231\n",
      "Iteration 40354, loss = 2.51980170\n",
      "Iteration 40355, loss = 3.59008526\n",
      "Iteration 40356, loss = 2.99819982\n",
      "Iteration 40357, loss = 2.87347121\n",
      "Iteration 40358, loss = 2.26158166\n",
      "Iteration 40359, loss = 1.93767888\n",
      "Iteration 40360, loss = 2.02137159\n",
      "Iteration 40361, loss = 2.15833244\n",
      "Iteration 40362, loss = 3.27362688\n",
      "Iteration 40363, loss = 2.98308864\n",
      "Iteration 40364, loss = 3.29083337\n",
      "Iteration 40365, loss = 3.39825382\n",
      "Iteration 40366, loss = 4.38168601\n",
      "Iteration 40367, loss = 2.95013093\n",
      "Iteration 40368, loss = 2.71602668\n",
      "Iteration 40369, loss = 2.83909385\n",
      "Iteration 40370, loss = 3.09710259\n",
      "Iteration 40371, loss = 3.14980097\n",
      "Iteration 40372, loss = 3.35643151\n",
      "Iteration 40373, loss = 3.13656484\n",
      "Iteration 40374, loss = 3.51543292\n",
      "Iteration 40375, loss = 2.29638451\n",
      "Iteration 40376, loss = 2.15752169\n",
      "Iteration 40377, loss = 2.04237555\n",
      "Iteration 40378, loss = 1.79982832\n",
      "Iteration 40379, loss = 2.07423045\n",
      "Iteration 40380, loss = 1.96663295\n",
      "Iteration 40381, loss = 2.35744554\n",
      "Iteration 40382, loss = 2.51045917\n",
      "Iteration 40383, loss = 2.64631309\n",
      "Iteration 40384, loss = 3.35365715\n",
      "Iteration 40385, loss = 3.31000670\n",
      "Iteration 40386, loss = 2.35446310\n",
      "Iteration 40387, loss = 3.28001056\n",
      "Iteration 40388, loss = 2.81340324\n",
      "Iteration 40389, loss = 2.74380799\n",
      "Iteration 40390, loss = 2.42736112\n",
      "Iteration 40391, loss = 2.15736555\n",
      "Iteration 40392, loss = 2.56516253\n",
      "Iteration 40393, loss = 2.50883646\n",
      "Iteration 40394, loss = 2.79823652\n",
      "Iteration 40395, loss = 3.35446648\n",
      "Iteration 40396, loss = 2.27947164\n",
      "Iteration 40397, loss = 2.06156544\n",
      "Iteration 40398, loss = 2.99364260\n",
      "Iteration 40399, loss = 2.78062842\n",
      "Iteration 40400, loss = 3.00353903\n",
      "Iteration 40401, loss = 3.47220987\n",
      "Iteration 40402, loss = 5.73385775\n",
      "Iteration 40403, loss = 3.88301777\n",
      "Iteration 40404, loss = 3.52944816\n",
      "Iteration 40405, loss = 3.18078570\n",
      "Iteration 40406, loss = 2.54930812\n",
      "Iteration 40407, loss = 2.69981277\n",
      "Iteration 40408, loss = 2.65431737\n",
      "Iteration 40409, loss = 3.08119092\n",
      "Iteration 40410, loss = 2.93275206\n",
      "Iteration 40411, loss = 2.50767624\n",
      "Iteration 40412, loss = 2.40914000\n",
      "Iteration 40413, loss = 2.47829469\n",
      "Iteration 40414, loss = 3.51492247\n",
      "Iteration 40415, loss = 3.03717148\n",
      "Iteration 40416, loss = 2.57241892\n",
      "Iteration 40417, loss = 2.46438309\n",
      "Iteration 40418, loss = 2.97600655\n",
      "Iteration 40419, loss = 2.02678411\n",
      "Iteration 40420, loss = 2.16002003\n",
      "Iteration 40421, loss = 2.54576109\n",
      "Iteration 40422, loss = 2.31282879\n",
      "Iteration 40423, loss = 2.31284781\n",
      "Iteration 40424, loss = 2.13865497\n",
      "Iteration 40425, loss = 1.80728712\n",
      "Iteration 40426, loss = 1.91315340\n",
      "Iteration 40427, loss = 1.93193149\n",
      "Iteration 40428, loss = 2.01092977\n",
      "Iteration 40429, loss = 2.25822285\n",
      "Iteration 40430, loss = 2.16376081\n",
      "Iteration 40431, loss = 2.45866312\n",
      "Iteration 40432, loss = 2.32317737\n",
      "Iteration 40433, loss = 1.87286680\n",
      "Iteration 40434, loss = 1.90265660\n",
      "Iteration 40435, loss = 2.40414225\n",
      "Iteration 40436, loss = 2.07098156\n",
      "Iteration 40437, loss = 2.13682392\n",
      "Iteration 40438, loss = 2.40031400\n",
      "Iteration 40439, loss = 3.05934684\n",
      "Iteration 40440, loss = 2.39942398\n",
      "Iteration 40441, loss = 2.98378057\n",
      "Iteration 40442, loss = 3.36580436\n",
      "Iteration 40443, loss = 2.56885334\n",
      "Iteration 40444, loss = 2.59078263\n",
      "Iteration 40445, loss = 2.08161617\n",
      "Iteration 40446, loss = 2.19720111\n",
      "Iteration 40447, loss = 2.04853402\n",
      "Iteration 40448, loss = 2.08813615\n",
      "Iteration 40449, loss = 2.22092460\n",
      "Iteration 40450, loss = 2.22966749\n",
      "Iteration 40451, loss = 2.08766837\n",
      "Iteration 40452, loss = 2.07685236\n",
      "Iteration 40453, loss = 1.96127394\n",
      "Iteration 40454, loss = 1.87188563\n",
      "Iteration 40455, loss = 1.91564632\n",
      "Iteration 40456, loss = 1.85752027\n",
      "Iteration 40457, loss = 1.84715023\n",
      "Iteration 40458, loss = 1.98428759\n",
      "Iteration 40459, loss = 1.99542571\n",
      "Iteration 40460, loss = 2.21639416\n",
      "Iteration 40461, loss = 2.39951502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 40462, loss = 2.14658144\n",
      "Iteration 40463, loss = 2.03984106\n",
      "Iteration 40464, loss = 2.21939405\n",
      "Iteration 40465, loss = 1.89472641\n",
      "Iteration 40466, loss = 1.93318570\n",
      "Iteration 40467, loss = 1.86137427\n",
      "Iteration 40468, loss = 1.93869409\n",
      "Iteration 40469, loss = 1.99592211\n",
      "Iteration 40470, loss = 2.05047088\n",
      "Iteration 40471, loss = 1.94030758\n",
      "Iteration 40472, loss = 2.23251352\n",
      "Iteration 40473, loss = 2.45439263\n",
      "Iteration 40474, loss = 2.39125147\n",
      "Iteration 40475, loss = 2.04504889\n",
      "Iteration 40476, loss = 1.77062209\n",
      "Iteration 40477, loss = 1.81345526\n",
      "Iteration 40478, loss = 2.00482080\n",
      "Iteration 40479, loss = 1.96266114\n",
      "Iteration 40480, loss = 1.87208277\n",
      "Iteration 40481, loss = 1.82345018\n",
      "Iteration 40482, loss = 2.18502714\n",
      "Iteration 40483, loss = 2.64205463\n",
      "Iteration 40484, loss = 2.41477312\n",
      "Iteration 40485, loss = 2.68603358\n",
      "Iteration 40486, loss = 3.33119058\n",
      "Iteration 40487, loss = 3.30671409\n",
      "Iteration 40488, loss = 2.40839977\n",
      "Iteration 40489, loss = 2.54846130\n",
      "Iteration 40490, loss = 3.64388421\n",
      "Iteration 40491, loss = 5.89545039\n",
      "Iteration 40492, loss = 4.41898573\n",
      "Iteration 40493, loss = 4.09165127\n",
      "Iteration 40494, loss = 3.80689159\n",
      "Iteration 40495, loss = 3.35431790\n",
      "Iteration 40496, loss = 2.62481390\n",
      "Iteration 40497, loss = 2.93238108\n",
      "Iteration 40498, loss = 2.97417528\n",
      "Iteration 40499, loss = 2.47571734\n",
      "Iteration 40500, loss = 2.63650201\n",
      "Iteration 40501, loss = 2.54643701\n",
      "Iteration 40502, loss = 2.60881581\n",
      "Iteration 40503, loss = 2.41653500\n",
      "Iteration 40504, loss = 2.76164527\n",
      "Iteration 40505, loss = 2.44537715\n",
      "Iteration 40506, loss = 2.66985346\n",
      "Iteration 40507, loss = 2.67270456\n",
      "Iteration 40508, loss = 2.56239419\n",
      "Iteration 40509, loss = 2.47899254\n",
      "Iteration 40510, loss = 2.59425537\n",
      "Iteration 40511, loss = 2.23096828\n",
      "Iteration 40512, loss = 2.07745704\n",
      "Iteration 40513, loss = 2.36369030\n",
      "Iteration 40514, loss = 2.70129201\n",
      "Iteration 40515, loss = 2.63884637\n",
      "Iteration 40516, loss = 3.13745443\n",
      "Iteration 40517, loss = 2.53903166\n",
      "Iteration 40518, loss = 3.62710037\n",
      "Iteration 40519, loss = 3.20203613\n",
      "Iteration 40520, loss = 2.97581547\n",
      "Iteration 40521, loss = 2.76546700\n",
      "Iteration 40522, loss = 2.05093836\n",
      "Iteration 40523, loss = 2.35421268\n",
      "Iteration 40524, loss = 3.78736319\n",
      "Iteration 40525, loss = 3.85612584\n",
      "Iteration 40526, loss = 3.95100929\n",
      "Iteration 40527, loss = 2.50893312\n",
      "Iteration 40528, loss = 2.21921466\n",
      "Iteration 40529, loss = 2.20938318\n",
      "Iteration 40530, loss = 2.01820780\n",
      "Iteration 40531, loss = 2.06280646\n",
      "Iteration 40532, loss = 1.89067274\n",
      "Iteration 40533, loss = 1.77262097\n",
      "Iteration 40534, loss = 1.79780875\n",
      "Iteration 40535, loss = 1.92547426\n",
      "Iteration 40536, loss = 2.26545853\n",
      "Iteration 40537, loss = 2.38441063\n",
      "Iteration 40538, loss = 2.15077934\n",
      "Iteration 40539, loss = 2.00816892\n",
      "Iteration 40540, loss = 2.13754362\n",
      "Iteration 40541, loss = 2.40574805\n",
      "Iteration 40542, loss = 3.16057344\n",
      "Iteration 40543, loss = 2.52506736\n",
      "Iteration 40544, loss = 2.98080670\n",
      "Iteration 40545, loss = 2.63217891\n",
      "Iteration 40546, loss = 2.58540930\n",
      "Iteration 40547, loss = 2.35297744\n",
      "Iteration 40548, loss = 2.15736612\n",
      "Iteration 40549, loss = 1.88813965\n",
      "Iteration 40550, loss = 2.18747047\n",
      "Iteration 40551, loss = 2.17088439\n",
      "Iteration 40552, loss = 2.19452561\n",
      "Iteration 40553, loss = 2.47525197\n",
      "Iteration 40554, loss = 2.26494678\n",
      "Iteration 40555, loss = 2.35180539\n",
      "Iteration 40556, loss = 2.79842316\n",
      "Iteration 40557, loss = 4.22467686\n",
      "Iteration 40558, loss = 3.89340785\n",
      "Iteration 40559, loss = 3.52752095\n",
      "Iteration 40560, loss = 4.30505037\n",
      "Iteration 40561, loss = 2.52971594\n",
      "Iteration 40562, loss = 2.82450716\n",
      "Iteration 40563, loss = 2.11116359\n",
      "Iteration 40564, loss = 2.14256780\n",
      "Iteration 40565, loss = 2.21071390\n",
      "Iteration 40566, loss = 2.28739365\n",
      "Iteration 40567, loss = 2.37631194\n",
      "Iteration 40568, loss = 2.73514739\n",
      "Iteration 40569, loss = 2.07342855\n",
      "Iteration 40570, loss = 2.12767666\n",
      "Iteration 40571, loss = 1.98028922\n",
      "Iteration 40572, loss = 2.06247331\n",
      "Iteration 40573, loss = 1.84924606\n",
      "Iteration 40574, loss = 1.97788577\n",
      "Iteration 40575, loss = 2.59098161\n",
      "Iteration 40576, loss = 2.18034879\n",
      "Iteration 40577, loss = 2.23680332\n",
      "Iteration 40578, loss = 2.09284506\n",
      "Iteration 40579, loss = 1.76599929\n",
      "Iteration 40580, loss = 1.78626310\n",
      "Iteration 40581, loss = 1.80536573\n",
      "Iteration 40582, loss = 2.05323954\n",
      "Iteration 40583, loss = 1.97928493\n",
      "Iteration 40584, loss = 1.90105507\n",
      "Iteration 40585, loss = 1.87968688\n",
      "Iteration 40586, loss = 1.96294776\n",
      "Iteration 40587, loss = 1.91312533\n",
      "Iteration 40588, loss = 1.98482766\n",
      "Iteration 40589, loss = 2.15327915\n",
      "Iteration 40590, loss = 2.16405233\n",
      "Iteration 40591, loss = 2.34338338\n",
      "Iteration 40592, loss = 2.24469120\n",
      "Iteration 40593, loss = 3.09249093\n",
      "Iteration 40594, loss = 3.48809919\n",
      "Iteration 40595, loss = 3.13232360\n",
      "Iteration 40596, loss = 2.79659226\n",
      "Iteration 40597, loss = 2.04720400\n",
      "Iteration 40598, loss = 2.65652744\n",
      "Iteration 40599, loss = 3.16566991\n",
      "Iteration 40600, loss = 2.94101294\n",
      "Iteration 40601, loss = 2.79775351\n",
      "Iteration 40602, loss = 2.36622464\n",
      "Iteration 40603, loss = 2.00599596\n",
      "Iteration 40604, loss = 1.83672412\n",
      "Iteration 40605, loss = 2.05496712\n",
      "Iteration 40606, loss = 1.98417522\n",
      "Iteration 40607, loss = 1.77408479\n",
      "Iteration 40608, loss = 1.83710701\n",
      "Iteration 40609, loss = 1.83721793\n",
      "Iteration 40610, loss = 1.91892480\n",
      "Iteration 40611, loss = 1.86885154\n",
      "Iteration 40612, loss = 2.16791903\n",
      "Iteration 40613, loss = 2.03397970\n",
      "Iteration 40614, loss = 1.91348096\n",
      "Iteration 40615, loss = 1.84111534\n",
      "Iteration 40616, loss = 2.13795948\n",
      "Iteration 40617, loss = 2.10999862\n",
      "Iteration 40618, loss = 2.62981422\n",
      "Iteration 40619, loss = 2.11776439\n",
      "Iteration 40620, loss = 1.94063138\n",
      "Iteration 40621, loss = 2.01553356\n",
      "Iteration 40622, loss = 2.22958484\n",
      "Iteration 40623, loss = 2.29968221\n",
      "Iteration 40624, loss = 2.09969330\n",
      "Iteration 40625, loss = 2.81838383\n",
      "Iteration 40626, loss = 2.28142059\n",
      "Iteration 40627, loss = 2.88351061\n",
      "Iteration 40628, loss = 1.93406641\n",
      "Iteration 40629, loss = 2.24667929\n",
      "Iteration 40630, loss = 2.37914322\n",
      "Iteration 40631, loss = 2.12510601\n",
      "Iteration 40632, loss = 2.39683622\n",
      "Iteration 40633, loss = 2.31897207\n",
      "Iteration 40634, loss = 3.25231243\n",
      "Iteration 40635, loss = 3.57586751\n",
      "Iteration 40636, loss = 2.45853542\n",
      "Iteration 40637, loss = 2.02057777\n",
      "Iteration 40638, loss = 2.16857187\n",
      "Iteration 40639, loss = 2.65128125\n",
      "Iteration 40640, loss = 3.57737336\n",
      "Iteration 40641, loss = 3.40417195\n",
      "Iteration 40642, loss = 3.16015294\n",
      "Iteration 40643, loss = 2.18046467\n",
      "Iteration 40644, loss = 2.59577749\n",
      "Iteration 40645, loss = 2.52432418\n",
      "Iteration 40646, loss = 2.27708237\n",
      "Iteration 40647, loss = 2.10681391\n",
      "Iteration 40648, loss = 2.32907575\n",
      "Iteration 40649, loss = 2.93461299\n",
      "Iteration 40650, loss = 2.99186886\n",
      "Iteration 40651, loss = 3.66311981\n",
      "Iteration 40652, loss = 3.38071376\n",
      "Iteration 40653, loss = 2.23583266\n",
      "Iteration 40654, loss = 2.47355201\n",
      "Iteration 40655, loss = 2.35778998\n",
      "Iteration 40656, loss = 2.09371340\n",
      "Iteration 40657, loss = 2.27904172\n",
      "Iteration 40658, loss = 2.45801983\n",
      "Iteration 40659, loss = 2.83637089\n",
      "Iteration 40660, loss = 2.05916280\n",
      "Iteration 40661, loss = 2.15824170\n",
      "Iteration 40662, loss = 2.65537651\n",
      "Iteration 40663, loss = 2.12785026\n",
      "Iteration 40664, loss = 2.41136348\n",
      "Iteration 40665, loss = 2.31080503\n",
      "Iteration 40666, loss = 2.33193667\n",
      "Iteration 40667, loss = 1.78931385\n",
      "Iteration 40668, loss = 1.92986152\n",
      "Iteration 40669, loss = 1.77670767\n",
      "Iteration 40670, loss = 2.51347354\n",
      "Iteration 40671, loss = 2.58441029\n",
      "Iteration 40672, loss = 2.27027205\n",
      "Iteration 40673, loss = 2.25018625\n",
      "Iteration 40674, loss = 2.19158822\n",
      "Iteration 40675, loss = 2.06204670\n",
      "Iteration 40676, loss = 2.10802601\n",
      "Iteration 40677, loss = 2.15725561\n",
      "Iteration 40678, loss = 2.08145977\n",
      "Iteration 40679, loss = 1.82769909\n",
      "Iteration 40680, loss = 1.98258537\n",
      "Iteration 40681, loss = 2.40002923\n",
      "Iteration 40682, loss = 2.11478637\n",
      "Iteration 40683, loss = 2.29720289\n",
      "Iteration 40684, loss = 2.35179924\n",
      "Iteration 40685, loss = 2.56344859\n",
      "Iteration 40686, loss = 2.42078304\n",
      "Iteration 40687, loss = 3.10731785\n",
      "Iteration 40688, loss = 2.39114335\n",
      "Iteration 40689, loss = 1.97158191\n",
      "Iteration 40690, loss = 2.05172739\n",
      "Iteration 40691, loss = 2.00116150\n",
      "Iteration 40692, loss = 2.30072670\n",
      "Iteration 40693, loss = 2.15863190\n",
      "Iteration 40694, loss = 2.11670652\n",
      "Iteration 40695, loss = 1.82461474\n",
      "Iteration 40696, loss = 2.08021142\n",
      "Iteration 40697, loss = 2.09474177\n",
      "Iteration 40698, loss = 1.80613782\n",
      "Iteration 40699, loss = 2.16308918\n",
      "Iteration 40700, loss = 2.85220959\n",
      "Iteration 40701, loss = 3.75589622\n",
      "Iteration 40702, loss = 3.75339905\n",
      "Iteration 40703, loss = 3.23569479\n",
      "Iteration 40704, loss = 2.92713966\n",
      "Iteration 40705, loss = 2.51052533\n",
      "Iteration 40706, loss = 2.66769662\n",
      "Iteration 40707, loss = 2.49392856\n",
      "Iteration 40708, loss = 2.32563751\n",
      "Iteration 40709, loss = 2.20543001\n",
      "Iteration 40710, loss = 1.87214370\n",
      "Iteration 40711, loss = 2.06559063\n",
      "Iteration 40712, loss = 2.21581816\n",
      "Iteration 40713, loss = 2.33026405\n",
      "Iteration 40714, loss = 2.25039937\n",
      "Iteration 40715, loss = 2.26070214\n",
      "Iteration 40716, loss = 1.99770965\n",
      "Iteration 40717, loss = 1.71249632\n",
      "Iteration 40718, loss = 1.89961742\n",
      "Iteration 40719, loss = 2.04625637\n",
      "Iteration 40720, loss = 2.49488205\n",
      "Iteration 40721, loss = 3.08981012\n",
      "Iteration 40722, loss = 2.34075843\n",
      "Iteration 40723, loss = 2.64822638\n",
      "Iteration 40724, loss = 2.36780403\n",
      "Iteration 40725, loss = 2.21886781\n",
      "Iteration 40726, loss = 2.39192077\n",
      "Iteration 40727, loss = 2.24931423\n",
      "Iteration 40728, loss = 2.30298896\n",
      "Iteration 40729, loss = 2.10421849\n",
      "Iteration 40730, loss = 2.06319773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 40731, loss = 2.02970931\n",
      "Iteration 40732, loss = 2.21069313\n",
      "Iteration 40733, loss = 2.14522772\n",
      "Iteration 40734, loss = 2.76733513\n",
      "Iteration 40735, loss = 3.17266094\n",
      "Iteration 40736, loss = 2.43399736\n",
      "Iteration 40737, loss = 2.31870452\n",
      "Iteration 40738, loss = 1.87818111\n",
      "Iteration 40739, loss = 2.09324434\n",
      "Iteration 40740, loss = 2.43020490\n",
      "Iteration 40741, loss = 2.51725512\n",
      "Iteration 40742, loss = 2.70048748\n",
      "Iteration 40743, loss = 1.84516183\n",
      "Iteration 40744, loss = 2.00947232\n",
      "Iteration 40745, loss = 1.96143268\n",
      "Iteration 40746, loss = 2.23216562\n",
      "Iteration 40747, loss = 2.04351415\n",
      "Iteration 40748, loss = 2.18866117\n",
      "Iteration 40749, loss = 2.09788928\n",
      "Iteration 40750, loss = 2.14994053\n",
      "Iteration 40751, loss = 1.95837735\n",
      "Iteration 40752, loss = 1.85828394\n",
      "Iteration 40753, loss = 1.72889167\n",
      "Iteration 40754, loss = 1.96568017\n",
      "Iteration 40755, loss = 2.20543601\n",
      "Iteration 40756, loss = 3.13130210\n",
      "Iteration 40757, loss = 2.01541057\n",
      "Iteration 40758, loss = 2.42816586\n",
      "Iteration 40759, loss = 2.46325441\n",
      "Iteration 40760, loss = 2.60009554\n",
      "Iteration 40761, loss = 1.93688786\n",
      "Iteration 40762, loss = 1.95897576\n",
      "Iteration 40763, loss = 1.85003536\n",
      "Iteration 40764, loss = 1.90926648\n",
      "Iteration 40765, loss = 1.88853882\n",
      "Iteration 40766, loss = 2.73095113\n",
      "Iteration 40767, loss = 2.65304954\n",
      "Iteration 40768, loss = 2.23196495\n",
      "Iteration 40769, loss = 2.39918872\n",
      "Iteration 40770, loss = 2.25324503\n",
      "Iteration 40771, loss = 2.26127592\n",
      "Iteration 40772, loss = 2.36336147\n",
      "Iteration 40773, loss = 2.12067491\n",
      "Iteration 40774, loss = 2.01494227\n",
      "Iteration 40775, loss = 2.48986657\n",
      "Iteration 40776, loss = 2.50464001\n",
      "Iteration 40777, loss = 2.25629287\n",
      "Iteration 40778, loss = 2.46114220\n",
      "Iteration 40779, loss = 2.49181340\n",
      "Iteration 40780, loss = 2.05657742\n",
      "Iteration 40781, loss = 2.17379195\n",
      "Iteration 40782, loss = 2.27342179\n",
      "Iteration 40783, loss = 2.63399227\n",
      "Iteration 40784, loss = 2.55306041\n",
      "Iteration 40785, loss = 2.15958208\n",
      "Iteration 40786, loss = 2.20287075\n",
      "Iteration 40787, loss = 2.74279110\n",
      "Iteration 40788, loss = 2.64608937\n",
      "Iteration 40789, loss = 2.32107553\n",
      "Iteration 40790, loss = 2.58385680\n",
      "Iteration 40791, loss = 3.16306204\n",
      "Iteration 40792, loss = 3.45435503\n",
      "Iteration 40793, loss = 2.46798618\n",
      "Iteration 40794, loss = 2.61288417\n",
      "Iteration 40795, loss = 2.70128755\n",
      "Iteration 40796, loss = 2.12243159\n",
      "Iteration 40797, loss = 1.94881359\n",
      "Iteration 40798, loss = 1.97130687\n",
      "Iteration 40799, loss = 1.78852746\n",
      "Iteration 40800, loss = 1.80611462\n",
      "Iteration 40801, loss = 1.91196417\n",
      "Iteration 40802, loss = 1.82327365\n",
      "Iteration 40803, loss = 2.03901782\n",
      "Iteration 40804, loss = 2.40509737\n",
      "Iteration 40805, loss = 1.88708138\n",
      "Iteration 40806, loss = 2.04101974\n",
      "Iteration 40807, loss = 2.83409050\n",
      "Iteration 40808, loss = 2.77723248\n",
      "Iteration 40809, loss = 2.31831479\n",
      "Iteration 40810, loss = 2.04836361\n",
      "Iteration 40811, loss = 2.31537597\n",
      "Iteration 40812, loss = 2.07050445\n",
      "Iteration 40813, loss = 1.83027868\n",
      "Iteration 40814, loss = 1.90910498\n",
      "Iteration 40815, loss = 1.99436618\n",
      "Iteration 40816, loss = 2.05783255\n",
      "Iteration 40817, loss = 2.00785103\n",
      "Iteration 40818, loss = 1.82764472\n",
      "Iteration 40819, loss = 1.97677212\n",
      "Iteration 40820, loss = 1.96654145\n",
      "Iteration 40821, loss = 1.77452919\n",
      "Iteration 40822, loss = 1.76481329\n",
      "Iteration 40823, loss = 1.94070278\n",
      "Iteration 40824, loss = 2.38368681\n",
      "Iteration 40825, loss = 2.72845008\n",
      "Iteration 40826, loss = 2.21686108\n",
      "Iteration 40827, loss = 2.37007720\n",
      "Iteration 40828, loss = 3.01482073\n",
      "Iteration 40829, loss = 3.29582943\n",
      "Iteration 40830, loss = 3.32390067\n",
      "Iteration 40831, loss = 4.15034178\n",
      "Iteration 40832, loss = 2.99794212\n",
      "Iteration 40833, loss = 2.90075919\n",
      "Iteration 40834, loss = 2.71574273\n",
      "Iteration 40835, loss = 2.47319231\n",
      "Iteration 40836, loss = 2.41430007\n",
      "Iteration 40837, loss = 2.23085048\n",
      "Iteration 40838, loss = 2.07329162\n",
      "Iteration 40839, loss = 1.96918467\n",
      "Iteration 40840, loss = 1.99421672\n",
      "Iteration 40841, loss = 2.07249001\n",
      "Iteration 40842, loss = 1.90494579\n",
      "Iteration 40843, loss = 1.86135175\n",
      "Iteration 40844, loss = 2.06100968\n",
      "Iteration 40845, loss = 2.02110920\n",
      "Iteration 40846, loss = 2.05458230\n",
      "Iteration 40847, loss = 2.58251608\n",
      "Iteration 40848, loss = 2.56495284\n",
      "Iteration 40849, loss = 2.47670348\n",
      "Iteration 40850, loss = 2.38507616\n",
      "Iteration 40851, loss = 2.27149400\n",
      "Iteration 40852, loss = 2.09563768\n",
      "Iteration 40853, loss = 1.91814581\n",
      "Iteration 40854, loss = 2.22751684\n",
      "Iteration 40855, loss = 2.26596916\n",
      "Iteration 40856, loss = 2.13868480\n",
      "Iteration 40857, loss = 2.12739788\n",
      "Iteration 40858, loss = 2.06557684\n",
      "Iteration 40859, loss = 2.51614262\n",
      "Iteration 40860, loss = 1.92252731\n",
      "Iteration 40861, loss = 1.82038048\n",
      "Iteration 40862, loss = 1.88485926\n",
      "Iteration 40863, loss = 1.94450317\n",
      "Iteration 40864, loss = 2.05775628\n",
      "Iteration 40865, loss = 2.26054987\n",
      "Iteration 40866, loss = 2.61269015\n",
      "Iteration 40867, loss = 2.83698710\n",
      "Iteration 40868, loss = 2.42437793\n",
      "Iteration 40869, loss = 2.37217779\n",
      "Iteration 40870, loss = 2.60820337\n",
      "Iteration 40871, loss = 2.68743776\n",
      "Iteration 40872, loss = 3.33823322\n",
      "Iteration 40873, loss = 2.38983573\n",
      "Iteration 40874, loss = 2.23916291\n",
      "Iteration 40875, loss = 2.25661236\n",
      "Iteration 40876, loss = 2.64281561\n",
      "Iteration 40877, loss = 2.44547495\n",
      "Iteration 40878, loss = 3.12320160\n",
      "Iteration 40879, loss = 3.51871197\n",
      "Iteration 40880, loss = 3.68783310\n",
      "Iteration 40881, loss = 4.41205669\n",
      "Iteration 40882, loss = 3.07461536\n",
      "Iteration 40883, loss = 2.64362219\n",
      "Iteration 40884, loss = 2.93222220\n",
      "Iteration 40885, loss = 2.17130334\n",
      "Iteration 40886, loss = 2.59952483\n",
      "Iteration 40887, loss = 2.55042551\n",
      "Iteration 40888, loss = 2.72593689\n",
      "Iteration 40889, loss = 2.85612235\n",
      "Iteration 40890, loss = 2.99266540\n",
      "Iteration 40891, loss = 2.11890954\n",
      "Iteration 40892, loss = 1.97423288\n",
      "Iteration 40893, loss = 2.20664616\n",
      "Iteration 40894, loss = 2.20436204\n",
      "Iteration 40895, loss = 2.38466245\n",
      "Iteration 40896, loss = 2.74983460\n",
      "Iteration 40897, loss = 2.26817946\n",
      "Iteration 40898, loss = 2.29580366\n",
      "Iteration 40899, loss = 2.19482218\n",
      "Iteration 40900, loss = 2.14767334\n",
      "Iteration 40901, loss = 2.05162763\n",
      "Iteration 40902, loss = 2.02968240\n",
      "Iteration 40903, loss = 1.91880142\n",
      "Iteration 40904, loss = 2.04179203\n",
      "Iteration 40905, loss = 2.17052198\n",
      "Iteration 40906, loss = 2.12775722\n",
      "Iteration 40907, loss = 2.40865773\n",
      "Iteration 40908, loss = 2.11664464\n",
      "Iteration 40909, loss = 2.06700815\n",
      "Iteration 40910, loss = 1.88706545\n",
      "Iteration 40911, loss = 1.89224917\n",
      "Iteration 40912, loss = 2.08894729\n",
      "Iteration 40913, loss = 1.85615685\n",
      "Iteration 40914, loss = 1.82356412\n",
      "Iteration 40915, loss = 2.03748362\n",
      "Iteration 40916, loss = 2.03906011\n",
      "Iteration 40917, loss = 1.97040882\n",
      "Iteration 40918, loss = 2.09470713\n",
      "Iteration 40919, loss = 1.94972034\n",
      "Iteration 40920, loss = 2.11761790\n",
      "Iteration 40921, loss = 2.39180425\n",
      "Iteration 40922, loss = 2.46109457\n",
      "Iteration 40923, loss = 2.06232859\n",
      "Iteration 40924, loss = 2.05782160\n",
      "Iteration 40925, loss = 1.92399955\n",
      "Iteration 40926, loss = 2.62287844\n",
      "Iteration 40927, loss = 2.58105003\n",
      "Iteration 40928, loss = 2.43097456\n",
      "Iteration 40929, loss = 2.39773244\n",
      "Iteration 40930, loss = 2.16321871\n",
      "Iteration 40931, loss = 2.59301488\n",
      "Iteration 40932, loss = 2.71294997\n",
      "Iteration 40933, loss = 2.93924035\n",
      "Iteration 40934, loss = 2.63017343\n",
      "Iteration 40935, loss = 2.34304634\n",
      "Iteration 40936, loss = 2.34415756\n",
      "Iteration 40937, loss = 1.98016701\n",
      "Iteration 40938, loss = 2.06842710\n",
      "Iteration 40939, loss = 1.82237304\n",
      "Iteration 40940, loss = 2.08045999\n",
      "Iteration 40941, loss = 1.83337255\n",
      "Iteration 40942, loss = 2.05876545\n",
      "Iteration 40943, loss = 2.36057464\n",
      "Iteration 40944, loss = 2.16771294\n",
      "Iteration 40945, loss = 1.75906322\n",
      "Iteration 40946, loss = 2.00158527\n",
      "Iteration 40947, loss = 1.95768936\n",
      "Iteration 40948, loss = 1.83296111\n",
      "Iteration 40949, loss = 2.80235294\n",
      "Iteration 40950, loss = 4.57012867\n",
      "Iteration 40951, loss = 5.76070047\n",
      "Iteration 40952, loss = 5.95630050\n",
      "Iteration 40953, loss = 5.22884916\n",
      "Iteration 40954, loss = 2.61945532\n",
      "Iteration 40955, loss = 2.43867787\n",
      "Iteration 40956, loss = 1.96456932\n",
      "Iteration 40957, loss = 2.05491469\n",
      "Iteration 40958, loss = 2.03180085\n",
      "Iteration 40959, loss = 1.94641361\n",
      "Iteration 40960, loss = 2.10796853\n",
      "Iteration 40961, loss = 2.50215397\n",
      "Iteration 40962, loss = 2.61093656\n",
      "Iteration 40963, loss = 2.30187163\n",
      "Iteration 40964, loss = 1.98723768\n",
      "Iteration 40965, loss = 2.59816040\n",
      "Iteration 40966, loss = 2.23022495\n",
      "Iteration 40967, loss = 2.10670383\n",
      "Iteration 40968, loss = 2.18064515\n",
      "Iteration 40969, loss = 1.90924966\n",
      "Iteration 40970, loss = 2.06881917\n",
      "Iteration 40971, loss = 2.00022473\n",
      "Iteration 40972, loss = 1.87910239\n",
      "Iteration 40973, loss = 1.91024310\n",
      "Iteration 40974, loss = 1.88543651\n",
      "Iteration 40975, loss = 2.74087486\n",
      "Iteration 40976, loss = 1.97779428\n",
      "Iteration 40977, loss = 1.88196249\n",
      "Iteration 40978, loss = 1.90057033\n",
      "Iteration 40979, loss = 2.05783284\n",
      "Iteration 40980, loss = 2.08210670\n",
      "Iteration 40981, loss = 2.63408854\n",
      "Iteration 40982, loss = 3.43744576\n",
      "Iteration 40983, loss = 2.18461383\n",
      "Iteration 40984, loss = 2.00982034\n",
      "Iteration 40985, loss = 2.25972730\n",
      "Iteration 40986, loss = 2.27514761\n",
      "Iteration 40987, loss = 2.52763096\n",
      "Iteration 40988, loss = 1.83865996\n",
      "Iteration 40989, loss = 2.02403462\n",
      "Iteration 40990, loss = 1.89109529\n",
      "Iteration 40991, loss = 1.87250822\n",
      "Iteration 40992, loss = 1.85741130\n",
      "Iteration 40993, loss = 2.19950011\n",
      "Iteration 40994, loss = 2.06001212\n",
      "Iteration 40995, loss = 2.24672728\n",
      "Iteration 40996, loss = 1.88946408\n",
      "Iteration 40997, loss = 1.77098376\n",
      "Iteration 40998, loss = 1.97467464\n",
      "Iteration 40999, loss = 1.72855719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 41000, loss = 2.01362782\n",
      "Iteration 41001, loss = 1.99959355\n",
      "Iteration 41002, loss = 2.54994664\n",
      "Iteration 41003, loss = 2.91313202\n",
      "Iteration 41004, loss = 3.30721203\n",
      "Iteration 41005, loss = 2.96824973\n",
      "Iteration 41006, loss = 3.15139690\n",
      "Iteration 41007, loss = 2.79856693\n",
      "Iteration 41008, loss = 2.12668067\n",
      "Iteration 41009, loss = 2.57626679\n",
      "Iteration 41010, loss = 2.37459537\n",
      "Iteration 41011, loss = 2.47064711\n",
      "Iteration 41012, loss = 2.20980144\n",
      "Iteration 41013, loss = 2.06783048\n",
      "Iteration 41014, loss = 1.88301588\n",
      "Iteration 41015, loss = 2.08309811\n",
      "Iteration 41016, loss = 2.33465841\n",
      "Iteration 41017, loss = 2.89321274\n",
      "Iteration 41018, loss = 4.70526399\n",
      "Iteration 41019, loss = 4.73737337\n",
      "Iteration 41020, loss = 3.88801492\n",
      "Iteration 41021, loss = 3.63141999\n",
      "Iteration 41022, loss = 3.55150190\n",
      "Iteration 41023, loss = 3.65018252\n",
      "Iteration 41024, loss = 3.74479845\n",
      "Iteration 41025, loss = 3.56646576\n",
      "Iteration 41026, loss = 3.53673414\n",
      "Iteration 41027, loss = 3.83779252\n",
      "Iteration 41028, loss = 4.28944493\n",
      "Iteration 41029, loss = 4.83288803\n",
      "Iteration 41030, loss = 3.32975970\n",
      "Iteration 41031, loss = 3.05570813\n",
      "Iteration 41032, loss = 2.51501086\n",
      "Iteration 41033, loss = 2.64228434\n",
      "Iteration 41034, loss = 2.06385680\n",
      "Iteration 41035, loss = 1.78402150\n",
      "Iteration 41036, loss = 1.82201538\n",
      "Iteration 41037, loss = 1.78967525\n",
      "Iteration 41038, loss = 1.79721503\n",
      "Iteration 41039, loss = 1.90618600\n",
      "Iteration 41040, loss = 2.03205342\n",
      "Iteration 41041, loss = 2.56527358\n",
      "Iteration 41042, loss = 2.78511652\n",
      "Iteration 41043, loss = 3.00997724\n",
      "Iteration 41044, loss = 2.37270655\n",
      "Iteration 41045, loss = 2.16607001\n",
      "Iteration 41046, loss = 1.94012627\n",
      "Iteration 41047, loss = 2.07450407\n",
      "Iteration 41048, loss = 2.62904132\n",
      "Iteration 41049, loss = 2.59369928\n",
      "Iteration 41050, loss = 2.25120649\n",
      "Iteration 41051, loss = 2.32132928\n",
      "Iteration 41052, loss = 1.92306913\n",
      "Iteration 41053, loss = 2.12430680\n",
      "Iteration 41054, loss = 3.26596415\n",
      "Iteration 41055, loss = 2.89740057\n",
      "Iteration 41056, loss = 2.64996314\n",
      "Iteration 41057, loss = 2.68662012\n",
      "Iteration 41058, loss = 2.05330729\n",
      "Iteration 41059, loss = 3.00109326\n",
      "Iteration 41060, loss = 2.70040354\n",
      "Iteration 41061, loss = 2.82947525\n",
      "Iteration 41062, loss = 3.20240010\n",
      "Iteration 41063, loss = 2.61213047\n",
      "Iteration 41064, loss = 2.27061066\n",
      "Iteration 41065, loss = 1.85463998\n",
      "Iteration 41066, loss = 1.97227108\n",
      "Iteration 41067, loss = 1.87417479\n",
      "Iteration 41068, loss = 1.98144474\n",
      "Iteration 41069, loss = 2.09017987\n",
      "Iteration 41070, loss = 2.13365577\n",
      "Iteration 41071, loss = 2.39972698\n",
      "Iteration 41072, loss = 2.19807827\n",
      "Iteration 41073, loss = 2.32730562\n",
      "Iteration 41074, loss = 2.31089084\n",
      "Iteration 41075, loss = 3.28740374\n",
      "Iteration 41076, loss = 2.30912385\n",
      "Iteration 41077, loss = 2.95264706\n",
      "Iteration 41078, loss = 2.50402320\n",
      "Iteration 41079, loss = 3.03695570\n",
      "Iteration 41080, loss = 3.10494352\n",
      "Iteration 41081, loss = 2.61311652\n",
      "Iteration 41082, loss = 2.06809447\n",
      "Iteration 41083, loss = 1.98167918\n",
      "Iteration 41084, loss = 1.92580562\n",
      "Iteration 41085, loss = 1.85984473\n",
      "Iteration 41086, loss = 1.95204961\n",
      "Iteration 41087, loss = 2.85345126\n",
      "Iteration 41088, loss = 5.48215084\n",
      "Iteration 41089, loss = 8.05120520\n",
      "Iteration 41090, loss = 6.59117962\n",
      "Iteration 41091, loss = 3.63136870\n",
      "Iteration 41092, loss = 3.51303227\n",
      "Iteration 41093, loss = 2.75691704\n",
      "Iteration 41094, loss = 2.14987559\n",
      "Iteration 41095, loss = 2.12442410\n",
      "Iteration 41096, loss = 2.32981443\n",
      "Iteration 41097, loss = 2.31234518\n",
      "Iteration 41098, loss = 2.45056458\n",
      "Iteration 41099, loss = 2.27330425\n",
      "Iteration 41100, loss = 2.42378527\n",
      "Iteration 41101, loss = 2.26113206\n",
      "Iteration 41102, loss = 2.14769785\n",
      "Iteration 41103, loss = 2.38120094\n",
      "Iteration 41104, loss = 2.24922871\n",
      "Iteration 41105, loss = 2.02896951\n",
      "Iteration 41106, loss = 2.40396918\n",
      "Iteration 41107, loss = 2.24284338\n",
      "Iteration 41108, loss = 1.92107755\n",
      "Iteration 41109, loss = 1.87744325\n",
      "Iteration 41110, loss = 1.86748825\n",
      "Iteration 41111, loss = 1.95778682\n",
      "Iteration 41112, loss = 2.17649662\n",
      "Iteration 41113, loss = 2.04349333\n",
      "Iteration 41114, loss = 1.75932514\n",
      "Iteration 41115, loss = 2.23728735\n",
      "Iteration 41116, loss = 2.16629888\n",
      "Iteration 41117, loss = 1.71298663\n",
      "Iteration 41118, loss = 2.10935966\n",
      "Iteration 41119, loss = 3.00501451\n",
      "Iteration 41120, loss = 2.24280799\n",
      "Iteration 41121, loss = 2.24825824\n",
      "Iteration 41122, loss = 2.55309162\n",
      "Iteration 41123, loss = 2.38570855\n",
      "Iteration 41124, loss = 2.23987165\n",
      "Iteration 41125, loss = 2.09089640\n",
      "Iteration 41126, loss = 1.91285247\n",
      "Iteration 41127, loss = 1.76123028\n",
      "Iteration 41128, loss = 1.94226911\n",
      "Iteration 41129, loss = 1.85307325\n",
      "Iteration 41130, loss = 2.16884506\n",
      "Iteration 41131, loss = 2.08250105\n",
      "Iteration 41132, loss = 1.96009845\n",
      "Iteration 41133, loss = 1.92681745\n",
      "Iteration 41134, loss = 2.19242476\n",
      "Iteration 41135, loss = 2.07796310\n",
      "Iteration 41136, loss = 2.40628680\n",
      "Iteration 41137, loss = 2.94322686\n",
      "Iteration 41138, loss = 3.08364438\n",
      "Iteration 41139, loss = 2.54401481\n",
      "Iteration 41140, loss = 2.36019711\n",
      "Iteration 41141, loss = 2.42324272\n",
      "Iteration 41142, loss = 2.69434035\n",
      "Iteration 41143, loss = 3.40523999\n",
      "Iteration 41144, loss = 3.13122854\n",
      "Iteration 41145, loss = 3.02518495\n",
      "Iteration 41146, loss = 3.46665432\n",
      "Iteration 41147, loss = 3.64178199\n",
      "Iteration 41148, loss = 3.00039770\n",
      "Iteration 41149, loss = 2.49631917\n",
      "Iteration 41150, loss = 2.09572436\n",
      "Iteration 41151, loss = 2.06154953\n",
      "Iteration 41152, loss = 1.88896114\n",
      "Iteration 41153, loss = 2.18140980\n",
      "Iteration 41154, loss = 2.33514730\n",
      "Iteration 41155, loss = 2.61697309\n",
      "Iteration 41156, loss = 2.72635790\n",
      "Iteration 41157, loss = 3.12883310\n",
      "Iteration 41158, loss = 2.87474424\n",
      "Iteration 41159, loss = 2.65414757\n",
      "Iteration 41160, loss = 2.14116200\n",
      "Iteration 41161, loss = 2.09109837\n",
      "Iteration 41162, loss = 2.71045868\n",
      "Iteration 41163, loss = 3.26949625\n",
      "Iteration 41164, loss = 2.61133421\n",
      "Iteration 41165, loss = 2.54494546\n",
      "Iteration 41166, loss = 3.17903947\n",
      "Iteration 41167, loss = 2.51970102\n",
      "Iteration 41168, loss = 2.23923951\n",
      "Iteration 41169, loss = 2.31439362\n",
      "Iteration 41170, loss = 2.57584252\n",
      "Iteration 41171, loss = 2.67653533\n",
      "Iteration 41172, loss = 2.39724435\n",
      "Iteration 41173, loss = 2.03759359\n",
      "Iteration 41174, loss = 2.68483496\n",
      "Iteration 41175, loss = 2.02927529\n",
      "Iteration 41176, loss = 2.43364167\n",
      "Iteration 41177, loss = 2.48250668\n",
      "Iteration 41178, loss = 2.34743159\n",
      "Iteration 41179, loss = 2.53120894\n",
      "Iteration 41180, loss = 3.04581988\n",
      "Iteration 41181, loss = 2.88672519\n",
      "Iteration 41182, loss = 3.42891983\n",
      "Iteration 41183, loss = 2.75863321\n",
      "Iteration 41184, loss = 2.11401710\n",
      "Iteration 41185, loss = 2.12669891\n",
      "Iteration 41186, loss = 1.86869188\n",
      "Iteration 41187, loss = 2.24779402\n",
      "Iteration 41188, loss = 2.07157548\n",
      "Iteration 41189, loss = 1.97813992\n",
      "Iteration 41190, loss = 2.53462140\n",
      "Iteration 41191, loss = 2.23993216\n",
      "Iteration 41192, loss = 2.24770741\n",
      "Iteration 41193, loss = 2.39895583\n",
      "Iteration 41194, loss = 2.25708257\n",
      "Iteration 41195, loss = 2.16194711\n",
      "Iteration 41196, loss = 1.93932825\n",
      "Iteration 41197, loss = 1.92395113\n",
      "Iteration 41198, loss = 1.94829792\n",
      "Iteration 41199, loss = 1.76438560\n",
      "Iteration 41200, loss = 1.84775678\n",
      "Iteration 41201, loss = 2.15806672\n",
      "Iteration 41202, loss = 3.62528227\n",
      "Iteration 41203, loss = 3.21092274\n",
      "Iteration 41204, loss = 2.57394370\n",
      "Iteration 41205, loss = 2.91687437\n",
      "Iteration 41206, loss = 2.58640423\n",
      "Iteration 41207, loss = 2.30555142\n",
      "Iteration 41208, loss = 2.06540587\n",
      "Iteration 41209, loss = 2.14296104\n",
      "Iteration 41210, loss = 1.98105489\n",
      "Iteration 41211, loss = 1.88076486\n",
      "Iteration 41212, loss = 2.20175436\n",
      "Iteration 41213, loss = 2.17458424\n",
      "Iteration 41214, loss = 2.39794094\n",
      "Iteration 41215, loss = 2.29815404\n",
      "Iteration 41216, loss = 2.15054260\n",
      "Iteration 41217, loss = 1.93837880\n",
      "Iteration 41218, loss = 2.12341836\n",
      "Iteration 41219, loss = 1.86900486\n",
      "Iteration 41220, loss = 2.08841226\n",
      "Iteration 41221, loss = 2.06244081\n",
      "Iteration 41222, loss = 1.90646243\n",
      "Iteration 41223, loss = 1.92262362\n",
      "Iteration 41224, loss = 2.04871435\n",
      "Iteration 41225, loss = 2.13263837\n",
      "Iteration 41226, loss = 1.84799347\n",
      "Iteration 41227, loss = 1.70828373\n",
      "Iteration 41228, loss = 1.77143181\n",
      "Iteration 41229, loss = 1.83821154\n",
      "Iteration 41230, loss = 1.69853231\n",
      "Iteration 41231, loss = 1.79044974\n",
      "Iteration 41232, loss = 1.87905945\n",
      "Iteration 41233, loss = 1.78798462\n",
      "Iteration 41234, loss = 1.94033730\n",
      "Iteration 41235, loss = 2.17388387\n",
      "Iteration 41236, loss = 2.28484220\n",
      "Iteration 41237, loss = 2.34282135\n",
      "Iteration 41238, loss = 2.49103516\n",
      "Iteration 41239, loss = 1.99278245\n",
      "Iteration 41240, loss = 1.93625980\n",
      "Iteration 41241, loss = 2.00823704\n",
      "Iteration 41242, loss = 2.63665350\n",
      "Iteration 41243, loss = 2.21148764\n",
      "Iteration 41244, loss = 2.29110108\n",
      "Iteration 41245, loss = 1.92206893\n",
      "Iteration 41246, loss = 1.74227480\n",
      "Iteration 41247, loss = 1.85035447\n",
      "Iteration 41248, loss = 1.82146551\n",
      "Iteration 41249, loss = 1.86786900\n",
      "Iteration 41250, loss = 1.84191194\n",
      "Iteration 41251, loss = 1.72506292\n",
      "Iteration 41252, loss = 1.85036391\n",
      "Iteration 41253, loss = 1.85891016\n",
      "Iteration 41254, loss = 2.02062615\n",
      "Iteration 41255, loss = 1.72925167\n",
      "Iteration 41256, loss = 2.04806497\n",
      "Iteration 41257, loss = 2.06438524\n",
      "Iteration 41258, loss = 2.31358517\n",
      "Iteration 41259, loss = 2.18694735\n",
      "Iteration 41260, loss = 1.77530070\n",
      "Iteration 41261, loss = 1.95586001\n",
      "Iteration 41262, loss = 2.23303046\n",
      "Iteration 41263, loss = 2.50656304\n",
      "Iteration 41264, loss = 2.65255130\n",
      "Iteration 41265, loss = 2.84229447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 41266, loss = 2.57398263\n",
      "Iteration 41267, loss = 2.68107852\n",
      "Iteration 41268, loss = 2.56722002\n",
      "Iteration 41269, loss = 2.39307698\n",
      "Iteration 41270, loss = 2.23465925\n",
      "Iteration 41271, loss = 2.07607325\n",
      "Iteration 41272, loss = 1.86911550\n",
      "Iteration 41273, loss = 1.99505468\n",
      "Iteration 41274, loss = 1.86315264\n",
      "Iteration 41275, loss = 1.98760031\n",
      "Iteration 41276, loss = 1.76805392\n",
      "Iteration 41277, loss = 1.78944067\n",
      "Iteration 41278, loss = 2.57731388\n",
      "Iteration 41279, loss = 2.64352337\n",
      "Iteration 41280, loss = 2.41464445\n",
      "Iteration 41281, loss = 2.75785979\n",
      "Iteration 41282, loss = 2.76781864\n",
      "Iteration 41283, loss = 4.23590549\n",
      "Iteration 41284, loss = 3.17564324\n",
      "Iteration 41285, loss = 2.73692053\n",
      "Iteration 41286, loss = 2.48367297\n",
      "Iteration 41287, loss = 2.18465963\n",
      "Iteration 41288, loss = 2.77134988\n",
      "Iteration 41289, loss = 2.85790633\n",
      "Iteration 41290, loss = 2.29545440\n",
      "Iteration 41291, loss = 2.22687484\n",
      "Iteration 41292, loss = 2.62762962\n",
      "Iteration 41293, loss = 2.86143747\n",
      "Iteration 41294, loss = 2.41681843\n",
      "Iteration 41295, loss = 3.63722093\n",
      "Iteration 41296, loss = 3.11201238\n",
      "Iteration 41297, loss = 4.39928497\n",
      "Iteration 41298, loss = 3.09381170\n",
      "Iteration 41299, loss = 4.83762254\n",
      "Iteration 41300, loss = 3.43574582\n",
      "Iteration 41301, loss = 3.41543630\n",
      "Iteration 41302, loss = 2.97425643\n",
      "Iteration 41303, loss = 4.43880913\n",
      "Iteration 41304, loss = 2.78056452\n",
      "Iteration 41305, loss = 6.47620669\n",
      "Iteration 41306, loss = 5.24778010\n",
      "Iteration 41307, loss = 6.25905366\n",
      "Iteration 41308, loss = 4.90650861\n",
      "Iteration 41309, loss = 5.12969911\n",
      "Iteration 41310, loss = 3.62875572\n",
      "Iteration 41311, loss = 3.81224892\n",
      "Iteration 41312, loss = 2.98970086\n",
      "Iteration 41313, loss = 2.33965685\n",
      "Iteration 41314, loss = 2.17609573\n",
      "Iteration 41315, loss = 2.13874209\n",
      "Iteration 41316, loss = 2.32705473\n",
      "Iteration 41317, loss = 1.95507729\n",
      "Iteration 41318, loss = 2.28058173\n",
      "Iteration 41319, loss = 2.79899374\n",
      "Iteration 41320, loss = 3.09405563\n",
      "Iteration 41321, loss = 2.76498418\n",
      "Iteration 41322, loss = 2.10359594\n",
      "Iteration 41323, loss = 2.10930433\n",
      "Iteration 41324, loss = 1.88679859\n",
      "Iteration 41325, loss = 1.91792734\n",
      "Iteration 41326, loss = 2.23568424\n",
      "Iteration 41327, loss = 2.85780145\n",
      "Iteration 41328, loss = 1.91121624\n",
      "Iteration 41329, loss = 1.77429887\n",
      "Iteration 41330, loss = 1.88252685\n",
      "Iteration 41331, loss = 2.05610118\n",
      "Iteration 41332, loss = 2.86719356\n",
      "Iteration 41333, loss = 3.03902424\n",
      "Iteration 41334, loss = 2.86827965\n",
      "Iteration 41335, loss = 2.34541469\n",
      "Iteration 41336, loss = 2.10483843\n",
      "Iteration 41337, loss = 2.21831261\n",
      "Iteration 41338, loss = 2.09289635\n",
      "Iteration 41339, loss = 2.16479289\n",
      "Iteration 41340, loss = 1.81452787\n",
      "Iteration 41341, loss = 1.87518122\n",
      "Iteration 41342, loss = 1.91707328\n",
      "Iteration 41343, loss = 1.93700342\n",
      "Iteration 41344, loss = 2.17809012\n",
      "Iteration 41345, loss = 2.02807510\n",
      "Iteration 41346, loss = 1.81623343\n",
      "Iteration 41347, loss = 1.76803068\n",
      "Iteration 41348, loss = 1.95960921\n",
      "Iteration 41349, loss = 2.08866498\n",
      "Iteration 41350, loss = 1.76576079\n",
      "Iteration 41351, loss = 1.87618415\n",
      "Iteration 41352, loss = 1.89817103\n",
      "Iteration 41353, loss = 2.01548271\n",
      "Iteration 41354, loss = 2.21375975\n",
      "Iteration 41355, loss = 2.03132579\n",
      "Iteration 41356, loss = 1.79222067\n",
      "Iteration 41357, loss = 1.85218200\n",
      "Iteration 41358, loss = 1.93599415\n",
      "Iteration 41359, loss = 2.08759803\n",
      "Iteration 41360, loss = 2.04692670\n",
      "Iteration 41361, loss = 2.02069310\n",
      "Iteration 41362, loss = 2.42800868\n",
      "Iteration 41363, loss = 2.70646265\n",
      "Iteration 41364, loss = 2.81865715\n",
      "Iteration 41365, loss = 2.85710037\n",
      "Iteration 41366, loss = 2.36525346\n",
      "Iteration 41367, loss = 2.53789340\n",
      "Iteration 41368, loss = 3.01191700\n",
      "Iteration 41369, loss = 2.36180919\n",
      "Iteration 41370, loss = 3.10080346\n",
      "Iteration 41371, loss = 2.46525013\n",
      "Iteration 41372, loss = 2.12299537\n",
      "Iteration 41373, loss = 1.88993300\n",
      "Iteration 41374, loss = 1.81526612\n",
      "Iteration 41375, loss = 1.83385046\n",
      "Iteration 41376, loss = 1.97596124\n",
      "Iteration 41377, loss = 1.84592530\n",
      "Iteration 41378, loss = 2.32853019\n",
      "Iteration 41379, loss = 2.05971411\n",
      "Iteration 41380, loss = 2.83068527\n",
      "Iteration 41381, loss = 2.47198127\n",
      "Iteration 41382, loss = 2.54487021\n",
      "Iteration 41383, loss = 2.14884106\n",
      "Iteration 41384, loss = 2.41817252\n",
      "Iteration 41385, loss = 2.39900230\n",
      "Iteration 41386, loss = 2.53582356\n",
      "Iteration 41387, loss = 3.41692996\n",
      "Iteration 41388, loss = 4.31479853\n",
      "Iteration 41389, loss = 2.86827354\n",
      "Iteration 41390, loss = 2.65175482\n",
      "Iteration 41391, loss = 1.96441480\n",
      "Iteration 41392, loss = 1.75650842\n",
      "Iteration 41393, loss = 1.78619353\n",
      "Iteration 41394, loss = 1.82359652\n",
      "Iteration 41395, loss = 1.88731799\n",
      "Iteration 41396, loss = 1.76861983\n",
      "Iteration 41397, loss = 1.97960753\n",
      "Iteration 41398, loss = 2.10189649\n",
      "Iteration 41399, loss = 2.61376930\n",
      "Iteration 41400, loss = 3.11230248\n",
      "Iteration 41401, loss = 3.06751634\n",
      "Iteration 41402, loss = 2.15116732\n",
      "Iteration 41403, loss = 2.58469630\n",
      "Iteration 41404, loss = 2.18627836\n",
      "Iteration 41405, loss = 2.58716674\n",
      "Iteration 41406, loss = 2.50279308\n",
      "Iteration 41407, loss = 2.56190167\n",
      "Iteration 41408, loss = 2.14908873\n",
      "Iteration 41409, loss = 1.94406332\n",
      "Iteration 41410, loss = 2.13455536\n",
      "Iteration 41411, loss = 1.76085954\n",
      "Iteration 41412, loss = 1.94217343\n",
      "Iteration 41413, loss = 2.33564280\n",
      "Iteration 41414, loss = 2.13764924\n",
      "Iteration 41415, loss = 1.82091026\n",
      "Iteration 41416, loss = 2.08423683\n",
      "Iteration 41417, loss = 2.59203822\n",
      "Iteration 41418, loss = 3.00823611\n",
      "Iteration 41419, loss = 2.07695664\n",
      "Iteration 41420, loss = 2.17956672\n",
      "Iteration 41421, loss = 2.14336380\n",
      "Iteration 41422, loss = 1.75884290\n",
      "Iteration 41423, loss = 2.41974972\n",
      "Iteration 41424, loss = 2.15167764\n",
      "Iteration 41425, loss = 2.38518980\n",
      "Iteration 41426, loss = 2.04051631\n",
      "Iteration 41427, loss = 2.00181619\n",
      "Iteration 41428, loss = 2.23565830\n",
      "Iteration 41429, loss = 3.41791681\n",
      "Iteration 41430, loss = 2.57770450\n",
      "Iteration 41431, loss = 2.70681539\n",
      "Iteration 41432, loss = 2.05208813\n",
      "Iteration 41433, loss = 2.90098634\n",
      "Iteration 41434, loss = 2.75080763\n",
      "Iteration 41435, loss = 2.32367806\n",
      "Iteration 41436, loss = 1.96317542\n",
      "Iteration 41437, loss = 1.75152987\n",
      "Iteration 41438, loss = 2.46412808\n",
      "Iteration 41439, loss = 2.09429114\n",
      "Iteration 41440, loss = 2.02093245\n",
      "Iteration 41441, loss = 1.82717678\n",
      "Iteration 41442, loss = 1.84342038\n",
      "Iteration 41443, loss = 2.17291864\n",
      "Iteration 41444, loss = 2.45396739\n",
      "Iteration 41445, loss = 2.45170057\n",
      "Iteration 41446, loss = 2.39221027\n",
      "Iteration 41447, loss = 2.11303686\n",
      "Iteration 41448, loss = 2.03846940\n",
      "Iteration 41449, loss = 2.02198544\n",
      "Iteration 41450, loss = 2.22224224\n",
      "Iteration 41451, loss = 2.18380832\n",
      "Iteration 41452, loss = 2.65620409\n",
      "Iteration 41453, loss = 2.26936965\n",
      "Iteration 41454, loss = 2.10331723\n",
      "Iteration 41455, loss = 1.85636779\n",
      "Iteration 41456, loss = 1.76902289\n",
      "Iteration 41457, loss = 1.80412664\n",
      "Iteration 41458, loss = 1.80262554\n",
      "Iteration 41459, loss = 1.97302544\n",
      "Iteration 41460, loss = 1.98545905\n",
      "Iteration 41461, loss = 2.37207461\n",
      "Iteration 41462, loss = 2.06778372\n",
      "Iteration 41463, loss = 2.76593644\n",
      "Iteration 41464, loss = 2.83402706\n",
      "Iteration 41465, loss = 2.22947131\n",
      "Iteration 41466, loss = 2.24065723\n",
      "Iteration 41467, loss = 2.30958568\n",
      "Iteration 41468, loss = 2.28290688\n",
      "Iteration 41469, loss = 2.36041778\n",
      "Iteration 41470, loss = 2.54019796\n",
      "Iteration 41471, loss = 2.71329428\n",
      "Iteration 41472, loss = 2.24098746\n",
      "Iteration 41473, loss = 3.04369997\n",
      "Iteration 41474, loss = 2.45175017\n",
      "Iteration 41475, loss = 2.09483039\n",
      "Iteration 41476, loss = 2.06106084\n",
      "Iteration 41477, loss = 2.45943539\n",
      "Iteration 41478, loss = 2.09461989\n",
      "Iteration 41479, loss = 1.91545950\n",
      "Iteration 41480, loss = 1.89697338\n",
      "Iteration 41481, loss = 2.14742357\n",
      "Iteration 41482, loss = 2.78892607\n",
      "Iteration 41483, loss = 2.21044108\n",
      "Iteration 41484, loss = 1.98538711\n",
      "Iteration 41485, loss = 2.09076964\n",
      "Iteration 41486, loss = 2.81319670\n",
      "Iteration 41487, loss = 2.60309230\n",
      "Iteration 41488, loss = 1.97974135\n",
      "Iteration 41489, loss = 2.37147457\n",
      "Iteration 41490, loss = 2.11626661\n",
      "Iteration 41491, loss = 1.97485050\n",
      "Iteration 41492, loss = 1.86913730\n",
      "Iteration 41493, loss = 2.05567075\n",
      "Iteration 41494, loss = 1.68713511\n",
      "Iteration 41495, loss = 1.81242570\n",
      "Iteration 41496, loss = 1.86554850\n",
      "Iteration 41497, loss = 2.12639466\n",
      "Iteration 41498, loss = 1.96882835\n",
      "Iteration 41499, loss = 1.95488254\n",
      "Iteration 41500, loss = 1.85272652\n",
      "Iteration 41501, loss = 1.83218539\n",
      "Iteration 41502, loss = 1.79341986\n",
      "Iteration 41503, loss = 1.92570786\n",
      "Iteration 41504, loss = 2.02637013\n",
      "Iteration 41505, loss = 1.81746336\n",
      "Iteration 41506, loss = 1.75184896\n",
      "Iteration 41507, loss = 2.23584162\n",
      "Iteration 41508, loss = 3.05048353\n",
      "Iteration 41509, loss = 2.91048340\n",
      "Iteration 41510, loss = 2.80185323\n",
      "Iteration 41511, loss = 3.54337475\n",
      "Iteration 41512, loss = 3.02209535\n",
      "Iteration 41513, loss = 3.07647404\n",
      "Iteration 41514, loss = 3.06241579\n",
      "Iteration 41515, loss = 2.89851162\n",
      "Iteration 41516, loss = 3.20079312\n",
      "Iteration 41517, loss = 3.20278432\n",
      "Iteration 41518, loss = 3.17688249\n",
      "Iteration 41519, loss = 3.14159085\n",
      "Iteration 41520, loss = 3.08182562\n",
      "Iteration 41521, loss = 2.40276737\n",
      "Iteration 41522, loss = 2.86212700\n",
      "Iteration 41523, loss = 2.73075108\n",
      "Iteration 41524, loss = 2.35983037\n",
      "Iteration 41525, loss = 2.99854000\n",
      "Iteration 41526, loss = 4.07508401\n",
      "Iteration 41527, loss = 3.51928376\n",
      "Iteration 41528, loss = 3.35061518\n",
      "Iteration 41529, loss = 3.91474564\n",
      "Iteration 41530, loss = 3.40097825\n",
      "Iteration 41531, loss = 7.41353387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 41532, loss = 5.27987865\n",
      "Iteration 41533, loss = 4.95915639\n",
      "Iteration 41534, loss = 4.43490826\n",
      "Iteration 41535, loss = 3.84980252\n",
      "Iteration 41536, loss = 2.62606125\n",
      "Iteration 41537, loss = 3.27439907\n",
      "Iteration 41538, loss = 2.23490111\n",
      "Iteration 41539, loss = 1.85737046\n",
      "Iteration 41540, loss = 2.09248731\n",
      "Iteration 41541, loss = 1.94515478\n",
      "Iteration 41542, loss = 2.21550691\n",
      "Iteration 41543, loss = 1.88187785\n",
      "Iteration 41544, loss = 1.84741568\n",
      "Iteration 41545, loss = 1.70966818\n",
      "Iteration 41546, loss = 1.65010196\n",
      "Iteration 41547, loss = 1.73912335\n",
      "Iteration 41548, loss = 1.72762649\n",
      "Iteration 41549, loss = 1.97657796\n",
      "Iteration 41550, loss = 1.96448727\n",
      "Iteration 41551, loss = 1.81146997\n",
      "Iteration 41552, loss = 1.78157347\n",
      "Iteration 41553, loss = 2.14884877\n",
      "Iteration 41554, loss = 2.82919349\n",
      "Iteration 41555, loss = 2.49927249\n",
      "Iteration 41556, loss = 2.10474257\n",
      "Iteration 41557, loss = 2.28120808\n",
      "Iteration 41558, loss = 1.72847957\n",
      "Iteration 41559, loss = 2.07047295\n",
      "Iteration 41560, loss = 2.16368655\n",
      "Iteration 41561, loss = 2.47372643\n",
      "Iteration 41562, loss = 2.33458175\n",
      "Iteration 41563, loss = 2.09312124\n",
      "Iteration 41564, loss = 2.08219517\n",
      "Iteration 41565, loss = 2.68513369\n",
      "Iteration 41566, loss = 2.53392385\n",
      "Iteration 41567, loss = 2.62592574\n",
      "Iteration 41568, loss = 2.45323760\n",
      "Iteration 41569, loss = 2.66292255\n",
      "Iteration 41570, loss = 2.15629209\n",
      "Iteration 41571, loss = 1.94945211\n",
      "Iteration 41572, loss = 1.90448601\n",
      "Iteration 41573, loss = 2.11376952\n",
      "Iteration 41574, loss = 2.87121669\n",
      "Iteration 41575, loss = 2.29708623\n",
      "Iteration 41576, loss = 2.37587276\n",
      "Iteration 41577, loss = 2.28586528\n",
      "Iteration 41578, loss = 2.64955868\n",
      "Iteration 41579, loss = 2.20644319\n",
      "Iteration 41580, loss = 2.18938662\n",
      "Iteration 41581, loss = 2.05208900\n",
      "Iteration 41582, loss = 1.89302692\n",
      "Iteration 41583, loss = 2.12485862\n",
      "Iteration 41584, loss = 2.12397569\n",
      "Iteration 41585, loss = 1.91082523\n",
      "Iteration 41586, loss = 2.28837369\n",
      "Iteration 41587, loss = 2.32546042\n",
      "Iteration 41588, loss = 2.10642512\n",
      "Iteration 41589, loss = 2.11421415\n",
      "Iteration 41590, loss = 3.24208987\n",
      "Iteration 41591, loss = 3.40285696\n",
      "Iteration 41592, loss = 3.00704889\n",
      "Iteration 41593, loss = 2.90803315\n",
      "Iteration 41594, loss = 3.10300915\n",
      "Iteration 41595, loss = 2.67290717\n",
      "Iteration 41596, loss = 2.49050383\n",
      "Iteration 41597, loss = 2.24525964\n",
      "Iteration 41598, loss = 1.96622300\n",
      "Iteration 41599, loss = 2.16011552\n",
      "Iteration 41600, loss = 2.46580335\n",
      "Iteration 41601, loss = 2.39124248\n",
      "Iteration 41602, loss = 2.58833621\n",
      "Iteration 41603, loss = 2.94599584\n",
      "Iteration 41604, loss = 2.70033438\n",
      "Iteration 41605, loss = 3.01770971\n",
      "Iteration 41606, loss = 1.99012069\n",
      "Iteration 41607, loss = 2.03942026\n",
      "Iteration 41608, loss = 2.03046654\n",
      "Iteration 41609, loss = 1.90670691\n",
      "Iteration 41610, loss = 2.45345338\n",
      "Iteration 41611, loss = 2.81663448\n",
      "Iteration 41612, loss = 2.97507183\n",
      "Iteration 41613, loss = 2.55198691\n",
      "Iteration 41614, loss = 2.56528995\n",
      "Iteration 41615, loss = 2.21721632\n",
      "Iteration 41616, loss = 1.85582110\n",
      "Iteration 41617, loss = 2.15148094\n",
      "Iteration 41618, loss = 2.10387338\n",
      "Iteration 41619, loss = 2.14143218\n",
      "Iteration 41620, loss = 2.23586174\n",
      "Iteration 41621, loss = 2.24492725\n",
      "Iteration 41622, loss = 2.43226496\n",
      "Iteration 41623, loss = 1.92954607\n",
      "Iteration 41624, loss = 2.28968661\n",
      "Iteration 41625, loss = 2.08833314\n",
      "Iteration 41626, loss = 1.88125023\n",
      "Iteration 41627, loss = 1.76746333\n",
      "Iteration 41628, loss = 1.91499286\n",
      "Iteration 41629, loss = 1.99284527\n",
      "Iteration 41630, loss = 2.03072494\n",
      "Iteration 41631, loss = 1.93284469\n",
      "Iteration 41632, loss = 2.18703502\n",
      "Iteration 41633, loss = 3.96348957\n",
      "Iteration 41634, loss = 3.08888599\n",
      "Iteration 41635, loss = 2.69013051\n",
      "Iteration 41636, loss = 2.88373442\n",
      "Iteration 41637, loss = 3.03285051\n",
      "Iteration 41638, loss = 3.09605579\n",
      "Iteration 41639, loss = 3.03844667\n",
      "Iteration 41640, loss = 2.67528002\n",
      "Iteration 41641, loss = 3.23212660\n",
      "Iteration 41642, loss = 4.24441183\n",
      "Iteration 41643, loss = 4.65587698\n",
      "Iteration 41644, loss = 4.30185185\n",
      "Iteration 41645, loss = 2.28089397\n",
      "Iteration 41646, loss = 2.31329665\n",
      "Iteration 41647, loss = 1.97854204\n",
      "Iteration 41648, loss = 1.94278530\n",
      "Iteration 41649, loss = 2.24655914\n",
      "Iteration 41650, loss = 2.22481675\n",
      "Iteration 41651, loss = 2.25840724\n",
      "Iteration 41652, loss = 2.04649116\n",
      "Iteration 41653, loss = 1.77210866\n",
      "Iteration 41654, loss = 1.78298581\n",
      "Iteration 41655, loss = 1.66705273\n",
      "Iteration 41656, loss = 1.93406746\n",
      "Iteration 41657, loss = 2.08704066\n",
      "Iteration 41658, loss = 1.91140089\n",
      "Iteration 41659, loss = 2.06922367\n",
      "Iteration 41660, loss = 2.00971706\n",
      "Iteration 41661, loss = 2.32253076\n",
      "Iteration 41662, loss = 1.94424454\n",
      "Iteration 41663, loss = 1.81087554\n",
      "Iteration 41664, loss = 1.96250888\n",
      "Iteration 41665, loss = 1.82354929\n",
      "Iteration 41666, loss = 1.99240270\n",
      "Iteration 41667, loss = 2.31943235\n",
      "Iteration 41668, loss = 3.59770377\n",
      "Iteration 41669, loss = 3.24703466\n",
      "Iteration 41670, loss = 2.94212141\n",
      "Iteration 41671, loss = 2.21969296\n",
      "Iteration 41672, loss = 2.35952463\n",
      "Iteration 41673, loss = 2.42313507\n",
      "Iteration 41674, loss = 1.78356943\n",
      "Iteration 41675, loss = 2.53097681\n",
      "Iteration 41676, loss = 2.47940571\n",
      "Iteration 41677, loss = 2.35677028\n",
      "Iteration 41678, loss = 3.08171322\n",
      "Iteration 41679, loss = 3.11974775\n",
      "Iteration 41680, loss = 3.37763691\n",
      "Iteration 41681, loss = 2.31316911\n",
      "Iteration 41682, loss = 1.89825553\n",
      "Iteration 41683, loss = 1.93609613\n",
      "Iteration 41684, loss = 1.99745131\n",
      "Iteration 41685, loss = 2.00834739\n",
      "Iteration 41686, loss = 2.00990033\n",
      "Iteration 41687, loss = 1.81470155\n",
      "Iteration 41688, loss = 1.99960129\n",
      "Iteration 41689, loss = 1.94645420\n",
      "Iteration 41690, loss = 2.08910365\n",
      "Iteration 41691, loss = 2.07833696\n",
      "Iteration 41692, loss = 2.05157727\n",
      "Iteration 41693, loss = 2.14348870\n",
      "Iteration 41694, loss = 2.23235281\n",
      "Iteration 41695, loss = 2.29803377\n",
      "Iteration 41696, loss = 2.31559572\n",
      "Iteration 41697, loss = 2.49875984\n",
      "Iteration 41698, loss = 2.15148487\n",
      "Iteration 41699, loss = 2.10471707\n",
      "Iteration 41700, loss = 1.75205496\n",
      "Iteration 41701, loss = 2.21744002\n",
      "Iteration 41702, loss = 1.65876329\n",
      "Iteration 41703, loss = 1.83394993\n",
      "Iteration 41704, loss = 1.79780091\n",
      "Iteration 41705, loss = 1.64328337\n",
      "Iteration 41706, loss = 1.80694707\n",
      "Iteration 41707, loss = 1.96456676\n",
      "Iteration 41708, loss = 1.93048128\n",
      "Iteration 41709, loss = 2.02317037\n",
      "Iteration 41710, loss = 1.97394934\n",
      "Iteration 41711, loss = 1.89061829\n",
      "Iteration 41712, loss = 2.00348663\n",
      "Iteration 41713, loss = 1.95812185\n",
      "Iteration 41714, loss = 2.21734379\n",
      "Iteration 41715, loss = 2.08671478\n",
      "Iteration 41716, loss = 1.94101365\n",
      "Iteration 41717, loss = 1.97020179\n",
      "Iteration 41718, loss = 1.90704525\n",
      "Iteration 41719, loss = 1.90371145\n",
      "Iteration 41720, loss = 1.80399954\n",
      "Iteration 41721, loss = 1.74353810\n",
      "Iteration 41722, loss = 2.06944473\n",
      "Iteration 41723, loss = 1.91820506\n",
      "Iteration 41724, loss = 1.90336106\n",
      "Iteration 41725, loss = 1.84781114\n",
      "Iteration 41726, loss = 2.25264236\n",
      "Iteration 41727, loss = 3.22446557\n",
      "Iteration 41728, loss = 3.21867680\n",
      "Iteration 41729, loss = 3.55295631\n",
      "Iteration 41730, loss = 2.84373130\n",
      "Iteration 41731, loss = 2.90718726\n",
      "Iteration 41732, loss = 2.38077116\n",
      "Iteration 41733, loss = 2.00125546\n",
      "Iteration 41734, loss = 2.00575498\n",
      "Iteration 41735, loss = 1.76192971\n",
      "Iteration 41736, loss = 1.88721022\n",
      "Iteration 41737, loss = 2.15077057\n",
      "Iteration 41738, loss = 1.94841446\n",
      "Iteration 41739, loss = 1.99202758\n",
      "Iteration 41740, loss = 1.74676437\n",
      "Iteration 41741, loss = 1.81184698\n",
      "Iteration 41742, loss = 1.78552049\n",
      "Iteration 41743, loss = 1.69660009\n",
      "Iteration 41744, loss = 2.04869271\n",
      "Iteration 41745, loss = 2.57475634\n",
      "Iteration 41746, loss = 2.35335681\n",
      "Iteration 41747, loss = 2.00058413\n",
      "Iteration 41748, loss = 1.94755279\n",
      "Iteration 41749, loss = 2.11758510\n",
      "Iteration 41750, loss = 1.85043617\n",
      "Iteration 41751, loss = 1.99629391\n",
      "Iteration 41752, loss = 1.92976392\n",
      "Iteration 41753, loss = 2.00503072\n",
      "Iteration 41754, loss = 2.29287039\n",
      "Iteration 41755, loss = 2.60889321\n",
      "Iteration 41756, loss = 3.04488765\n",
      "Iteration 41757, loss = 2.62094711\n",
      "Iteration 41758, loss = 2.97755376\n",
      "Iteration 41759, loss = 2.77528555\n",
      "Iteration 41760, loss = 3.44211224\n",
      "Iteration 41761, loss = 3.25365849\n",
      "Iteration 41762, loss = 2.56677798\n",
      "Iteration 41763, loss = 2.11501306\n",
      "Iteration 41764, loss = 2.79829698\n",
      "Iteration 41765, loss = 3.25212619\n",
      "Iteration 41766, loss = 2.87210585\n",
      "Iteration 41767, loss = 3.02452024\n",
      "Iteration 41768, loss = 2.42164085\n",
      "Iteration 41769, loss = 2.70530849\n",
      "Iteration 41770, loss = 1.96450734\n",
      "Iteration 41771, loss = 1.80026372\n",
      "Iteration 41772, loss = 1.82335823\n",
      "Iteration 41773, loss = 1.77004044\n",
      "Iteration 41774, loss = 1.91689321\n",
      "Iteration 41775, loss = 1.93459828\n",
      "Iteration 41776, loss = 1.98052332\n",
      "Iteration 41777, loss = 2.10568406\n",
      "Iteration 41778, loss = 2.48149927\n",
      "Iteration 41779, loss = 2.27127014\n",
      "Iteration 41780, loss = 2.01816446\n",
      "Iteration 41781, loss = 2.15647651\n",
      "Iteration 41782, loss = 1.89084343\n",
      "Iteration 41783, loss = 1.84803928\n",
      "Iteration 41784, loss = 1.72840584\n",
      "Iteration 41785, loss = 1.86448051\n",
      "Iteration 41786, loss = 1.90636935\n",
      "Iteration 41787, loss = 2.04012662\n",
      "Iteration 41788, loss = 1.89290766\n",
      "Iteration 41789, loss = 2.33613566\n",
      "Iteration 41790, loss = 2.48354435\n",
      "Iteration 41791, loss = 1.89322729\n",
      "Iteration 41792, loss = 1.94143391\n",
      "Iteration 41793, loss = 3.19480246\n",
      "Iteration 41794, loss = 2.05573670\n",
      "Iteration 41795, loss = 2.33081711\n",
      "Iteration 41796, loss = 2.28547991\n",
      "Iteration 41797, loss = 2.10056110\n",
      "Iteration 41798, loss = 2.05617617\n",
      "Iteration 41799, loss = 2.34144313\n",
      "Iteration 41800, loss = 2.32846339\n",
      "Iteration 41801, loss = 2.02297459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 41802, loss = 1.75292285\n",
      "Iteration 41803, loss = 1.87061373\n",
      "Iteration 41804, loss = 2.61629465\n",
      "Iteration 41805, loss = 2.50902189\n",
      "Iteration 41806, loss = 1.86844105\n",
      "Iteration 41807, loss = 2.42643996\n",
      "Iteration 41808, loss = 2.02119000\n",
      "Iteration 41809, loss = 2.40975014\n",
      "Iteration 41810, loss = 2.46935782\n",
      "Iteration 41811, loss = 2.21788905\n",
      "Iteration 41812, loss = 2.05025588\n",
      "Iteration 41813, loss = 2.03415267\n",
      "Iteration 41814, loss = 1.92012203\n",
      "Iteration 41815, loss = 1.83588312\n",
      "Iteration 41816, loss = 1.88910511\n",
      "Iteration 41817, loss = 2.88998631\n",
      "Iteration 41818, loss = 2.74825070\n",
      "Iteration 41819, loss = 2.59821226\n",
      "Iteration 41820, loss = 2.82921030\n",
      "Iteration 41821, loss = 2.21084342\n",
      "Iteration 41822, loss = 2.50721160\n",
      "Iteration 41823, loss = 2.23032780\n",
      "Iteration 41824, loss = 2.59276030\n",
      "Iteration 41825, loss = 2.33786775\n",
      "Iteration 41826, loss = 2.13549462\n",
      "Iteration 41827, loss = 2.52507204\n",
      "Iteration 41828, loss = 2.46223323\n",
      "Iteration 41829, loss = 2.53437326\n",
      "Iteration 41830, loss = 3.26848466\n",
      "Iteration 41831, loss = 2.68266146\n",
      "Iteration 41832, loss = 2.45795967\n",
      "Iteration 41833, loss = 2.98847005\n",
      "Iteration 41834, loss = 2.58365832\n",
      "Iteration 41835, loss = 2.28897972\n",
      "Iteration 41836, loss = 2.17985711\n",
      "Iteration 41837, loss = 2.30414472\n",
      "Iteration 41838, loss = 2.36551658\n",
      "Iteration 41839, loss = 2.11508273\n",
      "Iteration 41840, loss = 2.09023532\n",
      "Iteration 41841, loss = 2.11495776\n",
      "Iteration 41842, loss = 2.26794700\n",
      "Iteration 41843, loss = 2.15703268\n",
      "Iteration 41844, loss = 2.55792499\n",
      "Iteration 41845, loss = 2.14049777\n",
      "Iteration 41846, loss = 2.32317864\n",
      "Iteration 41847, loss = 1.84680018\n",
      "Iteration 41848, loss = 1.71749917\n",
      "Iteration 41849, loss = 1.94081028\n",
      "Iteration 41850, loss = 1.84624942\n",
      "Iteration 41851, loss = 1.77653588\n",
      "Iteration 41852, loss = 1.84451052\n",
      "Iteration 41853, loss = 1.74346739\n",
      "Iteration 41854, loss = 1.87545492\n",
      "Iteration 41855, loss = 1.98822124\n",
      "Iteration 41856, loss = 2.63384698\n",
      "Iteration 41857, loss = 2.12217192\n",
      "Iteration 41858, loss = 2.11305307\n",
      "Iteration 41859, loss = 2.16182613\n",
      "Iteration 41860, loss = 1.98416971\n",
      "Iteration 41861, loss = 1.88789183\n",
      "Iteration 41862, loss = 1.79286175\n",
      "Iteration 41863, loss = 1.79942157\n",
      "Iteration 41864, loss = 1.72604682\n",
      "Iteration 41865, loss = 1.74680042\n",
      "Iteration 41866, loss = 1.94885739\n",
      "Iteration 41867, loss = 1.92139644\n",
      "Iteration 41868, loss = 1.83453345\n",
      "Iteration 41869, loss = 2.14306301\n",
      "Iteration 41870, loss = 1.97454199\n",
      "Iteration 41871, loss = 1.74105516\n",
      "Iteration 41872, loss = 1.70247509\n",
      "Iteration 41873, loss = 1.74334384\n",
      "Iteration 41874, loss = 2.22071648\n",
      "Iteration 41875, loss = 2.17200870\n",
      "Iteration 41876, loss = 2.06385506\n",
      "Iteration 41877, loss = 2.50961810\n",
      "Iteration 41878, loss = 2.62602163\n",
      "Iteration 41879, loss = 1.91484305\n",
      "Iteration 41880, loss = 1.96941199\n",
      "Iteration 41881, loss = 2.10761922\n",
      "Iteration 41882, loss = 2.35634075\n",
      "Iteration 41883, loss = 2.46317469\n",
      "Iteration 41884, loss = 2.19125912\n",
      "Iteration 41885, loss = 2.49899272\n",
      "Iteration 41886, loss = 2.40640383\n",
      "Iteration 41887, loss = 2.40583307\n",
      "Iteration 41888, loss = 2.23594211\n",
      "Iteration 41889, loss = 2.14702778\n",
      "Iteration 41890, loss = 2.28485321\n",
      "Iteration 41891, loss = 2.90812399\n",
      "Iteration 41892, loss = 2.52066759\n",
      "Iteration 41893, loss = 3.31257304\n",
      "Iteration 41894, loss = 2.77925667\n",
      "Iteration 41895, loss = 3.49897024\n",
      "Iteration 41896, loss = 2.64371094\n",
      "Iteration 41897, loss = 2.73927130\n",
      "Iteration 41898, loss = 2.32166449\n",
      "Iteration 41899, loss = 2.80594267\n",
      "Iteration 41900, loss = 2.00512086\n",
      "Iteration 41901, loss = 2.61932642\n",
      "Iteration 41902, loss = 2.02547398\n",
      "Iteration 41903, loss = 1.86336823\n",
      "Iteration 41904, loss = 1.82455323\n",
      "Iteration 41905, loss = 2.15366653\n",
      "Iteration 41906, loss = 1.92459064\n",
      "Iteration 41907, loss = 1.80252431\n",
      "Iteration 41908, loss = 2.02045691\n",
      "Iteration 41909, loss = 1.97295828\n",
      "Iteration 41910, loss = 1.92009564\n",
      "Iteration 41911, loss = 2.22858644\n",
      "Iteration 41912, loss = 2.15729321\n",
      "Iteration 41913, loss = 2.46824587\n",
      "Iteration 41914, loss = 1.97989107\n",
      "Iteration 41915, loss = 2.04720586\n",
      "Iteration 41916, loss = 2.61870563\n",
      "Iteration 41917, loss = 4.65486502\n",
      "Iteration 41918, loss = 2.77422064\n",
      "Iteration 41919, loss = 2.49240922\n",
      "Iteration 41920, loss = 3.24125893\n",
      "Iteration 41921, loss = 3.86878320\n",
      "Iteration 41922, loss = 3.20324046\n",
      "Iteration 41923, loss = 2.40012632\n",
      "Iteration 41924, loss = 2.08045954\n",
      "Iteration 41925, loss = 1.83748734\n",
      "Iteration 41926, loss = 1.95378511\n",
      "Iteration 41927, loss = 1.75435608\n",
      "Iteration 41928, loss = 1.69729398\n",
      "Iteration 41929, loss = 1.70656694\n",
      "Iteration 41930, loss = 1.81896631\n",
      "Iteration 41931, loss = 1.90018073\n",
      "Iteration 41932, loss = 1.87532143\n",
      "Iteration 41933, loss = 1.74054867\n",
      "Iteration 41934, loss = 1.79215703\n",
      "Iteration 41935, loss = 1.86280673\n",
      "Iteration 41936, loss = 1.68076848\n",
      "Iteration 41937, loss = 2.01554347\n",
      "Iteration 41938, loss = 1.87376349\n",
      "Iteration 41939, loss = 2.04614458\n",
      "Iteration 41940, loss = 1.80575235\n",
      "Iteration 41941, loss = 1.64193925\n",
      "Iteration 41942, loss = 1.63486171\n",
      "Iteration 41943, loss = 1.89168888\n",
      "Iteration 41944, loss = 1.82817365\n",
      "Iteration 41945, loss = 2.04522248\n",
      "Iteration 41946, loss = 1.95521912\n",
      "Iteration 41947, loss = 1.78005799\n",
      "Iteration 41948, loss = 1.69386079\n",
      "Iteration 41949, loss = 1.95276176\n",
      "Iteration 41950, loss = 1.91569617\n",
      "Iteration 41951, loss = 1.99172390\n",
      "Iteration 41952, loss = 2.19525700\n",
      "Iteration 41953, loss = 2.42351547\n",
      "Iteration 41954, loss = 3.07627975\n",
      "Iteration 41955, loss = 2.31581691\n",
      "Iteration 41956, loss = 1.93470952\n",
      "Iteration 41957, loss = 2.72082232\n",
      "Iteration 41958, loss = 2.08790881\n",
      "Iteration 41959, loss = 2.32638688\n",
      "Iteration 41960, loss = 2.17938306\n",
      "Iteration 41961, loss = 1.92046074\n",
      "Iteration 41962, loss = 1.92941793\n",
      "Iteration 41963, loss = 1.88471199\n",
      "Iteration 41964, loss = 2.03274258\n",
      "Iteration 41965, loss = 1.69166468\n",
      "Iteration 41966, loss = 2.03010099\n",
      "Iteration 41967, loss = 2.18528981\n",
      "Iteration 41968, loss = 2.22238376\n",
      "Iteration 41969, loss = 2.44307836\n",
      "Iteration 41970, loss = 2.46974186\n",
      "Iteration 41971, loss = 2.76977478\n",
      "Iteration 41972, loss = 2.09149830\n",
      "Iteration 41973, loss = 2.17504014\n",
      "Iteration 41974, loss = 1.97198015\n",
      "Iteration 41975, loss = 2.09736679\n",
      "Iteration 41976, loss = 2.22493076\n",
      "Iteration 41977, loss = 2.35682864\n",
      "Iteration 41978, loss = 2.21350810\n",
      "Iteration 41979, loss = 2.29283407\n",
      "Iteration 41980, loss = 2.79964716\n",
      "Iteration 41981, loss = 2.92398886\n",
      "Iteration 41982, loss = 2.72446756\n",
      "Iteration 41983, loss = 2.75848581\n",
      "Iteration 41984, loss = 2.22072422\n",
      "Iteration 41985, loss = 2.54757902\n",
      "Iteration 41986, loss = 2.64764829\n",
      "Iteration 41987, loss = 3.12585936\n",
      "Iteration 41988, loss = 2.31588610\n",
      "Iteration 41989, loss = 2.84365038\n",
      "Iteration 41990, loss = 2.31634244\n",
      "Iteration 41991, loss = 1.83601909\n",
      "Iteration 41992, loss = 2.02441509\n",
      "Iteration 41993, loss = 2.38041622\n",
      "Iteration 41994, loss = 1.99522452\n",
      "Iteration 41995, loss = 2.02424673\n",
      "Iteration 41996, loss = 2.22985640\n",
      "Iteration 41997, loss = 2.07763763\n",
      "Iteration 41998, loss = 1.74714777\n",
      "Iteration 41999, loss = 1.91124180\n",
      "Iteration 42000, loss = 1.60726205\n",
      "Iteration 42001, loss = 1.91174973\n",
      "Iteration 42002, loss = 1.76357974\n",
      "Iteration 42003, loss = 1.65992716\n",
      "Iteration 42004, loss = 2.21579948\n",
      "Iteration 42005, loss = 2.04725901\n",
      "Iteration 42006, loss = 1.96060642\n",
      "Iteration 42007, loss = 2.12327133\n",
      "Iteration 42008, loss = 2.19838254\n",
      "Iteration 42009, loss = 2.24471167\n",
      "Iteration 42010, loss = 2.38747681\n",
      "Iteration 42011, loss = 2.22873489\n",
      "Iteration 42012, loss = 1.87658057\n",
      "Iteration 42013, loss = 2.04341658\n",
      "Iteration 42014, loss = 2.10349351\n",
      "Iteration 42015, loss = 1.90215937\n",
      "Iteration 42016, loss = 2.05890329\n",
      "Iteration 42017, loss = 2.15980338\n",
      "Iteration 42018, loss = 2.04029618\n",
      "Iteration 42019, loss = 1.89818053\n",
      "Iteration 42020, loss = 1.92490287\n",
      "Iteration 42021, loss = 2.01635648\n",
      "Iteration 42022, loss = 1.96893018\n",
      "Iteration 42023, loss = 2.01273870\n",
      "Iteration 42024, loss = 1.80254898\n",
      "Iteration 42025, loss = 2.00809593\n",
      "Iteration 42026, loss = 2.17165354\n",
      "Iteration 42027, loss = 1.67959385\n",
      "Iteration 42028, loss = 1.83619524\n",
      "Iteration 42029, loss = 1.92486313\n",
      "Iteration 42030, loss = 1.86606353\n",
      "Iteration 42031, loss = 1.74261467\n",
      "Iteration 42032, loss = 1.75206309\n",
      "Iteration 42033, loss = 1.73482761\n",
      "Iteration 42034, loss = 1.83654713\n",
      "Iteration 42035, loss = 1.67023273\n",
      "Iteration 42036, loss = 1.71269793\n",
      "Iteration 42037, loss = 1.97007830\n",
      "Iteration 42038, loss = 2.47508343\n",
      "Iteration 42039, loss = 2.46495918\n",
      "Iteration 42040, loss = 2.96346626\n",
      "Iteration 42041, loss = 2.25621429\n",
      "Iteration 42042, loss = 2.37389439\n",
      "Iteration 42043, loss = 2.55416778\n",
      "Iteration 42044, loss = 2.08057360\n",
      "Iteration 42045, loss = 2.02751882\n",
      "Iteration 42046, loss = 1.79819355\n",
      "Iteration 42047, loss = 1.74499999\n",
      "Iteration 42048, loss = 1.93337476\n",
      "Iteration 42049, loss = 1.87466121\n",
      "Iteration 42050, loss = 2.03582732\n",
      "Iteration 42051, loss = 2.01749304\n",
      "Iteration 42052, loss = 1.76769244\n",
      "Iteration 42053, loss = 1.84534698\n",
      "Iteration 42054, loss = 2.41793921\n",
      "Iteration 42055, loss = 2.31987847\n",
      "Iteration 42056, loss = 2.79571397\n",
      "Iteration 42057, loss = 3.12786253\n",
      "Iteration 42058, loss = 2.92526773\n",
      "Iteration 42059, loss = 2.83417196\n",
      "Iteration 42060, loss = 2.18447059\n",
      "Iteration 42061, loss = 2.06122998\n",
      "Iteration 42062, loss = 2.66759109\n",
      "Iteration 42063, loss = 2.15108291\n",
      "Iteration 42064, loss = 2.16892998\n",
      "Iteration 42065, loss = 2.00500649\n",
      "Iteration 42066, loss = 1.82748525\n",
      "Iteration 42067, loss = 1.84306002\n",
      "Iteration 42068, loss = 2.03135605\n",
      "Iteration 42069, loss = 2.35439037\n",
      "Iteration 42070, loss = 2.64710359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 42071, loss = 2.10514282\n",
      "Iteration 42072, loss = 2.46823579\n",
      "Iteration 42073, loss = 1.99785529\n",
      "Iteration 42074, loss = 1.80815944\n",
      "Iteration 42075, loss = 1.82208626\n",
      "Iteration 42076, loss = 1.89033537\n",
      "Iteration 42077, loss = 2.71102494\n",
      "Iteration 42078, loss = 2.56750867\n",
      "Iteration 42079, loss = 2.42412869\n",
      "Iteration 42080, loss = 2.22287850\n",
      "Iteration 42081, loss = 3.06236037\n",
      "Iteration 42082, loss = 2.50377116\n",
      "Iteration 42083, loss = 2.57104854\n",
      "Iteration 42084, loss = 2.69772955\n",
      "Iteration 42085, loss = 2.44781612\n",
      "Iteration 42086, loss = 2.56182745\n",
      "Iteration 42087, loss = 2.10347183\n",
      "Iteration 42088, loss = 1.95598289\n",
      "Iteration 42089, loss = 1.69581515\n",
      "Iteration 42090, loss = 1.71221056\n",
      "Iteration 42091, loss = 1.77683999\n",
      "Iteration 42092, loss = 1.82734012\n",
      "Iteration 42093, loss = 1.72238217\n",
      "Iteration 42094, loss = 1.89217614\n",
      "Iteration 42095, loss = 2.01102162\n",
      "Iteration 42096, loss = 1.86518094\n",
      "Iteration 42097, loss = 2.00221410\n",
      "Iteration 42098, loss = 2.55877597\n",
      "Iteration 42099, loss = 2.69085512\n",
      "Iteration 42100, loss = 2.56889285\n",
      "Iteration 42101, loss = 2.62996748\n",
      "Iteration 42102, loss = 3.22470625\n",
      "Iteration 42103, loss = 2.43618531\n",
      "Iteration 42104, loss = 2.72449772\n",
      "Iteration 42105, loss = 2.65826149\n",
      "Iteration 42106, loss = 2.17179319\n",
      "Iteration 42107, loss = 2.91479897\n",
      "Iteration 42108, loss = 2.45011915\n",
      "Iteration 42109, loss = 2.61804684\n",
      "Iteration 42110, loss = 3.00452859\n",
      "Iteration 42111, loss = 3.32959566\n",
      "Iteration 42112, loss = 2.45746503\n",
      "Iteration 42113, loss = 2.34426673\n",
      "Iteration 42114, loss = 2.50771700\n",
      "Iteration 42115, loss = 2.88465933\n",
      "Iteration 42116, loss = 2.54346426\n",
      "Iteration 42117, loss = 3.08278440\n",
      "Iteration 42118, loss = 6.20584009\n",
      "Iteration 42119, loss = 5.26411637\n",
      "Iteration 42120, loss = 3.97466841\n",
      "Iteration 42121, loss = 4.79735905\n",
      "Iteration 42122, loss = 3.80246676\n",
      "Iteration 42123, loss = 3.60085079\n",
      "Iteration 42124, loss = 3.60177312\n",
      "Iteration 42125, loss = 3.67290709\n",
      "Iteration 42126, loss = 2.71497777\n",
      "Iteration 42127, loss = 2.22550058\n",
      "Iteration 42128, loss = 2.55442155\n",
      "Iteration 42129, loss = 2.52792869\n",
      "Iteration 42130, loss = 2.00078922\n",
      "Iteration 42131, loss = 1.82228061\n",
      "Iteration 42132, loss = 2.23923828\n",
      "Iteration 42133, loss = 2.01963894\n",
      "Iteration 42134, loss = 1.98499562\n",
      "Iteration 42135, loss = 1.96896616\n",
      "Iteration 42136, loss = 1.85133574\n",
      "Iteration 42137, loss = 2.34648934\n",
      "Iteration 42138, loss = 1.90882882\n",
      "Iteration 42139, loss = 1.88262831\n",
      "Iteration 42140, loss = 1.95480285\n",
      "Iteration 42141, loss = 2.08944822\n",
      "Iteration 42142, loss = 1.74175291\n",
      "Iteration 42143, loss = 2.17011555\n",
      "Iteration 42144, loss = 2.20693274\n",
      "Iteration 42145, loss = 2.18747861\n",
      "Iteration 42146, loss = 1.68396755\n",
      "Iteration 42147, loss = 1.82913072\n",
      "Iteration 42148, loss = 1.83607488\n",
      "Iteration 42149, loss = 1.88856804\n",
      "Iteration 42150, loss = 1.82605375\n",
      "Iteration 42151, loss = 2.07454198\n",
      "Iteration 42152, loss = 1.87316067\n",
      "Iteration 42153, loss = 2.60237677\n",
      "Iteration 42154, loss = 2.40556149\n",
      "Iteration 42155, loss = 2.15290612\n",
      "Iteration 42156, loss = 2.08447407\n",
      "Iteration 42157, loss = 2.20770773\n",
      "Iteration 42158, loss = 2.02380951\n",
      "Iteration 42159, loss = 2.14550125\n",
      "Iteration 42160, loss = 1.90043296\n",
      "Iteration 42161, loss = 2.23635076\n",
      "Iteration 42162, loss = 2.16002028\n",
      "Iteration 42163, loss = 1.95048005\n",
      "Iteration 42164, loss = 2.10863032\n",
      "Iteration 42165, loss = 1.95845142\n",
      "Iteration 42166, loss = 2.12841149\n",
      "Iteration 42167, loss = 1.93798801\n",
      "Iteration 42168, loss = 1.91228974\n",
      "Iteration 42169, loss = 1.82940735\n",
      "Iteration 42170, loss = 1.91378325\n",
      "Iteration 42171, loss = 1.92968461\n",
      "Iteration 42172, loss = 1.81694320\n",
      "Iteration 42173, loss = 2.03207866\n",
      "Iteration 42174, loss = 1.99355578\n",
      "Iteration 42175, loss = 2.03396503\n",
      "Iteration 42176, loss = 1.75188306\n",
      "Iteration 42177, loss = 1.86144842\n",
      "Iteration 42178, loss = 2.08683588\n",
      "Iteration 42179, loss = 2.77170734\n",
      "Iteration 42180, loss = 3.60737236\n",
      "Iteration 42181, loss = 3.27618493\n",
      "Iteration 42182, loss = 2.82288754\n",
      "Iteration 42183, loss = 2.04434160\n",
      "Iteration 42184, loss = 1.88762594\n",
      "Iteration 42185, loss = 1.81497149\n",
      "Iteration 42186, loss = 1.73007556\n",
      "Iteration 42187, loss = 1.73128090\n",
      "Iteration 42188, loss = 1.82261620\n",
      "Iteration 42189, loss = 1.77576899\n",
      "Iteration 42190, loss = 1.81416396\n",
      "Iteration 42191, loss = 2.40785409\n",
      "Iteration 42192, loss = 1.86758698\n",
      "Iteration 42193, loss = 1.83454123\n",
      "Iteration 42194, loss = 2.05828902\n",
      "Iteration 42195, loss = 1.89812658\n",
      "Iteration 42196, loss = 2.18924374\n",
      "Iteration 42197, loss = 2.04021769\n",
      "Iteration 42198, loss = 2.56493397\n",
      "Iteration 42199, loss = 3.40973386\n",
      "Iteration 42200, loss = 2.85542617\n",
      "Iteration 42201, loss = 2.41912983\n",
      "Iteration 42202, loss = 2.46958117\n",
      "Iteration 42203, loss = 2.21630777\n",
      "Iteration 42204, loss = 2.55048408\n",
      "Iteration 42205, loss = 2.69942130\n",
      "Iteration 42206, loss = 2.52843310\n",
      "Iteration 42207, loss = 1.86369097\n",
      "Iteration 42208, loss = 1.67312874\n",
      "Iteration 42209, loss = 1.84546389\n",
      "Iteration 42210, loss = 2.32214586\n",
      "Iteration 42211, loss = 2.09748264\n",
      "Iteration 42212, loss = 2.58222248\n",
      "Iteration 42213, loss = 2.37268828\n",
      "Iteration 42214, loss = 2.09417754\n",
      "Iteration 42215, loss = 2.57529269\n",
      "Iteration 42216, loss = 3.08158110\n",
      "Iteration 42217, loss = 2.37631143\n",
      "Iteration 42218, loss = 3.21577645\n",
      "Iteration 42219, loss = 2.49280472\n",
      "Iteration 42220, loss = 2.76475173\n",
      "Iteration 42221, loss = 2.62131455\n",
      "Iteration 42222, loss = 2.82197325\n",
      "Iteration 42223, loss = 2.25266607\n",
      "Iteration 42224, loss = 2.93050598\n",
      "Iteration 42225, loss = 2.84483215\n",
      "Iteration 42226, loss = 1.91331476\n",
      "Iteration 42227, loss = 2.17341693\n",
      "Iteration 42228, loss = 2.46745753\n",
      "Iteration 42229, loss = 2.02984676\n",
      "Iteration 42230, loss = 2.20338987\n",
      "Iteration 42231, loss = 1.87354860\n",
      "Iteration 42232, loss = 1.78280818\n",
      "Iteration 42233, loss = 1.73032128\n",
      "Iteration 42234, loss = 1.98695921\n",
      "Iteration 42235, loss = 1.95255359\n",
      "Iteration 42236, loss = 1.68201757\n",
      "Iteration 42237, loss = 2.31288717\n",
      "Iteration 42238, loss = 1.90117420\n",
      "Iteration 42239, loss = 1.83745684\n",
      "Iteration 42240, loss = 1.72629648\n",
      "Iteration 42241, loss = 1.85286225\n",
      "Iteration 42242, loss = 1.90968983\n",
      "Iteration 42243, loss = 2.11720010\n",
      "Iteration 42244, loss = 1.97455925\n",
      "Iteration 42245, loss = 1.99613602\n",
      "Iteration 42246, loss = 1.93955866\n",
      "Iteration 42247, loss = 2.04479719\n",
      "Iteration 42248, loss = 2.52857847\n",
      "Iteration 42249, loss = 1.95217680\n",
      "Iteration 42250, loss = 2.28292976\n",
      "Iteration 42251, loss = 2.73441040\n",
      "Iteration 42252, loss = 2.62658483\n",
      "Iteration 42253, loss = 3.19690522\n",
      "Iteration 42254, loss = 4.99147341\n",
      "Iteration 42255, loss = 3.86375311\n",
      "Iteration 42256, loss = 2.70535569\n",
      "Iteration 42257, loss = 1.78412750\n",
      "Iteration 42258, loss = 1.89589102\n",
      "Iteration 42259, loss = 1.93487799\n",
      "Iteration 42260, loss = 2.15148174\n",
      "Iteration 42261, loss = 2.24474421\n",
      "Iteration 42262, loss = 2.09194762\n",
      "Iteration 42263, loss = 1.81469094\n",
      "Iteration 42264, loss = 1.77562610\n",
      "Iteration 42265, loss = 1.86485613\n",
      "Iteration 42266, loss = 1.87932733\n",
      "Iteration 42267, loss = 1.74732227\n",
      "Iteration 42268, loss = 1.72397494\n",
      "Iteration 42269, loss = 1.88202774\n",
      "Iteration 42270, loss = 1.93073877\n",
      "Iteration 42271, loss = 1.66773333\n",
      "Iteration 42272, loss = 1.61020037\n",
      "Iteration 42273, loss = 1.63570920\n",
      "Iteration 42274, loss = 1.75732907\n",
      "Iteration 42275, loss = 1.94612838\n",
      "Iteration 42276, loss = 1.97372485\n",
      "Iteration 42277, loss = 2.13640319\n",
      "Iteration 42278, loss = 2.38039441\n",
      "Iteration 42279, loss = 2.32084307\n",
      "Iteration 42280, loss = 2.09904177\n",
      "Iteration 42281, loss = 2.25121207\n",
      "Iteration 42282, loss = 2.21645063\n",
      "Iteration 42283, loss = 1.98943419\n",
      "Iteration 42284, loss = 1.84720231\n",
      "Iteration 42285, loss = 2.04378779\n",
      "Iteration 42286, loss = 2.14107482\n",
      "Iteration 42287, loss = 2.16710660\n",
      "Iteration 42288, loss = 1.84900810\n",
      "Iteration 42289, loss = 2.04606453\n",
      "Iteration 42290, loss = 1.88835632\n",
      "Iteration 42291, loss = 1.94495876\n",
      "Iteration 42292, loss = 1.74242029\n",
      "Iteration 42293, loss = 2.14047086\n",
      "Iteration 42294, loss = 1.76572867\n",
      "Iteration 42295, loss = 1.92119312\n",
      "Iteration 42296, loss = 2.27242941\n",
      "Iteration 42297, loss = 2.44294546\n",
      "Iteration 42298, loss = 2.35195983\n",
      "Iteration 42299, loss = 2.02777838\n",
      "Iteration 42300, loss = 2.49976750\n",
      "Iteration 42301, loss = 2.67131747\n",
      "Iteration 42302, loss = 3.06555441\n",
      "Iteration 42303, loss = 2.89597234\n",
      "Iteration 42304, loss = 2.87547537\n",
      "Iteration 42305, loss = 1.86576596\n",
      "Iteration 42306, loss = 2.01369018\n",
      "Iteration 42307, loss = 2.53317003\n",
      "Iteration 42308, loss = 2.74344664\n",
      "Iteration 42309, loss = 2.38546685\n",
      "Iteration 42310, loss = 2.91352295\n",
      "Iteration 42311, loss = 3.08695891\n",
      "Iteration 42312, loss = 3.55603153\n",
      "Iteration 42313, loss = 3.22509915\n",
      "Iteration 42314, loss = 3.20295338\n",
      "Iteration 42315, loss = 3.58452609\n",
      "Iteration 42316, loss = 2.82911609\n",
      "Iteration 42317, loss = 2.13882080\n",
      "Iteration 42318, loss = 1.97872750\n",
      "Iteration 42319, loss = 1.79390497\n",
      "Iteration 42320, loss = 1.74196674\n",
      "Iteration 42321, loss = 1.89977618\n",
      "Iteration 42322, loss = 2.13290604\n",
      "Iteration 42323, loss = 1.92915483\n",
      "Iteration 42324, loss = 1.85496675\n",
      "Iteration 42325, loss = 1.76228694\n",
      "Iteration 42326, loss = 1.88570379\n",
      "Iteration 42327, loss = 1.80365328\n",
      "Iteration 42328, loss = 1.81502907\n",
      "Iteration 42329, loss = 1.73294368\n",
      "Iteration 42330, loss = 2.68226220\n",
      "Iteration 42331, loss = 2.30995842\n",
      "Iteration 42332, loss = 1.81418100\n",
      "Iteration 42333, loss = 1.94826829\n",
      "Iteration 42334, loss = 1.81105565\n",
      "Iteration 42335, loss = 2.22910426\n",
      "Iteration 42336, loss = 2.10378223\n",
      "Iteration 42337, loss = 2.03531160\n",
      "Iteration 42338, loss = 2.01250274\n",
      "Iteration 42339, loss = 1.89525710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 42340, loss = 1.91149741\n",
      "Iteration 42341, loss = 1.90897328\n",
      "Iteration 42342, loss = 1.71682781\n",
      "Iteration 42343, loss = 1.87978756\n",
      "Iteration 42344, loss = 1.76571842\n",
      "Iteration 42345, loss = 1.68430887\n",
      "Iteration 42346, loss = 1.75652764\n",
      "Iteration 42347, loss = 2.43002979\n",
      "Iteration 42348, loss = 2.57516311\n",
      "Iteration 42349, loss = 2.02572972\n",
      "Iteration 42350, loss = 2.30157481\n",
      "Iteration 42351, loss = 3.13281271\n",
      "Iteration 42352, loss = 2.22043451\n",
      "Iteration 42353, loss = 2.80011427\n",
      "Iteration 42354, loss = 2.89352784\n",
      "Iteration 42355, loss = 2.79086454\n",
      "Iteration 42356, loss = 3.13909281\n",
      "Iteration 42357, loss = 1.76818054\n",
      "Iteration 42358, loss = 2.20774090\n",
      "Iteration 42359, loss = 2.70313563\n",
      "Iteration 42360, loss = 2.43910419\n",
      "Iteration 42361, loss = 1.89067727\n",
      "Iteration 42362, loss = 2.07690826\n",
      "Iteration 42363, loss = 2.20487826\n",
      "Iteration 42364, loss = 1.75678138\n",
      "Iteration 42365, loss = 1.91328008\n",
      "Iteration 42366, loss = 1.72032991\n",
      "Iteration 42367, loss = 1.65308875\n",
      "Iteration 42368, loss = 2.12220189\n",
      "Iteration 42369, loss = 2.55612722\n",
      "Iteration 42370, loss = 4.12751472\n",
      "Iteration 42371, loss = 3.54102812\n",
      "Iteration 42372, loss = 2.24096937\n",
      "Iteration 42373, loss = 2.73374791\n",
      "Iteration 42374, loss = 3.72653805\n",
      "Iteration 42375, loss = 3.84535529\n",
      "Iteration 42376, loss = 3.80970405\n",
      "Iteration 42377, loss = 3.64234855\n",
      "Iteration 42378, loss = 3.77948457\n",
      "Iteration 42379, loss = 3.05602821\n",
      "Iteration 42380, loss = 2.72909167\n",
      "Iteration 42381, loss = 2.54171900\n",
      "Iteration 42382, loss = 2.77031041\n",
      "Iteration 42383, loss = 2.50210547\n",
      "Iteration 42384, loss = 2.24725503\n",
      "Iteration 42385, loss = 1.70266493\n",
      "Iteration 42386, loss = 2.07045215\n",
      "Iteration 42387, loss = 2.10886719\n",
      "Iteration 42388, loss = 2.75985446\n",
      "Iteration 42389, loss = 2.81749305\n",
      "Iteration 42390, loss = 1.88570836\n",
      "Iteration 42391, loss = 1.98558964\n",
      "Iteration 42392, loss = 2.20114651\n",
      "Iteration 42393, loss = 1.96410176\n",
      "Iteration 42394, loss = 2.12507516\n",
      "Iteration 42395, loss = 1.74982298\n",
      "Iteration 42396, loss = 1.78831719\n",
      "Iteration 42397, loss = 2.50850126\n",
      "Iteration 42398, loss = 1.90523789\n",
      "Iteration 42399, loss = 2.06070799\n",
      "Iteration 42400, loss = 1.72166052\n",
      "Iteration 42401, loss = 2.08639981\n",
      "Iteration 42402, loss = 2.08006427\n",
      "Iteration 42403, loss = 2.18643292\n",
      "Iteration 42404, loss = 2.05119747\n",
      "Iteration 42405, loss = 2.03196318\n",
      "Iteration 42406, loss = 2.03737104\n",
      "Iteration 42407, loss = 1.83617484\n",
      "Iteration 42408, loss = 1.84051029\n",
      "Iteration 42409, loss = 2.01184707\n",
      "Iteration 42410, loss = 2.23921780\n",
      "Iteration 42411, loss = 2.53990733\n",
      "Iteration 42412, loss = 2.31936768\n",
      "Iteration 42413, loss = 2.32372003\n",
      "Iteration 42414, loss = 1.98356247\n",
      "Iteration 42415, loss = 1.86221137\n",
      "Iteration 42416, loss = 2.01915337\n",
      "Iteration 42417, loss = 1.80554026\n",
      "Iteration 42418, loss = 2.02642448\n",
      "Iteration 42419, loss = 1.89220481\n",
      "Iteration 42420, loss = 1.74713367\n",
      "Iteration 42421, loss = 1.69404891\n",
      "Iteration 42422, loss = 1.77467682\n",
      "Iteration 42423, loss = 1.87804854\n",
      "Iteration 42424, loss = 1.83116066\n",
      "Iteration 42425, loss = 1.89773603\n",
      "Iteration 42426, loss = 1.98323933\n",
      "Iteration 42427, loss = 2.15775879\n",
      "Iteration 42428, loss = 2.49494570\n",
      "Iteration 42429, loss = 3.00388690\n",
      "Iteration 42430, loss = 2.52046308\n",
      "Iteration 42431, loss = 2.12883408\n",
      "Iteration 42432, loss = 1.88942052\n",
      "Iteration 42433, loss = 1.95518299\n",
      "Iteration 42434, loss = 2.07608991\n",
      "Iteration 42435, loss = 2.03738084\n",
      "Iteration 42436, loss = 2.58551385\n",
      "Iteration 42437, loss = 2.42496945\n",
      "Iteration 42438, loss = 2.52981635\n",
      "Iteration 42439, loss = 2.43347124\n",
      "Iteration 42440, loss = 2.58746674\n",
      "Iteration 42441, loss = 2.03945053\n",
      "Iteration 42442, loss = 2.61737013\n",
      "Iteration 42443, loss = 2.50458230\n",
      "Iteration 42444, loss = 1.86736219\n",
      "Iteration 42445, loss = 1.67634562\n",
      "Iteration 42446, loss = 1.70245084\n",
      "Iteration 42447, loss = 1.83064115\n",
      "Iteration 42448, loss = 2.30392941\n",
      "Iteration 42449, loss = 2.50396450\n",
      "Iteration 42450, loss = 2.45571647\n",
      "Iteration 42451, loss = 2.49109193\n",
      "Iteration 42452, loss = 2.32226570\n",
      "Iteration 42453, loss = 1.98113132\n",
      "Iteration 42454, loss = 1.83669578\n",
      "Iteration 42455, loss = 1.77171618\n",
      "Iteration 42456, loss = 2.49996215\n",
      "Iteration 42457, loss = 2.32853832\n",
      "Iteration 42458, loss = 1.89829185\n",
      "Iteration 42459, loss = 1.87771517\n",
      "Iteration 42460, loss = 2.10102919\n",
      "Iteration 42461, loss = 2.15726837\n",
      "Iteration 42462, loss = 3.24653477\n",
      "Iteration 42463, loss = 2.62386491\n",
      "Iteration 42464, loss = 2.42536884\n",
      "Iteration 42465, loss = 2.12396127\n",
      "Iteration 42466, loss = 1.78330276\n",
      "Iteration 42467, loss = 1.63671801\n",
      "Iteration 42468, loss = 1.85350453\n",
      "Iteration 42469, loss = 1.62083346\n",
      "Iteration 42470, loss = 1.71743708\n",
      "Iteration 42471, loss = 1.82942102\n",
      "Iteration 42472, loss = 1.61959468\n",
      "Iteration 42473, loss = 1.67667496\n",
      "Iteration 42474, loss = 2.12801488\n",
      "Iteration 42475, loss = 2.27944421\n",
      "Iteration 42476, loss = 2.94070056\n",
      "Iteration 42477, loss = 2.12209159\n",
      "Iteration 42478, loss = 2.93066767\n",
      "Iteration 42479, loss = 2.89139876\n",
      "Iteration 42480, loss = 2.32830606\n",
      "Iteration 42481, loss = 2.37442655\n",
      "Iteration 42482, loss = 3.04865679\n",
      "Iteration 42483, loss = 2.72979570\n",
      "Iteration 42484, loss = 2.63147046\n",
      "Iteration 42485, loss = 2.48413303\n",
      "Iteration 42486, loss = 2.85486322\n",
      "Iteration 42487, loss = 2.14064111\n",
      "Iteration 42488, loss = 2.23564989\n",
      "Iteration 42489, loss = 1.88459844\n",
      "Iteration 42490, loss = 1.92203813\n",
      "Iteration 42491, loss = 2.03478881\n",
      "Iteration 42492, loss = 2.38747871\n",
      "Iteration 42493, loss = 2.01127545\n",
      "Iteration 42494, loss = 2.16337429\n",
      "Iteration 42495, loss = 1.99019629\n",
      "Iteration 42496, loss = 1.91354179\n",
      "Iteration 42497, loss = 2.31545333\n",
      "Iteration 42498, loss = 2.31168869\n",
      "Iteration 42499, loss = 2.25547176\n",
      "Iteration 42500, loss = 2.19309645\n",
      "Iteration 42501, loss = 2.13061980\n",
      "Iteration 42502, loss = 2.34214540\n",
      "Iteration 42503, loss = 2.03465180\n",
      "Iteration 42504, loss = 2.09737617\n",
      "Iteration 42505, loss = 1.71704178\n",
      "Iteration 42506, loss = 1.80189217\n",
      "Iteration 42507, loss = 1.88995731\n",
      "Iteration 42508, loss = 2.17084127\n",
      "Iteration 42509, loss = 2.93010270\n",
      "Iteration 42510, loss = 2.05159998\n",
      "Iteration 42511, loss = 2.00912436\n",
      "Iteration 42512, loss = 1.70037938\n",
      "Iteration 42513, loss = 1.70311956\n",
      "Iteration 42514, loss = 1.63660402\n",
      "Iteration 42515, loss = 1.78905229\n",
      "Iteration 42516, loss = 1.68891504\n",
      "Iteration 42517, loss = 1.93490324\n",
      "Iteration 42518, loss = 1.89225109\n",
      "Iteration 42519, loss = 2.38395414\n",
      "Iteration 42520, loss = 2.59213820\n",
      "Iteration 42521, loss = 1.94777141\n",
      "Iteration 42522, loss = 2.11179230\n",
      "Iteration 42523, loss = 2.18951781\n",
      "Iteration 42524, loss = 2.39644868\n",
      "Iteration 42525, loss = 2.59296204\n",
      "Iteration 42526, loss = 2.76874544\n",
      "Iteration 42527, loss = 3.52539403\n",
      "Iteration 42528, loss = 4.18624333\n",
      "Iteration 42529, loss = 3.88786626\n",
      "Iteration 42530, loss = 3.75102596\n",
      "Iteration 42531, loss = 3.09822825\n",
      "Iteration 42532, loss = 3.26571488\n",
      "Iteration 42533, loss = 3.30593202\n",
      "Iteration 42534, loss = 3.00558319\n",
      "Iteration 42535, loss = 2.65784745\n",
      "Iteration 42536, loss = 2.15150441\n",
      "Iteration 42537, loss = 2.01553704\n",
      "Iteration 42538, loss = 1.80074378\n",
      "Iteration 42539, loss = 1.81130212\n",
      "Iteration 42540, loss = 2.08374925\n",
      "Iteration 42541, loss = 2.17956211\n",
      "Iteration 42542, loss = 1.90632614\n",
      "Iteration 42543, loss = 1.93475121\n",
      "Iteration 42544, loss = 1.80960038\n",
      "Iteration 42545, loss = 1.64226153\n",
      "Iteration 42546, loss = 1.80616899\n",
      "Iteration 42547, loss = 1.73218063\n",
      "Iteration 42548, loss = 1.67419919\n",
      "Iteration 42549, loss = 1.92142484\n",
      "Iteration 42550, loss = 1.93588401\n",
      "Iteration 42551, loss = 2.15613385\n",
      "Iteration 42552, loss = 1.70076021\n",
      "Iteration 42553, loss = 1.63448808\n",
      "Iteration 42554, loss = 1.61265268\n",
      "Iteration 42555, loss = 1.61292552\n",
      "Iteration 42556, loss = 1.72854289\n",
      "Iteration 42557, loss = 1.69007227\n",
      "Iteration 42558, loss = 1.69099911\n",
      "Iteration 42559, loss = 2.06535553\n",
      "Iteration 42560, loss = 1.77384051\n",
      "Iteration 42561, loss = 1.80607201\n",
      "Iteration 42562, loss = 1.71241472\n",
      "Iteration 42563, loss = 1.69460658\n",
      "Iteration 42564, loss = 2.25041582\n",
      "Iteration 42565, loss = 2.16327560\n",
      "Iteration 42566, loss = 2.23660497\n",
      "Iteration 42567, loss = 2.30152140\n",
      "Iteration 42568, loss = 1.89154707\n",
      "Iteration 42569, loss = 2.13157981\n",
      "Iteration 42570, loss = 2.11558476\n",
      "Iteration 42571, loss = 2.77877964\n",
      "Iteration 42572, loss = 2.43251367\n",
      "Iteration 42573, loss = 2.24200807\n",
      "Iteration 42574, loss = 1.87785766\n",
      "Iteration 42575, loss = 1.80624570\n",
      "Iteration 42576, loss = 1.78485127\n",
      "Iteration 42577, loss = 1.89843693\n",
      "Iteration 42578, loss = 2.11331745\n",
      "Iteration 42579, loss = 1.79787754\n",
      "Iteration 42580, loss = 1.95899457\n",
      "Iteration 42581, loss = 1.86036912\n",
      "Iteration 42582, loss = 1.94961499\n",
      "Iteration 42583, loss = 2.04833981\n",
      "Iteration 42584, loss = 1.96318530\n",
      "Iteration 42585, loss = 1.99001337\n",
      "Iteration 42586, loss = 1.88632255\n",
      "Iteration 42587, loss = 2.03160951\n",
      "Iteration 42588, loss = 2.13394193\n",
      "Iteration 42589, loss = 2.43486514\n",
      "Iteration 42590, loss = 2.14058589\n",
      "Iteration 42591, loss = 1.88173847\n",
      "Iteration 42592, loss = 2.29409763\n",
      "Iteration 42593, loss = 2.12630402\n",
      "Iteration 42594, loss = 2.11056263\n",
      "Iteration 42595, loss = 1.88608760\n",
      "Iteration 42596, loss = 2.78309360\n",
      "Iteration 42597, loss = 2.28912967\n",
      "Iteration 42598, loss = 2.62938667\n",
      "Iteration 42599, loss = 2.16088006\n",
      "Iteration 42600, loss = 1.99967262\n",
      "Iteration 42601, loss = 1.88099472\n",
      "Iteration 42602, loss = 1.51780015\n",
      "Iteration 42603, loss = 1.87427218\n",
      "Iteration 42604, loss = 2.04746179\n",
      "Iteration 42605, loss = 1.87483133\n",
      "Iteration 42606, loss = 2.16317994\n",
      "Iteration 42607, loss = 2.23686277\n",
      "Iteration 42608, loss = 2.81257231\n",
      "Iteration 42609, loss = 3.44236909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 42610, loss = 2.23773169\n",
      "Iteration 42611, loss = 2.76964326\n",
      "Iteration 42612, loss = 2.39255245\n",
      "Iteration 42613, loss = 2.22049938\n",
      "Iteration 42614, loss = 1.89406236\n",
      "Iteration 42615, loss = 2.23305753\n",
      "Iteration 42616, loss = 1.91249433\n",
      "Iteration 42617, loss = 1.69103192\n",
      "Iteration 42618, loss = 1.95801187\n",
      "Iteration 42619, loss = 1.82173745\n",
      "Iteration 42620, loss = 2.17059035\n",
      "Iteration 42621, loss = 2.65787590\n",
      "Iteration 42622, loss = 2.61957496\n",
      "Iteration 42623, loss = 2.23060135\n",
      "Iteration 42624, loss = 2.09891762\n",
      "Iteration 42625, loss = 1.89232431\n",
      "Iteration 42626, loss = 1.68961903\n",
      "Iteration 42627, loss = 1.76991985\n",
      "Iteration 42628, loss = 1.79162330\n",
      "Iteration 42629, loss = 1.85260637\n",
      "Iteration 42630, loss = 1.98677769\n",
      "Iteration 42631, loss = 1.76407599\n",
      "Iteration 42632, loss = 1.84020025\n",
      "Iteration 42633, loss = 1.85644033\n",
      "Iteration 42634, loss = 1.73614417\n",
      "Iteration 42635, loss = 1.92589239\n",
      "Iteration 42636, loss = 1.79209959\n",
      "Iteration 42637, loss = 2.45985622\n",
      "Iteration 42638, loss = 2.40915042\n",
      "Iteration 42639, loss = 2.20613254\n",
      "Iteration 42640, loss = 2.76057423\n",
      "Iteration 42641, loss = 2.50783752\n",
      "Iteration 42642, loss = 1.91748640\n",
      "Iteration 42643, loss = 1.97736664\n",
      "Iteration 42644, loss = 2.03459805\n",
      "Iteration 42645, loss = 1.70328298\n",
      "Iteration 42646, loss = 2.06472733\n",
      "Iteration 42647, loss = 1.94884352\n",
      "Iteration 42648, loss = 1.84260419\n",
      "Iteration 42649, loss = 1.89123450\n",
      "Iteration 42650, loss = 1.81393728\n",
      "Iteration 42651, loss = 1.88882911\n",
      "Iteration 42652, loss = 2.03244839\n",
      "Iteration 42653, loss = 1.80263592\n",
      "Iteration 42654, loss = 2.38692231\n",
      "Iteration 42655, loss = 2.48917988\n",
      "Iteration 42656, loss = 3.05230509\n",
      "Iteration 42657, loss = 3.03295098\n",
      "Iteration 42658, loss = 3.33699688\n",
      "Iteration 42659, loss = 3.96281661\n",
      "Iteration 42660, loss = 3.75867507\n",
      "Iteration 42661, loss = 2.69655559\n",
      "Iteration 42662, loss = 2.70042081\n",
      "Iteration 42663, loss = 2.39556236\n",
      "Iteration 42664, loss = 2.32860400\n",
      "Iteration 42665, loss = 2.02489106\n",
      "Iteration 42666, loss = 2.74106995\n",
      "Iteration 42667, loss = 2.18134521\n",
      "Iteration 42668, loss = 1.86667963\n",
      "Iteration 42669, loss = 1.75174790\n",
      "Iteration 42670, loss = 1.88414876\n",
      "Iteration 42671, loss = 1.80742718\n",
      "Iteration 42672, loss = 2.46723023\n",
      "Iteration 42673, loss = 2.33593243\n",
      "Iteration 42674, loss = 2.58910545\n",
      "Iteration 42675, loss = 2.17928458\n",
      "Iteration 42676, loss = 2.09481084\n",
      "Iteration 42677, loss = 1.93851175\n",
      "Iteration 42678, loss = 1.82117995\n",
      "Iteration 42679, loss = 1.99912235\n",
      "Iteration 42680, loss = 1.64827534\n",
      "Iteration 42681, loss = 2.20435088\n",
      "Iteration 42682, loss = 2.42956538\n",
      "Iteration 42683, loss = 2.40319291\n",
      "Iteration 42684, loss = 2.25629961\n",
      "Iteration 42685, loss = 2.18835907\n",
      "Iteration 42686, loss = 1.93294278\n",
      "Iteration 42687, loss = 2.06429293\n",
      "Iteration 42688, loss = 2.00251631\n",
      "Iteration 42689, loss = 2.00208535\n",
      "Iteration 42690, loss = 1.96820301\n",
      "Iteration 42691, loss = 1.94285575\n",
      "Iteration 42692, loss = 1.97192647\n",
      "Iteration 42693, loss = 2.02517140\n",
      "Iteration 42694, loss = 1.91597955\n",
      "Iteration 42695, loss = 1.97571974\n",
      "Iteration 42696, loss = 1.97932565\n",
      "Iteration 42697, loss = 1.84748493\n",
      "Iteration 42698, loss = 2.19196511\n",
      "Iteration 42699, loss = 2.39665233\n",
      "Iteration 42700, loss = 2.17085606\n",
      "Iteration 42701, loss = 2.43234965\n",
      "Iteration 42702, loss = 2.97512971\n",
      "Iteration 42703, loss = 3.91154928\n",
      "Iteration 42704, loss = 2.85157965\n",
      "Iteration 42705, loss = 2.66556854\n",
      "Iteration 42706, loss = 3.30146167\n",
      "Iteration 42707, loss = 4.13149675\n",
      "Iteration 42708, loss = 4.46587788\n",
      "Iteration 42709, loss = 3.13709100\n",
      "Iteration 42710, loss = 2.44553662\n",
      "Iteration 42711, loss = 1.83795914\n",
      "Iteration 42712, loss = 1.82939134\n",
      "Iteration 42713, loss = 1.81123764\n",
      "Iteration 42714, loss = 2.13988665\n",
      "Iteration 42715, loss = 2.18453849\n",
      "Iteration 42716, loss = 1.94996710\n",
      "Iteration 42717, loss = 1.92526812\n",
      "Iteration 42718, loss = 1.73566770\n",
      "Iteration 42719, loss = 1.65848342\n",
      "Iteration 42720, loss = 1.84364046\n",
      "Iteration 42721, loss = 2.42990869\n",
      "Iteration 42722, loss = 1.96088336\n",
      "Iteration 42723, loss = 1.95703455\n",
      "Iteration 42724, loss = 2.00488867\n",
      "Iteration 42725, loss = 1.92382813\n",
      "Iteration 42726, loss = 2.08954255\n",
      "Iteration 42727, loss = 1.86328172\n",
      "Iteration 42728, loss = 2.03736671\n",
      "Iteration 42729, loss = 1.88205172\n",
      "Iteration 42730, loss = 1.62656276\n",
      "Iteration 42731, loss = 1.84215503\n",
      "Iteration 42732, loss = 2.00678520\n",
      "Iteration 42733, loss = 1.90160144\n",
      "Iteration 42734, loss = 1.97396588\n",
      "Iteration 42735, loss = 1.90838214\n",
      "Iteration 42736, loss = 1.90420453\n",
      "Iteration 42737, loss = 2.44589232\n",
      "Iteration 42738, loss = 2.00243227\n",
      "Iteration 42739, loss = 2.03947063\n",
      "Iteration 42740, loss = 2.14825701\n",
      "Iteration 42741, loss = 2.05000543\n",
      "Iteration 42742, loss = 1.93594572\n",
      "Iteration 42743, loss = 2.43669879\n",
      "Iteration 42744, loss = 2.58523214\n",
      "Iteration 42745, loss = 2.09251055\n",
      "Iteration 42746, loss = 1.95161223\n",
      "Iteration 42747, loss = 2.38678876\n",
      "Iteration 42748, loss = 1.87562386\n",
      "Iteration 42749, loss = 2.07557627\n",
      "Iteration 42750, loss = 2.00857498\n",
      "Iteration 42751, loss = 1.98885001\n",
      "Iteration 42752, loss = 2.50997851\n",
      "Iteration 42753, loss = 2.01695328\n",
      "Iteration 42754, loss = 1.99765797\n",
      "Iteration 42755, loss = 1.76483600\n",
      "Iteration 42756, loss = 1.86986927\n",
      "Iteration 42757, loss = 1.80348550\n",
      "Iteration 42758, loss = 1.92238449\n",
      "Iteration 42759, loss = 2.42540834\n",
      "Iteration 42760, loss = 2.56218704\n",
      "Iteration 42761, loss = 2.26788102\n",
      "Iteration 42762, loss = 3.26560784\n",
      "Iteration 42763, loss = 2.82813819\n",
      "Iteration 42764, loss = 2.24901465\n",
      "Iteration 42765, loss = 2.19524363\n",
      "Iteration 42766, loss = 2.08973203\n",
      "Iteration 42767, loss = 1.99034691\n",
      "Iteration 42768, loss = 2.04437132\n",
      "Iteration 42769, loss = 2.07470471\n",
      "Iteration 42770, loss = 1.92967698\n",
      "Iteration 42771, loss = 2.03523426\n",
      "Iteration 42772, loss = 2.21115316\n",
      "Iteration 42773, loss = 1.99841305\n",
      "Iteration 42774, loss = 1.90851019\n",
      "Iteration 42775, loss = 2.06287041\n",
      "Iteration 42776, loss = 2.14361578\n",
      "Iteration 42777, loss = 2.41873314\n",
      "Iteration 42778, loss = 1.93658772\n",
      "Iteration 42779, loss = 1.99355776\n",
      "Iteration 42780, loss = 1.79104647\n",
      "Iteration 42781, loss = 1.78742487\n",
      "Iteration 42782, loss = 1.93743581\n",
      "Iteration 42783, loss = 1.98027123\n",
      "Iteration 42784, loss = 2.10507832\n",
      "Iteration 42785, loss = 1.99894151\n",
      "Iteration 42786, loss = 2.13261508\n",
      "Iteration 42787, loss = 2.18847953\n",
      "Iteration 42788, loss = 1.86888754\n",
      "Iteration 42789, loss = 2.00490449\n",
      "Iteration 42790, loss = 1.89611496\n",
      "Iteration 42791, loss = 2.35929252\n",
      "Iteration 42792, loss = 2.70254081\n",
      "Iteration 42793, loss = 2.06591045\n",
      "Iteration 42794, loss = 2.13135043\n",
      "Iteration 42795, loss = 2.89016805\n",
      "Iteration 42796, loss = 2.72684709\n",
      "Iteration 42797, loss = 2.71819957\n",
      "Iteration 42798, loss = 2.58382023\n",
      "Iteration 42799, loss = 2.31063363\n",
      "Iteration 42800, loss = 2.15684105\n",
      "Iteration 42801, loss = 2.07261395\n",
      "Iteration 42802, loss = 2.08108587\n",
      "Iteration 42803, loss = 2.77840124\n",
      "Iteration 42804, loss = 3.60465645\n",
      "Iteration 42805, loss = 2.65683083\n",
      "Iteration 42806, loss = 2.43584037\n",
      "Iteration 42807, loss = 2.14692116\n",
      "Iteration 42808, loss = 2.43027470\n",
      "Iteration 42809, loss = 2.11057187\n",
      "Iteration 42810, loss = 2.85972946\n",
      "Iteration 42811, loss = 2.97289952\n",
      "Iteration 42812, loss = 2.65163803\n",
      "Iteration 42813, loss = 2.63862055\n",
      "Iteration 42814, loss = 2.38139561\n",
      "Iteration 42815, loss = 2.27466669\n",
      "Iteration 42816, loss = 2.34414930\n",
      "Iteration 42817, loss = 2.98785250\n",
      "Iteration 42818, loss = 2.86513027\n",
      "Iteration 42819, loss = 2.86037043\n",
      "Iteration 42820, loss = 2.59066421\n",
      "Iteration 42821, loss = 3.00484072\n",
      "Iteration 42822, loss = 2.31155113\n",
      "Iteration 42823, loss = 2.14151358\n",
      "Iteration 42824, loss = 1.94435195\n",
      "Iteration 42825, loss = 1.61943052\n",
      "Iteration 42826, loss = 1.89053912\n",
      "Iteration 42827, loss = 2.13241979\n",
      "Iteration 42828, loss = 2.54082891\n",
      "Iteration 42829, loss = 2.24816235\n",
      "Iteration 42830, loss = 1.90267761\n",
      "Iteration 42831, loss = 2.04296639\n",
      "Iteration 42832, loss = 1.94471610\n",
      "Iteration 42833, loss = 2.22877480\n",
      "Iteration 42834, loss = 2.35440148\n",
      "Iteration 42835, loss = 1.89740842\n",
      "Iteration 42836, loss = 1.91644278\n",
      "Iteration 42837, loss = 1.79884018\n",
      "Iteration 42838, loss = 1.73212808\n",
      "Iteration 42839, loss = 1.61270709\n",
      "Iteration 42840, loss = 1.60694120\n",
      "Iteration 42841, loss = 1.87989719\n",
      "Iteration 42842, loss = 1.95016925\n",
      "Iteration 42843, loss = 1.73215141\n",
      "Iteration 42844, loss = 1.69695037\n",
      "Iteration 42845, loss = 1.98281270\n",
      "Iteration 42846, loss = 1.95642749\n",
      "Iteration 42847, loss = 2.79752823\n",
      "Iteration 42848, loss = 2.80632496\n",
      "Iteration 42849, loss = 2.37793098\n",
      "Iteration 42850, loss = 2.03697446\n",
      "Iteration 42851, loss = 2.73413812\n",
      "Iteration 42852, loss = 2.24462337\n",
      "Iteration 42853, loss = 2.74959259\n",
      "Iteration 42854, loss = 2.44420177\n",
      "Iteration 42855, loss = 2.47119435\n",
      "Iteration 42856, loss = 2.62867816\n",
      "Iteration 42857, loss = 2.36486925\n",
      "Iteration 42858, loss = 2.63394258\n",
      "Iteration 42859, loss = 2.42842737\n",
      "Iteration 42860, loss = 2.16434963\n",
      "Iteration 42861, loss = 2.19834047\n",
      "Iteration 42862, loss = 2.13395025\n",
      "Iteration 42863, loss = 1.71568467\n",
      "Iteration 42864, loss = 2.20129894\n",
      "Iteration 42865, loss = 2.57340445\n",
      "Iteration 42866, loss = 2.81776198\n",
      "Iteration 42867, loss = 2.61437951\n",
      "Iteration 42868, loss = 2.73995858\n",
      "Iteration 42869, loss = 2.70564515\n",
      "Iteration 42870, loss = 2.81390690\n",
      "Iteration 42871, loss = 2.40926890\n",
      "Iteration 42872, loss = 2.25874658\n",
      "Iteration 42873, loss = 2.16290391\n",
      "Iteration 42874, loss = 1.83343328\n",
      "Iteration 42875, loss = 1.97122933\n",
      "Iteration 42876, loss = 2.05129226\n",
      "Iteration 42877, loss = 2.25841916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 42878, loss = 2.63175198\n",
      "Iteration 42879, loss = 2.66558927\n",
      "Iteration 42880, loss = 3.44551586\n",
      "Iteration 42881, loss = 3.07629033\n",
      "Iteration 42882, loss = 3.45013698\n",
      "Iteration 42883, loss = 2.54683465\n",
      "Iteration 42884, loss = 1.95135595\n",
      "Iteration 42885, loss = 1.81692585\n",
      "Iteration 42886, loss = 1.90141843\n",
      "Iteration 42887, loss = 2.13968260\n",
      "Iteration 42888, loss = 2.07725950\n",
      "Iteration 42889, loss = 2.54680094\n",
      "Iteration 42890, loss = 2.74550582\n",
      "Iteration 42891, loss = 2.29686541\n",
      "Iteration 42892, loss = 1.88649947\n",
      "Iteration 42893, loss = 2.69833634\n",
      "Iteration 42894, loss = 2.15773516\n",
      "Iteration 42895, loss = 2.00786195\n",
      "Iteration 42896, loss = 1.85739287\n",
      "Iteration 42897, loss = 1.90183316\n",
      "Iteration 42898, loss = 2.02089212\n",
      "Iteration 42899, loss = 2.75930930\n",
      "Iteration 42900, loss = 2.42365032\n",
      "Iteration 42901, loss = 2.49296936\n",
      "Iteration 42902, loss = 2.61803418\n",
      "Iteration 42903, loss = 1.83779553\n",
      "Iteration 42904, loss = 2.01799557\n",
      "Iteration 42905, loss = 2.51360994\n",
      "Iteration 42906, loss = 2.13357568\n",
      "Iteration 42907, loss = 1.86324906\n",
      "Iteration 42908, loss = 1.86195288\n",
      "Iteration 42909, loss = 1.62945096\n",
      "Iteration 42910, loss = 1.69470330\n",
      "Iteration 42911, loss = 1.75704908\n",
      "Iteration 42912, loss = 2.36538642\n",
      "Iteration 42913, loss = 2.00399908\n",
      "Iteration 42914, loss = 1.77456904\n",
      "Iteration 42915, loss = 2.06722076\n",
      "Iteration 42916, loss = 2.16101069\n",
      "Iteration 42917, loss = 2.06028822\n",
      "Iteration 42918, loss = 2.13835953\n",
      "Iteration 42919, loss = 2.00292547\n",
      "Iteration 42920, loss = 2.50793245\n",
      "Iteration 42921, loss = 2.35242930\n",
      "Iteration 42922, loss = 2.23583836\n",
      "Iteration 42923, loss = 2.03657552\n",
      "Iteration 42924, loss = 2.43440132\n",
      "Iteration 42925, loss = 2.02752223\n",
      "Iteration 42926, loss = 2.05768233\n",
      "Iteration 42927, loss = 2.01818842\n",
      "Iteration 42928, loss = 2.41488242\n",
      "Iteration 42929, loss = 2.50569715\n",
      "Iteration 42930, loss = 2.13591859\n",
      "Iteration 42931, loss = 1.95897156\n",
      "Iteration 42932, loss = 1.99778789\n",
      "Iteration 42933, loss = 2.12013115\n",
      "Iteration 42934, loss = 2.10893975\n",
      "Iteration 42935, loss = 2.07304779\n",
      "Iteration 42936, loss = 2.85833111\n",
      "Iteration 42937, loss = 2.66014014\n",
      "Iteration 42938, loss = 2.61621558\n",
      "Iteration 42939, loss = 2.37431417\n",
      "Iteration 42940, loss = 3.35989597\n",
      "Iteration 42941, loss = 3.30732156\n",
      "Iteration 42942, loss = 2.33956514\n",
      "Iteration 42943, loss = 2.14405558\n",
      "Iteration 42944, loss = 2.07044027\n",
      "Iteration 42945, loss = 2.16223544\n",
      "Iteration 42946, loss = 2.10585852\n",
      "Iteration 42947, loss = 1.93002648\n",
      "Iteration 42948, loss = 1.91330902\n",
      "Iteration 42949, loss = 1.98194883\n",
      "Iteration 42950, loss = 2.04163755\n",
      "Iteration 42951, loss = 2.03153446\n",
      "Iteration 42952, loss = 2.06835438\n",
      "Iteration 42953, loss = 1.92599029\n",
      "Iteration 42954, loss = 2.00260937\n",
      "Iteration 42955, loss = 1.73480031\n",
      "Iteration 42956, loss = 1.92359705\n",
      "Iteration 42957, loss = 1.75036302\n",
      "Iteration 42958, loss = 1.61353720\n",
      "Iteration 42959, loss = 2.00340110\n",
      "Iteration 42960, loss = 2.51375576\n",
      "Iteration 42961, loss = 2.25122271\n",
      "Iteration 42962, loss = 2.25837521\n",
      "Iteration 42963, loss = 1.89524295\n",
      "Iteration 42964, loss = 1.77143715\n",
      "Iteration 42965, loss = 1.72273393\n",
      "Iteration 42966, loss = 2.06062384\n",
      "Iteration 42967, loss = 2.07900247\n",
      "Iteration 42968, loss = 1.67961699\n",
      "Iteration 42969, loss = 1.59333780\n",
      "Iteration 42970, loss = 1.66356642\n",
      "Iteration 42971, loss = 1.61851118\n",
      "Iteration 42972, loss = 1.73229422\n",
      "Iteration 42973, loss = 2.10617673\n",
      "Iteration 42974, loss = 3.12963166\n",
      "Iteration 42975, loss = 3.01833844\n",
      "Iteration 42976, loss = 2.39170946\n",
      "Iteration 42977, loss = 2.36745438\n",
      "Iteration 42978, loss = 2.52745503\n",
      "Iteration 42979, loss = 2.20376648\n",
      "Iteration 42980, loss = 2.14823966\n",
      "Iteration 42981, loss = 2.24197591\n",
      "Iteration 42982, loss = 2.17540311\n",
      "Iteration 42983, loss = 2.10361387\n",
      "Iteration 42984, loss = 2.02867809\n",
      "Iteration 42985, loss = 1.93927915\n",
      "Iteration 42986, loss = 2.02624047\n",
      "Iteration 42987, loss = 1.92615697\n",
      "Iteration 42988, loss = 1.91015756\n",
      "Iteration 42989, loss = 1.73731195\n",
      "Iteration 42990, loss = 1.68084109\n",
      "Iteration 42991, loss = 1.82136954\n",
      "Iteration 42992, loss = 1.59044058\n",
      "Iteration 42993, loss = 1.71981883\n",
      "Iteration 42994, loss = 1.85448720\n",
      "Iteration 42995, loss = 1.89088009\n",
      "Iteration 42996, loss = 1.78535233\n",
      "Iteration 42997, loss = 2.24052393\n",
      "Iteration 42998, loss = 1.97223465\n",
      "Iteration 42999, loss = 1.96313412\n",
      "Iteration 43000, loss = 1.85243430\n",
      "Iteration 43001, loss = 1.93787831\n",
      "Iteration 43002, loss = 2.14377483\n",
      "Iteration 43003, loss = 1.96710506\n",
      "Iteration 43004, loss = 2.15339538\n",
      "Iteration 43005, loss = 2.47911947\n",
      "Iteration 43006, loss = 3.73344784\n",
      "Iteration 43007, loss = 4.10000083\n",
      "Iteration 43008, loss = 2.99616948\n",
      "Iteration 43009, loss = 2.60564004\n",
      "Iteration 43010, loss = 2.68477098\n",
      "Iteration 43011, loss = 2.68192680\n",
      "Iteration 43012, loss = 2.45714599\n",
      "Iteration 43013, loss = 2.58910807\n",
      "Iteration 43014, loss = 3.26905236\n",
      "Iteration 43015, loss = 2.67994315\n",
      "Iteration 43016, loss = 1.96748655\n",
      "Iteration 43017, loss = 1.64187227\n",
      "Iteration 43018, loss = 2.08188722\n",
      "Iteration 43019, loss = 1.94468682\n",
      "Iteration 43020, loss = 2.37596625\n",
      "Iteration 43021, loss = 2.12496515\n",
      "Iteration 43022, loss = 1.94607492\n",
      "Iteration 43023, loss = 2.14704154\n",
      "Iteration 43024, loss = 1.96166688\n",
      "Iteration 43025, loss = 1.63748900\n",
      "Iteration 43026, loss = 2.17557560\n",
      "Iteration 43027, loss = 2.78305293\n",
      "Iteration 43028, loss = 2.49002526\n",
      "Iteration 43029, loss = 2.01513828\n",
      "Iteration 43030, loss = 1.78355707\n",
      "Iteration 43031, loss = 1.80204320\n",
      "Iteration 43032, loss = 2.35343386\n",
      "Iteration 43033, loss = 2.35989788\n",
      "Iteration 43034, loss = 1.82027599\n",
      "Iteration 43035, loss = 1.74156520\n",
      "Iteration 43036, loss = 2.02488425\n",
      "Iteration 43037, loss = 2.26118787\n",
      "Iteration 43038, loss = 2.67594042\n",
      "Iteration 43039, loss = 2.29843376\n",
      "Iteration 43040, loss = 2.43209346\n",
      "Iteration 43041, loss = 2.20350905\n",
      "Iteration 43042, loss = 2.47116618\n",
      "Iteration 43043, loss = 2.69237887\n",
      "Iteration 43044, loss = 2.28830316\n",
      "Iteration 43045, loss = 2.34362282\n",
      "Iteration 43046, loss = 2.58409571\n",
      "Iteration 43047, loss = 2.46533022\n",
      "Iteration 43048, loss = 2.33883251\n",
      "Iteration 43049, loss = 1.96098920\n",
      "Iteration 43050, loss = 1.78236441\n",
      "Iteration 43051, loss = 2.68040551\n",
      "Iteration 43052, loss = 2.61056946\n",
      "Iteration 43053, loss = 2.64356894\n",
      "Iteration 43054, loss = 2.49362437\n",
      "Iteration 43055, loss = 2.96490719\n",
      "Iteration 43056, loss = 2.90391545\n",
      "Iteration 43057, loss = 2.33508710\n",
      "Iteration 43058, loss = 2.35644694\n",
      "Iteration 43059, loss = 2.12306120\n",
      "Iteration 43060, loss = 2.15872688\n",
      "Iteration 43061, loss = 2.02008834\n",
      "Iteration 43062, loss = 1.65729523\n",
      "Iteration 43063, loss = 1.79544807\n",
      "Iteration 43064, loss = 1.71173140\n",
      "Iteration 43065, loss = 1.71674187\n",
      "Iteration 43066, loss = 1.58262665\n",
      "Iteration 43067, loss = 1.59056240\n",
      "Iteration 43068, loss = 1.66359821\n",
      "Iteration 43069, loss = 1.82030776\n",
      "Iteration 43070, loss = 2.02951162\n",
      "Iteration 43071, loss = 2.03484887\n",
      "Iteration 43072, loss = 1.85171629\n",
      "Iteration 43073, loss = 1.77375096\n",
      "Iteration 43074, loss = 1.87241238\n",
      "Iteration 43075, loss = 2.20092493\n",
      "Iteration 43076, loss = 2.74945616\n",
      "Iteration 43077, loss = 2.14379390\n",
      "Iteration 43078, loss = 2.63425375\n",
      "Iteration 43079, loss = 2.55642279\n",
      "Iteration 43080, loss = 2.63108160\n",
      "Iteration 43081, loss = 2.01562732\n",
      "Iteration 43082, loss = 2.01101871\n",
      "Iteration 43083, loss = 2.34899446\n",
      "Iteration 43084, loss = 2.17210960\n",
      "Iteration 43085, loss = 2.15301542\n",
      "Iteration 43086, loss = 2.57721485\n",
      "Iteration 43087, loss = 2.80003434\n",
      "Iteration 43088, loss = 1.85334775\n",
      "Iteration 43089, loss = 2.18790176\n",
      "Iteration 43090, loss = 1.94240682\n",
      "Iteration 43091, loss = 2.00745820\n",
      "Iteration 43092, loss = 1.79426574\n",
      "Iteration 43093, loss = 1.81830094\n",
      "Iteration 43094, loss = 1.55061446\n",
      "Iteration 43095, loss = 1.79371822\n",
      "Iteration 43096, loss = 1.92192770\n",
      "Iteration 43097, loss = 1.87877828\n",
      "Iteration 43098, loss = 1.94132957\n",
      "Iteration 43099, loss = 2.01663901\n",
      "Iteration 43100, loss = 2.03338423\n",
      "Iteration 43101, loss = 1.89077070\n",
      "Iteration 43102, loss = 1.88741360\n",
      "Iteration 43103, loss = 2.06007346\n",
      "Iteration 43104, loss = 1.85294329\n",
      "Iteration 43105, loss = 1.82733269\n",
      "Iteration 43106, loss = 1.82715876\n",
      "Iteration 43107, loss = 1.69910195\n",
      "Iteration 43108, loss = 1.63479722\n",
      "Iteration 43109, loss = 1.78892133\n",
      "Iteration 43110, loss = 2.13421614\n",
      "Iteration 43111, loss = 2.20951804\n",
      "Iteration 43112, loss = 2.07993721\n",
      "Iteration 43113, loss = 1.85310960\n",
      "Iteration 43114, loss = 1.65450523\n",
      "Iteration 43115, loss = 1.72756303\n",
      "Iteration 43116, loss = 1.94796988\n",
      "Iteration 43117, loss = 1.98097430\n",
      "Iteration 43118, loss = 1.96527336\n",
      "Iteration 43119, loss = 2.80962168\n",
      "Iteration 43120, loss = 2.56520237\n",
      "Iteration 43121, loss = 2.06060473\n",
      "Iteration 43122, loss = 2.91371363\n",
      "Iteration 43123, loss = 2.07309492\n",
      "Iteration 43124, loss = 2.16598942\n",
      "Iteration 43125, loss = 2.00081900\n",
      "Iteration 43126, loss = 1.74661804\n",
      "Iteration 43127, loss = 1.68924660\n",
      "Iteration 43128, loss = 1.80560205\n",
      "Iteration 43129, loss = 1.68347212\n",
      "Iteration 43130, loss = 1.59415783\n",
      "Iteration 43131, loss = 1.64033292\n",
      "Iteration 43132, loss = 1.65806044\n",
      "Iteration 43133, loss = 1.82204303\n",
      "Iteration 43134, loss = 2.21428876\n",
      "Iteration 43135, loss = 2.26919678\n",
      "Iteration 43136, loss = 2.07240033\n",
      "Iteration 43137, loss = 2.18004564\n",
      "Iteration 43138, loss = 2.22742603\n",
      "Iteration 43139, loss = 2.96321668\n",
      "Iteration 43140, loss = 3.80053947\n",
      "Iteration 43141, loss = 3.12244632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 43142, loss = 3.18912269\n",
      "Iteration 43143, loss = 2.39337328\n",
      "Iteration 43144, loss = 2.72481341\n",
      "Iteration 43145, loss = 2.85851619\n",
      "Iteration 43146, loss = 2.59966066\n",
      "Iteration 43147, loss = 2.15771324\n",
      "Iteration 43148, loss = 1.99234189\n",
      "Iteration 43149, loss = 1.71352343\n",
      "Iteration 43150, loss = 1.67847131\n",
      "Iteration 43151, loss = 1.77004455\n",
      "Iteration 43152, loss = 1.85066305\n",
      "Iteration 43153, loss = 2.08318327\n",
      "Iteration 43154, loss = 2.11543001\n",
      "Iteration 43155, loss = 2.44890960\n",
      "Iteration 43156, loss = 2.27781586\n",
      "Iteration 43157, loss = 2.07996980\n",
      "Iteration 43158, loss = 1.80820557\n",
      "Iteration 43159, loss = 2.05365908\n",
      "Iteration 43160, loss = 1.94144157\n",
      "Iteration 43161, loss = 1.94661772\n",
      "Iteration 43162, loss = 1.76191573\n",
      "Iteration 43163, loss = 1.74101107\n",
      "Iteration 43164, loss = 1.71882156\n",
      "Iteration 43165, loss = 2.55959184\n",
      "Iteration 43166, loss = 3.40621769\n",
      "Iteration 43167, loss = 2.93768247\n",
      "Iteration 43168, loss = 3.69764807\n",
      "Iteration 43169, loss = 2.56602834\n",
      "Iteration 43170, loss = 3.84747141\n",
      "Iteration 43171, loss = 3.12045131\n",
      "Iteration 43172, loss = 2.46604451\n",
      "Iteration 43173, loss = 2.17301311\n",
      "Iteration 43174, loss = 2.64949514\n",
      "Iteration 43175, loss = 1.86912246\n",
      "Iteration 43176, loss = 1.69636298\n",
      "Iteration 43177, loss = 1.62761124\n",
      "Iteration 43178, loss = 1.99767922\n",
      "Iteration 43179, loss = 1.77861630\n",
      "Iteration 43180, loss = 1.69870839\n",
      "Iteration 43181, loss = 1.76665405\n",
      "Iteration 43182, loss = 1.79379652\n",
      "Iteration 43183, loss = 1.57434078\n",
      "Iteration 43184, loss = 1.83310401\n",
      "Iteration 43185, loss = 2.28859654\n",
      "Iteration 43186, loss = 3.26439464\n",
      "Iteration 43187, loss = 2.96216343\n",
      "Iteration 43188, loss = 3.50423144\n",
      "Iteration 43189, loss = 3.30345007\n",
      "Iteration 43190, loss = 2.57697743\n",
      "Iteration 43191, loss = 2.30997352\n",
      "Iteration 43192, loss = 2.04793011\n",
      "Iteration 43193, loss = 2.06610248\n",
      "Iteration 43194, loss = 2.69605029\n",
      "Iteration 43195, loss = 2.47067195\n",
      "Iteration 43196, loss = 2.06525482\n",
      "Iteration 43197, loss = 1.85315848\n",
      "Iteration 43198, loss = 2.05920575\n",
      "Iteration 43199, loss = 2.12412485\n",
      "Iteration 43200, loss = 1.88603297\n",
      "Iteration 43201, loss = 1.64383690\n",
      "Iteration 43202, loss = 1.79931706\n",
      "Iteration 43203, loss = 2.04199111\n",
      "Iteration 43204, loss = 2.20821391\n",
      "Iteration 43205, loss = 2.22320217\n",
      "Iteration 43206, loss = 2.62077786\n",
      "Iteration 43207, loss = 2.45026279\n",
      "Iteration 43208, loss = 2.62788538\n",
      "Iteration 43209, loss = 1.99987439\n",
      "Iteration 43210, loss = 2.53883398\n",
      "Iteration 43211, loss = 3.19662942\n",
      "Iteration 43212, loss = 3.11774920\n",
      "Iteration 43213, loss = 1.95604604\n",
      "Iteration 43214, loss = 1.94351634\n",
      "Iteration 43215, loss = 2.10457459\n",
      "Iteration 43216, loss = 1.85542250\n",
      "Iteration 43217, loss = 1.83592067\n",
      "Iteration 43218, loss = 1.76605561\n",
      "Iteration 43219, loss = 1.88237682\n",
      "Iteration 43220, loss = 1.86355988\n",
      "Iteration 43221, loss = 2.14136107\n",
      "Iteration 43222, loss = 2.19606447\n",
      "Iteration 43223, loss = 1.96566645\n",
      "Iteration 43224, loss = 1.68479764\n",
      "Iteration 43225, loss = 2.07335210\n",
      "Iteration 43226, loss = 2.18355650\n",
      "Iteration 43227, loss = 2.13990890\n",
      "Iteration 43228, loss = 2.63489223\n",
      "Iteration 43229, loss = 2.75403904\n",
      "Iteration 43230, loss = 2.17241299\n",
      "Iteration 43231, loss = 2.45948463\n",
      "Iteration 43232, loss = 2.12405696\n",
      "Iteration 43233, loss = 2.25725791\n",
      "Iteration 43234, loss = 2.07477211\n",
      "Iteration 43235, loss = 2.49914549\n",
      "Iteration 43236, loss = 2.14307146\n",
      "Iteration 43237, loss = 2.20413654\n",
      "Iteration 43238, loss = 2.74706504\n",
      "Iteration 43239, loss = 2.34036254\n",
      "Iteration 43240, loss = 2.42066133\n",
      "Iteration 43241, loss = 2.06619624\n",
      "Iteration 43242, loss = 1.95735505\n",
      "Iteration 43243, loss = 1.90849331\n",
      "Iteration 43244, loss = 1.74556442\n",
      "Iteration 43245, loss = 2.25461829\n",
      "Iteration 43246, loss = 2.44848904\n",
      "Iteration 43247, loss = 2.38469002\n",
      "Iteration 43248, loss = 2.38407384\n",
      "Iteration 43249, loss = 2.36927043\n",
      "Iteration 43250, loss = 1.99338577\n",
      "Iteration 43251, loss = 1.73027534\n",
      "Iteration 43252, loss = 1.92575561\n",
      "Iteration 43253, loss = 2.55726496\n",
      "Iteration 43254, loss = 2.76426451\n",
      "Iteration 43255, loss = 1.94967147\n",
      "Iteration 43256, loss = 2.07917055\n",
      "Iteration 43257, loss = 1.95807726\n",
      "Iteration 43258, loss = 1.89661563\n",
      "Iteration 43259, loss = 1.68429212\n",
      "Iteration 43260, loss = 1.65752937\n",
      "Iteration 43261, loss = 2.25566393\n",
      "Iteration 43262, loss = 2.02903622\n",
      "Iteration 43263, loss = 2.25362945\n",
      "Iteration 43264, loss = 2.40876492\n",
      "Iteration 43265, loss = 1.75971773\n",
      "Iteration 43266, loss = 1.59071278\n",
      "Iteration 43267, loss = 1.70764279\n",
      "Iteration 43268, loss = 1.82566284\n",
      "Iteration 43269, loss = 1.99055500\n",
      "Iteration 43270, loss = 2.00141480\n",
      "Iteration 43271, loss = 2.14275428\n",
      "Iteration 43272, loss = 1.84987129\n",
      "Iteration 43273, loss = 1.90089253\n",
      "Iteration 43274, loss = 2.63892109\n",
      "Iteration 43275, loss = 2.53787169\n",
      "Iteration 43276, loss = 2.00254130\n",
      "Iteration 43277, loss = 2.04741071\n",
      "Iteration 43278, loss = 1.72449041\n",
      "Iteration 43279, loss = 1.91327876\n",
      "Iteration 43280, loss = 1.73543898\n",
      "Iteration 43281, loss = 1.68865396\n",
      "Iteration 43282, loss = 1.92660789\n",
      "Iteration 43283, loss = 1.75255446\n",
      "Iteration 43284, loss = 2.08294964\n",
      "Iteration 43285, loss = 1.99071514\n",
      "Iteration 43286, loss = 1.75538011\n",
      "Iteration 43287, loss = 1.65059515\n",
      "Iteration 43288, loss = 1.61986835\n",
      "Iteration 43289, loss = 1.74580249\n",
      "Iteration 43290, loss = 2.27674109\n",
      "Iteration 43291, loss = 1.87737186\n",
      "Iteration 43292, loss = 2.92476876\n",
      "Iteration 43293, loss = 2.45041262\n",
      "Iteration 43294, loss = 3.23167663\n",
      "Iteration 43295, loss = 3.45367048\n",
      "Iteration 43296, loss = 3.23318666\n",
      "Iteration 43297, loss = 2.20223718\n",
      "Iteration 43298, loss = 2.13923039\n",
      "Iteration 43299, loss = 2.11182678\n",
      "Iteration 43300, loss = 2.27680423\n",
      "Iteration 43301, loss = 1.93721032\n",
      "Iteration 43302, loss = 1.68915769\n",
      "Iteration 43303, loss = 1.61619503\n",
      "Iteration 43304, loss = 1.65610686\n",
      "Iteration 43305, loss = 1.64621235\n",
      "Iteration 43306, loss = 1.63081829\n",
      "Iteration 43307, loss = 1.78252136\n",
      "Iteration 43308, loss = 1.89426287\n",
      "Iteration 43309, loss = 2.15575765\n",
      "Iteration 43310, loss = 2.14496955\n",
      "Iteration 43311, loss = 2.11322133\n",
      "Iteration 43312, loss = 2.10156237\n",
      "Iteration 43313, loss = 1.88558824\n",
      "Iteration 43314, loss = 3.01876953\n",
      "Iteration 43315, loss = 3.33786855\n",
      "Iteration 43316, loss = 2.29222850\n",
      "Iteration 43317, loss = 2.37249318\n",
      "Iteration 43318, loss = 1.96210813\n",
      "Iteration 43319, loss = 1.97370919\n",
      "Iteration 43320, loss = 1.96476551\n",
      "Iteration 43321, loss = 2.50865199\n",
      "Iteration 43322, loss = 2.32312633\n",
      "Iteration 43323, loss = 2.47347287\n",
      "Iteration 43324, loss = 2.10033650\n",
      "Iteration 43325, loss = 1.82262947\n",
      "Iteration 43326, loss = 2.60921541\n",
      "Iteration 43327, loss = 2.47710053\n",
      "Iteration 43328, loss = 2.40597084\n",
      "Iteration 43329, loss = 2.51879182\n",
      "Iteration 43330, loss = 2.21690162\n",
      "Iteration 43331, loss = 2.56690904\n",
      "Iteration 43332, loss = 2.22201368\n",
      "Iteration 43333, loss = 3.27877911\n",
      "Iteration 43334, loss = 3.58206833\n",
      "Iteration 43335, loss = 3.29354044\n",
      "Iteration 43336, loss = 3.66733737\n",
      "Iteration 43337, loss = 4.11369751\n",
      "Iteration 43338, loss = 3.17964364\n",
      "Iteration 43339, loss = 3.77385028\n",
      "Iteration 43340, loss = 3.83828338\n",
      "Iteration 43341, loss = 4.10097915\n",
      "Iteration 43342, loss = 3.65979306\n",
      "Iteration 43343, loss = 2.53277518\n",
      "Iteration 43344, loss = 2.60409457\n",
      "Iteration 43345, loss = 2.02125870\n",
      "Iteration 43346, loss = 1.92694842\n",
      "Iteration 43347, loss = 2.09339259\n",
      "Iteration 43348, loss = 2.03086101\n",
      "Iteration 43349, loss = 2.36823285\n",
      "Iteration 43350, loss = 3.05057784\n",
      "Iteration 43351, loss = 2.44494736\n",
      "Iteration 43352, loss = 2.56168348\n",
      "Iteration 43353, loss = 1.93707140\n",
      "Iteration 43354, loss = 2.10223405\n",
      "Iteration 43355, loss = 1.83673846\n",
      "Iteration 43356, loss = 1.98244467\n",
      "Iteration 43357, loss = 1.76412141\n",
      "Iteration 43358, loss = 1.57739483\n",
      "Iteration 43359, loss = 1.72936452\n",
      "Iteration 43360, loss = 1.81626815\n",
      "Iteration 43361, loss = 1.96124882\n",
      "Iteration 43362, loss = 1.85322162\n",
      "Iteration 43363, loss = 2.31235667\n",
      "Iteration 43364, loss = 2.37156993\n",
      "Iteration 43365, loss = 2.14684343\n",
      "Iteration 43366, loss = 2.05449082\n",
      "Iteration 43367, loss = 2.02621493\n",
      "Iteration 43368, loss = 2.05141623\n",
      "Iteration 43369, loss = 1.84883499\n",
      "Iteration 43370, loss = 1.88594099\n",
      "Iteration 43371, loss = 2.31536825\n",
      "Iteration 43372, loss = 2.46466139\n",
      "Iteration 43373, loss = 2.27984067\n",
      "Iteration 43374, loss = 2.90037217\n",
      "Iteration 43375, loss = 2.30164001\n",
      "Iteration 43376, loss = 2.14010474\n",
      "Iteration 43377, loss = 1.93566013\n",
      "Iteration 43378, loss = 1.89331548\n",
      "Iteration 43379, loss = 1.62201265\n",
      "Iteration 43380, loss = 1.83249824\n",
      "Iteration 43381, loss = 2.02342008\n",
      "Iteration 43382, loss = 2.23325556\n",
      "Iteration 43383, loss = 2.88089490\n",
      "Iteration 43384, loss = 2.49802111\n",
      "Iteration 43385, loss = 2.28972788\n",
      "Iteration 43386, loss = 2.31811465\n",
      "Iteration 43387, loss = 2.22017263\n",
      "Iteration 43388, loss = 1.77352757\n",
      "Iteration 43389, loss = 1.70885846\n",
      "Iteration 43390, loss = 1.71756907\n",
      "Iteration 43391, loss = 1.91559559\n",
      "Iteration 43392, loss = 1.97413490\n",
      "Iteration 43393, loss = 1.97677356\n",
      "Iteration 43394, loss = 1.94220504\n",
      "Iteration 43395, loss = 1.81549971\n",
      "Iteration 43396, loss = 2.12774800\n",
      "Iteration 43397, loss = 2.54332812\n",
      "Iteration 43398, loss = 2.34999329\n",
      "Iteration 43399, loss = 2.09242288\n",
      "Iteration 43400, loss = 2.04957551\n",
      "Iteration 43401, loss = 2.09531026\n",
      "Iteration 43402, loss = 2.91709565\n",
      "Iteration 43403, loss = 2.02392840\n",
      "Iteration 43404, loss = 2.46616003\n",
      "Iteration 43405, loss = 2.13772056\n",
      "Iteration 43406, loss = 2.03018938\n",
      "Iteration 43407, loss = 1.86243366\n",
      "Iteration 43408, loss = 1.78713803\n",
      "Iteration 43409, loss = 1.87973299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 43410, loss = 1.69414984\n",
      "Iteration 43411, loss = 1.83927895\n",
      "Iteration 43412, loss = 1.72438672\n",
      "Iteration 43413, loss = 2.12390560\n",
      "Iteration 43414, loss = 2.24079538\n",
      "Iteration 43415, loss = 2.13827510\n",
      "Iteration 43416, loss = 2.10968011\n",
      "Iteration 43417, loss = 2.18251859\n",
      "Iteration 43418, loss = 1.90011551\n",
      "Iteration 43419, loss = 1.79091833\n",
      "Iteration 43420, loss = 1.76736629\n",
      "Iteration 43421, loss = 1.97036514\n",
      "Iteration 43422, loss = 2.16304522\n",
      "Iteration 43423, loss = 1.96281510\n",
      "Iteration 43424, loss = 1.79216120\n",
      "Iteration 43425, loss = 1.78269352\n",
      "Iteration 43426, loss = 2.68771587\n",
      "Iteration 43427, loss = 2.16221860\n",
      "Iteration 43428, loss = 1.85529112\n",
      "Iteration 43429, loss = 1.75201181\n",
      "Iteration 43430, loss = 1.73104290\n",
      "Iteration 43431, loss = 1.79845987\n",
      "Iteration 43432, loss = 1.88547505\n",
      "Iteration 43433, loss = 1.73635647\n",
      "Iteration 43434, loss = 1.83626344\n",
      "Iteration 43435, loss = 1.74208460\n",
      "Iteration 43436, loss = 1.87484610\n",
      "Iteration 43437, loss = 2.01706372\n",
      "Iteration 43438, loss = 1.93499041\n",
      "Iteration 43439, loss = 2.28047670\n",
      "Iteration 43440, loss = 2.36535928\n",
      "Iteration 43441, loss = 2.42207082\n",
      "Iteration 43442, loss = 1.97858961\n",
      "Iteration 43443, loss = 1.77727803\n",
      "Iteration 43444, loss = 2.01629234\n",
      "Iteration 43445, loss = 2.22072569\n",
      "Iteration 43446, loss = 1.69876009\n",
      "Iteration 43447, loss = 1.56590984\n",
      "Iteration 43448, loss = 1.61985867\n",
      "Iteration 43449, loss = 1.53663415\n",
      "Iteration 43450, loss = 1.89297531\n",
      "Iteration 43451, loss = 1.74696811\n",
      "Iteration 43452, loss = 1.68314184\n",
      "Iteration 43453, loss = 1.65642655\n",
      "Iteration 43454, loss = 1.71208680\n",
      "Iteration 43455, loss = 1.65928463\n",
      "Iteration 43456, loss = 1.90307168\n",
      "Iteration 43457, loss = 1.87525008\n",
      "Iteration 43458, loss = 1.97767981\n",
      "Iteration 43459, loss = 2.38347137\n",
      "Iteration 43460, loss = 2.16186929\n",
      "Iteration 43461, loss = 1.77176873\n",
      "Iteration 43462, loss = 1.64468436\n",
      "Iteration 43463, loss = 1.72507637\n",
      "Iteration 43464, loss = 2.12079345\n",
      "Iteration 43465, loss = 2.46000420\n",
      "Iteration 43466, loss = 3.27345330\n",
      "Iteration 43467, loss = 2.43861909\n",
      "Iteration 43468, loss = 1.92525633\n",
      "Iteration 43469, loss = 1.76700711\n",
      "Iteration 43470, loss = 1.70673647\n",
      "Iteration 43471, loss = 1.81461268\n",
      "Iteration 43472, loss = 1.77232564\n",
      "Iteration 43473, loss = 1.82661995\n",
      "Iteration 43474, loss = 1.69992611\n",
      "Iteration 43475, loss = 2.06736835\n",
      "Iteration 43476, loss = 2.45515163\n",
      "Iteration 43477, loss = 3.75893155\n",
      "Iteration 43478, loss = 4.67504498\n",
      "Iteration 43479, loss = 2.49538952\n",
      "Iteration 43480, loss = 2.16119016\n",
      "Iteration 43481, loss = 2.56270838\n",
      "Iteration 43482, loss = 2.36243317\n",
      "Iteration 43483, loss = 2.47387454\n",
      "Iteration 43484, loss = 2.50758090\n",
      "Iteration 43485, loss = 2.19220664\n",
      "Iteration 43486, loss = 2.58246288\n",
      "Iteration 43487, loss = 2.02273627\n",
      "Iteration 43488, loss = 2.04456904\n",
      "Iteration 43489, loss = 2.93372732\n",
      "Iteration 43490, loss = 3.49097336\n",
      "Iteration 43491, loss = 2.95108945\n",
      "Iteration 43492, loss = 2.52969348\n",
      "Iteration 43493, loss = 2.64493827\n",
      "Iteration 43494, loss = 2.48374051\n",
      "Iteration 43495, loss = 2.59620123\n",
      "Iteration 43496, loss = 1.75839346\n",
      "Iteration 43497, loss = 1.66203690\n",
      "Iteration 43498, loss = 1.58810569\n",
      "Iteration 43499, loss = 1.64131985\n",
      "Iteration 43500, loss = 1.89227024\n",
      "Iteration 43501, loss = 1.66197591\n",
      "Iteration 43502, loss = 1.72108777\n",
      "Iteration 43503, loss = 1.64912387\n",
      "Iteration 43504, loss = 1.83589094\n",
      "Iteration 43505, loss = 1.84246455\n",
      "Iteration 43506, loss = 2.01634836\n",
      "Iteration 43507, loss = 1.77527932\n",
      "Iteration 43508, loss = 1.68903639\n",
      "Iteration 43509, loss = 1.65495303\n",
      "Iteration 43510, loss = 1.70707144\n",
      "Iteration 43511, loss = 1.63504165\n",
      "Iteration 43512, loss = 2.15636502\n",
      "Iteration 43513, loss = 2.36739880\n",
      "Iteration 43514, loss = 2.08908799\n",
      "Iteration 43515, loss = 2.35612258\n",
      "Iteration 43516, loss = 1.97074752\n",
      "Iteration 43517, loss = 1.80535617\n",
      "Iteration 43518, loss = 1.96100646\n",
      "Iteration 43519, loss = 1.89157977\n",
      "Iteration 43520, loss = 2.08756185\n",
      "Iteration 43521, loss = 2.20780745\n",
      "Iteration 43522, loss = 2.87834007\n",
      "Iteration 43523, loss = 3.55921629\n",
      "Iteration 43524, loss = 3.19216389\n",
      "Iteration 43525, loss = 3.00981574\n",
      "Iteration 43526, loss = 2.76556955\n",
      "Iteration 43527, loss = 2.72626865\n",
      "Iteration 43528, loss = 3.43182668\n",
      "Iteration 43529, loss = 2.43648315\n",
      "Iteration 43530, loss = 1.80625198\n",
      "Iteration 43531, loss = 2.12061806\n",
      "Iteration 43532, loss = 2.12992770\n",
      "Iteration 43533, loss = 3.14230182\n",
      "Iteration 43534, loss = 4.24036105\n",
      "Iteration 43535, loss = 3.78494147\n",
      "Iteration 43536, loss = 2.85770110\n",
      "Iteration 43537, loss = 1.78267228\n",
      "Iteration 43538, loss = 1.97537344\n",
      "Iteration 43539, loss = 1.85703093\n",
      "Iteration 43540, loss = 2.08528219\n",
      "Iteration 43541, loss = 1.99938865\n",
      "Iteration 43542, loss = 1.81874787\n",
      "Iteration 43543, loss = 1.95250416\n",
      "Iteration 43544, loss = 2.27253008\n",
      "Iteration 43545, loss = 2.37399187\n",
      "Iteration 43546, loss = 2.46860846\n",
      "Iteration 43547, loss = 3.23495437\n",
      "Iteration 43548, loss = 1.96452098\n",
      "Iteration 43549, loss = 2.33624740\n",
      "Iteration 43550, loss = 1.94537214\n",
      "Iteration 43551, loss = 2.02240349\n",
      "Iteration 43552, loss = 1.90593819\n",
      "Iteration 43553, loss = 1.99975915\n",
      "Iteration 43554, loss = 2.25285513\n",
      "Iteration 43555, loss = 2.02769809\n",
      "Iteration 43556, loss = 1.89756745\n",
      "Iteration 43557, loss = 1.82355745\n",
      "Iteration 43558, loss = 2.48590858\n",
      "Iteration 43559, loss = 3.31940953\n",
      "Iteration 43560, loss = 3.93502916\n",
      "Iteration 43561, loss = 3.26805998\n",
      "Iteration 43562, loss = 2.49530332\n",
      "Iteration 43563, loss = 2.32896707\n",
      "Iteration 43564, loss = 2.56896315\n",
      "Iteration 43565, loss = 2.55384478\n",
      "Iteration 43566, loss = 2.35339149\n",
      "Iteration 43567, loss = 2.01635623\n",
      "Iteration 43568, loss = 1.82755895\n",
      "Iteration 43569, loss = 1.86252202\n",
      "Iteration 43570, loss = 1.61646901\n",
      "Iteration 43571, loss = 1.63368065\n",
      "Iteration 43572, loss = 1.67562324\n",
      "Iteration 43573, loss = 1.84495448\n",
      "Iteration 43574, loss = 1.85369500\n",
      "Iteration 43575, loss = 1.71793530\n",
      "Iteration 43576, loss = 1.99539888\n",
      "Iteration 43577, loss = 1.88226152\n",
      "Iteration 43578, loss = 2.16725302\n",
      "Iteration 43579, loss = 2.12545055\n",
      "Iteration 43580, loss = 2.00610702\n",
      "Iteration 43581, loss = 1.54099752\n",
      "Iteration 43582, loss = 1.84618974\n",
      "Iteration 43583, loss = 1.72790230\n",
      "Iteration 43584, loss = 1.72504822\n",
      "Iteration 43585, loss = 1.72844670\n",
      "Iteration 43586, loss = 1.77517758\n",
      "Iteration 43587, loss = 2.04530036\n",
      "Iteration 43588, loss = 2.75274293\n",
      "Iteration 43589, loss = 2.22736094\n",
      "Iteration 43590, loss = 2.28086622\n",
      "Iteration 43591, loss = 2.33904428\n",
      "Iteration 43592, loss = 2.13725679\n",
      "Iteration 43593, loss = 1.69723706\n",
      "Iteration 43594, loss = 2.00386283\n",
      "Iteration 43595, loss = 1.63199569\n",
      "Iteration 43596, loss = 1.75160514\n",
      "Iteration 43597, loss = 1.88211302\n",
      "Iteration 43598, loss = 1.98553733\n",
      "Iteration 43599, loss = 1.79434025\n",
      "Iteration 43600, loss = 1.81133632\n",
      "Iteration 43601, loss = 1.64028898\n",
      "Iteration 43602, loss = 1.83724739\n",
      "Iteration 43603, loss = 1.80155615\n",
      "Iteration 43604, loss = 1.98547967\n",
      "Iteration 43605, loss = 2.00131337\n",
      "Iteration 43606, loss = 2.09804970\n",
      "Iteration 43607, loss = 1.87168987\n",
      "Iteration 43608, loss = 1.91473795\n",
      "Iteration 43609, loss = 1.53993862\n",
      "Iteration 43610, loss = 1.67217192\n",
      "Iteration 43611, loss = 2.15884471\n",
      "Iteration 43612, loss = 1.82670637\n",
      "Iteration 43613, loss = 1.68360512\n",
      "Iteration 43614, loss = 1.67010296\n",
      "Iteration 43615, loss = 1.55771768\n",
      "Iteration 43616, loss = 1.68158167\n",
      "Iteration 43617, loss = 1.68387685\n",
      "Iteration 43618, loss = 1.92184853\n",
      "Iteration 43619, loss = 1.86716374\n",
      "Iteration 43620, loss = 2.42561491\n",
      "Iteration 43621, loss = 1.99190028\n",
      "Iteration 43622, loss = 2.15662674\n",
      "Iteration 43623, loss = 1.75389555\n",
      "Iteration 43624, loss = 1.54522378\n",
      "Iteration 43625, loss = 1.70754498\n",
      "Iteration 43626, loss = 1.68011561\n",
      "Iteration 43627, loss = 1.59357400\n",
      "Iteration 43628, loss = 1.47491043\n",
      "Iteration 43629, loss = 1.53657976\n",
      "Iteration 43630, loss = 2.26382166\n",
      "Iteration 43631, loss = 2.47937423\n",
      "Iteration 43632, loss = 2.42112731\n",
      "Iteration 43633, loss = 1.97448913\n",
      "Iteration 43634, loss = 2.21184613\n",
      "Iteration 43635, loss = 1.95870606\n",
      "Iteration 43636, loss = 2.02577446\n",
      "Iteration 43637, loss = 2.24394182\n",
      "Iteration 43638, loss = 2.53591039\n",
      "Iteration 43639, loss = 2.53462221\n",
      "Iteration 43640, loss = 2.10287389\n",
      "Iteration 43641, loss = 1.91838779\n",
      "Iteration 43642, loss = 1.82916782\n",
      "Iteration 43643, loss = 1.69725081\n",
      "Iteration 43644, loss = 1.95442965\n",
      "Iteration 43645, loss = 1.87906340\n",
      "Iteration 43646, loss = 2.42069637\n",
      "Iteration 43647, loss = 2.14598520\n",
      "Iteration 43648, loss = 3.08503211\n",
      "Iteration 43649, loss = 4.34658196\n",
      "Iteration 43650, loss = 3.89983855\n",
      "Iteration 43651, loss = 2.73921466\n",
      "Iteration 43652, loss = 2.30006467\n",
      "Iteration 43653, loss = 2.59665104\n",
      "Iteration 43654, loss = 1.85576694\n",
      "Iteration 43655, loss = 1.87204428\n",
      "Iteration 43656, loss = 1.98683248\n",
      "Iteration 43657, loss = 2.17134683\n",
      "Iteration 43658, loss = 1.83815003\n",
      "Iteration 43659, loss = 1.84398724\n",
      "Iteration 43660, loss = 1.76811846\n",
      "Iteration 43661, loss = 1.79415592\n",
      "Iteration 43662, loss = 1.85483807\n",
      "Iteration 43663, loss = 1.81475502\n",
      "Iteration 43664, loss = 1.70198091\n",
      "Iteration 43665, loss = 1.78816665\n",
      "Iteration 43666, loss = 2.03051004\n",
      "Iteration 43667, loss = 2.18928551\n",
      "Iteration 43668, loss = 2.20885670\n",
      "Iteration 43669, loss = 1.98951633\n",
      "Iteration 43670, loss = 1.85888045\n",
      "Iteration 43671, loss = 1.70600725\n",
      "Iteration 43672, loss = 1.81565692\n",
      "Iteration 43673, loss = 1.82513602\n",
      "Iteration 43674, loss = 1.57515452\n",
      "Iteration 43675, loss = 1.69194069\n",
      "Iteration 43676, loss = 1.97698963\n",
      "Iteration 43677, loss = 2.00734107\n",
      "Iteration 43678, loss = 2.11172919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 43679, loss = 2.41069068\n",
      "Iteration 43680, loss = 2.98466364\n",
      "Iteration 43681, loss = 2.57388045\n",
      "Iteration 43682, loss = 2.45997574\n",
      "Iteration 43683, loss = 2.21545064\n",
      "Iteration 43684, loss = 2.21880003\n",
      "Iteration 43685, loss = 1.99749548\n",
      "Iteration 43686, loss = 2.02340335\n",
      "Iteration 43687, loss = 1.97234544\n",
      "Iteration 43688, loss = 2.06000204\n",
      "Iteration 43689, loss = 1.84090956\n",
      "Iteration 43690, loss = 2.22417240\n",
      "Iteration 43691, loss = 2.14594276\n",
      "Iteration 43692, loss = 1.85574901\n",
      "Iteration 43693, loss = 2.01397666\n",
      "Iteration 43694, loss = 2.06906294\n",
      "Iteration 43695, loss = 2.01252758\n",
      "Iteration 43696, loss = 1.87961351\n",
      "Iteration 43697, loss = 2.68224512\n",
      "Iteration 43698, loss = 2.74537952\n",
      "Iteration 43699, loss = 2.00083032\n",
      "Iteration 43700, loss = 2.28771881\n",
      "Iteration 43701, loss = 2.75920902\n",
      "Iteration 43702, loss = 2.04381856\n",
      "Iteration 43703, loss = 2.82867973\n",
      "Iteration 43704, loss = 2.47613726\n",
      "Iteration 43705, loss = 1.78734282\n",
      "Iteration 43706, loss = 1.75991078\n",
      "Iteration 43707, loss = 1.83279683\n",
      "Iteration 43708, loss = 2.00592548\n",
      "Iteration 43709, loss = 1.84746962\n",
      "Iteration 43710, loss = 2.37367763\n",
      "Iteration 43711, loss = 2.23167254\n",
      "Iteration 43712, loss = 2.22098134\n",
      "Iteration 43713, loss = 2.22612656\n",
      "Iteration 43714, loss = 2.89555653\n",
      "Iteration 43715, loss = 2.56449108\n",
      "Iteration 43716, loss = 3.14884699\n",
      "Iteration 43717, loss = 3.03840170\n",
      "Iteration 43718, loss = 4.00743613\n",
      "Iteration 43719, loss = 3.72225048\n",
      "Iteration 43720, loss = 2.04196610\n",
      "Iteration 43721, loss = 2.08752525\n",
      "Iteration 43722, loss = 1.73737492\n",
      "Iteration 43723, loss = 1.84932882\n",
      "Iteration 43724, loss = 1.98756986\n",
      "Iteration 43725, loss = 1.62518622\n",
      "Iteration 43726, loss = 1.63140755\n",
      "Iteration 43727, loss = 1.74825310\n",
      "Iteration 43728, loss = 1.57887685\n",
      "Iteration 43729, loss = 1.66829548\n",
      "Iteration 43730, loss = 1.68435405\n",
      "Iteration 43731, loss = 1.63324287\n",
      "Iteration 43732, loss = 1.72735176\n",
      "Iteration 43733, loss = 2.06588011\n",
      "Iteration 43734, loss = 2.01358688\n",
      "Iteration 43735, loss = 1.71783981\n",
      "Iteration 43736, loss = 1.87078175\n",
      "Iteration 43737, loss = 2.23492017\n",
      "Iteration 43738, loss = 1.94720890\n",
      "Iteration 43739, loss = 2.21166874\n",
      "Iteration 43740, loss = 2.29020396\n",
      "Iteration 43741, loss = 2.41695547\n",
      "Iteration 43742, loss = 1.72336320\n",
      "Iteration 43743, loss = 1.60154623\n",
      "Iteration 43744, loss = 1.62738369\n",
      "Iteration 43745, loss = 1.79523489\n",
      "Iteration 43746, loss = 2.36175916\n",
      "Iteration 43747, loss = 2.02627414\n",
      "Iteration 43748, loss = 2.05885347\n",
      "Iteration 43749, loss = 1.64058370\n",
      "Iteration 43750, loss = 1.79623657\n",
      "Iteration 43751, loss = 1.89853880\n",
      "Iteration 43752, loss = 2.05747988\n",
      "Iteration 43753, loss = 1.91505469\n",
      "Iteration 43754, loss = 1.78090665\n",
      "Iteration 43755, loss = 1.79337214\n",
      "Iteration 43756, loss = 1.74128729\n",
      "Iteration 43757, loss = 2.37846577\n",
      "Iteration 43758, loss = 1.90056802\n",
      "Iteration 43759, loss = 1.73449455\n",
      "Iteration 43760, loss = 1.99699642\n",
      "Iteration 43761, loss = 1.92451314\n",
      "Iteration 43762, loss = 1.82327674\n",
      "Iteration 43763, loss = 1.94827098\n",
      "Iteration 43764, loss = 2.03982944\n",
      "Iteration 43765, loss = 1.72549030\n",
      "Iteration 43766, loss = 1.65895191\n",
      "Iteration 43767, loss = 1.88985042\n",
      "Iteration 43768, loss = 1.77127454\n",
      "Iteration 43769, loss = 1.85158806\n",
      "Iteration 43770, loss = 1.63056282\n",
      "Iteration 43771, loss = 1.78487739\n",
      "Iteration 43772, loss = 1.72566122\n",
      "Iteration 43773, loss = 1.63798136\n",
      "Iteration 43774, loss = 2.04086674\n",
      "Iteration 43775, loss = 2.07206641\n",
      "Iteration 43776, loss = 1.90962357\n",
      "Iteration 43777, loss = 1.86863273\n",
      "Iteration 43778, loss = 2.46767929\n",
      "Iteration 43779, loss = 2.50354672\n",
      "Iteration 43780, loss = 1.98711701\n",
      "Iteration 43781, loss = 2.06596661\n",
      "Iteration 43782, loss = 2.01846552\n",
      "Iteration 43783, loss = 1.94951845\n",
      "Iteration 43784, loss = 1.70429320\n",
      "Iteration 43785, loss = 1.63932203\n",
      "Iteration 43786, loss = 1.61575059\n",
      "Iteration 43787, loss = 1.59779515\n",
      "Iteration 43788, loss = 1.69340987\n",
      "Iteration 43789, loss = 1.59430224\n",
      "Iteration 43790, loss = 1.65640097\n",
      "Iteration 43791, loss = 1.87808689\n",
      "Iteration 43792, loss = 2.30376548\n",
      "Iteration 43793, loss = 1.95742712\n",
      "Iteration 43794, loss = 1.80658267\n",
      "Iteration 43795, loss = 1.69321234\n",
      "Iteration 43796, loss = 1.80051567\n",
      "Iteration 43797, loss = 1.83303806\n",
      "Iteration 43798, loss = 1.71272385\n",
      "Iteration 43799, loss = 1.68000116\n",
      "Iteration 43800, loss = 1.86909171\n",
      "Iteration 43801, loss = 2.00074762\n",
      "Iteration 43802, loss = 1.76807551\n",
      "Iteration 43803, loss = 1.88188540\n",
      "Iteration 43804, loss = 1.82496148\n",
      "Iteration 43805, loss = 1.73825105\n",
      "Iteration 43806, loss = 1.72977083\n",
      "Iteration 43807, loss = 2.03956204\n",
      "Iteration 43808, loss = 1.61567846\n",
      "Iteration 43809, loss = 1.95482873\n",
      "Iteration 43810, loss = 1.78361644\n",
      "Iteration 43811, loss = 2.19631986\n",
      "Iteration 43812, loss = 1.86007107\n",
      "Iteration 43813, loss = 1.99466914\n",
      "Iteration 43814, loss = 1.65853668\n",
      "Iteration 43815, loss = 1.64550984\n",
      "Iteration 43816, loss = 1.58350476\n",
      "Iteration 43817, loss = 1.61889605\n",
      "Iteration 43818, loss = 1.70234266\n",
      "Iteration 43819, loss = 2.27912061\n",
      "Iteration 43820, loss = 2.61177765\n",
      "Iteration 43821, loss = 1.94600940\n",
      "Iteration 43822, loss = 2.50300389\n",
      "Iteration 43823, loss = 2.51199015\n",
      "Iteration 43824, loss = 1.87709328\n",
      "Iteration 43825, loss = 1.87638850\n",
      "Iteration 43826, loss = 2.54063382\n",
      "Iteration 43827, loss = 2.00058056\n",
      "Iteration 43828, loss = 2.29002126\n",
      "Iteration 43829, loss = 2.26551067\n",
      "Iteration 43830, loss = 2.74332284\n",
      "Iteration 43831, loss = 2.20042564\n",
      "Iteration 43832, loss = 1.75326542\n",
      "Iteration 43833, loss = 2.35646707\n",
      "Iteration 43834, loss = 2.16678445\n",
      "Iteration 43835, loss = 2.17701568\n",
      "Iteration 43836, loss = 2.15977082\n",
      "Iteration 43837, loss = 2.23440263\n",
      "Iteration 43838, loss = 2.57739083\n",
      "Iteration 43839, loss = 3.02002586\n",
      "Iteration 43840, loss = 3.33340263\n",
      "Iteration 43841, loss = 2.46353218\n",
      "Iteration 43842, loss = 2.66291535\n",
      "Iteration 43843, loss = 1.94729673\n",
      "Iteration 43844, loss = 1.74506308\n",
      "Iteration 43845, loss = 1.75030672\n",
      "Iteration 43846, loss = 2.03867865\n",
      "Iteration 43847, loss = 1.75608370\n",
      "Iteration 43848, loss = 1.89656899\n",
      "Iteration 43849, loss = 1.77928226\n",
      "Iteration 43850, loss = 1.74122874\n",
      "Iteration 43851, loss = 2.13136149\n",
      "Iteration 43852, loss = 2.07755389\n",
      "Iteration 43853, loss = 1.91755598\n",
      "Iteration 43854, loss = 2.10276825\n",
      "Iteration 43855, loss = 2.02530989\n",
      "Iteration 43856, loss = 1.83260365\n",
      "Iteration 43857, loss = 2.37489082\n",
      "Iteration 43858, loss = 2.05790251\n",
      "Iteration 43859, loss = 2.15503801\n",
      "Iteration 43860, loss = 4.59596354\n",
      "Iteration 43861, loss = 6.74351933\n",
      "Iteration 43862, loss = 6.09792129\n",
      "Iteration 43863, loss = 5.46618462\n",
      "Iteration 43864, loss = 4.03735579\n",
      "Iteration 43865, loss = 4.64205465\n",
      "Iteration 43866, loss = 3.97777902\n",
      "Iteration 43867, loss = 3.23880156\n",
      "Iteration 43868, loss = 2.22437664\n",
      "Iteration 43869, loss = 2.61315242\n",
      "Iteration 43870, loss = 3.76654961\n",
      "Iteration 43871, loss = 3.50727983\n",
      "Iteration 43872, loss = 2.42414076\n",
      "Iteration 43873, loss = 3.03441286\n",
      "Iteration 43874, loss = 4.06961783\n",
      "Iteration 43875, loss = 3.02006573\n",
      "Iteration 43876, loss = 3.07034156\n",
      "Iteration 43877, loss = 2.84826665\n",
      "Iteration 43878, loss = 2.35618327\n",
      "Iteration 43879, loss = 2.42954151\n",
      "Iteration 43880, loss = 1.82617172\n",
      "Iteration 43881, loss = 2.21444670\n",
      "Iteration 43882, loss = 2.16848566\n",
      "Iteration 43883, loss = 2.08602953\n",
      "Iteration 43884, loss = 2.01985465\n",
      "Iteration 43885, loss = 2.16638089\n",
      "Iteration 43886, loss = 2.28575866\n",
      "Iteration 43887, loss = 2.26674857\n",
      "Iteration 43888, loss = 2.53178317\n",
      "Iteration 43889, loss = 2.18702428\n",
      "Iteration 43890, loss = 1.91011197\n",
      "Iteration 43891, loss = 1.84032400\n",
      "Iteration 43892, loss = 2.13359111\n",
      "Iteration 43893, loss = 1.97877558\n",
      "Iteration 43894, loss = 1.75268854\n",
      "Iteration 43895, loss = 1.65619321\n",
      "Iteration 43896, loss = 2.06244835\n",
      "Iteration 43897, loss = 1.78731435\n",
      "Iteration 43898, loss = 1.81302731\n",
      "Iteration 43899, loss = 1.76283536\n",
      "Iteration 43900, loss = 1.58587348\n",
      "Iteration 43901, loss = 1.55436809\n",
      "Iteration 43902, loss = 1.52773116\n",
      "Iteration 43903, loss = 1.76231519\n",
      "Iteration 43904, loss = 1.76372245\n",
      "Iteration 43905, loss = 1.60391633\n",
      "Iteration 43906, loss = 1.94803932\n",
      "Iteration 43907, loss = 1.85429464\n",
      "Iteration 43908, loss = 2.06262238\n",
      "Iteration 43909, loss = 1.82927297\n",
      "Iteration 43910, loss = 2.11509429\n",
      "Iteration 43911, loss = 1.77891772\n",
      "Iteration 43912, loss = 1.87441432\n",
      "Iteration 43913, loss = 1.67907974\n",
      "Iteration 43914, loss = 2.06896464\n",
      "Iteration 43915, loss = 1.79322136\n",
      "Iteration 43916, loss = 1.50973373\n",
      "Iteration 43917, loss = 1.56287473\n",
      "Iteration 43918, loss = 1.54170590\n",
      "Iteration 43919, loss = 1.67039920\n",
      "Iteration 43920, loss = 1.72873228\n",
      "Iteration 43921, loss = 2.02547479\n",
      "Iteration 43922, loss = 2.10527032\n",
      "Iteration 43923, loss = 2.60822488\n",
      "Iteration 43924, loss = 2.19724397\n",
      "Iteration 43925, loss = 1.81386586\n",
      "Iteration 43926, loss = 1.95861034\n",
      "Iteration 43927, loss = 1.88618666\n",
      "Iteration 43928, loss = 1.70686847\n",
      "Iteration 43929, loss = 1.83472857\n",
      "Iteration 43930, loss = 1.79732595\n",
      "Iteration 43931, loss = 2.09800834\n",
      "Iteration 43932, loss = 2.34416538\n",
      "Iteration 43933, loss = 2.42438451\n",
      "Iteration 43934, loss = 1.92343641\n",
      "Iteration 43935, loss = 1.76753368\n",
      "Iteration 43936, loss = 1.71678189\n",
      "Iteration 43937, loss = 2.03232714\n",
      "Iteration 43938, loss = 2.08238401\n",
      "Iteration 43939, loss = 2.54841050\n",
      "Iteration 43940, loss = 2.75502413\n",
      "Iteration 43941, loss = 3.19370381\n",
      "Iteration 43942, loss = 2.98060945\n",
      "Iteration 43943, loss = 2.84577527\n",
      "Iteration 43944, loss = 2.92363423\n",
      "Iteration 43945, loss = 2.49719695\n",
      "Iteration 43946, loss = 1.78077860\n",
      "Iteration 43947, loss = 2.19958412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 43948, loss = 1.92367438\n",
      "Iteration 43949, loss = 2.06740526\n",
      "Iteration 43950, loss = 1.74294701\n",
      "Iteration 43951, loss = 1.79620953\n",
      "Iteration 43952, loss = 1.80458376\n",
      "Iteration 43953, loss = 1.81570826\n",
      "Iteration 43954, loss = 1.93666333\n",
      "Iteration 43955, loss = 1.91793211\n",
      "Iteration 43956, loss = 2.14030370\n",
      "Iteration 43957, loss = 1.92698091\n",
      "Iteration 43958, loss = 1.67070784\n",
      "Iteration 43959, loss = 1.81341525\n",
      "Iteration 43960, loss = 2.11507295\n",
      "Iteration 43961, loss = 1.88162955\n",
      "Iteration 43962, loss = 1.84396322\n",
      "Iteration 43963, loss = 1.55135667\n",
      "Iteration 43964, loss = 1.63160345\n",
      "Iteration 43965, loss = 1.99021906\n",
      "Iteration 43966, loss = 1.72486594\n",
      "Iteration 43967, loss = 1.78044022\n",
      "Iteration 43968, loss = 1.63694428\n",
      "Iteration 43969, loss = 1.67377052\n",
      "Iteration 43970, loss = 1.95617571\n",
      "Iteration 43971, loss = 2.45985964\n",
      "Iteration 43972, loss = 1.90723621\n",
      "Iteration 43973, loss = 3.43624255\n",
      "Iteration 43974, loss = 2.76158216\n",
      "Iteration 43975, loss = 2.75694151\n",
      "Iteration 43976, loss = 2.90536665\n",
      "Iteration 43977, loss = 2.61100691\n",
      "Iteration 43978, loss = 2.66605981\n",
      "Iteration 43979, loss = 3.60901415\n",
      "Iteration 43980, loss = 2.61788651\n",
      "Iteration 43981, loss = 2.67397154\n",
      "Iteration 43982, loss = 2.63055771\n",
      "Iteration 43983, loss = 1.86944198\n",
      "Iteration 43984, loss = 1.94399996\n",
      "Iteration 43985, loss = 1.83969887\n",
      "Iteration 43986, loss = 1.77520813\n",
      "Iteration 43987, loss = 1.74623765\n",
      "Iteration 43988, loss = 1.63888851\n",
      "Iteration 43989, loss = 1.75511258\n",
      "Iteration 43990, loss = 1.78367427\n",
      "Iteration 43991, loss = 1.66136899\n",
      "Iteration 43992, loss = 1.69252905\n",
      "Iteration 43993, loss = 2.04013140\n",
      "Iteration 43994, loss = 2.23010532\n",
      "Iteration 43995, loss = 1.78387024\n",
      "Iteration 43996, loss = 2.33246492\n",
      "Iteration 43997, loss = 2.38125138\n",
      "Iteration 43998, loss = 2.15440058\n",
      "Iteration 43999, loss = 2.04561808\n",
      "Iteration 44000, loss = 1.76494052\n",
      "Iteration 44001, loss = 1.93566580\n",
      "Iteration 44002, loss = 1.95798128\n",
      "Iteration 44003, loss = 1.66593215\n",
      "Iteration 44004, loss = 1.54067307\n",
      "Iteration 44005, loss = 1.63446055\n",
      "Iteration 44006, loss = 2.35641552\n",
      "Iteration 44007, loss = 2.00783066\n",
      "Iteration 44008, loss = 3.35421130\n",
      "Iteration 44009, loss = 2.67010729\n",
      "Iteration 44010, loss = 2.69249302\n",
      "Iteration 44011, loss = 3.24968332\n",
      "Iteration 44012, loss = 3.05037674\n",
      "Iteration 44013, loss = 2.18349079\n",
      "Iteration 44014, loss = 2.83560732\n",
      "Iteration 44015, loss = 2.28082959\n",
      "Iteration 44016, loss = 2.74373563\n",
      "Iteration 44017, loss = 3.23908895\n",
      "Iteration 44018, loss = 2.78784504\n",
      "Iteration 44019, loss = 2.66163648\n",
      "Iteration 44020, loss = 2.30957280\n",
      "Iteration 44021, loss = 1.70967270\n",
      "Iteration 44022, loss = 1.73091457\n",
      "Iteration 44023, loss = 1.61812605\n",
      "Iteration 44024, loss = 1.84232707\n",
      "Iteration 44025, loss = 1.84975064\n",
      "Iteration 44026, loss = 1.71449028\n",
      "Iteration 44027, loss = 1.71402985\n",
      "Iteration 44028, loss = 1.73137493\n",
      "Iteration 44029, loss = 1.87580182\n",
      "Iteration 44030, loss = 2.24306460\n",
      "Iteration 44031, loss = 3.09520787\n",
      "Iteration 44032, loss = 2.50644197\n",
      "Iteration 44033, loss = 1.76270215\n",
      "Iteration 44034, loss = 1.74161737\n",
      "Iteration 44035, loss = 1.65762979\n",
      "Iteration 44036, loss = 1.86921737\n",
      "Iteration 44037, loss = 1.83903473\n",
      "Iteration 44038, loss = 1.69340592\n",
      "Iteration 44039, loss = 1.74745714\n",
      "Iteration 44040, loss = 1.86653423\n",
      "Iteration 44041, loss = 2.23793442\n",
      "Iteration 44042, loss = 2.25280648\n",
      "Iteration 44043, loss = 3.23906881\n",
      "Iteration 44044, loss = 2.71737000\n",
      "Iteration 44045, loss = 2.60192877\n",
      "Iteration 44046, loss = 2.03378646\n",
      "Iteration 44047, loss = 1.80237939\n",
      "Iteration 44048, loss = 2.03877889\n",
      "Iteration 44049, loss = 2.21397148\n",
      "Iteration 44050, loss = 2.55338287\n",
      "Iteration 44051, loss = 2.94772577\n",
      "Iteration 44052, loss = 2.84644007\n",
      "Iteration 44053, loss = 3.09143335\n",
      "Iteration 44054, loss = 2.35549929\n",
      "Iteration 44055, loss = 1.99976187\n",
      "Iteration 44056, loss = 2.13919929\n",
      "Iteration 44057, loss = 2.93989308\n",
      "Iteration 44058, loss = 2.73791748\n",
      "Iteration 44059, loss = 2.69492688\n",
      "Iteration 44060, loss = 3.17160138\n",
      "Iteration 44061, loss = 2.57218806\n",
      "Iteration 44062, loss = 2.38719891\n",
      "Iteration 44063, loss = 1.99104672\n",
      "Iteration 44064, loss = 2.17671847\n",
      "Iteration 44065, loss = 2.48462856\n",
      "Iteration 44066, loss = 1.73505153\n",
      "Iteration 44067, loss = 1.64458685\n",
      "Iteration 44068, loss = 1.82422208\n",
      "Iteration 44069, loss = 1.85804404\n",
      "Iteration 44070, loss = 1.80795246\n",
      "Iteration 44071, loss = 1.86278975\n",
      "Iteration 44072, loss = 1.84441710\n",
      "Iteration 44073, loss = 1.80015304\n",
      "Iteration 44074, loss = 1.66968485\n",
      "Iteration 44075, loss = 1.64935702\n",
      "Iteration 44076, loss = 1.94192944\n",
      "Iteration 44077, loss = 1.94257341\n",
      "Iteration 44078, loss = 1.46337039\n",
      "Iteration 44079, loss = 1.90728592\n",
      "Iteration 44080, loss = 2.19533515\n",
      "Iteration 44081, loss = 1.81323053\n",
      "Iteration 44082, loss = 1.60603811\n",
      "Iteration 44083, loss = 2.15179227\n",
      "Iteration 44084, loss = 1.75730844\n",
      "Iteration 44085, loss = 1.48275022\n",
      "Iteration 44086, loss = 1.89654053\n",
      "Iteration 44087, loss = 1.92488371\n",
      "Iteration 44088, loss = 2.40007905\n",
      "Iteration 44089, loss = 1.94728001\n",
      "Iteration 44090, loss = 2.85873607\n",
      "Iteration 44091, loss = 2.17613859\n",
      "Iteration 44092, loss = 2.69282188\n",
      "Iteration 44093, loss = 2.96428114\n",
      "Iteration 44094, loss = 2.93305717\n",
      "Iteration 44095, loss = 2.59165252\n",
      "Iteration 44096, loss = 1.93192810\n",
      "Iteration 44097, loss = 2.17949791\n",
      "Iteration 44098, loss = 1.82891514\n",
      "Iteration 44099, loss = 2.25599474\n",
      "Iteration 44100, loss = 1.70121586\n",
      "Iteration 44101, loss = 1.68445230\n",
      "Iteration 44102, loss = 1.59548294\n",
      "Iteration 44103, loss = 1.65594709\n",
      "Iteration 44104, loss = 1.87138511\n",
      "Iteration 44105, loss = 2.56926625\n",
      "Iteration 44106, loss = 2.54309133\n",
      "Iteration 44107, loss = 2.43502873\n",
      "Iteration 44108, loss = 1.77787889\n",
      "Iteration 44109, loss = 1.85847189\n",
      "Iteration 44110, loss = 1.69227122\n",
      "Iteration 44111, loss = 2.09728987\n",
      "Iteration 44112, loss = 2.12440237\n",
      "Iteration 44113, loss = 1.78338059\n",
      "Iteration 44114, loss = 1.82313816\n",
      "Iteration 44115, loss = 1.76736466\n",
      "Iteration 44116, loss = 1.89633008\n",
      "Iteration 44117, loss = 1.81025620\n",
      "Iteration 44118, loss = 1.80579660\n",
      "Iteration 44119, loss = 1.77459140\n",
      "Iteration 44120, loss = 2.05172971\n",
      "Iteration 44121, loss = 1.95844127\n",
      "Iteration 44122, loss = 1.88888420\n",
      "Iteration 44123, loss = 1.66355340\n",
      "Iteration 44124, loss = 1.98373091\n",
      "Iteration 44125, loss = 2.20993574\n",
      "Iteration 44126, loss = 2.27071916\n",
      "Iteration 44127, loss = 2.68294728\n",
      "Iteration 44128, loss = 2.45483404\n",
      "Iteration 44129, loss = 1.90932628\n",
      "Iteration 44130, loss = 2.04366982\n",
      "Iteration 44131, loss = 2.16354128\n",
      "Iteration 44132, loss = 2.39298171\n",
      "Iteration 44133, loss = 1.85446674\n",
      "Iteration 44134, loss = 1.66483538\n",
      "Iteration 44135, loss = 1.63104958\n",
      "Iteration 44136, loss = 1.64060424\n",
      "Iteration 44137, loss = 1.99678216\n",
      "Iteration 44138, loss = 1.62415606\n",
      "Iteration 44139, loss = 1.92585723\n",
      "Iteration 44140, loss = 1.99487651\n",
      "Iteration 44141, loss = 1.75347322\n",
      "Iteration 44142, loss = 1.54520867\n",
      "Iteration 44143, loss = 1.61677567\n",
      "Iteration 44144, loss = 1.68798776\n",
      "Iteration 44145, loss = 1.98407321\n",
      "Iteration 44146, loss = 1.94757082\n",
      "Iteration 44147, loss = 1.70026149\n",
      "Iteration 44148, loss = 1.98398008\n",
      "Iteration 44149, loss = 1.97722478\n",
      "Iteration 44150, loss = 2.11888845\n",
      "Iteration 44151, loss = 1.88781057\n",
      "Iteration 44152, loss = 2.29970085\n",
      "Iteration 44153, loss = 1.75392714\n",
      "Iteration 44154, loss = 2.06328399\n",
      "Iteration 44155, loss = 1.82394192\n",
      "Iteration 44156, loss = 1.93850041\n",
      "Iteration 44157, loss = 1.98633810\n",
      "Iteration 44158, loss = 1.91397869\n",
      "Iteration 44159, loss = 1.91964792\n",
      "Iteration 44160, loss = 1.81889563\n",
      "Iteration 44161, loss = 1.81701600\n",
      "Iteration 44162, loss = 1.46374485\n",
      "Iteration 44163, loss = 1.81574913\n",
      "Iteration 44164, loss = 1.82373738\n",
      "Iteration 44165, loss = 1.65836420\n",
      "Iteration 44166, loss = 1.62129164\n",
      "Iteration 44167, loss = 1.94363065\n",
      "Iteration 44168, loss = 2.46091116\n",
      "Iteration 44169, loss = 2.16700472\n",
      "Iteration 44170, loss = 2.34387422\n",
      "Iteration 44171, loss = 1.86205158\n",
      "Iteration 44172, loss = 1.93749601\n",
      "Iteration 44173, loss = 1.75813422\n",
      "Iteration 44174, loss = 2.21303730\n",
      "Iteration 44175, loss = 3.37008641\n",
      "Iteration 44176, loss = 4.94048631\n",
      "Iteration 44177, loss = 3.81666773\n",
      "Iteration 44178, loss = 2.65708281\n",
      "Iteration 44179, loss = 2.33040172\n",
      "Iteration 44180, loss = 2.39391460\n",
      "Iteration 44181, loss = 2.26651286\n",
      "Iteration 44182, loss = 1.90642926\n",
      "Iteration 44183, loss = 1.98906289\n",
      "Iteration 44184, loss = 2.06132859\n",
      "Iteration 44185, loss = 2.24044036\n",
      "Iteration 44186, loss = 2.00409920\n",
      "Iteration 44187, loss = 1.94268941\n",
      "Iteration 44188, loss = 1.84308039\n",
      "Iteration 44189, loss = 2.24506791\n",
      "Iteration 44190, loss = 1.72107237\n",
      "Iteration 44191, loss = 1.61432715\n",
      "Iteration 44192, loss = 1.71381699\n",
      "Iteration 44193, loss = 1.48151556\n",
      "Iteration 44194, loss = 1.72960702\n",
      "Iteration 44195, loss = 1.79252444\n",
      "Iteration 44196, loss = 1.97070712\n",
      "Iteration 44197, loss = 1.79268816\n",
      "Iteration 44198, loss = 1.69685324\n",
      "Iteration 44199, loss = 1.55994739\n",
      "Iteration 44200, loss = 1.66114205\n",
      "Iteration 44201, loss = 1.76260906\n",
      "Iteration 44202, loss = 1.59408175\n",
      "Iteration 44203, loss = 1.72570697\n",
      "Iteration 44204, loss = 1.60655887\n",
      "Iteration 44205, loss = 1.84011072\n",
      "Iteration 44206, loss = 1.83816641\n",
      "Iteration 44207, loss = 1.79992308\n",
      "Iteration 44208, loss = 1.75660332\n",
      "Iteration 44209, loss = 1.94743538\n",
      "Iteration 44210, loss = 2.11449453\n",
      "Iteration 44211, loss = 2.78104492\n",
      "Iteration 44212, loss = 2.70192251\n",
      "Iteration 44213, loss = 2.50099088\n",
      "Iteration 44214, loss = 2.22451380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 44215, loss = 1.75434038\n",
      "Iteration 44216, loss = 1.78300923\n",
      "Iteration 44217, loss = 1.81254152\n",
      "Iteration 44218, loss = 1.87077313\n",
      "Iteration 44219, loss = 1.94323283\n",
      "Iteration 44220, loss = 2.59842111\n",
      "Iteration 44221, loss = 2.38039242\n",
      "Iteration 44222, loss = 2.86139672\n",
      "Iteration 44223, loss = 3.32227086\n",
      "Iteration 44224, loss = 3.46397774\n",
      "Iteration 44225, loss = 3.41462181\n",
      "Iteration 44226, loss = 3.14688677\n",
      "Iteration 44227, loss = 2.88759149\n",
      "Iteration 44228, loss = 2.60258033\n",
      "Iteration 44229, loss = 2.29795163\n",
      "Iteration 44230, loss = 2.02747636\n",
      "Iteration 44231, loss = 2.23479727\n",
      "Iteration 44232, loss = 2.83533984\n",
      "Iteration 44233, loss = 1.97749856\n",
      "Iteration 44234, loss = 2.37114404\n",
      "Iteration 44235, loss = 2.05888248\n",
      "Iteration 44236, loss = 2.51078158\n",
      "Iteration 44237, loss = 2.68419839\n",
      "Iteration 44238, loss = 3.43265735\n",
      "Iteration 44239, loss = 2.36186519\n",
      "Iteration 44240, loss = 2.30179550\n",
      "Iteration 44241, loss = 2.19042886\n",
      "Iteration 44242, loss = 2.25964052\n",
      "Iteration 44243, loss = 2.49540973\n",
      "Iteration 44244, loss = 1.85034886\n",
      "Iteration 44245, loss = 2.23775846\n",
      "Iteration 44246, loss = 1.97131266\n",
      "Iteration 44247, loss = 1.82436204\n",
      "Iteration 44248, loss = 1.58235134\n",
      "Iteration 44249, loss = 1.94124792\n",
      "Iteration 44250, loss = 2.24734520\n",
      "Iteration 44251, loss = 1.95948165\n",
      "Iteration 44252, loss = 1.87726405\n",
      "Iteration 44253, loss = 1.78543240\n",
      "Iteration 44254, loss = 2.02128076\n",
      "Iteration 44255, loss = 1.95023922\n",
      "Iteration 44256, loss = 2.07824331\n",
      "Iteration 44257, loss = 2.25174037\n",
      "Iteration 44258, loss = 2.95209543\n",
      "Iteration 44259, loss = 2.74843770\n",
      "Iteration 44260, loss = 2.71191323\n",
      "Iteration 44261, loss = 2.93118164\n",
      "Iteration 44262, loss = 2.96699387\n",
      "Iteration 44263, loss = 3.32116184\n",
      "Iteration 44264, loss = 3.21779017\n",
      "Iteration 44265, loss = 2.32479824\n",
      "Iteration 44266, loss = 2.53026953\n",
      "Iteration 44267, loss = 1.92773980\n",
      "Iteration 44268, loss = 1.70069750\n",
      "Iteration 44269, loss = 1.77119956\n",
      "Iteration 44270, loss = 1.79587918\n",
      "Iteration 44271, loss = 1.97416177\n",
      "Iteration 44272, loss = 1.95921173\n",
      "Iteration 44273, loss = 1.72567403\n",
      "Iteration 44274, loss = 1.74736782\n",
      "Iteration 44275, loss = 1.89037780\n",
      "Iteration 44276, loss = 2.18538453\n",
      "Iteration 44277, loss = 2.42736051\n",
      "Iteration 44278, loss = 1.89413879\n",
      "Iteration 44279, loss = 1.71331715\n",
      "Iteration 44280, loss = 2.24661429\n",
      "Iteration 44281, loss = 2.28139208\n",
      "Iteration 44282, loss = 2.15004499\n",
      "Iteration 44283, loss = 2.28677817\n",
      "Iteration 44284, loss = 2.50379498\n",
      "Iteration 44285, loss = 2.72456741\n",
      "Iteration 44286, loss = 3.56015160\n",
      "Iteration 44287, loss = 3.31464810\n",
      "Iteration 44288, loss = 2.28212571\n",
      "Iteration 44289, loss = 2.14028088\n",
      "Iteration 44290, loss = 2.24868413\n",
      "Iteration 44291, loss = 2.21159540\n",
      "Iteration 44292, loss = 2.35066498\n",
      "Iteration 44293, loss = 1.87567062\n",
      "Iteration 44294, loss = 1.90658480\n",
      "Iteration 44295, loss = 2.62472673\n",
      "Iteration 44296, loss = 2.45614102\n",
      "Iteration 44297, loss = 1.84687415\n",
      "Iteration 44298, loss = 2.45465610\n",
      "Iteration 44299, loss = 2.26505514\n",
      "Iteration 44300, loss = 2.22641287\n",
      "Iteration 44301, loss = 2.26252086\n",
      "Iteration 44302, loss = 2.27125002\n",
      "Iteration 44303, loss = 2.04736493\n",
      "Iteration 44304, loss = 2.15272326\n",
      "Iteration 44305, loss = 1.83666053\n",
      "Iteration 44306, loss = 2.19945234\n",
      "Iteration 44307, loss = 2.18992175\n",
      "Iteration 44308, loss = 2.17633134\n",
      "Iteration 44309, loss = 1.76798132\n",
      "Iteration 44310, loss = 1.70656376\n",
      "Iteration 44311, loss = 1.71244064\n",
      "Iteration 44312, loss = 1.87604845\n",
      "Iteration 44313, loss = 1.75916914\n",
      "Iteration 44314, loss = 1.92296246\n",
      "Iteration 44315, loss = 2.22950275\n",
      "Iteration 44316, loss = 2.17273748\n",
      "Iteration 44317, loss = 1.71964860\n",
      "Iteration 44318, loss = 2.24339036\n",
      "Iteration 44319, loss = 2.55748094\n",
      "Iteration 44320, loss = 2.00938205\n",
      "Iteration 44321, loss = 2.01664292\n",
      "Iteration 44322, loss = 2.05357181\n",
      "Iteration 44323, loss = 1.82253933\n",
      "Iteration 44324, loss = 2.01773998\n",
      "Iteration 44325, loss = 2.06651237\n",
      "Iteration 44326, loss = 2.14875517\n",
      "Iteration 44327, loss = 2.59879380\n",
      "Iteration 44328, loss = 2.14481111\n",
      "Iteration 44329, loss = 2.31281283\n",
      "Iteration 44330, loss = 1.86313914\n",
      "Iteration 44331, loss = 1.82869030\n",
      "Iteration 44332, loss = 1.95906596\n",
      "Iteration 44333, loss = 2.20546371\n",
      "Iteration 44334, loss = 2.74999917\n",
      "Iteration 44335, loss = 3.63939973\n",
      "Iteration 44336, loss = 2.65243480\n",
      "Iteration 44337, loss = 2.48725269\n",
      "Iteration 44338, loss = 1.67943782\n",
      "Iteration 44339, loss = 1.71978145\n",
      "Iteration 44340, loss = 1.83408899\n",
      "Iteration 44341, loss = 1.80694676\n",
      "Iteration 44342, loss = 2.07764235\n",
      "Iteration 44343, loss = 2.09521586\n",
      "Iteration 44344, loss = 1.91379686\n",
      "Iteration 44345, loss = 1.63137442\n",
      "Iteration 44346, loss = 1.74451121\n",
      "Iteration 44347, loss = 1.63110747\n",
      "Iteration 44348, loss = 1.66058168\n",
      "Iteration 44349, loss = 1.65122001\n",
      "Iteration 44350, loss = 1.61903807\n",
      "Iteration 44351, loss = 1.66914889\n",
      "Iteration 44352, loss = 1.67218470\n",
      "Iteration 44353, loss = 1.59471561\n",
      "Iteration 44354, loss = 1.67637810\n",
      "Iteration 44355, loss = 2.01867883\n",
      "Iteration 44356, loss = 1.66512486\n",
      "Iteration 44357, loss = 1.68567984\n",
      "Iteration 44358, loss = 1.77881021\n",
      "Iteration 44359, loss = 2.05682298\n",
      "Iteration 44360, loss = 2.53343646\n",
      "Iteration 44361, loss = 2.44233024\n",
      "Iteration 44362, loss = 2.04473254\n",
      "Iteration 44363, loss = 2.10159924\n",
      "Iteration 44364, loss = 1.89022769\n",
      "Iteration 44365, loss = 2.67202708\n",
      "Iteration 44366, loss = 2.27169152\n",
      "Iteration 44367, loss = 2.42600782\n",
      "Iteration 44368, loss = 3.02139625\n",
      "Iteration 44369, loss = 2.48487179\n",
      "Iteration 44370, loss = 2.40551268\n",
      "Iteration 44371, loss = 2.01215155\n",
      "Iteration 44372, loss = 2.19439305\n",
      "Iteration 44373, loss = 1.90369877\n",
      "Iteration 44374, loss = 1.76788227\n",
      "Iteration 44375, loss = 1.75045576\n",
      "Iteration 44376, loss = 2.23206324\n",
      "Iteration 44377, loss = 1.89581775\n",
      "Iteration 44378, loss = 1.61921811\n",
      "Iteration 44379, loss = 1.62547010\n",
      "Iteration 44380, loss = 1.59905945\n",
      "Iteration 44381, loss = 1.66540079\n",
      "Iteration 44382, loss = 1.78435208\n",
      "Iteration 44383, loss = 2.19828807\n",
      "Iteration 44384, loss = 1.96323045\n",
      "Iteration 44385, loss = 1.82521680\n",
      "Iteration 44386, loss = 1.76505499\n",
      "Iteration 44387, loss = 1.76209837\n",
      "Iteration 44388, loss = 1.98752966\n",
      "Iteration 44389, loss = 2.01229464\n",
      "Iteration 44390, loss = 1.69729219\n",
      "Iteration 44391, loss = 1.72709943\n",
      "Iteration 44392, loss = 2.23363412\n",
      "Iteration 44393, loss = 2.05289387\n",
      "Iteration 44394, loss = 1.79568923\n",
      "Iteration 44395, loss = 1.61412361\n",
      "Iteration 44396, loss = 1.65140284\n",
      "Iteration 44397, loss = 2.13452767\n",
      "Iteration 44398, loss = 1.71201369\n",
      "Iteration 44399, loss = 1.75925723\n",
      "Iteration 44400, loss = 1.58237788\n",
      "Iteration 44401, loss = 1.80877854\n",
      "Iteration 44402, loss = 1.88538217\n",
      "Iteration 44403, loss = 1.71248376\n",
      "Iteration 44404, loss = 1.54384783\n",
      "Iteration 44405, loss = 1.59983654\n",
      "Iteration 44406, loss = 1.59887199\n",
      "Iteration 44407, loss = 1.78633975\n",
      "Iteration 44408, loss = 1.91267784\n",
      "Iteration 44409, loss = 2.65752507\n",
      "Iteration 44410, loss = 2.62166939\n",
      "Iteration 44411, loss = 2.30583188\n",
      "Iteration 44412, loss = 2.01288112\n",
      "Iteration 44413, loss = 1.56459094\n",
      "Iteration 44414, loss = 1.57635561\n",
      "Iteration 44415, loss = 1.60815331\n",
      "Iteration 44416, loss = 1.58400555\n",
      "Iteration 44417, loss = 1.56395253\n",
      "Iteration 44418, loss = 1.58667137\n",
      "Iteration 44419, loss = 1.81501012\n",
      "Iteration 44420, loss = 1.62415278\n",
      "Iteration 44421, loss = 1.67630052\n",
      "Iteration 44422, loss = 1.74564362\n",
      "Iteration 44423, loss = 1.53385181\n",
      "Iteration 44424, loss = 1.75085404\n",
      "Iteration 44425, loss = 1.96632230\n",
      "Iteration 44426, loss = 1.80191047\n",
      "Iteration 44427, loss = 2.13569348\n",
      "Iteration 44428, loss = 2.07593593\n",
      "Iteration 44429, loss = 1.79078303\n",
      "Iteration 44430, loss = 1.68505438\n",
      "Iteration 44431, loss = 1.54102945\n",
      "Iteration 44432, loss = 1.59770526\n",
      "Iteration 44433, loss = 1.47841910\n",
      "Iteration 44434, loss = 1.60165283\n",
      "Iteration 44435, loss = 1.83350663\n",
      "Iteration 44436, loss = 2.15050793\n",
      "Iteration 44437, loss = 2.17839019\n",
      "Iteration 44438, loss = 1.96371294\n",
      "Iteration 44439, loss = 1.95799535\n",
      "Iteration 44440, loss = 1.62412617\n",
      "Iteration 44441, loss = 1.94953640\n",
      "Iteration 44442, loss = 2.30167597\n",
      "Iteration 44443, loss = 3.03599958\n",
      "Iteration 44444, loss = 2.55418572\n",
      "Iteration 44445, loss = 2.84031145\n",
      "Iteration 44446, loss = 2.19914436\n",
      "Iteration 44447, loss = 2.87280993\n",
      "Iteration 44448, loss = 2.47347114\n",
      "Iteration 44449, loss = 1.93748978\n",
      "Iteration 44450, loss = 1.54865010\n",
      "Iteration 44451, loss = 1.58398870\n",
      "Iteration 44452, loss = 1.73467358\n",
      "Iteration 44453, loss = 1.55387809\n",
      "Iteration 44454, loss = 1.59911428\n",
      "Iteration 44455, loss = 2.00627664\n",
      "Iteration 44456, loss = 1.74662259\n",
      "Iteration 44457, loss = 1.53644052\n",
      "Iteration 44458, loss = 1.58867093\n",
      "Iteration 44459, loss = 1.70351296\n",
      "Iteration 44460, loss = 1.61703335\n",
      "Iteration 44461, loss = 1.62323320\n",
      "Iteration 44462, loss = 1.60660612\n",
      "Iteration 44463, loss = 1.54852396\n",
      "Iteration 44464, loss = 1.69244816\n",
      "Iteration 44465, loss = 1.62686484\n",
      "Iteration 44466, loss = 1.77142240\n",
      "Iteration 44467, loss = 1.76926962\n",
      "Iteration 44468, loss = 2.00243598\n",
      "Iteration 44469, loss = 2.71074367\n",
      "Iteration 44470, loss = 1.84859206\n",
      "Iteration 44471, loss = 1.97293209\n",
      "Iteration 44472, loss = 2.49615551\n",
      "Iteration 44473, loss = 2.68304334\n",
      "Iteration 44474, loss = 2.22485908\n",
      "Iteration 44475, loss = 2.84540929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 44476, loss = 2.16774122\n",
      "Iteration 44477, loss = 2.40961304\n",
      "Iteration 44478, loss = 2.54859186\n",
      "Iteration 44479, loss = 2.25185951\n",
      "Iteration 44480, loss = 2.33951897\n",
      "Iteration 44481, loss = 2.15190779\n",
      "Iteration 44482, loss = 2.95095284\n",
      "Iteration 44483, loss = 2.67756951\n",
      "Iteration 44484, loss = 1.90408706\n",
      "Iteration 44485, loss = 1.92091256\n",
      "Iteration 44486, loss = 2.01130930\n",
      "Iteration 44487, loss = 3.57564677\n",
      "Iteration 44488, loss = 2.98315177\n",
      "Iteration 44489, loss = 2.79351455\n",
      "Iteration 44490, loss = 3.96793261\n",
      "Iteration 44491, loss = 4.20660173\n",
      "Iteration 44492, loss = 3.53713670\n",
      "Iteration 44493, loss = 2.81580983\n",
      "Iteration 44494, loss = 2.05515301\n",
      "Iteration 44495, loss = 2.12327493\n",
      "Iteration 44496, loss = 1.72926754\n",
      "Iteration 44497, loss = 1.81917361\n",
      "Iteration 44498, loss = 1.75438656\n",
      "Iteration 44499, loss = 1.81062106\n",
      "Iteration 44500, loss = 1.79518427\n",
      "Iteration 44501, loss = 2.04243285\n",
      "Iteration 44502, loss = 1.92370447\n",
      "Iteration 44503, loss = 1.70511645\n",
      "Iteration 44504, loss = 1.71097002\n",
      "Iteration 44505, loss = 2.22448064\n",
      "Iteration 44506, loss = 1.75127870\n",
      "Iteration 44507, loss = 2.02208665\n",
      "Iteration 44508, loss = 2.03022252\n",
      "Iteration 44509, loss = 1.97222868\n",
      "Iteration 44510, loss = 1.87266969\n",
      "Iteration 44511, loss = 1.90819434\n",
      "Iteration 44512, loss = 1.80551494\n",
      "Iteration 44513, loss = 2.25391888\n",
      "Iteration 44514, loss = 2.04718235\n",
      "Iteration 44515, loss = 1.56098673\n",
      "Iteration 44516, loss = 1.79345125\n",
      "Iteration 44517, loss = 1.89920441\n",
      "Iteration 44518, loss = 1.71261574\n",
      "Iteration 44519, loss = 1.69657371\n",
      "Iteration 44520, loss = 2.05798874\n",
      "Iteration 44521, loss = 2.66743132\n",
      "Iteration 44522, loss = 2.75042222\n",
      "Iteration 44523, loss = 2.82143258\n",
      "Iteration 44524, loss = 2.00989874\n",
      "Iteration 44525, loss = 2.04135973\n",
      "Iteration 44526, loss = 1.90362726\n",
      "Iteration 44527, loss = 2.46793464\n",
      "Iteration 44528, loss = 1.93234348\n",
      "Iteration 44529, loss = 1.67712879\n",
      "Iteration 44530, loss = 1.73945650\n",
      "Iteration 44531, loss = 2.25539809\n",
      "Iteration 44532, loss = 2.79726342\n",
      "Iteration 44533, loss = 3.12025271\n",
      "Iteration 44534, loss = 2.49470052\n",
      "Iteration 44535, loss = 2.14782552\n",
      "Iteration 44536, loss = 2.00053963\n",
      "Iteration 44537, loss = 1.83320346\n",
      "Iteration 44538, loss = 1.91201558\n",
      "Iteration 44539, loss = 1.95154722\n",
      "Iteration 44540, loss = 1.80201328\n",
      "Iteration 44541, loss = 1.87382520\n",
      "Iteration 44542, loss = 2.03588874\n",
      "Iteration 44543, loss = 2.57319146\n",
      "Iteration 44544, loss = 3.17335284\n",
      "Iteration 44545, loss = 2.04559805\n",
      "Iteration 44546, loss = 2.02727288\n",
      "Iteration 44547, loss = 2.03357621\n",
      "Iteration 44548, loss = 1.83490267\n",
      "Iteration 44549, loss = 2.30265350\n",
      "Iteration 44550, loss = 1.76093319\n",
      "Iteration 44551, loss = 1.70121656\n",
      "Iteration 44552, loss = 1.85514057\n",
      "Iteration 44553, loss = 1.82729541\n",
      "Iteration 44554, loss = 1.86128001\n",
      "Iteration 44555, loss = 2.42883280\n",
      "Iteration 44556, loss = 2.20299086\n",
      "Iteration 44557, loss = 2.25673111\n",
      "Iteration 44558, loss = 1.98840447\n",
      "Iteration 44559, loss = 2.06696282\n",
      "Iteration 44560, loss = 2.01050770\n",
      "Iteration 44561, loss = 1.68165313\n",
      "Iteration 44562, loss = 1.82342989\n",
      "Iteration 44563, loss = 1.80232206\n",
      "Iteration 44564, loss = 1.90340979\n",
      "Iteration 44565, loss = 1.70487213\n",
      "Iteration 44566, loss = 1.78735823\n",
      "Iteration 44567, loss = 1.79415771\n",
      "Iteration 44568, loss = 1.97423297\n",
      "Iteration 44569, loss = 2.10063689\n",
      "Iteration 44570, loss = 1.86725501\n",
      "Iteration 44571, loss = 1.82327922\n",
      "Iteration 44572, loss = 2.39999953\n",
      "Iteration 44573, loss = 1.83903089\n",
      "Iteration 44574, loss = 2.05316231\n",
      "Iteration 44575, loss = 2.39451387\n",
      "Iteration 44576, loss = 2.23563989\n",
      "Iteration 44577, loss = 1.82957058\n",
      "Iteration 44578, loss = 2.20591908\n",
      "Iteration 44579, loss = 1.74640642\n",
      "Iteration 44580, loss = 2.00842443\n",
      "Iteration 44581, loss = 2.06189212\n",
      "Iteration 44582, loss = 1.72938068\n",
      "Iteration 44583, loss = 1.76436600\n",
      "Iteration 44584, loss = 1.92391098\n",
      "Iteration 44585, loss = 1.74287964\n",
      "Iteration 44586, loss = 1.81638384\n",
      "Iteration 44587, loss = 1.81106962\n",
      "Iteration 44588, loss = 1.78851553\n",
      "Iteration 44589, loss = 1.55269269\n",
      "Iteration 44590, loss = 1.90933784\n",
      "Iteration 44591, loss = 1.93273097\n",
      "Iteration 44592, loss = 1.88618132\n",
      "Iteration 44593, loss = 2.27256404\n",
      "Iteration 44594, loss = 2.21761442\n",
      "Iteration 44595, loss = 1.96971612\n",
      "Iteration 44596, loss = 2.21593422\n",
      "Iteration 44597, loss = 1.86160088\n",
      "Iteration 44598, loss = 1.70579086\n",
      "Iteration 44599, loss = 1.57150107\n",
      "Iteration 44600, loss = 1.72088802\n",
      "Iteration 44601, loss = 2.07841989\n",
      "Iteration 44602, loss = 2.00550733\n",
      "Iteration 44603, loss = 2.09479313\n",
      "Iteration 44604, loss = 1.95334620\n",
      "Iteration 44605, loss = 2.28002382\n",
      "Iteration 44606, loss = 2.21575296\n",
      "Iteration 44607, loss = 1.87879127\n",
      "Iteration 44608, loss = 1.84649734\n",
      "Iteration 44609, loss = 2.09809460\n",
      "Iteration 44610, loss = 2.20952233\n",
      "Iteration 44611, loss = 1.90832627\n",
      "Iteration 44612, loss = 2.12721511\n",
      "Iteration 44613, loss = 1.86168146\n",
      "Iteration 44614, loss = 1.76253183\n",
      "Iteration 44615, loss = 1.80421548\n",
      "Iteration 44616, loss = 1.90942027\n",
      "Iteration 44617, loss = 2.09470665\n",
      "Iteration 44618, loss = 2.25371733\n",
      "Iteration 44619, loss = 2.67222144\n",
      "Iteration 44620, loss = 1.99950056\n",
      "Iteration 44621, loss = 2.05617526\n",
      "Iteration 44622, loss = 1.72991755\n",
      "Iteration 44623, loss = 1.71698515\n",
      "Iteration 44624, loss = 1.49743545\n",
      "Iteration 44625, loss = 2.18267473\n",
      "Iteration 44626, loss = 2.55714255\n",
      "Iteration 44627, loss = 2.88580692\n",
      "Iteration 44628, loss = 2.82973489\n",
      "Iteration 44629, loss = 1.91984153\n",
      "Iteration 44630, loss = 1.89288512\n",
      "Iteration 44631, loss = 2.20578163\n",
      "Iteration 44632, loss = 2.44708631\n",
      "Iteration 44633, loss = 2.54863072\n",
      "Iteration 44634, loss = 2.33926035\n",
      "Iteration 44635, loss = 2.68189943\n",
      "Iteration 44636, loss = 2.97752616\n",
      "Iteration 44637, loss = 2.23664893\n",
      "Iteration 44638, loss = 2.83895171\n",
      "Iteration 44639, loss = 2.68122743\n",
      "Iteration 44640, loss = 3.14929000\n",
      "Iteration 44641, loss = 2.62270692\n",
      "Iteration 44642, loss = 3.11616240\n",
      "Iteration 44643, loss = 2.37360758\n",
      "Iteration 44644, loss = 2.07431783\n",
      "Iteration 44645, loss = 1.96128435\n",
      "Iteration 44646, loss = 1.97649041\n",
      "Iteration 44647, loss = 2.00479088\n",
      "Iteration 44648, loss = 1.90163390\n",
      "Iteration 44649, loss = 1.76241974\n",
      "Iteration 44650, loss = 1.63039499\n",
      "Iteration 44651, loss = 1.68257900\n",
      "Iteration 44652, loss = 1.76473078\n",
      "Iteration 44653, loss = 1.82378409\n",
      "Iteration 44654, loss = 1.70807706\n",
      "Iteration 44655, loss = 1.85007021\n",
      "Iteration 44656, loss = 1.68134819\n",
      "Iteration 44657, loss = 2.08651372\n",
      "Iteration 44658, loss = 2.28516041\n",
      "Iteration 44659, loss = 1.76439658\n",
      "Iteration 44660, loss = 2.03198336\n",
      "Iteration 44661, loss = 1.78792305\n",
      "Iteration 44662, loss = 2.11019739\n",
      "Iteration 44663, loss = 2.02492931\n",
      "Iteration 44664, loss = 2.01988152\n",
      "Iteration 44665, loss = 1.98712586\n",
      "Iteration 44666, loss = 2.09260345\n",
      "Iteration 44667, loss = 1.73669958\n",
      "Iteration 44668, loss = 1.63675160\n",
      "Iteration 44669, loss = 1.65184179\n",
      "Iteration 44670, loss = 1.78194047\n",
      "Iteration 44671, loss = 1.90863579\n",
      "Iteration 44672, loss = 2.27308277\n",
      "Iteration 44673, loss = 1.97808076\n",
      "Iteration 44674, loss = 2.10474740\n",
      "Iteration 44675, loss = 1.54414108\n",
      "Iteration 44676, loss = 1.75842853\n",
      "Iteration 44677, loss = 1.70323654\n",
      "Iteration 44678, loss = 1.68493089\n",
      "Iteration 44679, loss = 1.87378374\n",
      "Iteration 44680, loss = 1.55582910\n",
      "Iteration 44681, loss = 1.66451349\n",
      "Iteration 44682, loss = 1.75100554\n",
      "Iteration 44683, loss = 2.10305718\n",
      "Iteration 44684, loss = 2.61030738\n",
      "Iteration 44685, loss = 1.89440825\n",
      "Iteration 44686, loss = 1.95488139\n",
      "Iteration 44687, loss = 1.73241588\n",
      "Iteration 44688, loss = 2.04695019\n",
      "Iteration 44689, loss = 2.17821337\n",
      "Iteration 44690, loss = 1.93656963\n",
      "Iteration 44691, loss = 2.09621604\n",
      "Iteration 44692, loss = 1.83693447\n",
      "Iteration 44693, loss = 1.53990270\n",
      "Iteration 44694, loss = 2.19682357\n",
      "Iteration 44695, loss = 1.96596698\n",
      "Iteration 44696, loss = 2.59950426\n",
      "Iteration 44697, loss = 2.04101403\n",
      "Iteration 44698, loss = 1.61088999\n",
      "Iteration 44699, loss = 1.79143989\n",
      "Iteration 44700, loss = 1.76884710\n",
      "Iteration 44701, loss = 1.72902801\n",
      "Iteration 44702, loss = 1.53002159\n",
      "Iteration 44703, loss = 1.77819125\n",
      "Iteration 44704, loss = 2.04690405\n",
      "Iteration 44705, loss = 1.67729851\n",
      "Iteration 44706, loss = 1.54904285\n",
      "Iteration 44707, loss = 1.74121192\n",
      "Iteration 44708, loss = 1.76420376\n",
      "Iteration 44709, loss = 1.82515409\n",
      "Iteration 44710, loss = 1.96512889\n",
      "Iteration 44711, loss = 1.72667603\n",
      "Iteration 44712, loss = 2.06404650\n",
      "Iteration 44713, loss = 1.92932050\n",
      "Iteration 44714, loss = 1.82478489\n",
      "Iteration 44715, loss = 1.99461108\n",
      "Iteration 44716, loss = 1.83863863\n",
      "Iteration 44717, loss = 1.75531869\n",
      "Iteration 44718, loss = 1.75178451\n",
      "Iteration 44719, loss = 1.99097321\n",
      "Iteration 44720, loss = 1.96680611\n",
      "Iteration 44721, loss = 1.97229414\n",
      "Iteration 44722, loss = 1.99473333\n",
      "Iteration 44723, loss = 1.84566941\n",
      "Iteration 44724, loss = 1.75621857\n",
      "Iteration 44725, loss = 1.76274713\n",
      "Iteration 44726, loss = 1.99515854\n",
      "Iteration 44727, loss = 2.10866356\n",
      "Iteration 44728, loss = 1.74035033\n",
      "Iteration 44729, loss = 2.03095366\n",
      "Iteration 44730, loss = 1.88953310\n",
      "Iteration 44731, loss = 2.04826853\n",
      "Iteration 44732, loss = 2.78709145\n",
      "Iteration 44733, loss = 2.40916644\n",
      "Iteration 44734, loss = 4.21725599\n",
      "Iteration 44735, loss = 4.20347168\n",
      "Iteration 44736, loss = 5.08177077\n",
      "Iteration 44737, loss = 3.39759088\n",
      "Iteration 44738, loss = 3.56963506\n",
      "Iteration 44739, loss = 4.13337438\n",
      "Iteration 44740, loss = 2.81238382\n",
      "Iteration 44741, loss = 2.69949026\n",
      "Iteration 44742, loss = 2.90185999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 44743, loss = 2.14763173\n",
      "Iteration 44744, loss = 1.91537495\n",
      "Iteration 44745, loss = 1.79973327\n",
      "Iteration 44746, loss = 1.84878961\n",
      "Iteration 44747, loss = 1.72421244\n",
      "Iteration 44748, loss = 1.69845543\n",
      "Iteration 44749, loss = 1.85011717\n",
      "Iteration 44750, loss = 1.83115951\n",
      "Iteration 44751, loss = 1.69569138\n",
      "Iteration 44752, loss = 1.61224000\n",
      "Iteration 44753, loss = 1.94360543\n",
      "Iteration 44754, loss = 1.73608889\n",
      "Iteration 44755, loss = 1.57157693\n",
      "Iteration 44756, loss = 1.55118573\n",
      "Iteration 44757, loss = 1.60142970\n",
      "Iteration 44758, loss = 1.49563906\n",
      "Iteration 44759, loss = 1.52509748\n",
      "Iteration 44760, loss = 1.61331271\n",
      "Iteration 44761, loss = 1.53716510\n",
      "Iteration 44762, loss = 1.66487138\n",
      "Iteration 44763, loss = 1.67960637\n",
      "Iteration 44764, loss = 2.27542702\n",
      "Iteration 44765, loss = 1.92739267\n",
      "Iteration 44766, loss = 2.00588886\n",
      "Iteration 44767, loss = 2.02580822\n",
      "Iteration 44768, loss = 1.67919728\n",
      "Iteration 44769, loss = 1.91881378\n",
      "Iteration 44770, loss = 1.87453391\n",
      "Iteration 44771, loss = 1.78228124\n",
      "Iteration 44772, loss = 1.68179881\n",
      "Iteration 44773, loss = 1.87393845\n",
      "Iteration 44774, loss = 1.83682125\n",
      "Iteration 44775, loss = 2.41493393\n",
      "Iteration 44776, loss = 3.02539370\n",
      "Iteration 44777, loss = 2.62246522\n",
      "Iteration 44778, loss = 2.66767851\n",
      "Iteration 44779, loss = 3.34460461\n",
      "Iteration 44780, loss = 3.45968844\n",
      "Iteration 44781, loss = 4.15313436\n",
      "Iteration 44782, loss = 3.06198524\n",
      "Iteration 44783, loss = 2.37630676\n",
      "Iteration 44784, loss = 2.02850845\n",
      "Iteration 44785, loss = 1.87146649\n",
      "Iteration 44786, loss = 1.89604367\n",
      "Iteration 44787, loss = 2.04094243\n",
      "Iteration 44788, loss = 1.72876639\n",
      "Iteration 44789, loss = 1.74543124\n",
      "Iteration 44790, loss = 1.73718711\n",
      "Iteration 44791, loss = 1.76532667\n",
      "Iteration 44792, loss = 1.69359662\n",
      "Iteration 44793, loss = 1.63769901\n",
      "Iteration 44794, loss = 1.62168401\n",
      "Iteration 44795, loss = 1.64296626\n",
      "Iteration 44796, loss = 1.54908169\n",
      "Iteration 44797, loss = 2.04537341\n",
      "Iteration 44798, loss = 2.45160250\n",
      "Iteration 44799, loss = 2.18907134\n",
      "Iteration 44800, loss = 1.78574554\n",
      "Iteration 44801, loss = 1.78792369\n",
      "Iteration 44802, loss = 1.65165721\n",
      "Iteration 44803, loss = 1.67074375\n",
      "Iteration 44804, loss = 2.03857553\n",
      "Iteration 44805, loss = 2.06061688\n",
      "Iteration 44806, loss = 2.38672042\n",
      "Iteration 44807, loss = 2.36949200\n",
      "Iteration 44808, loss = 2.56994347\n",
      "Iteration 44809, loss = 2.49393883\n",
      "Iteration 44810, loss = 3.35662262\n",
      "Iteration 44811, loss = 3.10436928\n",
      "Iteration 44812, loss = 3.15389233\n",
      "Iteration 44813, loss = 2.68714175\n",
      "Iteration 44814, loss = 2.48699851\n",
      "Iteration 44815, loss = 2.14251112\n",
      "Iteration 44816, loss = 2.25162776\n",
      "Iteration 44817, loss = 2.64314817\n",
      "Iteration 44818, loss = 2.64283660\n",
      "Iteration 44819, loss = 2.74583307\n",
      "Iteration 44820, loss = 2.67154709\n",
      "Iteration 44821, loss = 1.89015209\n",
      "Iteration 44822, loss = 1.83472576\n",
      "Iteration 44823, loss = 1.79851167\n",
      "Iteration 44824, loss = 2.58185124\n",
      "Iteration 44825, loss = 2.07040983\n",
      "Iteration 44826, loss = 2.62724703\n",
      "Iteration 44827, loss = 2.07826029\n",
      "Iteration 44828, loss = 2.01486257\n",
      "Iteration 44829, loss = 2.00366238\n",
      "Iteration 44830, loss = 2.22872564\n",
      "Iteration 44831, loss = 2.30116250\n",
      "Iteration 44832, loss = 1.81359519\n",
      "Iteration 44833, loss = 1.90565220\n",
      "Iteration 44834, loss = 2.22444173\n",
      "Iteration 44835, loss = 2.45318864\n",
      "Iteration 44836, loss = 2.22374598\n",
      "Iteration 44837, loss = 2.24278103\n",
      "Iteration 44838, loss = 1.99638575\n",
      "Iteration 44839, loss = 1.54729177\n",
      "Iteration 44840, loss = 1.91790947\n",
      "Iteration 44841, loss = 1.65487172\n",
      "Iteration 44842, loss = 1.73620371\n",
      "Iteration 44843, loss = 1.78462533\n",
      "Iteration 44844, loss = 1.56498449\n",
      "Iteration 44845, loss = 1.60475266\n",
      "Iteration 44846, loss = 1.75493107\n",
      "Iteration 44847, loss = 1.99734583\n",
      "Iteration 44848, loss = 2.37662391\n",
      "Iteration 44849, loss = 2.38562740\n",
      "Iteration 44850, loss = 2.40361872\n",
      "Iteration 44851, loss = 2.48944093\n",
      "Iteration 44852, loss = 1.96041493\n",
      "Iteration 44853, loss = 1.81326734\n",
      "Iteration 44854, loss = 1.95748492\n",
      "Iteration 44855, loss = 2.02600868\n",
      "Iteration 44856, loss = 2.12527664\n",
      "Iteration 44857, loss = 1.97820299\n",
      "Iteration 44858, loss = 1.66604167\n",
      "Iteration 44859, loss = 1.65447698\n",
      "Iteration 44860, loss = 1.77737987\n",
      "Iteration 44861, loss = 1.74394991\n",
      "Iteration 44862, loss = 1.81209516\n",
      "Iteration 44863, loss = 1.67846974\n",
      "Iteration 44864, loss = 1.64728679\n",
      "Iteration 44865, loss = 1.58576989\n",
      "Iteration 44866, loss = 2.23978930\n",
      "Iteration 44867, loss = 1.91074220\n",
      "Iteration 44868, loss = 1.90673127\n",
      "Iteration 44869, loss = 1.63078802\n",
      "Iteration 44870, loss = 1.87403260\n",
      "Iteration 44871, loss = 2.27940764\n",
      "Iteration 44872, loss = 1.83110923\n",
      "Iteration 44873, loss = 1.64372443\n",
      "Iteration 44874, loss = 1.52827336\n",
      "Iteration 44875, loss = 1.47460132\n",
      "Iteration 44876, loss = 1.63333492\n",
      "Iteration 44877, loss = 1.47975657\n",
      "Iteration 44878, loss = 1.54667315\n",
      "Iteration 44879, loss = 1.64061661\n",
      "Iteration 44880, loss = 2.11359269\n",
      "Iteration 44881, loss = 2.55650144\n",
      "Iteration 44882, loss = 2.34887293\n",
      "Iteration 44883, loss = 2.48451262\n",
      "Iteration 44884, loss = 2.07883150\n",
      "Iteration 44885, loss = 2.14294812\n",
      "Iteration 44886, loss = 2.29971037\n",
      "Iteration 44887, loss = 2.00159703\n",
      "Iteration 44888, loss = 2.28116346\n",
      "Iteration 44889, loss = 2.37583349\n",
      "Iteration 44890, loss = 2.40275868\n",
      "Iteration 44891, loss = 2.46006620\n",
      "Iteration 44892, loss = 3.07360137\n",
      "Iteration 44893, loss = 2.87691557\n",
      "Iteration 44894, loss = 3.93924221\n",
      "Iteration 44895, loss = 2.63458178\n",
      "Iteration 44896, loss = 2.23340749\n",
      "Iteration 44897, loss = 2.35633113\n",
      "Iteration 44898, loss = 1.89892829\n",
      "Iteration 44899, loss = 2.31981049\n",
      "Iteration 44900, loss = 2.14855512\n",
      "Iteration 44901, loss = 2.21813186\n",
      "Iteration 44902, loss = 1.92071479\n",
      "Iteration 44903, loss = 1.72825761\n",
      "Iteration 44904, loss = 1.92816027\n",
      "Iteration 44905, loss = 1.80171172\n",
      "Iteration 44906, loss = 1.85337210\n",
      "Iteration 44907, loss = 1.60783136\n",
      "Iteration 44908, loss = 1.77947445\n",
      "Iteration 44909, loss = 1.49130337\n",
      "Iteration 44910, loss = 1.63839700\n",
      "Iteration 44911, loss = 2.00097570\n",
      "Iteration 44912, loss = 2.29721029\n",
      "Iteration 44913, loss = 2.67857942\n",
      "Iteration 44914, loss = 2.32517311\n",
      "Iteration 44915, loss = 2.23502609\n",
      "Iteration 44916, loss = 2.50183726\n",
      "Iteration 44917, loss = 2.05606069\n",
      "Iteration 44918, loss = 1.98887041\n",
      "Iteration 44919, loss = 2.20011231\n",
      "Iteration 44920, loss = 1.98055015\n",
      "Iteration 44921, loss = 2.02623289\n",
      "Iteration 44922, loss = 2.26954914\n",
      "Iteration 44923, loss = 1.97175081\n",
      "Iteration 44924, loss = 2.41422145\n",
      "Iteration 44925, loss = 1.80584978\n",
      "Iteration 44926, loss = 1.71780450\n",
      "Iteration 44927, loss = 1.78143902\n",
      "Iteration 44928, loss = 1.65869316\n",
      "Iteration 44929, loss = 1.81349616\n",
      "Iteration 44930, loss = 1.85470790\n",
      "Iteration 44931, loss = 2.03279298\n",
      "Iteration 44932, loss = 1.78948070\n",
      "Iteration 44933, loss = 1.59844277\n",
      "Iteration 44934, loss = 2.11956856\n",
      "Iteration 44935, loss = 2.32246520\n",
      "Iteration 44936, loss = 1.88692633\n",
      "Iteration 44937, loss = 2.03260526\n",
      "Iteration 44938, loss = 2.32960066\n",
      "Iteration 44939, loss = 2.68690922\n",
      "Iteration 44940, loss = 2.29149212\n",
      "Iteration 44941, loss = 2.75098898\n",
      "Iteration 44942, loss = 1.96550939\n",
      "Iteration 44943, loss = 1.69574086\n",
      "Iteration 44944, loss = 1.93765572\n",
      "Iteration 44945, loss = 1.83442326\n",
      "Iteration 44946, loss = 1.76184303\n",
      "Iteration 44947, loss = 1.74426437\n",
      "Iteration 44948, loss = 1.50411838\n",
      "Iteration 44949, loss = 1.70959594\n",
      "Iteration 44950, loss = 1.81257108\n",
      "Iteration 44951, loss = 1.66066748\n",
      "Iteration 44952, loss = 1.80290146\n",
      "Iteration 44953, loss = 2.26934641\n",
      "Iteration 44954, loss = 2.06812398\n",
      "Iteration 44955, loss = 1.70278905\n",
      "Iteration 44956, loss = 1.69241527\n",
      "Iteration 44957, loss = 2.22828312\n",
      "Iteration 44958, loss = 1.97797551\n",
      "Iteration 44959, loss = 2.20583682\n",
      "Iteration 44960, loss = 2.32083135\n",
      "Iteration 44961, loss = 2.63730692\n",
      "Iteration 44962, loss = 2.15672195\n",
      "Iteration 44963, loss = 2.21136655\n",
      "Iteration 44964, loss = 1.87508839\n",
      "Iteration 44965, loss = 2.58106791\n",
      "Iteration 44966, loss = 2.93033785\n",
      "Iteration 44967, loss = 3.31171659\n",
      "Iteration 44968, loss = 2.67148661\n",
      "Iteration 44969, loss = 3.65263574\n",
      "Iteration 44970, loss = 2.92627293\n",
      "Iteration 44971, loss = 4.37605998\n",
      "Iteration 44972, loss = 5.37949541\n",
      "Iteration 44973, loss = 4.51907151\n",
      "Iteration 44974, loss = 4.45459940\n",
      "Iteration 44975, loss = 2.80869853\n",
      "Iteration 44976, loss = 2.90379787\n",
      "Iteration 44977, loss = 2.29250383\n",
      "Iteration 44978, loss = 2.37358203\n",
      "Iteration 44979, loss = 2.02977381\n",
      "Iteration 44980, loss = 1.83316460\n",
      "Iteration 44981, loss = 2.09741429\n",
      "Iteration 44982, loss = 2.21003076\n",
      "Iteration 44983, loss = 1.97359487\n",
      "Iteration 44984, loss = 1.81934539\n",
      "Iteration 44985, loss = 1.99135929\n",
      "Iteration 44986, loss = 1.73380640\n",
      "Iteration 44987, loss = 1.85916228\n",
      "Iteration 44988, loss = 1.63731060\n",
      "Iteration 44989, loss = 1.54070602\n",
      "Iteration 44990, loss = 1.65790681\n",
      "Iteration 44991, loss = 1.46299143\n",
      "Iteration 44992, loss = 1.56614062\n",
      "Iteration 44993, loss = 1.49518695\n",
      "Iteration 44994, loss = 1.49857093\n",
      "Iteration 44995, loss = 1.54469142\n",
      "Iteration 44996, loss = 1.74699876\n",
      "Iteration 44997, loss = 2.03929690\n",
      "Iteration 44998, loss = 1.92642323\n",
      "Iteration 44999, loss = 2.52015103\n",
      "Iteration 45000, loss = 2.10854611\n",
      "Iteration 45001, loss = 2.16328848\n",
      "Iteration 45002, loss = 2.01230399\n",
      "Iteration 45003, loss = 1.81363745\n",
      "Iteration 45004, loss = 1.65141146\n",
      "Iteration 45005, loss = 1.92888262\n",
      "Iteration 45006, loss = 1.97114705\n",
      "Iteration 45007, loss = 1.72372591\n",
      "Iteration 45008, loss = 1.77240174\n",
      "Iteration 45009, loss = 1.72282015\n",
      "Iteration 45010, loss = 2.10796056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 45011, loss = 2.21077404\n",
      "Iteration 45012, loss = 2.30280693\n",
      "Iteration 45013, loss = 2.52797428\n",
      "Iteration 45014, loss = 2.26992334\n",
      "Iteration 45015, loss = 2.53753364\n",
      "Iteration 45016, loss = 2.01190549\n",
      "Iteration 45017, loss = 2.23867222\n",
      "Iteration 45018, loss = 2.02679827\n",
      "Iteration 45019, loss = 2.24832221\n",
      "Iteration 45020, loss = 2.06340847\n",
      "Iteration 45021, loss = 1.68567280\n",
      "Iteration 45022, loss = 1.61024584\n",
      "Iteration 45023, loss = 1.63266323\n",
      "Iteration 45024, loss = 1.64904815\n",
      "Iteration 45025, loss = 1.56475202\n",
      "Iteration 45026, loss = 1.58239369\n",
      "Iteration 45027, loss = 1.62904433\n",
      "Iteration 45028, loss = 1.73827244\n",
      "Iteration 45029, loss = 1.84987857\n",
      "Iteration 45030, loss = 1.53465695\n",
      "Iteration 45031, loss = 1.61017125\n",
      "Iteration 45032, loss = 1.66737530\n",
      "Iteration 45033, loss = 1.71451457\n",
      "Iteration 45034, loss = 1.78637594\n",
      "Iteration 45035, loss = 1.90008529\n",
      "Iteration 45036, loss = 1.89281312\n",
      "Iteration 45037, loss = 2.15404204\n",
      "Iteration 45038, loss = 1.90595798\n",
      "Iteration 45039, loss = 1.88752880\n",
      "Iteration 45040, loss = 1.60350641\n",
      "Iteration 45041, loss = 1.71623743\n",
      "Iteration 45042, loss = 1.72191897\n",
      "Iteration 45043, loss = 1.92925664\n",
      "Iteration 45044, loss = 2.08585986\n",
      "Iteration 45045, loss = 1.66757460\n",
      "Iteration 45046, loss = 2.13592699\n",
      "Iteration 45047, loss = 2.06887844\n",
      "Iteration 45048, loss = 2.59824319\n",
      "Iteration 45049, loss = 2.60237489\n",
      "Iteration 45050, loss = 1.82748049\n",
      "Iteration 45051, loss = 2.05166693\n",
      "Iteration 45052, loss = 2.00361279\n",
      "Iteration 45053, loss = 1.74641637\n",
      "Iteration 45054, loss = 1.53824918\n",
      "Iteration 45055, loss = 1.53183872\n",
      "Iteration 45056, loss = 1.58644559\n",
      "Iteration 45057, loss = 1.77879628\n",
      "Iteration 45058, loss = 1.61793452\n",
      "Iteration 45059, loss = 1.50342902\n",
      "Iteration 45060, loss = 1.74148434\n",
      "Iteration 45061, loss = 1.79534554\n",
      "Iteration 45062, loss = 1.73971042\n",
      "Iteration 45063, loss = 1.94777598\n",
      "Iteration 45064, loss = 2.24914976\n",
      "Iteration 45065, loss = 2.53074514\n",
      "Iteration 45066, loss = 2.49075898\n",
      "Iteration 45067, loss = 2.24451125\n",
      "Iteration 45068, loss = 2.58802307\n",
      "Iteration 45069, loss = 2.01203702\n",
      "Iteration 45070, loss = 2.09393339\n",
      "Iteration 45071, loss = 1.74281333\n",
      "Iteration 45072, loss = 1.83974013\n",
      "Iteration 45073, loss = 1.80493757\n",
      "Iteration 45074, loss = 1.85176667\n",
      "Iteration 45075, loss = 2.05021189\n",
      "Iteration 45076, loss = 2.17409040\n",
      "Iteration 45077, loss = 1.91330016\n",
      "Iteration 45078, loss = 1.83299921\n",
      "Iteration 45079, loss = 1.64660839\n",
      "Iteration 45080, loss = 1.65172841\n",
      "Iteration 45081, loss = 1.64255735\n",
      "Iteration 45082, loss = 1.87882954\n",
      "Iteration 45083, loss = 1.73971184\n",
      "Iteration 45084, loss = 1.62395120\n",
      "Iteration 45085, loss = 1.77222845\n",
      "Iteration 45086, loss = 1.83884464\n",
      "Iteration 45087, loss = 1.52690872\n",
      "Iteration 45088, loss = 1.54557256\n",
      "Iteration 45089, loss = 1.64669843\n",
      "Iteration 45090, loss = 1.81892973\n",
      "Iteration 45091, loss = 1.78955706\n",
      "Iteration 45092, loss = 1.96655497\n",
      "Iteration 45093, loss = 1.70156717\n",
      "Iteration 45094, loss = 1.78700942\n",
      "Iteration 45095, loss = 1.53861983\n",
      "Iteration 45096, loss = 1.75517397\n",
      "Iteration 45097, loss = 1.90509394\n",
      "Iteration 45098, loss = 1.87824268\n",
      "Iteration 45099, loss = 1.63917690\n",
      "Iteration 45100, loss = 1.82353078\n",
      "Iteration 45101, loss = 1.83091943\n",
      "Iteration 45102, loss = 2.01715664\n",
      "Iteration 45103, loss = 2.29788664\n",
      "Iteration 45104, loss = 1.96743224\n",
      "Iteration 45105, loss = 2.30946816\n",
      "Iteration 45106, loss = 1.88532054\n",
      "Iteration 45107, loss = 1.77163937\n",
      "Iteration 45108, loss = 1.57215778\n",
      "Iteration 45109, loss = 1.52455532\n",
      "Iteration 45110, loss = 1.70612622\n",
      "Iteration 45111, loss = 1.71010262\n",
      "Iteration 45112, loss = 1.56140155\n",
      "Iteration 45113, loss = 1.72562848\n",
      "Iteration 45114, loss = 1.89580965\n",
      "Iteration 45115, loss = 1.83316137\n",
      "Iteration 45116, loss = 1.99056231\n",
      "Iteration 45117, loss = 2.03801364\n",
      "Iteration 45118, loss = 1.87523440\n",
      "Iteration 45119, loss = 2.71858812\n",
      "Iteration 45120, loss = 3.41555687\n",
      "Iteration 45121, loss = 3.98841030\n",
      "Iteration 45122, loss = 2.71421141\n",
      "Iteration 45123, loss = 1.93804313\n",
      "Iteration 45124, loss = 1.55563821\n",
      "Iteration 45125, loss = 1.78284076\n",
      "Iteration 45126, loss = 2.10697809\n",
      "Iteration 45127, loss = 1.89121570\n",
      "Iteration 45128, loss = 1.79921058\n",
      "Iteration 45129, loss = 1.87200037\n",
      "Iteration 45130, loss = 2.14276397\n",
      "Iteration 45131, loss = 2.12757362\n",
      "Iteration 45132, loss = 2.05637353\n",
      "Iteration 45133, loss = 2.09793689\n",
      "Iteration 45134, loss = 1.89173496\n",
      "Iteration 45135, loss = 1.82787567\n",
      "Iteration 45136, loss = 1.93191606\n",
      "Iteration 45137, loss = 1.90495123\n",
      "Iteration 45138, loss = 1.93176960\n",
      "Iteration 45139, loss = 1.85513882\n",
      "Iteration 45140, loss = 2.00499763\n",
      "Iteration 45141, loss = 1.94293891\n",
      "Iteration 45142, loss = 1.92705782\n",
      "Iteration 45143, loss = 1.81819989\n",
      "Iteration 45144, loss = 1.78390093\n",
      "Iteration 45145, loss = 1.70285070\n",
      "Iteration 45146, loss = 1.64592601\n",
      "Iteration 45147, loss = 2.04035461\n",
      "Iteration 45148, loss = 2.22718461\n",
      "Iteration 45149, loss = 2.11588640\n",
      "Iteration 45150, loss = 2.22408082\n",
      "Iteration 45151, loss = 2.01502409\n",
      "Iteration 45152, loss = 2.53898417\n",
      "Iteration 45153, loss = 2.49179985\n",
      "Iteration 45154, loss = 2.20738266\n",
      "Iteration 45155, loss = 2.07539694\n",
      "Iteration 45156, loss = 1.99724993\n",
      "Iteration 45157, loss = 1.73504000\n",
      "Iteration 45158, loss = 1.91608904\n",
      "Iteration 45159, loss = 1.80115441\n",
      "Iteration 45160, loss = 1.86728276\n",
      "Iteration 45161, loss = 1.62634841\n",
      "Iteration 45162, loss = 1.61705014\n",
      "Iteration 45163, loss = 1.87847003\n",
      "Iteration 45164, loss = 2.06378470\n",
      "Iteration 45165, loss = 1.78907512\n",
      "Iteration 45166, loss = 1.76172135\n",
      "Iteration 45167, loss = 1.93938827\n",
      "Iteration 45168, loss = 1.76018747\n",
      "Iteration 45169, loss = 2.01941037\n",
      "Iteration 45170, loss = 1.59037580\n",
      "Iteration 45171, loss = 1.44754475\n",
      "Iteration 45172, loss = 1.52297239\n",
      "Iteration 45173, loss = 1.63474476\n",
      "Iteration 45174, loss = 1.64020748\n",
      "Iteration 45175, loss = 1.92394127\n",
      "Iteration 45176, loss = 1.80785024\n",
      "Iteration 45177, loss = 1.82840636\n",
      "Iteration 45178, loss = 1.88676788\n",
      "Iteration 45179, loss = 2.37002532\n",
      "Iteration 45180, loss = 2.07988658\n",
      "Iteration 45181, loss = 1.83988562\n",
      "Iteration 45182, loss = 1.67726382\n",
      "Iteration 45183, loss = 1.55167410\n",
      "Iteration 45184, loss = 1.69474207\n",
      "Iteration 45185, loss = 1.89100678\n",
      "Iteration 45186, loss = 1.70870659\n",
      "Iteration 45187, loss = 1.91556498\n",
      "Iteration 45188, loss = 2.27001034\n",
      "Iteration 45189, loss = 1.90605505\n",
      "Iteration 45190, loss = 1.93839074\n",
      "Iteration 45191, loss = 1.73902585\n",
      "Iteration 45192, loss = 1.70874512\n",
      "Iteration 45193, loss = 2.07973559\n",
      "Iteration 45194, loss = 2.10588617\n",
      "Iteration 45195, loss = 1.82747557\n",
      "Iteration 45196, loss = 2.64839059\n",
      "Iteration 45197, loss = 2.81143462\n",
      "Iteration 45198, loss = 2.75699323\n",
      "Iteration 45199, loss = 2.33353253\n",
      "Iteration 45200, loss = 1.72024317\n",
      "Iteration 45201, loss = 1.74112675\n",
      "Iteration 45202, loss = 1.70216133\n",
      "Iteration 45203, loss = 1.71951062\n",
      "Iteration 45204, loss = 1.69338896\n",
      "Iteration 45205, loss = 1.85746802\n",
      "Iteration 45206, loss = 1.58463916\n",
      "Iteration 45207, loss = 1.77014508\n",
      "Iteration 45208, loss = 1.70095455\n",
      "Iteration 45209, loss = 2.35598988\n",
      "Iteration 45210, loss = 2.93302335\n",
      "Iteration 45211, loss = 2.65436728\n",
      "Iteration 45212, loss = 2.06902275\n",
      "Iteration 45213, loss = 1.85589142\n",
      "Iteration 45214, loss = 1.87618825\n",
      "Iteration 45215, loss = 1.91706989\n",
      "Iteration 45216, loss = 1.86095044\n",
      "Iteration 45217, loss = 2.01997320\n",
      "Iteration 45218, loss = 2.35118745\n",
      "Iteration 45219, loss = 2.61647917\n",
      "Iteration 45220, loss = 4.15650286\n",
      "Iteration 45221, loss = 3.98895729\n",
      "Iteration 45222, loss = 4.53082561\n",
      "Iteration 45223, loss = 6.89696425\n",
      "Iteration 45224, loss = 7.25491563\n",
      "Iteration 45225, loss = 6.13222078\n",
      "Iteration 45226, loss = 4.34867745\n",
      "Iteration 45227, loss = 3.34579563\n",
      "Iteration 45228, loss = 4.15371856\n",
      "Iteration 45229, loss = 2.70810129\n",
      "Iteration 45230, loss = 3.27242669\n",
      "Iteration 45231, loss = 2.89357544\n",
      "Iteration 45232, loss = 2.51026170\n",
      "Iteration 45233, loss = 2.20607313\n",
      "Iteration 45234, loss = 2.20978899\n",
      "Iteration 45235, loss = 1.67143962\n",
      "Iteration 45236, loss = 2.02265023\n",
      "Iteration 45237, loss = 1.87672007\n",
      "Iteration 45238, loss = 1.81795857\n",
      "Iteration 45239, loss = 1.74298717\n",
      "Iteration 45240, loss = 1.81271767\n",
      "Iteration 45241, loss = 2.25073227\n",
      "Iteration 45242, loss = 2.08205001\n",
      "Iteration 45243, loss = 1.88582721\n",
      "Iteration 45244, loss = 1.66406539\n",
      "Iteration 45245, loss = 1.86667889\n",
      "Iteration 45246, loss = 1.87066841\n",
      "Iteration 45247, loss = 1.97091053\n",
      "Iteration 45248, loss = 1.66301657\n",
      "Iteration 45249, loss = 1.43139582\n",
      "Iteration 45250, loss = 1.74592612\n",
      "Iteration 45251, loss = 1.58787056\n",
      "Iteration 45252, loss = 1.68656066\n",
      "Iteration 45253, loss = 1.82749509\n",
      "Iteration 45254, loss = 2.02998149\n",
      "Iteration 45255, loss = 1.97857489\n",
      "Iteration 45256, loss = 1.61323457\n",
      "Iteration 45257, loss = 1.77798436\n",
      "Iteration 45258, loss = 1.85182530\n",
      "Iteration 45259, loss = 1.79905483\n",
      "Iteration 45260, loss = 1.64747741\n",
      "Iteration 45261, loss = 1.73573260\n",
      "Iteration 45262, loss = 1.93342031\n",
      "Iteration 45263, loss = 1.92762216\n",
      "Iteration 45264, loss = 1.78947572\n",
      "Iteration 45265, loss = 1.75813934\n",
      "Iteration 45266, loss = 1.65229361\n",
      "Iteration 45267, loss = 1.73303210\n",
      "Iteration 45268, loss = 1.83437741\n",
      "Iteration 45269, loss = 3.05061078\n",
      "Iteration 45270, loss = 2.80448870\n",
      "Iteration 45271, loss = 2.40104067\n",
      "Iteration 45272, loss = 2.59016502\n",
      "Iteration 45273, loss = 2.06264937\n",
      "Iteration 45274, loss = 1.83013882\n",
      "Iteration 45275, loss = 1.76525262\n",
      "Iteration 45276, loss = 1.64068375\n",
      "Iteration 45277, loss = 1.73136705\n",
      "Iteration 45278, loss = 1.63137547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 45279, loss = 1.42625331\n",
      "Iteration 45280, loss = 1.54120656\n",
      "Iteration 45281, loss = 1.58559418\n",
      "Iteration 45282, loss = 1.70784597\n",
      "Iteration 45283, loss = 1.65063087\n",
      "Iteration 45284, loss = 1.62735616\n",
      "Iteration 45285, loss = 1.79430776\n",
      "Iteration 45286, loss = 2.38475254\n",
      "Iteration 45287, loss = 2.37617603\n",
      "Iteration 45288, loss = 2.41555871\n",
      "Iteration 45289, loss = 2.89397658\n",
      "Iteration 45290, loss = 3.04942870\n",
      "Iteration 45291, loss = 2.78225527\n",
      "Iteration 45292, loss = 2.52535358\n",
      "Iteration 45293, loss = 2.40544057\n",
      "Iteration 45294, loss = 2.29227288\n",
      "Iteration 45295, loss = 3.21922291\n",
      "Iteration 45296, loss = 2.39530193\n",
      "Iteration 45297, loss = 1.82606145\n",
      "Iteration 45298, loss = 1.97101533\n",
      "Iteration 45299, loss = 1.86650093\n",
      "Iteration 45300, loss = 2.32024170\n",
      "Iteration 45301, loss = 2.33393188\n",
      "Iteration 45302, loss = 2.70850767\n",
      "Iteration 45303, loss = 2.30382787\n",
      "Iteration 45304, loss = 2.33160400\n",
      "Iteration 45305, loss = 1.99629083\n",
      "Iteration 45306, loss = 1.46384108\n",
      "Iteration 45307, loss = 1.51350531\n",
      "Iteration 45308, loss = 1.46983889\n",
      "Iteration 45309, loss = 1.54985833\n",
      "Iteration 45310, loss = 1.57223698\n",
      "Iteration 45311, loss = 1.58990561\n",
      "Iteration 45312, loss = 1.63635111\n",
      "Iteration 45313, loss = 1.48149755\n",
      "Iteration 45314, loss = 1.65151836\n",
      "Iteration 45315, loss = 2.20635339\n",
      "Iteration 45316, loss = 2.31181826\n",
      "Iteration 45317, loss = 2.05275682\n",
      "Iteration 45318, loss = 1.81126622\n",
      "Iteration 45319, loss = 1.83340046\n",
      "Iteration 45320, loss = 1.58029503\n",
      "Iteration 45321, loss = 1.68312227\n",
      "Iteration 45322, loss = 1.61934862\n",
      "Iteration 45323, loss = 2.03445850\n",
      "Iteration 45324, loss = 2.32004237\n",
      "Iteration 45325, loss = 1.85044097\n",
      "Iteration 45326, loss = 2.27987154\n",
      "Iteration 45327, loss = 2.27963627\n",
      "Iteration 45328, loss = 1.85009432\n",
      "Iteration 45329, loss = 1.93802700\n",
      "Iteration 45330, loss = 1.79813402\n",
      "Iteration 45331, loss = 1.72365676\n",
      "Iteration 45332, loss = 1.80021101\n",
      "Iteration 45333, loss = 1.84968863\n",
      "Iteration 45334, loss = 1.95861012\n",
      "Iteration 45335, loss = 1.96348618\n",
      "Iteration 45336, loss = 2.50356653\n",
      "Iteration 45337, loss = 2.25935680\n",
      "Iteration 45338, loss = 2.13224034\n",
      "Iteration 45339, loss = 1.86996437\n",
      "Iteration 45340, loss = 1.93185796\n",
      "Iteration 45341, loss = 2.08934598\n",
      "Iteration 45342, loss = 2.28964011\n",
      "Iteration 45343, loss = 2.06467218\n",
      "Iteration 45344, loss = 2.04249665\n",
      "Iteration 45345, loss = 1.93755943\n",
      "Iteration 45346, loss = 2.18492848\n",
      "Iteration 45347, loss = 1.59898488\n",
      "Iteration 45348, loss = 1.92659252\n",
      "Iteration 45349, loss = 2.02626405\n",
      "Iteration 45350, loss = 2.08276641\n",
      "Iteration 45351, loss = 2.17606226\n",
      "Iteration 45352, loss = 1.75675563\n",
      "Iteration 45353, loss = 2.31787125\n",
      "Iteration 45354, loss = 2.30838646\n",
      "Iteration 45355, loss = 3.03438746\n",
      "Iteration 45356, loss = 2.29965495\n",
      "Iteration 45357, loss = 2.34943393\n",
      "Iteration 45358, loss = 2.06674430\n",
      "Iteration 45359, loss = 2.35085689\n",
      "Iteration 45360, loss = 1.90885288\n",
      "Iteration 45361, loss = 2.30564562\n",
      "Iteration 45362, loss = 2.03073810\n",
      "Iteration 45363, loss = 2.16949283\n",
      "Iteration 45364, loss = 2.79696011\n",
      "Iteration 45365, loss = 2.56794508\n",
      "Iteration 45366, loss = 2.55788046\n",
      "Iteration 45367, loss = 1.89762275\n",
      "Iteration 45368, loss = 1.70601713\n",
      "Iteration 45369, loss = 1.80757163\n",
      "Iteration 45370, loss = 1.93292694\n",
      "Iteration 45371, loss = 1.96625432\n",
      "Iteration 45372, loss = 2.15202637\n",
      "Iteration 45373, loss = 2.41333124\n",
      "Iteration 45374, loss = 3.09384235\n",
      "Iteration 45375, loss = 3.51225652\n",
      "Iteration 45376, loss = 3.52031232\n",
      "Iteration 45377, loss = 2.77403089\n",
      "Iteration 45378, loss = 3.02663124\n",
      "Iteration 45379, loss = 4.65407731\n",
      "Iteration 45380, loss = 3.12878205\n",
      "Iteration 45381, loss = 2.80374785\n",
      "Iteration 45382, loss = 3.46956972\n",
      "Iteration 45383, loss = 2.13737678\n",
      "Iteration 45384, loss = 1.77107915\n",
      "Iteration 45385, loss = 1.93526952\n",
      "Iteration 45386, loss = 1.81296647\n",
      "Iteration 45387, loss = 1.60796758\n",
      "Iteration 45388, loss = 1.50456572\n",
      "Iteration 45389, loss = 1.59078154\n",
      "Iteration 45390, loss = 1.56682639\n",
      "Iteration 45391, loss = 1.68930625\n",
      "Iteration 45392, loss = 1.70721607\n",
      "Iteration 45393, loss = 1.62833985\n",
      "Iteration 45394, loss = 1.72521535\n",
      "Iteration 45395, loss = 1.86118659\n",
      "Iteration 45396, loss = 1.78963177\n",
      "Iteration 45397, loss = 1.74649699\n",
      "Iteration 45398, loss = 1.90514573\n",
      "Iteration 45399, loss = 1.83105054\n",
      "Iteration 45400, loss = 1.77747525\n",
      "Iteration 45401, loss = 1.84008501\n",
      "Iteration 45402, loss = 1.73199844\n",
      "Iteration 45403, loss = 1.58206190\n",
      "Iteration 45404, loss = 1.77673736\n",
      "Iteration 45405, loss = 1.74217425\n",
      "Iteration 45406, loss = 2.29700383\n",
      "Iteration 45407, loss = 2.15537504\n",
      "Iteration 45408, loss = 1.82035738\n",
      "Iteration 45409, loss = 2.00727456\n",
      "Iteration 45410, loss = 1.65968856\n",
      "Iteration 45411, loss = 1.50681384\n",
      "Iteration 45412, loss = 1.62235135\n",
      "Iteration 45413, loss = 1.70235364\n",
      "Iteration 45414, loss = 1.98134026\n",
      "Iteration 45415, loss = 2.70155501\n",
      "Iteration 45416, loss = 2.31699885\n",
      "Iteration 45417, loss = 1.97727132\n",
      "Iteration 45418, loss = 1.63361301\n",
      "Iteration 45419, loss = 1.93035346\n",
      "Iteration 45420, loss = 1.73796675\n",
      "Iteration 45421, loss = 1.72050826\n",
      "Iteration 45422, loss = 1.45568358\n",
      "Iteration 45423, loss = 1.66332964\n",
      "Iteration 45424, loss = 1.51067886\n",
      "Iteration 45425, loss = 1.67502775\n",
      "Iteration 45426, loss = 1.64888499\n",
      "Iteration 45427, loss = 1.54802190\n",
      "Iteration 45428, loss = 1.74001476\n",
      "Iteration 45429, loss = 2.19766452\n",
      "Iteration 45430, loss = 2.37807604\n",
      "Iteration 45431, loss = 2.10725252\n",
      "Iteration 45432, loss = 2.37422062\n",
      "Iteration 45433, loss = 2.08624198\n",
      "Iteration 45434, loss = 2.53959910\n",
      "Iteration 45435, loss = 1.97620765\n",
      "Iteration 45436, loss = 1.80589927\n",
      "Iteration 45437, loss = 1.52445273\n",
      "Iteration 45438, loss = 1.65694925\n",
      "Iteration 45439, loss = 1.80978745\n",
      "Iteration 45440, loss = 1.83069389\n",
      "Iteration 45441, loss = 1.99556266\n",
      "Iteration 45442, loss = 1.85098366\n",
      "Iteration 45443, loss = 2.17074776\n",
      "Iteration 45444, loss = 2.61074199\n",
      "Iteration 45445, loss = 2.71171773\n",
      "Iteration 45446, loss = 1.99936604\n",
      "Iteration 45447, loss = 2.15965424\n",
      "Iteration 45448, loss = 2.31520262\n",
      "Iteration 45449, loss = 1.88255191\n",
      "Iteration 45450, loss = 1.66582320\n",
      "Iteration 45451, loss = 1.46671912\n",
      "Iteration 45452, loss = 1.56372864\n",
      "Iteration 45453, loss = 1.78379147\n",
      "Iteration 45454, loss = 1.80686749\n",
      "Iteration 45455, loss = 2.10189917\n",
      "Iteration 45456, loss = 2.33565318\n",
      "Iteration 45457, loss = 1.81433737\n",
      "Iteration 45458, loss = 2.02406700\n",
      "Iteration 45459, loss = 2.33269137\n",
      "Iteration 45460, loss = 2.80271352\n",
      "Iteration 45461, loss = 2.99677970\n",
      "Iteration 45462, loss = 3.00950252\n",
      "Iteration 45463, loss = 4.06313255\n",
      "Iteration 45464, loss = 3.47841806\n",
      "Iteration 45465, loss = 3.98090470\n",
      "Iteration 45466, loss = 3.78492370\n",
      "Iteration 45467, loss = 3.08794146\n",
      "Iteration 45468, loss = 2.12544545\n",
      "Iteration 45469, loss = 1.81983510\n",
      "Iteration 45470, loss = 1.72278614\n",
      "Iteration 45471, loss = 1.70982794\n",
      "Iteration 45472, loss = 1.85661498\n",
      "Iteration 45473, loss = 1.61199948\n",
      "Iteration 45474, loss = 2.22316888\n",
      "Iteration 45475, loss = 1.97798914\n",
      "Iteration 45476, loss = 1.81073828\n",
      "Iteration 45477, loss = 2.32956836\n",
      "Iteration 45478, loss = 2.10136460\n",
      "Iteration 45479, loss = 2.04602144\n",
      "Iteration 45480, loss = 2.02435810\n",
      "Iteration 45481, loss = 1.81209482\n",
      "Iteration 45482, loss = 1.87646530\n",
      "Iteration 45483, loss = 2.21230001\n",
      "Iteration 45484, loss = 1.95978467\n",
      "Iteration 45485, loss = 2.51530557\n",
      "Iteration 45486, loss = 2.06329726\n",
      "Iteration 45487, loss = 1.72467194\n",
      "Iteration 45488, loss = 1.85258164\n",
      "Iteration 45489, loss = 1.59943454\n",
      "Iteration 45490, loss = 1.51071245\n",
      "Iteration 45491, loss = 1.48480697\n",
      "Iteration 45492, loss = 1.55486236\n",
      "Iteration 45493, loss = 1.49361155\n",
      "Iteration 45494, loss = 1.58348513\n",
      "Iteration 45495, loss = 1.71027779\n",
      "Iteration 45496, loss = 1.47820467\n",
      "Iteration 45497, loss = 1.71493055\n",
      "Iteration 45498, loss = 1.58712930\n",
      "Iteration 45499, loss = 1.61091599\n",
      "Iteration 45500, loss = 1.63288643\n",
      "Iteration 45501, loss = 1.54992389\n",
      "Iteration 45502, loss = 1.72611104\n",
      "Iteration 45503, loss = 1.88764701\n",
      "Iteration 45504, loss = 2.24942618\n",
      "Iteration 45505, loss = 2.17842505\n",
      "Iteration 45506, loss = 2.17665241\n",
      "Iteration 45507, loss = 2.17767612\n",
      "Iteration 45508, loss = 1.93822577\n",
      "Iteration 45509, loss = 1.91564971\n",
      "Iteration 45510, loss = 1.74506663\n",
      "Iteration 45511, loss = 2.04062708\n",
      "Iteration 45512, loss = 1.71231999\n",
      "Iteration 45513, loss = 1.58925847\n",
      "Iteration 45514, loss = 1.96668690\n",
      "Iteration 45515, loss = 2.10384627\n",
      "Iteration 45516, loss = 2.19964106\n",
      "Iteration 45517, loss = 2.61099719\n",
      "Iteration 45518, loss = 2.70052386\n",
      "Iteration 45519, loss = 2.48383026\n",
      "Iteration 45520, loss = 2.62072471\n",
      "Iteration 45521, loss = 2.80453101\n",
      "Iteration 45522, loss = 2.58759192\n",
      "Iteration 45523, loss = 2.33576574\n",
      "Iteration 45524, loss = 2.58629241\n",
      "Iteration 45525, loss = 2.64070250\n",
      "Iteration 45526, loss = 2.38925151\n",
      "Iteration 45527, loss = 2.43778678\n",
      "Iteration 45528, loss = 2.09974151\n",
      "Iteration 45529, loss = 2.20661213\n",
      "Iteration 45530, loss = 1.93026865\n",
      "Iteration 45531, loss = 2.30458027\n",
      "Iteration 45532, loss = 2.39952080\n",
      "Iteration 45533, loss = 2.41225039\n",
      "Iteration 45534, loss = 1.90443622\n",
      "Iteration 45535, loss = 1.85569760\n",
      "Iteration 45536, loss = 1.92396086\n",
      "Iteration 45537, loss = 1.59700574\n",
      "Iteration 45538, loss = 2.03365272\n",
      "Iteration 45539, loss = 1.66346336\n",
      "Iteration 45540, loss = 1.83166024\n",
      "Iteration 45541, loss = 1.80881570\n",
      "Iteration 45542, loss = 1.76251345\n",
      "Iteration 45543, loss = 1.81614590\n",
      "Iteration 45544, loss = 1.58053940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 45545, loss = 1.76666330\n",
      "Iteration 45546, loss = 2.05518938\n",
      "Iteration 45547, loss = 1.62873294\n",
      "Iteration 45548, loss = 1.80753667\n",
      "Iteration 45549, loss = 1.94779576\n",
      "Iteration 45550, loss = 1.62581034\n",
      "Iteration 45551, loss = 1.68107631\n",
      "Iteration 45552, loss = 1.74452917\n",
      "Iteration 45553, loss = 1.53918352\n",
      "Iteration 45554, loss = 1.62605827\n",
      "Iteration 45555, loss = 1.61053773\n",
      "Iteration 45556, loss = 1.59214709\n",
      "Iteration 45557, loss = 1.88603815\n",
      "Iteration 45558, loss = 1.92414311\n",
      "Iteration 45559, loss = 1.67412395\n",
      "Iteration 45560, loss = 1.92529192\n",
      "Iteration 45561, loss = 1.87313119\n",
      "Iteration 45562, loss = 1.65022585\n",
      "Iteration 45563, loss = 1.54118463\n",
      "Iteration 45564, loss = 1.52800843\n",
      "Iteration 45565, loss = 1.80176229\n",
      "Iteration 45566, loss = 1.81421667\n",
      "Iteration 45567, loss = 1.70546811\n",
      "Iteration 45568, loss = 1.90479044\n",
      "Iteration 45569, loss = 2.13857398\n",
      "Iteration 45570, loss = 1.57467359\n",
      "Iteration 45571, loss = 1.43143101\n",
      "Iteration 45572, loss = 1.69727337\n",
      "Iteration 45573, loss = 1.60777073\n",
      "Iteration 45574, loss = 1.78812337\n",
      "Iteration 45575, loss = 1.41663663\n",
      "Iteration 45576, loss = 1.49512316\n",
      "Iteration 45577, loss = 1.53447884\n",
      "Iteration 45578, loss = 1.61993331\n",
      "Iteration 45579, loss = 2.41816081\n",
      "Iteration 45580, loss = 2.74944071\n",
      "Iteration 45581, loss = 2.29558415\n",
      "Iteration 45582, loss = 1.74603660\n",
      "Iteration 45583, loss = 1.75820471\n",
      "Iteration 45584, loss = 1.89834666\n",
      "Iteration 45585, loss = 2.52187961\n",
      "Iteration 45586, loss = 2.19946510\n",
      "Iteration 45587, loss = 1.70633789\n",
      "Iteration 45588, loss = 1.91222846\n",
      "Iteration 45589, loss = 2.19294276\n",
      "Iteration 45590, loss = 1.72648422\n",
      "Iteration 45591, loss = 1.89390805\n",
      "Iteration 45592, loss = 1.76150185\n",
      "Iteration 45593, loss = 1.71623664\n",
      "Iteration 45594, loss = 1.48752372\n",
      "Iteration 45595, loss = 1.51558676\n",
      "Iteration 45596, loss = 1.58521227\n",
      "Iteration 45597, loss = 1.49054567\n",
      "Iteration 45598, loss = 1.49423634\n",
      "Iteration 45599, loss = 1.78029547\n",
      "Iteration 45600, loss = 1.92730468\n",
      "Iteration 45601, loss = 1.47288108\n",
      "Iteration 45602, loss = 1.47516134\n",
      "Iteration 45603, loss = 1.52667686\n",
      "Iteration 45604, loss = 1.76417230\n",
      "Iteration 45605, loss = 1.74278009\n",
      "Iteration 45606, loss = 2.24951446\n",
      "Iteration 45607, loss = 1.87494097\n",
      "Iteration 45608, loss = 1.99086693\n",
      "Iteration 45609, loss = 1.76331928\n",
      "Iteration 45610, loss = 1.75824896\n",
      "Iteration 45611, loss = 1.77904009\n",
      "Iteration 45612, loss = 1.70560547\n",
      "Iteration 45613, loss = 1.80007211\n",
      "Iteration 45614, loss = 1.73038899\n",
      "Iteration 45615, loss = 2.14955782\n",
      "Iteration 45616, loss = 2.40320202\n",
      "Iteration 45617, loss = 1.98271126\n",
      "Iteration 45618, loss = 1.88542211\n",
      "Iteration 45619, loss = 1.77203397\n",
      "Iteration 45620, loss = 1.87008942\n",
      "Iteration 45621, loss = 2.35831539\n",
      "Iteration 45622, loss = 2.25113508\n",
      "Iteration 45623, loss = 2.92582007\n",
      "Iteration 45624, loss = 2.03824026\n",
      "Iteration 45625, loss = 2.36557275\n",
      "Iteration 45626, loss = 2.96823929\n",
      "Iteration 45627, loss = 2.06812077\n",
      "Iteration 45628, loss = 2.45605724\n",
      "Iteration 45629, loss = 1.79521012\n",
      "Iteration 45630, loss = 1.80089685\n",
      "Iteration 45631, loss = 1.76478135\n",
      "Iteration 45632, loss = 1.47006577\n",
      "Iteration 45633, loss = 1.70307207\n",
      "Iteration 45634, loss = 1.88012176\n",
      "Iteration 45635, loss = 1.60375846\n",
      "Iteration 45636, loss = 1.56932250\n",
      "Iteration 45637, loss = 1.47143015\n",
      "Iteration 45638, loss = 1.46038579\n",
      "Iteration 45639, loss = 1.65131797\n",
      "Iteration 45640, loss = 1.63341695\n",
      "Iteration 45641, loss = 1.73282820\n",
      "Iteration 45642, loss = 1.57313718\n",
      "Iteration 45643, loss = 1.99502030\n",
      "Iteration 45644, loss = 2.03509223\n",
      "Iteration 45645, loss = 2.00818935\n",
      "Iteration 45646, loss = 1.89493325\n",
      "Iteration 45647, loss = 1.49091015\n",
      "Iteration 45648, loss = 1.42555737\n",
      "Iteration 45649, loss = 1.56958581\n",
      "Iteration 45650, loss = 1.47789204\n",
      "Iteration 45651, loss = 1.48698846\n",
      "Iteration 45652, loss = 1.51884748\n",
      "Iteration 45653, loss = 1.44695031\n",
      "Iteration 45654, loss = 1.50180336\n",
      "Iteration 45655, loss = 1.49633179\n",
      "Iteration 45656, loss = 2.08192812\n",
      "Iteration 45657, loss = 1.96207410\n",
      "Iteration 45658, loss = 2.05828831\n",
      "Iteration 45659, loss = 1.58436993\n",
      "Iteration 45660, loss = 1.61008452\n",
      "Iteration 45661, loss = 1.62121192\n",
      "Iteration 45662, loss = 1.59595018\n",
      "Iteration 45663, loss = 1.79902547\n",
      "Iteration 45664, loss = 2.40613005\n",
      "Iteration 45665, loss = 2.02866254\n",
      "Iteration 45666, loss = 1.66651624\n",
      "Iteration 45667, loss = 2.12230501\n",
      "Iteration 45668, loss = 2.01272914\n",
      "Iteration 45669, loss = 1.99228930\n",
      "Iteration 45670, loss = 2.18873968\n",
      "Iteration 45671, loss = 2.09724788\n",
      "Iteration 45672, loss = 2.00889226\n",
      "Iteration 45673, loss = 2.30964251\n",
      "Iteration 45674, loss = 2.52767318\n",
      "Iteration 45675, loss = 2.46496946\n",
      "Iteration 45676, loss = 1.77809643\n",
      "Iteration 45677, loss = 1.78799785\n",
      "Iteration 45678, loss = 1.85544924\n",
      "Iteration 45679, loss = 1.75286313\n",
      "Iteration 45680, loss = 1.77024462\n",
      "Iteration 45681, loss = 1.75984729\n",
      "Iteration 45682, loss = 1.51288754\n",
      "Iteration 45683, loss = 1.43488293\n",
      "Iteration 45684, loss = 1.55117968\n",
      "Iteration 45685, loss = 1.66500838\n",
      "Iteration 45686, loss = 1.57478369\n",
      "Iteration 45687, loss = 2.01422685\n",
      "Iteration 45688, loss = 2.20788884\n",
      "Iteration 45689, loss = 2.09351589\n",
      "Iteration 45690, loss = 2.48029350\n",
      "Iteration 45691, loss = 1.81043101\n",
      "Iteration 45692, loss = 1.61365516\n",
      "Iteration 45693, loss = 1.54612901\n",
      "Iteration 45694, loss = 1.62587239\n",
      "Iteration 45695, loss = 1.56719129\n",
      "Iteration 45696, loss = 1.56320856\n",
      "Iteration 45697, loss = 1.67054667\n",
      "Iteration 45698, loss = 1.64247135\n",
      "Iteration 45699, loss = 1.99935109\n",
      "Iteration 45700, loss = 1.85909904\n",
      "Iteration 45701, loss = 1.52503811\n",
      "Iteration 45702, loss = 1.83775337\n",
      "Iteration 45703, loss = 1.64010117\n",
      "Iteration 45704, loss = 1.61829242\n",
      "Iteration 45705, loss = 1.52081701\n",
      "Iteration 45706, loss = 1.62462896\n",
      "Iteration 45707, loss = 2.07757662\n",
      "Iteration 45708, loss = 1.79029728\n",
      "Iteration 45709, loss = 1.69957200\n",
      "Iteration 45710, loss = 2.50180304\n",
      "Iteration 45711, loss = 2.27239571\n",
      "Iteration 45712, loss = 2.22711749\n",
      "Iteration 45713, loss = 2.89841309\n",
      "Iteration 45714, loss = 2.80809878\n",
      "Iteration 45715, loss = 2.86992501\n",
      "Iteration 45716, loss = 3.36652559\n",
      "Iteration 45717, loss = 2.20214478\n",
      "Iteration 45718, loss = 2.64964259\n",
      "Iteration 45719, loss = 2.31305050\n",
      "Iteration 45720, loss = 3.03129684\n",
      "Iteration 45721, loss = 5.93375375\n",
      "Iteration 45722, loss = 6.65053706\n",
      "Iteration 45723, loss = 4.42989770\n",
      "Iteration 45724, loss = 4.41670558\n",
      "Iteration 45725, loss = 3.45804664\n",
      "Iteration 45726, loss = 3.97409426\n",
      "Iteration 45727, loss = 2.40383416\n",
      "Iteration 45728, loss = 1.88892036\n",
      "Iteration 45729, loss = 1.85108179\n",
      "Iteration 45730, loss = 1.59767365\n",
      "Iteration 45731, loss = 1.84698596\n",
      "Iteration 45732, loss = 2.21478846\n",
      "Iteration 45733, loss = 1.87328158\n",
      "Iteration 45734, loss = 1.74234030\n",
      "Iteration 45735, loss = 1.72179404\n",
      "Iteration 45736, loss = 2.04958807\n",
      "Iteration 45737, loss = 2.39874706\n",
      "Iteration 45738, loss = 2.44798235\n",
      "Iteration 45739, loss = 2.27795162\n",
      "Iteration 45740, loss = 2.02987132\n",
      "Iteration 45741, loss = 2.19012501\n",
      "Iteration 45742, loss = 2.60411457\n",
      "Iteration 45743, loss = 2.48433442\n",
      "Iteration 45744, loss = 1.88991036\n",
      "Iteration 45745, loss = 2.26779221\n",
      "Iteration 45746, loss = 2.17109553\n",
      "Iteration 45747, loss = 1.59662353\n",
      "Iteration 45748, loss = 1.88562932\n",
      "Iteration 45749, loss = 1.70385895\n",
      "Iteration 45750, loss = 1.56864143\n",
      "Iteration 45751, loss = 1.60244264\n",
      "Iteration 45752, loss = 1.71846813\n",
      "Iteration 45753, loss = 1.82520973\n",
      "Iteration 45754, loss = 1.73852560\n",
      "Iteration 45755, loss = 1.67151184\n",
      "Iteration 45756, loss = 1.65774574\n",
      "Iteration 45757, loss = 1.60605275\n",
      "Iteration 45758, loss = 1.55288237\n",
      "Iteration 45759, loss = 1.54155256\n",
      "Iteration 45760, loss = 1.52165152\n",
      "Iteration 45761, loss = 1.48950635\n",
      "Iteration 45762, loss = 1.49650512\n",
      "Iteration 45763, loss = 1.42762964\n",
      "Iteration 45764, loss = 1.55346035\n",
      "Iteration 45765, loss = 1.58781280\n",
      "Iteration 45766, loss = 1.53754422\n",
      "Iteration 45767, loss = 1.68109333\n",
      "Iteration 45768, loss = 1.93486066\n",
      "Iteration 45769, loss = 2.29070922\n",
      "Iteration 45770, loss = 2.12120281\n",
      "Iteration 45771, loss = 2.15587549\n",
      "Iteration 45772, loss = 2.38693465\n",
      "Iteration 45773, loss = 2.07651507\n",
      "Iteration 45774, loss = 2.08267462\n",
      "Iteration 45775, loss = 1.90424914\n",
      "Iteration 45776, loss = 1.82190059\n",
      "Iteration 45777, loss = 2.00474846\n",
      "Iteration 45778, loss = 1.59157185\n",
      "Iteration 45779, loss = 1.59376534\n",
      "Iteration 45780, loss = 1.82349160\n",
      "Iteration 45781, loss = 2.10004955\n",
      "Iteration 45782, loss = 1.91127813\n",
      "Iteration 45783, loss = 1.75736397\n",
      "Iteration 45784, loss = 1.61613322\n",
      "Iteration 45785, loss = 2.07287078\n",
      "Iteration 45786, loss = 2.00622638\n",
      "Iteration 45787, loss = 2.50156254\n",
      "Iteration 45788, loss = 1.96843113\n",
      "Iteration 45789, loss = 1.73359794\n",
      "Iteration 45790, loss = 2.03087950\n",
      "Iteration 45791, loss = 2.24692777\n",
      "Iteration 45792, loss = 2.21300481\n",
      "Iteration 45793, loss = 2.31950616\n",
      "Iteration 45794, loss = 2.35888616\n",
      "Iteration 45795, loss = 2.53237636\n",
      "Iteration 45796, loss = 2.26705641\n",
      "Iteration 45797, loss = 2.35204003\n",
      "Iteration 45798, loss = 2.15437747\n",
      "Iteration 45799, loss = 2.09854230\n",
      "Iteration 45800, loss = 1.95641058\n",
      "Iteration 45801, loss = 2.01807828\n",
      "Iteration 45802, loss = 1.67873398\n",
      "Iteration 45803, loss = 1.75982510\n",
      "Iteration 45804, loss = 1.55208137\n",
      "Iteration 45805, loss = 1.51949513\n",
      "Iteration 45806, loss = 1.93396590\n",
      "Iteration 45807, loss = 2.15979829\n",
      "Iteration 45808, loss = 3.97629178\n",
      "Iteration 45809, loss = 2.28148846\n",
      "Iteration 45810, loss = 2.13362465\n",
      "Iteration 45811, loss = 2.06833183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 45812, loss = 1.70470247\n",
      "Iteration 45813, loss = 1.83996687\n",
      "Iteration 45814, loss = 2.02248696\n",
      "Iteration 45815, loss = 1.79826761\n",
      "Iteration 45816, loss = 2.28874099\n",
      "Iteration 45817, loss = 1.87968077\n",
      "Iteration 45818, loss = 1.88174206\n",
      "Iteration 45819, loss = 1.85438214\n",
      "Iteration 45820, loss = 1.53285571\n",
      "Iteration 45821, loss = 1.53130518\n",
      "Iteration 45822, loss = 1.58416127\n",
      "Iteration 45823, loss = 1.62360285\n",
      "Iteration 45824, loss = 1.99194653\n",
      "Iteration 45825, loss = 2.20583716\n",
      "Iteration 45826, loss = 1.95770842\n",
      "Iteration 45827, loss = 1.53328833\n",
      "Iteration 45828, loss = 1.61295598\n",
      "Iteration 45829, loss = 1.61198412\n",
      "Iteration 45830, loss = 1.67560033\n",
      "Iteration 45831, loss = 1.75702497\n",
      "Iteration 45832, loss = 2.09841186\n",
      "Iteration 45833, loss = 1.83588893\n",
      "Iteration 45834, loss = 1.91828286\n",
      "Iteration 45835, loss = 1.73123640\n",
      "Iteration 45836, loss = 1.72660078\n",
      "Iteration 45837, loss = 1.61522194\n",
      "Iteration 45838, loss = 1.73067311\n",
      "Iteration 45839, loss = 1.70135218\n",
      "Iteration 45840, loss = 1.89090239\n",
      "Iteration 45841, loss = 1.95516957\n",
      "Iteration 45842, loss = 1.67008634\n",
      "Iteration 45843, loss = 1.49911251\n",
      "Iteration 45844, loss = 1.59146805\n",
      "Iteration 45845, loss = 1.89464152\n",
      "Iteration 45846, loss = 1.86720033\n",
      "Iteration 45847, loss = 1.81113929\n",
      "Iteration 45848, loss = 1.60009536\n",
      "Iteration 45849, loss = 1.46767368\n",
      "Iteration 45850, loss = 1.63274095\n",
      "Iteration 45851, loss = 1.71721916\n",
      "Iteration 45852, loss = 1.57692662\n",
      "Iteration 45853, loss = 1.81759921\n",
      "Iteration 45854, loss = 1.58028029\n",
      "Iteration 45855, loss = 1.76827352\n",
      "Iteration 45856, loss = 1.91133745\n",
      "Iteration 45857, loss = 1.84877282\n",
      "Iteration 45858, loss = 2.01968081\n",
      "Iteration 45859, loss = 1.88382579\n",
      "Iteration 45860, loss = 2.44208175\n",
      "Iteration 45861, loss = 2.54363925\n",
      "Iteration 45862, loss = 2.32998302\n",
      "Iteration 45863, loss = 2.41742750\n",
      "Iteration 45864, loss = 2.04712793\n",
      "Iteration 45865, loss = 1.84550890\n",
      "Iteration 45866, loss = 1.86989830\n",
      "Iteration 45867, loss = 1.87101539\n",
      "Iteration 45868, loss = 1.86677788\n",
      "Iteration 45869, loss = 2.00717258\n",
      "Iteration 45870, loss = 1.93079615\n",
      "Iteration 45871, loss = 1.76275213\n",
      "Iteration 45872, loss = 1.66559758\n",
      "Iteration 45873, loss = 1.59238067\n",
      "Iteration 45874, loss = 1.53115089\n",
      "Iteration 45875, loss = 1.57531099\n",
      "Iteration 45876, loss = 1.69419863\n",
      "Iteration 45877, loss = 1.57173956\n",
      "Iteration 45878, loss = 1.90285303\n",
      "Iteration 45879, loss = 2.91321780\n",
      "Iteration 45880, loss = 3.21460992\n",
      "Iteration 45881, loss = 2.98668731\n",
      "Iteration 45882, loss = 2.92681043\n",
      "Iteration 45883, loss = 3.29978051\n",
      "Iteration 45884, loss = 2.90035911\n",
      "Iteration 45885, loss = 2.86582284\n",
      "Iteration 45886, loss = 2.43290491\n",
      "Iteration 45887, loss = 2.15808051\n",
      "Iteration 45888, loss = 2.43711661\n",
      "Iteration 45889, loss = 1.98705915\n",
      "Iteration 45890, loss = 1.93250687\n",
      "Iteration 45891, loss = 1.88094933\n",
      "Iteration 45892, loss = 1.88822117\n",
      "Iteration 45893, loss = 1.96174977\n",
      "Iteration 45894, loss = 1.83491019\n",
      "Iteration 45895, loss = 1.67471705\n",
      "Iteration 45896, loss = 2.11541588\n",
      "Iteration 45897, loss = 1.83821584\n",
      "Iteration 45898, loss = 2.19940654\n",
      "Iteration 45899, loss = 3.47221727\n",
      "Iteration 45900, loss = 1.96163187\n",
      "Iteration 45901, loss = 2.23312546\n",
      "Iteration 45902, loss = 2.34333733\n",
      "Iteration 45903, loss = 2.40067148\n",
      "Iteration 45904, loss = 2.50880794\n",
      "Iteration 45905, loss = 2.15062644\n",
      "Iteration 45906, loss = 1.65250503\n",
      "Iteration 45907, loss = 1.76070028\n",
      "Iteration 45908, loss = 1.51655560\n",
      "Iteration 45909, loss = 1.71373683\n",
      "Iteration 45910, loss = 1.78668623\n",
      "Iteration 45911, loss = 1.60067295\n",
      "Iteration 45912, loss = 1.45576990\n",
      "Iteration 45913, loss = 1.42581757\n",
      "Iteration 45914, loss = 1.50583070\n",
      "Iteration 45915, loss = 1.46754771\n",
      "Iteration 45916, loss = 1.44502872\n",
      "Iteration 45917, loss = 1.52528564\n",
      "Iteration 45918, loss = 1.86984934\n",
      "Iteration 45919, loss = 1.91514342\n",
      "Iteration 45920, loss = 1.78281710\n",
      "Iteration 45921, loss = 2.15362444\n",
      "Iteration 45922, loss = 2.15854738\n",
      "Iteration 45923, loss = 1.92463493\n",
      "Iteration 45924, loss = 1.68768229\n",
      "Iteration 45925, loss = 1.77509645\n",
      "Iteration 45926, loss = 1.71449944\n",
      "Iteration 45927, loss = 2.15139822\n",
      "Iteration 45928, loss = 1.82294260\n",
      "Iteration 45929, loss = 1.75166392\n",
      "Iteration 45930, loss = 1.65881250\n",
      "Iteration 45931, loss = 1.74520745\n",
      "Iteration 45932, loss = 1.73858732\n",
      "Iteration 45933, loss = 1.99760101\n",
      "Iteration 45934, loss = 2.07871488\n",
      "Iteration 45935, loss = 2.10570316\n",
      "Iteration 45936, loss = 1.63830178\n",
      "Iteration 45937, loss = 1.57952779\n",
      "Iteration 45938, loss = 1.86427110\n",
      "Iteration 45939, loss = 1.55143921\n",
      "Iteration 45940, loss = 1.51011947\n",
      "Iteration 45941, loss = 1.79706934\n",
      "Iteration 45942, loss = 1.96475731\n",
      "Iteration 45943, loss = 2.09124745\n",
      "Iteration 45944, loss = 2.34609944\n",
      "Iteration 45945, loss = 1.69198596\n",
      "Iteration 45946, loss = 1.85481414\n",
      "Iteration 45947, loss = 1.63635816\n",
      "Iteration 45948, loss = 1.64934002\n",
      "Iteration 45949, loss = 2.41860002\n",
      "Iteration 45950, loss = 2.78058420\n",
      "Iteration 45951, loss = 2.34922488\n",
      "Iteration 45952, loss = 2.13823901\n",
      "Iteration 45953, loss = 1.56308187\n",
      "Iteration 45954, loss = 3.35014183\n",
      "Iteration 45955, loss = 2.84379446\n",
      "Iteration 45956, loss = 2.18938475\n",
      "Iteration 45957, loss = 2.42337691\n",
      "Iteration 45958, loss = 3.20190782\n",
      "Iteration 45959, loss = 2.37109023\n",
      "Iteration 45960, loss = 1.91249598\n",
      "Iteration 45961, loss = 1.83167278\n",
      "Iteration 45962, loss = 1.70289629\n",
      "Iteration 45963, loss = 1.96620764\n",
      "Iteration 45964, loss = 2.39562448\n",
      "Iteration 45965, loss = 2.31096823\n",
      "Iteration 45966, loss = 1.85124373\n",
      "Iteration 45967, loss = 1.80634942\n",
      "Iteration 45968, loss = 2.11379505\n",
      "Iteration 45969, loss = 1.70801146\n",
      "Iteration 45970, loss = 1.71207324\n",
      "Iteration 45971, loss = 1.86958661\n",
      "Iteration 45972, loss = 2.07400611\n",
      "Iteration 45973, loss = 2.10309949\n",
      "Iteration 45974, loss = 2.14664309\n",
      "Iteration 45975, loss = 2.04652609\n",
      "Iteration 45976, loss = 1.92708150\n",
      "Iteration 45977, loss = 1.80662501\n",
      "Iteration 45978, loss = 1.49907830\n",
      "Iteration 45979, loss = 1.66739263\n",
      "Iteration 45980, loss = 1.59888389\n",
      "Iteration 45981, loss = 1.87633365\n",
      "Iteration 45982, loss = 1.86576207\n",
      "Iteration 45983, loss = 2.02775617\n",
      "Iteration 45984, loss = 1.97804034\n",
      "Iteration 45985, loss = 1.77965928\n",
      "Iteration 45986, loss = 1.72780716\n",
      "Iteration 45987, loss = 1.85974583\n",
      "Iteration 45988, loss = 1.93725832\n",
      "Iteration 45989, loss = 2.55460878\n",
      "Iteration 45990, loss = 2.58476621\n",
      "Iteration 45991, loss = 1.89809884\n",
      "Iteration 45992, loss = 1.69889741\n",
      "Iteration 45993, loss = 1.76917819\n",
      "Iteration 45994, loss = 1.88525615\n",
      "Iteration 45995, loss = 1.56208389\n",
      "Iteration 45996, loss = 1.93392998\n",
      "Iteration 45997, loss = 2.03482978\n",
      "Iteration 45998, loss = 1.88711048\n",
      "Iteration 45999, loss = 1.66557877\n",
      "Iteration 46000, loss = 1.60682792\n",
      "Iteration 46001, loss = 1.51224021\n",
      "Iteration 46002, loss = 1.71731802\n",
      "Iteration 46003, loss = 1.82241238\n",
      "Iteration 46004, loss = 2.17278415\n",
      "Iteration 46005, loss = 1.80865236\n",
      "Iteration 46006, loss = 2.26705750\n",
      "Iteration 46007, loss = 1.64517924\n",
      "Iteration 46008, loss = 1.69049493\n",
      "Iteration 46009, loss = 1.65348251\n",
      "Iteration 46010, loss = 1.62973243\n",
      "Iteration 46011, loss = 1.79764195\n",
      "Iteration 46012, loss = 1.98908065\n",
      "Iteration 46013, loss = 1.52989616\n",
      "Iteration 46014, loss = 1.78241244\n",
      "Iteration 46015, loss = 2.09274111\n",
      "Iteration 46016, loss = 2.04841105\n",
      "Iteration 46017, loss = 2.19102901\n",
      "Iteration 46018, loss = 2.34307712\n",
      "Iteration 46019, loss = 2.19492587\n",
      "Iteration 46020, loss = 2.73120788\n",
      "Iteration 46021, loss = 2.12028704\n",
      "Iteration 46022, loss = 1.76417076\n",
      "Iteration 46023, loss = 1.80824723\n",
      "Iteration 46024, loss = 1.72193598\n",
      "Iteration 46025, loss = 1.79499716\n",
      "Iteration 46026, loss = 1.73128327\n",
      "Iteration 46027, loss = 1.61534314\n",
      "Iteration 46028, loss = 1.71917055\n",
      "Iteration 46029, loss = 2.00462854\n",
      "Iteration 46030, loss = 1.76687835\n",
      "Iteration 46031, loss = 2.09862401\n",
      "Iteration 46032, loss = 1.87036396\n",
      "Iteration 46033, loss = 1.87533771\n",
      "Iteration 46034, loss = 2.24291835\n",
      "Iteration 46035, loss = 1.89725896\n",
      "Iteration 46036, loss = 2.49138547\n",
      "Iteration 46037, loss = 2.61023203\n",
      "Iteration 46038, loss = 1.91456492\n",
      "Iteration 46039, loss = 2.78152805\n",
      "Iteration 46040, loss = 2.49591642\n",
      "Iteration 46041, loss = 2.24946649\n",
      "Iteration 46042, loss = 1.77006354\n",
      "Iteration 46043, loss = 1.89144665\n",
      "Iteration 46044, loss = 1.88008969\n",
      "Iteration 46045, loss = 1.82243940\n",
      "Iteration 46046, loss = 1.73974022\n",
      "Iteration 46047, loss = 1.72299596\n",
      "Iteration 46048, loss = 1.60170454\n",
      "Iteration 46049, loss = 1.55574947\n",
      "Iteration 46050, loss = 1.65435035\n",
      "Iteration 46051, loss = 1.56904241\n",
      "Iteration 46052, loss = 1.82125950\n",
      "Iteration 46053, loss = 1.69032374\n",
      "Iteration 46054, loss = 1.51226093\n",
      "Iteration 46055, loss = 1.71913761\n",
      "Iteration 46056, loss = 2.00895357\n",
      "Iteration 46057, loss = 2.14590601\n",
      "Iteration 46058, loss = 2.50401091\n",
      "Iteration 46059, loss = 2.42544011\n",
      "Iteration 46060, loss = 1.94864164\n",
      "Iteration 46061, loss = 2.17878856\n",
      "Iteration 46062, loss = 2.40938352\n",
      "Iteration 46063, loss = 2.20861950\n",
      "Iteration 46064, loss = 2.39016782\n",
      "Iteration 46065, loss = 2.34353171\n",
      "Iteration 46066, loss = 2.50418979\n",
      "Iteration 46067, loss = 1.97662611\n",
      "Iteration 46068, loss = 2.08510725\n",
      "Iteration 46069, loss = 1.76769536\n",
      "Iteration 46070, loss = 1.99815946\n",
      "Iteration 46071, loss = 1.56226040\n",
      "Iteration 46072, loss = 1.76789501\n",
      "Iteration 46073, loss = 1.72769469\n",
      "Iteration 46074, loss = 1.84714782\n",
      "Iteration 46075, loss = 1.61194253\n",
      "Iteration 46076, loss = 1.51857522\n",
      "Iteration 46077, loss = 1.54170372\n",
      "Iteration 46078, loss = 1.44255924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 46079, loss = 1.62673643\n",
      "Iteration 46080, loss = 1.81113969\n",
      "Iteration 46081, loss = 1.58502472\n",
      "Iteration 46082, loss = 1.47324008\n",
      "Iteration 46083, loss = 1.85101901\n",
      "Iteration 46084, loss = 1.61037711\n",
      "Iteration 46085, loss = 1.62335919\n",
      "Iteration 46086, loss = 1.56515733\n",
      "Iteration 46087, loss = 1.61429514\n",
      "Iteration 46088, loss = 1.75008488\n",
      "Iteration 46089, loss = 1.75784306\n",
      "Iteration 46090, loss = 1.85702264\n",
      "Iteration 46091, loss = 1.64159286\n",
      "Iteration 46092, loss = 1.74536746\n",
      "Iteration 46093, loss = 1.94240667\n",
      "Iteration 46094, loss = 2.13954420\n",
      "Iteration 46095, loss = 2.16921744\n",
      "Iteration 46096, loss = 2.19676433\n",
      "Iteration 46097, loss = 1.78701880\n",
      "Iteration 46098, loss = 1.78735043\n",
      "Iteration 46099, loss = 1.55454974\n",
      "Iteration 46100, loss = 1.65906065\n",
      "Iteration 46101, loss = 2.64212398\n",
      "Iteration 46102, loss = 2.57058390\n",
      "Iteration 46103, loss = 2.97228307\n",
      "Iteration 46104, loss = 2.49485921\n",
      "Iteration 46105, loss = 1.99574389\n",
      "Iteration 46106, loss = 1.75626134\n",
      "Iteration 46107, loss = 2.59379186\n",
      "Iteration 46108, loss = 2.49628406\n",
      "Iteration 46109, loss = 1.75677310\n",
      "Iteration 46110, loss = 1.60084697\n",
      "Iteration 46111, loss = 1.70691406\n",
      "Iteration 46112, loss = 1.63311221\n",
      "Iteration 46113, loss = 1.89895507\n",
      "Iteration 46114, loss = 1.94820855\n",
      "Iteration 46115, loss = 1.73539811\n",
      "Iteration 46116, loss = 1.80039323\n",
      "Iteration 46117, loss = 2.37804738\n",
      "Iteration 46118, loss = 2.01606672\n",
      "Iteration 46119, loss = 1.81294104\n",
      "Iteration 46120, loss = 1.85937380\n",
      "Iteration 46121, loss = 1.84477983\n",
      "Iteration 46122, loss = 1.70071378\n",
      "Iteration 46123, loss = 1.95632412\n",
      "Iteration 46124, loss = 1.87550236\n",
      "Iteration 46125, loss = 1.90012544\n",
      "Iteration 46126, loss = 1.87007706\n",
      "Iteration 46127, loss = 2.18093091\n",
      "Iteration 46128, loss = 1.84390407\n",
      "Iteration 46129, loss = 2.18520139\n",
      "Iteration 46130, loss = 2.38889950\n",
      "Iteration 46131, loss = 2.24255661\n",
      "Iteration 46132, loss = 2.12278325\n",
      "Iteration 46133, loss = 1.61755901\n",
      "Iteration 46134, loss = 1.54083312\n",
      "Iteration 46135, loss = 1.61721159\n",
      "Iteration 46136, loss = 1.88997373\n",
      "Iteration 46137, loss = 2.14019834\n",
      "Iteration 46138, loss = 2.25567903\n",
      "Iteration 46139, loss = 1.82986140\n",
      "Iteration 46140, loss = 2.07553283\n",
      "Iteration 46141, loss = 1.98716005\n",
      "Iteration 46142, loss = 1.79108857\n",
      "Iteration 46143, loss = 2.00629198\n",
      "Iteration 46144, loss = 1.79777351\n",
      "Iteration 46145, loss = 2.14254985\n",
      "Iteration 46146, loss = 2.05770015\n",
      "Iteration 46147, loss = 1.99526267\n",
      "Iteration 46148, loss = 1.96843104\n",
      "Iteration 46149, loss = 2.52357451\n",
      "Iteration 46150, loss = 2.83818920\n",
      "Iteration 46151, loss = 2.52596662\n",
      "Iteration 46152, loss = 2.55938847\n",
      "Iteration 46153, loss = 2.50204189\n",
      "Iteration 46154, loss = 2.94935514\n",
      "Iteration 46155, loss = 3.47096658\n",
      "Iteration 46156, loss = 3.14099384\n",
      "Iteration 46157, loss = 2.26500253\n",
      "Iteration 46158, loss = 2.19478964\n",
      "Iteration 46159, loss = 2.08145124\n",
      "Iteration 46160, loss = 2.10908341\n",
      "Iteration 46161, loss = 1.58312482\n",
      "Iteration 46162, loss = 1.90125177\n",
      "Iteration 46163, loss = 1.85341758\n",
      "Iteration 46164, loss = 1.78087564\n",
      "Iteration 46165, loss = 1.71094144\n",
      "Iteration 46166, loss = 2.02683173\n",
      "Iteration 46167, loss = 1.60739077\n",
      "Iteration 46168, loss = 1.71774341\n",
      "Iteration 46169, loss = 1.60311508\n",
      "Iteration 46170, loss = 1.56148364\n",
      "Iteration 46171, loss = 1.85518277\n",
      "Iteration 46172, loss = 2.07764156\n",
      "Iteration 46173, loss = 2.36897214\n",
      "Iteration 46174, loss = 2.26326620\n",
      "Iteration 46175, loss = 2.39528721\n",
      "Iteration 46176, loss = 2.03431596\n",
      "Iteration 46177, loss = 1.58563992\n",
      "Iteration 46178, loss = 1.70856718\n",
      "Iteration 46179, loss = 1.99952658\n",
      "Iteration 46180, loss = 2.49474709\n",
      "Iteration 46181, loss = 2.19557345\n",
      "Iteration 46182, loss = 1.75387089\n",
      "Iteration 46183, loss = 1.99204415\n",
      "Iteration 46184, loss = 3.58156269\n",
      "Iteration 46185, loss = 2.64314565\n",
      "Iteration 46186, loss = 2.07790153\n",
      "Iteration 46187, loss = 1.91428120\n",
      "Iteration 46188, loss = 1.69383687\n",
      "Iteration 46189, loss = 1.93498926\n",
      "Iteration 46190, loss = 1.52855216\n",
      "Iteration 46191, loss = 1.70408853\n",
      "Iteration 46192, loss = 1.61786636\n",
      "Iteration 46193, loss = 1.50605313\n",
      "Iteration 46194, loss = 1.53451812\n",
      "Iteration 46195, loss = 1.74163748\n",
      "Iteration 46196, loss = 2.14662775\n",
      "Iteration 46197, loss = 1.69257901\n",
      "Iteration 46198, loss = 1.54157606\n",
      "Iteration 46199, loss = 1.61290081\n",
      "Iteration 46200, loss = 1.74327293\n",
      "Iteration 46201, loss = 1.69649553\n",
      "Iteration 46202, loss = 2.41213698\n",
      "Iteration 46203, loss = 4.45873975\n",
      "Iteration 46204, loss = 6.18170043\n",
      "Iteration 46205, loss = 3.87188249\n",
      "Iteration 46206, loss = 2.84926330\n",
      "Iteration 46207, loss = 2.42375384\n",
      "Iteration 46208, loss = 2.65297023\n",
      "Iteration 46209, loss = 2.05890831\n",
      "Iteration 46210, loss = 1.71945403\n",
      "Iteration 46211, loss = 1.85617944\n",
      "Iteration 46212, loss = 1.88481695\n",
      "Iteration 46213, loss = 1.75358210\n",
      "Iteration 46214, loss = 1.65448944\n",
      "Iteration 46215, loss = 1.64061388\n",
      "Iteration 46216, loss = 1.59087473\n",
      "Iteration 46217, loss = 1.57512492\n",
      "Iteration 46218, loss = 1.55565394\n",
      "Iteration 46219, loss = 1.45213334\n",
      "Iteration 46220, loss = 1.46487749\n",
      "Iteration 46221, loss = 1.67441748\n",
      "Iteration 46222, loss = 1.69918048\n",
      "Iteration 46223, loss = 1.59168422\n",
      "Iteration 46224, loss = 1.51882427\n",
      "Iteration 46225, loss = 1.69181069\n",
      "Iteration 46226, loss = 1.76461367\n",
      "Iteration 46227, loss = 1.72993779\n",
      "Iteration 46228, loss = 1.75931203\n",
      "Iteration 46229, loss = 1.62174046\n",
      "Iteration 46230, loss = 1.53736491\n",
      "Iteration 46231, loss = 1.79091800\n",
      "Iteration 46232, loss = 2.00283083\n",
      "Iteration 46233, loss = 2.27868660\n",
      "Iteration 46234, loss = 1.68195242\n",
      "Iteration 46235, loss = 1.97212884\n",
      "Iteration 46236, loss = 1.74701980\n",
      "Iteration 46237, loss = 1.75515166\n",
      "Iteration 46238, loss = 1.48751644\n",
      "Iteration 46239, loss = 1.42337772\n",
      "Iteration 46240, loss = 1.64536383\n",
      "Iteration 46241, loss = 1.78862079\n",
      "Iteration 46242, loss = 2.28076195\n",
      "Iteration 46243, loss = 2.05751469\n",
      "Iteration 46244, loss = 2.23151124\n",
      "Iteration 46245, loss = 2.40508396\n",
      "Iteration 46246, loss = 2.72676068\n",
      "Iteration 46247, loss = 2.26218020\n",
      "Iteration 46248, loss = 2.58926097\n",
      "Iteration 46249, loss = 1.89847790\n",
      "Iteration 46250, loss = 1.93882530\n",
      "Iteration 46251, loss = 1.57041631\n",
      "Iteration 46252, loss = 1.54366222\n",
      "Iteration 46253, loss = 1.59326060\n",
      "Iteration 46254, loss = 1.57897978\n",
      "Iteration 46255, loss = 1.61878590\n",
      "Iteration 46256, loss = 1.67036006\n",
      "Iteration 46257, loss = 1.75838566\n",
      "Iteration 46258, loss = 1.75037394\n",
      "Iteration 46259, loss = 1.80634659\n",
      "Iteration 46260, loss = 2.20060011\n",
      "Iteration 46261, loss = 2.38350006\n",
      "Iteration 46262, loss = 2.08697717\n",
      "Iteration 46263, loss = 2.43579331\n",
      "Iteration 46264, loss = 2.58462505\n",
      "Iteration 46265, loss = 2.44166382\n",
      "Iteration 46266, loss = 1.55918969\n",
      "Iteration 46267, loss = 1.58259690\n",
      "Iteration 46268, loss = 1.48080302\n",
      "Iteration 46269, loss = 1.65598191\n",
      "Iteration 46270, loss = 1.65779368\n",
      "Iteration 46271, loss = 1.68031608\n",
      "Iteration 46272, loss = 1.60968921\n",
      "Iteration 46273, loss = 1.62600379\n",
      "Iteration 46274, loss = 1.78929276\n",
      "Iteration 46275, loss = 1.88565675\n",
      "Iteration 46276, loss = 1.72329721\n",
      "Iteration 46277, loss = 2.09246744\n",
      "Iteration 46278, loss = 1.83657937\n",
      "Iteration 46279, loss = 2.18620164\n",
      "Iteration 46280, loss = 2.04715613\n",
      "Iteration 46281, loss = 1.66586889\n",
      "Iteration 46282, loss = 1.64773445\n",
      "Iteration 46283, loss = 1.86029097\n",
      "Iteration 46284, loss = 1.80424729\n",
      "Iteration 46285, loss = 1.86361424\n",
      "Iteration 46286, loss = 1.86093659\n",
      "Iteration 46287, loss = 1.73394637\n",
      "Iteration 46288, loss = 1.70055230\n",
      "Iteration 46289, loss = 1.82548905\n",
      "Iteration 46290, loss = 1.79358621\n",
      "Iteration 46291, loss = 2.13421267\n",
      "Iteration 46292, loss = 1.69101819\n",
      "Iteration 46293, loss = 1.72871713\n",
      "Iteration 46294, loss = 2.43886166\n",
      "Iteration 46295, loss = 2.13661065\n",
      "Iteration 46296, loss = 2.95132310\n",
      "Iteration 46297, loss = 2.00521635\n",
      "Iteration 46298, loss = 2.03969470\n",
      "Iteration 46299, loss = 2.67949424\n",
      "Iteration 46300, loss = 2.28736830\n",
      "Iteration 46301, loss = 3.39671408\n",
      "Iteration 46302, loss = 3.49889450\n",
      "Iteration 46303, loss = 2.61265841\n",
      "Iteration 46304, loss = 2.18727431\n",
      "Iteration 46305, loss = 2.25587427\n",
      "Iteration 46306, loss = 2.49800957\n",
      "Iteration 46307, loss = 1.99433190\n",
      "Iteration 46308, loss = 1.95940224\n",
      "Iteration 46309, loss = 1.82601157\n",
      "Iteration 46310, loss = 2.09349817\n",
      "Iteration 46311, loss = 2.13553474\n",
      "Iteration 46312, loss = 1.68085377\n",
      "Iteration 46313, loss = 1.57238790\n",
      "Iteration 46314, loss = 1.50409583\n",
      "Iteration 46315, loss = 1.42535302\n",
      "Iteration 46316, loss = 1.49769143\n",
      "Iteration 46317, loss = 1.34604763\n",
      "Iteration 46318, loss = 1.70450808\n",
      "Iteration 46319, loss = 2.15276230\n",
      "Iteration 46320, loss = 2.43307538\n",
      "Iteration 46321, loss = 2.27332070\n",
      "Iteration 46322, loss = 2.54781615\n",
      "Iteration 46323, loss = 2.96820977\n",
      "Iteration 46324, loss = 2.80186494\n",
      "Iteration 46325, loss = 2.22018015\n",
      "Iteration 46326, loss = 2.10389996\n",
      "Iteration 46327, loss = 1.91961209\n",
      "Iteration 46328, loss = 2.04618326\n",
      "Iteration 46329, loss = 1.88019191\n",
      "Iteration 46330, loss = 2.68258324\n",
      "Iteration 46331, loss = 2.31675821\n",
      "Iteration 46332, loss = 2.61217704\n",
      "Iteration 46333, loss = 2.35633552\n",
      "Iteration 46334, loss = 2.08892495\n",
      "Iteration 46335, loss = 2.14038731\n",
      "Iteration 46336, loss = 2.38633849\n",
      "Iteration 46337, loss = 2.87862410\n",
      "Iteration 46338, loss = 2.16073026\n",
      "Iteration 46339, loss = 2.00322374\n",
      "Iteration 46340, loss = 1.85171442\n",
      "Iteration 46341, loss = 2.47729476\n",
      "Iteration 46342, loss = 2.60227363\n",
      "Iteration 46343, loss = 2.48616343\n",
      "Iteration 46344, loss = 1.99385245\n",
      "Iteration 46345, loss = 2.14536292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 46346, loss = 1.77381640\n",
      "Iteration 46347, loss = 1.69816739\n",
      "Iteration 46348, loss = 1.65781065\n",
      "Iteration 46349, loss = 1.53615643\n",
      "Iteration 46350, loss = 1.46018389\n",
      "Iteration 46351, loss = 1.48938641\n",
      "Iteration 46352, loss = 1.60987977\n",
      "Iteration 46353, loss = 1.84196462\n",
      "Iteration 46354, loss = 1.53181006\n",
      "Iteration 46355, loss = 1.69384196\n",
      "Iteration 46356, loss = 1.71441123\n",
      "Iteration 46357, loss = 1.98814724\n",
      "Iteration 46358, loss = 1.68747293\n",
      "Iteration 46359, loss = 2.59286326\n",
      "Iteration 46360, loss = 2.27073297\n",
      "Iteration 46361, loss = 3.38649504\n",
      "Iteration 46362, loss = 4.07440219\n",
      "Iteration 46363, loss = 5.45585711\n",
      "Iteration 46364, loss = 3.44904336\n",
      "Iteration 46365, loss = 2.93473126\n",
      "Iteration 46366, loss = 3.57598380\n",
      "Iteration 46367, loss = 3.58966778\n",
      "Iteration 46368, loss = 2.74998062\n",
      "Iteration 46369, loss = 2.01687449\n",
      "Iteration 46370, loss = 2.21196810\n",
      "Iteration 46371, loss = 2.45980879\n",
      "Iteration 46372, loss = 2.76554392\n",
      "Iteration 46373, loss = 2.82413145\n",
      "Iteration 46374, loss = 3.66801315\n",
      "Iteration 46375, loss = 2.74687657\n",
      "Iteration 46376, loss = 2.36641092\n",
      "Iteration 46377, loss = 2.87615683\n",
      "Iteration 46378, loss = 4.10847015\n",
      "Iteration 46379, loss = 2.87002997\n",
      "Iteration 46380, loss = 2.47644758\n",
      "Iteration 46381, loss = 1.99220359\n",
      "Iteration 46382, loss = 2.08765685\n",
      "Iteration 46383, loss = 2.30825712\n",
      "Iteration 46384, loss = 2.50227531\n",
      "Iteration 46385, loss = 2.99209091\n",
      "Iteration 46386, loss = 1.94959353\n",
      "Iteration 46387, loss = 2.98932917\n",
      "Iteration 46388, loss = 1.97441905\n",
      "Iteration 46389, loss = 1.90724753\n",
      "Iteration 46390, loss = 1.82726461\n",
      "Iteration 46391, loss = 1.92364183\n",
      "Iteration 46392, loss = 2.66066539\n",
      "Iteration 46393, loss = 2.35055047\n",
      "Iteration 46394, loss = 2.34946521\n",
      "Iteration 46395, loss = 1.81649040\n",
      "Iteration 46396, loss = 1.83834864\n",
      "Iteration 46397, loss = 1.90379569\n",
      "Iteration 46398, loss = 1.79074736\n",
      "Iteration 46399, loss = 1.78831298\n",
      "Iteration 46400, loss = 2.02215121\n",
      "Iteration 46401, loss = 1.96578915\n",
      "Iteration 46402, loss = 1.86137741\n",
      "Iteration 46403, loss = 1.82374468\n",
      "Iteration 46404, loss = 1.55408943\n",
      "Iteration 46405, loss = 1.66985664\n",
      "Iteration 46406, loss = 1.70183127\n",
      "Iteration 46407, loss = 1.78831367\n",
      "Iteration 46408, loss = 1.53994897\n",
      "Iteration 46409, loss = 1.59835573\n",
      "Iteration 46410, loss = 1.53043916\n",
      "Iteration 46411, loss = 1.60593494\n",
      "Iteration 46412, loss = 1.98595630\n",
      "Iteration 46413, loss = 1.82741297\n",
      "Iteration 46414, loss = 1.53323424\n",
      "Iteration 46415, loss = 1.48271696\n",
      "Iteration 46416, loss = 1.44330764\n",
      "Iteration 46417, loss = 1.57221453\n",
      "Iteration 46418, loss = 1.63321277\n",
      "Iteration 46419, loss = 1.90177636\n",
      "Iteration 46420, loss = 2.18514111\n",
      "Iteration 46421, loss = 1.89416252\n",
      "Iteration 46422, loss = 2.14911838\n",
      "Iteration 46423, loss = 2.21361367\n",
      "Iteration 46424, loss = 2.41014952\n",
      "Iteration 46425, loss = 2.01148989\n",
      "Iteration 46426, loss = 1.63866844\n",
      "Iteration 46427, loss = 1.63835346\n",
      "Iteration 46428, loss = 1.53618341\n",
      "Iteration 46429, loss = 1.46902297\n",
      "Iteration 46430, loss = 1.49433139\n",
      "Iteration 46431, loss = 1.56759881\n",
      "Iteration 46432, loss = 1.75326283\n",
      "Iteration 46433, loss = 1.76359747\n",
      "Iteration 46434, loss = 1.54600166\n",
      "Iteration 46435, loss = 1.77920301\n",
      "Iteration 46436, loss = 1.61141022\n",
      "Iteration 46437, loss = 1.97282257\n",
      "Iteration 46438, loss = 1.88091120\n",
      "Iteration 46439, loss = 1.94352910\n",
      "Iteration 46440, loss = 1.98625021\n",
      "Iteration 46441, loss = 1.67436262\n",
      "Iteration 46442, loss = 1.52909895\n",
      "Iteration 46443, loss = 1.50446542\n",
      "Iteration 46444, loss = 1.47025053\n",
      "Iteration 46445, loss = 1.50081874\n",
      "Iteration 46446, loss = 1.74103761\n",
      "Iteration 46447, loss = 1.81515609\n",
      "Iteration 46448, loss = 1.93061882\n",
      "Iteration 46449, loss = 2.10027412\n",
      "Iteration 46450, loss = 1.78742793\n",
      "Iteration 46451, loss = 1.93718539\n",
      "Iteration 46452, loss = 2.05555356\n",
      "Iteration 46453, loss = 1.83750820\n",
      "Iteration 46454, loss = 1.51654163\n",
      "Iteration 46455, loss = 1.72962570\n",
      "Iteration 46456, loss = 2.07132831\n",
      "Iteration 46457, loss = 2.04478939\n",
      "Iteration 46458, loss = 2.00006838\n",
      "Iteration 46459, loss = 3.03723713\n",
      "Iteration 46460, loss = 4.00991028\n",
      "Iteration 46461, loss = 5.42656949\n",
      "Iteration 46462, loss = 4.36124899\n",
      "Iteration 46463, loss = 4.55310352\n",
      "Iteration 46464, loss = 3.11231140\n",
      "Iteration 46465, loss = 2.97310511\n",
      "Iteration 46466, loss = 2.25910731\n",
      "Iteration 46467, loss = 2.04227056\n",
      "Iteration 46468, loss = 1.80156296\n",
      "Iteration 46469, loss = 1.67746025\n",
      "Iteration 46470, loss = 1.57283084\n",
      "Iteration 46471, loss = 1.72712077\n",
      "Iteration 46472, loss = 1.54607140\n",
      "Iteration 46473, loss = 1.62955351\n",
      "Iteration 46474, loss = 2.12089033\n",
      "Iteration 46475, loss = 1.98775895\n",
      "Iteration 46476, loss = 2.33668016\n",
      "Iteration 46477, loss = 1.94956019\n",
      "Iteration 46478, loss = 1.71530334\n",
      "Iteration 46479, loss = 1.86830626\n",
      "Iteration 46480, loss = 1.46073231\n",
      "Iteration 46481, loss = 1.84563429\n",
      "Iteration 46482, loss = 2.12748613\n",
      "Iteration 46483, loss = 1.80229239\n",
      "Iteration 46484, loss = 1.51014954\n",
      "Iteration 46485, loss = 1.59262034\n",
      "Iteration 46486, loss = 1.51428422\n",
      "Iteration 46487, loss = 1.63345025\n",
      "Iteration 46488, loss = 1.79517969\n",
      "Iteration 46489, loss = 1.83576676\n",
      "Iteration 46490, loss = 1.77252144\n",
      "Iteration 46491, loss = 1.77003132\n",
      "Iteration 46492, loss = 1.79798331\n",
      "Iteration 46493, loss = 2.03435101\n",
      "Iteration 46494, loss = 1.71478128\n",
      "Iteration 46495, loss = 1.76275864\n",
      "Iteration 46496, loss = 1.69640000\n",
      "Iteration 46497, loss = 1.71611552\n",
      "Iteration 46498, loss = 1.69964974\n",
      "Iteration 46499, loss = 2.18169476\n",
      "Iteration 46500, loss = 2.07963660\n",
      "Iteration 46501, loss = 1.99609681\n",
      "Iteration 46502, loss = 2.16553126\n",
      "Iteration 46503, loss = 1.83693406\n",
      "Iteration 46504, loss = 2.10505712\n",
      "Iteration 46505, loss = 1.74337206\n",
      "Iteration 46506, loss = 1.55238234\n",
      "Iteration 46507, loss = 1.59813179\n",
      "Iteration 46508, loss = 1.92472652\n",
      "Iteration 46509, loss = 1.75809408\n",
      "Iteration 46510, loss = 1.63599281\n",
      "Iteration 46511, loss = 1.53574391\n",
      "Iteration 46512, loss = 1.76523654\n",
      "Iteration 46513, loss = 1.62464761\n",
      "Iteration 46514, loss = 1.74034416\n",
      "Iteration 46515, loss = 1.41353655\n",
      "Iteration 46516, loss = 1.67639895\n",
      "Iteration 46517, loss = 1.81528014\n",
      "Iteration 46518, loss = 1.72655883\n",
      "Iteration 46519, loss = 1.79269226\n",
      "Iteration 46520, loss = 1.54204453\n",
      "Iteration 46521, loss = 1.76269216\n",
      "Iteration 46522, loss = 1.65714652\n",
      "Iteration 46523, loss = 1.84796311\n",
      "Iteration 46524, loss = 1.98550929\n",
      "Iteration 46525, loss = 2.61381537\n",
      "Iteration 46526, loss = 2.64067022\n",
      "Iteration 46527, loss = 2.61233930\n",
      "Iteration 46528, loss = 1.98493774\n",
      "Iteration 46529, loss = 1.86817737\n",
      "Iteration 46530, loss = 1.85601551\n",
      "Iteration 46531, loss = 1.59959495\n",
      "Iteration 46532, loss = 1.72566040\n",
      "Iteration 46533, loss = 1.58156200\n",
      "Iteration 46534, loss = 2.31571772\n",
      "Iteration 46535, loss = 2.58394470\n",
      "Iteration 46536, loss = 2.48189877\n",
      "Iteration 46537, loss = 2.42397249\n",
      "Iteration 46538, loss = 2.27247292\n",
      "Iteration 46539, loss = 1.85588010\n",
      "Iteration 46540, loss = 1.87267070\n",
      "Iteration 46541, loss = 1.84324443\n",
      "Iteration 46542, loss = 1.80991383\n",
      "Iteration 46543, loss = 1.58817110\n",
      "Iteration 46544, loss = 1.51645181\n",
      "Iteration 46545, loss = 1.49981822\n",
      "Iteration 46546, loss = 1.51974251\n",
      "Iteration 46547, loss = 1.91574911\n",
      "Iteration 46548, loss = 2.21167603\n",
      "Iteration 46549, loss = 1.64409471\n",
      "Iteration 46550, loss = 1.67819420\n",
      "Iteration 46551, loss = 1.62175995\n",
      "Iteration 46552, loss = 1.69952478\n",
      "Iteration 46553, loss = 1.92862450\n",
      "Iteration 46554, loss = 1.68556910\n",
      "Iteration 46555, loss = 1.88475854\n",
      "Iteration 46556, loss = 1.91366766\n",
      "Iteration 46557, loss = 1.62558631\n",
      "Iteration 46558, loss = 1.67099906\n",
      "Iteration 46559, loss = 1.72893458\n",
      "Iteration 46560, loss = 1.92048344\n",
      "Iteration 46561, loss = 2.23647329\n",
      "Iteration 46562, loss = 2.54812855\n",
      "Iteration 46563, loss = 3.33369999\n",
      "Iteration 46564, loss = 2.95239842\n",
      "Iteration 46565, loss = 2.70540810\n",
      "Iteration 46566, loss = 2.28471037\n",
      "Iteration 46567, loss = 2.78016323\n",
      "Iteration 46568, loss = 2.12129188\n",
      "Iteration 46569, loss = 1.80208976\n",
      "Iteration 46570, loss = 1.87919581\n",
      "Iteration 46571, loss = 1.73419029\n",
      "Iteration 46572, loss = 1.91518259\n",
      "Iteration 46573, loss = 1.82383370\n",
      "Iteration 46574, loss = 2.01677203\n",
      "Iteration 46575, loss = 2.22387850\n",
      "Iteration 46576, loss = 2.09169935\n",
      "Iteration 46577, loss = 1.63469490\n",
      "Iteration 46578, loss = 2.00731347\n",
      "Iteration 46579, loss = 1.95693607\n",
      "Iteration 46580, loss = 2.29827220\n",
      "Iteration 46581, loss = 2.50527181\n",
      "Iteration 46582, loss = 2.30570483\n",
      "Iteration 46583, loss = 2.87978940\n",
      "Iteration 46584, loss = 2.17938174\n",
      "Iteration 46585, loss = 2.06681124\n",
      "Iteration 46586, loss = 1.98032849\n",
      "Iteration 46587, loss = 3.52877711\n",
      "Iteration 46588, loss = 2.63256492\n",
      "Iteration 46589, loss = 2.89065365\n",
      "Iteration 46590, loss = 2.15046031\n",
      "Iteration 46591, loss = 1.88192916\n",
      "Iteration 46592, loss = 2.16519279\n",
      "Iteration 46593, loss = 2.37183415\n",
      "Iteration 46594, loss = 2.16357281\n",
      "Iteration 46595, loss = 2.14347995\n",
      "Iteration 46596, loss = 1.99624724\n",
      "Iteration 46597, loss = 2.12901296\n",
      "Iteration 46598, loss = 1.75173224\n",
      "Iteration 46599, loss = 1.71648311\n",
      "Iteration 46600, loss = 1.83345164\n",
      "Iteration 46601, loss = 1.50570357\n",
      "Iteration 46602, loss = 1.42491273\n",
      "Iteration 46603, loss = 1.70594195\n",
      "Iteration 46604, loss = 1.59043400\n",
      "Iteration 46605, loss = 1.55654356\n",
      "Iteration 46606, loss = 1.56915574\n",
      "Iteration 46607, loss = 1.59909365\n",
      "Iteration 46608, loss = 1.86123463\n",
      "Iteration 46609, loss = 1.93484889\n",
      "Iteration 46610, loss = 1.92276301\n",
      "Iteration 46611, loss = 1.82757013\n",
      "Iteration 46612, loss = 1.86787391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 46613, loss = 1.59772004\n",
      "Iteration 46614, loss = 1.48002946\n",
      "Iteration 46615, loss = 1.43611265\n",
      "Iteration 46616, loss = 1.54625744\n",
      "Iteration 46617, loss = 1.85185689\n",
      "Iteration 46618, loss = 1.73499646\n",
      "Iteration 46619, loss = 1.88285486\n",
      "Iteration 46620, loss = 1.83874373\n",
      "Iteration 46621, loss = 1.76892234\n",
      "Iteration 46622, loss = 1.83712877\n",
      "Iteration 46623, loss = 1.64268629\n",
      "Iteration 46624, loss = 1.42352557\n",
      "Iteration 46625, loss = 1.62422107\n",
      "Iteration 46626, loss = 1.76281155\n",
      "Iteration 46627, loss = 1.54433059\n",
      "Iteration 46628, loss = 1.70043294\n",
      "Iteration 46629, loss = 1.87231777\n",
      "Iteration 46630, loss = 1.74932217\n",
      "Iteration 46631, loss = 1.78636388\n",
      "Iteration 46632, loss = 2.17917102\n",
      "Iteration 46633, loss = 1.69074955\n",
      "Iteration 46634, loss = 1.82737009\n",
      "Iteration 46635, loss = 1.70295113\n",
      "Iteration 46636, loss = 1.45619026\n",
      "Iteration 46637, loss = 1.50207536\n",
      "Iteration 46638, loss = 1.59189147\n",
      "Iteration 46639, loss = 1.72130835\n",
      "Iteration 46640, loss = 1.86806213\n",
      "Iteration 46641, loss = 2.23139854\n",
      "Iteration 46642, loss = 1.78933027\n",
      "Iteration 46643, loss = 1.85881976\n",
      "Iteration 46644, loss = 2.25894390\n",
      "Iteration 46645, loss = 1.87645698\n",
      "Iteration 46646, loss = 2.26164949\n",
      "Iteration 46647, loss = 2.09836256\n",
      "Iteration 46648, loss = 2.69208411\n",
      "Iteration 46649, loss = 3.56295965\n",
      "Iteration 46650, loss = 3.07418116\n",
      "Iteration 46651, loss = 3.20673451\n",
      "Iteration 46652, loss = 5.26245395\n",
      "Iteration 46653, loss = 4.00965495\n",
      "Iteration 46654, loss = 3.48454056\n",
      "Iteration 46655, loss = 2.26759556\n",
      "Iteration 46656, loss = 2.43972793\n",
      "Iteration 46657, loss = 2.20730052\n",
      "Iteration 46658, loss = 2.19934297\n",
      "Iteration 46659, loss = 2.06860960\n",
      "Iteration 46660, loss = 2.16967905\n",
      "Iteration 46661, loss = 1.75051518\n",
      "Iteration 46662, loss = 2.00281360\n",
      "Iteration 46663, loss = 2.00428541\n",
      "Iteration 46664, loss = 1.70065782\n",
      "Iteration 46665, loss = 1.53510534\n",
      "Iteration 46666, loss = 1.57724816\n",
      "Iteration 46667, loss = 1.44408601\n",
      "Iteration 46668, loss = 1.44547394\n",
      "Iteration 46669, loss = 1.49473462\n",
      "Iteration 46670, loss = 1.62982519\n",
      "Iteration 46671, loss = 1.99742623\n",
      "Iteration 46672, loss = 2.49370791\n",
      "Iteration 46673, loss = 2.24014418\n",
      "Iteration 46674, loss = 2.14010399\n",
      "Iteration 46675, loss = 2.09164222\n",
      "Iteration 46676, loss = 1.67745489\n",
      "Iteration 46677, loss = 1.79835372\n",
      "Iteration 46678, loss = 1.62251427\n",
      "Iteration 46679, loss = 1.54206860\n",
      "Iteration 46680, loss = 1.87396360\n",
      "Iteration 46681, loss = 1.75466791\n",
      "Iteration 46682, loss = 1.82466364\n",
      "Iteration 46683, loss = 1.64086496\n",
      "Iteration 46684, loss = 1.57889067\n",
      "Iteration 46685, loss = 1.54261665\n",
      "Iteration 46686, loss = 1.56334389\n",
      "Iteration 46687, loss = 1.53398800\n",
      "Iteration 46688, loss = 1.53357220\n",
      "Iteration 46689, loss = 1.44890033\n",
      "Iteration 46690, loss = 1.61528746\n",
      "Iteration 46691, loss = 1.87335906\n",
      "Iteration 46692, loss = 1.66745096\n",
      "Iteration 46693, loss = 1.68693453\n",
      "Iteration 46694, loss = 1.79803358\n",
      "Iteration 46695, loss = 1.97855562\n",
      "Iteration 46696, loss = 2.01144970\n",
      "Iteration 46697, loss = 1.61715956\n",
      "Iteration 46698, loss = 1.74234656\n",
      "Iteration 46699, loss = 1.67600173\n",
      "Iteration 46700, loss = 1.54712423\n",
      "Iteration 46701, loss = 1.72152014\n",
      "Iteration 46702, loss = 1.64833296\n",
      "Iteration 46703, loss = 1.47679351\n",
      "Iteration 46704, loss = 1.53129317\n",
      "Iteration 46705, loss = 1.65613492\n",
      "Iteration 46706, loss = 1.64112618\n",
      "Iteration 46707, loss = 1.58085406\n",
      "Iteration 46708, loss = 1.48885393\n",
      "Iteration 46709, loss = 2.00022521\n",
      "Iteration 46710, loss = 1.86123532\n",
      "Iteration 46711, loss = 2.14840479\n",
      "Iteration 46712, loss = 2.03354304\n",
      "Iteration 46713, loss = 2.28904685\n",
      "Iteration 46714, loss = 2.23417699\n",
      "Iteration 46715, loss = 1.92678892\n",
      "Iteration 46716, loss = 1.92650299\n",
      "Iteration 46717, loss = 2.08796098\n",
      "Iteration 46718, loss = 2.07624462\n",
      "Iteration 46719, loss = 1.91676848\n",
      "Iteration 46720, loss = 1.85126274\n",
      "Iteration 46721, loss = 1.94577905\n",
      "Iteration 46722, loss = 2.17722620\n",
      "Iteration 46723, loss = 2.27206917\n",
      "Iteration 46724, loss = 1.69781336\n",
      "Iteration 46725, loss = 1.99956099\n",
      "Iteration 46726, loss = 1.95673372\n",
      "Iteration 46727, loss = 2.00542958\n",
      "Iteration 46728, loss = 1.75453415\n",
      "Iteration 46729, loss = 1.90637502\n",
      "Iteration 46730, loss = 1.87444992\n",
      "Iteration 46731, loss = 1.74815964\n",
      "Iteration 46732, loss = 1.75606154\n",
      "Iteration 46733, loss = 1.64539194\n",
      "Iteration 46734, loss = 1.77946141\n",
      "Iteration 46735, loss = 1.76090719\n",
      "Iteration 46736, loss = 1.85881552\n",
      "Iteration 46737, loss = 1.70497102\n",
      "Iteration 46738, loss = 2.26216014\n",
      "Iteration 46739, loss = 2.57240255\n",
      "Iteration 46740, loss = 2.49283673\n",
      "Iteration 46741, loss = 1.96181261\n",
      "Iteration 46742, loss = 1.65934208\n",
      "Iteration 46743, loss = 1.56322446\n",
      "Iteration 46744, loss = 1.74972752\n",
      "Iteration 46745, loss = 2.04182508\n",
      "Iteration 46746, loss = 2.42911825\n",
      "Iteration 46747, loss = 3.38418054\n",
      "Iteration 46748, loss = 3.62089192\n",
      "Iteration 46749, loss = 3.08283521\n",
      "Iteration 46750, loss = 2.96113676\n",
      "Iteration 46751, loss = 2.42033578\n",
      "Iteration 46752, loss = 2.23452107\n",
      "Iteration 46753, loss = 2.49457248\n",
      "Iteration 46754, loss = 2.34180158\n",
      "Iteration 46755, loss = 2.23645500\n",
      "Iteration 46756, loss = 1.98453972\n",
      "Iteration 46757, loss = 2.17550487\n",
      "Iteration 46758, loss = 2.48213670\n",
      "Iteration 46759, loss = 2.16371798\n",
      "Iteration 46760, loss = 2.12535803\n",
      "Iteration 46761, loss = 1.69482624\n",
      "Iteration 46762, loss = 1.57124398\n",
      "Iteration 46763, loss = 1.57034450\n",
      "Iteration 46764, loss = 1.54376988\n",
      "Iteration 46765, loss = 1.49610925\n",
      "Iteration 46766, loss = 1.53018942\n",
      "Iteration 46767, loss = 1.37248791\n",
      "Iteration 46768, loss = 1.58112983\n",
      "Iteration 46769, loss = 1.96561072\n",
      "Iteration 46770, loss = 4.16603189\n",
      "Iteration 46771, loss = 4.34550822\n",
      "Iteration 46772, loss = 3.56223543\n",
      "Iteration 46773, loss = 3.97532457\n",
      "Iteration 46774, loss = 2.34867406\n",
      "Iteration 46775, loss = 2.92410630\n",
      "Iteration 46776, loss = 2.38901850\n",
      "Iteration 46777, loss = 2.03623242\n",
      "Iteration 46778, loss = 1.79800651\n",
      "Iteration 46779, loss = 1.91646866\n",
      "Iteration 46780, loss = 1.59826640\n",
      "Iteration 46781, loss = 1.82415200\n",
      "Iteration 46782, loss = 2.04571395\n",
      "Iteration 46783, loss = 1.87592484\n",
      "Iteration 46784, loss = 1.78872773\n",
      "Iteration 46785, loss = 1.75611727\n",
      "Iteration 46786, loss = 1.60138069\n",
      "Iteration 46787, loss = 1.97346945\n",
      "Iteration 46788, loss = 2.10431837\n",
      "Iteration 46789, loss = 2.03325926\n",
      "Iteration 46790, loss = 2.35012300\n",
      "Iteration 46791, loss = 2.03300171\n",
      "Iteration 46792, loss = 1.51458376\n",
      "Iteration 46793, loss = 1.50667296\n",
      "Iteration 46794, loss = 1.46223835\n",
      "Iteration 46795, loss = 1.58788178\n",
      "Iteration 46796, loss = 1.67148987\n",
      "Iteration 46797, loss = 1.87945958\n",
      "Iteration 46798, loss = 1.78748432\n",
      "Iteration 46799, loss = 1.91099313\n",
      "Iteration 46800, loss = 1.66436409\n",
      "Iteration 46801, loss = 1.49822764\n",
      "Iteration 46802, loss = 1.47170434\n",
      "Iteration 46803, loss = 1.60632137\n",
      "Iteration 46804, loss = 1.62940288\n",
      "Iteration 46805, loss = 1.99163607\n",
      "Iteration 46806, loss = 2.41111659\n",
      "Iteration 46807, loss = 2.64744702\n",
      "Iteration 46808, loss = 4.11594707\n",
      "Iteration 46809, loss = 3.30984412\n",
      "Iteration 46810, loss = 2.57289841\n",
      "Iteration 46811, loss = 1.95279796\n",
      "Iteration 46812, loss = 1.88876190\n",
      "Iteration 46813, loss = 1.74088437\n",
      "Iteration 46814, loss = 1.96558087\n",
      "Iteration 46815, loss = 1.75693853\n",
      "Iteration 46816, loss = 1.71809995\n",
      "Iteration 46817, loss = 1.80658420\n",
      "Iteration 46818, loss = 2.06077233\n",
      "Iteration 46819, loss = 1.98445236\n",
      "Iteration 46820, loss = 2.88673775\n",
      "Iteration 46821, loss = 3.35424231\n",
      "Iteration 46822, loss = 3.57130556\n",
      "Iteration 46823, loss = 3.31569202\n",
      "Iteration 46824, loss = 2.76243423\n",
      "Iteration 46825, loss = 2.55830460\n",
      "Iteration 46826, loss = 1.94790467\n",
      "Iteration 46827, loss = 2.03123125\n",
      "Iteration 46828, loss = 2.10947959\n",
      "Iteration 46829, loss = 2.01446834\n",
      "Iteration 46830, loss = 1.75807133\n",
      "Iteration 46831, loss = 1.91849111\n",
      "Iteration 46832, loss = 1.73301055\n",
      "Iteration 46833, loss = 2.09983113\n",
      "Iteration 46834, loss = 1.97446355\n",
      "Iteration 46835, loss = 1.89525973\n",
      "Iteration 46836, loss = 1.99203262\n",
      "Iteration 46837, loss = 1.68262024\n",
      "Iteration 46838, loss = 2.06422453\n",
      "Iteration 46839, loss = 1.97823690\n",
      "Iteration 46840, loss = 1.58222906\n",
      "Iteration 46841, loss = 1.69605189\n",
      "Iteration 46842, loss = 1.56552006\n",
      "Iteration 46843, loss = 1.53143834\n",
      "Iteration 46844, loss = 1.67432425\n",
      "Iteration 46845, loss = 1.98913631\n",
      "Iteration 46846, loss = 1.96088334\n",
      "Iteration 46847, loss = 1.88334600\n",
      "Iteration 46848, loss = 1.97412252\n",
      "Iteration 46849, loss = 1.81685703\n",
      "Iteration 46850, loss = 1.78066579\n",
      "Iteration 46851, loss = 1.93914520\n",
      "Iteration 46852, loss = 1.81726875\n",
      "Iteration 46853, loss = 1.44828918\n",
      "Iteration 46854, loss = 1.57091321\n",
      "Iteration 46855, loss = 1.59902471\n",
      "Iteration 46856, loss = 1.72905302\n",
      "Iteration 46857, loss = 1.92841244\n",
      "Iteration 46858, loss = 1.69501006\n",
      "Iteration 46859, loss = 1.45366068\n",
      "Iteration 46860, loss = 1.56555673\n",
      "Iteration 46861, loss = 1.57668439\n",
      "Iteration 46862, loss = 1.84738792\n",
      "Iteration 46863, loss = 1.57082456\n",
      "Iteration 46864, loss = 1.54533977\n",
      "Iteration 46865, loss = 1.53596205\n",
      "Iteration 46866, loss = 1.69696215\n",
      "Iteration 46867, loss = 1.53122762\n",
      "Iteration 46868, loss = 1.49582057\n",
      "Iteration 46869, loss = 1.73336242\n",
      "Iteration 46870, loss = 1.93679236\n",
      "Iteration 46871, loss = 1.97936795\n",
      "Iteration 46872, loss = 1.84919581\n",
      "Iteration 46873, loss = 1.64014209\n",
      "Iteration 46874, loss = 1.46259016\n",
      "Iteration 46875, loss = 1.49799067\n",
      "Iteration 46876, loss = 1.78692811\n",
      "Iteration 46877, loss = 1.62323151\n",
      "Iteration 46878, loss = 1.53350423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 46879, loss = 1.77173392\n",
      "Iteration 46880, loss = 1.62618641\n",
      "Iteration 46881, loss = 1.91334560\n",
      "Iteration 46882, loss = 1.78488391\n",
      "Iteration 46883, loss = 1.67794700\n",
      "Iteration 46884, loss = 1.96754594\n",
      "Iteration 46885, loss = 1.93091484\n",
      "Iteration 46886, loss = 2.35151334\n",
      "Iteration 46887, loss = 2.54328909\n",
      "Iteration 46888, loss = 2.63502456\n",
      "Iteration 46889, loss = 2.36571450\n",
      "Iteration 46890, loss = 2.76918908\n",
      "Iteration 46891, loss = 2.18415729\n",
      "Iteration 46892, loss = 1.87721232\n",
      "Iteration 46893, loss = 1.79254814\n",
      "Iteration 46894, loss = 2.43416965\n",
      "Iteration 46895, loss = 2.63732811\n",
      "Iteration 46896, loss = 2.15619836\n",
      "Iteration 46897, loss = 2.67576636\n",
      "Iteration 46898, loss = 2.57381739\n",
      "Iteration 46899, loss = 2.21736833\n",
      "Iteration 46900, loss = 2.56813623\n",
      "Iteration 46901, loss = 3.07599636\n",
      "Iteration 46902, loss = 2.56971243\n",
      "Iteration 46903, loss = 2.12801492\n",
      "Iteration 46904, loss = 1.95258869\n",
      "Iteration 46905, loss = 1.95081559\n",
      "Iteration 46906, loss = 1.81243636\n",
      "Iteration 46907, loss = 2.13178059\n",
      "Iteration 46908, loss = 2.32091215\n",
      "Iteration 46909, loss = 2.38779443\n",
      "Iteration 46910, loss = 3.42103311\n",
      "Iteration 46911, loss = 2.43099925\n",
      "Iteration 46912, loss = 2.49508043\n",
      "Iteration 46913, loss = 2.28947719\n",
      "Iteration 46914, loss = 2.07714080\n",
      "Iteration 46915, loss = 2.02435396\n",
      "Iteration 46916, loss = 1.92494879\n",
      "Iteration 46917, loss = 2.08885237\n",
      "Iteration 46918, loss = 1.72325894\n",
      "Iteration 46919, loss = 1.86154160\n",
      "Iteration 46920, loss = 1.64267042\n",
      "Iteration 46921, loss = 1.71501914\n",
      "Iteration 46922, loss = 1.53584838\n",
      "Iteration 46923, loss = 1.95465005\n",
      "Iteration 46924, loss = 1.87196431\n",
      "Iteration 46925, loss = 1.69078214\n",
      "Iteration 46926, loss = 1.91398492\n",
      "Iteration 46927, loss = 1.93884304\n",
      "Iteration 46928, loss = 2.20506160\n",
      "Iteration 46929, loss = 1.98400562\n",
      "Iteration 46930, loss = 1.89781848\n",
      "Iteration 46931, loss = 1.84167416\n",
      "Iteration 46932, loss = 1.98867612\n",
      "Iteration 46933, loss = 1.68093445\n",
      "Iteration 46934, loss = 2.13496405\n",
      "Iteration 46935, loss = 1.57681357\n",
      "Iteration 46936, loss = 1.43605765\n",
      "Iteration 46937, loss = 1.44768935\n",
      "Iteration 46938, loss = 1.60576602\n",
      "Iteration 46939, loss = 1.54710295\n",
      "Iteration 46940, loss = 1.63136359\n",
      "Iteration 46941, loss = 1.76225069\n",
      "Iteration 46942, loss = 1.56369519\n",
      "Iteration 46943, loss = 1.65554757\n",
      "Iteration 46944, loss = 1.48345858\n",
      "Iteration 46945, loss = 1.92293416\n",
      "Iteration 46946, loss = 1.99540038\n",
      "Iteration 46947, loss = 2.00889556\n",
      "Iteration 46948, loss = 2.09689468\n",
      "Iteration 46949, loss = 1.98068545\n",
      "Iteration 46950, loss = 2.07623601\n",
      "Iteration 46951, loss = 1.89859333\n",
      "Iteration 46952, loss = 1.57909313\n",
      "Iteration 46953, loss = 1.51196829\n",
      "Iteration 46954, loss = 1.49059163\n",
      "Iteration 46955, loss = 1.81914085\n",
      "Iteration 46956, loss = 1.62389399\n",
      "Iteration 46957, loss = 1.68443216\n",
      "Iteration 46958, loss = 1.49365238\n",
      "Iteration 46959, loss = 1.47565171\n",
      "Iteration 46960, loss = 1.70530961\n",
      "Iteration 46961, loss = 1.58757504\n",
      "Iteration 46962, loss = 1.60642305\n",
      "Iteration 46963, loss = 1.43142790\n",
      "Iteration 46964, loss = 1.59144423\n",
      "Iteration 46965, loss = 1.52807943\n",
      "Iteration 46966, loss = 1.53262518\n",
      "Iteration 46967, loss = 1.78526045\n",
      "Iteration 46968, loss = 1.97130879\n",
      "Iteration 46969, loss = 1.80364080\n",
      "Iteration 46970, loss = 1.58450961\n",
      "Iteration 46971, loss = 1.69251944\n",
      "Iteration 46972, loss = 1.65627327\n",
      "Iteration 46973, loss = 1.77141569\n",
      "Iteration 46974, loss = 1.79434232\n",
      "Iteration 46975, loss = 1.96335079\n",
      "Iteration 46976, loss = 2.01406172\n",
      "Iteration 46977, loss = 1.59733927\n",
      "Iteration 46978, loss = 1.64116891\n",
      "Iteration 46979, loss = 1.49194111\n",
      "Iteration 46980, loss = 1.54912333\n",
      "Iteration 46981, loss = 1.37446298\n",
      "Iteration 46982, loss = 1.49991086\n",
      "Iteration 46983, loss = 1.63596507\n",
      "Iteration 46984, loss = 1.60806305\n",
      "Iteration 46985, loss = 1.79669124\n",
      "Iteration 46986, loss = 2.22034133\n",
      "Iteration 46987, loss = 1.90856065\n",
      "Iteration 46988, loss = 2.22313270\n",
      "Iteration 46989, loss = 2.88549480\n",
      "Iteration 46990, loss = 3.47166539\n",
      "Iteration 46991, loss = 2.86544876\n",
      "Iteration 46992, loss = 2.16979302\n",
      "Iteration 46993, loss = 1.73993364\n",
      "Iteration 46994, loss = 1.54687310\n",
      "Iteration 46995, loss = 1.55867773\n",
      "Iteration 46996, loss = 1.72243949\n",
      "Iteration 46997, loss = 1.69460182\n",
      "Iteration 46998, loss = 1.71788384\n",
      "Iteration 46999, loss = 1.57609348\n",
      "Iteration 47000, loss = 1.46736410\n",
      "Iteration 47001, loss = 1.71830478\n",
      "Iteration 47002, loss = 1.91470549\n",
      "Iteration 47003, loss = 1.99446277\n",
      "Iteration 47004, loss = 2.24568772\n",
      "Iteration 47005, loss = 2.85303007\n",
      "Iteration 47006, loss = 3.90197101\n",
      "Iteration 47007, loss = 2.64651860\n",
      "Iteration 47008, loss = 2.01535885\n",
      "Iteration 47009, loss = 2.14236583\n",
      "Iteration 47010, loss = 2.45000498\n",
      "Iteration 47011, loss = 2.13392016\n",
      "Iteration 47012, loss = 1.70098082\n",
      "Iteration 47013, loss = 1.72257469\n",
      "Iteration 47014, loss = 1.79742565\n",
      "Iteration 47015, loss = 1.71433700\n",
      "Iteration 47016, loss = 1.58846601\n",
      "Iteration 47017, loss = 1.61092572\n",
      "Iteration 47018, loss = 1.95812128\n",
      "Iteration 47019, loss = 1.81268157\n",
      "Iteration 47020, loss = 1.57477248\n",
      "Iteration 47021, loss = 1.47891014\n",
      "Iteration 47022, loss = 1.50077778\n",
      "Iteration 47023, loss = 1.92859351\n",
      "Iteration 47024, loss = 2.54849540\n",
      "Iteration 47025, loss = 2.05485165\n",
      "Iteration 47026, loss = 1.79229949\n",
      "Iteration 47027, loss = 1.81463762\n",
      "Iteration 47028, loss = 1.78124077\n",
      "Iteration 47029, loss = 1.70836124\n",
      "Iteration 47030, loss = 1.93287683\n",
      "Iteration 47031, loss = 2.03455755\n",
      "Iteration 47032, loss = 1.55679882\n",
      "Iteration 47033, loss = 1.79191840\n",
      "Iteration 47034, loss = 2.00540360\n",
      "Iteration 47035, loss = 1.79624824\n",
      "Iteration 47036, loss = 1.73212244\n",
      "Iteration 47037, loss = 1.85529775\n",
      "Iteration 47038, loss = 2.02447185\n",
      "Iteration 47039, loss = 2.32386007\n",
      "Iteration 47040, loss = 2.96877898\n",
      "Iteration 47041, loss = 2.03386690\n",
      "Iteration 47042, loss = 2.79298957\n",
      "Iteration 47043, loss = 2.62758819\n",
      "Iteration 47044, loss = 1.95609081\n",
      "Iteration 47045, loss = 1.82930260\n",
      "Iteration 47046, loss = 1.87501528\n",
      "Iteration 47047, loss = 2.32335665\n",
      "Iteration 47048, loss = 2.39775888\n",
      "Iteration 47049, loss = 1.85561007\n",
      "Iteration 47050, loss = 2.03357808\n",
      "Iteration 47051, loss = 2.48022251\n",
      "Iteration 47052, loss = 2.19454762\n",
      "Iteration 47053, loss = 3.56395607\n",
      "Iteration 47054, loss = 2.07354373\n",
      "Iteration 47055, loss = 2.42391412\n",
      "Iteration 47056, loss = 2.88429150\n",
      "Iteration 47057, loss = 2.18404179\n",
      "Iteration 47058, loss = 2.52029050\n",
      "Iteration 47059, loss = 3.59411866\n",
      "Iteration 47060, loss = 2.80812423\n",
      "Iteration 47061, loss = 2.24509717\n",
      "Iteration 47062, loss = 2.31076429\n",
      "Iteration 47063, loss = 1.95832626\n",
      "Iteration 47064, loss = 2.17781872\n",
      "Iteration 47065, loss = 2.71972024\n",
      "Iteration 47066, loss = 2.11210220\n",
      "Iteration 47067, loss = 1.96911066\n",
      "Iteration 47068, loss = 2.42978251\n",
      "Iteration 47069, loss = 3.20015556\n",
      "Iteration 47070, loss = 2.01565293\n",
      "Iteration 47071, loss = 1.83332070\n",
      "Iteration 47072, loss = 2.16346685\n",
      "Iteration 47073, loss = 2.27215591\n",
      "Iteration 47074, loss = 2.07045695\n",
      "Iteration 47075, loss = 2.02706140\n",
      "Iteration 47076, loss = 2.11430885\n",
      "Iteration 47077, loss = 1.83100305\n",
      "Iteration 47078, loss = 1.57930254\n",
      "Iteration 47079, loss = 2.33107121\n",
      "Iteration 47080, loss = 1.57558298\n",
      "Iteration 47081, loss = 2.07789412\n",
      "Iteration 47082, loss = 1.64752833\n",
      "Iteration 47083, loss = 1.65559054\n",
      "Iteration 47084, loss = 1.72398496\n",
      "Iteration 47085, loss = 1.74921193\n",
      "Iteration 47086, loss = 2.13989067\n",
      "Iteration 47087, loss = 2.20770077\n",
      "Iteration 47088, loss = 1.85476814\n",
      "Iteration 47089, loss = 1.83216260\n",
      "Iteration 47090, loss = 1.82476623\n",
      "Iteration 47091, loss = 1.71313155\n",
      "Iteration 47092, loss = 1.98834097\n",
      "Iteration 47093, loss = 1.76814468\n",
      "Iteration 47094, loss = 1.79676452\n",
      "Iteration 47095, loss = 2.00625961\n",
      "Iteration 47096, loss = 1.65809619\n",
      "Iteration 47097, loss = 1.64925141\n",
      "Iteration 47098, loss = 1.41403445\n",
      "Iteration 47099, loss = 1.59425892\n",
      "Iteration 47100, loss = 1.57827164\n",
      "Iteration 47101, loss = 1.45391657\n",
      "Iteration 47102, loss = 1.97369913\n",
      "Iteration 47103, loss = 1.85231855\n",
      "Iteration 47104, loss = 1.79251188\n",
      "Iteration 47105, loss = 1.72219324\n",
      "Iteration 47106, loss = 2.04295426\n",
      "Iteration 47107, loss = 2.02124555\n",
      "Iteration 47108, loss = 2.24969800\n",
      "Iteration 47109, loss = 2.53737068\n",
      "Iteration 47110, loss = 1.68191899\n",
      "Iteration 47111, loss = 1.85257223\n",
      "Iteration 47112, loss = 2.23933432\n",
      "Iteration 47113, loss = 1.60968351\n",
      "Iteration 47114, loss = 2.18440681\n",
      "Iteration 47115, loss = 2.06950970\n",
      "Iteration 47116, loss = 1.84861265\n",
      "Iteration 47117, loss = 1.63984708\n",
      "Iteration 47118, loss = 1.61373613\n",
      "Iteration 47119, loss = 1.66494126\n",
      "Iteration 47120, loss = 1.53370407\n",
      "Iteration 47121, loss = 1.88711511\n",
      "Iteration 47122, loss = 2.12136267\n",
      "Iteration 47123, loss = 2.40582336\n",
      "Iteration 47124, loss = 2.07981557\n",
      "Iteration 47125, loss = 2.06522491\n",
      "Iteration 47126, loss = 1.69443279\n",
      "Iteration 47127, loss = 1.73016841\n",
      "Iteration 47128, loss = 1.94315083\n",
      "Iteration 47129, loss = 1.58211152\n",
      "Iteration 47130, loss = 1.73025959\n",
      "Iteration 47131, loss = 1.44459186\n",
      "Iteration 47132, loss = 1.41371903\n",
      "Iteration 47133, loss = 1.72044141\n",
      "Iteration 47134, loss = 1.38336896\n",
      "Iteration 47135, loss = 1.44791798\n",
      "Iteration 47136, loss = 1.87908012\n",
      "Iteration 47137, loss = 1.72359592\n",
      "Iteration 47138, loss = 1.69131317\n",
      "Iteration 47139, loss = 1.43741550\n",
      "Iteration 47140, loss = 1.57182586\n",
      "Iteration 47141, loss = 1.63377678\n",
      "Iteration 47142, loss = 1.61558974\n",
      "Iteration 47143, loss = 1.75574259\n",
      "Iteration 47144, loss = 1.51010119\n",
      "Iteration 47145, loss = 1.64133570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 47146, loss = 1.76711563\n",
      "Iteration 47147, loss = 1.86752119\n",
      "Iteration 47148, loss = 1.68898170\n",
      "Iteration 47149, loss = 1.65960600\n",
      "Iteration 47150, loss = 1.53597362\n",
      "Iteration 47151, loss = 1.84089608\n",
      "Iteration 47152, loss = 1.83705968\n",
      "Iteration 47153, loss = 1.41614689\n",
      "Iteration 47154, loss = 1.38445100\n",
      "Iteration 47155, loss = 1.42401125\n",
      "Iteration 47156, loss = 1.36353263\n",
      "Iteration 47157, loss = 1.57470741\n",
      "Iteration 47158, loss = 1.74296831\n",
      "Iteration 47159, loss = 1.67760670\n",
      "Iteration 47160, loss = 1.97214357\n",
      "Iteration 47161, loss = 1.96697507\n",
      "Iteration 47162, loss = 3.16891044\n",
      "Iteration 47163, loss = 3.08138189\n",
      "Iteration 47164, loss = 2.61597199\n",
      "Iteration 47165, loss = 3.09665813\n",
      "Iteration 47166, loss = 2.34417271\n",
      "Iteration 47167, loss = 3.00171592\n",
      "Iteration 47168, loss = 2.31679802\n",
      "Iteration 47169, loss = 1.83377667\n",
      "Iteration 47170, loss = 1.73645303\n",
      "Iteration 47171, loss = 1.93920460\n",
      "Iteration 47172, loss = 2.15890097\n",
      "Iteration 47173, loss = 3.24654274\n",
      "Iteration 47174, loss = 3.07893145\n",
      "Iteration 47175, loss = 2.11417511\n",
      "Iteration 47176, loss = 1.80466855\n",
      "Iteration 47177, loss = 2.13261013\n",
      "Iteration 47178, loss = 2.46687777\n",
      "Iteration 47179, loss = 2.58879903\n",
      "Iteration 47180, loss = 3.25696660\n",
      "Iteration 47181, loss = 2.72687675\n",
      "Iteration 47182, loss = 2.06734459\n",
      "Iteration 47183, loss = 2.09635606\n",
      "Iteration 47184, loss = 2.63916210\n",
      "Iteration 47185, loss = 2.04728128\n",
      "Iteration 47186, loss = 1.94677688\n",
      "Iteration 47187, loss = 1.65561364\n",
      "Iteration 47188, loss = 2.09184172\n",
      "Iteration 47189, loss = 2.25159747\n",
      "Iteration 47190, loss = 2.63226497\n",
      "Iteration 47191, loss = 2.10285371\n",
      "Iteration 47192, loss = 1.89404389\n",
      "Iteration 47193, loss = 1.85229069\n",
      "Iteration 47194, loss = 2.22675423\n",
      "Iteration 47195, loss = 2.76902187\n",
      "Iteration 47196, loss = 2.16548149\n",
      "Iteration 47197, loss = 2.13089295\n",
      "Iteration 47198, loss = 1.80960421\n",
      "Iteration 47199, loss = 1.64241446\n",
      "Iteration 47200, loss = 1.58117411\n",
      "Iteration 47201, loss = 1.60254077\n",
      "Iteration 47202, loss = 1.58643063\n",
      "Iteration 47203, loss = 1.64596213\n",
      "Iteration 47204, loss = 1.82128255\n",
      "Iteration 47205, loss = 1.86675459\n",
      "Iteration 47206, loss = 1.73340159\n",
      "Iteration 47207, loss = 1.70548182\n",
      "Iteration 47208, loss = 1.59431181\n",
      "Iteration 47209, loss = 1.38020258\n",
      "Iteration 47210, loss = 1.86733198\n",
      "Iteration 47211, loss = 2.10099890\n",
      "Iteration 47212, loss = 2.34218637\n",
      "Iteration 47213, loss = 2.16662244\n",
      "Iteration 47214, loss = 1.73196766\n",
      "Iteration 47215, loss = 1.75293991\n",
      "Iteration 47216, loss = 1.87585300\n",
      "Iteration 47217, loss = 1.99120866\n",
      "Iteration 47218, loss = 2.15264044\n",
      "Iteration 47219, loss = 2.60707485\n",
      "Iteration 47220, loss = 2.58127907\n",
      "Iteration 47221, loss = 2.05916438\n",
      "Iteration 47222, loss = 1.89349208\n",
      "Iteration 47223, loss = 1.73401986\n",
      "Iteration 47224, loss = 1.49300144\n",
      "Iteration 47225, loss = 1.69020544\n",
      "Iteration 47226, loss = 2.51048913\n",
      "Iteration 47227, loss = 2.23460773\n",
      "Iteration 47228, loss = 2.14037839\n",
      "Iteration 47229, loss = 1.90170530\n",
      "Iteration 47230, loss = 1.49419843\n",
      "Iteration 47231, loss = 2.03383823\n",
      "Iteration 47232, loss = 2.07466205\n",
      "Iteration 47233, loss = 2.05105681\n",
      "Iteration 47234, loss = 1.79209329\n",
      "Iteration 47235, loss = 1.71917321\n",
      "Iteration 47236, loss = 1.50502846\n",
      "Iteration 47237, loss = 1.56554806\n",
      "Iteration 47238, loss = 1.52040388\n",
      "Iteration 47239, loss = 1.46702334\n",
      "Iteration 47240, loss = 1.56481486\n",
      "Iteration 47241, loss = 1.69083196\n",
      "Iteration 47242, loss = 1.75023330\n",
      "Iteration 47243, loss = 1.71026419\n",
      "Iteration 47244, loss = 1.74242735\n",
      "Iteration 47245, loss = 2.12106471\n",
      "Iteration 47246, loss = 1.99634216\n",
      "Iteration 47247, loss = 2.13395700\n",
      "Iteration 47248, loss = 1.78051899\n",
      "Iteration 47249, loss = 1.54535699\n",
      "Iteration 47250, loss = 1.42096900\n",
      "Iteration 47251, loss = 1.57532941\n",
      "Iteration 47252, loss = 2.21714831\n",
      "Iteration 47253, loss = 1.84507066\n",
      "Iteration 47254, loss = 2.04693603\n",
      "Iteration 47255, loss = 1.70507163\n",
      "Iteration 47256, loss = 2.21988256\n",
      "Iteration 47257, loss = 2.72374056\n",
      "Iteration 47258, loss = 2.25873581\n",
      "Iteration 47259, loss = 2.49791296\n",
      "Iteration 47260, loss = 2.53825360\n",
      "Iteration 47261, loss = 2.17436853\n",
      "Iteration 47262, loss = 2.94889184\n",
      "Iteration 47263, loss = 2.82927119\n",
      "Iteration 47264, loss = 2.61983671\n",
      "Iteration 47265, loss = 2.23760783\n",
      "Iteration 47266, loss = 1.88022116\n",
      "Iteration 47267, loss = 1.90826328\n",
      "Iteration 47268, loss = 2.22284684\n",
      "Iteration 47269, loss = 1.84610703\n",
      "Iteration 47270, loss = 1.65198090\n",
      "Iteration 47271, loss = 1.89307903\n",
      "Iteration 47272, loss = 1.96148490\n",
      "Iteration 47273, loss = 1.58159349\n",
      "Iteration 47274, loss = 1.75529243\n",
      "Iteration 47275, loss = 1.60054744\n",
      "Iteration 47276, loss = 1.60427369\n",
      "Iteration 47277, loss = 1.40339423\n",
      "Iteration 47278, loss = 1.54663984\n",
      "Iteration 47279, loss = 1.75001481\n",
      "Iteration 47280, loss = 1.40754232\n",
      "Iteration 47281, loss = 1.46643409\n",
      "Iteration 47282, loss = 1.47369089\n",
      "Iteration 47283, loss = 1.81657658\n",
      "Iteration 47284, loss = 2.08531961\n",
      "Iteration 47285, loss = 1.69186611\n",
      "Iteration 47286, loss = 1.62719629\n",
      "Iteration 47287, loss = 1.54820188\n",
      "Iteration 47288, loss = 1.46830272\n",
      "Iteration 47289, loss = 1.78153148\n",
      "Iteration 47290, loss = 1.60543356\n",
      "Iteration 47291, loss = 1.63322507\n",
      "Iteration 47292, loss = 1.68313436\n",
      "Iteration 47293, loss = 1.62565687\n",
      "Iteration 47294, loss = 1.55275077\n",
      "Iteration 47295, loss = 1.61523068\n",
      "Iteration 47296, loss = 1.53809136\n",
      "Iteration 47297, loss = 1.57447577\n",
      "Iteration 47298, loss = 1.58348881\n",
      "Iteration 47299, loss = 1.67729840\n",
      "Iteration 47300, loss = 1.63848943\n",
      "Iteration 47301, loss = 1.54668109\n",
      "Iteration 47302, loss = 1.66725801\n",
      "Iteration 47303, loss = 1.72705457\n",
      "Iteration 47304, loss = 1.88589476\n",
      "Iteration 47305, loss = 1.65351462\n",
      "Iteration 47306, loss = 1.56616558\n",
      "Iteration 47307, loss = 1.81743928\n",
      "Iteration 47308, loss = 1.91587261\n",
      "Iteration 47309, loss = 1.84488508\n",
      "Iteration 47310, loss = 1.67152766\n",
      "Iteration 47311, loss = 1.76173255\n",
      "Iteration 47312, loss = 1.65924883\n",
      "Iteration 47313, loss = 1.78224952\n",
      "Iteration 47314, loss = 2.01413645\n",
      "Iteration 47315, loss = 2.35347543\n",
      "Iteration 47316, loss = 3.14276369\n",
      "Iteration 47317, loss = 2.61381004\n",
      "Iteration 47318, loss = 2.46986710\n",
      "Iteration 47319, loss = 2.56139905\n",
      "Iteration 47320, loss = 2.18636074\n",
      "Iteration 47321, loss = 1.93260923\n",
      "Iteration 47322, loss = 2.03138318\n",
      "Iteration 47323, loss = 2.35390745\n",
      "Iteration 47324, loss = 3.65614057\n",
      "Iteration 47325, loss = 2.47887131\n",
      "Iteration 47326, loss = 2.14085417\n",
      "Iteration 47327, loss = 2.07576904\n",
      "Iteration 47328, loss = 1.79051372\n",
      "Iteration 47329, loss = 1.78734596\n",
      "Iteration 47330, loss = 2.72902152\n",
      "Iteration 47331, loss = 2.04392299\n",
      "Iteration 47332, loss = 2.37342067\n",
      "Iteration 47333, loss = 2.24356318\n",
      "Iteration 47334, loss = 2.45365949\n",
      "Iteration 47335, loss = 2.53786132\n",
      "Iteration 47336, loss = 3.05176355\n",
      "Iteration 47337, loss = 2.74118883\n",
      "Iteration 47338, loss = 2.36040385\n",
      "Iteration 47339, loss = 2.64512771\n",
      "Iteration 47340, loss = 2.84712020\n",
      "Iteration 47341, loss = 3.23117356\n",
      "Iteration 47342, loss = 2.69624277\n",
      "Iteration 47343, loss = 2.53433895\n",
      "Iteration 47344, loss = 2.42897594\n",
      "Iteration 47345, loss = 1.99165164\n",
      "Iteration 47346, loss = 1.78121129\n",
      "Iteration 47347, loss = 1.90508466\n",
      "Iteration 47348, loss = 1.77772377\n",
      "Iteration 47349, loss = 1.77246838\n",
      "Iteration 47350, loss = 1.68775199\n",
      "Iteration 47351, loss = 1.71042952\n",
      "Iteration 47352, loss = 1.67344871\n",
      "Iteration 47353, loss = 1.55414105\n",
      "Iteration 47354, loss = 1.81034393\n",
      "Iteration 47355, loss = 1.98898917\n",
      "Iteration 47356, loss = 2.06236849\n",
      "Iteration 47357, loss = 2.06702256\n",
      "Iteration 47358, loss = 2.35957387\n",
      "Iteration 47359, loss = 2.60464747\n",
      "Iteration 47360, loss = 2.67035106\n",
      "Iteration 47361, loss = 2.43518701\n",
      "Iteration 47362, loss = 2.24355983\n",
      "Iteration 47363, loss = 2.30303675\n",
      "Iteration 47364, loss = 1.99734366\n",
      "Iteration 47365, loss = 2.25935116\n",
      "Iteration 47366, loss = 2.26362274\n",
      "Iteration 47367, loss = 2.30377769\n",
      "Iteration 47368, loss = 2.13142171\n",
      "Iteration 47369, loss = 2.64549683\n",
      "Iteration 47370, loss = 2.31225209\n",
      "Iteration 47371, loss = 2.00189291\n",
      "Iteration 47372, loss = 1.47693576\n",
      "Iteration 47373, loss = 1.76801325\n",
      "Iteration 47374, loss = 1.79887848\n",
      "Iteration 47375, loss = 1.82937417\n",
      "Iteration 47376, loss = 2.21580940\n",
      "Iteration 47377, loss = 2.06989632\n",
      "Iteration 47378, loss = 1.87205363\n",
      "Iteration 47379, loss = 2.46371666\n",
      "Iteration 47380, loss = 1.98614151\n",
      "Iteration 47381, loss = 2.50440293\n",
      "Iteration 47382, loss = 2.36015060\n",
      "Iteration 47383, loss = 1.61088211\n",
      "Iteration 47384, loss = 1.55708744\n",
      "Iteration 47385, loss = 1.63845213\n",
      "Iteration 47386, loss = 1.73988222\n",
      "Iteration 47387, loss = 1.74672590\n",
      "Iteration 47388, loss = 1.65709212\n",
      "Iteration 47389, loss = 1.81640061\n",
      "Iteration 47390, loss = 1.54895166\n",
      "Iteration 47391, loss = 1.63035622\n",
      "Iteration 47392, loss = 1.61743248\n",
      "Iteration 47393, loss = 1.50624928\n",
      "Iteration 47394, loss = 1.70368314\n",
      "Iteration 47395, loss = 1.64529486\n",
      "Iteration 47396, loss = 1.57403334\n",
      "Iteration 47397, loss = 1.40202429\n",
      "Iteration 47398, loss = 1.44479308\n",
      "Iteration 47399, loss = 1.33393138\n",
      "Iteration 47400, loss = 1.39430350\n",
      "Iteration 47401, loss = 1.41544744\n",
      "Iteration 47402, loss = 1.62110924\n",
      "Iteration 47403, loss = 2.08675264\n",
      "Iteration 47404, loss = 1.83744374\n",
      "Iteration 47405, loss = 2.20284357\n",
      "Iteration 47406, loss = 1.92475532\n",
      "Iteration 47407, loss = 1.71579042\n",
      "Iteration 47408, loss = 1.47376575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 47409, loss = 1.59589053\n",
      "Iteration 47410, loss = 1.62947639\n",
      "Iteration 47411, loss = 1.68361391\n",
      "Iteration 47412, loss = 1.57986938\n",
      "Iteration 47413, loss = 1.60006789\n",
      "Iteration 47414, loss = 1.74750938\n",
      "Iteration 47415, loss = 2.05552359\n",
      "Iteration 47416, loss = 1.89683470\n",
      "Iteration 47417, loss = 1.72244248\n",
      "Iteration 47418, loss = 1.47276518\n",
      "Iteration 47419, loss = 1.71080686\n",
      "Iteration 47420, loss = 1.72418247\n",
      "Iteration 47421, loss = 1.85990592\n",
      "Iteration 47422, loss = 1.83895357\n",
      "Iteration 47423, loss = 2.53241173\n",
      "Iteration 47424, loss = 2.59662002\n",
      "Iteration 47425, loss = 2.36284131\n",
      "Iteration 47426, loss = 1.91302111\n",
      "Iteration 47427, loss = 2.00096051\n",
      "Iteration 47428, loss = 1.60627384\n",
      "Iteration 47429, loss = 1.56137917\n",
      "Iteration 47430, loss = 1.49880800\n",
      "Iteration 47431, loss = 1.61137092\n",
      "Iteration 47432, loss = 1.86570224\n",
      "Iteration 47433, loss = 1.75764789\n",
      "Iteration 47434, loss = 1.71041271\n",
      "Iteration 47435, loss = 1.97838144\n",
      "Iteration 47436, loss = 1.90921363\n",
      "Iteration 47437, loss = 1.84589172\n",
      "Iteration 47438, loss = 1.90603745\n",
      "Iteration 47439, loss = 1.74356172\n",
      "Iteration 47440, loss = 1.56511660\n",
      "Iteration 47441, loss = 1.64946201\n",
      "Iteration 47442, loss = 1.54455636\n",
      "Iteration 47443, loss = 1.87213748\n",
      "Iteration 47444, loss = 1.96862907\n",
      "Iteration 47445, loss = 2.25626378\n",
      "Iteration 47446, loss = 1.97985813\n",
      "Iteration 47447, loss = 2.56170361\n",
      "Iteration 47448, loss = 5.33435008\n",
      "Iteration 47449, loss = 5.02805395\n",
      "Iteration 47450, loss = 3.96880786\n",
      "Iteration 47451, loss = 2.91214998\n",
      "Iteration 47452, loss = 2.33132564\n",
      "Iteration 47453, loss = 1.79851104\n",
      "Iteration 47454, loss = 1.77482811\n",
      "Iteration 47455, loss = 1.78855016\n",
      "Iteration 47456, loss = 2.01804787\n",
      "Iteration 47457, loss = 1.77117003\n",
      "Iteration 47458, loss = 2.00173856\n",
      "Iteration 47459, loss = 2.04571236\n",
      "Iteration 47460, loss = 1.52546051\n",
      "Iteration 47461, loss = 1.50001320\n",
      "Iteration 47462, loss = 1.49372984\n",
      "Iteration 47463, loss = 1.40423383\n",
      "Iteration 47464, loss = 1.39295172\n",
      "Iteration 47465, loss = 1.38655852\n",
      "Iteration 47466, loss = 1.52344610\n",
      "Iteration 47467, loss = 1.51289880\n",
      "Iteration 47468, loss = 2.05800112\n",
      "Iteration 47469, loss = 2.11748082\n",
      "Iteration 47470, loss = 1.52125814\n",
      "Iteration 47471, loss = 1.75200520\n",
      "Iteration 47472, loss = 1.74745982\n",
      "Iteration 47473, loss = 1.99290439\n",
      "Iteration 47474, loss = 2.06190830\n",
      "Iteration 47475, loss = 1.57930467\n",
      "Iteration 47476, loss = 1.57627038\n",
      "Iteration 47477, loss = 1.61250428\n",
      "Iteration 47478, loss = 1.54366800\n",
      "Iteration 47479, loss = 2.02395920\n",
      "Iteration 47480, loss = 1.94472010\n",
      "Iteration 47481, loss = 1.66833238\n",
      "Iteration 47482, loss = 1.81241976\n",
      "Iteration 47483, loss = 2.15392235\n",
      "Iteration 47484, loss = 2.60807175\n",
      "Iteration 47485, loss = 1.74761075\n",
      "Iteration 47486, loss = 1.98613132\n",
      "Iteration 47487, loss = 2.14215150\n",
      "Iteration 47488, loss = 2.10782081\n",
      "Iteration 47489, loss = 2.08548118\n",
      "Iteration 47490, loss = 1.79538089\n",
      "Iteration 47491, loss = 2.23259868\n",
      "Iteration 47492, loss = 2.39456240\n",
      "Iteration 47493, loss = 3.38021230\n",
      "Iteration 47494, loss = 2.41688102\n",
      "Iteration 47495, loss = 2.31511685\n",
      "Iteration 47496, loss = 2.48515400\n",
      "Iteration 47497, loss = 2.47451085\n",
      "Iteration 47498, loss = 1.83235092\n",
      "Iteration 47499, loss = 1.73972711\n",
      "Iteration 47500, loss = 1.72131938\n",
      "Iteration 47501, loss = 2.02863832\n",
      "Iteration 47502, loss = 2.31833722\n",
      "Iteration 47503, loss = 1.88425999\n",
      "Iteration 47504, loss = 2.16891995\n",
      "Iteration 47505, loss = 2.56374539\n",
      "Iteration 47506, loss = 2.57132939\n",
      "Iteration 47507, loss = 2.12528455\n",
      "Iteration 47508, loss = 1.56434463\n",
      "Iteration 47509, loss = 1.54025407\n",
      "Iteration 47510, loss = 1.75093397\n",
      "Iteration 47511, loss = 1.62359813\n",
      "Iteration 47512, loss = 1.60533053\n",
      "Iteration 47513, loss = 1.65473596\n",
      "Iteration 47514, loss = 2.09533893\n",
      "Iteration 47515, loss = 1.95991041\n",
      "Iteration 47516, loss = 1.84530444\n",
      "Iteration 47517, loss = 1.81258219\n",
      "Iteration 47518, loss = 1.65332584\n",
      "Iteration 47519, loss = 1.63085807\n",
      "Iteration 47520, loss = 1.62890432\n",
      "Iteration 47521, loss = 2.32437960\n",
      "Iteration 47522, loss = 2.47889151\n",
      "Iteration 47523, loss = 2.97880113\n",
      "Iteration 47524, loss = 2.80186443\n",
      "Iteration 47525, loss = 2.82023447\n",
      "Iteration 47526, loss = 2.01257775\n",
      "Iteration 47527, loss = 2.09494773\n",
      "Iteration 47528, loss = 2.41522328\n",
      "Iteration 47529, loss = 1.78269984\n",
      "Iteration 47530, loss = 1.68727264\n",
      "Iteration 47531, loss = 1.43664799\n",
      "Iteration 47532, loss = 1.59930964\n",
      "Iteration 47533, loss = 1.62758632\n",
      "Iteration 47534, loss = 1.71314011\n",
      "Iteration 47535, loss = 1.82716511\n",
      "Iteration 47536, loss = 1.66307888\n",
      "Iteration 47537, loss = 1.76382580\n",
      "Iteration 47538, loss = 1.48764814\n",
      "Iteration 47539, loss = 1.75106031\n",
      "Iteration 47540, loss = 2.39746054\n",
      "Iteration 47541, loss = 2.39170359\n",
      "Iteration 47542, loss = 3.07865156\n",
      "Iteration 47543, loss = 2.58719286\n",
      "Iteration 47544, loss = 2.35448838\n",
      "Iteration 47545, loss = 2.02755834\n",
      "Iteration 47546, loss = 1.85481589\n",
      "Iteration 47547, loss = 1.94343451\n",
      "Iteration 47548, loss = 1.97197251\n",
      "Iteration 47549, loss = 1.78180875\n",
      "Iteration 47550, loss = 1.84757796\n",
      "Iteration 47551, loss = 1.66447043\n",
      "Iteration 47552, loss = 1.55950551\n",
      "Iteration 47553, loss = 1.89163598\n",
      "Iteration 47554, loss = 1.51666989\n",
      "Iteration 47555, loss = 1.70260199\n",
      "Iteration 47556, loss = 1.91163107\n",
      "Iteration 47557, loss = 1.96946451\n",
      "Iteration 47558, loss = 1.79115543\n",
      "Iteration 47559, loss = 1.49774756\n",
      "Iteration 47560, loss = 1.49685549\n",
      "Iteration 47561, loss = 1.48696431\n",
      "Iteration 47562, loss = 1.43120912\n",
      "Iteration 47563, loss = 1.53255387\n",
      "Iteration 47564, loss = 2.19968109\n",
      "Iteration 47565, loss = 1.83214445\n",
      "Iteration 47566, loss = 1.64071569\n",
      "Iteration 47567, loss = 1.48711861\n",
      "Iteration 47568, loss = 1.89465763\n",
      "Iteration 47569, loss = 1.85216887\n",
      "Iteration 47570, loss = 1.67718565\n",
      "Iteration 47571, loss = 2.25134073\n",
      "Iteration 47572, loss = 2.18293060\n",
      "Iteration 47573, loss = 2.03347157\n",
      "Iteration 47574, loss = 3.07300100\n",
      "Iteration 47575, loss = 3.13250987\n",
      "Iteration 47576, loss = 3.18513558\n",
      "Iteration 47577, loss = 2.33037565\n",
      "Iteration 47578, loss = 2.35687616\n",
      "Iteration 47579, loss = 2.35788391\n",
      "Iteration 47580, loss = 2.26040590\n",
      "Iteration 47581, loss = 1.91375926\n",
      "Iteration 47582, loss = 1.63507604\n",
      "Iteration 47583, loss = 1.75070557\n",
      "Iteration 47584, loss = 1.90789333\n",
      "Iteration 47585, loss = 2.03372139\n",
      "Iteration 47586, loss = 1.69915324\n",
      "Iteration 47587, loss = 1.64253143\n",
      "Iteration 47588, loss = 2.11288760\n",
      "Iteration 47589, loss = 1.77062228\n",
      "Iteration 47590, loss = 2.33729902\n",
      "Iteration 47591, loss = 2.45746289\n",
      "Iteration 47592, loss = 2.28810649\n",
      "Iteration 47593, loss = 1.84491549\n",
      "Iteration 47594, loss = 1.92242586\n",
      "Iteration 47595, loss = 1.80465219\n",
      "Iteration 47596, loss = 1.80195169\n",
      "Iteration 47597, loss = 1.75066282\n",
      "Iteration 47598, loss = 1.82096775\n",
      "Iteration 47599, loss = 1.62738380\n",
      "Iteration 47600, loss = 1.64446889\n",
      "Iteration 47601, loss = 1.77927584\n",
      "Iteration 47602, loss = 2.15812819\n",
      "Iteration 47603, loss = 2.29048565\n",
      "Iteration 47604, loss = 2.24288863\n",
      "Iteration 47605, loss = 2.37940862\n",
      "Iteration 47606, loss = 2.36721260\n",
      "Iteration 47607, loss = 2.57097004\n",
      "Iteration 47608, loss = 1.72359390\n",
      "Iteration 47609, loss = 1.63704453\n",
      "Iteration 47610, loss = 1.52353985\n",
      "Iteration 47611, loss = 1.74239120\n",
      "Iteration 47612, loss = 1.82649385\n",
      "Iteration 47613, loss = 2.02722247\n",
      "Iteration 47614, loss = 1.95536877\n",
      "Iteration 47615, loss = 1.78717054\n",
      "Iteration 47616, loss = 1.93632217\n",
      "Iteration 47617, loss = 1.80312219\n",
      "Iteration 47618, loss = 1.58362935\n",
      "Iteration 47619, loss = 1.68073550\n",
      "Iteration 47620, loss = 1.75845194\n",
      "Iteration 47621, loss = 1.85125286\n",
      "Iteration 47622, loss = 1.93480660\n",
      "Iteration 47623, loss = 2.02139061\n",
      "Iteration 47624, loss = 1.95177322\n",
      "Iteration 47625, loss = 1.91799629\n",
      "Iteration 47626, loss = 1.59712279\n",
      "Iteration 47627, loss = 1.57599883\n",
      "Iteration 47628, loss = 1.56924504\n",
      "Iteration 47629, loss = 1.39523663\n",
      "Iteration 47630, loss = 1.46056758\n",
      "Iteration 47631, loss = 1.63826257\n",
      "Iteration 47632, loss = 1.52256957\n",
      "Iteration 47633, loss = 1.72691434\n",
      "Iteration 47634, loss = 1.64257082\n",
      "Iteration 47635, loss = 1.74229162\n",
      "Iteration 47636, loss = 1.61150871\n",
      "Iteration 47637, loss = 1.36723422\n",
      "Iteration 47638, loss = 1.47622578\n",
      "Iteration 47639, loss = 1.43105828\n",
      "Iteration 47640, loss = 1.53138668\n",
      "Iteration 47641, loss = 1.56124862\n",
      "Iteration 47642, loss = 1.73638297\n",
      "Iteration 47643, loss = 2.16026911\n",
      "Iteration 47644, loss = 2.61345989\n",
      "Iteration 47645, loss = 2.17441404\n",
      "Iteration 47646, loss = 2.04771890\n",
      "Iteration 47647, loss = 1.93162895\n",
      "Iteration 47648, loss = 2.00815215\n",
      "Iteration 47649, loss = 1.68240286\n",
      "Iteration 47650, loss = 1.56697804\n",
      "Iteration 47651, loss = 1.79339082\n",
      "Iteration 47652, loss = 1.76450973\n",
      "Iteration 47653, loss = 1.49060275\n",
      "Iteration 47654, loss = 1.46469127\n",
      "Iteration 47655, loss = 2.46974673\n",
      "Iteration 47656, loss = 2.35086813\n",
      "Iteration 47657, loss = 1.90716044\n",
      "Iteration 47658, loss = 2.68002201\n",
      "Iteration 47659, loss = 2.24931092\n",
      "Iteration 47660, loss = 1.50398609\n",
      "Iteration 47661, loss = 1.52511764\n",
      "Iteration 47662, loss = 1.64824627\n",
      "Iteration 47663, loss = 1.69336100\n",
      "Iteration 47664, loss = 1.66617849\n",
      "Iteration 47665, loss = 1.55609496\n",
      "Iteration 47666, loss = 1.57645828\n",
      "Iteration 47667, loss = 1.58493484\n",
      "Iteration 47668, loss = 1.60568118\n",
      "Iteration 47669, loss = 1.64592501\n",
      "Iteration 47670, loss = 1.76868337\n",
      "Iteration 47671, loss = 2.00108319\n",
      "Iteration 47672, loss = 2.38609691\n",
      "Iteration 47673, loss = 2.30803642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 47674, loss = 2.17729866\n",
      "Iteration 47675, loss = 2.82089311\n",
      "Iteration 47676, loss = 2.56603411\n",
      "Iteration 47677, loss = 1.66780398\n",
      "Iteration 47678, loss = 2.09052021\n",
      "Iteration 47679, loss = 1.98684149\n",
      "Iteration 47680, loss = 2.75127763\n",
      "Iteration 47681, loss = 2.47328797\n",
      "Iteration 47682, loss = 2.85900546\n",
      "Iteration 47683, loss = 3.14762307\n",
      "Iteration 47684, loss = 3.37986899\n",
      "Iteration 47685, loss = 3.19961027\n",
      "Iteration 47686, loss = 3.32528987\n",
      "Iteration 47687, loss = 3.17201840\n",
      "Iteration 47688, loss = 2.59917387\n",
      "Iteration 47689, loss = 2.27316489\n",
      "Iteration 47690, loss = 2.06685703\n",
      "Iteration 47691, loss = 1.86393959\n",
      "Iteration 47692, loss = 1.84101203\n",
      "Iteration 47693, loss = 1.69217384\n",
      "Iteration 47694, loss = 2.00908243\n",
      "Iteration 47695, loss = 1.73897740\n",
      "Iteration 47696, loss = 1.64903800\n",
      "Iteration 47697, loss = 1.67032369\n",
      "Iteration 47698, loss = 2.01552136\n",
      "Iteration 47699, loss = 1.65667908\n",
      "Iteration 47700, loss = 1.64029543\n",
      "Iteration 47701, loss = 1.54985775\n",
      "Iteration 47702, loss = 1.52807076\n",
      "Iteration 47703, loss = 1.76733472\n",
      "Iteration 47704, loss = 2.47755505\n",
      "Iteration 47705, loss = 2.85963207\n",
      "Iteration 47706, loss = 1.73800127\n",
      "Iteration 47707, loss = 1.96801252\n",
      "Iteration 47708, loss = 2.14498505\n",
      "Iteration 47709, loss = 1.90337687\n",
      "Iteration 47710, loss = 2.26816457\n",
      "Iteration 47711, loss = 2.02699316\n",
      "Iteration 47712, loss = 2.19961904\n",
      "Iteration 47713, loss = 2.65246668\n",
      "Iteration 47714, loss = 2.07791317\n",
      "Iteration 47715, loss = 1.94948375\n",
      "Iteration 47716, loss = 2.42705749\n",
      "Iteration 47717, loss = 1.82899937\n",
      "Iteration 47718, loss = 1.69434491\n",
      "Iteration 47719, loss = 1.81032031\n",
      "Iteration 47720, loss = 1.53521428\n",
      "Iteration 47721, loss = 1.86334786\n",
      "Iteration 47722, loss = 1.61533611\n",
      "Iteration 47723, loss = 1.72398103\n",
      "Iteration 47724, loss = 1.32055267\n",
      "Iteration 47725, loss = 1.89119626\n",
      "Iteration 47726, loss = 1.66833587\n",
      "Iteration 47727, loss = 1.51002347\n",
      "Iteration 47728, loss = 1.65077348\n",
      "Iteration 47729, loss = 1.46283111\n",
      "Iteration 47730, loss = 1.64667502\n",
      "Iteration 47731, loss = 1.76708982\n",
      "Iteration 47732, loss = 1.57974057\n",
      "Iteration 47733, loss = 1.66222149\n",
      "Iteration 47734, loss = 1.59057525\n",
      "Iteration 47735, loss = 1.73335397\n",
      "Iteration 47736, loss = 1.77552874\n",
      "Iteration 47737, loss = 1.59771978\n",
      "Iteration 47738, loss = 1.66133312\n",
      "Iteration 47739, loss = 1.63743124\n",
      "Iteration 47740, loss = 1.52450009\n",
      "Iteration 47741, loss = 1.86754694\n",
      "Iteration 47742, loss = 1.89435026\n",
      "Iteration 47743, loss = 1.58459747\n",
      "Iteration 47744, loss = 1.69899417\n",
      "Iteration 47745, loss = 1.58229864\n",
      "Iteration 47746, loss = 1.78240170\n",
      "Iteration 47747, loss = 1.79516849\n",
      "Iteration 47748, loss = 2.10948538\n",
      "Iteration 47749, loss = 2.08629043\n",
      "Iteration 47750, loss = 1.54388072\n",
      "Iteration 47751, loss = 1.62722081\n",
      "Iteration 47752, loss = 2.02753867\n",
      "Iteration 47753, loss = 1.72780005\n",
      "Iteration 47754, loss = 1.93584706\n",
      "Iteration 47755, loss = 1.45405736\n",
      "Iteration 47756, loss = 1.49739008\n",
      "Iteration 47757, loss = 1.33279111\n",
      "Iteration 47758, loss = 1.43877583\n",
      "Iteration 47759, loss = 1.76810359\n",
      "Iteration 47760, loss = 2.34845049\n",
      "Iteration 47761, loss = 2.53186503\n",
      "Iteration 47762, loss = 2.41069712\n",
      "Iteration 47763, loss = 2.59390213\n",
      "Iteration 47764, loss = 1.74292755\n",
      "Iteration 47765, loss = 1.59735353\n",
      "Iteration 47766, loss = 1.77084115\n",
      "Iteration 47767, loss = 1.66595290\n",
      "Iteration 47768, loss = 1.59062205\n",
      "Iteration 47769, loss = 1.48856029\n",
      "Iteration 47770, loss = 1.41697060\n",
      "Iteration 47771, loss = 1.51616583\n",
      "Iteration 47772, loss = 1.53767779\n",
      "Iteration 47773, loss = 1.50694929\n",
      "Iteration 47774, loss = 1.65641712\n",
      "Iteration 47775, loss = 1.67276362\n",
      "Iteration 47776, loss = 1.74143503\n",
      "Iteration 47777, loss = 2.01842628\n",
      "Iteration 47778, loss = 1.97341523\n",
      "Iteration 47779, loss = 1.62537297\n",
      "Iteration 47780, loss = 1.70970775\n",
      "Iteration 47781, loss = 1.49696249\n",
      "Iteration 47782, loss = 1.66567914\n",
      "Iteration 47783, loss = 1.78059046\n",
      "Iteration 47784, loss = 2.20033451\n",
      "Iteration 47785, loss = 1.95944861\n",
      "Iteration 47786, loss = 2.71152158\n",
      "Iteration 47787, loss = 1.94061202\n",
      "Iteration 47788, loss = 1.54666299\n",
      "Iteration 47789, loss = 1.83695999\n",
      "Iteration 47790, loss = 2.13250854\n",
      "Iteration 47791, loss = 2.11167466\n",
      "Iteration 47792, loss = 2.06831294\n",
      "Iteration 47793, loss = 2.56129641\n",
      "Iteration 47794, loss = 2.81930796\n",
      "Iteration 47795, loss = 2.65697061\n",
      "Iteration 47796, loss = 4.16541259\n",
      "Iteration 47797, loss = 14.48889089\n",
      "Iteration 47798, loss = 13.73469752\n",
      "Iteration 47799, loss = 6.67764043\n",
      "Iteration 47800, loss = 7.67687427\n",
      "Iteration 47801, loss = 7.05739705\n",
      "Iteration 47802, loss = 5.81626409\n",
      "Iteration 47803, loss = 4.51163582\n",
      "Iteration 47804, loss = 4.59838548\n",
      "Iteration 47805, loss = 3.19761509\n",
      "Iteration 47806, loss = 3.08429937\n",
      "Iteration 47807, loss = 3.56134984\n",
      "Iteration 47808, loss = 2.43907407\n",
      "Iteration 47809, loss = 2.44561198\n",
      "Iteration 47810, loss = 2.21093801\n",
      "Iteration 47811, loss = 2.33211450\n",
      "Iteration 47812, loss = 2.10274793\n",
      "Iteration 47813, loss = 1.96556122\n",
      "Iteration 47814, loss = 2.30870055\n",
      "Iteration 47815, loss = 1.75802324\n",
      "Iteration 47816, loss = 1.86768095\n",
      "Iteration 47817, loss = 2.14713410\n",
      "Iteration 47818, loss = 2.16040150\n",
      "Iteration 47819, loss = 1.90505149\n",
      "Iteration 47820, loss = 1.93499200\n",
      "Iteration 47821, loss = 1.69518870\n",
      "Iteration 47822, loss = 1.60156563\n",
      "Iteration 47823, loss = 1.64315661\n",
      "Iteration 47824, loss = 1.63101593\n",
      "Iteration 47825, loss = 1.53773581\n",
      "Iteration 47826, loss = 1.37783531\n",
      "Iteration 47827, loss = 1.44266731\n",
      "Iteration 47828, loss = 1.46320074\n",
      "Iteration 47829, loss = 1.42579217\n",
      "Iteration 47830, loss = 1.38710868\n",
      "Iteration 47831, loss = 1.48833780\n",
      "Iteration 47832, loss = 1.61076512\n",
      "Iteration 47833, loss = 1.56600122\n",
      "Iteration 47834, loss = 1.66217828\n",
      "Iteration 47835, loss = 1.48563091\n",
      "Iteration 47836, loss = 1.50597138\n",
      "Iteration 47837, loss = 1.39085287\n",
      "Iteration 47838, loss = 1.37307279\n",
      "Iteration 47839, loss = 1.69540594\n",
      "Iteration 47840, loss = 1.94577529\n",
      "Iteration 47841, loss = 2.07993355\n",
      "Iteration 47842, loss = 1.92013449\n",
      "Iteration 47843, loss = 1.67383349\n",
      "Iteration 47844, loss = 1.55434493\n",
      "Iteration 47845, loss = 1.63334414\n",
      "Iteration 47846, loss = 1.66567269\n",
      "Iteration 47847, loss = 1.59162583\n",
      "Iteration 47848, loss = 1.56379921\n",
      "Iteration 47849, loss = 1.50558627\n",
      "Iteration 47850, loss = 1.51250024\n",
      "Iteration 47851, loss = 1.70760376\n",
      "Iteration 47852, loss = 1.56611025\n",
      "Iteration 47853, loss = 1.55538311\n",
      "Iteration 47854, loss = 1.67141256\n",
      "Iteration 47855, loss = 1.51558567\n",
      "Iteration 47856, loss = 1.65891151\n",
      "Iteration 47857, loss = 1.69965629\n",
      "Iteration 47858, loss = 1.74710617\n",
      "Iteration 47859, loss = 1.76279404\n",
      "Iteration 47860, loss = 1.59341295\n",
      "Iteration 47861, loss = 1.76412173\n",
      "Iteration 47862, loss = 1.53777268\n",
      "Iteration 47863, loss = 1.58234770\n",
      "Iteration 47864, loss = 1.60325998\n",
      "Iteration 47865, loss = 1.66366874\n",
      "Iteration 47866, loss = 1.88759275\n",
      "Iteration 47867, loss = 2.17862052\n",
      "Iteration 47868, loss = 2.10929360\n",
      "Iteration 47869, loss = 1.88920074\n",
      "Iteration 47870, loss = 1.85063900\n",
      "Iteration 47871, loss = 1.80570616\n",
      "Iteration 47872, loss = 2.34604873\n",
      "Iteration 47873, loss = 2.52249793\n",
      "Iteration 47874, loss = 1.94794748\n",
      "Iteration 47875, loss = 3.40344932\n",
      "Iteration 47876, loss = 2.85952473\n",
      "Iteration 47877, loss = 2.04539769\n",
      "Iteration 47878, loss = 2.35340352\n",
      "Iteration 47879, loss = 2.39100881\n",
      "Iteration 47880, loss = 3.25336334\n",
      "Iteration 47881, loss = 3.00019281\n",
      "Iteration 47882, loss = 3.54855150\n",
      "Iteration 47883, loss = 3.57338200\n",
      "Iteration 47884, loss = 3.61729572\n",
      "Iteration 47885, loss = 2.76087350\n",
      "Iteration 47886, loss = 2.56895637\n",
      "Iteration 47887, loss = 1.93908852\n",
      "Iteration 47888, loss = 2.45201624\n",
      "Iteration 47889, loss = 2.17981951\n",
      "Iteration 47890, loss = 2.25941812\n",
      "Iteration 47891, loss = 1.56923025\n",
      "Iteration 47892, loss = 1.78974420\n",
      "Iteration 47893, loss = 1.93585130\n",
      "Iteration 47894, loss = 2.00426254\n",
      "Iteration 47895, loss = 2.25814302\n",
      "Iteration 47896, loss = 2.35463894\n",
      "Iteration 47897, loss = 1.91214523\n",
      "Iteration 47898, loss = 1.92164203\n",
      "Iteration 47899, loss = 1.83510996\n",
      "Iteration 47900, loss = 2.02832879\n",
      "Iteration 47901, loss = 2.50363521\n",
      "Iteration 47902, loss = 2.51004751\n",
      "Iteration 47903, loss = 2.50735245\n",
      "Iteration 47904, loss = 2.34259736\n",
      "Iteration 47905, loss = 2.45261077\n",
      "Iteration 47906, loss = 2.33703108\n",
      "Iteration 47907, loss = 2.52404222\n",
      "Iteration 47908, loss = 1.92438449\n",
      "Iteration 47909, loss = 1.62447867\n",
      "Iteration 47910, loss = 1.83994838\n",
      "Iteration 47911, loss = 1.48941912\n",
      "Iteration 47912, loss = 1.46617327\n",
      "Iteration 47913, loss = 1.54579173\n",
      "Iteration 47914, loss = 1.47968808\n",
      "Iteration 47915, loss = 1.37081911\n",
      "Iteration 47916, loss = 1.47444260\n",
      "Iteration 47917, loss = 1.45375828\n",
      "Iteration 47918, loss = 1.42662454\n",
      "Iteration 47919, loss = 1.38074986\n",
      "Iteration 47920, loss = 1.50669590\n",
      "Iteration 47921, loss = 1.77368575\n",
      "Iteration 47922, loss = 1.80308797\n",
      "Iteration 47923, loss = 1.90336420\n",
      "Iteration 47924, loss = 1.58754297\n",
      "Iteration 47925, loss = 2.04958821\n",
      "Iteration 47926, loss = 2.45388171\n",
      "Iteration 47927, loss = 1.78074851\n",
      "Iteration 47928, loss = 1.47735727\n",
      "Iteration 47929, loss = 1.58125297\n",
      "Iteration 47930, loss = 1.48213220\n",
      "Iteration 47931, loss = 1.54384900\n",
      "Iteration 47932, loss = 1.47132562\n",
      "Iteration 47933, loss = 1.56595158\n",
      "Iteration 47934, loss = 1.45712142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 47935, loss = 1.53653119\n",
      "Iteration 47936, loss = 1.96961743\n",
      "Iteration 47937, loss = 1.96571692\n",
      "Iteration 47938, loss = 2.45566013\n",
      "Iteration 47939, loss = 1.51221008\n",
      "Iteration 47940, loss = 1.51029968\n",
      "Iteration 47941, loss = 1.79747486\n",
      "Iteration 47942, loss = 1.80064423\n",
      "Iteration 47943, loss = 1.69570489\n",
      "Iteration 47944, loss = 1.75579143\n",
      "Iteration 47945, loss = 1.63073578\n",
      "Iteration 47946, loss = 1.44765277\n",
      "Iteration 47947, loss = 1.55463960\n",
      "Iteration 47948, loss = 1.77811946\n",
      "Iteration 47949, loss = 2.13871009\n",
      "Iteration 47950, loss = 1.82531231\n",
      "Iteration 47951, loss = 1.76064562\n",
      "Iteration 47952, loss = 1.41703742\n",
      "Iteration 47953, loss = 1.44826454\n",
      "Iteration 47954, loss = 1.84030231\n",
      "Iteration 47955, loss = 2.05531186\n",
      "Iteration 47956, loss = 2.10042749\n",
      "Iteration 47957, loss = 1.90359603\n",
      "Iteration 47958, loss = 2.29741818\n",
      "Iteration 47959, loss = 2.90375392\n",
      "Iteration 47960, loss = 3.67283440\n",
      "Iteration 47961, loss = 2.98640253\n",
      "Iteration 47962, loss = 2.98726857\n",
      "Iteration 47963, loss = 2.53333493\n",
      "Iteration 47964, loss = 2.57721123\n",
      "Iteration 47965, loss = 2.45103327\n",
      "Iteration 47966, loss = 1.61464131\n",
      "Iteration 47967, loss = 1.48079215\n",
      "Iteration 47968, loss = 2.13247202\n",
      "Iteration 47969, loss = 2.08732470\n",
      "Iteration 47970, loss = 1.56860131\n",
      "Iteration 47971, loss = 1.55508826\n",
      "Iteration 47972, loss = 1.50889976\n",
      "Iteration 47973, loss = 1.75573209\n",
      "Iteration 47974, loss = 1.40882253\n",
      "Iteration 47975, loss = 1.37390768\n",
      "Iteration 47976, loss = 1.42923175\n",
      "Iteration 47977, loss = 1.35920419\n",
      "Iteration 47978, loss = 1.35398307\n",
      "Iteration 47979, loss = 1.36245856\n",
      "Iteration 47980, loss = 1.73072921\n",
      "Iteration 47981, loss = 1.66897427\n",
      "Iteration 47982, loss = 1.48470806\n",
      "Iteration 47983, loss = 1.54007549\n",
      "Iteration 47984, loss = 1.89369053\n",
      "Iteration 47985, loss = 1.75378084\n",
      "Iteration 47986, loss = 1.79505644\n",
      "Iteration 47987, loss = 1.49613859\n",
      "Iteration 47988, loss = 1.48428261\n",
      "Iteration 47989, loss = 1.53888155\n",
      "Iteration 47990, loss = 1.77160439\n",
      "Iteration 47991, loss = 2.20227447\n",
      "Iteration 47992, loss = 1.82809142\n",
      "Iteration 47993, loss = 1.81413361\n",
      "Iteration 47994, loss = 1.87948236\n",
      "Iteration 47995, loss = 1.83223891\n",
      "Iteration 47996, loss = 1.77242558\n",
      "Iteration 47997, loss = 1.58805205\n",
      "Iteration 47998, loss = 1.45705920\n",
      "Iteration 47999, loss = 1.47058181\n",
      "Iteration 48000, loss = 1.63875726\n",
      "Iteration 48001, loss = 1.67222907\n",
      "Iteration 48002, loss = 1.64667852\n",
      "Iteration 48003, loss = 1.65259620\n",
      "Iteration 48004, loss = 1.76988521\n",
      "Iteration 48005, loss = 1.75224661\n",
      "Iteration 48006, loss = 1.55532193\n",
      "Iteration 48007, loss = 1.45801421\n",
      "Iteration 48008, loss = 1.39319539\n",
      "Iteration 48009, loss = 1.56757621\n",
      "Iteration 48010, loss = 1.96379043\n",
      "Iteration 48011, loss = 1.56233008\n",
      "Iteration 48012, loss = 1.80891281\n",
      "Iteration 48013, loss = 1.74162325\n",
      "Iteration 48014, loss = 1.40664242\n",
      "Iteration 48015, loss = 1.35429915\n",
      "Iteration 48016, loss = 1.52420400\n",
      "Iteration 48017, loss = 1.47739542\n",
      "Iteration 48018, loss = 1.64954538\n",
      "Iteration 48019, loss = 1.59110308\n",
      "Iteration 48020, loss = 1.62814392\n",
      "Iteration 48021, loss = 2.04000955\n",
      "Iteration 48022, loss = 1.68649843\n",
      "Iteration 48023, loss = 1.72005197\n",
      "Iteration 48024, loss = 1.57856035\n",
      "Iteration 48025, loss = 1.48364566\n",
      "Iteration 48026, loss = 1.51865286\n",
      "Iteration 48027, loss = 1.52593009\n",
      "Iteration 48028, loss = 2.03315284\n",
      "Iteration 48029, loss = 1.78372995\n",
      "Iteration 48030, loss = 1.79159975\n",
      "Iteration 48031, loss = 1.61596673\n",
      "Iteration 48032, loss = 1.79353563\n",
      "Iteration 48033, loss = 1.49220292\n",
      "Iteration 48034, loss = 2.01974715\n",
      "Iteration 48035, loss = 1.68746618\n",
      "Iteration 48036, loss = 1.66345304\n",
      "Iteration 48037, loss = 2.18443658\n",
      "Iteration 48038, loss = 2.17932391\n",
      "Iteration 48039, loss = 2.34218018\n",
      "Iteration 48040, loss = 2.44487540\n",
      "Iteration 48041, loss = 4.38432152\n",
      "Iteration 48042, loss = 4.39529135\n",
      "Iteration 48043, loss = 3.02360708\n",
      "Iteration 48044, loss = 5.21409981\n",
      "Iteration 48045, loss = 3.60356609\n",
      "Iteration 48046, loss = 2.71883740\n",
      "Iteration 48047, loss = 1.80288740\n",
      "Iteration 48048, loss = 2.27805144\n",
      "Iteration 48049, loss = 1.83694327\n",
      "Iteration 48050, loss = 1.69675194\n",
      "Iteration 48051, loss = 1.39837080\n",
      "Iteration 48052, loss = 1.42287348\n",
      "Iteration 48053, loss = 1.75068053\n",
      "Iteration 48054, loss = 1.99038414\n",
      "Iteration 48055, loss = 2.45953225\n",
      "Iteration 48056, loss = 1.88531028\n",
      "Iteration 48057, loss = 1.75076824\n",
      "Iteration 48058, loss = 1.70475906\n",
      "Iteration 48059, loss = 1.56304522\n",
      "Iteration 48060, loss = 1.59958433\n",
      "Iteration 48061, loss = 1.46457755\n",
      "Iteration 48062, loss = 1.90425986\n",
      "Iteration 48063, loss = 1.96772825\n",
      "Iteration 48064, loss = 2.29302552\n",
      "Iteration 48065, loss = 2.95305885\n",
      "Iteration 48066, loss = 2.98662285\n",
      "Iteration 48067, loss = 2.49183635\n",
      "Iteration 48068, loss = 1.62236244\n",
      "Iteration 48069, loss = 2.09683924\n",
      "Iteration 48070, loss = 1.91941717\n",
      "Iteration 48071, loss = 2.10332471\n",
      "Iteration 48072, loss = 1.78262132\n",
      "Iteration 48073, loss = 1.61395796\n",
      "Iteration 48074, loss = 1.75300855\n",
      "Iteration 48075, loss = 1.87369098\n",
      "Iteration 48076, loss = 1.83210101\n",
      "Iteration 48077, loss = 1.57891190\n",
      "Iteration 48078, loss = 1.67417968\n",
      "Iteration 48079, loss = 1.89393170\n",
      "Iteration 48080, loss = 1.85939618\n",
      "Iteration 48081, loss = 2.05443425\n",
      "Iteration 48082, loss = 2.07858152\n",
      "Iteration 48083, loss = 2.53847206\n",
      "Iteration 48084, loss = 2.64823965\n",
      "Iteration 48085, loss = 2.56227577\n",
      "Iteration 48086, loss = 2.29681702\n",
      "Iteration 48087, loss = 1.58611980\n",
      "Iteration 48088, loss = 1.82538763\n",
      "Iteration 48089, loss = 1.73300324\n",
      "Iteration 48090, loss = 1.86503446\n",
      "Iteration 48091, loss = 1.91282141\n",
      "Iteration 48092, loss = 1.83081160\n",
      "Iteration 48093, loss = 1.97827191\n",
      "Iteration 48094, loss = 1.59954764\n",
      "Iteration 48095, loss = 1.90072102\n",
      "Iteration 48096, loss = 1.74358579\n",
      "Iteration 48097, loss = 1.91865383\n",
      "Iteration 48098, loss = 1.73294450\n",
      "Iteration 48099, loss = 1.86333313\n",
      "Iteration 48100, loss = 1.66978275\n",
      "Iteration 48101, loss = 2.24491051\n",
      "Iteration 48102, loss = 1.73580269\n",
      "Iteration 48103, loss = 2.11652375\n",
      "Iteration 48104, loss = 2.19773876\n",
      "Iteration 48105, loss = 2.29024095\n",
      "Iteration 48106, loss = 1.79521857\n",
      "Iteration 48107, loss = 1.54169709\n",
      "Iteration 48108, loss = 1.40609048\n",
      "Iteration 48109, loss = 1.49625835\n",
      "Iteration 48110, loss = 1.40469282\n",
      "Iteration 48111, loss = 1.58723709\n",
      "Iteration 48112, loss = 1.51445826\n",
      "Iteration 48113, loss = 1.67743600\n",
      "Iteration 48114, loss = 1.88587195\n",
      "Iteration 48115, loss = 1.74296728\n",
      "Iteration 48116, loss = 1.58148096\n",
      "Iteration 48117, loss = 1.49831144\n",
      "Iteration 48118, loss = 1.49965663\n",
      "Iteration 48119, loss = 1.63614549\n",
      "Iteration 48120, loss = 1.49444026\n",
      "Iteration 48121, loss = 1.68934801\n",
      "Iteration 48122, loss = 1.78646082\n",
      "Iteration 48123, loss = 1.55265716\n",
      "Iteration 48124, loss = 1.76994129\n",
      "Iteration 48125, loss = 1.80535905\n",
      "Iteration 48126, loss = 2.11468373\n",
      "Iteration 48127, loss = 1.49380526\n",
      "Iteration 48128, loss = 1.68509497\n",
      "Iteration 48129, loss = 1.50446962\n",
      "Iteration 48130, loss = 1.54064302\n",
      "Iteration 48131, loss = 1.65880885\n",
      "Iteration 48132, loss = 1.77577118\n",
      "Iteration 48133, loss = 1.88148280\n",
      "Iteration 48134, loss = 1.74180569\n",
      "Iteration 48135, loss = 2.04245738\n",
      "Iteration 48136, loss = 2.32868768\n",
      "Iteration 48137, loss = 2.61836890\n",
      "Iteration 48138, loss = 1.89349032\n",
      "Iteration 48139, loss = 1.88151814\n",
      "Iteration 48140, loss = 1.64372059\n",
      "Iteration 48141, loss = 1.60893293\n",
      "Iteration 48142, loss = 1.47634758\n",
      "Iteration 48143, loss = 1.51104216\n",
      "Iteration 48144, loss = 1.83130200\n",
      "Iteration 48145, loss = 2.16352444\n",
      "Iteration 48146, loss = 1.79360293\n",
      "Iteration 48147, loss = 1.72403137\n",
      "Iteration 48148, loss = 1.91233738\n",
      "Iteration 48149, loss = 1.62277192\n",
      "Iteration 48150, loss = 1.49661046\n",
      "Iteration 48151, loss = 1.63839005\n",
      "Iteration 48152, loss = 1.65449600\n",
      "Iteration 48153, loss = 1.72059889\n",
      "Iteration 48154, loss = 1.59246830\n",
      "Iteration 48155, loss = 1.43346373\n",
      "Iteration 48156, loss = 1.62360392\n",
      "Iteration 48157, loss = 1.96797692\n",
      "Iteration 48158, loss = 1.79921263\n",
      "Iteration 48159, loss = 1.90398086\n",
      "Iteration 48160, loss = 1.93119996\n",
      "Iteration 48161, loss = 1.60626792\n",
      "Iteration 48162, loss = 2.68223411\n",
      "Iteration 48163, loss = 2.25210419\n",
      "Iteration 48164, loss = 2.31175041\n",
      "Iteration 48165, loss = 2.68085388\n",
      "Iteration 48166, loss = 1.94545905\n",
      "Iteration 48167, loss = 2.16179558\n",
      "Iteration 48168, loss = 2.18321782\n",
      "Iteration 48169, loss = 2.48264275\n",
      "Iteration 48170, loss = 2.23619956\n",
      "Iteration 48171, loss = 2.04902082\n",
      "Iteration 48172, loss = 2.49989485\n",
      "Iteration 48173, loss = 2.35679785\n",
      "Iteration 48174, loss = 1.97172420\n",
      "Iteration 48175, loss = 1.84914325\n",
      "Iteration 48176, loss = 1.97924340\n",
      "Iteration 48177, loss = 1.76137112\n",
      "Iteration 48178, loss = 1.63958013\n",
      "Iteration 48179, loss = 2.15962785\n",
      "Iteration 48180, loss = 2.71096339\n",
      "Iteration 48181, loss = 1.95645892\n",
      "Iteration 48182, loss = 2.15789628\n",
      "Iteration 48183, loss = 1.77588417\n",
      "Iteration 48184, loss = 1.76701251\n",
      "Iteration 48185, loss = 1.53587973\n",
      "Iteration 48186, loss = 1.60857391\n",
      "Iteration 48187, loss = 1.69834304\n",
      "Iteration 48188, loss = 1.53528833\n",
      "Iteration 48189, loss = 1.47041418\n",
      "Iteration 48190, loss = 1.59221344\n",
      "Iteration 48191, loss = 1.70608475\n",
      "Iteration 48192, loss = 1.80911437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 48193, loss = 1.81961012\n",
      "Iteration 48194, loss = 1.92230670\n",
      "Iteration 48195, loss = 1.67268567\n",
      "Iteration 48196, loss = 1.50970250\n",
      "Iteration 48197, loss = 1.63170991\n",
      "Iteration 48198, loss = 1.77735130\n",
      "Iteration 48199, loss = 1.89041254\n",
      "Iteration 48200, loss = 1.68138308\n",
      "Iteration 48201, loss = 1.48204506\n",
      "Iteration 48202, loss = 1.48713049\n",
      "Iteration 48203, loss = 1.54504595\n",
      "Iteration 48204, loss = 1.43032744\n",
      "Iteration 48205, loss = 1.68071492\n",
      "Iteration 48206, loss = 1.47752981\n",
      "Iteration 48207, loss = 1.45132776\n",
      "Iteration 48208, loss = 1.59115472\n",
      "Iteration 48209, loss = 1.89629451\n",
      "Iteration 48210, loss = 2.83686324\n",
      "Iteration 48211, loss = 2.71941494\n",
      "Iteration 48212, loss = 2.73949105\n",
      "Iteration 48213, loss = 2.10881339\n",
      "Iteration 48214, loss = 2.33355095\n",
      "Iteration 48215, loss = 2.35539540\n",
      "Iteration 48216, loss = 2.43271712\n",
      "Iteration 48217, loss = 1.68341645\n",
      "Iteration 48218, loss = 1.93255004\n",
      "Iteration 48219, loss = 2.07770774\n",
      "Iteration 48220, loss = 1.89513808\n",
      "Iteration 48221, loss = 1.83737133\n",
      "Iteration 48222, loss = 1.93101846\n",
      "Iteration 48223, loss = 1.79060214\n",
      "Iteration 48224, loss = 2.33414378\n",
      "Iteration 48225, loss = 2.69467648\n",
      "Iteration 48226, loss = 2.08246112\n",
      "Iteration 48227, loss = 2.06170315\n",
      "Iteration 48228, loss = 1.76367802\n",
      "Iteration 48229, loss = 1.76881171\n",
      "Iteration 48230, loss = 1.64448839\n",
      "Iteration 48231, loss = 1.81842141\n",
      "Iteration 48232, loss = 2.67140789\n",
      "Iteration 48233, loss = 2.03780020\n",
      "Iteration 48234, loss = 1.78130812\n",
      "Iteration 48235, loss = 1.53515418\n",
      "Iteration 48236, loss = 1.88518585\n",
      "Iteration 48237, loss = 1.87863972\n",
      "Iteration 48238, loss = 1.69676564\n",
      "Iteration 48239, loss = 1.94401105\n",
      "Iteration 48240, loss = 2.22444639\n",
      "Iteration 48241, loss = 1.69113459\n",
      "Iteration 48242, loss = 2.05871848\n",
      "Iteration 48243, loss = 1.85567325\n",
      "Iteration 48244, loss = 1.59527574\n",
      "Iteration 48245, loss = 1.62903362\n",
      "Iteration 48246, loss = 1.61317138\n",
      "Iteration 48247, loss = 2.04446166\n",
      "Iteration 48248, loss = 1.49263076\n",
      "Iteration 48249, loss = 1.57611548\n",
      "Iteration 48250, loss = 1.50422383\n",
      "Iteration 48251, loss = 1.75326243\n",
      "Iteration 48252, loss = 1.87121274\n",
      "Iteration 48253, loss = 1.62271229\n",
      "Iteration 48254, loss = 1.88266459\n",
      "Iteration 48255, loss = 1.69233691\n",
      "Iteration 48256, loss = 1.67890305\n",
      "Iteration 48257, loss = 1.60856222\n",
      "Iteration 48258, loss = 1.36253526\n",
      "Iteration 48259, loss = 1.48204053\n",
      "Iteration 48260, loss = 1.53533061\n",
      "Iteration 48261, loss = 1.70009187\n",
      "Iteration 48262, loss = 2.04698653\n",
      "Iteration 48263, loss = 1.98019515\n",
      "Iteration 48264, loss = 1.84084806\n",
      "Iteration 48265, loss = 1.84001996\n",
      "Iteration 48266, loss = 1.78416645\n",
      "Iteration 48267, loss = 2.05920066\n",
      "Iteration 48268, loss = 2.06141383\n",
      "Iteration 48269, loss = 1.88029408\n",
      "Iteration 48270, loss = 2.00088251\n",
      "Iteration 48271, loss = 1.54260970\n",
      "Iteration 48272, loss = 1.75082503\n",
      "Iteration 48273, loss = 1.73336791\n",
      "Iteration 48274, loss = 1.66468234\n",
      "Iteration 48275, loss = 1.44478599\n",
      "Iteration 48276, loss = 2.05442563\n",
      "Iteration 48277, loss = 1.84111118\n",
      "Iteration 48278, loss = 2.06020507\n",
      "Iteration 48279, loss = 2.08780754\n",
      "Iteration 48280, loss = 2.11536211\n",
      "Iteration 48281, loss = 2.21586010\n",
      "Iteration 48282, loss = 2.26150833\n",
      "Iteration 48283, loss = 1.65259172\n",
      "Iteration 48284, loss = 1.79616245\n",
      "Iteration 48285, loss = 1.88576230\n",
      "Iteration 48286, loss = 1.43041535\n",
      "Iteration 48287, loss = 1.62591654\n",
      "Iteration 48288, loss = 1.84287487\n",
      "Iteration 48289, loss = 1.71267919\n",
      "Iteration 48290, loss = 1.88305635\n",
      "Iteration 48291, loss = 1.76477571\n",
      "Iteration 48292, loss = 1.81653570\n",
      "Iteration 48293, loss = 1.44831237\n",
      "Iteration 48294, loss = 1.44284704\n",
      "Iteration 48295, loss = 1.74702936\n",
      "Iteration 48296, loss = 1.52513161\n",
      "Iteration 48297, loss = 1.91980807\n",
      "Iteration 48298, loss = 1.85408018\n",
      "Iteration 48299, loss = 1.78681281\n",
      "Iteration 48300, loss = 1.72476950\n",
      "Iteration 48301, loss = 1.90264508\n",
      "Iteration 48302, loss = 1.93694557\n",
      "Iteration 48303, loss = 2.12166347\n",
      "Iteration 48304, loss = 2.59033312\n",
      "Iteration 48305, loss = 4.07869657\n",
      "Iteration 48306, loss = 3.59450719\n",
      "Iteration 48307, loss = 3.26758675\n",
      "Iteration 48308, loss = 2.51566426\n",
      "Iteration 48309, loss = 2.79775361\n",
      "Iteration 48310, loss = 2.75497231\n",
      "Iteration 48311, loss = 1.89760189\n",
      "Iteration 48312, loss = 2.21906136\n",
      "Iteration 48313, loss = 4.50242737\n",
      "Iteration 48314, loss = 3.20738127\n",
      "Iteration 48315, loss = 3.47349106\n",
      "Iteration 48316, loss = 2.60629942\n",
      "Iteration 48317, loss = 2.37361996\n",
      "Iteration 48318, loss = 2.78569156\n",
      "Iteration 48319, loss = 3.07262022\n",
      "Iteration 48320, loss = 2.21139008\n",
      "Iteration 48321, loss = 2.57950190\n",
      "Iteration 48322, loss = 2.51877141\n",
      "Iteration 48323, loss = 2.25511464\n",
      "Iteration 48324, loss = 2.22535106\n",
      "Iteration 48325, loss = 1.73801152\n",
      "Iteration 48326, loss = 1.62504162\n",
      "Iteration 48327, loss = 1.45684867\n",
      "Iteration 48328, loss = 1.43990600\n",
      "Iteration 48329, loss = 1.47968450\n",
      "Iteration 48330, loss = 1.52972761\n",
      "Iteration 48331, loss = 1.40955872\n",
      "Iteration 48332, loss = 1.55588145\n",
      "Iteration 48333, loss = 1.73752593\n",
      "Iteration 48334, loss = 1.79389129\n",
      "Iteration 48335, loss = 2.05992675\n",
      "Iteration 48336, loss = 1.80871538\n",
      "Iteration 48337, loss = 1.98595324\n",
      "Iteration 48338, loss = 2.15035033\n",
      "Iteration 48339, loss = 2.17963059\n",
      "Iteration 48340, loss = 1.74633036\n",
      "Iteration 48341, loss = 1.76847905\n",
      "Iteration 48342, loss = 2.09011001\n",
      "Iteration 48343, loss = 1.89178818\n",
      "Iteration 48344, loss = 1.71062118\n",
      "Iteration 48345, loss = 1.60783372\n",
      "Iteration 48346, loss = 1.52454060\n",
      "Iteration 48347, loss = 1.42138925\n",
      "Iteration 48348, loss = 1.66510210\n",
      "Iteration 48349, loss = 1.39797971\n",
      "Iteration 48350, loss = 1.44145270\n",
      "Iteration 48351, loss = 1.39562350\n",
      "Iteration 48352, loss = 1.55472417\n",
      "Iteration 48353, loss = 1.49248298\n",
      "Iteration 48354, loss = 1.75618507\n",
      "Iteration 48355, loss = 1.61245590\n",
      "Iteration 48356, loss = 1.86654917\n",
      "Iteration 48357, loss = 2.15335419\n",
      "Iteration 48358, loss = 1.78669802\n",
      "Iteration 48359, loss = 1.64836406\n",
      "Iteration 48360, loss = 1.46687566\n",
      "Iteration 48361, loss = 1.47718947\n",
      "Iteration 48362, loss = 1.53231252\n",
      "Iteration 48363, loss = 1.36951473\n",
      "Iteration 48364, loss = 1.46858409\n",
      "Iteration 48365, loss = 1.51928896\n",
      "Iteration 48366, loss = 1.85618236\n",
      "Iteration 48367, loss = 1.85081941\n",
      "Iteration 48368, loss = 2.09055963\n",
      "Iteration 48369, loss = 2.11737758\n",
      "Iteration 48370, loss = 2.26839347\n",
      "Iteration 48371, loss = 1.69918056\n",
      "Iteration 48372, loss = 1.41276768\n",
      "Iteration 48373, loss = 1.56644665\n",
      "Iteration 48374, loss = 1.76635462\n",
      "Iteration 48375, loss = 1.60520691\n",
      "Iteration 48376, loss = 1.71253881\n",
      "Iteration 48377, loss = 2.31527691\n",
      "Iteration 48378, loss = 2.82600361\n",
      "Iteration 48379, loss = 2.62271715\n",
      "Iteration 48380, loss = 2.36892012\n",
      "Iteration 48381, loss = 2.49243502\n",
      "Iteration 48382, loss = 2.43497841\n",
      "Iteration 48383, loss = 3.07812749\n",
      "Iteration 48384, loss = 2.72826460\n",
      "Iteration 48385, loss = 2.78261819\n",
      "Iteration 48386, loss = 2.62164913\n",
      "Iteration 48387, loss = 5.07543259\n",
      "Iteration 48388, loss = 6.52264072\n",
      "Iteration 48389, loss = 4.12981312\n",
      "Iteration 48390, loss = 4.42293556\n",
      "Iteration 48391, loss = 3.01949624\n",
      "Iteration 48392, loss = 3.24078472\n",
      "Iteration 48393, loss = 2.36766131\n",
      "Iteration 48394, loss = 2.14792850\n",
      "Iteration 48395, loss = 1.65694487\n",
      "Iteration 48396, loss = 1.56739819\n",
      "Iteration 48397, loss = 1.39417167\n",
      "Iteration 48398, loss = 1.50316274\n",
      "Iteration 48399, loss = 1.92080669\n",
      "Iteration 48400, loss = 1.64609393\n",
      "Iteration 48401, loss = 1.69348245\n",
      "Iteration 48402, loss = 1.61292700\n",
      "Iteration 48403, loss = 1.76159344\n",
      "Iteration 48404, loss = 1.62995316\n",
      "Iteration 48405, loss = 1.45417279\n",
      "Iteration 48406, loss = 1.48266962\n",
      "Iteration 48407, loss = 1.50021920\n",
      "Iteration 48408, loss = 1.41320843\n",
      "Iteration 48409, loss = 1.49797988\n",
      "Iteration 48410, loss = 1.55622376\n",
      "Iteration 48411, loss = 1.75058234\n",
      "Iteration 48412, loss = 1.66406187\n",
      "Iteration 48413, loss = 1.69284603\n",
      "Iteration 48414, loss = 1.85706011\n",
      "Iteration 48415, loss = 2.44015347\n",
      "Iteration 48416, loss = 2.35492866\n",
      "Iteration 48417, loss = 2.57985563\n",
      "Iteration 48418, loss = 2.42183119\n",
      "Iteration 48419, loss = 2.60876947\n",
      "Iteration 48420, loss = 1.81503113\n",
      "Iteration 48421, loss = 2.00357290\n",
      "Iteration 48422, loss = 1.69674037\n",
      "Iteration 48423, loss = 1.71376645\n",
      "Iteration 48424, loss = 1.80521827\n",
      "Iteration 48425, loss = 1.49713560\n",
      "Iteration 48426, loss = 1.56140904\n",
      "Iteration 48427, loss = 1.55344099\n",
      "Iteration 48428, loss = 1.52532367\n",
      "Iteration 48429, loss = 1.29531861\n",
      "Iteration 48430, loss = 1.57523093\n",
      "Iteration 48431, loss = 1.60597218\n",
      "Iteration 48432, loss = 1.43982210\n",
      "Iteration 48433, loss = 1.49799715\n",
      "Iteration 48434, loss = 1.88092289\n",
      "Iteration 48435, loss = 1.87336710\n",
      "Iteration 48436, loss = 2.01280739\n",
      "Iteration 48437, loss = 1.70508189\n",
      "Iteration 48438, loss = 1.75350764\n",
      "Iteration 48439, loss = 2.14908307\n",
      "Iteration 48440, loss = 2.38831302\n",
      "Iteration 48441, loss = 2.77030632\n",
      "Iteration 48442, loss = 2.50580830\n",
      "Iteration 48443, loss = 2.42864858\n",
      "Iteration 48444, loss = 2.27929300\n",
      "Iteration 48445, loss = 3.09846246\n",
      "Iteration 48446, loss = 2.85633625\n",
      "Iteration 48447, loss = 2.85371836\n",
      "Iteration 48448, loss = 2.74895852\n",
      "Iteration 48449, loss = 2.25831576\n",
      "Iteration 48450, loss = 2.03988636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 48451, loss = 1.62204068\n",
      "Iteration 48452, loss = 1.46146058\n",
      "Iteration 48453, loss = 1.77266212\n",
      "Iteration 48454, loss = 1.58080502\n",
      "Iteration 48455, loss = 1.81837105\n",
      "Iteration 48456, loss = 1.85804263\n",
      "Iteration 48457, loss = 1.50103086\n",
      "Iteration 48458, loss = 1.40909228\n",
      "Iteration 48459, loss = 1.45636482\n",
      "Iteration 48460, loss = 1.53744252\n",
      "Iteration 48461, loss = 1.63640537\n",
      "Iteration 48462, loss = 1.72750520\n",
      "Iteration 48463, loss = 1.85270869\n",
      "Iteration 48464, loss = 2.51962085\n",
      "Iteration 48465, loss = 3.14131398\n",
      "Iteration 48466, loss = 2.29286821\n",
      "Iteration 48467, loss = 2.34256832\n",
      "Iteration 48468, loss = 1.67193806\n",
      "Iteration 48469, loss = 1.84184912\n",
      "Iteration 48470, loss = 1.50213703\n",
      "Iteration 48471, loss = 1.63888374\n",
      "Iteration 48472, loss = 1.61068787\n",
      "Iteration 48473, loss = 1.76728357\n",
      "Iteration 48474, loss = 1.88730370\n",
      "Iteration 48475, loss = 1.47828199\n",
      "Iteration 48476, loss = 1.78069434\n",
      "Iteration 48477, loss = 1.65897234\n",
      "Iteration 48478, loss = 1.86977048\n",
      "Iteration 48479, loss = 1.45664332\n",
      "Iteration 48480, loss = 1.53997138\n",
      "Iteration 48481, loss = 1.61431738\n",
      "Iteration 48482, loss = 1.67016936\n",
      "Iteration 48483, loss = 1.78437300\n",
      "Iteration 48484, loss = 1.62888554\n",
      "Iteration 48485, loss = 1.46312263\n",
      "Iteration 48486, loss = 1.51399811\n",
      "Iteration 48487, loss = 1.66592084\n",
      "Iteration 48488, loss = 1.54405852\n",
      "Iteration 48489, loss = 1.65717874\n",
      "Iteration 48490, loss = 1.40066011\n",
      "Iteration 48491, loss = 1.36411066\n",
      "Iteration 48492, loss = 1.41394434\n",
      "Iteration 48493, loss = 1.40991109\n",
      "Iteration 48494, loss = 1.41026574\n",
      "Iteration 48495, loss = 1.36686523\n",
      "Iteration 48496, loss = 1.45641866\n",
      "Iteration 48497, loss = 1.70646335\n",
      "Iteration 48498, loss = 1.89942594\n",
      "Iteration 48499, loss = 1.70376527\n",
      "Iteration 48500, loss = 2.00292034\n",
      "Iteration 48501, loss = 1.83740825\n",
      "Iteration 48502, loss = 1.81824500\n",
      "Iteration 48503, loss = 1.85947878\n",
      "Iteration 48504, loss = 2.23724733\n",
      "Iteration 48505, loss = 3.33253042\n",
      "Iteration 48506, loss = 2.57983934\n",
      "Iteration 48507, loss = 1.99019075\n",
      "Iteration 48508, loss = 2.11789981\n",
      "Iteration 48509, loss = 2.07475371\n",
      "Iteration 48510, loss = 1.93396898\n",
      "Iteration 48511, loss = 1.70931508\n",
      "Iteration 48512, loss = 1.62667301\n",
      "Iteration 48513, loss = 1.68416685\n",
      "Iteration 48514, loss = 1.85081552\n",
      "Iteration 48515, loss = 2.31276471\n",
      "Iteration 48516, loss = 1.84229024\n",
      "Iteration 48517, loss = 1.61106974\n",
      "Iteration 48518, loss = 1.32258806\n",
      "Iteration 48519, loss = 1.47348410\n",
      "Iteration 48520, loss = 1.64677063\n",
      "Iteration 48521, loss = 1.87333465\n",
      "Iteration 48522, loss = 1.49248766\n",
      "Iteration 48523, loss = 1.63635748\n",
      "Iteration 48524, loss = 1.76246124\n",
      "Iteration 48525, loss = 2.02748649\n",
      "Iteration 48526, loss = 1.71974837\n",
      "Iteration 48527, loss = 1.56573950\n",
      "Iteration 48528, loss = 1.69970643\n",
      "Iteration 48529, loss = 1.93097384\n",
      "Iteration 48530, loss = 1.53698404\n",
      "Iteration 48531, loss = 1.47824320\n",
      "Iteration 48532, loss = 1.48522667\n",
      "Iteration 48533, loss = 1.45893991\n",
      "Iteration 48534, loss = 1.63297032\n",
      "Iteration 48535, loss = 1.52489882\n",
      "Iteration 48536, loss = 1.41670066\n",
      "Iteration 48537, loss = 1.51443568\n",
      "Iteration 48538, loss = 1.83258746\n",
      "Iteration 48539, loss = 1.47366169\n",
      "Iteration 48540, loss = 1.45176130\n",
      "Iteration 48541, loss = 1.44302609\n",
      "Iteration 48542, loss = 1.52916910\n",
      "Iteration 48543, loss = 1.62108875\n",
      "Iteration 48544, loss = 1.93523491\n",
      "Iteration 48545, loss = 2.01416796\n",
      "Iteration 48546, loss = 2.38471628\n",
      "Iteration 48547, loss = 1.94428191\n",
      "Iteration 48548, loss = 1.87833096\n",
      "Iteration 48549, loss = 2.03606248\n",
      "Iteration 48550, loss = 2.20723335\n",
      "Iteration 48551, loss = 2.42789959\n",
      "Iteration 48552, loss = 2.10551782\n",
      "Iteration 48553, loss = 3.10221989\n",
      "Iteration 48554, loss = 2.71083581\n",
      "Iteration 48555, loss = 2.64346041\n",
      "Iteration 48556, loss = 2.25905186\n",
      "Iteration 48557, loss = 2.35120661\n",
      "Iteration 48558, loss = 2.32647745\n",
      "Iteration 48559, loss = 2.82618397\n",
      "Iteration 48560, loss = 2.37307396\n",
      "Iteration 48561, loss = 2.79722894\n",
      "Iteration 48562, loss = 2.53824538\n",
      "Iteration 48563, loss = 2.40408784\n",
      "Iteration 48564, loss = 2.44653225\n",
      "Iteration 48565, loss = 1.87955305\n",
      "Iteration 48566, loss = 1.94465627\n",
      "Iteration 48567, loss = 1.87175800\n",
      "Iteration 48568, loss = 1.49359647\n",
      "Iteration 48569, loss = 1.92610540\n",
      "Iteration 48570, loss = 2.02096855\n",
      "Iteration 48571, loss = 1.87203498\n",
      "Iteration 48572, loss = 2.03833011\n",
      "Iteration 48573, loss = 2.25599740\n",
      "Iteration 48574, loss = 2.24740154\n",
      "Iteration 48575, loss = 2.51198001\n",
      "Iteration 48576, loss = 2.80741883\n",
      "Iteration 48577, loss = 2.22691139\n",
      "Iteration 48578, loss = 1.95306270\n",
      "Iteration 48579, loss = 1.68783472\n",
      "Iteration 48580, loss = 1.77396484\n",
      "Iteration 48581, loss = 1.56479481\n",
      "Iteration 48582, loss = 1.39212524\n",
      "Iteration 48583, loss = 1.83767283\n",
      "Iteration 48584, loss = 1.51519361\n",
      "Iteration 48585, loss = 1.48327403\n",
      "Iteration 48586, loss = 1.39722284\n",
      "Iteration 48587, loss = 1.66544320\n",
      "Iteration 48588, loss = 1.74128777\n",
      "Iteration 48589, loss = 1.79358902\n",
      "Iteration 48590, loss = 1.94103454\n",
      "Iteration 48591, loss = 2.10152135\n",
      "Iteration 48592, loss = 3.38723149\n",
      "Iteration 48593, loss = 2.82548317\n",
      "Iteration 48594, loss = 2.48973116\n",
      "Iteration 48595, loss = 2.31052242\n",
      "Iteration 48596, loss = 2.11534329\n",
      "Iteration 48597, loss = 2.04126945\n",
      "Iteration 48598, loss = 2.27333603\n",
      "Iteration 48599, loss = 2.28758306\n",
      "Iteration 48600, loss = 2.30121321\n",
      "Iteration 48601, loss = 1.78615216\n",
      "Iteration 48602, loss = 1.75039457\n",
      "Iteration 48603, loss = 1.92159821\n",
      "Iteration 48604, loss = 1.88553357\n",
      "Iteration 48605, loss = 2.12389457\n",
      "Iteration 48606, loss = 1.83500268\n",
      "Iteration 48607, loss = 1.66477092\n",
      "Iteration 48608, loss = 1.52084198\n",
      "Iteration 48609, loss = 1.48221575\n",
      "Iteration 48610, loss = 1.61245707\n",
      "Iteration 48611, loss = 1.53048128\n",
      "Iteration 48612, loss = 1.49315288\n",
      "Iteration 48613, loss = 1.46604980\n",
      "Iteration 48614, loss = 1.63017844\n",
      "Iteration 48615, loss = 1.75269145\n",
      "Iteration 48616, loss = 2.02885460\n",
      "Iteration 48617, loss = 2.75036492\n",
      "Iteration 48618, loss = 2.98078455\n",
      "Iteration 48619, loss = 3.49645204\n",
      "Iteration 48620, loss = 2.38218477\n",
      "Iteration 48621, loss = 2.69079656\n",
      "Iteration 48622, loss = 2.01965389\n",
      "Iteration 48623, loss = 2.08775774\n",
      "Iteration 48624, loss = 2.22607347\n",
      "Iteration 48625, loss = 1.93572707\n",
      "Iteration 48626, loss = 2.04036420\n",
      "Iteration 48627, loss = 1.99169736\n",
      "Iteration 48628, loss = 2.07627389\n",
      "Iteration 48629, loss = 2.03855180\n",
      "Iteration 48630, loss = 1.59476968\n",
      "Iteration 48631, loss = 1.51022988\n",
      "Iteration 48632, loss = 1.57168224\n",
      "Iteration 48633, loss = 1.63733473\n",
      "Iteration 48634, loss = 1.57293160\n",
      "Iteration 48635, loss = 1.49770336\n",
      "Iteration 48636, loss = 1.53665377\n",
      "Iteration 48637, loss = 1.45772736\n",
      "Iteration 48638, loss = 1.39752615\n",
      "Iteration 48639, loss = 1.45486434\n",
      "Iteration 48640, loss = 1.43487419\n",
      "Iteration 48641, loss = 1.39503145\n",
      "Iteration 48642, loss = 1.37921755\n",
      "Iteration 48643, loss = 1.77191461\n",
      "Iteration 48644, loss = 2.19283307\n",
      "Iteration 48645, loss = 1.83641483\n",
      "Iteration 48646, loss = 1.91853584\n",
      "Iteration 48647, loss = 2.17023075\n",
      "Iteration 48648, loss = 2.38690620\n",
      "Iteration 48649, loss = 2.49132659\n",
      "Iteration 48650, loss = 2.96832770\n",
      "Iteration 48651, loss = 8.33347883\n",
      "Iteration 48652, loss = 7.92081882\n",
      "Iteration 48653, loss = 5.81720759\n",
      "Iteration 48654, loss = 4.02450900\n",
      "Iteration 48655, loss = 4.13966580\n",
      "Iteration 48656, loss = 2.85255297\n",
      "Iteration 48657, loss = 2.38410309\n",
      "Iteration 48658, loss = 3.56201937\n",
      "Iteration 48659, loss = 2.47187429\n",
      "Iteration 48660, loss = 1.90244074\n",
      "Iteration 48661, loss = 1.73826777\n",
      "Iteration 48662, loss = 1.55643601\n",
      "Iteration 48663, loss = 1.52234088\n",
      "Iteration 48664, loss = 1.49656226\n",
      "Iteration 48665, loss = 1.66050212\n",
      "Iteration 48666, loss = 1.37762206\n",
      "Iteration 48667, loss = 1.54415512\n",
      "Iteration 48668, loss = 1.42823950\n",
      "Iteration 48669, loss = 1.52864850\n",
      "Iteration 48670, loss = 1.44203771\n",
      "Iteration 48671, loss = 1.44817831\n",
      "Iteration 48672, loss = 1.50785196\n",
      "Iteration 48673, loss = 1.56059739\n",
      "Iteration 48674, loss = 1.58229471\n",
      "Iteration 48675, loss = 1.61758876\n",
      "Iteration 48676, loss = 1.79081855\n",
      "Iteration 48677, loss = 1.75350783\n",
      "Iteration 48678, loss = 1.78236562\n",
      "Iteration 48679, loss = 1.81150086\n",
      "Iteration 48680, loss = 2.56380506\n",
      "Iteration 48681, loss = 2.04057017\n",
      "Iteration 48682, loss = 1.94391088\n",
      "Iteration 48683, loss = 2.19809461\n",
      "Iteration 48684, loss = 2.09138521\n",
      "Iteration 48685, loss = 1.62896192\n",
      "Iteration 48686, loss = 1.57872521\n",
      "Iteration 48687, loss = 1.47246579\n",
      "Iteration 48688, loss = 2.11411068\n",
      "Iteration 48689, loss = 2.65494601\n",
      "Iteration 48690, loss = 2.15326149\n",
      "Iteration 48691, loss = 1.88576316\n",
      "Iteration 48692, loss = 1.91354112\n",
      "Iteration 48693, loss = 1.92622623\n",
      "Iteration 48694, loss = 2.06041763\n",
      "Iteration 48695, loss = 1.78842389\n",
      "Iteration 48696, loss = 1.83471956\n",
      "Iteration 48697, loss = 1.53151253\n",
      "Iteration 48698, loss = 1.55525439\n",
      "Iteration 48699, loss = 1.59451633\n",
      "Iteration 48700, loss = 1.67507100\n",
      "Iteration 48701, loss = 1.61179869\n",
      "Iteration 48702, loss = 1.53230477\n",
      "Iteration 48703, loss = 1.54221239\n",
      "Iteration 48704, loss = 1.59617707\n",
      "Iteration 48705, loss = 1.69738560\n",
      "Iteration 48706, loss = 1.63329157\n",
      "Iteration 48707, loss = 1.69510296\n",
      "Iteration 48708, loss = 1.68634801\n",
      "Iteration 48709, loss = 1.59279626\n",
      "Iteration 48710, loss = 1.48796016\n",
      "Iteration 48711, loss = 1.43909374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 48712, loss = 1.35969787\n",
      "Iteration 48713, loss = 1.51004410\n",
      "Iteration 48714, loss = 1.37996625\n",
      "Iteration 48715, loss = 1.46592250\n",
      "Iteration 48716, loss = 1.82030362\n",
      "Iteration 48717, loss = 1.85892824\n",
      "Iteration 48718, loss = 1.58593954\n",
      "Iteration 48719, loss = 1.62900590\n",
      "Iteration 48720, loss = 1.41991534\n",
      "Iteration 48721, loss = 1.39671802\n",
      "Iteration 48722, loss = 1.39481432\n",
      "Iteration 48723, loss = 1.49408902\n",
      "Iteration 48724, loss = 1.61431258\n",
      "Iteration 48725, loss = 1.52212273\n",
      "Iteration 48726, loss = 1.37386393\n",
      "Iteration 48727, loss = 1.65908383\n",
      "Iteration 48728, loss = 1.72810230\n",
      "Iteration 48729, loss = 1.58800149\n",
      "Iteration 48730, loss = 1.50369810\n",
      "Iteration 48731, loss = 1.66136606\n",
      "Iteration 48732, loss = 1.86651049\n",
      "Iteration 48733, loss = 1.64046797\n",
      "Iteration 48734, loss = 1.79405564\n",
      "Iteration 48735, loss = 1.70159805\n",
      "Iteration 48736, loss = 1.57783734\n",
      "Iteration 48737, loss = 1.49994597\n",
      "Iteration 48738, loss = 1.63651577\n",
      "Iteration 48739, loss = 1.66829669\n",
      "Iteration 48740, loss = 1.65965634\n",
      "Iteration 48741, loss = 1.62472458\n",
      "Iteration 48742, loss = 1.52693332\n",
      "Iteration 48743, loss = 1.56100840\n",
      "Iteration 48744, loss = 1.54543367\n",
      "Iteration 48745, loss = 1.59649673\n",
      "Iteration 48746, loss = 1.67970007\n",
      "Iteration 48747, loss = 1.52331605\n",
      "Iteration 48748, loss = 1.64740979\n",
      "Iteration 48749, loss = 1.51031176\n",
      "Iteration 48750, loss = 1.56655987\n",
      "Iteration 48751, loss = 1.61196740\n",
      "Iteration 48752, loss = 1.56122501\n",
      "Iteration 48753, loss = 1.62711840\n",
      "Iteration 48754, loss = 1.56881127\n",
      "Iteration 48755, loss = 1.47350536\n",
      "Iteration 48756, loss = 1.81618456\n",
      "Iteration 48757, loss = 1.73675647\n",
      "Iteration 48758, loss = 1.95583631\n",
      "Iteration 48759, loss = 1.79634411\n",
      "Iteration 48760, loss = 1.89663454\n",
      "Iteration 48761, loss = 1.39064761\n",
      "Iteration 48762, loss = 1.54021499\n",
      "Iteration 48763, loss = 1.50944439\n",
      "Iteration 48764, loss = 1.44595863\n",
      "Iteration 48765, loss = 1.43206352\n",
      "Iteration 48766, loss = 1.38537424\n",
      "Iteration 48767, loss = 1.39134542\n",
      "Iteration 48768, loss = 1.66732335\n",
      "Iteration 48769, loss = 1.95687298\n",
      "Iteration 48770, loss = 2.11047888\n",
      "Iteration 48771, loss = 1.88370243\n",
      "Iteration 48772, loss = 2.25775794\n",
      "Iteration 48773, loss = 1.83124770\n",
      "Iteration 48774, loss = 2.04802194\n",
      "Iteration 48775, loss = 1.87915415\n",
      "Iteration 48776, loss = 1.72940054\n",
      "Iteration 48777, loss = 1.65847171\n",
      "Iteration 48778, loss = 1.42257726\n",
      "Iteration 48779, loss = 1.34684403\n",
      "Iteration 48780, loss = 1.41499907\n",
      "Iteration 48781, loss = 1.43943646\n",
      "Iteration 48782, loss = 1.70728430\n",
      "Iteration 48783, loss = 2.53397909\n",
      "Iteration 48784, loss = 2.23748044\n",
      "Iteration 48785, loss = 1.83064483\n",
      "Iteration 48786, loss = 1.65004897\n",
      "Iteration 48787, loss = 2.05734844\n",
      "Iteration 48788, loss = 1.80309621\n",
      "Iteration 48789, loss = 1.44879412\n",
      "Iteration 48790, loss = 1.49991245\n",
      "Iteration 48791, loss = 1.32512391\n",
      "Iteration 48792, loss = 1.79916366\n",
      "Iteration 48793, loss = 1.64727067\n",
      "Iteration 48794, loss = 1.37229362\n",
      "Iteration 48795, loss = 1.41643748\n",
      "Iteration 48796, loss = 1.50761704\n",
      "Iteration 48797, loss = 1.73725846\n",
      "Iteration 48798, loss = 1.56021312\n",
      "Iteration 48799, loss = 1.32703977\n",
      "Iteration 48800, loss = 1.53834359\n",
      "Iteration 48801, loss = 1.42402921\n",
      "Iteration 48802, loss = 1.38115246\n",
      "Iteration 48803, loss = 1.62972395\n",
      "Iteration 48804, loss = 1.70614493\n",
      "Iteration 48805, loss = 1.89122809\n",
      "Iteration 48806, loss = 1.67991115\n",
      "Iteration 48807, loss = 1.70397634\n",
      "Iteration 48808, loss = 1.82760439\n",
      "Iteration 48809, loss = 2.15497751\n",
      "Iteration 48810, loss = 2.15746516\n",
      "Iteration 48811, loss = 1.92408280\n",
      "Iteration 48812, loss = 1.81525460\n",
      "Iteration 48813, loss = 1.91178057\n",
      "Iteration 48814, loss = 1.64502982\n",
      "Iteration 48815, loss = 1.67263520\n",
      "Iteration 48816, loss = 1.85896648\n",
      "Iteration 48817, loss = 2.39269484\n",
      "Iteration 48818, loss = 1.86129015\n",
      "Iteration 48819, loss = 1.69948397\n",
      "Iteration 48820, loss = 1.56234543\n",
      "Iteration 48821, loss = 1.65554700\n",
      "Iteration 48822, loss = 1.39183077\n",
      "Iteration 48823, loss = 1.43467288\n",
      "Iteration 48824, loss = 1.62010875\n",
      "Iteration 48825, loss = 1.74198055\n",
      "Iteration 48826, loss = 1.81534311\n",
      "Iteration 48827, loss = 2.09723558\n",
      "Iteration 48828, loss = 2.31531797\n",
      "Iteration 48829, loss = 2.97609741\n",
      "Iteration 48830, loss = 2.66103647\n",
      "Iteration 48831, loss = 2.79584151\n",
      "Iteration 48832, loss = 2.36013367\n",
      "Iteration 48833, loss = 2.25241213\n",
      "Iteration 48834, loss = 1.88770313\n",
      "Iteration 48835, loss = 1.71229401\n",
      "Iteration 48836, loss = 1.78473625\n",
      "Iteration 48837, loss = 1.92580084\n",
      "Iteration 48838, loss = 1.91806989\n",
      "Iteration 48839, loss = 1.72056626\n",
      "Iteration 48840, loss = 1.74554523\n",
      "Iteration 48841, loss = 1.76800880\n",
      "Iteration 48842, loss = 1.61397726\n",
      "Iteration 48843, loss = 1.64623456\n",
      "Iteration 48844, loss = 1.71682787\n",
      "Iteration 48845, loss = 1.45474279\n",
      "Iteration 48846, loss = 1.62548029\n",
      "Iteration 48847, loss = 1.42245526\n",
      "Iteration 48848, loss = 1.36839349\n",
      "Iteration 48849, loss = 1.37439120\n",
      "Iteration 48850, loss = 1.57741382\n",
      "Iteration 48851, loss = 1.61218554\n",
      "Iteration 48852, loss = 1.61474532\n",
      "Iteration 48853, loss = 1.91801769\n",
      "Iteration 48854, loss = 1.76980101\n",
      "Iteration 48855, loss = 2.04771017\n",
      "Iteration 48856, loss = 1.92213905\n",
      "Iteration 48857, loss = 1.64988492\n",
      "Iteration 48858, loss = 1.98698732\n",
      "Iteration 48859, loss = 2.29140574\n",
      "Iteration 48860, loss = 2.28104306\n",
      "Iteration 48861, loss = 2.46676242\n",
      "Iteration 48862, loss = 1.81548297\n",
      "Iteration 48863, loss = 1.93299150\n",
      "Iteration 48864, loss = 1.96808031\n",
      "Iteration 48865, loss = 1.95936682\n",
      "Iteration 48866, loss = 2.27843549\n",
      "Iteration 48867, loss = 2.03358164\n",
      "Iteration 48868, loss = 2.61864606\n",
      "Iteration 48869, loss = 1.90533208\n",
      "Iteration 48870, loss = 1.79614118\n",
      "Iteration 48871, loss = 1.63482794\n",
      "Iteration 48872, loss = 1.53480108\n",
      "Iteration 48873, loss = 1.66714507\n",
      "Iteration 48874, loss = 1.49006535\n",
      "Iteration 48875, loss = 1.85198100\n",
      "Iteration 48876, loss = 1.40150271\n",
      "Iteration 48877, loss = 1.49345542\n",
      "Iteration 48878, loss = 1.44136955\n",
      "Iteration 48879, loss = 1.36564704\n",
      "Iteration 48880, loss = 1.49616271\n",
      "Iteration 48881, loss = 1.68314462\n",
      "Iteration 48882, loss = 1.76275633\n",
      "Iteration 48883, loss = 1.92419029\n",
      "Iteration 48884, loss = 2.28141110\n",
      "Iteration 48885, loss = 1.70809279\n",
      "Iteration 48886, loss = 1.47012684\n",
      "Iteration 48887, loss = 1.56140882\n",
      "Iteration 48888, loss = 1.72687858\n",
      "Iteration 48889, loss = 1.48669133\n",
      "Iteration 48890, loss = 1.72495618\n",
      "Iteration 48891, loss = 1.51172525\n",
      "Iteration 48892, loss = 1.49220031\n",
      "Iteration 48893, loss = 1.57569837\n",
      "Iteration 48894, loss = 1.49420825\n",
      "Iteration 48895, loss = 1.54077894\n",
      "Iteration 48896, loss = 1.51186888\n",
      "Iteration 48897, loss = 1.62872717\n",
      "Iteration 48898, loss = 1.80822370\n",
      "Iteration 48899, loss = 1.33604765\n",
      "Iteration 48900, loss = 1.39062646\n",
      "Iteration 48901, loss = 1.45124979\n",
      "Iteration 48902, loss = 1.44409578\n",
      "Iteration 48903, loss = 1.56375405\n",
      "Iteration 48904, loss = 1.47198523\n",
      "Iteration 48905, loss = 1.44372005\n",
      "Iteration 48906, loss = 1.46761739\n",
      "Iteration 48907, loss = 1.85270834\n",
      "Iteration 48908, loss = 2.03509022\n",
      "Iteration 48909, loss = 1.82144050\n",
      "Iteration 48910, loss = 1.86435905\n",
      "Iteration 48911, loss = 1.45497389\n",
      "Iteration 48912, loss = 1.42276564\n",
      "Iteration 48913, loss = 1.78401393\n",
      "Iteration 48914, loss = 2.07435317\n",
      "Iteration 48915, loss = 1.94387959\n",
      "Iteration 48916, loss = 2.04193678\n",
      "Iteration 48917, loss = 2.02261622\n",
      "Iteration 48918, loss = 2.55333122\n",
      "Iteration 48919, loss = 2.06064722\n",
      "Iteration 48920, loss = 1.82419038\n",
      "Iteration 48921, loss = 1.79391466\n",
      "Iteration 48922, loss = 1.40762532\n",
      "Iteration 48923, loss = 1.59989118\n",
      "Iteration 48924, loss = 1.81583919\n",
      "Iteration 48925, loss = 1.79451095\n",
      "Iteration 48926, loss = 1.58594817\n",
      "Iteration 48927, loss = 1.62868160\n",
      "Iteration 48928, loss = 1.44363036\n",
      "Iteration 48929, loss = 1.66447986\n",
      "Iteration 48930, loss = 1.99293877\n",
      "Iteration 48931, loss = 2.27147428\n",
      "Iteration 48932, loss = 1.81546378\n",
      "Iteration 48933, loss = 2.09297091\n",
      "Iteration 48934, loss = 1.89088077\n",
      "Iteration 48935, loss = 1.91701130\n",
      "Iteration 48936, loss = 2.02486613\n",
      "Iteration 48937, loss = 1.95507378\n",
      "Iteration 48938, loss = 2.02680626\n",
      "Iteration 48939, loss = 1.95932116\n",
      "Iteration 48940, loss = 2.21728721\n",
      "Iteration 48941, loss = 1.68070271\n",
      "Iteration 48942, loss = 1.74356350\n",
      "Iteration 48943, loss = 4.11932050\n",
      "Iteration 48944, loss = 2.89325473\n",
      "Iteration 48945, loss = 3.80404996\n",
      "Iteration 48946, loss = 2.87308679\n",
      "Iteration 48947, loss = 2.58287642\n",
      "Iteration 48948, loss = 2.73480663\n",
      "Iteration 48949, loss = 3.03754438\n",
      "Iteration 48950, loss = 2.67700351\n",
      "Iteration 48951, loss = 2.98271153\n",
      "Iteration 48952, loss = 3.01731593\n",
      "Iteration 48953, loss = 2.10743090\n",
      "Iteration 48954, loss = 1.60645512\n",
      "Iteration 48955, loss = 1.58604785\n",
      "Iteration 48956, loss = 1.62224631\n",
      "Iteration 48957, loss = 1.49334276\n",
      "Iteration 48958, loss = 1.57857711\n",
      "Iteration 48959, loss = 2.14154467\n",
      "Iteration 48960, loss = 1.53479169\n",
      "Iteration 48961, loss = 1.34893237\n",
      "Iteration 48962, loss = 1.46209293\n",
      "Iteration 48963, loss = 1.78683379\n",
      "Iteration 48964, loss = 1.41969449\n",
      "Iteration 48965, loss = 1.93315333\n",
      "Iteration 48966, loss = 1.55099850\n",
      "Iteration 48967, loss = 2.17230297\n",
      "Iteration 48968, loss = 1.75828101\n",
      "Iteration 48969, loss = 1.71906462\n",
      "Iteration 48970, loss = 1.54481788\n",
      "Iteration 48971, loss = 1.51830345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 48972, loss = 1.90269261\n",
      "Iteration 48973, loss = 1.77200395\n",
      "Iteration 48974, loss = 1.98165231\n",
      "Iteration 48975, loss = 1.58270033\n",
      "Iteration 48976, loss = 1.75721886\n",
      "Iteration 48977, loss = 1.87663544\n",
      "Iteration 48978, loss = 1.77885341\n",
      "Iteration 48979, loss = 1.89082976\n",
      "Iteration 48980, loss = 1.55911082\n",
      "Iteration 48981, loss = 1.56045388\n",
      "Iteration 48982, loss = 1.74546611\n",
      "Iteration 48983, loss = 2.12306241\n",
      "Iteration 48984, loss = 2.45597212\n",
      "Iteration 48985, loss = 2.19290117\n",
      "Iteration 48986, loss = 1.82649062\n",
      "Iteration 48987, loss = 1.74803924\n",
      "Iteration 48988, loss = 2.05022945\n",
      "Iteration 48989, loss = 1.97308549\n",
      "Iteration 48990, loss = 1.85370696\n",
      "Iteration 48991, loss = 1.98827418\n",
      "Iteration 48992, loss = 2.14674213\n",
      "Iteration 48993, loss = 1.91947423\n",
      "Iteration 48994, loss = 1.99909412\n",
      "Iteration 48995, loss = 1.97120351\n",
      "Iteration 48996, loss = 1.38645714\n",
      "Iteration 48997, loss = 2.18175538\n",
      "Iteration 48998, loss = 1.79911009\n",
      "Iteration 48999, loss = 1.77136597\n",
      "Iteration 49000, loss = 1.67987839\n",
      "Iteration 49001, loss = 2.09727700\n",
      "Iteration 49002, loss = 2.11884878\n",
      "Iteration 49003, loss = 2.92416658\n",
      "Iteration 49004, loss = 2.13249205\n",
      "Iteration 49005, loss = 2.37715009\n",
      "Iteration 49006, loss = 1.90861267\n",
      "Iteration 49007, loss = 1.98622495\n",
      "Iteration 49008, loss = 1.78707389\n",
      "Iteration 49009, loss = 1.79856325\n",
      "Iteration 49010, loss = 1.83787859\n",
      "Iteration 49011, loss = 1.60124937\n",
      "Iteration 49012, loss = 1.42746870\n",
      "Iteration 49013, loss = 1.73251190\n",
      "Iteration 49014, loss = 1.65219886\n",
      "Iteration 49015, loss = 1.70820586\n",
      "Iteration 49016, loss = 2.06246702\n",
      "Iteration 49017, loss = 2.85118314\n",
      "Iteration 49018, loss = 1.83946975\n",
      "Iteration 49019, loss = 2.00807340\n",
      "Iteration 49020, loss = 2.35439793\n",
      "Iteration 49021, loss = 1.96718993\n",
      "Iteration 49022, loss = 1.78427609\n",
      "Iteration 49023, loss = 2.37767820\n",
      "Iteration 49024, loss = 2.27204151\n",
      "Iteration 49025, loss = 2.17460929\n",
      "Iteration 49026, loss = 2.18053050\n",
      "Iteration 49027, loss = 1.94934916\n",
      "Iteration 49028, loss = 1.90077524\n",
      "Iteration 49029, loss = 1.48472411\n",
      "Iteration 49030, loss = 1.45785621\n",
      "Iteration 49031, loss = 1.60825944\n",
      "Iteration 49032, loss = 1.69169952\n",
      "Iteration 49033, loss = 1.72774406\n",
      "Iteration 49034, loss = 1.60043736\n",
      "Iteration 49035, loss = 1.57138344\n",
      "Iteration 49036, loss = 1.56966457\n",
      "Iteration 49037, loss = 1.49125529\n",
      "Iteration 49038, loss = 1.75060788\n",
      "Iteration 49039, loss = 1.59645821\n",
      "Iteration 49040, loss = 1.49239000\n",
      "Iteration 49041, loss = 1.60060004\n",
      "Iteration 49042, loss = 2.03628885\n",
      "Iteration 49043, loss = 1.92188006\n",
      "Iteration 49044, loss = 2.16798958\n",
      "Iteration 49045, loss = 2.01964131\n",
      "Iteration 49046, loss = 2.11437294\n",
      "Iteration 49047, loss = 1.99564242\n",
      "Iteration 49048, loss = 2.21877188\n",
      "Iteration 49049, loss = 2.40325777\n",
      "Iteration 49050, loss = 2.05940193\n",
      "Iteration 49051, loss = 2.07784960\n",
      "Iteration 49052, loss = 2.18886684\n",
      "Iteration 49053, loss = 3.09969355\n",
      "Iteration 49054, loss = 2.75287054\n",
      "Iteration 49055, loss = 2.24743580\n",
      "Iteration 49056, loss = 2.57485537\n",
      "Iteration 49057, loss = 2.17182704\n",
      "Iteration 49058, loss = 1.82224831\n",
      "Iteration 49059, loss = 1.67419357\n",
      "Iteration 49060, loss = 1.57512318\n",
      "Iteration 49061, loss = 1.91877829\n",
      "Iteration 49062, loss = 1.55412834\n",
      "Iteration 49063, loss = 1.53107587\n",
      "Iteration 49064, loss = 1.76268507\n",
      "Iteration 49065, loss = 1.59557023\n",
      "Iteration 49066, loss = 1.82162869\n",
      "Iteration 49067, loss = 1.83201125\n",
      "Iteration 49068, loss = 1.81122350\n",
      "Iteration 49069, loss = 1.79457965\n",
      "Iteration 49070, loss = 1.94923472\n",
      "Iteration 49071, loss = 2.23796345\n",
      "Iteration 49072, loss = 2.46602190\n",
      "Iteration 49073, loss = 2.81738389\n",
      "Iteration 49074, loss = 2.66442837\n",
      "Iteration 49075, loss = 2.71275542\n",
      "Iteration 49076, loss = 2.83203076\n",
      "Iteration 49077, loss = 2.54456694\n",
      "Iteration 49078, loss = 1.91071924\n",
      "Iteration 49079, loss = 1.74759874\n",
      "Iteration 49080, loss = 1.69915531\n",
      "Iteration 49081, loss = 2.02698016\n",
      "Iteration 49082, loss = 2.01173429\n",
      "Iteration 49083, loss = 1.72759082\n",
      "Iteration 49084, loss = 1.75527376\n",
      "Iteration 49085, loss = 1.41426433\n",
      "Iteration 49086, loss = 1.44306098\n",
      "Iteration 49087, loss = 1.74607872\n",
      "Iteration 49088, loss = 1.97290328\n",
      "Iteration 49089, loss = 1.67134525\n",
      "Iteration 49090, loss = 1.82157990\n",
      "Iteration 49091, loss = 1.48305522\n",
      "Iteration 49092, loss = 1.35239536\n",
      "Iteration 49093, loss = 1.46300659\n",
      "Iteration 49094, loss = 1.64507575\n",
      "Iteration 49095, loss = 1.69083386\n",
      "Iteration 49096, loss = 1.58489559\n",
      "Iteration 49097, loss = 1.72779669\n",
      "Iteration 49098, loss = 1.67169618\n",
      "Iteration 49099, loss = 1.68766598\n",
      "Iteration 49100, loss = 1.78431920\n",
      "Iteration 49101, loss = 1.92457713\n",
      "Iteration 49102, loss = 1.82858646\n",
      "Iteration 49103, loss = 1.91555553\n",
      "Iteration 49104, loss = 1.56002984\n",
      "Iteration 49105, loss = 1.46407251\n",
      "Iteration 49106, loss = 1.59783145\n",
      "Iteration 49107, loss = 1.37550837\n",
      "Iteration 49108, loss = 1.40616323\n",
      "Iteration 49109, loss = 1.47014374\n",
      "Iteration 49110, loss = 1.61733834\n",
      "Iteration 49111, loss = 1.59045598\n",
      "Iteration 49112, loss = 1.94990602\n",
      "Iteration 49113, loss = 1.62712505\n",
      "Iteration 49114, loss = 1.74791546\n",
      "Iteration 49115, loss = 1.69739442\n",
      "Iteration 49116, loss = 1.78647118\n",
      "Iteration 49117, loss = 1.99332945\n",
      "Iteration 49118, loss = 1.77142989\n",
      "Iteration 49119, loss = 2.02117951\n",
      "Iteration 49120, loss = 1.69878544\n",
      "Iteration 49121, loss = 1.73544490\n",
      "Iteration 49122, loss = 2.01915166\n",
      "Iteration 49123, loss = 1.86660887\n",
      "Iteration 49124, loss = 2.06946507\n",
      "Iteration 49125, loss = 3.25344473\n",
      "Iteration 49126, loss = 2.40085892\n",
      "Iteration 49127, loss = 2.05605938\n",
      "Iteration 49128, loss = 1.59755915\n",
      "Iteration 49129, loss = 1.68068446\n",
      "Iteration 49130, loss = 1.57807696\n",
      "Iteration 49131, loss = 1.67738721\n",
      "Iteration 49132, loss = 1.78284312\n",
      "Iteration 49133, loss = 1.71068168\n",
      "Iteration 49134, loss = 1.69909829\n",
      "Iteration 49135, loss = 1.52846772\n",
      "Iteration 49136, loss = 1.55204388\n",
      "Iteration 49137, loss = 1.60324922\n",
      "Iteration 49138, loss = 1.50757623\n",
      "Iteration 49139, loss = 1.65845994\n",
      "Iteration 49140, loss = 1.75467971\n",
      "Iteration 49141, loss = 1.70674855\n",
      "Iteration 49142, loss = 1.55605327\n",
      "Iteration 49143, loss = 1.45480389\n",
      "Iteration 49144, loss = 1.50270791\n",
      "Iteration 49145, loss = 1.73724300\n",
      "Iteration 49146, loss = 1.51614843\n",
      "Iteration 49147, loss = 1.50129049\n",
      "Iteration 49148, loss = 1.84031034\n",
      "Iteration 49149, loss = 1.75851712\n",
      "Iteration 49150, loss = 1.83400154\n",
      "Iteration 49151, loss = 1.67924103\n",
      "Iteration 49152, loss = 1.48126672\n",
      "Iteration 49153, loss = 1.45053563\n",
      "Iteration 49154, loss = 1.55093400\n",
      "Iteration 49155, loss = 1.61601938\n",
      "Iteration 49156, loss = 1.60524245\n",
      "Iteration 49157, loss = 1.52682652\n",
      "Iteration 49158, loss = 1.61633128\n",
      "Iteration 49159, loss = 1.78691078\n",
      "Iteration 49160, loss = 1.66785178\n",
      "Iteration 49161, loss = 1.65110225\n",
      "Iteration 49162, loss = 1.80286295\n",
      "Iteration 49163, loss = 1.72023391\n",
      "Iteration 49164, loss = 1.54676682\n",
      "Iteration 49165, loss = 1.54820774\n",
      "Iteration 49166, loss = 1.82011223\n",
      "Iteration 49167, loss = 1.61097301\n",
      "Iteration 49168, loss = 1.92975294\n",
      "Iteration 49169, loss = 1.67957259\n",
      "Iteration 49170, loss = 1.66631500\n",
      "Iteration 49171, loss = 1.89724018\n",
      "Iteration 49172, loss = 1.85831734\n",
      "Iteration 49173, loss = 1.69747345\n",
      "Iteration 49174, loss = 1.52564797\n",
      "Iteration 49175, loss = 1.67791769\n",
      "Iteration 49176, loss = 1.72987872\n",
      "Iteration 49177, loss = 1.56199591\n",
      "Iteration 49178, loss = 2.19860939\n",
      "Iteration 49179, loss = 1.84674597\n",
      "Iteration 49180, loss = 1.88108128\n",
      "Iteration 49181, loss = 1.78293635\n",
      "Iteration 49182, loss = 1.92521679\n",
      "Iteration 49183, loss = 1.43438396\n",
      "Iteration 49184, loss = 1.84215922\n",
      "Iteration 49185, loss = 1.90876951\n",
      "Iteration 49186, loss = 1.72116917\n",
      "Iteration 49187, loss = 1.81580540\n",
      "Iteration 49188, loss = 1.81838988\n",
      "Iteration 49189, loss = 1.55203658\n",
      "Iteration 49190, loss = 2.19740742\n",
      "Iteration 49191, loss = 2.24396429\n",
      "Iteration 49192, loss = 2.68420913\n",
      "Iteration 49193, loss = 2.49280782\n",
      "Iteration 49194, loss = 1.64090322\n",
      "Iteration 49195, loss = 1.53693996\n",
      "Iteration 49196, loss = 1.60678738\n",
      "Iteration 49197, loss = 1.64141019\n",
      "Iteration 49198, loss = 1.76918330\n",
      "Iteration 49199, loss = 2.00879754\n",
      "Iteration 49200, loss = 2.24061199\n",
      "Iteration 49201, loss = 2.21663222\n",
      "Iteration 49202, loss = 1.78147879\n",
      "Iteration 49203, loss = 2.33206892\n",
      "Iteration 49204, loss = 2.08298213\n",
      "Iteration 49205, loss = 2.21988690\n",
      "Iteration 49206, loss = 1.69569998\n",
      "Iteration 49207, loss = 1.88316563\n",
      "Iteration 49208, loss = 1.84812493\n",
      "Iteration 49209, loss = 2.33125969\n",
      "Iteration 49210, loss = 3.10378681\n",
      "Iteration 49211, loss = 2.46748108\n",
      "Iteration 49212, loss = 3.22778630\n",
      "Iteration 49213, loss = 2.29180042\n",
      "Iteration 49214, loss = 3.38800693\n",
      "Iteration 49215, loss = 4.66545924\n",
      "Iteration 49216, loss = 4.47988814\n",
      "Iteration 49217, loss = 3.95242729\n",
      "Iteration 49218, loss = 4.96918338\n",
      "Iteration 49219, loss = 3.75947451\n",
      "Iteration 49220, loss = 2.26052925\n",
      "Iteration 49221, loss = 2.78805352\n",
      "Iteration 49222, loss = 2.02836911\n",
      "Iteration 49223, loss = 2.22126873\n",
      "Iteration 49224, loss = 1.73470097\n",
      "Iteration 49225, loss = 1.60714198\n",
      "Iteration 49226, loss = 1.68310720\n",
      "Iteration 49227, loss = 1.44551802\n",
      "Iteration 49228, loss = 1.45896877\n",
      "Iteration 49229, loss = 1.49200344\n",
      "Iteration 49230, loss = 1.45729601\n",
      "Iteration 49231, loss = 1.60486239\n",
      "Iteration 49232, loss = 1.46005136\n",
      "Iteration 49233, loss = 1.44001327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 49234, loss = 1.73954289\n",
      "Iteration 49235, loss = 2.04670439\n",
      "Iteration 49236, loss = 1.66188517\n",
      "Iteration 49237, loss = 2.56103026\n",
      "Iteration 49238, loss = 2.38696628\n",
      "Iteration 49239, loss = 2.14633624\n",
      "Iteration 49240, loss = 1.58220177\n",
      "Iteration 49241, loss = 1.49777145\n",
      "Iteration 49242, loss = 1.79316139\n",
      "Iteration 49243, loss = 1.64514938\n",
      "Iteration 49244, loss = 1.60770633\n",
      "Iteration 49245, loss = 1.80020627\n",
      "Iteration 49246, loss = 2.10642435\n",
      "Iteration 49247, loss = 2.26046580\n",
      "Iteration 49248, loss = 2.52702990\n",
      "Iteration 49249, loss = 2.12405905\n",
      "Iteration 49250, loss = 2.06957993\n",
      "Iteration 49251, loss = 1.79597583\n",
      "Iteration 49252, loss = 2.02247744\n",
      "Iteration 49253, loss = 1.78385698\n",
      "Iteration 49254, loss = 1.62108796\n",
      "Iteration 49255, loss = 1.48705023\n",
      "Iteration 49256, loss = 1.60285011\n",
      "Iteration 49257, loss = 1.89877299\n",
      "Iteration 49258, loss = 1.92134128\n",
      "Iteration 49259, loss = 2.01250100\n",
      "Iteration 49260, loss = 2.28739224\n",
      "Iteration 49261, loss = 2.25807170\n",
      "Iteration 49262, loss = 2.04719839\n",
      "Iteration 49263, loss = 1.63240775\n",
      "Iteration 49264, loss = 1.84103221\n",
      "Iteration 49265, loss = 1.61711367\n",
      "Iteration 49266, loss = 1.52262671\n",
      "Iteration 49267, loss = 1.86767171\n",
      "Iteration 49268, loss = 1.66096709\n",
      "Iteration 49269, loss = 1.89843781\n",
      "Iteration 49270, loss = 1.44475271\n",
      "Iteration 49271, loss = 1.50888880\n",
      "Iteration 49272, loss = 1.47615808\n",
      "Iteration 49273, loss = 1.97271872\n",
      "Iteration 49274, loss = 1.83231169\n",
      "Iteration 49275, loss = 1.86614828\n",
      "Iteration 49276, loss = 2.15293048\n",
      "Iteration 49277, loss = 2.11298401\n",
      "Iteration 49278, loss = 2.06719779\n",
      "Iteration 49279, loss = 1.73182329\n",
      "Iteration 49280, loss = 1.98552058\n",
      "Iteration 49281, loss = 1.97853869\n",
      "Iteration 49282, loss = 1.64726739\n",
      "Iteration 49283, loss = 1.34868642\n",
      "Iteration 49284, loss = 1.64104588\n",
      "Iteration 49285, loss = 1.79350243\n",
      "Iteration 49286, loss = 1.65419958\n",
      "Iteration 49287, loss = 1.84532007\n",
      "Iteration 49288, loss = 1.90771111\n",
      "Iteration 49289, loss = 1.62242559\n",
      "Iteration 49290, loss = 1.40729460\n",
      "Iteration 49291, loss = 1.86760891\n",
      "Iteration 49292, loss = 1.93893664\n",
      "Iteration 49293, loss = 1.57631047\n",
      "Iteration 49294, loss = 1.65919049\n",
      "Iteration 49295, loss = 1.93913102\n",
      "Iteration 49296, loss = 1.77622064\n",
      "Iteration 49297, loss = 1.70888452\n",
      "Iteration 49298, loss = 1.64603831\n",
      "Iteration 49299, loss = 1.90895781\n",
      "Iteration 49300, loss = 1.76244498\n",
      "Iteration 49301, loss = 1.50900256\n",
      "Iteration 49302, loss = 1.42649780\n",
      "Iteration 49303, loss = 1.31850295\n",
      "Iteration 49304, loss = 1.53357557\n",
      "Iteration 49305, loss = 1.42783576\n",
      "Iteration 49306, loss = 1.32174636\n",
      "Iteration 49307, loss = 1.53661053\n",
      "Iteration 49308, loss = 1.75823155\n",
      "Iteration 49309, loss = 1.65867304\n",
      "Iteration 49310, loss = 1.58277830\n",
      "Iteration 49311, loss = 1.70928332\n",
      "Iteration 49312, loss = 1.52514680\n",
      "Iteration 49313, loss = 1.47292499\n",
      "Iteration 49314, loss = 1.44969347\n",
      "Iteration 49315, loss = 1.46377926\n",
      "Iteration 49316, loss = 1.45890952\n",
      "Iteration 49317, loss = 1.60414193\n",
      "Iteration 49318, loss = 2.05252679\n",
      "Iteration 49319, loss = 2.45709631\n",
      "Iteration 49320, loss = 2.12812491\n",
      "Iteration 49321, loss = 1.76383512\n",
      "Iteration 49322, loss = 2.10533034\n",
      "Iteration 49323, loss = 2.33933054\n",
      "Iteration 49324, loss = 3.10626534\n",
      "Iteration 49325, loss = 2.39552640\n",
      "Iteration 49326, loss = 1.88239375\n",
      "Iteration 49327, loss = 1.91063292\n",
      "Iteration 49328, loss = 1.71343909\n",
      "Iteration 49329, loss = 1.67706462\n",
      "Iteration 49330, loss = 1.66160851\n",
      "Iteration 49331, loss = 2.16989674\n",
      "Iteration 49332, loss = 2.21381529\n",
      "Iteration 49333, loss = 1.83652029\n",
      "Iteration 49334, loss = 1.39809078\n",
      "Iteration 49335, loss = 1.57169649\n",
      "Iteration 49336, loss = 1.61027285\n",
      "Iteration 49337, loss = 1.58927919\n",
      "Iteration 49338, loss = 1.44804670\n",
      "Iteration 49339, loss = 1.51182521\n",
      "Iteration 49340, loss = 1.38975593\n",
      "Iteration 49341, loss = 1.41302713\n",
      "Iteration 49342, loss = 1.37099140\n",
      "Iteration 49343, loss = 1.31188485\n",
      "Iteration 49344, loss = 1.31038985\n",
      "Iteration 49345, loss = 1.50849440\n",
      "Iteration 49346, loss = 1.37605012\n",
      "Iteration 49347, loss = 1.32558209\n",
      "Iteration 49348, loss = 1.40173917\n",
      "Iteration 49349, loss = 1.37777460\n",
      "Iteration 49350, loss = 1.36068797\n",
      "Iteration 49351, loss = 1.44739141\n",
      "Iteration 49352, loss = 1.53008978\n",
      "Iteration 49353, loss = 1.48161686\n",
      "Iteration 49354, loss = 1.56923841\n",
      "Iteration 49355, loss = 1.43688810\n",
      "Iteration 49356, loss = 1.40731356\n",
      "Iteration 49357, loss = 1.63651465\n",
      "Iteration 49358, loss = 1.42725210\n",
      "Iteration 49359, loss = 1.66539687\n",
      "Iteration 49360, loss = 1.40274001\n",
      "Iteration 49361, loss = 1.38043048\n",
      "Iteration 49362, loss = 1.38995788\n",
      "Iteration 49363, loss = 1.55930880\n",
      "Iteration 49364, loss = 1.54262345\n",
      "Iteration 49365, loss = 1.56680633\n",
      "Iteration 49366, loss = 1.81467316\n",
      "Iteration 49367, loss = 1.78710402\n",
      "Iteration 49368, loss = 1.63752468\n",
      "Iteration 49369, loss = 1.54154790\n",
      "Iteration 49370, loss = 1.46431399\n",
      "Iteration 49371, loss = 1.41069096\n",
      "Iteration 49372, loss = 1.33828391\n",
      "Iteration 49373, loss = 1.61368940\n",
      "Iteration 49374, loss = 1.94045750\n",
      "Iteration 49375, loss = 1.76280877\n",
      "Iteration 49376, loss = 1.59579443\n",
      "Iteration 49377, loss = 1.55824091\n",
      "Iteration 49378, loss = 2.29268158\n",
      "Iteration 49379, loss = 1.48227340\n",
      "Iteration 49380, loss = 1.72338821\n",
      "Iteration 49381, loss = 1.50252985\n",
      "Iteration 49382, loss = 1.56093497\n",
      "Iteration 49383, loss = 1.50860634\n",
      "Iteration 49384, loss = 1.40171436\n",
      "Iteration 49385, loss = 1.40392834\n",
      "Iteration 49386, loss = 1.54734749\n",
      "Iteration 49387, loss = 1.81514501\n",
      "Iteration 49388, loss = 1.53141865\n",
      "Iteration 49389, loss = 1.61360379\n",
      "Iteration 49390, loss = 2.09054803\n",
      "Iteration 49391, loss = 2.67172743\n",
      "Iteration 49392, loss = 3.16468440\n",
      "Iteration 49393, loss = 2.45461922\n",
      "Iteration 49394, loss = 2.07445383\n",
      "Iteration 49395, loss = 1.98181104\n",
      "Iteration 49396, loss = 2.11800049\n",
      "Iteration 49397, loss = 1.76728618\n",
      "Iteration 49398, loss = 1.97011152\n",
      "Iteration 49399, loss = 2.07702327\n",
      "Iteration 49400, loss = 1.50230267\n",
      "Iteration 49401, loss = 1.44988326\n",
      "Iteration 49402, loss = 1.88576517\n",
      "Iteration 49403, loss = 1.76779297\n",
      "Iteration 49404, loss = 1.90542703\n",
      "Iteration 49405, loss = 1.62261725\n",
      "Iteration 49406, loss = 1.78408478\n",
      "Iteration 49407, loss = 2.11667497\n",
      "Iteration 49408, loss = 2.44872703\n",
      "Iteration 49409, loss = 3.31781104\n",
      "Iteration 49410, loss = 2.29565455\n",
      "Iteration 49411, loss = 2.42446863\n",
      "Iteration 49412, loss = 1.70838896\n",
      "Iteration 49413, loss = 2.32464163\n",
      "Iteration 49414, loss = 2.17437762\n",
      "Iteration 49415, loss = 2.10273260\n",
      "Iteration 49416, loss = 1.89590305\n",
      "Iteration 49417, loss = 2.06218226\n",
      "Iteration 49418, loss = 1.99034750\n",
      "Iteration 49419, loss = 2.21873551\n",
      "Iteration 49420, loss = 1.67264171\n",
      "Iteration 49421, loss = 1.64118200\n",
      "Iteration 49422, loss = 1.85952483\n",
      "Iteration 49423, loss = 1.62941152\n",
      "Iteration 49424, loss = 1.70720989\n",
      "Iteration 49425, loss = 1.49027574\n",
      "Iteration 49426, loss = 1.50776635\n",
      "Iteration 49427, loss = 1.47450257\n",
      "Iteration 49428, loss = 1.64506120\n",
      "Iteration 49429, loss = 2.27068844\n",
      "Iteration 49430, loss = 2.55483879\n",
      "Iteration 49431, loss = 1.75948691\n",
      "Iteration 49432, loss = 1.65637392\n",
      "Iteration 49433, loss = 1.82146258\n",
      "Iteration 49434, loss = 1.60176536\n",
      "Iteration 49435, loss = 2.16738279\n",
      "Iteration 49436, loss = 1.71605799\n",
      "Iteration 49437, loss = 2.17674865\n",
      "Iteration 49438, loss = 1.99748432\n",
      "Iteration 49439, loss = 2.49160742\n",
      "Iteration 49440, loss = 1.87746890\n",
      "Iteration 49441, loss = 1.58807892\n",
      "Iteration 49442, loss = 1.58630918\n",
      "Iteration 49443, loss = 1.78213621\n",
      "Iteration 49444, loss = 2.16375823\n",
      "Iteration 49445, loss = 1.63711802\n",
      "Iteration 49446, loss = 1.64402559\n",
      "Iteration 49447, loss = 1.48764301\n",
      "Iteration 49448, loss = 1.75092136\n",
      "Iteration 49449, loss = 1.71002527\n",
      "Iteration 49450, loss = 1.74678685\n",
      "Iteration 49451, loss = 1.68372374\n",
      "Iteration 49452, loss = 1.75950142\n",
      "Iteration 49453, loss = 1.68508800\n",
      "Iteration 49454, loss = 1.71016766\n",
      "Iteration 49455, loss = 1.80612796\n",
      "Iteration 49456, loss = 1.60063475\n",
      "Iteration 49457, loss = 1.51952405\n",
      "Iteration 49458, loss = 1.53787868\n",
      "Iteration 49459, loss = 1.59177855\n",
      "Iteration 49460, loss = 1.60799713\n",
      "Iteration 49461, loss = 1.42960421\n",
      "Iteration 49462, loss = 1.65989717\n",
      "Iteration 49463, loss = 1.45349600\n",
      "Iteration 49464, loss = 1.74927690\n",
      "Iteration 49465, loss = 1.74424728\n",
      "Iteration 49466, loss = 1.58203096\n",
      "Iteration 49467, loss = 1.67795384\n",
      "Iteration 49468, loss = 1.71991095\n",
      "Iteration 49469, loss = 1.73302833\n",
      "Iteration 49470, loss = 2.07538490\n",
      "Iteration 49471, loss = 1.56210614\n",
      "Iteration 49472, loss = 1.48016671\n",
      "Iteration 49473, loss = 1.79983915\n",
      "Iteration 49474, loss = 1.63780176\n",
      "Iteration 49475, loss = 1.61623346\n",
      "Iteration 49476, loss = 2.16570899\n",
      "Iteration 49477, loss = 2.62424287\n",
      "Iteration 49478, loss = 2.18103617\n",
      "Iteration 49479, loss = 1.80003953\n",
      "Iteration 49480, loss = 1.71411332\n",
      "Iteration 49481, loss = 1.98192545\n",
      "Iteration 49482, loss = 2.19358179\n",
      "Iteration 49483, loss = 2.08419227\n",
      "Iteration 49484, loss = 2.53781248\n",
      "Iteration 49485, loss = 3.28353570\n",
      "Iteration 49486, loss = 2.39228328\n",
      "Iteration 49487, loss = 2.44135875\n",
      "Iteration 49488, loss = 1.61333254\n",
      "Iteration 49489, loss = 1.45693184\n",
      "Iteration 49490, loss = 1.60673635\n",
      "Iteration 49491, loss = 1.68652417\n",
      "Iteration 49492, loss = 1.46216570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 49493, loss = 1.69227909\n",
      "Iteration 49494, loss = 1.33837013\n",
      "Iteration 49495, loss = 1.51450016\n",
      "Iteration 49496, loss = 1.62471518\n",
      "Iteration 49497, loss = 1.58383579\n",
      "Iteration 49498, loss = 1.91995266\n",
      "Iteration 49499, loss = 1.52745724\n",
      "Iteration 49500, loss = 2.14653194\n",
      "Iteration 49501, loss = 1.99826734\n",
      "Iteration 49502, loss = 1.81809843\n",
      "Iteration 49503, loss = 1.71383981\n",
      "Iteration 49504, loss = 1.80353553\n",
      "Iteration 49505, loss = 1.80807028\n",
      "Iteration 49506, loss = 2.08663122\n",
      "Iteration 49507, loss = 1.48270074\n",
      "Iteration 49508, loss = 1.87200112\n",
      "Iteration 49509, loss = 1.89800561\n",
      "Iteration 49510, loss = 1.67703480\n",
      "Iteration 49511, loss = 2.34348671\n",
      "Iteration 49512, loss = 1.99351190\n",
      "Iteration 49513, loss = 1.81247066\n",
      "Iteration 49514, loss = 1.80221562\n",
      "Iteration 49515, loss = 1.58396887\n",
      "Iteration 49516, loss = 1.48391064\n",
      "Iteration 49517, loss = 1.39273053\n",
      "Iteration 49518, loss = 1.66335207\n",
      "Iteration 49519, loss = 1.47458590\n",
      "Iteration 49520, loss = 1.71251624\n",
      "Iteration 49521, loss = 1.60851114\n",
      "Iteration 49522, loss = 1.63165693\n",
      "Iteration 49523, loss = 1.43326313\n",
      "Iteration 49524, loss = 1.65094566\n",
      "Iteration 49525, loss = 1.68769520\n",
      "Iteration 49526, loss = 1.60682720\n",
      "Iteration 49527, loss = 1.93112396\n",
      "Iteration 49528, loss = 1.89151962\n",
      "Iteration 49529, loss = 1.60112119\n",
      "Iteration 49530, loss = 1.71560823\n",
      "Iteration 49531, loss = 1.90161428\n",
      "Iteration 49532, loss = 1.57687730\n",
      "Iteration 49533, loss = 1.88046107\n",
      "Iteration 49534, loss = 1.86465025\n",
      "Iteration 49535, loss = 1.85415128\n",
      "Iteration 49536, loss = 1.47019638\n",
      "Iteration 49537, loss = 1.41060730\n",
      "Iteration 49538, loss = 1.43761677\n",
      "Iteration 49539, loss = 1.31255773\n",
      "Iteration 49540, loss = 1.56151378\n",
      "Iteration 49541, loss = 1.72541175\n",
      "Iteration 49542, loss = 1.78756065\n",
      "Iteration 49543, loss = 1.67160096\n",
      "Iteration 49544, loss = 1.88471032\n",
      "Iteration 49545, loss = 1.62985330\n",
      "Iteration 49546, loss = 1.36918752\n",
      "Iteration 49547, loss = 1.40939440\n",
      "Iteration 49548, loss = 1.66110288\n",
      "Iteration 49549, loss = 1.81910890\n",
      "Iteration 49550, loss = 1.49570967\n",
      "Iteration 49551, loss = 1.48002301\n",
      "Iteration 49552, loss = 1.42417227\n",
      "Iteration 49553, loss = 1.46813212\n",
      "Iteration 49554, loss = 1.65025330\n",
      "Iteration 49555, loss = 1.69419361\n",
      "Iteration 49556, loss = 1.77461994\n",
      "Iteration 49557, loss = 1.75155120\n",
      "Iteration 49558, loss = 1.84272832\n",
      "Iteration 49559, loss = 1.60541583\n",
      "Iteration 49560, loss = 1.55576312\n",
      "Iteration 49561, loss = 1.57333079\n",
      "Iteration 49562, loss = 1.61793975\n",
      "Iteration 49563, loss = 1.36872308\n",
      "Iteration 49564, loss = 1.48868443\n",
      "Iteration 49565, loss = 1.98700065\n",
      "Iteration 49566, loss = 1.83608405\n",
      "Iteration 49567, loss = 1.89383514\n",
      "Iteration 49568, loss = 1.83150318\n",
      "Iteration 49569, loss = 1.98843869\n",
      "Iteration 49570, loss = 1.80217775\n",
      "Iteration 49571, loss = 1.74111045\n",
      "Iteration 49572, loss = 1.91800754\n",
      "Iteration 49573, loss = 2.27167035\n",
      "Iteration 49574, loss = 2.00445287\n",
      "Iteration 49575, loss = 1.81219859\n",
      "Iteration 49576, loss = 2.00294478\n",
      "Iteration 49577, loss = 2.19901861\n",
      "Iteration 49578, loss = 2.43133999\n",
      "Iteration 49579, loss = 2.57473961\n",
      "Iteration 49580, loss = 2.48334092\n",
      "Iteration 49581, loss = 2.22131602\n",
      "Iteration 49582, loss = 2.20875619\n",
      "Iteration 49583, loss = 2.14235346\n",
      "Iteration 49584, loss = 1.72295589\n",
      "Iteration 49585, loss = 1.85949506\n",
      "Iteration 49586, loss = 1.73806844\n",
      "Iteration 49587, loss = 2.03146747\n",
      "Iteration 49588, loss = 2.10631676\n",
      "Iteration 49589, loss = 1.63295554\n",
      "Iteration 49590, loss = 1.74025277\n",
      "Iteration 49591, loss = 1.51480498\n",
      "Iteration 49592, loss = 1.61667306\n",
      "Iteration 49593, loss = 1.41653910\n",
      "Iteration 49594, loss = 1.64388566\n",
      "Iteration 49595, loss = 1.99938027\n",
      "Iteration 49596, loss = 2.14604594\n",
      "Iteration 49597, loss = 3.24880398\n",
      "Iteration 49598, loss = 3.09448545\n",
      "Iteration 49599, loss = 5.09060545\n",
      "Iteration 49600, loss = 3.53931288\n",
      "Iteration 49601, loss = 4.53856086\n",
      "Iteration 49602, loss = 3.97417123\n",
      "Iteration 49603, loss = 2.80713101\n",
      "Iteration 49604, loss = 2.01696220\n",
      "Iteration 49605, loss = 2.37382850\n",
      "Iteration 49606, loss = 1.92658943\n",
      "Iteration 49607, loss = 1.66993052\n",
      "Iteration 49608, loss = 1.66924659\n",
      "Iteration 49609, loss = 1.52712272\n",
      "Iteration 49610, loss = 1.64563732\n",
      "Iteration 49611, loss = 1.52044272\n",
      "Iteration 49612, loss = 1.68738657\n",
      "Iteration 49613, loss = 1.73971033\n",
      "Iteration 49614, loss = 1.79460985\n",
      "Iteration 49615, loss = 1.50051315\n",
      "Iteration 49616, loss = 1.59225358\n",
      "Iteration 49617, loss = 1.51711979\n",
      "Iteration 49618, loss = 1.59684315\n",
      "Iteration 49619, loss = 1.44245783\n",
      "Iteration 49620, loss = 1.81035169\n",
      "Iteration 49621, loss = 1.62758148\n",
      "Iteration 49622, loss = 1.65206751\n",
      "Iteration 49623, loss = 1.36676650\n",
      "Iteration 49624, loss = 1.43963835\n",
      "Iteration 49625, loss = 1.99776507\n",
      "Iteration 49626, loss = 2.22054974\n",
      "Iteration 49627, loss = 1.87909907\n",
      "Iteration 49628, loss = 1.63596899\n",
      "Iteration 49629, loss = 1.50644094\n",
      "Iteration 49630, loss = 1.38384962\n",
      "Iteration 49631, loss = 1.60642287\n",
      "Iteration 49632, loss = 1.58087008\n",
      "Iteration 49633, loss = 1.79009100\n",
      "Iteration 49634, loss = 1.68663305\n",
      "Iteration 49635, loss = 1.29935939\n",
      "Iteration 49636, loss = 1.45108915\n",
      "Iteration 49637, loss = 1.47956885\n",
      "Iteration 49638, loss = 1.33978213\n",
      "Iteration 49639, loss = 1.33374606\n",
      "Iteration 49640, loss = 1.36798067\n",
      "Iteration 49641, loss = 1.38037698\n",
      "Iteration 49642, loss = 1.66563569\n",
      "Iteration 49643, loss = 1.83741392\n",
      "Iteration 49644, loss = 2.05333410\n",
      "Iteration 49645, loss = 1.78217864\n",
      "Iteration 49646, loss = 1.82086539\n",
      "Iteration 49647, loss = 1.79562546\n",
      "Iteration 49648, loss = 1.95785279\n",
      "Iteration 49649, loss = 1.81636520\n",
      "Iteration 49650, loss = 2.34133115\n",
      "Iteration 49651, loss = 2.01198036\n",
      "Iteration 49652, loss = 1.92879010\n",
      "Iteration 49653, loss = 1.70028056\n",
      "Iteration 49654, loss = 1.68095010\n",
      "Iteration 49655, loss = 2.26660447\n",
      "Iteration 49656, loss = 2.72799725\n",
      "Iteration 49657, loss = 2.72784348\n",
      "Iteration 49658, loss = 1.60014681\n",
      "Iteration 49659, loss = 1.74460970\n",
      "Iteration 49660, loss = 1.99442339\n",
      "Iteration 49661, loss = 2.15667573\n",
      "Iteration 49662, loss = 2.69706274\n",
      "Iteration 49663, loss = 2.40413890\n",
      "Iteration 49664, loss = 2.36925337\n",
      "Iteration 49665, loss = 2.20743310\n",
      "Iteration 49666, loss = 1.78526945\n",
      "Iteration 49667, loss = 1.54705519\n",
      "Iteration 49668, loss = 1.68615410\n",
      "Iteration 49669, loss = 1.54243551\n",
      "Iteration 49670, loss = 1.80371259\n",
      "Iteration 49671, loss = 1.80772136\n",
      "Iteration 49672, loss = 1.45174227\n",
      "Iteration 49673, loss = 1.52706832\n",
      "Iteration 49674, loss = 1.28632486\n",
      "Iteration 49675, loss = 1.57880036\n",
      "Iteration 49676, loss = 1.39293594\n",
      "Iteration 49677, loss = 1.35857616\n",
      "Iteration 49678, loss = 1.40951924\n",
      "Iteration 49679, loss = 1.60770932\n",
      "Iteration 49680, loss = 2.33087333\n",
      "Iteration 49681, loss = 2.15647243\n",
      "Iteration 49682, loss = 1.94764405\n",
      "Iteration 49683, loss = 3.16255955\n",
      "Iteration 49684, loss = 3.00553762\n",
      "Iteration 49685, loss = 2.14202930\n",
      "Iteration 49686, loss = 2.82137170\n",
      "Iteration 49687, loss = 2.73276567\n",
      "Iteration 49688, loss = 2.39629513\n",
      "Iteration 49689, loss = 3.34512490\n",
      "Iteration 49690, loss = 2.91036532\n",
      "Iteration 49691, loss = 2.98213133\n",
      "Iteration 49692, loss = 4.12010195\n",
      "Iteration 49693, loss = 3.73634736\n",
      "Iteration 49694, loss = 2.98533573\n",
      "Iteration 49695, loss = 1.82532283\n",
      "Iteration 49696, loss = 2.61475347\n",
      "Iteration 49697, loss = 2.72300293\n",
      "Iteration 49698, loss = 1.95299194\n",
      "Iteration 49699, loss = 2.09965341\n",
      "Iteration 49700, loss = 2.09129720\n",
      "Iteration 49701, loss = 2.03299038\n",
      "Iteration 49702, loss = 2.07536546\n",
      "Iteration 49703, loss = 2.51844563\n",
      "Iteration 49704, loss = 1.89258673\n",
      "Iteration 49705, loss = 1.79170686\n",
      "Iteration 49706, loss = 1.64297438\n",
      "Iteration 49707, loss = 1.74637518\n",
      "Iteration 49708, loss = 1.77978343\n",
      "Iteration 49709, loss = 1.80118017\n",
      "Iteration 49710, loss = 2.34482766\n",
      "Iteration 49711, loss = 1.86604539\n",
      "Iteration 49712, loss = 2.46678366\n",
      "Iteration 49713, loss = 1.76991120\n",
      "Iteration 49714, loss = 1.80411679\n",
      "Iteration 49715, loss = 1.46970752\n",
      "Iteration 49716, loss = 1.38920198\n",
      "Iteration 49717, loss = 1.32114436\n",
      "Iteration 49718, loss = 1.46688362\n",
      "Iteration 49719, loss = 1.55091030\n",
      "Iteration 49720, loss = 1.55990924\n",
      "Iteration 49721, loss = 1.32799798\n",
      "Iteration 49722, loss = 1.39859427\n",
      "Iteration 49723, loss = 1.58891995\n",
      "Iteration 49724, loss = 1.62840226\n",
      "Iteration 49725, loss = 1.97538524\n",
      "Iteration 49726, loss = 2.06797844\n",
      "Iteration 49727, loss = 1.90818284\n",
      "Iteration 49728, loss = 1.78422174\n",
      "Iteration 49729, loss = 1.94109391\n",
      "Iteration 49730, loss = 1.48359563\n",
      "Iteration 49731, loss = 1.60490966\n",
      "Iteration 49732, loss = 1.49751794\n",
      "Iteration 49733, loss = 1.67228330\n",
      "Iteration 49734, loss = 1.75624513\n",
      "Iteration 49735, loss = 1.88619353\n",
      "Iteration 49736, loss = 1.81977130\n",
      "Iteration 49737, loss = 2.63226935\n",
      "Iteration 49738, loss = 2.81449144\n",
      "Iteration 49739, loss = 2.67337117\n",
      "Iteration 49740, loss = 3.23144291\n",
      "Iteration 49741, loss = 2.56630070\n",
      "Iteration 49742, loss = 1.80053993\n",
      "Iteration 49743, loss = 1.44503680\n",
      "Iteration 49744, loss = 1.72921191\n",
      "Iteration 49745, loss = 1.61759805\n",
      "Iteration 49746, loss = 1.90145547\n",
      "Iteration 49747, loss = 1.91811788\n",
      "Iteration 49748, loss = 1.84143779\n",
      "Iteration 49749, loss = 1.87746375\n",
      "Iteration 49750, loss = 2.24638632\n",
      "Iteration 49751, loss = 1.77010910\n",
      "Iteration 49752, loss = 1.63815763\n",
      "Iteration 49753, loss = 1.91652686\n",
      "Iteration 49754, loss = 2.28671083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 49755, loss = 2.15353989\n",
      "Iteration 49756, loss = 2.49989679\n",
      "Iteration 49757, loss = 2.78453327\n",
      "Iteration 49758, loss = 2.69547897\n",
      "Iteration 49759, loss = 2.30921140\n",
      "Iteration 49760, loss = 1.78132897\n",
      "Iteration 49761, loss = 1.74218683\n",
      "Iteration 49762, loss = 1.90649137\n",
      "Iteration 49763, loss = 1.69542854\n",
      "Iteration 49764, loss = 1.60540131\n",
      "Iteration 49765, loss = 1.29978983\n",
      "Iteration 49766, loss = 1.32077161\n",
      "Iteration 49767, loss = 1.39916009\n",
      "Iteration 49768, loss = 1.67871054\n",
      "Iteration 49769, loss = 1.76308297\n",
      "Iteration 49770, loss = 1.65282078\n",
      "Iteration 49771, loss = 1.52540436\n",
      "Iteration 49772, loss = 1.81429685\n",
      "Iteration 49773, loss = 2.72666699\n",
      "Iteration 49774, loss = 2.38446896\n",
      "Iteration 49775, loss = 2.41289412\n",
      "Iteration 49776, loss = 2.10227906\n",
      "Iteration 49777, loss = 1.67955872\n",
      "Iteration 49778, loss = 1.66857814\n",
      "Iteration 49779, loss = 1.76650659\n",
      "Iteration 49780, loss = 1.93531822\n",
      "Iteration 49781, loss = 3.01950194\n",
      "Iteration 49782, loss = 3.51932718\n",
      "Iteration 49783, loss = 2.10994410\n",
      "Iteration 49784, loss = 1.81365013\n",
      "Iteration 49785, loss = 1.79804051\n",
      "Iteration 49786, loss = 2.01447736\n",
      "Iteration 49787, loss = 2.72572100\n",
      "Iteration 49788, loss = 2.48877945\n",
      "Iteration 49789, loss = 3.77296205\n",
      "Iteration 49790, loss = 2.99477308\n",
      "Iteration 49791, loss = 2.29764260\n",
      "Iteration 49792, loss = 2.44697233\n",
      "Iteration 49793, loss = 2.21449281\n",
      "Iteration 49794, loss = 1.77725931\n",
      "Iteration 49795, loss = 1.68809685\n",
      "Iteration 49796, loss = 1.69032370\n",
      "Iteration 49797, loss = 1.45848322\n",
      "Iteration 49798, loss = 1.39799578\n",
      "Iteration 49799, loss = 1.58004061\n",
      "Iteration 49800, loss = 1.75988511\n",
      "Iteration 49801, loss = 1.56594377\n",
      "Iteration 49802, loss = 1.80582444\n",
      "Iteration 49803, loss = 1.39787479\n",
      "Iteration 49804, loss = 1.50740875\n",
      "Iteration 49805, loss = 1.52249547\n",
      "Iteration 49806, loss = 1.78746351\n",
      "Iteration 49807, loss = 1.54070283\n",
      "Iteration 49808, loss = 1.66880763\n",
      "Iteration 49809, loss = 1.68061886\n",
      "Iteration 49810, loss = 1.62220531\n",
      "Iteration 49811, loss = 1.96126199\n",
      "Iteration 49812, loss = 2.05282901\n",
      "Iteration 49813, loss = 1.66997380\n",
      "Iteration 49814, loss = 1.89181643\n",
      "Iteration 49815, loss = 1.75018835\n",
      "Iteration 49816, loss = 1.72382260\n",
      "Iteration 49817, loss = 1.56815333\n",
      "Iteration 49818, loss = 1.51719201\n",
      "Iteration 49819, loss = 1.46233361\n",
      "Iteration 49820, loss = 1.38368705\n",
      "Iteration 49821, loss = 1.58002938\n",
      "Iteration 49822, loss = 1.62664365\n",
      "Iteration 49823, loss = 1.32307493\n",
      "Iteration 49824, loss = 1.35076871\n",
      "Iteration 49825, loss = 1.34329580\n",
      "Iteration 49826, loss = 1.42598801\n",
      "Iteration 49827, loss = 1.66878190\n",
      "Iteration 49828, loss = 1.75935918\n",
      "Iteration 49829, loss = 1.89456671\n",
      "Iteration 49830, loss = 1.80114464\n",
      "Iteration 49831, loss = 2.04369944\n",
      "Iteration 49832, loss = 1.57574071\n",
      "Iteration 49833, loss = 1.36490688\n",
      "Iteration 49834, loss = 1.49798020\n",
      "Iteration 49835, loss = 1.45133993\n",
      "Iteration 49836, loss = 1.44356102\n",
      "Iteration 49837, loss = 1.62972610\n",
      "Iteration 49838, loss = 1.58180832\n",
      "Iteration 49839, loss = 1.63656768\n",
      "Iteration 49840, loss = 1.72807622\n",
      "Iteration 49841, loss = 2.01206524\n",
      "Iteration 49842, loss = 2.54566612\n",
      "Iteration 49843, loss = 1.74239055\n",
      "Iteration 49844, loss = 1.84892870\n",
      "Iteration 49845, loss = 1.94256779\n",
      "Iteration 49846, loss = 1.89491300\n",
      "Iteration 49847, loss = 1.91148849\n",
      "Iteration 49848, loss = 1.99895324\n",
      "Iteration 49849, loss = 1.66567784\n",
      "Iteration 49850, loss = 1.48106473\n",
      "Iteration 49851, loss = 1.46154576\n",
      "Iteration 49852, loss = 1.43419044\n",
      "Iteration 49853, loss = 1.58354130\n",
      "Iteration 49854, loss = 1.40029213\n",
      "Iteration 49855, loss = 1.37289976\n",
      "Iteration 49856, loss = 1.58080671\n",
      "Iteration 49857, loss = 1.46130784\n",
      "Iteration 49858, loss = 1.32379385\n",
      "Iteration 49859, loss = 1.46431310\n",
      "Iteration 49860, loss = 1.93090911\n",
      "Iteration 49861, loss = 1.75553815\n",
      "Iteration 49862, loss = 1.94885839\n",
      "Iteration 49863, loss = 2.41797148\n",
      "Iteration 49864, loss = 1.76278702\n",
      "Iteration 49865, loss = 1.66959756\n",
      "Iteration 49866, loss = 1.49759215\n",
      "Iteration 49867, loss = 1.43235488\n",
      "Iteration 49868, loss = 1.38077051\n",
      "Iteration 49869, loss = 1.46696379\n",
      "Iteration 49870, loss = 1.73493973\n",
      "Iteration 49871, loss = 1.62385507\n",
      "Iteration 49872, loss = 2.16175153\n",
      "Iteration 49873, loss = 1.97731303\n",
      "Iteration 49874, loss = 3.11035287\n",
      "Iteration 49875, loss = 3.60715571\n",
      "Iteration 49876, loss = 2.97653474\n",
      "Iteration 49877, loss = 3.63073020\n",
      "Iteration 49878, loss = 3.18869521\n",
      "Iteration 49879, loss = 2.96833812\n",
      "Iteration 49880, loss = 2.34050670\n",
      "Iteration 49881, loss = 2.62936660\n",
      "Iteration 49882, loss = 2.13666700\n",
      "Iteration 49883, loss = 1.78785778\n",
      "Iteration 49884, loss = 1.78548336\n",
      "Iteration 49885, loss = 1.58500039\n",
      "Iteration 49886, loss = 1.52746014\n",
      "Iteration 49887, loss = 1.66388515\n",
      "Iteration 49888, loss = 1.55214640\n",
      "Iteration 49889, loss = 1.53310233\n",
      "Iteration 49890, loss = 1.41090893\n",
      "Iteration 49891, loss = 1.54976290\n",
      "Iteration 49892, loss = 1.56677871\n",
      "Iteration 49893, loss = 1.62620254\n",
      "Iteration 49894, loss = 1.91437664\n",
      "Iteration 49895, loss = 1.64851254\n",
      "Iteration 49896, loss = 1.58311948\n",
      "Iteration 49897, loss = 1.49731035\n",
      "Iteration 49898, loss = 1.70767736\n",
      "Iteration 49899, loss = 1.73456992\n",
      "Iteration 49900, loss = 1.97005573\n",
      "Iteration 49901, loss = 1.81727524\n",
      "Iteration 49902, loss = 2.13394348\n",
      "Iteration 49903, loss = 2.08690696\n",
      "Iteration 49904, loss = 3.24746910\n",
      "Iteration 49905, loss = 3.10237445\n",
      "Iteration 49906, loss = 3.63601187\n",
      "Iteration 49907, loss = 3.78975701\n",
      "Iteration 49908, loss = 3.22412580\n",
      "Iteration 49909, loss = 2.55832091\n",
      "Iteration 49910, loss = 2.45074961\n",
      "Iteration 49911, loss = 2.05538669\n",
      "Iteration 49912, loss = 1.88510207\n",
      "Iteration 49913, loss = 1.55111239\n",
      "Iteration 49914, loss = 1.39828512\n",
      "Iteration 49915, loss = 1.57862539\n",
      "Iteration 49916, loss = 1.92301544\n",
      "Iteration 49917, loss = 2.06912425\n",
      "Iteration 49918, loss = 2.17951094\n",
      "Iteration 49919, loss = 1.65978349\n",
      "Iteration 49920, loss = 1.73980273\n",
      "Iteration 49921, loss = 1.43541460\n",
      "Iteration 49922, loss = 1.35397836\n",
      "Iteration 49923, loss = 1.38861360\n",
      "Iteration 49924, loss = 1.59308810\n",
      "Iteration 49925, loss = 1.76917262\n",
      "Iteration 49926, loss = 2.03733320\n",
      "Iteration 49927, loss = 1.77045921\n",
      "Iteration 49928, loss = 2.01146926\n",
      "Iteration 49929, loss = 1.79642214\n",
      "Iteration 49930, loss = 2.65628471\n",
      "Iteration 49931, loss = 2.16003648\n",
      "Iteration 49932, loss = 1.87374372\n",
      "Iteration 49933, loss = 1.94103270\n",
      "Iteration 49934, loss = 1.60286788\n",
      "Iteration 49935, loss = 1.59654037\n",
      "Iteration 49936, loss = 1.74300125\n",
      "Iteration 49937, loss = 1.71808919\n",
      "Iteration 49938, loss = 1.54563271\n",
      "Iteration 49939, loss = 1.41739290\n",
      "Iteration 49940, loss = 1.56567486\n",
      "Iteration 49941, loss = 1.52833321\n",
      "Iteration 49942, loss = 1.59955746\n",
      "Iteration 49943, loss = 1.54203329\n",
      "Iteration 49944, loss = 1.50601061\n",
      "Iteration 49945, loss = 1.36604702\n",
      "Iteration 49946, loss = 1.30182580\n",
      "Iteration 49947, loss = 1.43540005\n",
      "Iteration 49948, loss = 1.71941827\n",
      "Iteration 49949, loss = 2.06483156\n",
      "Iteration 49950, loss = 2.01237744\n",
      "Iteration 49951, loss = 2.02525395\n",
      "Iteration 49952, loss = 2.90523423\n",
      "Iteration 49953, loss = 2.55987798\n",
      "Iteration 49954, loss = 2.65339016\n",
      "Iteration 49955, loss = 1.82163954\n",
      "Iteration 49956, loss = 1.81934437\n",
      "Iteration 49957, loss = 1.80690662\n",
      "Iteration 49958, loss = 1.63489893\n",
      "Iteration 49959, loss = 1.70020656\n",
      "Iteration 49960, loss = 1.86646449\n",
      "Iteration 49961, loss = 2.01106892\n",
      "Iteration 49962, loss = 1.73507626\n",
      "Iteration 49963, loss = 1.80606545\n",
      "Iteration 49964, loss = 1.83454434\n",
      "Iteration 49965, loss = 1.96321270\n",
      "Iteration 49966, loss = 1.70499387\n",
      "Iteration 49967, loss = 2.51809621\n",
      "Iteration 49968, loss = 1.72070828\n",
      "Iteration 49969, loss = 1.57797080\n",
      "Iteration 49970, loss = 1.73185557\n",
      "Iteration 49971, loss = 1.94599215\n",
      "Iteration 49972, loss = 2.35919356\n",
      "Iteration 49973, loss = 2.47954391\n",
      "Iteration 49974, loss = 1.88409138\n",
      "Iteration 49975, loss = 1.71181899\n",
      "Iteration 49976, loss = 1.75300122\n",
      "Iteration 49977, loss = 1.74722648\n",
      "Iteration 49978, loss = 1.48322851\n",
      "Iteration 49979, loss = 1.74132138\n",
      "Iteration 49980, loss = 1.55560850\n",
      "Iteration 49981, loss = 1.46251499\n",
      "Iteration 49982, loss = 1.34205320\n",
      "Iteration 49983, loss = 1.43023743\n",
      "Iteration 49984, loss = 1.58549892\n",
      "Iteration 49985, loss = 1.59274957\n",
      "Iteration 49986, loss = 1.43233733\n",
      "Iteration 49987, loss = 1.49236238\n",
      "Iteration 49988, loss = 1.56017649\n",
      "Iteration 49989, loss = 1.58597691\n",
      "Iteration 49990, loss = 1.58809113\n",
      "Iteration 49991, loss = 1.67871857\n",
      "Iteration 49992, loss = 1.78115498\n",
      "Iteration 49993, loss = 1.90909535\n",
      "Iteration 49994, loss = 2.03255228\n",
      "Iteration 49995, loss = 1.87452748\n",
      "Iteration 49996, loss = 1.94183501\n",
      "Iteration 49997, loss = 1.89956629\n",
      "Iteration 49998, loss = 1.52768693\n",
      "Iteration 49999, loss = 1.37906188\n",
      "Iteration 50000, loss = 1.48827472\n",
      "Iteration 50001, loss = 1.37396256\n",
      "Iteration 50002, loss = 1.64509780\n",
      "Iteration 50003, loss = 1.55642502\n",
      "Iteration 50004, loss = 1.48351446\n",
      "Iteration 50005, loss = 1.46610464\n",
      "Iteration 50006, loss = 1.71151570\n",
      "Iteration 50007, loss = 1.63561084\n",
      "Iteration 50008, loss = 1.57477235\n",
      "Iteration 50009, loss = 1.70310892\n",
      "Iteration 50010, loss = 1.54995512\n",
      "Iteration 50011, loss = 1.35455661\n",
      "Iteration 50012, loss = 1.39598170\n",
      "Iteration 50013, loss = 1.67793078\n",
      "Iteration 50014, loss = 1.73302644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50015, loss = 1.97504473\n",
      "Iteration 50016, loss = 1.57045352\n",
      "Iteration 50017, loss = 1.52399666\n",
      "Iteration 50018, loss = 1.44356955\n",
      "Iteration 50019, loss = 1.41588573\n",
      "Iteration 50020, loss = 1.54512571\n",
      "Iteration 50021, loss = 1.77086609\n",
      "Iteration 50022, loss = 1.42589425\n",
      "Iteration 50023, loss = 1.66974379\n",
      "Iteration 50024, loss = 1.62254979\n",
      "Iteration 50025, loss = 1.63733424\n",
      "Iteration 50026, loss = 1.68504470\n",
      "Iteration 50027, loss = 1.61785420\n",
      "Iteration 50028, loss = 1.46141702\n",
      "Iteration 50029, loss = 1.47634824\n",
      "Iteration 50030, loss = 1.44189681\n",
      "Iteration 50031, loss = 1.52330816\n",
      "Iteration 50032, loss = 1.82406520\n",
      "Iteration 50033, loss = 1.82343068\n",
      "Iteration 50034, loss = 1.76822894\n",
      "Iteration 50035, loss = 2.08936522\n",
      "Iteration 50036, loss = 1.68950640\n",
      "Iteration 50037, loss = 2.07990252\n",
      "Iteration 50038, loss = 1.73640871\n",
      "Iteration 50039, loss = 2.25224166\n",
      "Iteration 50040, loss = 2.31593197\n",
      "Iteration 50041, loss = 2.54947445\n",
      "Iteration 50042, loss = 2.34913886\n",
      "Iteration 50043, loss = 2.26303202\n",
      "Iteration 50044, loss = 1.96246307\n",
      "Iteration 50045, loss = 2.03060276\n",
      "Iteration 50046, loss = 1.77962350\n",
      "Iteration 50047, loss = 1.84413092\n",
      "Iteration 50048, loss = 1.59116465\n",
      "Iteration 50049, loss = 1.70777342\n",
      "Iteration 50050, loss = 1.88746706\n",
      "Iteration 50051, loss = 1.92468124\n",
      "Iteration 50052, loss = 1.48019191\n",
      "Iteration 50053, loss = 1.61816365\n",
      "Iteration 50054, loss = 1.56089360\n",
      "Iteration 50055, loss = 1.45704489\n",
      "Iteration 50056, loss = 1.63076871\n",
      "Iteration 50057, loss = 1.93064535\n",
      "Iteration 50058, loss = 1.98688037\n",
      "Iteration 50059, loss = 2.10894446\n",
      "Iteration 50060, loss = 2.38569799\n",
      "Iteration 50061, loss = 2.32867294\n",
      "Iteration 50062, loss = 2.15435363\n",
      "Iteration 50063, loss = 2.13353487\n",
      "Iteration 50064, loss = 2.65065923\n",
      "Iteration 50065, loss = 2.32686694\n",
      "Iteration 50066, loss = 2.22469405\n",
      "Iteration 50067, loss = 2.03690478\n",
      "Iteration 50068, loss = 2.20841831\n",
      "Iteration 50069, loss = 2.07120330\n",
      "Iteration 50070, loss = 2.20320783\n",
      "Iteration 50071, loss = 1.90673451\n",
      "Iteration 50072, loss = 1.85664543\n",
      "Iteration 50073, loss = 1.66630793\n",
      "Iteration 50074, loss = 1.91719391\n",
      "Iteration 50075, loss = 2.34333534\n",
      "Iteration 50076, loss = 3.61898158\n",
      "Iteration 50077, loss = 4.12755387\n",
      "Iteration 50078, loss = 3.31689982\n",
      "Iteration 50079, loss = 6.61725105\n",
      "Iteration 50080, loss = 11.90577486\n",
      "Iteration 50081, loss = 17.04504821\n",
      "Iteration 50082, loss = 13.35589950\n",
      "Iteration 50083, loss = 10.37609556\n",
      "Iteration 50084, loss = 9.03697170\n",
      "Iteration 50085, loss = 6.72628140\n",
      "Iteration 50086, loss = 7.35418704\n",
      "Iteration 50087, loss = 7.74549866\n",
      "Iteration 50088, loss = 5.25034711\n",
      "Iteration 50089, loss = 6.33663295\n",
      "Iteration 50090, loss = 4.66163839\n",
      "Iteration 50091, loss = 4.93769022\n",
      "Iteration 50092, loss = 2.98925601\n",
      "Iteration 50093, loss = 3.00780052\n",
      "Iteration 50094, loss = 2.73082750\n",
      "Iteration 50095, loss = 2.16501282\n",
      "Iteration 50096, loss = 2.53364222\n",
      "Iteration 50097, loss = 2.77253390\n",
      "Iteration 50098, loss = 2.00336308\n",
      "Iteration 50099, loss = 1.94517048\n",
      "Iteration 50100, loss = 1.79625184\n",
      "Iteration 50101, loss = 1.92148223\n",
      "Iteration 50102, loss = 1.60559428\n",
      "Iteration 50103, loss = 1.54858675\n",
      "Iteration 50104, loss = 1.57097159\n",
      "Iteration 50105, loss = 1.59361700\n",
      "Iteration 50106, loss = 1.55708086\n",
      "Iteration 50107, loss = 1.37889456\n",
      "Iteration 50108, loss = 1.28040618\n",
      "Iteration 50109, loss = 1.46465612\n",
      "Iteration 50110, loss = 1.39335312\n",
      "Iteration 50111, loss = 1.37245986\n",
      "Iteration 50112, loss = 1.44897428\n",
      "Iteration 50113, loss = 1.53562700\n",
      "Iteration 50114, loss = 1.55638564\n",
      "Iteration 50115, loss = 1.62959475\n",
      "Iteration 50116, loss = 1.63902206\n",
      "Iteration 50117, loss = 1.65152413\n",
      "Iteration 50118, loss = 1.41754054\n",
      "Iteration 50119, loss = 1.88833669\n",
      "Iteration 50120, loss = 2.23228382\n",
      "Iteration 50121, loss = 1.90038331\n",
      "Iteration 50122, loss = 1.81610325\n",
      "Iteration 50123, loss = 1.61398856\n",
      "Iteration 50124, loss = 1.52404103\n",
      "Iteration 50125, loss = 1.45401852\n",
      "Iteration 50126, loss = 1.59915074\n",
      "Iteration 50127, loss = 1.80168293\n",
      "Iteration 50128, loss = 2.08695597\n",
      "Iteration 50129, loss = 1.64455186\n",
      "Iteration 50130, loss = 1.71003505\n",
      "Iteration 50131, loss = 1.52249951\n",
      "Iteration 50132, loss = 1.74773330\n",
      "Iteration 50133, loss = 1.85839662\n",
      "Iteration 50134, loss = 1.57991984\n",
      "Iteration 50135, loss = 1.88753010\n",
      "Iteration 50136, loss = 1.98834969\n",
      "Iteration 50137, loss = 1.88121265\n",
      "Iteration 50138, loss = 1.93854976\n",
      "Iteration 50139, loss = 2.09587263\n",
      "Iteration 50140, loss = 2.25137349\n",
      "Iteration 50141, loss = 2.13828139\n",
      "Iteration 50142, loss = 2.08040624\n",
      "Iteration 50143, loss = 3.04438973\n",
      "Iteration 50144, loss = 2.69470725\n",
      "Iteration 50145, loss = 2.16859606\n",
      "Iteration 50146, loss = 1.97810821\n",
      "Iteration 50147, loss = 1.87463059\n",
      "Iteration 50148, loss = 1.55507998\n",
      "Iteration 50149, loss = 1.86190248\n",
      "Iteration 50150, loss = 1.49467289\n",
      "Iteration 50151, loss = 1.60766857\n",
      "Iteration 50152, loss = 1.68810375\n",
      "Iteration 50153, loss = 1.45797144\n",
      "Iteration 50154, loss = 1.49838687\n",
      "Iteration 50155, loss = 1.55543424\n",
      "Iteration 50156, loss = 1.67411655\n",
      "Iteration 50157, loss = 1.62782225\n",
      "Iteration 50158, loss = 1.64443583\n",
      "Iteration 50159, loss = 1.90251429\n",
      "Iteration 50160, loss = 1.52777662\n",
      "Iteration 50161, loss = 1.58673132\n",
      "Iteration 50162, loss = 1.48143111\n",
      "Iteration 50163, loss = 1.52274911\n",
      "Iteration 50164, loss = 1.58949240\n",
      "Iteration 50165, loss = 1.39091391\n",
      "Iteration 50166, loss = 1.43573399\n",
      "Iteration 50167, loss = 1.49699873\n",
      "Iteration 50168, loss = 1.55950777\n",
      "Iteration 50169, loss = 1.71707240\n",
      "Iteration 50170, loss = 1.65885700\n",
      "Iteration 50171, loss = 1.89420549\n",
      "Iteration 50172, loss = 1.77642667\n",
      "Iteration 50173, loss = 1.36797279\n",
      "Iteration 50174, loss = 1.47700889\n",
      "Iteration 50175, loss = 1.32691843\n",
      "Iteration 50176, loss = 1.80907133\n",
      "Iteration 50177, loss = 1.49658012\n",
      "Iteration 50178, loss = 1.58263358\n",
      "Iteration 50179, loss = 1.67385737\n",
      "Iteration 50180, loss = 1.62571041\n",
      "Iteration 50181, loss = 1.32587115\n",
      "Iteration 50182, loss = 1.39319719\n",
      "Iteration 50183, loss = 1.39241960\n",
      "Iteration 50184, loss = 1.48848167\n",
      "Iteration 50185, loss = 1.49293611\n",
      "Iteration 50186, loss = 1.56921726\n",
      "Iteration 50187, loss = 1.55902734\n",
      "Iteration 50188, loss = 1.70898100\n",
      "Iteration 50189, loss = 1.98032831\n",
      "Iteration 50190, loss = 2.01266838\n",
      "Iteration 50191, loss = 1.92766111\n",
      "Iteration 50192, loss = 2.05144460\n",
      "Iteration 50193, loss = 1.63020634\n",
      "Iteration 50194, loss = 1.54099956\n",
      "Iteration 50195, loss = 1.54302564\n",
      "Iteration 50196, loss = 1.40968087\n",
      "Iteration 50197, loss = 1.52273003\n",
      "Iteration 50198, loss = 1.45306103\n",
      "Iteration 50199, loss = 1.63096734\n",
      "Iteration 50200, loss = 1.69391056\n",
      "Iteration 50201, loss = 2.24251938\n",
      "Iteration 50202, loss = 3.58407895\n",
      "Iteration 50203, loss = 3.19443796\n",
      "Iteration 50204, loss = 2.25392979\n",
      "Iteration 50205, loss = 2.15429046\n",
      "Iteration 50206, loss = 2.25210014\n",
      "Iteration 50207, loss = 1.87638460\n",
      "Iteration 50208, loss = 1.64094719\n",
      "Iteration 50209, loss = 1.95337829\n",
      "Iteration 50210, loss = 2.52839687\n",
      "Iteration 50211, loss = 2.07137490\n",
      "Iteration 50212, loss = 2.20089457\n",
      "Iteration 50213, loss = 1.94548196\n",
      "Iteration 50214, loss = 1.73809325\n",
      "Iteration 50215, loss = 1.63720329\n",
      "Iteration 50216, loss = 1.63187663\n",
      "Iteration 50217, loss = 1.48063316\n",
      "Iteration 50218, loss = 1.53556700\n",
      "Iteration 50219, loss = 1.75845661\n",
      "Iteration 50220, loss = 2.12185002\n",
      "Iteration 50221, loss = 2.60453615\n",
      "Iteration 50222, loss = 2.38014279\n",
      "Iteration 50223, loss = 2.11167214\n",
      "Iteration 50224, loss = 2.30909964\n",
      "Iteration 50225, loss = 1.97645376\n",
      "Iteration 50226, loss = 2.07683582\n",
      "Iteration 50227, loss = 2.28982076\n",
      "Iteration 50228, loss = 2.10751944\n",
      "Iteration 50229, loss = 1.79703101\n",
      "Iteration 50230, loss = 1.60438930\n",
      "Iteration 50231, loss = 1.80014086\n",
      "Iteration 50232, loss = 2.10877781\n",
      "Iteration 50233, loss = 2.15817067\n",
      "Iteration 50234, loss = 2.39269681\n",
      "Iteration 50235, loss = 1.99895378\n",
      "Iteration 50236, loss = 1.58396469\n",
      "Iteration 50237, loss = 1.45505635\n",
      "Iteration 50238, loss = 1.57957797\n",
      "Iteration 50239, loss = 1.47425609\n",
      "Iteration 50240, loss = 1.67131839\n",
      "Iteration 50241, loss = 1.96641770\n",
      "Iteration 50242, loss = 1.74661215\n",
      "Iteration 50243, loss = 1.58259140\n",
      "Iteration 50244, loss = 1.71827855\n",
      "Iteration 50245, loss = 1.51091154\n",
      "Iteration 50246, loss = 1.44561064\n",
      "Iteration 50247, loss = 1.37503143\n",
      "Iteration 50248, loss = 1.47982319\n",
      "Iteration 50249, loss = 1.44217551\n",
      "Iteration 50250, loss = 1.38739969\n",
      "Iteration 50251, loss = 1.43991052\n",
      "Iteration 50252, loss = 1.32606548\n",
      "Iteration 50253, loss = 1.35360192\n",
      "Iteration 50254, loss = 1.51579921\n",
      "Iteration 50255, loss = 1.49407839\n",
      "Iteration 50256, loss = 1.58361041\n",
      "Iteration 50257, loss = 1.65865296\n",
      "Iteration 50258, loss = 1.48632061\n",
      "Iteration 50259, loss = 1.45471396\n",
      "Iteration 50260, loss = 1.61389935\n",
      "Iteration 50261, loss = 1.75726682\n",
      "Iteration 50262, loss = 1.92577520\n",
      "Iteration 50263, loss = 1.60256475\n",
      "Iteration 50264, loss = 1.70391610\n",
      "Iteration 50265, loss = 2.30706424\n",
      "Iteration 50266, loss = 1.99282751\n",
      "Iteration 50267, loss = 1.85617642\n",
      "Iteration 50268, loss = 2.40602962\n",
      "Iteration 50269, loss = 1.90459239\n",
      "Iteration 50270, loss = 2.20956826\n",
      "Iteration 50271, loss = 2.47732695\n",
      "Iteration 50272, loss = 1.87801409\n",
      "Iteration 50273, loss = 1.94177632\n",
      "Iteration 50274, loss = 1.75464433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50275, loss = 1.39548180\n",
      "Iteration 50276, loss = 1.28948767\n",
      "Iteration 50277, loss = 1.38299893\n",
      "Iteration 50278, loss = 1.62707446\n",
      "Iteration 50279, loss = 2.00759160\n",
      "Iteration 50280, loss = 1.72309875\n",
      "Iteration 50281, loss = 2.32339495\n",
      "Iteration 50282, loss = 2.01454042\n",
      "Iteration 50283, loss = 1.71838940\n",
      "Iteration 50284, loss = 1.70677962\n",
      "Iteration 50285, loss = 1.45115214\n",
      "Iteration 50286, loss = 1.52414086\n",
      "Iteration 50287, loss = 1.58178948\n",
      "Iteration 50288, loss = 1.66321887\n",
      "Iteration 50289, loss = 1.56086239\n",
      "Iteration 50290, loss = 1.49892439\n",
      "Iteration 50291, loss = 1.48116346\n",
      "Iteration 50292, loss = 1.42013749\n",
      "Iteration 50293, loss = 1.67392097\n",
      "Iteration 50294, loss = 1.63191826\n",
      "Iteration 50295, loss = 1.73605882\n",
      "Iteration 50296, loss = 1.76655380\n",
      "Iteration 50297, loss = 1.65638334\n",
      "Iteration 50298, loss = 1.95229584\n",
      "Iteration 50299, loss = 1.66612875\n",
      "Iteration 50300, loss = 1.74658991\n",
      "Iteration 50301, loss = 1.67404937\n",
      "Iteration 50302, loss = 1.80710941\n",
      "Iteration 50303, loss = 1.98098127\n",
      "Iteration 50304, loss = 1.83381365\n",
      "Iteration 50305, loss = 1.61218654\n",
      "Iteration 50306, loss = 1.63696948\n",
      "Iteration 50307, loss = 1.73757426\n",
      "Iteration 50308, loss = 1.60435628\n",
      "Iteration 50309, loss = 1.33099937\n",
      "Iteration 50310, loss = 1.58359277\n",
      "Iteration 50311, loss = 1.56263896\n",
      "Iteration 50312, loss = 1.42833273\n",
      "Iteration 50313, loss = 1.66744492\n",
      "Iteration 50314, loss = 3.20270681\n",
      "Iteration 50315, loss = 3.38881887\n",
      "Iteration 50316, loss = 3.73399810\n",
      "Iteration 50317, loss = 3.95168575\n",
      "Iteration 50318, loss = 3.12559492\n",
      "Iteration 50319, loss = 2.69073809\n",
      "Iteration 50320, loss = 1.81031608\n",
      "Iteration 50321, loss = 2.13057913\n",
      "Iteration 50322, loss = 1.86650872\n",
      "Iteration 50323, loss = 1.86794081\n",
      "Iteration 50324, loss = 1.69753339\n",
      "Iteration 50325, loss = 1.77493645\n",
      "Iteration 50326, loss = 1.84458184\n",
      "Iteration 50327, loss = 1.76468541\n",
      "Iteration 50328, loss = 1.90857367\n",
      "Iteration 50329, loss = 2.43843710\n",
      "Iteration 50330, loss = 3.31971696\n",
      "Iteration 50331, loss = 2.52868484\n",
      "Iteration 50332, loss = 2.00025926\n",
      "Iteration 50333, loss = 1.90579981\n",
      "Iteration 50334, loss = 2.04161868\n",
      "Iteration 50335, loss = 1.51066711\n",
      "Iteration 50336, loss = 1.36203554\n",
      "Iteration 50337, loss = 1.37032387\n",
      "Iteration 50338, loss = 1.36544966\n",
      "Iteration 50339, loss = 1.33317395\n",
      "Iteration 50340, loss = 1.34183593\n",
      "Iteration 50341, loss = 1.37296478\n",
      "Iteration 50342, loss = 1.64054642\n",
      "Iteration 50343, loss = 1.60625476\n",
      "Iteration 50344, loss = 1.42846318\n",
      "Iteration 50345, loss = 1.45351775\n",
      "Iteration 50346, loss = 1.45594928\n",
      "Iteration 50347, loss = 1.60662989\n",
      "Iteration 50348, loss = 1.40649457\n",
      "Iteration 50349, loss = 1.42689004\n",
      "Iteration 50350, loss = 1.45484078\n",
      "Iteration 50351, loss = 1.40535310\n",
      "Iteration 50352, loss = 1.61312334\n",
      "Iteration 50353, loss = 1.54380203\n",
      "Iteration 50354, loss = 1.52430291\n",
      "Iteration 50355, loss = 1.56673648\n",
      "Iteration 50356, loss = 1.76075663\n",
      "Iteration 50357, loss = 1.80546173\n",
      "Iteration 50358, loss = 1.73701004\n",
      "Iteration 50359, loss = 1.72610889\n",
      "Iteration 50360, loss = 1.47950811\n",
      "Iteration 50361, loss = 1.62424310\n",
      "Iteration 50362, loss = 1.64805405\n",
      "Iteration 50363, loss = 1.57853635\n",
      "Iteration 50364, loss = 1.84094808\n",
      "Iteration 50365, loss = 1.77119160\n",
      "Iteration 50366, loss = 1.45359703\n",
      "Iteration 50367, loss = 1.61235019\n",
      "Iteration 50368, loss = 1.64581134\n",
      "Iteration 50369, loss = 1.51761608\n",
      "Iteration 50370, loss = 2.12331651\n",
      "Iteration 50371, loss = 2.20768235\n",
      "Iteration 50372, loss = 1.98116411\n",
      "Iteration 50373, loss = 1.75223821\n",
      "Iteration 50374, loss = 1.74124551\n",
      "Iteration 50375, loss = 1.74341456\n",
      "Iteration 50376, loss = 1.85803641\n",
      "Iteration 50377, loss = 1.96708430\n",
      "Iteration 50378, loss = 1.65065244\n",
      "Iteration 50379, loss = 1.77985507\n",
      "Iteration 50380, loss = 1.63784027\n",
      "Iteration 50381, loss = 1.73125925\n",
      "Iteration 50382, loss = 1.45573795\n",
      "Iteration 50383, loss = 1.38480890\n",
      "Iteration 50384, loss = 1.47256770\n",
      "Iteration 50385, loss = 1.41000318\n",
      "Iteration 50386, loss = 1.69264671\n",
      "Iteration 50387, loss = 1.48604533\n",
      "Iteration 50388, loss = 1.50323482\n",
      "Iteration 50389, loss = 1.58752849\n",
      "Iteration 50390, loss = 1.87784396\n",
      "Iteration 50391, loss = 1.56589804\n",
      "Iteration 50392, loss = 1.56904067\n",
      "Iteration 50393, loss = 1.55832198\n",
      "Iteration 50394, loss = 1.85038914\n",
      "Iteration 50395, loss = 1.79508003\n",
      "Iteration 50396, loss = 1.41989591\n",
      "Iteration 50397, loss = 1.64808460\n",
      "Iteration 50398, loss = 1.56692993\n",
      "Iteration 50399, loss = 1.59521265\n",
      "Iteration 50400, loss = 1.44704279\n",
      "Iteration 50401, loss = 1.65516486\n",
      "Iteration 50402, loss = 1.65416762\n",
      "Iteration 50403, loss = 1.28995213\n",
      "Iteration 50404, loss = 1.39642650\n",
      "Iteration 50405, loss = 1.41633859\n",
      "Iteration 50406, loss = 1.59435616\n",
      "Iteration 50407, loss = 1.65396788\n",
      "Iteration 50408, loss = 1.75031112\n",
      "Iteration 50409, loss = 1.82270401\n",
      "Iteration 50410, loss = 2.30495707\n",
      "Iteration 50411, loss = 1.95177283\n",
      "Iteration 50412, loss = 1.92019719\n",
      "Iteration 50413, loss = 1.84562742\n",
      "Iteration 50414, loss = 2.34034347\n",
      "Iteration 50415, loss = 1.97239236\n",
      "Iteration 50416, loss = 1.43418945\n",
      "Iteration 50417, loss = 1.57047292\n",
      "Iteration 50418, loss = 1.63096918\n",
      "Iteration 50419, loss = 1.87806371\n",
      "Iteration 50420, loss = 2.40144065\n",
      "Iteration 50421, loss = 1.76967805\n",
      "Iteration 50422, loss = 1.89685977\n",
      "Iteration 50423, loss = 2.01804044\n",
      "Iteration 50424, loss = 1.88714056\n",
      "Iteration 50425, loss = 2.00809869\n",
      "Iteration 50426, loss = 2.23806538\n",
      "Iteration 50427, loss = 1.93076003\n",
      "Iteration 50428, loss = 2.17648494\n",
      "Iteration 50429, loss = 2.27367023\n",
      "Iteration 50430, loss = 1.67787582\n",
      "Iteration 50431, loss = 1.56971076\n",
      "Iteration 50432, loss = 1.63980139\n",
      "Iteration 50433, loss = 1.50833094\n",
      "Iteration 50434, loss = 2.27879193\n",
      "Iteration 50435, loss = 1.62712269\n",
      "Iteration 50436, loss = 1.45961548\n",
      "Iteration 50437, loss = 1.55073108\n",
      "Iteration 50438, loss = 1.73584693\n",
      "Iteration 50439, loss = 1.62408986\n",
      "Iteration 50440, loss = 1.62302294\n",
      "Iteration 50441, loss = 1.39709410\n",
      "Iteration 50442, loss = 1.58965684\n",
      "Iteration 50443, loss = 1.56740330\n",
      "Iteration 50444, loss = 1.37588787\n",
      "Iteration 50445, loss = 1.46962581\n",
      "Iteration 50446, loss = 1.57680200\n",
      "Iteration 50447, loss = 1.58699578\n",
      "Iteration 50448, loss = 1.42737915\n",
      "Iteration 50449, loss = 1.40451057\n",
      "Iteration 50450, loss = 1.32541256\n",
      "Iteration 50451, loss = 1.31029272\n",
      "Iteration 50452, loss = 1.60308081\n",
      "Iteration 50453, loss = 1.61066312\n",
      "Iteration 50454, loss = 1.43983855\n",
      "Iteration 50455, loss = 1.98392050\n",
      "Iteration 50456, loss = 1.95635384\n",
      "Iteration 50457, loss = 1.89385101\n",
      "Iteration 50458, loss = 1.70912079\n",
      "Iteration 50459, loss = 1.68838568\n",
      "Iteration 50460, loss = 1.41173032\n",
      "Iteration 50461, loss = 1.42892350\n",
      "Iteration 50462, loss = 1.35413956\n",
      "Iteration 50463, loss = 1.34287877\n",
      "Iteration 50464, loss = 1.36767272\n",
      "Iteration 50465, loss = 1.36217470\n",
      "Iteration 50466, loss = 1.76182669\n",
      "Iteration 50467, loss = 1.63913489\n",
      "Iteration 50468, loss = 1.65725509\n",
      "Iteration 50469, loss = 1.57295583\n",
      "Iteration 50470, loss = 1.82990328\n",
      "Iteration 50471, loss = 1.76064881\n",
      "Iteration 50472, loss = 1.87449906\n",
      "Iteration 50473, loss = 1.96082844\n",
      "Iteration 50474, loss = 2.08252472\n",
      "Iteration 50475, loss = 1.95267793\n",
      "Iteration 50476, loss = 2.71067468\n",
      "Iteration 50477, loss = 2.49121635\n",
      "Iteration 50478, loss = 2.42963162\n",
      "Iteration 50479, loss = 2.30595743\n",
      "Iteration 50480, loss = 2.57562670\n",
      "Iteration 50481, loss = 2.95867557\n",
      "Iteration 50482, loss = 3.24953719\n",
      "Iteration 50483, loss = 4.29822162\n",
      "Iteration 50484, loss = 3.62847132\n",
      "Iteration 50485, loss = 2.81026457\n",
      "Iteration 50486, loss = 2.29366218\n",
      "Iteration 50487, loss = 2.22605769\n",
      "Iteration 50488, loss = 1.84937874\n",
      "Iteration 50489, loss = 1.44562137\n",
      "Iteration 50490, loss = 1.50762248\n",
      "Iteration 50491, loss = 1.64685638\n",
      "Iteration 50492, loss = 1.72897277\n",
      "Iteration 50493, loss = 1.62913882\n",
      "Iteration 50494, loss = 1.68724369\n",
      "Iteration 50495, loss = 1.57700846\n",
      "Iteration 50496, loss = 1.68538587\n",
      "Iteration 50497, loss = 2.14488249\n",
      "Iteration 50498, loss = 1.77261337\n",
      "Iteration 50499, loss = 1.85618629\n",
      "Iteration 50500, loss = 1.35641154\n",
      "Iteration 50501, loss = 1.33897663\n",
      "Iteration 50502, loss = 1.46207027\n",
      "Iteration 50503, loss = 1.61752456\n",
      "Iteration 50504, loss = 1.33750103\n",
      "Iteration 50505, loss = 1.43911995\n",
      "Iteration 50506, loss = 1.46365087\n",
      "Iteration 50507, loss = 1.49468970\n",
      "Iteration 50508, loss = 1.46711974\n",
      "Iteration 50509, loss = 1.49117030\n",
      "Iteration 50510, loss = 1.35968114\n",
      "Iteration 50511, loss = 1.48755260\n",
      "Iteration 50512, loss = 1.51219670\n",
      "Iteration 50513, loss = 1.66119132\n",
      "Iteration 50514, loss = 1.75585165\n",
      "Iteration 50515, loss = 1.61650131\n",
      "Iteration 50516, loss = 1.46371615\n",
      "Iteration 50517, loss = 1.38961575\n",
      "Iteration 50518, loss = 1.41911288\n",
      "Iteration 50519, loss = 1.49113946\n",
      "Iteration 50520, loss = 1.70444179\n",
      "Iteration 50521, loss = 1.84701028\n",
      "Iteration 50522, loss = 2.33402666\n",
      "Iteration 50523, loss = 1.86765720\n",
      "Iteration 50524, loss = 1.53508055\n",
      "Iteration 50525, loss = 1.30192128\n",
      "Iteration 50526, loss = 1.37255715\n",
      "Iteration 50527, loss = 1.30364891\n",
      "Iteration 50528, loss = 1.51294951\n",
      "Iteration 50529, loss = 1.30924555\n",
      "Iteration 50530, loss = 1.45070478\n",
      "Iteration 50531, loss = 1.48146558\n",
      "Iteration 50532, loss = 2.25153909\n",
      "Iteration 50533, loss = 1.76812376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50534, loss = 2.03429086\n",
      "Iteration 50535, loss = 1.99711924\n",
      "Iteration 50536, loss = 1.58304333\n",
      "Iteration 50537, loss = 1.66908123\n",
      "Iteration 50538, loss = 1.58197207\n",
      "Iteration 50539, loss = 2.10627704\n",
      "Iteration 50540, loss = 1.84883140\n",
      "Iteration 50541, loss = 1.80227934\n",
      "Iteration 50542, loss = 1.51961783\n",
      "Iteration 50543, loss = 2.09244752\n",
      "Iteration 50544, loss = 2.43390414\n",
      "Iteration 50545, loss = 2.17923042\n",
      "Iteration 50546, loss = 3.30019486\n",
      "Iteration 50547, loss = 4.53010068\n",
      "Iteration 50548, loss = 3.52266199\n",
      "Iteration 50549, loss = 2.96203927\n",
      "Iteration 50550, loss = 2.93033425\n",
      "Iteration 50551, loss = 2.37388548\n",
      "Iteration 50552, loss = 1.82090532\n",
      "Iteration 50553, loss = 1.36040244\n",
      "Iteration 50554, loss = 1.37205995\n",
      "Iteration 50555, loss = 1.33795193\n",
      "Iteration 50556, loss = 1.37092425\n",
      "Iteration 50557, loss = 1.41401878\n",
      "Iteration 50558, loss = 1.40120457\n",
      "Iteration 50559, loss = 1.52858912\n",
      "Iteration 50560, loss = 1.67673453\n",
      "Iteration 50561, loss = 1.81363131\n",
      "Iteration 50562, loss = 1.57708009\n",
      "Iteration 50563, loss = 1.88685991\n",
      "Iteration 50564, loss = 1.71343156\n",
      "Iteration 50565, loss = 1.44941121\n",
      "Iteration 50566, loss = 1.51745568\n",
      "Iteration 50567, loss = 1.78998082\n",
      "Iteration 50568, loss = 1.73908500\n",
      "Iteration 50569, loss = 1.74464227\n",
      "Iteration 50570, loss = 2.22609455\n",
      "Iteration 50571, loss = 2.25441272\n",
      "Iteration 50572, loss = 2.28911426\n",
      "Iteration 50573, loss = 2.20114468\n",
      "Iteration 50574, loss = 1.94110139\n",
      "Iteration 50575, loss = 1.89166915\n",
      "Iteration 50576, loss = 1.67992440\n",
      "Iteration 50577, loss = 1.48065513\n",
      "Iteration 50578, loss = 1.44934221\n",
      "Iteration 50579, loss = 1.50381948\n",
      "Iteration 50580, loss = 2.07622961\n",
      "Iteration 50581, loss = 2.70539726\n",
      "Iteration 50582, loss = 2.12280560\n",
      "Iteration 50583, loss = 2.17181343\n",
      "Iteration 50584, loss = 1.99310858\n",
      "Iteration 50585, loss = 2.51552849\n",
      "Iteration 50586, loss = 2.25726047\n",
      "Iteration 50587, loss = 2.49312760\n",
      "Iteration 50588, loss = 1.92127819\n",
      "Iteration 50589, loss = 1.69622819\n",
      "Iteration 50590, loss = 1.71853910\n",
      "Iteration 50591, loss = 1.41970187\n",
      "Iteration 50592, loss = 1.64434817\n",
      "Iteration 50593, loss = 1.82656132\n",
      "Iteration 50594, loss = 1.83665240\n",
      "Iteration 50595, loss = 1.69805934\n",
      "Iteration 50596, loss = 1.66637498\n",
      "Iteration 50597, loss = 1.50536590\n",
      "Iteration 50598, loss = 1.48935447\n",
      "Iteration 50599, loss = 1.44889738\n",
      "Iteration 50600, loss = 1.55437138\n",
      "Iteration 50601, loss = 1.30625736\n",
      "Iteration 50602, loss = 1.38245328\n",
      "Iteration 50603, loss = 1.54204492\n",
      "Iteration 50604, loss = 1.35335844\n",
      "Iteration 50605, loss = 1.71413265\n",
      "Iteration 50606, loss = 1.74582594\n",
      "Iteration 50607, loss = 1.59602991\n",
      "Iteration 50608, loss = 1.43169992\n",
      "Iteration 50609, loss = 1.68201962\n",
      "Iteration 50610, loss = 1.61272741\n",
      "Iteration 50611, loss = 1.65760977\n",
      "Iteration 50612, loss = 1.58903919\n",
      "Iteration 50613, loss = 1.97046360\n",
      "Iteration 50614, loss = 1.53939381\n",
      "Iteration 50615, loss = 1.39645430\n",
      "Iteration 50616, loss = 1.87868801\n",
      "Iteration 50617, loss = 1.78401499\n",
      "Iteration 50618, loss = 1.58561010\n",
      "Iteration 50619, loss = 1.36504985\n",
      "Iteration 50620, loss = 1.41964423\n",
      "Iteration 50621, loss = 1.99041328\n",
      "Iteration 50622, loss = 1.73351303\n",
      "Iteration 50623, loss = 1.51323067\n",
      "Iteration 50624, loss = 1.42713519\n",
      "Iteration 50625, loss = 1.43374595\n",
      "Iteration 50626, loss = 1.59201329\n",
      "Iteration 50627, loss = 1.65809338\n",
      "Iteration 50628, loss = 1.50168398\n",
      "Iteration 50629, loss = 1.49394690\n",
      "Iteration 50630, loss = 1.55880765\n",
      "Iteration 50631, loss = 1.68358746\n",
      "Iteration 50632, loss = 1.69053044\n",
      "Iteration 50633, loss = 1.46109551\n",
      "Iteration 50634, loss = 1.61763094\n",
      "Iteration 50635, loss = 1.71988488\n",
      "Iteration 50636, loss = 1.67579366\n",
      "Iteration 50637, loss = 1.62347724\n",
      "Iteration 50638, loss = 1.40996911\n",
      "Iteration 50639, loss = 1.50233994\n",
      "Iteration 50640, loss = 1.35137577\n",
      "Iteration 50641, loss = 1.33250303\n",
      "Iteration 50642, loss = 1.40780508\n",
      "Iteration 50643, loss = 1.33133271\n",
      "Iteration 50644, loss = 1.43299219\n",
      "Iteration 50645, loss = 1.38614781\n",
      "Iteration 50646, loss = 1.49451252\n",
      "Iteration 50647, loss = 1.68381454\n",
      "Iteration 50648, loss = 1.49807923\n",
      "Iteration 50649, loss = 1.30327805\n",
      "Iteration 50650, loss = 1.44474081\n",
      "Iteration 50651, loss = 1.38581037\n",
      "Iteration 50652, loss = 1.53499947\n",
      "Iteration 50653, loss = 1.56446429\n",
      "Iteration 50654, loss = 1.54001737\n",
      "Iteration 50655, loss = 1.84165182\n",
      "Iteration 50656, loss = 1.78930722\n",
      "Iteration 50657, loss = 1.73966135\n",
      "Iteration 50658, loss = 1.64853038\n",
      "Iteration 50659, loss = 1.60841392\n",
      "Iteration 50660, loss = 2.00538234\n",
      "Iteration 50661, loss = 1.63670504\n",
      "Iteration 50662, loss = 1.66135789\n",
      "Iteration 50663, loss = 2.04358030\n",
      "Iteration 50664, loss = 2.05499488\n",
      "Iteration 50665, loss = 1.70018378\n",
      "Iteration 50666, loss = 1.69235126\n",
      "Iteration 50667, loss = 1.61313187\n",
      "Iteration 50668, loss = 1.62155656\n",
      "Iteration 50669, loss = 1.31294803\n",
      "Iteration 50670, loss = 1.76976605\n",
      "Iteration 50671, loss = 1.75832634\n",
      "Iteration 50672, loss = 1.93414469\n",
      "Iteration 50673, loss = 2.64295904\n",
      "Iteration 50674, loss = 2.63141924\n",
      "Iteration 50675, loss = 2.93268054\n",
      "Iteration 50676, loss = 2.56646723\n",
      "Iteration 50677, loss = 2.33025595\n",
      "Iteration 50678, loss = 1.94444330\n",
      "Iteration 50679, loss = 2.08469569\n",
      "Iteration 50680, loss = 1.54076198\n",
      "Iteration 50681, loss = 1.71878441\n",
      "Iteration 50682, loss = 1.42378623\n",
      "Iteration 50683, loss = 1.55308398\n",
      "Iteration 50684, loss = 1.57874336\n",
      "Iteration 50685, loss = 1.69692178\n",
      "Iteration 50686, loss = 2.00554037\n",
      "Iteration 50687, loss = 1.66891299\n",
      "Iteration 50688, loss = 1.80779897\n",
      "Iteration 50689, loss = 1.46928869\n",
      "Iteration 50690, loss = 1.54541346\n",
      "Iteration 50691, loss = 1.96576937\n",
      "Iteration 50692, loss = 2.24231023\n",
      "Iteration 50693, loss = 1.97384349\n",
      "Iteration 50694, loss = 2.00150653\n",
      "Iteration 50695, loss = 1.72995275\n",
      "Iteration 50696, loss = 1.65919711\n",
      "Iteration 50697, loss = 1.66833644\n",
      "Iteration 50698, loss = 1.60529035\n",
      "Iteration 50699, loss = 1.91588746\n",
      "Iteration 50700, loss = 2.15217552\n",
      "Iteration 50701, loss = 1.85755795\n",
      "Iteration 50702, loss = 1.53902824\n",
      "Iteration 50703, loss = 1.64885562\n",
      "Iteration 50704, loss = 1.73605243\n",
      "Iteration 50705, loss = 1.54589023\n",
      "Iteration 50706, loss = 1.50733107\n",
      "Iteration 50707, loss = 1.47753408\n",
      "Iteration 50708, loss = 1.70798579\n",
      "Iteration 50709, loss = 1.77842399\n",
      "Iteration 50710, loss = 1.82122170\n",
      "Iteration 50711, loss = 1.62205588\n",
      "Iteration 50712, loss = 1.59371289\n",
      "Iteration 50713, loss = 1.91552374\n",
      "Iteration 50714, loss = 1.60828104\n",
      "Iteration 50715, loss = 1.72835698\n",
      "Iteration 50716, loss = 1.71781716\n",
      "Iteration 50717, loss = 1.54672346\n",
      "Iteration 50718, loss = 1.55797223\n",
      "Iteration 50719, loss = 1.52300310\n",
      "Iteration 50720, loss = 1.78485772\n",
      "Iteration 50721, loss = 1.79419938\n",
      "Iteration 50722, loss = 2.16158322\n",
      "Iteration 50723, loss = 1.85945090\n",
      "Iteration 50724, loss = 1.70050052\n",
      "Iteration 50725, loss = 1.74091530\n",
      "Iteration 50726, loss = 1.93152934\n",
      "Iteration 50727, loss = 2.24040002\n",
      "Iteration 50728, loss = 2.15897208\n",
      "Iteration 50729, loss = 1.93032427\n",
      "Iteration 50730, loss = 1.89594324\n",
      "Iteration 50731, loss = 2.20405987\n",
      "Iteration 50732, loss = 1.68147926\n",
      "Iteration 50733, loss = 2.04116780\n",
      "Iteration 50734, loss = 1.85631623\n",
      "Iteration 50735, loss = 1.87034331\n",
      "Iteration 50736, loss = 2.09110324\n",
      "Iteration 50737, loss = 1.77703541\n",
      "Iteration 50738, loss = 1.97504383\n",
      "Iteration 50739, loss = 1.81979572\n",
      "Iteration 50740, loss = 1.94073385\n",
      "Iteration 50741, loss = 1.46958436\n",
      "Iteration 50742, loss = 1.46573781\n",
      "Iteration 50743, loss = 1.76548184\n",
      "Iteration 50744, loss = 2.09965182\n",
      "Iteration 50745, loss = 1.87523974\n",
      "Iteration 50746, loss = 1.69239949\n",
      "Iteration 50747, loss = 1.48703666\n",
      "Iteration 50748, loss = 1.46492115\n",
      "Iteration 50749, loss = 2.22407083\n",
      "Iteration 50750, loss = 1.94638174\n",
      "Iteration 50751, loss = 3.63080471\n",
      "Iteration 50752, loss = 3.24706283\n",
      "Iteration 50753, loss = 1.91950766\n",
      "Iteration 50754, loss = 1.88716728\n",
      "Iteration 50755, loss = 1.86165014\n",
      "Iteration 50756, loss = 1.78372506\n",
      "Iteration 50757, loss = 1.86590081\n",
      "Iteration 50758, loss = 1.81944772\n",
      "Iteration 50759, loss = 1.62855239\n",
      "Iteration 50760, loss = 1.39622688\n",
      "Iteration 50761, loss = 1.60627046\n",
      "Iteration 50762, loss = 2.15265339\n",
      "Iteration 50763, loss = 2.23021610\n",
      "Iteration 50764, loss = 1.82627652\n",
      "Iteration 50765, loss = 2.13669211\n",
      "Iteration 50766, loss = 1.75746708\n",
      "Iteration 50767, loss = 1.60979808\n",
      "Iteration 50768, loss = 1.89656759\n",
      "Iteration 50769, loss = 1.62322089\n",
      "Iteration 50770, loss = 1.53778944\n",
      "Iteration 50771, loss = 1.57454509\n",
      "Iteration 50772, loss = 1.67490646\n",
      "Iteration 50773, loss = 1.93315054\n",
      "Iteration 50774, loss = 2.25087589\n",
      "Iteration 50775, loss = 2.00406629\n",
      "Iteration 50776, loss = 2.02595198\n",
      "Iteration 50777, loss = 1.99465436\n",
      "Iteration 50778, loss = 1.37267664\n",
      "Iteration 50779, loss = 1.56701772\n",
      "Iteration 50780, loss = 1.52527544\n",
      "Iteration 50781, loss = 1.64750149\n",
      "Iteration 50782, loss = 1.81431273\n",
      "Iteration 50783, loss = 1.97611380\n",
      "Iteration 50784, loss = 2.62700017\n",
      "Iteration 50785, loss = 2.16419907\n",
      "Iteration 50786, loss = 2.62004233\n",
      "Iteration 50787, loss = 2.00220412\n",
      "Iteration 50788, loss = 1.77189260\n",
      "Iteration 50789, loss = 1.84559653\n",
      "Iteration 50790, loss = 2.14123870\n",
      "Iteration 50791, loss = 2.93241584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50792, loss = 3.03413486\n",
      "Iteration 50793, loss = 4.25113329\n",
      "Iteration 50794, loss = 3.14929289\n",
      "Iteration 50795, loss = 2.51635901\n",
      "Iteration 50796, loss = 2.05553779\n",
      "Iteration 50797, loss = 1.85173158\n",
      "Iteration 50798, loss = 2.05183870\n",
      "Iteration 50799, loss = 2.24854629\n",
      "Iteration 50800, loss = 2.15152821\n",
      "Iteration 50801, loss = 2.56557015\n",
      "Iteration 50802, loss = 1.93233048\n",
      "Iteration 50803, loss = 1.53920102\n",
      "Iteration 50804, loss = 1.71618219\n",
      "Iteration 50805, loss = 1.54159238\n",
      "Iteration 50806, loss = 1.53009149\n",
      "Iteration 50807, loss = 1.51045614\n",
      "Iteration 50808, loss = 1.88317222\n",
      "Iteration 50809, loss = 1.58402082\n",
      "Iteration 50810, loss = 1.61755623\n",
      "Iteration 50811, loss = 1.45283494\n",
      "Iteration 50812, loss = 1.29935992\n",
      "Iteration 50813, loss = 1.38276383\n",
      "Iteration 50814, loss = 1.43387164\n",
      "Iteration 50815, loss = 1.44408361\n",
      "Iteration 50816, loss = 1.47492065\n",
      "Iteration 50817, loss = 1.58892382\n",
      "Iteration 50818, loss = 1.63067448\n",
      "Iteration 50819, loss = 1.56875132\n",
      "Iteration 50820, loss = 1.87695834\n",
      "Iteration 50821, loss = 1.78762428\n",
      "Iteration 50822, loss = 1.83566848\n",
      "Iteration 50823, loss = 1.85386420\n",
      "Iteration 50824, loss = 1.50974383\n",
      "Iteration 50825, loss = 1.56010660\n",
      "Iteration 50826, loss = 1.61526800\n",
      "Iteration 50827, loss = 1.55014604\n",
      "Iteration 50828, loss = 1.36200171\n",
      "Iteration 50829, loss = 1.59634092\n",
      "Iteration 50830, loss = 2.26507031\n",
      "Iteration 50831, loss = 1.82151510\n",
      "Iteration 50832, loss = 1.54784215\n",
      "Iteration 50833, loss = 1.34133373\n",
      "Iteration 50834, loss = 1.74727380\n",
      "Iteration 50835, loss = 1.49774292\n",
      "Iteration 50836, loss = 1.81917836\n",
      "Iteration 50837, loss = 1.74385358\n",
      "Iteration 50838, loss = 1.52046024\n",
      "Iteration 50839, loss = 1.51102581\n",
      "Iteration 50840, loss = 1.41152431\n",
      "Iteration 50841, loss = 1.73018144\n",
      "Iteration 50842, loss = 1.51237866\n",
      "Iteration 50843, loss = 1.38203883\n",
      "Iteration 50844, loss = 1.39398288\n",
      "Iteration 50845, loss = 1.41359054\n",
      "Iteration 50846, loss = 1.63799292\n",
      "Iteration 50847, loss = 1.88699700\n",
      "Iteration 50848, loss = 2.05544235\n",
      "Iteration 50849, loss = 1.37167653\n",
      "Iteration 50850, loss = 1.74314552\n",
      "Iteration 50851, loss = 1.67741088\n",
      "Iteration 50852, loss = 2.04168376\n",
      "Iteration 50853, loss = 1.98223833\n",
      "Iteration 50854, loss = 1.70328218\n",
      "Iteration 50855, loss = 1.45576280\n",
      "Iteration 50856, loss = 1.40864301\n",
      "Iteration 50857, loss = 1.45502250\n",
      "Iteration 50858, loss = 1.28964772\n",
      "Iteration 50859, loss = 1.51668170\n",
      "Iteration 50860, loss = 1.60938653\n",
      "Iteration 50861, loss = 2.15045779\n",
      "Iteration 50862, loss = 2.39874499\n",
      "Iteration 50863, loss = 2.09092648\n",
      "Iteration 50864, loss = 1.65775155\n",
      "Iteration 50865, loss = 1.40291956\n",
      "Iteration 50866, loss = 1.29443972\n",
      "Iteration 50867, loss = 1.50972900\n",
      "Iteration 50868, loss = 1.65631913\n",
      "Iteration 50869, loss = 1.52786723\n",
      "Iteration 50870, loss = 1.60925636\n",
      "Iteration 50871, loss = 1.55269787\n",
      "Iteration 50872, loss = 1.50639762\n",
      "Iteration 50873, loss = 1.59902499\n",
      "Iteration 50874, loss = 1.55839842\n",
      "Iteration 50875, loss = 1.99184413\n",
      "Iteration 50876, loss = 1.89836498\n",
      "Iteration 50877, loss = 1.65800866\n",
      "Iteration 50878, loss = 1.71508042\n",
      "Iteration 50879, loss = 1.54876326\n",
      "Iteration 50880, loss = 1.41071024\n",
      "Iteration 50881, loss = 1.36100675\n",
      "Iteration 50882, loss = 1.86790908\n",
      "Iteration 50883, loss = 1.72416492\n",
      "Iteration 50884, loss = 1.71449691\n",
      "Iteration 50885, loss = 2.02687149\n",
      "Iteration 50886, loss = 2.23310628\n",
      "Iteration 50887, loss = 2.14075448\n",
      "Iteration 50888, loss = 2.83817619\n",
      "Iteration 50889, loss = 2.50549476\n",
      "Iteration 50890, loss = 2.14205766\n",
      "Iteration 50891, loss = 2.22992042\n",
      "Iteration 50892, loss = 1.97310438\n",
      "Iteration 50893, loss = 2.06021232\n",
      "Iteration 50894, loss = 3.10442348\n",
      "Iteration 50895, loss = 2.62452932\n",
      "Iteration 50896, loss = 2.19126831\n",
      "Iteration 50897, loss = 3.78107740\n",
      "Iteration 50898, loss = 3.49773937\n",
      "Iteration 50899, loss = 3.88084536\n",
      "Iteration 50900, loss = 3.46822210\n",
      "Iteration 50901, loss = 2.14843347\n",
      "Iteration 50902, loss = 2.24576354\n",
      "Iteration 50903, loss = 1.61204031\n",
      "Iteration 50904, loss = 1.72715559\n",
      "Iteration 50905, loss = 1.70740469\n",
      "Iteration 50906, loss = 1.53945380\n",
      "Iteration 50907, loss = 1.46047299\n",
      "Iteration 50908, loss = 1.39203769\n",
      "Iteration 50909, loss = 1.36365955\n",
      "Iteration 50910, loss = 1.40529397\n",
      "Iteration 50911, loss = 1.60404194\n",
      "Iteration 50912, loss = 1.59204406\n",
      "Iteration 50913, loss = 1.59868867\n",
      "Iteration 50914, loss = 1.50721498\n",
      "Iteration 50915, loss = 1.79446203\n",
      "Iteration 50916, loss = 1.66298127\n",
      "Iteration 50917, loss = 1.73154486\n",
      "Iteration 50918, loss = 1.52821074\n",
      "Iteration 50919, loss = 1.35634218\n",
      "Iteration 50920, loss = 1.43107288\n",
      "Iteration 50921, loss = 2.02747902\n",
      "Iteration 50922, loss = 2.01892608\n",
      "Iteration 50923, loss = 2.33246274\n",
      "Iteration 50924, loss = 2.04295232\n",
      "Iteration 50925, loss = 1.85434429\n",
      "Iteration 50926, loss = 1.57013928\n",
      "Iteration 50927, loss = 1.67859860\n",
      "Iteration 50928, loss = 1.63820701\n",
      "Iteration 50929, loss = 1.45389419\n",
      "Iteration 50930, loss = 1.47776926\n",
      "Iteration 50931, loss = 1.68807341\n",
      "Iteration 50932, loss = 1.50560381\n",
      "Iteration 50933, loss = 1.60599523\n",
      "Iteration 50934, loss = 1.48207580\n",
      "Iteration 50935, loss = 1.48835569\n",
      "Iteration 50936, loss = 1.53480739\n",
      "Iteration 50937, loss = 1.58558802\n",
      "Iteration 50938, loss = 1.86522117\n",
      "Iteration 50939, loss = 1.56996712\n",
      "Iteration 50940, loss = 1.70625835\n",
      "Iteration 50941, loss = 1.60272827\n",
      "Iteration 50942, loss = 1.59782214\n",
      "Iteration 50943, loss = 1.67963950\n",
      "Iteration 50944, loss = 2.11131925\n",
      "Iteration 50945, loss = 2.35326346\n",
      "Iteration 50946, loss = 2.65519878\n",
      "Iteration 50947, loss = 3.65930045\n",
      "Iteration 50948, loss = 3.16016048\n",
      "Iteration 50949, loss = 2.19459221\n",
      "Iteration 50950, loss = 1.87295903\n",
      "Iteration 50951, loss = 1.96185655\n",
      "Iteration 50952, loss = 2.38716883\n",
      "Iteration 50953, loss = 2.36502412\n",
      "Iteration 50954, loss = 1.99939631\n",
      "Iteration 50955, loss = 1.56751535\n",
      "Iteration 50956, loss = 1.53267105\n",
      "Iteration 50957, loss = 1.61758767\n",
      "Iteration 50958, loss = 1.60100187\n",
      "Iteration 50959, loss = 1.39426164\n",
      "Iteration 50960, loss = 1.58208203\n",
      "Iteration 50961, loss = 2.13506292\n",
      "Iteration 50962, loss = 1.63736141\n",
      "Iteration 50963, loss = 1.50187236\n",
      "Iteration 50964, loss = 1.64076341\n",
      "Iteration 50965, loss = 1.54595670\n",
      "Iteration 50966, loss = 1.56600005\n",
      "Iteration 50967, loss = 2.03986865\n",
      "Iteration 50968, loss = 1.80431821\n",
      "Iteration 50969, loss = 2.34322455\n",
      "Iteration 50970, loss = 2.01890810\n",
      "Iteration 50971, loss = 1.95575124\n",
      "Iteration 50972, loss = 1.59294021\n",
      "Iteration 50973, loss = 1.48773390\n",
      "Iteration 50974, loss = 1.46501335\n",
      "Iteration 50975, loss = 1.71895490\n",
      "Iteration 50976, loss = 1.50979599\n",
      "Iteration 50977, loss = 1.66787699\n",
      "Iteration 50978, loss = 1.43609419\n",
      "Iteration 50979, loss = 1.34000137\n",
      "Iteration 50980, loss = 1.53213906\n",
      "Iteration 50981, loss = 1.76442588\n",
      "Iteration 50982, loss = 1.76221530\n",
      "Iteration 50983, loss = 1.42011437\n",
      "Iteration 50984, loss = 1.44819991\n",
      "Iteration 50985, loss = 1.48416271\n",
      "Iteration 50986, loss = 1.38876728\n",
      "Iteration 50987, loss = 1.39078617\n",
      "Iteration 50988, loss = 1.48290731\n",
      "Iteration 50989, loss = 2.36743608\n",
      "Iteration 50990, loss = 2.35943216\n",
      "Iteration 50991, loss = 1.98864816\n",
      "Iteration 50992, loss = 1.83494403\n",
      "Iteration 50993, loss = 2.00127979\n",
      "Iteration 50994, loss = 1.80861886\n",
      "Iteration 50995, loss = 1.90855533\n",
      "Iteration 50996, loss = 2.66711554\n",
      "Iteration 50997, loss = 2.71421026\n",
      "Iteration 50998, loss = 2.65631031\n",
      "Iteration 50999, loss = 1.98698865\n",
      "Iteration 51000, loss = 1.55013715\n",
      "Iteration 51001, loss = 1.52788142\n",
      "Iteration 51002, loss = 1.56451925\n",
      "Iteration 51003, loss = 1.67195709\n",
      "Iteration 51004, loss = 1.59941206\n",
      "Iteration 51005, loss = 1.66110548\n",
      "Iteration 51006, loss = 1.50098737\n",
      "Iteration 51007, loss = 2.39648548\n",
      "Iteration 51008, loss = 2.16156073\n",
      "Iteration 51009, loss = 2.04268862\n",
      "Iteration 51010, loss = 1.91441491\n",
      "Iteration 51011, loss = 1.67870307\n",
      "Iteration 51012, loss = 1.67030386\n",
      "Iteration 51013, loss = 1.62642778\n",
      "Iteration 51014, loss = 1.68099032\n",
      "Iteration 51015, loss = 1.62891659\n",
      "Iteration 51016, loss = 1.36195749\n",
      "Iteration 51017, loss = 1.41290327\n",
      "Iteration 51018, loss = 1.49692857\n",
      "Iteration 51019, loss = 1.54495638\n",
      "Iteration 51020, loss = 1.69880099\n",
      "Iteration 51021, loss = 1.93908464\n",
      "Iteration 51022, loss = 1.92643198\n",
      "Iteration 51023, loss = 1.83561367\n",
      "Iteration 51024, loss = 1.86056852\n",
      "Iteration 51025, loss = 2.32604122\n",
      "Iteration 51026, loss = 2.31062512\n",
      "Iteration 51027, loss = 1.80288519\n",
      "Iteration 51028, loss = 1.86096300\n",
      "Iteration 51029, loss = 1.75146654\n",
      "Iteration 51030, loss = 1.67844798\n",
      "Iteration 51031, loss = 1.46950440\n",
      "Iteration 51032, loss = 1.36826211\n",
      "Iteration 51033, loss = 1.37623103\n",
      "Iteration 51034, loss = 1.45704855\n",
      "Iteration 51035, loss = 1.47555203\n",
      "Iteration 51036, loss = 1.66125355\n",
      "Iteration 51037, loss = 2.01862872\n",
      "Iteration 51038, loss = 2.50081408\n",
      "Iteration 51039, loss = 2.25010684\n",
      "Iteration 51040, loss = 2.26981000\n",
      "Iteration 51041, loss = 3.17842342\n",
      "Iteration 51042, loss = 3.11127280\n",
      "Iteration 51043, loss = 2.52344927\n",
      "Iteration 51044, loss = 2.60187069\n",
      "Iteration 51045, loss = 2.66696045\n",
      "Iteration 51046, loss = 2.03379842\n",
      "Iteration 51047, loss = 1.71937126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 51048, loss = 1.62499092\n",
      "Iteration 51049, loss = 1.41257372\n",
      "Iteration 51050, loss = 1.53653384\n",
      "Iteration 51051, loss = 1.64093288\n",
      "Iteration 51052, loss = 1.54972566\n",
      "Iteration 51053, loss = 1.60818575\n",
      "Iteration 51054, loss = 1.64490914\n",
      "Iteration 51055, loss = 1.53145569\n",
      "Iteration 51056, loss = 1.55440871\n",
      "Iteration 51057, loss = 1.66599715\n",
      "Iteration 51058, loss = 1.84358664\n",
      "Iteration 51059, loss = 1.56240623\n",
      "Iteration 51060, loss = 1.68147098\n",
      "Iteration 51061, loss = 1.43205699\n",
      "Iteration 51062, loss = 1.64361040\n",
      "Iteration 51063, loss = 1.68458925\n",
      "Iteration 51064, loss = 1.51301473\n",
      "Iteration 51065, loss = 1.51597762\n",
      "Iteration 51066, loss = 1.54790245\n",
      "Iteration 51067, loss = 1.82068780\n",
      "Iteration 51068, loss = 1.80786040\n",
      "Iteration 51069, loss = 1.95072807\n",
      "Iteration 51070, loss = 1.81617254\n",
      "Iteration 51071, loss = 1.76062836\n",
      "Iteration 51072, loss = 1.61833654\n",
      "Iteration 51073, loss = 1.72324844\n",
      "Iteration 51074, loss = 1.50012600\n",
      "Iteration 51075, loss = 1.47512142\n",
      "Iteration 51076, loss = 1.57651408\n",
      "Iteration 51077, loss = 1.39065111\n",
      "Iteration 51078, loss = 1.27349586\n",
      "Iteration 51079, loss = 1.32157881\n",
      "Iteration 51080, loss = 1.29702673\n",
      "Iteration 51081, loss = 1.32873576\n",
      "Iteration 51082, loss = 1.41869812\n",
      "Iteration 51083, loss = 1.66762701\n",
      "Iteration 51084, loss = 1.33632734\n",
      "Iteration 51085, loss = 1.56004030\n",
      "Iteration 51086, loss = 1.55759322\n",
      "Iteration 51087, loss = 1.57042337\n",
      "Iteration 51088, loss = 1.65696705\n",
      "Iteration 51089, loss = 1.82645426\n",
      "Iteration 51090, loss = 1.83551202\n",
      "Iteration 51091, loss = 1.93548012\n",
      "Iteration 51092, loss = 2.18731375\n",
      "Iteration 51093, loss = 1.59181120\n",
      "Iteration 51094, loss = 2.06091921\n",
      "Iteration 51095, loss = 2.02895449\n",
      "Iteration 51096, loss = 1.74787102\n",
      "Iteration 51097, loss = 1.70497238\n",
      "Iteration 51098, loss = 1.81288910\n",
      "Iteration 51099, loss = 1.44906158\n",
      "Iteration 51100, loss = 1.60138876\n",
      "Iteration 51101, loss = 1.52304138\n",
      "Iteration 51102, loss = 1.77860665\n",
      "Iteration 51103, loss = 1.93709147\n",
      "Iteration 51104, loss = 2.74752415\n",
      "Iteration 51105, loss = 2.61296364\n",
      "Iteration 51106, loss = 1.85930642\n",
      "Iteration 51107, loss = 1.58145468\n",
      "Iteration 51108, loss = 1.88866766\n",
      "Iteration 51109, loss = 1.80280332\n",
      "Iteration 51110, loss = 1.51475748\n",
      "Iteration 51111, loss = 1.34313692\n",
      "Iteration 51112, loss = 1.32906496\n",
      "Iteration 51113, loss = 1.44533189\n",
      "Iteration 51114, loss = 1.45473088\n",
      "Iteration 51115, loss = 1.39042801\n",
      "Iteration 51116, loss = 1.35000796\n",
      "Iteration 51117, loss = 1.37295905\n",
      "Iteration 51118, loss = 1.47482509\n",
      "Iteration 51119, loss = 1.57848369\n",
      "Iteration 51120, loss = 1.75244096\n",
      "Iteration 51121, loss = 2.34623159\n",
      "Iteration 51122, loss = 1.64136220\n",
      "Iteration 51123, loss = 1.67662340\n",
      "Iteration 51124, loss = 1.72045813\n",
      "Iteration 51125, loss = 1.56675715\n",
      "Iteration 51126, loss = 1.61350757\n",
      "Iteration 51127, loss = 1.70356503\n",
      "Iteration 51128, loss = 1.83695229\n",
      "Iteration 51129, loss = 1.95739202\n",
      "Iteration 51130, loss = 1.81263667\n",
      "Iteration 51131, loss = 1.63554231\n",
      "Iteration 51132, loss = 2.02788422\n",
      "Iteration 51133, loss = 1.83323990\n",
      "Iteration 51134, loss = 1.97984362\n",
      "Iteration 51135, loss = 1.68793182\n",
      "Iteration 51136, loss = 1.43258359\n",
      "Iteration 51137, loss = 1.63551062\n",
      "Iteration 51138, loss = 1.68664781\n",
      "Iteration 51139, loss = 1.76914507\n",
      "Iteration 51140, loss = 1.60721378\n",
      "Iteration 51141, loss = 1.46682486\n",
      "Iteration 51142, loss = 1.45964051\n",
      "Iteration 51143, loss = 1.61235063\n",
      "Iteration 51144, loss = 1.75100237\n",
      "Iteration 51145, loss = 1.76363150\n",
      "Iteration 51146, loss = 1.62691501\n",
      "Iteration 51147, loss = 2.02123354\n",
      "Iteration 51148, loss = 1.56672393\n",
      "Iteration 51149, loss = 1.56259523\n",
      "Iteration 51150, loss = 1.74292009\n",
      "Iteration 51151, loss = 1.66184079\n",
      "Iteration 51152, loss = 1.71708436\n",
      "Iteration 51153, loss = 1.55745923\n",
      "Iteration 51154, loss = 1.36137874\n",
      "Iteration 51155, loss = 1.40424891\n",
      "Iteration 51156, loss = 1.45811617\n",
      "Iteration 51157, loss = 1.68763115\n",
      "Iteration 51158, loss = 1.34393048\n",
      "Iteration 51159, loss = 1.51581995\n",
      "Iteration 51160, loss = 1.39122855\n",
      "Iteration 51161, loss = 1.36609925\n",
      "Iteration 51162, loss = 1.50162743\n",
      "Iteration 51163, loss = 1.37595489\n",
      "Iteration 51164, loss = 1.80056510\n",
      "Iteration 51165, loss = 1.77957802\n",
      "Iteration 51166, loss = 1.62502806\n",
      "Iteration 51167, loss = 1.70943940\n",
      "Iteration 51168, loss = 1.54113771\n",
      "Iteration 51169, loss = 1.70791262\n",
      "Iteration 51170, loss = 1.83550505\n",
      "Iteration 51171, loss = 1.97266665\n",
      "Iteration 51172, loss = 1.60978886\n",
      "Iteration 51173, loss = 1.61226486\n",
      "Iteration 51174, loss = 1.43203442\n",
      "Iteration 51175, loss = 1.45302398\n",
      "Iteration 51176, loss = 1.38423000\n",
      "Iteration 51177, loss = 1.26669040\n",
      "Iteration 51178, loss = 1.36621214\n",
      "Iteration 51179, loss = 1.50021875\n",
      "Iteration 51180, loss = 1.53881456\n",
      "Iteration 51181, loss = 1.40646626\n",
      "Iteration 51182, loss = 1.67595941\n",
      "Iteration 51183, loss = 1.86170275\n",
      "Iteration 51184, loss = 1.91572277\n",
      "Iteration 51185, loss = 1.61210349\n",
      "Iteration 51186, loss = 1.45859916\n",
      "Iteration 51187, loss = 1.47835606\n",
      "Iteration 51188, loss = 1.37375299\n",
      "Iteration 51189, loss = 1.46565030\n",
      "Iteration 51190, loss = 1.58330215\n",
      "Iteration 51191, loss = 1.59107768\n",
      "Iteration 51192, loss = 1.74301035\n",
      "Iteration 51193, loss = 1.74799388\n",
      "Iteration 51194, loss = 1.77471205\n",
      "Iteration 51195, loss = 1.47049334\n",
      "Iteration 51196, loss = 1.41920246\n",
      "Iteration 51197, loss = 1.66639296\n",
      "Iteration 51198, loss = 1.56663304\n",
      "Iteration 51199, loss = 1.49112795\n",
      "Iteration 51200, loss = 1.36618087\n",
      "Iteration 51201, loss = 1.40169709\n",
      "Iteration 51202, loss = 1.37669910\n",
      "Iteration 51203, loss = 1.65924781\n",
      "Iteration 51204, loss = 2.52958169\n",
      "Iteration 51205, loss = 2.12532521\n",
      "Iteration 51206, loss = 2.15802964\n",
      "Iteration 51207, loss = 1.94758362\n",
      "Iteration 51208, loss = 2.18827468\n",
      "Iteration 51209, loss = 2.21715612\n",
      "Iteration 51210, loss = 2.01061979\n",
      "Iteration 51211, loss = 2.26288757\n",
      "Iteration 51212, loss = 2.37219863\n",
      "Iteration 51213, loss = 1.80138610\n",
      "Iteration 51214, loss = 2.19880978\n",
      "Iteration 51215, loss = 2.01502254\n",
      "Iteration 51216, loss = 1.72797650\n",
      "Iteration 51217, loss = 1.46851369\n",
      "Iteration 51218, loss = 1.43346240\n",
      "Iteration 51219, loss = 1.48509318\n",
      "Iteration 51220, loss = 1.44860669\n",
      "Iteration 51221, loss = 1.39616558\n",
      "Iteration 51222, loss = 1.74182084\n",
      "Iteration 51223, loss = 1.90124026\n",
      "Iteration 51224, loss = 1.83936286\n",
      "Iteration 51225, loss = 1.69381721\n",
      "Iteration 51226, loss = 1.97730901\n",
      "Iteration 51227, loss = 1.91435082\n",
      "Iteration 51228, loss = 1.73328444\n",
      "Iteration 51229, loss = 1.67570731\n",
      "Iteration 51230, loss = 1.78708421\n",
      "Iteration 51231, loss = 1.39250357\n",
      "Iteration 51232, loss = 1.48626598\n",
      "Iteration 51233, loss = 1.79505113\n",
      "Iteration 51234, loss = 1.79132995\n",
      "Iteration 51235, loss = 1.62916585\n",
      "Iteration 51236, loss = 2.07600793\n",
      "Iteration 51237, loss = 1.76334854\n",
      "Iteration 51238, loss = 2.18136487\n",
      "Iteration 51239, loss = 1.95820341\n",
      "Iteration 51240, loss = 1.99614141\n",
      "Iteration 51241, loss = 1.84211421\n",
      "Iteration 51242, loss = 1.73548206\n",
      "Iteration 51243, loss = 1.57295123\n",
      "Iteration 51244, loss = 1.92332060\n",
      "Iteration 51245, loss = 2.09851638\n",
      "Iteration 51246, loss = 2.83534373\n",
      "Iteration 51247, loss = 2.05345147\n",
      "Iteration 51248, loss = 2.76460171\n",
      "Iteration 51249, loss = 3.07353068\n",
      "Iteration 51250, loss = 1.87468238\n",
      "Iteration 51251, loss = 1.86022099\n",
      "Iteration 51252, loss = 2.66208083\n",
      "Iteration 51253, loss = 1.96631799\n",
      "Iteration 51254, loss = 1.90455872\n",
      "Iteration 51255, loss = 1.35240887\n",
      "Iteration 51256, loss = 1.98039021\n",
      "Iteration 51257, loss = 1.67699247\n",
      "Iteration 51258, loss = 1.53945548\n",
      "Iteration 51259, loss = 1.77687791\n",
      "Iteration 51260, loss = 1.81396076\n",
      "Iteration 51261, loss = 1.79615996\n",
      "Iteration 51262, loss = 2.20382031\n",
      "Iteration 51263, loss = 2.74165614\n",
      "Iteration 51264, loss = 1.89195330\n",
      "Iteration 51265, loss = 1.97213078\n",
      "Iteration 51266, loss = 2.05696909\n",
      "Iteration 51267, loss = 1.88465414\n",
      "Iteration 51268, loss = 2.00338775\n",
      "Iteration 51269, loss = 1.76113598\n",
      "Iteration 51270, loss = 1.76038500\n",
      "Iteration 51271, loss = 2.15300503\n",
      "Iteration 51272, loss = 2.42472415\n",
      "Iteration 51273, loss = 1.99447011\n",
      "Iteration 51274, loss = 1.67343420\n",
      "Iteration 51275, loss = 1.61672073\n",
      "Iteration 51276, loss = 1.66866391\n",
      "Iteration 51277, loss = 1.59446439\n",
      "Iteration 51278, loss = 1.43148875\n",
      "Iteration 51279, loss = 1.63403272\n",
      "Iteration 51280, loss = 2.23958373\n",
      "Iteration 51281, loss = 1.89394128\n",
      "Iteration 51282, loss = 1.73023323\n",
      "Iteration 51283, loss = 1.82899724\n",
      "Iteration 51284, loss = 2.20439953\n",
      "Iteration 51285, loss = 2.41554416\n",
      "Iteration 51286, loss = 2.11454740\n",
      "Iteration 51287, loss = 1.46829605\n",
      "Iteration 51288, loss = 1.63835920\n",
      "Iteration 51289, loss = 1.61883902\n",
      "Iteration 51290, loss = 1.77896262\n",
      "Iteration 51291, loss = 1.56573728\n",
      "Iteration 51292, loss = 1.52499220\n",
      "Iteration 51293, loss = 1.46006751\n",
      "Iteration 51294, loss = 1.92485442\n",
      "Iteration 51295, loss = 2.23939445\n",
      "Iteration 51296, loss = 1.81944317\n",
      "Iteration 51297, loss = 2.36018002\n",
      "Iteration 51298, loss = 2.57490441\n",
      "Iteration 51299, loss = 1.78674925\n",
      "Iteration 51300, loss = 1.84048384\n",
      "Iteration 51301, loss = 1.80622832\n",
      "Iteration 51302, loss = 1.97570463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 51303, loss = 1.73852462\n",
      "Iteration 51304, loss = 2.01945058\n",
      "Iteration 51305, loss = 1.55601761\n",
      "Iteration 51306, loss = 1.48104248\n",
      "Iteration 51307, loss = 1.86732698\n",
      "Iteration 51308, loss = 1.85179097\n",
      "Iteration 51309, loss = 1.54311505\n",
      "Iteration 51310, loss = 1.55428623\n",
      "Iteration 51311, loss = 1.64922566\n",
      "Iteration 51312, loss = 1.60150219\n",
      "Iteration 51313, loss = 1.66777280\n",
      "Iteration 51314, loss = 1.41102664\n",
      "Iteration 51315, loss = 1.50826519\n",
      "Iteration 51316, loss = 1.77324354\n",
      "Iteration 51317, loss = 1.99721107\n",
      "Iteration 51318, loss = 1.87627132\n",
      "Iteration 51319, loss = 1.56083727\n",
      "Iteration 51320, loss = 1.37113023\n",
      "Iteration 51321, loss = 1.39979055\n",
      "Iteration 51322, loss = 1.49012724\n",
      "Iteration 51323, loss = 1.76161723\n",
      "Iteration 51324, loss = 1.69765909\n",
      "Iteration 51325, loss = 1.87331863\n",
      "Iteration 51326, loss = 1.89525823\n",
      "Iteration 51327, loss = 1.56206555\n",
      "Iteration 51328, loss = 1.70883361\n",
      "Iteration 51329, loss = 1.78591775\n",
      "Iteration 51330, loss = 1.57192944\n",
      "Iteration 51331, loss = 1.57212384\n",
      "Iteration 51332, loss = 1.31869687\n",
      "Iteration 51333, loss = 1.40951630\n",
      "Iteration 51334, loss = 1.53623383\n",
      "Iteration 51335, loss = 1.56681887\n",
      "Iteration 51336, loss = 1.45962639\n",
      "Iteration 51337, loss = 1.54732332\n",
      "Iteration 51338, loss = 1.66548791\n",
      "Iteration 51339, loss = 1.86780057\n",
      "Iteration 51340, loss = 1.66077087\n",
      "Iteration 51341, loss = 2.07655993\n",
      "Iteration 51342, loss = 1.72662779\n",
      "Iteration 51343, loss = 2.42450195\n",
      "Iteration 51344, loss = 2.38049288\n",
      "Iteration 51345, loss = 1.80285962\n",
      "Iteration 51346, loss = 1.51098327\n",
      "Iteration 51347, loss = 1.63805427\n",
      "Iteration 51348, loss = 1.52342254\n",
      "Iteration 51349, loss = 1.28467555\n",
      "Iteration 51350, loss = 1.42232336\n",
      "Iteration 51351, loss = 1.77605836\n",
      "Iteration 51352, loss = 2.11044913\n",
      "Iteration 51353, loss = 2.13529846\n",
      "Iteration 51354, loss = 2.37096739\n",
      "Iteration 51355, loss = 1.79249655\n",
      "Iteration 51356, loss = 1.81529833\n",
      "Iteration 51357, loss = 1.52020177\n",
      "Iteration 51358, loss = 1.43387694\n",
      "Iteration 51359, loss = 1.60565429\n",
      "Iteration 51360, loss = 1.70057205\n",
      "Iteration 51361, loss = 1.49799787\n",
      "Iteration 51362, loss = 1.28906988\n",
      "Iteration 51363, loss = 1.36902607\n",
      "Iteration 51364, loss = 1.47391371\n",
      "Iteration 51365, loss = 1.47446272\n",
      "Iteration 51366, loss = 1.50300260\n",
      "Iteration 51367, loss = 1.76887517\n",
      "Iteration 51368, loss = 1.95048025\n",
      "Iteration 51369, loss = 1.33889025\n",
      "Iteration 51370, loss = 1.65723388\n",
      "Iteration 51371, loss = 1.64970682\n",
      "Iteration 51372, loss = 1.72722382\n",
      "Iteration 51373, loss = 1.71340207\n",
      "Iteration 51374, loss = 1.77549537\n",
      "Iteration 51375, loss = 1.64050966\n",
      "Iteration 51376, loss = 1.54126093\n",
      "Iteration 51377, loss = 1.46264606\n",
      "Iteration 51378, loss = 1.81221955\n",
      "Iteration 51379, loss = 2.16941743\n",
      "Iteration 51380, loss = 1.50343426\n",
      "Iteration 51381, loss = 1.44922012\n",
      "Iteration 51382, loss = 1.55732177\n",
      "Iteration 51383, loss = 1.90781742\n",
      "Iteration 51384, loss = 1.50352137\n",
      "Iteration 51385, loss = 1.83056361\n",
      "Iteration 51386, loss = 1.99755370\n",
      "Iteration 51387, loss = 2.47303054\n",
      "Iteration 51388, loss = 2.31624244\n",
      "Iteration 51389, loss = 2.34534029\n",
      "Iteration 51390, loss = 2.23143079\n",
      "Iteration 51391, loss = 1.68812208\n",
      "Iteration 51392, loss = 1.51440706\n",
      "Iteration 51393, loss = 1.44108833\n",
      "Iteration 51394, loss = 1.29572930\n",
      "Iteration 51395, loss = 1.35462845\n",
      "Iteration 51396, loss = 1.49296045\n",
      "Iteration 51397, loss = 1.41328007\n",
      "Iteration 51398, loss = 1.62653347\n",
      "Iteration 51399, loss = 1.49407405\n",
      "Iteration 51400, loss = 1.38125626\n",
      "Iteration 51401, loss = 1.33468508\n",
      "Iteration 51402, loss = 1.36843373\n",
      "Iteration 51403, loss = 1.51095866\n",
      "Iteration 51404, loss = 1.64358301\n",
      "Iteration 51405, loss = 1.51962985\n",
      "Iteration 51406, loss = 1.63095551\n",
      "Iteration 51407, loss = 1.85404764\n",
      "Iteration 51408, loss = 1.80393571\n",
      "Iteration 51409, loss = 1.77693266\n",
      "Iteration 51410, loss = 1.72519824\n",
      "Iteration 51411, loss = 2.56834594\n",
      "Iteration 51412, loss = 2.20403064\n",
      "Iteration 51413, loss = 2.46156889\n",
      "Iteration 51414, loss = 1.63147082\n",
      "Iteration 51415, loss = 1.45428144\n",
      "Iteration 51416, loss = 1.49963799\n",
      "Iteration 51417, loss = 1.88953112\n",
      "Iteration 51418, loss = 1.82067830\n",
      "Iteration 51419, loss = 1.57929192\n",
      "Iteration 51420, loss = 1.40945820\n",
      "Iteration 51421, loss = 1.39111957\n",
      "Iteration 51422, loss = 1.33320165\n",
      "Iteration 51423, loss = 1.36860022\n",
      "Iteration 51424, loss = 1.28456374\n",
      "Iteration 51425, loss = 1.36082921\n",
      "Iteration 51426, loss = 1.34349743\n",
      "Iteration 51427, loss = 1.38060125\n",
      "Iteration 51428, loss = 1.42037881\n",
      "Iteration 51429, loss = 1.59082564\n",
      "Iteration 51430, loss = 1.39525470\n",
      "Iteration 51431, loss = 1.31335136\n",
      "Iteration 51432, loss = 1.68512314\n",
      "Iteration 51433, loss = 1.91937051\n",
      "Iteration 51434, loss = 2.02571974\n",
      "Iteration 51435, loss = 1.80517315\n",
      "Iteration 51436, loss = 2.67708888\n",
      "Iteration 51437, loss = 2.78833192\n",
      "Iteration 51438, loss = 3.09695245\n",
      "Iteration 51439, loss = 2.67129378\n",
      "Iteration 51440, loss = 3.65925702\n",
      "Iteration 51441, loss = 3.65628436\n",
      "Iteration 51442, loss = 3.96887673\n",
      "Iteration 51443, loss = 4.19167770\n",
      "Iteration 51444, loss = 2.47288422\n",
      "Iteration 51445, loss = 2.64198757\n",
      "Iteration 51446, loss = 2.39947127\n",
      "Iteration 51447, loss = 3.23125633\n",
      "Iteration 51448, loss = 2.13675750\n",
      "Iteration 51449, loss = 1.70848969\n",
      "Iteration 51450, loss = 2.46949306\n",
      "Iteration 51451, loss = 2.41600885\n",
      "Iteration 51452, loss = 2.09765520\n",
      "Iteration 51453, loss = 1.81073199\n",
      "Iteration 51454, loss = 1.90848027\n",
      "Iteration 51455, loss = 1.55382909\n",
      "Iteration 51456, loss = 1.66327397\n",
      "Iteration 51457, loss = 1.36242032\n",
      "Iteration 51458, loss = 1.34125819\n",
      "Iteration 51459, loss = 1.25513990\n",
      "Iteration 51460, loss = 1.27070934\n",
      "Iteration 51461, loss = 1.27807519\n",
      "Iteration 51462, loss = 1.23718292\n",
      "Iteration 51463, loss = 1.29943190\n",
      "Iteration 51464, loss = 1.43136590\n",
      "Iteration 51465, loss = 1.44155748\n",
      "Iteration 51466, loss = 1.38333211\n",
      "Iteration 51467, loss = 1.28257829\n",
      "Iteration 51468, loss = 1.34069147\n",
      "Iteration 51469, loss = 1.62784823\n",
      "Iteration 51470, loss = 1.65455065\n",
      "Iteration 51471, loss = 1.60639951\n",
      "Iteration 51472, loss = 1.98399104\n",
      "Iteration 51473, loss = 1.82867010\n",
      "Iteration 51474, loss = 1.59331932\n",
      "Iteration 51475, loss = 1.28101077\n",
      "Iteration 51476, loss = 1.41537104\n",
      "Iteration 51477, loss = 1.77475786\n",
      "Iteration 51478, loss = 2.30718986\n",
      "Iteration 51479, loss = 2.29298618\n",
      "Iteration 51480, loss = 2.10116399\n",
      "Iteration 51481, loss = 2.06928650\n",
      "Iteration 51482, loss = 2.90675834\n",
      "Iteration 51483, loss = 2.49596024\n",
      "Iteration 51484, loss = 1.97255580\n",
      "Iteration 51485, loss = 1.96049398\n",
      "Iteration 51486, loss = 1.70687110\n",
      "Iteration 51487, loss = 2.17388739\n",
      "Iteration 51488, loss = 2.22647941\n",
      "Iteration 51489, loss = 1.87800628\n",
      "Iteration 51490, loss = 1.85678442\n",
      "Iteration 51491, loss = 1.61880971\n",
      "Iteration 51492, loss = 1.58947708\n",
      "Iteration 51493, loss = 1.30627328\n",
      "Iteration 51494, loss = 1.51311991\n",
      "Iteration 51495, loss = 1.48671869\n",
      "Iteration 51496, loss = 1.51752490\n",
      "Iteration 51497, loss = 1.58189328\n",
      "Iteration 51498, loss = 1.45206057\n",
      "Iteration 51499, loss = 1.94462767\n",
      "Iteration 51500, loss = 2.05394970\n",
      "Iteration 51501, loss = 2.25431293\n",
      "Iteration 51502, loss = 2.12709644\n",
      "Iteration 51503, loss = 1.62274554\n",
      "Iteration 51504, loss = 1.44131566\n",
      "Iteration 51505, loss = 1.59160077\n",
      "Iteration 51506, loss = 2.02247283\n",
      "Iteration 51507, loss = 1.99658672\n",
      "Iteration 51508, loss = 2.00456740\n",
      "Iteration 51509, loss = 1.97447469\n",
      "Iteration 51510, loss = 2.19271033\n",
      "Iteration 51511, loss = 1.81804162\n",
      "Iteration 51512, loss = 1.56653425\n",
      "Iteration 51513, loss = 2.26104323\n",
      "Iteration 51514, loss = 1.88082989\n",
      "Iteration 51515, loss = 1.96695346\n",
      "Iteration 51516, loss = 1.88181740\n",
      "Iteration 51517, loss = 1.72139920\n",
      "Iteration 51518, loss = 1.80601633\n",
      "Iteration 51519, loss = 1.66412304\n",
      "Iteration 51520, loss = 2.33271725\n",
      "Iteration 51521, loss = 1.53070471\n",
      "Iteration 51522, loss = 1.41158054\n",
      "Iteration 51523, loss = 1.35102713\n",
      "Iteration 51524, loss = 1.51108150\n",
      "Iteration 51525, loss = 1.44212374\n",
      "Iteration 51526, loss = 1.54252938\n",
      "Iteration 51527, loss = 1.41719103\n",
      "Iteration 51528, loss = 1.42354611\n",
      "Iteration 51529, loss = 1.42750229\n",
      "Iteration 51530, loss = 1.61106345\n",
      "Iteration 51531, loss = 1.58324488\n",
      "Iteration 51532, loss = 1.33021091\n",
      "Iteration 51533, loss = 1.49617028\n",
      "Iteration 51534, loss = 1.54702112\n",
      "Iteration 51535, loss = 1.52712284\n",
      "Iteration 51536, loss = 1.53282359\n",
      "Iteration 51537, loss = 1.47807233\n",
      "Iteration 51538, loss = 1.73030478\n",
      "Iteration 51539, loss = 1.91902884\n",
      "Iteration 51540, loss = 2.13579011\n",
      "Iteration 51541, loss = 2.45425377\n",
      "Iteration 51542, loss = 2.47823016\n",
      "Iteration 51543, loss = 3.24616484\n",
      "Iteration 51544, loss = 2.59831225\n",
      "Iteration 51545, loss = 2.47278634\n",
      "Iteration 51546, loss = 2.37823526\n",
      "Iteration 51547, loss = 2.29801011\n",
      "Iteration 51548, loss = 1.72070981\n",
      "Iteration 51549, loss = 2.14951193\n",
      "Iteration 51550, loss = 2.08041945\n",
      "Iteration 51551, loss = 2.13803573\n",
      "Iteration 51552, loss = 2.34746263\n",
      "Iteration 51553, loss = 1.68674452\n",
      "Iteration 51554, loss = 1.60619706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 51555, loss = 1.60723439\n",
      "Iteration 51556, loss = 1.67426684\n",
      "Iteration 51557, loss = 1.77810998\n",
      "Iteration 51558, loss = 1.47547910\n",
      "Iteration 51559, loss = 1.47489649\n",
      "Iteration 51560, loss = 1.39636359\n",
      "Iteration 51561, loss = 1.56367880\n",
      "Iteration 51562, loss = 1.58985559\n",
      "Iteration 51563, loss = 1.43597800\n",
      "Iteration 51564, loss = 1.40707299\n",
      "Iteration 51565, loss = 1.79951722\n",
      "Iteration 51566, loss = 1.81163208\n",
      "Iteration 51567, loss = 1.40949368\n",
      "Iteration 51568, loss = 1.40478497\n",
      "Iteration 51569, loss = 1.32789242\n",
      "Iteration 51570, loss = 1.37741829\n",
      "Iteration 51571, loss = 1.33772079\n",
      "Iteration 51572, loss = 1.40084146\n",
      "Iteration 51573, loss = 1.43749236\n",
      "Iteration 51574, loss = 1.49224231\n",
      "Iteration 51575, loss = 1.35393037\n",
      "Iteration 51576, loss = 1.32957114\n",
      "Iteration 51577, loss = 1.37825490\n",
      "Iteration 51578, loss = 1.59485407\n",
      "Iteration 51579, loss = 1.76742842\n",
      "Iteration 51580, loss = 1.92425205\n",
      "Iteration 51581, loss = 1.65491216\n",
      "Iteration 51582, loss = 1.33947155\n",
      "Iteration 51583, loss = 1.31377408\n",
      "Iteration 51584, loss = 1.27769886\n",
      "Iteration 51585, loss = 1.38856219\n",
      "Iteration 51586, loss = 1.53019765\n",
      "Iteration 51587, loss = 1.82373328\n",
      "Iteration 51588, loss = 1.71516635\n",
      "Iteration 51589, loss = 1.90456499\n",
      "Iteration 51590, loss = 1.88993874\n",
      "Iteration 51591, loss = 2.17969624\n",
      "Iteration 51592, loss = 1.48294942\n",
      "Iteration 51593, loss = 1.46812972\n",
      "Iteration 51594, loss = 1.98316532\n",
      "Iteration 51595, loss = 1.82308640\n",
      "Iteration 51596, loss = 2.01055996\n",
      "Iteration 51597, loss = 1.75796815\n",
      "Iteration 51598, loss = 1.96187671\n",
      "Iteration 51599, loss = 1.36713547\n",
      "Iteration 51600, loss = 1.37260663\n",
      "Iteration 51601, loss = 1.45771052\n",
      "Iteration 51602, loss = 1.64409164\n",
      "Iteration 51603, loss = 1.69211680\n",
      "Iteration 51604, loss = 2.13051534\n",
      "Iteration 51605, loss = 1.93941720\n",
      "Iteration 51606, loss = 2.15706193\n",
      "Iteration 51607, loss = 2.04969603\n",
      "Iteration 51608, loss = 2.12969558\n",
      "Iteration 51609, loss = 1.74448118\n",
      "Iteration 51610, loss = 1.67832465\n",
      "Iteration 51611, loss = 1.55399017\n",
      "Iteration 51612, loss = 1.99860369\n",
      "Iteration 51613, loss = 1.56737852\n",
      "Iteration 51614, loss = 1.60954468\n",
      "Iteration 51615, loss = 1.65373496\n",
      "Iteration 51616, loss = 2.10168320\n",
      "Iteration 51617, loss = 2.54302249\n",
      "Iteration 51618, loss = 4.01549197\n",
      "Iteration 51619, loss = 3.52808237\n",
      "Iteration 51620, loss = 2.58838304\n",
      "Iteration 51621, loss = 3.37487241\n",
      "Iteration 51622, loss = 2.42417599\n",
      "Iteration 51623, loss = 2.89628422\n",
      "Iteration 51624, loss = 2.94239415\n",
      "Iteration 51625, loss = 2.51418451\n",
      "Iteration 51626, loss = 2.08150243\n",
      "Iteration 51627, loss = 2.34137354\n",
      "Iteration 51628, loss = 1.87800266\n",
      "Iteration 51629, loss = 1.82511747\n",
      "Iteration 51630, loss = 2.13045234\n",
      "Iteration 51631, loss = 2.23060901\n",
      "Iteration 51632, loss = 1.78618818\n",
      "Iteration 51633, loss = 1.83156629\n",
      "Iteration 51634, loss = 1.56188818\n",
      "Iteration 51635, loss = 1.89684224\n",
      "Iteration 51636, loss = 1.88019505\n",
      "Iteration 51637, loss = 1.60284208\n",
      "Iteration 51638, loss = 1.42997766\n",
      "Iteration 51639, loss = 1.41433724\n",
      "Iteration 51640, loss = 1.45092093\n",
      "Iteration 51641, loss = 1.56310771\n",
      "Iteration 51642, loss = 1.80281279\n",
      "Iteration 51643, loss = 1.50260300\n",
      "Iteration 51644, loss = 1.48269849\n",
      "Iteration 51645, loss = 1.48926273\n",
      "Iteration 51646, loss = 1.37100066\n",
      "Iteration 51647, loss = 1.46791071\n",
      "Iteration 51648, loss = 1.50266439\n",
      "Iteration 51649, loss = 1.79887930\n",
      "Iteration 51650, loss = 1.77797994\n",
      "Iteration 51651, loss = 1.60992236\n",
      "Iteration 51652, loss = 1.56133684\n",
      "Iteration 51653, loss = 1.60839661\n",
      "Iteration 51654, loss = 1.56042526\n",
      "Iteration 51655, loss = 1.97244090\n",
      "Iteration 51656, loss = 2.14912837\n",
      "Iteration 51657, loss = 2.82862594\n",
      "Iteration 51658, loss = 2.91076062\n",
      "Iteration 51659, loss = 4.41004448\n",
      "Iteration 51660, loss = 2.26630583\n",
      "Iteration 51661, loss = 1.99303499\n",
      "Iteration 51662, loss = 1.94417226\n",
      "Iteration 51663, loss = 1.90945335\n",
      "Iteration 51664, loss = 1.76620389\n",
      "Iteration 51665, loss = 1.58445029\n",
      "Iteration 51666, loss = 1.57544657\n",
      "Iteration 51667, loss = 1.51636266\n",
      "Iteration 51668, loss = 1.48924851\n",
      "Iteration 51669, loss = 1.39166768\n",
      "Iteration 51670, loss = 1.53734908\n",
      "Iteration 51671, loss = 1.74301937\n",
      "Iteration 51672, loss = 1.84229875\n",
      "Iteration 51673, loss = 1.56280379\n",
      "Iteration 51674, loss = 1.52654638\n",
      "Iteration 51675, loss = 1.49995638\n",
      "Iteration 51676, loss = 1.90136266\n",
      "Iteration 51677, loss = 1.76970803\n",
      "Iteration 51678, loss = 2.13511072\n",
      "Iteration 51679, loss = 1.88536719\n",
      "Iteration 51680, loss = 1.64655119\n",
      "Iteration 51681, loss = 1.96884101\n",
      "Iteration 51682, loss = 1.90452389\n",
      "Iteration 51683, loss = 1.82869530\n",
      "Iteration 51684, loss = 2.42809171\n",
      "Iteration 51685, loss = 2.70291827\n",
      "Iteration 51686, loss = 4.28549229\n",
      "Iteration 51687, loss = 3.74045357\n",
      "Iteration 51688, loss = 2.85280942\n",
      "Iteration 51689, loss = 2.74683856\n",
      "Iteration 51690, loss = 2.67118251\n",
      "Iteration 51691, loss = 2.04947908\n",
      "Iteration 51692, loss = 2.56765414\n",
      "Iteration 51693, loss = 2.30494636\n",
      "Iteration 51694, loss = 2.00929554\n",
      "Iteration 51695, loss = 1.84297793\n",
      "Iteration 51696, loss = 2.02668800\n",
      "Iteration 51697, loss = 1.60302479\n",
      "Iteration 51698, loss = 1.50203255\n",
      "Iteration 51699, loss = 2.05172143\n",
      "Iteration 51700, loss = 1.88034952\n",
      "Iteration 51701, loss = 2.40314776\n",
      "Iteration 51702, loss = 2.00149026\n",
      "Iteration 51703, loss = 2.62688001\n",
      "Iteration 51704, loss = 3.55638411\n",
      "Iteration 51705, loss = 2.21292074\n",
      "Iteration 51706, loss = 2.97927517\n",
      "Iteration 51707, loss = 2.75313483\n",
      "Iteration 51708, loss = 1.59738285\n",
      "Iteration 51709, loss = 1.61993280\n",
      "Iteration 51710, loss = 1.73788432\n",
      "Iteration 51711, loss = 1.58522251\n",
      "Iteration 51712, loss = 1.69199018\n",
      "Iteration 51713, loss = 1.50261769\n",
      "Iteration 51714, loss = 1.60016332\n",
      "Iteration 51715, loss = 1.60560313\n",
      "Iteration 51716, loss = 1.83918823\n",
      "Iteration 51717, loss = 2.11638104\n",
      "Iteration 51718, loss = 1.81728613\n",
      "Iteration 51719, loss = 2.13775246\n",
      "Iteration 51720, loss = 2.02479167\n",
      "Iteration 51721, loss = 1.79258763\n",
      "Iteration 51722, loss = 1.66841575\n",
      "Iteration 51723, loss = 1.46251846\n",
      "Iteration 51724, loss = 1.36211941\n",
      "Iteration 51725, loss = 1.42289981\n",
      "Iteration 51726, loss = 1.72948315\n",
      "Iteration 51727, loss = 1.83159094\n",
      "Iteration 51728, loss = 1.66776730\n",
      "Iteration 51729, loss = 1.83479168\n",
      "Iteration 51730, loss = 1.66264340\n",
      "Iteration 51731, loss = 1.46328185\n",
      "Iteration 51732, loss = 1.49548129\n",
      "Iteration 51733, loss = 1.42831171\n",
      "Iteration 51734, loss = 1.55910184\n",
      "Iteration 51735, loss = 1.88526710\n",
      "Iteration 51736, loss = 1.72584431\n",
      "Iteration 51737, loss = 1.80731336\n",
      "Iteration 51738, loss = 1.40645919\n",
      "Iteration 51739, loss = 1.63070456\n",
      "Iteration 51740, loss = 1.31450655\n",
      "Iteration 51741, loss = 1.41161435\n",
      "Iteration 51742, loss = 1.48250059\n",
      "Iteration 51743, loss = 1.46202532\n",
      "Iteration 51744, loss = 1.52267892\n",
      "Iteration 51745, loss = 1.56464662\n",
      "Iteration 51746, loss = 1.65437681\n",
      "Iteration 51747, loss = 1.61068998\n",
      "Iteration 51748, loss = 1.34166773\n",
      "Iteration 51749, loss = 1.46176632\n",
      "Iteration 51750, loss = 1.52752923\n",
      "Iteration 51751, loss = 1.33924873\n",
      "Iteration 51752, loss = 1.39762231\n",
      "Iteration 51753, loss = 1.64473060\n",
      "Iteration 51754, loss = 1.69928785\n",
      "Iteration 51755, loss = 1.75555063\n",
      "Iteration 51756, loss = 1.49799210\n",
      "Iteration 51757, loss = 1.56252124\n",
      "Iteration 51758, loss = 1.52761910\n",
      "Iteration 51759, loss = 1.36464200\n",
      "Iteration 51760, loss = 1.29320478\n",
      "Iteration 51761, loss = 1.37218669\n",
      "Iteration 51762, loss = 1.46808732\n",
      "Iteration 51763, loss = 1.35660003\n",
      "Iteration 51764, loss = 1.42065283\n",
      "Iteration 51765, loss = 1.44817652\n",
      "Iteration 51766, loss = 1.64442133\n",
      "Iteration 51767, loss = 1.53147901\n",
      "Iteration 51768, loss = 1.51924525\n",
      "Iteration 51769, loss = 1.62458116\n",
      "Iteration 51770, loss = 1.42474149\n",
      "Iteration 51771, loss = 1.79947170\n",
      "Iteration 51772, loss = 1.76325347\n",
      "Iteration 51773, loss = 1.45762858\n",
      "Iteration 51774, loss = 1.37062305\n",
      "Iteration 51775, loss = 1.47201934\n",
      "Iteration 51776, loss = 1.40066105\n",
      "Iteration 51777, loss = 1.44297494\n",
      "Iteration 51778, loss = 1.71134270\n",
      "Iteration 51779, loss = 1.39404496\n",
      "Iteration 51780, loss = 1.38007229\n",
      "Iteration 51781, loss = 1.27565996\n",
      "Iteration 51782, loss = 1.37798465\n",
      "Iteration 51783, loss = 1.47333101\n",
      "Iteration 51784, loss = 1.72838800\n",
      "Iteration 51785, loss = 1.47325617\n",
      "Iteration 51786, loss = 1.54049815\n",
      "Iteration 51787, loss = 2.06166624\n",
      "Iteration 51788, loss = 2.22610969\n",
      "Iteration 51789, loss = 2.26484807\n",
      "Iteration 51790, loss = 1.86373884\n",
      "Iteration 51791, loss = 1.57840273\n",
      "Iteration 51792, loss = 1.39011128\n",
      "Iteration 51793, loss = 1.71001728\n",
      "Iteration 51794, loss = 1.78344945\n",
      "Iteration 51795, loss = 2.05112064\n",
      "Iteration 51796, loss = 2.07583791\n",
      "Iteration 51797, loss = 1.90585448\n",
      "Iteration 51798, loss = 1.60358220\n",
      "Iteration 51799, loss = 1.64687633\n",
      "Iteration 51800, loss = 1.60094921\n",
      "Iteration 51801, loss = 1.36675624\n",
      "Iteration 51802, loss = 1.47771710\n",
      "Iteration 51803, loss = 1.41469542\n",
      "Iteration 51804, loss = 1.62343989\n",
      "Iteration 51805, loss = 1.51032067\n",
      "Iteration 51806, loss = 1.68632881\n",
      "Iteration 51807, loss = 1.54778190\n",
      "Iteration 51808, loss = 1.44825713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 51809, loss = 1.84442035\n",
      "Iteration 51810, loss = 1.69432893\n",
      "Iteration 51811, loss = 1.87872610\n",
      "Iteration 51812, loss = 1.62959689\n",
      "Iteration 51813, loss = 1.46587872\n",
      "Iteration 51814, loss = 1.57176390\n",
      "Iteration 51815, loss = 1.65298773\n",
      "Iteration 51816, loss = 1.42991345\n",
      "Iteration 51817, loss = 1.48383242\n",
      "Iteration 51818, loss = 1.54816405\n",
      "Iteration 51819, loss = 1.50391939\n",
      "Iteration 51820, loss = 1.46233221\n",
      "Iteration 51821, loss = 1.38005618\n",
      "Iteration 51822, loss = 1.37545498\n",
      "Iteration 51823, loss = 1.55486908\n",
      "Iteration 51824, loss = 1.41815114\n",
      "Iteration 51825, loss = 1.47637826\n",
      "Iteration 51826, loss = 1.61444332\n",
      "Iteration 51827, loss = 1.54091190\n",
      "Iteration 51828, loss = 1.48217968\n",
      "Iteration 51829, loss = 1.71118683\n",
      "Iteration 51830, loss = 1.82391523\n",
      "Iteration 51831, loss = 1.82324895\n",
      "Iteration 51832, loss = 2.33789855\n",
      "Iteration 51833, loss = 1.91839166\n",
      "Iteration 51834, loss = 1.77930617\n",
      "Iteration 51835, loss = 1.75689996\n",
      "Iteration 51836, loss = 1.68860250\n",
      "Iteration 51837, loss = 1.83075396\n",
      "Iteration 51838, loss = 1.87768877\n",
      "Iteration 51839, loss = 1.70488231\n",
      "Iteration 51840, loss = 2.12269525\n",
      "Iteration 51841, loss = 2.18880267\n",
      "Iteration 51842, loss = 1.57434895\n",
      "Iteration 51843, loss = 2.13622117\n",
      "Iteration 51844, loss = 1.96340459\n",
      "Iteration 51845, loss = 2.15508537\n",
      "Iteration 51846, loss = 2.07792049\n",
      "Iteration 51847, loss = 2.12855403\n",
      "Iteration 51848, loss = 2.67287922\n",
      "Iteration 51849, loss = 2.98316375\n",
      "Iteration 51850, loss = 2.86060130\n",
      "Iteration 51851, loss = 2.02550829\n",
      "Iteration 51852, loss = 2.21240306\n",
      "Iteration 51853, loss = 2.38342310\n",
      "Iteration 51854, loss = 2.93558590\n",
      "Iteration 51855, loss = 2.33792582\n",
      "Iteration 51856, loss = 2.35153446\n",
      "Iteration 51857, loss = 2.40250679\n",
      "Iteration 51858, loss = 1.79032867\n",
      "Iteration 51859, loss = 1.96870783\n",
      "Iteration 51860, loss = 2.08332253\n",
      "Iteration 51861, loss = 1.61262304\n",
      "Iteration 51862, loss = 1.73668957\n",
      "Iteration 51863, loss = 1.51294869\n",
      "Iteration 51864, loss = 1.85217811\n",
      "Iteration 51865, loss = 1.85032306\n",
      "Iteration 51866, loss = 1.48572864\n",
      "Iteration 51867, loss = 1.74842770\n",
      "Iteration 51868, loss = 1.54404075\n",
      "Iteration 51869, loss = 1.36917060\n",
      "Iteration 51870, loss = 1.36521177\n",
      "Iteration 51871, loss = 1.42558134\n",
      "Iteration 51872, loss = 1.55839710\n",
      "Iteration 51873, loss = 1.61939397\n",
      "Iteration 51874, loss = 1.89213367\n",
      "Iteration 51875, loss = 1.70181512\n",
      "Iteration 51876, loss = 1.64819428\n",
      "Iteration 51877, loss = 2.05097261\n",
      "Iteration 51878, loss = 2.14982291\n",
      "Iteration 51879, loss = 2.54030827\n",
      "Iteration 51880, loss = 1.86724539\n",
      "Iteration 51881, loss = 1.82322578\n",
      "Iteration 51882, loss = 1.84045293\n",
      "Iteration 51883, loss = 1.95234867\n",
      "Iteration 51884, loss = 1.90171851\n",
      "Iteration 51885, loss = 1.80030428\n",
      "Iteration 51886, loss = 1.42575831\n",
      "Iteration 51887, loss = 1.38398404\n",
      "Iteration 51888, loss = 1.39071546\n",
      "Iteration 51889, loss = 1.63640676\n",
      "Iteration 51890, loss = 2.24582954\n",
      "Iteration 51891, loss = 2.25649128\n",
      "Iteration 51892, loss = 2.14710911\n",
      "Iteration 51893, loss = 2.08087979\n",
      "Iteration 51894, loss = 2.76443067\n",
      "Iteration 51895, loss = 2.14746178\n",
      "Iteration 51896, loss = 2.13673051\n",
      "Iteration 51897, loss = 2.39748377\n",
      "Iteration 51898, loss = 1.98175866\n",
      "Iteration 51899, loss = 1.91454385\n",
      "Iteration 51900, loss = 1.85252524\n",
      "Iteration 51901, loss = 2.05634136\n",
      "Iteration 51902, loss = 2.10743554\n",
      "Iteration 51903, loss = 2.28148305\n",
      "Iteration 51904, loss = 2.63858583\n",
      "Iteration 51905, loss = 2.24658127\n",
      "Iteration 51906, loss = 2.01758697\n",
      "Iteration 51907, loss = 2.45641667\n",
      "Iteration 51908, loss = 2.06057477\n",
      "Iteration 51909, loss = 2.19687407\n",
      "Iteration 51910, loss = 2.45391851\n",
      "Iteration 51911, loss = 2.60367490\n",
      "Iteration 51912, loss = 2.04844919\n",
      "Iteration 51913, loss = 1.77959528\n",
      "Iteration 51914, loss = 2.24029801\n",
      "Iteration 51915, loss = 2.85400206\n",
      "Iteration 51916, loss = 1.83525055\n",
      "Iteration 51917, loss = 1.68584782\n",
      "Iteration 51918, loss = 1.73396481\n",
      "Iteration 51919, loss = 1.45206849\n",
      "Iteration 51920, loss = 1.34488360\n",
      "Iteration 51921, loss = 1.42913929\n",
      "Iteration 51922, loss = 1.56657760\n",
      "Iteration 51923, loss = 1.76606441\n",
      "Iteration 51924, loss = 1.86128339\n",
      "Iteration 51925, loss = 1.97597226\n",
      "Iteration 51926, loss = 1.72826829\n",
      "Iteration 51927, loss = 1.87574150\n",
      "Iteration 51928, loss = 1.71470209\n",
      "Iteration 51929, loss = 1.57598047\n",
      "Iteration 51930, loss = 1.76788135\n",
      "Iteration 51931, loss = 1.93699972\n",
      "Iteration 51932, loss = 1.82839473\n",
      "Iteration 51933, loss = 1.53383818\n",
      "Iteration 51934, loss = 1.94572069\n",
      "Iteration 51935, loss = 1.62551387\n",
      "Iteration 51936, loss = 2.20044890\n",
      "Iteration 51937, loss = 2.38108015\n",
      "Iteration 51938, loss = 2.14477350\n",
      "Iteration 51939, loss = 2.01638528\n",
      "Iteration 51940, loss = 1.81482488\n",
      "Iteration 51941, loss = 1.76353191\n",
      "Iteration 51942, loss = 1.83734949\n",
      "Iteration 51943, loss = 2.37617135\n",
      "Iteration 51944, loss = 2.87582075\n",
      "Iteration 51945, loss = 3.16489291\n",
      "Iteration 51946, loss = 2.12586286\n",
      "Iteration 51947, loss = 2.80873600\n",
      "Iteration 51948, loss = 2.91090633\n",
      "Iteration 51949, loss = 2.88443593\n",
      "Iteration 51950, loss = 2.75639003\n",
      "Iteration 51951, loss = 1.65905756\n",
      "Iteration 51952, loss = 1.71556523\n",
      "Iteration 51953, loss = 1.67623113\n",
      "Iteration 51954, loss = 1.47234088\n",
      "Iteration 51955, loss = 1.48106286\n",
      "Iteration 51956, loss = 1.46998002\n",
      "Iteration 51957, loss = 1.50391161\n",
      "Iteration 51958, loss = 1.33244977\n",
      "Iteration 51959, loss = 1.29309518\n",
      "Iteration 51960, loss = 1.30008639\n",
      "Iteration 51961, loss = 1.29991866\n",
      "Iteration 51962, loss = 1.41403362\n",
      "Iteration 51963, loss = 1.27723143\n",
      "Iteration 51964, loss = 1.33853016\n",
      "Iteration 51965, loss = 1.53166214\n",
      "Iteration 51966, loss = 1.66112373\n",
      "Iteration 51967, loss = 1.45653730\n",
      "Iteration 51968, loss = 1.52988827\n",
      "Iteration 51969, loss = 1.42265291\n",
      "Iteration 51970, loss = 1.35066426\n",
      "Iteration 51971, loss = 1.40827406\n",
      "Iteration 51972, loss = 1.83704315\n",
      "Iteration 51973, loss = 1.90282674\n",
      "Iteration 51974, loss = 2.11909442\n",
      "Iteration 51975, loss = 1.75712368\n",
      "Iteration 51976, loss = 1.60127972\n",
      "Iteration 51977, loss = 1.84626599\n",
      "Iteration 51978, loss = 1.43984662\n",
      "Iteration 51979, loss = 1.64980213\n",
      "Iteration 51980, loss = 1.54160699\n",
      "Iteration 51981, loss = 1.54525176\n",
      "Iteration 51982, loss = 1.47253422\n",
      "Iteration 51983, loss = 1.49117863\n",
      "Iteration 51984, loss = 1.59858614\n",
      "Iteration 51985, loss = 1.37734254\n",
      "Iteration 51986, loss = 1.31347529\n",
      "Iteration 51987, loss = 1.30666300\n",
      "Iteration 51988, loss = 1.47851211\n",
      "Iteration 51989, loss = 1.39520323\n",
      "Iteration 51990, loss = 1.30457174\n",
      "Iteration 51991, loss = 1.43232920\n",
      "Iteration 51992, loss = 1.50852424\n",
      "Iteration 51993, loss = 1.40854289\n",
      "Iteration 51994, loss = 1.33832755\n",
      "Iteration 51995, loss = 1.29483039\n",
      "Iteration 51996, loss = 1.31462582\n",
      "Iteration 51997, loss = 1.34916201\n",
      "Iteration 51998, loss = 1.37934768\n",
      "Iteration 51999, loss = 1.53443693\n",
      "Iteration 52000, loss = 1.50925297\n",
      "Iteration 52001, loss = 1.50340383\n",
      "Iteration 52002, loss = 1.58065278\n",
      "Iteration 52003, loss = 1.71724494\n",
      "Iteration 52004, loss = 1.84394282\n",
      "Iteration 52005, loss = 1.95027159\n",
      "Iteration 52006, loss = 1.83908290\n",
      "Iteration 52007, loss = 1.79861118\n",
      "Iteration 52008, loss = 1.80430693\n",
      "Iteration 52009, loss = 1.67025727\n",
      "Iteration 52010, loss = 1.47273951\n",
      "Iteration 52011, loss = 1.69773795\n",
      "Iteration 52012, loss = 1.55386590\n",
      "Iteration 52013, loss = 1.45890962\n",
      "Iteration 52014, loss = 1.63648334\n",
      "Iteration 52015, loss = 1.77650329\n",
      "Iteration 52016, loss = 1.90659844\n",
      "Iteration 52017, loss = 2.24547488\n",
      "Iteration 52018, loss = 2.49324995\n",
      "Iteration 52019, loss = 2.44467046\n",
      "Iteration 52020, loss = 2.51990922\n",
      "Iteration 52021, loss = 2.18847058\n",
      "Iteration 52022, loss = 2.05932936\n",
      "Iteration 52023, loss = 1.67470086\n",
      "Iteration 52024, loss = 1.64126840\n",
      "Iteration 52025, loss = 1.40081607\n",
      "Iteration 52026, loss = 1.58202670\n",
      "Iteration 52027, loss = 1.55983372\n",
      "Iteration 52028, loss = 1.44454707\n",
      "Iteration 52029, loss = 1.33591484\n",
      "Iteration 52030, loss = 1.39913925\n",
      "Iteration 52031, loss = 1.64117135\n",
      "Iteration 52032, loss = 1.48055838\n",
      "Iteration 52033, loss = 1.41262627\n",
      "Iteration 52034, loss = 1.59948003\n",
      "Iteration 52035, loss = 2.01734789\n",
      "Iteration 52036, loss = 1.94497568\n",
      "Iteration 52037, loss = 1.72159450\n",
      "Iteration 52038, loss = 1.76513634\n",
      "Iteration 52039, loss = 1.49285577\n",
      "Iteration 52040, loss = 1.64208167\n",
      "Iteration 52041, loss = 1.49597692\n",
      "Iteration 52042, loss = 1.49918810\n",
      "Iteration 52043, loss = 1.42592375\n",
      "Iteration 52044, loss = 1.44688959\n",
      "Iteration 52045, loss = 1.48382404\n",
      "Iteration 52046, loss = 1.55366370\n",
      "Iteration 52047, loss = 1.52256839\n",
      "Iteration 52048, loss = 1.71730155\n",
      "Iteration 52049, loss = 1.73057676\n",
      "Iteration 52050, loss = 1.75009406\n",
      "Iteration 52051, loss = 1.67664815\n",
      "Iteration 52052, loss = 1.50959584\n",
      "Iteration 52053, loss = 1.60052295\n",
      "Iteration 52054, loss = 1.63809239\n",
      "Iteration 52055, loss = 1.53631983\n",
      "Iteration 52056, loss = 1.67540882\n",
      "Iteration 52057, loss = 1.66113091\n",
      "Iteration 52058, loss = 1.74045644\n",
      "Iteration 52059, loss = 1.78809016\n",
      "Iteration 52060, loss = 1.47680361\n",
      "Iteration 52061, loss = 1.67181037\n",
      "Iteration 52062, loss = 1.93315191\n",
      "Iteration 52063, loss = 2.07223627\n",
      "Iteration 52064, loss = 2.86787444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 52065, loss = 1.93953309\n",
      "Iteration 52066, loss = 2.03089296\n",
      "Iteration 52067, loss = 1.89541061\n",
      "Iteration 52068, loss = 1.74854231\n",
      "Iteration 52069, loss = 1.41234817\n",
      "Iteration 52070, loss = 1.48462111\n",
      "Iteration 52071, loss = 1.48357646\n",
      "Iteration 52072, loss = 1.45156239\n",
      "Iteration 52073, loss = 1.52940735\n",
      "Iteration 52074, loss = 1.62076671\n",
      "Iteration 52075, loss = 1.31787774\n",
      "Iteration 52076, loss = 1.46606346\n",
      "Iteration 52077, loss = 1.68380440\n",
      "Iteration 52078, loss = 2.07817455\n",
      "Iteration 52079, loss = 1.53617512\n",
      "Iteration 52080, loss = 1.82215461\n",
      "Iteration 52081, loss = 1.63536497\n",
      "Iteration 52082, loss = 1.55146122\n",
      "Iteration 52083, loss = 1.97794288\n",
      "Iteration 52084, loss = 2.04475316\n",
      "Iteration 52085, loss = 1.70559241\n",
      "Iteration 52086, loss = 1.81185995\n",
      "Iteration 52087, loss = 2.09758146\n",
      "Iteration 52088, loss = 1.72399516\n",
      "Iteration 52089, loss = 1.31254337\n",
      "Iteration 52090, loss = 1.45604253\n",
      "Iteration 52091, loss = 1.46014423\n",
      "Iteration 52092, loss = 1.47627094\n",
      "Iteration 52093, loss = 1.68540531\n",
      "Iteration 52094, loss = 1.71134080\n",
      "Iteration 52095, loss = 1.85791552\n",
      "Iteration 52096, loss = 1.64074566\n",
      "Iteration 52097, loss = 1.47364407\n",
      "Iteration 52098, loss = 1.63487608\n",
      "Iteration 52099, loss = 1.47272681\n",
      "Iteration 52100, loss = 1.56000398\n",
      "Iteration 52101, loss = 1.65646936\n",
      "Iteration 52102, loss = 1.63875740\n",
      "Iteration 52103, loss = 1.51801310\n",
      "Iteration 52104, loss = 2.03088685\n",
      "Iteration 52105, loss = 1.84052409\n",
      "Iteration 52106, loss = 2.26062221\n",
      "Iteration 52107, loss = 3.69924395\n",
      "Iteration 52108, loss = 2.24595745\n",
      "Iteration 52109, loss = 1.97389665\n",
      "Iteration 52110, loss = 2.12122524\n",
      "Iteration 52111, loss = 2.08024481\n",
      "Iteration 52112, loss = 2.67055369\n",
      "Iteration 52113, loss = 2.66675277\n",
      "Iteration 52114, loss = 2.37572787\n",
      "Iteration 52115, loss = 1.90861760\n",
      "Iteration 52116, loss = 1.58868766\n",
      "Iteration 52117, loss = 1.71565523\n",
      "Iteration 52118, loss = 1.67927208\n",
      "Iteration 52119, loss = 1.75132764\n",
      "Iteration 52120, loss = 1.65931214\n",
      "Iteration 52121, loss = 1.90950562\n",
      "Iteration 52122, loss = 1.75456245\n",
      "Iteration 52123, loss = 2.65992989\n",
      "Iteration 52124, loss = 2.21680346\n",
      "Iteration 52125, loss = 2.00025759\n",
      "Iteration 52126, loss = 2.42311979\n",
      "Iteration 52127, loss = 1.93840626\n",
      "Iteration 52128, loss = 1.60984818\n",
      "Iteration 52129, loss = 1.52541999\n",
      "Iteration 52130, loss = 1.71796936\n",
      "Iteration 52131, loss = 1.48572096\n",
      "Iteration 52132, loss = 1.47031424\n",
      "Iteration 52133, loss = 1.48147672\n",
      "Iteration 52134, loss = 1.46582972\n",
      "Iteration 52135, loss = 1.52475426\n",
      "Iteration 52136, loss = 1.66681227\n",
      "Iteration 52137, loss = 1.84076227\n",
      "Iteration 52138, loss = 1.81444620\n",
      "Iteration 52139, loss = 1.82216296\n",
      "Iteration 52140, loss = 2.23969685\n",
      "Iteration 52141, loss = 1.86339725\n",
      "Iteration 52142, loss = 1.60714111\n",
      "Iteration 52143, loss = 1.51107255\n",
      "Iteration 52144, loss = 1.42232546\n",
      "Iteration 52145, loss = 1.49655774\n",
      "Iteration 52146, loss = 1.66658645\n",
      "Iteration 52147, loss = 1.53590959\n",
      "Iteration 52148, loss = 2.70963898\n",
      "Iteration 52149, loss = 3.49026660\n",
      "Iteration 52150, loss = 3.81701804\n",
      "Iteration 52151, loss = 3.27330328\n",
      "Iteration 52152, loss = 2.25383269\n",
      "Iteration 52153, loss = 3.08280501\n",
      "Iteration 52154, loss = 2.89581762\n",
      "Iteration 52155, loss = 2.37874415\n",
      "Iteration 52156, loss = 2.19699706\n",
      "Iteration 52157, loss = 1.94912108\n",
      "Iteration 52158, loss = 2.14624476\n",
      "Iteration 52159, loss = 1.95470807\n",
      "Iteration 52160, loss = 2.26682781\n",
      "Iteration 52161, loss = 2.03997161\n",
      "Iteration 52162, loss = 2.26332116\n",
      "Iteration 52163, loss = 1.74618605\n",
      "Iteration 52164, loss = 1.60311972\n",
      "Iteration 52165, loss = 1.39682750\n",
      "Iteration 52166, loss = 1.37925955\n",
      "Iteration 52167, loss = 1.34441921\n",
      "Iteration 52168, loss = 1.41242911\n",
      "Iteration 52169, loss = 1.40468432\n",
      "Iteration 52170, loss = 1.36619059\n",
      "Iteration 52171, loss = 1.38369769\n",
      "Iteration 52172, loss = 1.47580988\n",
      "Iteration 52173, loss = 1.37712090\n",
      "Iteration 52174, loss = 1.43164923\n",
      "Iteration 52175, loss = 1.47152848\n",
      "Iteration 52176, loss = 1.41073726\n",
      "Iteration 52177, loss = 1.38973581\n",
      "Iteration 52178, loss = 1.41107120\n",
      "Iteration 52179, loss = 1.46167044\n",
      "Iteration 52180, loss = 1.39725930\n",
      "Iteration 52181, loss = 1.37050451\n",
      "Iteration 52182, loss = 1.31681703\n",
      "Iteration 52183, loss = 1.55828524\n",
      "Iteration 52184, loss = 1.30625469\n",
      "Iteration 52185, loss = 1.41525781\n",
      "Iteration 52186, loss = 1.43001130\n",
      "Iteration 52187, loss = 1.78746007\n",
      "Iteration 52188, loss = 2.40589360\n",
      "Iteration 52189, loss = 2.52928521\n",
      "Iteration 52190, loss = 2.67357942\n",
      "Iteration 52191, loss = 2.38758670\n",
      "Iteration 52192, loss = 2.95137572\n",
      "Iteration 52193, loss = 2.25484910\n",
      "Iteration 52194, loss = 2.42464911\n",
      "Iteration 52195, loss = 2.18667668\n",
      "Iteration 52196, loss = 2.42041370\n",
      "Iteration 52197, loss = 2.04737030\n",
      "Iteration 52198, loss = 1.97965291\n",
      "Iteration 52199, loss = 1.74733303\n",
      "Iteration 52200, loss = 1.58893845\n",
      "Iteration 52201, loss = 1.54260046\n",
      "Iteration 52202, loss = 1.92363317\n",
      "Iteration 52203, loss = 1.64442967\n",
      "Iteration 52204, loss = 1.52313322\n",
      "Iteration 52205, loss = 1.44906103\n",
      "Iteration 52206, loss = 1.52712375\n",
      "Iteration 52207, loss = 1.57346538\n",
      "Iteration 52208, loss = 1.47733480\n",
      "Iteration 52209, loss = 1.78133981\n",
      "Iteration 52210, loss = 1.38549766\n",
      "Iteration 52211, loss = 1.65153245\n",
      "Iteration 52212, loss = 1.66018030\n",
      "Iteration 52213, loss = 1.37875284\n",
      "Iteration 52214, loss = 1.46701876\n",
      "Iteration 52215, loss = 1.75351701\n",
      "Iteration 52216, loss = 1.87572111\n",
      "Iteration 52217, loss = 1.87777321\n",
      "Iteration 52218, loss = 2.14720864\n",
      "Iteration 52219, loss = 1.82895775\n",
      "Iteration 52220, loss = 1.60225786\n",
      "Iteration 52221, loss = 1.58312051\n",
      "Iteration 52222, loss = 1.66420361\n",
      "Iteration 52223, loss = 1.90550467\n",
      "Iteration 52224, loss = 1.48650146\n",
      "Iteration 52225, loss = 1.33813372\n",
      "Iteration 52226, loss = 1.40983029\n",
      "Iteration 52227, loss = 1.34216457\n",
      "Iteration 52228, loss = 1.34669374\n",
      "Iteration 52229, loss = 1.47814645\n",
      "Iteration 52230, loss = 1.47360356\n",
      "Iteration 52231, loss = 1.47815135\n",
      "Iteration 52232, loss = 1.40975468\n",
      "Iteration 52233, loss = 1.32132051\n",
      "Iteration 52234, loss = 1.44843475\n",
      "Iteration 52235, loss = 1.41667299\n",
      "Iteration 52236, loss = 1.63017163\n",
      "Iteration 52237, loss = 1.66292872\n",
      "Iteration 52238, loss = 2.05493298\n",
      "Iteration 52239, loss = 1.72088077\n",
      "Iteration 52240, loss = 1.91567143\n",
      "Iteration 52241, loss = 1.61714834\n",
      "Iteration 52242, loss = 2.10357089\n",
      "Iteration 52243, loss = 2.38103523\n",
      "Iteration 52244, loss = 2.30387250\n",
      "Iteration 52245, loss = 2.27146347\n",
      "Iteration 52246, loss = 1.87212684\n",
      "Iteration 52247, loss = 2.31116797\n",
      "Iteration 52248, loss = 1.67813403\n",
      "Iteration 52249, loss = 1.67140326\n",
      "Iteration 52250, loss = 1.61168649\n",
      "Iteration 52251, loss = 1.71732932\n",
      "Iteration 52252, loss = 1.64131415\n",
      "Iteration 52253, loss = 1.46517115\n",
      "Iteration 52254, loss = 1.43830284\n",
      "Iteration 52255, loss = 1.90573205\n",
      "Iteration 52256, loss = 2.29033996\n",
      "Iteration 52257, loss = 2.31797969\n",
      "Iteration 52258, loss = 1.95056517\n",
      "Iteration 52259, loss = 1.75832211\n",
      "Iteration 52260, loss = 2.01777314\n",
      "Iteration 52261, loss = 2.04130827\n",
      "Iteration 52262, loss = 1.96016567\n",
      "Iteration 52263, loss = 1.92700181\n",
      "Iteration 52264, loss = 2.82052762\n",
      "Iteration 52265, loss = 2.66167039\n",
      "Iteration 52266, loss = 2.97340436\n",
      "Iteration 52267, loss = 2.82175071\n",
      "Iteration 52268, loss = 2.25555583\n",
      "Iteration 52269, loss = 2.38715346\n",
      "Iteration 52270, loss = 2.30765738\n",
      "Iteration 52271, loss = 2.27393440\n",
      "Iteration 52272, loss = 1.95641608\n",
      "Iteration 52273, loss = 2.03578400\n",
      "Iteration 52274, loss = 1.69232279\n",
      "Iteration 52275, loss = 1.68037230\n",
      "Iteration 52276, loss = 1.59732769\n",
      "Iteration 52277, loss = 1.57974115\n",
      "Iteration 52278, loss = 1.43408413\n",
      "Iteration 52279, loss = 1.82446914\n",
      "Iteration 52280, loss = 1.61892135\n",
      "Iteration 52281, loss = 1.74381475\n",
      "Iteration 52282, loss = 1.71020134\n",
      "Iteration 52283, loss = 2.03451263\n",
      "Iteration 52284, loss = 1.83025729\n",
      "Iteration 52285, loss = 1.96055420\n",
      "Iteration 52286, loss = 2.39773393\n",
      "Iteration 52287, loss = 1.81440311\n",
      "Iteration 52288, loss = 1.92049507\n",
      "Iteration 52289, loss = 1.80321177\n",
      "Iteration 52290, loss = 1.75720522\n",
      "Iteration 52291, loss = 1.94801008\n",
      "Iteration 52292, loss = 2.30617408\n",
      "Iteration 52293, loss = 2.17268520\n",
      "Iteration 52294, loss = 2.53660542\n",
      "Iteration 52295, loss = 2.22983247\n",
      "Iteration 52296, loss = 2.37849766\n",
      "Iteration 52297, loss = 3.29142169\n",
      "Iteration 52298, loss = 2.78120521\n",
      "Iteration 52299, loss = 2.53954543\n",
      "Iteration 52300, loss = 2.03631618\n",
      "Iteration 52301, loss = 1.98930208\n",
      "Iteration 52302, loss = 1.61330644\n",
      "Iteration 52303, loss = 1.56638162\n",
      "Iteration 52304, loss = 1.56433107\n",
      "Iteration 52305, loss = 1.42824814\n",
      "Iteration 52306, loss = 1.43794053\n",
      "Iteration 52307, loss = 1.43531874\n",
      "Iteration 52308, loss = 1.58206700\n",
      "Iteration 52309, loss = 1.68851474\n",
      "Iteration 52310, loss = 1.66570416\n",
      "Iteration 52311, loss = 1.74650159\n",
      "Iteration 52312, loss = 1.99725507\n",
      "Iteration 52313, loss = 1.46954306\n",
      "Iteration 52314, loss = 1.29138866\n",
      "Iteration 52315, loss = 1.42828189\n",
      "Iteration 52316, loss = 1.30898487\n",
      "Iteration 52317, loss = 1.35694759\n",
      "Iteration 52318, loss = 1.39641368\n",
      "Iteration 52319, loss = 1.57335697\n",
      "Iteration 52320, loss = 1.79527662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 52321, loss = 1.65754969\n",
      "Iteration 52322, loss = 1.47000978\n",
      "Iteration 52323, loss = 1.38373170\n",
      "Iteration 52324, loss = 1.57612386\n",
      "Iteration 52325, loss = 1.49225448\n",
      "Iteration 52326, loss = 1.45573139\n",
      "Iteration 52327, loss = 1.59258821\n",
      "Iteration 52328, loss = 1.42726424\n",
      "Iteration 52329, loss = 1.43148943\n",
      "Iteration 52330, loss = 1.31421699\n",
      "Iteration 52331, loss = 1.38977823\n",
      "Iteration 52332, loss = 1.27829796\n",
      "Iteration 52333, loss = 1.48261264\n",
      "Iteration 52334, loss = 1.46841591\n",
      "Iteration 52335, loss = 1.69322308\n",
      "Iteration 52336, loss = 1.37644117\n",
      "Iteration 52337, loss = 1.60032062\n",
      "Iteration 52338, loss = 1.35116610\n",
      "Iteration 52339, loss = 1.32371469\n",
      "Iteration 52340, loss = 1.47116393\n",
      "Iteration 52341, loss = 1.36616439\n",
      "Iteration 52342, loss = 1.40589287\n",
      "Iteration 52343, loss = 1.49917258\n",
      "Iteration 52344, loss = 1.40455329\n",
      "Iteration 52345, loss = 1.67804378\n",
      "Iteration 52346, loss = 1.81586613\n",
      "Iteration 52347, loss = 1.63912226\n",
      "Iteration 52348, loss = 1.72030232\n",
      "Iteration 52349, loss = 1.63923788\n",
      "Iteration 52350, loss = 1.54969361\n",
      "Iteration 52351, loss = 2.11314442\n",
      "Iteration 52352, loss = 1.69780138\n",
      "Iteration 52353, loss = 1.60460435\n",
      "Iteration 52354, loss = 1.75325226\n",
      "Iteration 52355, loss = 2.25636821\n",
      "Iteration 52356, loss = 2.37413558\n",
      "Iteration 52357, loss = 1.92270365\n",
      "Iteration 52358, loss = 1.76420511\n",
      "Iteration 52359, loss = 1.63317805\n",
      "Iteration 52360, loss = 1.49329437\n",
      "Iteration 52361, loss = 1.46201826\n",
      "Iteration 52362, loss = 1.36629841\n",
      "Iteration 52363, loss = 1.41990287\n",
      "Iteration 52364, loss = 1.49410716\n",
      "Iteration 52365, loss = 1.45425759\n",
      "Iteration 52366, loss = 1.38561751\n",
      "Iteration 52367, loss = 1.40938221\n",
      "Iteration 52368, loss = 1.49215315\n",
      "Iteration 52369, loss = 1.64536391\n",
      "Iteration 52370, loss = 1.72565172\n",
      "Iteration 52371, loss = 2.22422099\n",
      "Iteration 52372, loss = 1.71722302\n",
      "Iteration 52373, loss = 1.60159795\n",
      "Iteration 52374, loss = 1.42051791\n",
      "Iteration 52375, loss = 1.64629475\n",
      "Iteration 52376, loss = 1.56632313\n",
      "Iteration 52377, loss = 1.67296051\n",
      "Iteration 52378, loss = 1.96470791\n",
      "Iteration 52379, loss = 2.09083263\n",
      "Iteration 52380, loss = 1.96915484\n",
      "Iteration 52381, loss = 1.65698399\n",
      "Iteration 52382, loss = 2.30103950\n",
      "Iteration 52383, loss = 2.11782826\n",
      "Iteration 52384, loss = 2.15639660\n",
      "Iteration 52385, loss = 1.93497148\n",
      "Iteration 52386, loss = 2.08168381\n",
      "Iteration 52387, loss = 2.35743665\n",
      "Iteration 52388, loss = 1.79744847\n",
      "Iteration 52389, loss = 1.84316892\n",
      "Iteration 52390, loss = 1.59589190\n",
      "Iteration 52391, loss = 1.84003811\n",
      "Iteration 52392, loss = 1.94845815\n",
      "Iteration 52393, loss = 2.57299447\n",
      "Iteration 52394, loss = 4.27044378\n",
      "Iteration 52395, loss = 3.82950270\n",
      "Iteration 52396, loss = 2.98873427\n",
      "Iteration 52397, loss = 2.69294578\n",
      "Iteration 52398, loss = 3.19719662\n",
      "Iteration 52399, loss = 3.26330255\n",
      "Iteration 52400, loss = 3.03974631\n",
      "Iteration 52401, loss = 2.56372693\n",
      "Iteration 52402, loss = 2.21855957\n",
      "Iteration 52403, loss = 2.41327116\n",
      "Iteration 52404, loss = 2.79248180\n",
      "Iteration 52405, loss = 2.11980427\n",
      "Iteration 52406, loss = 2.17444436\n",
      "Iteration 52407, loss = 1.86001862\n",
      "Iteration 52408, loss = 1.86098846\n",
      "Iteration 52409, loss = 1.64183064\n",
      "Iteration 52410, loss = 1.75311444\n",
      "Iteration 52411, loss = 1.58898595\n",
      "Iteration 52412, loss = 1.84121135\n",
      "Iteration 52413, loss = 2.15684665\n",
      "Iteration 52414, loss = 2.12485628\n",
      "Iteration 52415, loss = 2.10263781\n",
      "Iteration 52416, loss = 2.42186318\n",
      "Iteration 52417, loss = 2.19548416\n",
      "Iteration 52418, loss = 2.01677971\n",
      "Iteration 52419, loss = 2.05702153\n",
      "Iteration 52420, loss = 1.89541128\n",
      "Iteration 52421, loss = 1.83314368\n",
      "Iteration 52422, loss = 2.52278784\n",
      "Iteration 52423, loss = 2.38341229\n",
      "Iteration 52424, loss = 2.35198899\n",
      "Iteration 52425, loss = 2.37562169\n",
      "Iteration 52426, loss = 1.89562917\n",
      "Iteration 52427, loss = 1.96180718\n",
      "Iteration 52428, loss = 1.83801436\n",
      "Iteration 52429, loss = 1.66139552\n",
      "Iteration 52430, loss = 1.72929473\n",
      "Iteration 52431, loss = 1.75653660\n",
      "Iteration 52432, loss = 2.13092967\n",
      "Iteration 52433, loss = 2.44307834\n",
      "Iteration 52434, loss = 1.64387058\n",
      "Iteration 52435, loss = 1.58626170\n",
      "Iteration 52436, loss = 2.05020224\n",
      "Iteration 52437, loss = 1.72434483\n",
      "Iteration 52438, loss = 1.59526424\n",
      "Iteration 52439, loss = 1.79338079\n",
      "Iteration 52440, loss = 1.55926843\n",
      "Iteration 52441, loss = 1.46044079\n",
      "Iteration 52442, loss = 1.35478155\n",
      "Iteration 52443, loss = 1.43232628\n",
      "Iteration 52444, loss = 1.51855769\n",
      "Iteration 52445, loss = 1.59115589\n",
      "Iteration 52446, loss = 1.50712941\n",
      "Iteration 52447, loss = 1.69119655\n",
      "Iteration 52448, loss = 2.14056456\n",
      "Iteration 52449, loss = 3.64184950\n",
      "Iteration 52450, loss = 2.16722352\n",
      "Iteration 52451, loss = 2.07956243\n",
      "Iteration 52452, loss = 1.52818453\n",
      "Iteration 52453, loss = 1.46344099\n",
      "Iteration 52454, loss = 2.00102450\n",
      "Iteration 52455, loss = 1.72148623\n",
      "Iteration 52456, loss = 2.13245737\n",
      "Iteration 52457, loss = 1.85944890\n",
      "Iteration 52458, loss = 1.55896347\n",
      "Iteration 52459, loss = 1.53190562\n",
      "Iteration 52460, loss = 1.60190802\n",
      "Iteration 52461, loss = 1.45830889\n",
      "Iteration 52462, loss = 1.31290731\n",
      "Iteration 52463, loss = 1.32637031\n",
      "Iteration 52464, loss = 1.39329484\n",
      "Iteration 52465, loss = 1.49356307\n",
      "Iteration 52466, loss = 1.74175447\n",
      "Iteration 52467, loss = 1.43977856\n",
      "Iteration 52468, loss = 1.44170373\n",
      "Iteration 52469, loss = 1.33800198\n",
      "Iteration 52470, loss = 1.41223812\n",
      "Iteration 52471, loss = 1.55571291\n",
      "Iteration 52472, loss = 1.58739492\n",
      "Iteration 52473, loss = 1.50073344\n",
      "Iteration 52474, loss = 1.84634954\n",
      "Iteration 52475, loss = 1.53187809\n",
      "Iteration 52476, loss = 1.37220904\n",
      "Iteration 52477, loss = 1.37601685\n",
      "Iteration 52478, loss = 1.34005422\n",
      "Iteration 52479, loss = 1.33063858\n",
      "Iteration 52480, loss = 1.32968374\n",
      "Iteration 52481, loss = 1.36840654\n",
      "Iteration 52482, loss = 1.29300236\n",
      "Iteration 52483, loss = 1.42626409\n",
      "Iteration 52484, loss = 1.64530359\n",
      "Iteration 52485, loss = 1.66760121\n",
      "Iteration 52486, loss = 1.70676450\n",
      "Iteration 52487, loss = 1.60588815\n",
      "Iteration 52488, loss = 1.61366612\n",
      "Iteration 52489, loss = 1.40856789\n",
      "Iteration 52490, loss = 1.60149547\n",
      "Iteration 52491, loss = 1.62154487\n",
      "Iteration 52492, loss = 1.89734282\n",
      "Iteration 52493, loss = 1.65941984\n",
      "Iteration 52494, loss = 1.52699868\n",
      "Iteration 52495, loss = 1.39440374\n",
      "Iteration 52496, loss = 1.59133673\n",
      "Iteration 52497, loss = 1.40436262\n",
      "Iteration 52498, loss = 1.40118880\n",
      "Iteration 52499, loss = 1.50863422\n",
      "Iteration 52500, loss = 1.59013458\n",
      "Iteration 52501, loss = 1.41999480\n",
      "Iteration 52502, loss = 1.63756552\n",
      "Iteration 52503, loss = 1.56316548\n",
      "Iteration 52504, loss = 1.53634121\n",
      "Iteration 52505, loss = 1.52645594\n",
      "Iteration 52506, loss = 1.55032136\n",
      "Iteration 52507, loss = 1.41637454\n",
      "Iteration 52508, loss = 1.76836670\n",
      "Iteration 52509, loss = 1.74357618\n",
      "Iteration 52510, loss = 1.73715523\n",
      "Iteration 52511, loss = 1.38916657\n",
      "Iteration 52512, loss = 1.36292783\n",
      "Iteration 52513, loss = 1.34212582\n",
      "Iteration 52514, loss = 1.47897021\n",
      "Iteration 52515, loss = 1.66264980\n",
      "Iteration 52516, loss = 1.85277079\n",
      "Iteration 52517, loss = 1.58764824\n",
      "Iteration 52518, loss = 1.69350849\n",
      "Iteration 52519, loss = 1.58114063\n",
      "Iteration 52520, loss = 1.58274783\n",
      "Iteration 52521, loss = 1.68387185\n",
      "Iteration 52522, loss = 2.27184851\n",
      "Iteration 52523, loss = 1.68456186\n",
      "Iteration 52524, loss = 1.52434965\n",
      "Iteration 52525, loss = 1.51906977\n",
      "Iteration 52526, loss = 1.45819092\n",
      "Iteration 52527, loss = 1.46988866\n",
      "Iteration 52528, loss = 1.55956319\n",
      "Iteration 52529, loss = 1.56798607\n",
      "Iteration 52530, loss = 2.06388675\n",
      "Iteration 52531, loss = 2.17203085\n",
      "Iteration 52532, loss = 2.22121173\n",
      "Iteration 52533, loss = 1.86333516\n",
      "Iteration 52534, loss = 2.05295630\n",
      "Iteration 52535, loss = 1.68929116\n",
      "Iteration 52536, loss = 1.73396520\n",
      "Iteration 52537, loss = 1.60388217\n",
      "Iteration 52538, loss = 1.79384699\n",
      "Iteration 52539, loss = 1.71438851\n",
      "Iteration 52540, loss = 1.89316056\n",
      "Iteration 52541, loss = 1.52547761\n",
      "Iteration 52542, loss = 1.58327022\n",
      "Iteration 52543, loss = 1.56602242\n",
      "Iteration 52544, loss = 1.53991118\n",
      "Iteration 52545, loss = 1.60293101\n",
      "Iteration 52546, loss = 2.04663224\n",
      "Iteration 52547, loss = 2.08675310\n",
      "Iteration 52548, loss = 1.78483318\n",
      "Iteration 52549, loss = 1.61898726\n",
      "Iteration 52550, loss = 1.59125645\n",
      "Iteration 52551, loss = 1.49456952\n",
      "Iteration 52552, loss = 1.56419406\n",
      "Iteration 52553, loss = 1.38514689\n",
      "Iteration 52554, loss = 1.97356256\n",
      "Iteration 52555, loss = 1.90690930\n",
      "Iteration 52556, loss = 1.43604591\n",
      "Iteration 52557, loss = 1.38626608\n",
      "Iteration 52558, loss = 1.36642217\n",
      "Iteration 52559, loss = 1.30381653\n",
      "Iteration 52560, loss = 1.54816938\n",
      "Iteration 52561, loss = 1.42088406\n",
      "Iteration 52562, loss = 1.47423242\n",
      "Iteration 52563, loss = 1.60551420\n",
      "Iteration 52564, loss = 1.56126597\n",
      "Iteration 52565, loss = 1.77267459\n",
      "Iteration 52566, loss = 1.61105031\n",
      "Iteration 52567, loss = 1.73026265\n",
      "Iteration 52568, loss = 2.08100857\n",
      "Iteration 52569, loss = 2.20210658\n",
      "Iteration 52570, loss = 3.22177225\n",
      "Iteration 52571, loss = 2.86031453\n",
      "Iteration 52572, loss = 2.38703124\n",
      "Iteration 52573, loss = 2.09242344\n",
      "Iteration 52574, loss = 2.45262800\n",
      "Iteration 52575, loss = 1.69285600\n",
      "Iteration 52576, loss = 1.38064107\n",
      "Iteration 52577, loss = 1.36445173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 52578, loss = 1.34897319\n",
      "Iteration 52579, loss = 1.46045560\n",
      "Iteration 52580, loss = 1.56240551\n",
      "Iteration 52581, loss = 1.78379700\n",
      "Iteration 52582, loss = 1.69802202\n",
      "Iteration 52583, loss = 1.85843308\n",
      "Iteration 52584, loss = 1.80289947\n",
      "Iteration 52585, loss = 2.30777158\n",
      "Iteration 52586, loss = 2.08366387\n",
      "Iteration 52587, loss = 2.17100748\n",
      "Iteration 52588, loss = 1.68231085\n",
      "Iteration 52589, loss = 1.67239262\n",
      "Iteration 52590, loss = 1.59327726\n",
      "Iteration 52591, loss = 1.61584939\n",
      "Iteration 52592, loss = 1.49003472\n",
      "Iteration 52593, loss = 1.59087453\n",
      "Iteration 52594, loss = 1.80505624\n",
      "Iteration 52595, loss = 2.02928413\n",
      "Iteration 52596, loss = 2.12161255\n",
      "Iteration 52597, loss = 1.92018334\n",
      "Iteration 52598, loss = 1.89752112\n",
      "Iteration 52599, loss = 1.90815140\n",
      "Iteration 52600, loss = 2.43427230\n",
      "Iteration 52601, loss = 1.72041078\n",
      "Iteration 52602, loss = 1.92187243\n",
      "Iteration 52603, loss = 2.31957275\n",
      "Iteration 52604, loss = 5.54629302\n",
      "Iteration 52605, loss = 3.29427860\n",
      "Iteration 52606, loss = 3.49840392\n",
      "Iteration 52607, loss = 4.45074333\n",
      "Iteration 52608, loss = 7.52195959\n",
      "Iteration 52609, loss = 4.26001084\n",
      "Iteration 52610, loss = 4.30636092\n",
      "Iteration 52611, loss = 3.07651667\n",
      "Iteration 52612, loss = 2.34083502\n",
      "Iteration 52613, loss = 2.28914544\n",
      "Iteration 52614, loss = 2.20665364\n",
      "Iteration 52615, loss = 2.34845904\n",
      "Iteration 52616, loss = 2.49107505\n",
      "Iteration 52617, loss = 2.04691654\n",
      "Iteration 52618, loss = 1.49412722\n",
      "Iteration 52619, loss = 1.58140408\n",
      "Iteration 52620, loss = 1.52619844\n",
      "Iteration 52621, loss = 1.38130924\n",
      "Iteration 52622, loss = 1.34830697\n",
      "Iteration 52623, loss = 1.52153813\n",
      "Iteration 52624, loss = 1.36376484\n",
      "Iteration 52625, loss = 1.31240199\n",
      "Iteration 52626, loss = 1.35303188\n",
      "Iteration 52627, loss = 1.39742336\n",
      "Iteration 52628, loss = 1.57769722\n",
      "Iteration 52629, loss = 1.39274805\n",
      "Iteration 52630, loss = 1.51546411\n",
      "Iteration 52631, loss = 1.57567176\n",
      "Iteration 52632, loss = 1.49881897\n",
      "Iteration 52633, loss = 1.43726625\n",
      "Iteration 52634, loss = 1.37741463\n",
      "Iteration 52635, loss = 1.62914431\n",
      "Iteration 52636, loss = 2.07584244\n",
      "Iteration 52637, loss = 2.84469914\n",
      "Iteration 52638, loss = 2.32060573\n",
      "Iteration 52639, loss = 3.18180731\n",
      "Iteration 52640, loss = 2.95657656\n",
      "Iteration 52641, loss = 2.36822954\n",
      "Iteration 52642, loss = 1.96529056\n",
      "Iteration 52643, loss = 1.86200621\n",
      "Iteration 52644, loss = 2.41757815\n",
      "Iteration 52645, loss = 1.51006333\n",
      "Iteration 52646, loss = 1.50269891\n",
      "Iteration 52647, loss = 1.46455061\n",
      "Iteration 52648, loss = 1.51197637\n",
      "Iteration 52649, loss = 1.54844709\n",
      "Iteration 52650, loss = 1.55851591\n",
      "Iteration 52651, loss = 1.33985507\n",
      "Iteration 52652, loss = 1.61259434\n",
      "Iteration 52653, loss = 1.60871113\n",
      "Iteration 52654, loss = 1.38926343\n",
      "Iteration 52655, loss = 1.60325737\n",
      "Iteration 52656, loss = 1.63242103\n",
      "Iteration 52657, loss = 1.57448159\n",
      "Iteration 52658, loss = 1.40731123\n",
      "Iteration 52659, loss = 1.50750789\n",
      "Iteration 52660, loss = 1.42355647\n",
      "Iteration 52661, loss = 1.34818726\n",
      "Iteration 52662, loss = 1.52533158\n",
      "Iteration 52663, loss = 1.64128149\n",
      "Iteration 52664, loss = 1.99711806\n",
      "Iteration 52665, loss = 1.89121447\n",
      "Iteration 52666, loss = 1.72188741\n",
      "Iteration 52667, loss = 1.58380625\n",
      "Iteration 52668, loss = 1.86135765\n",
      "Iteration 52669, loss = 1.62047691\n",
      "Iteration 52670, loss = 1.44562814\n",
      "Iteration 52671, loss = 1.54300456\n",
      "Iteration 52672, loss = 1.81395265\n",
      "Iteration 52673, loss = 2.08781967\n",
      "Iteration 52674, loss = 1.57180425\n",
      "Iteration 52675, loss = 1.76422291\n",
      "Iteration 52676, loss = 1.97150415\n",
      "Iteration 52677, loss = 2.18190880\n",
      "Iteration 52678, loss = 2.29603577\n",
      "Iteration 52679, loss = 1.48719543\n",
      "Iteration 52680, loss = 1.59410609\n",
      "Iteration 52681, loss = 1.53101447\n",
      "Iteration 52682, loss = 1.56702465\n",
      "Iteration 52683, loss = 1.59027536\n",
      "Iteration 52684, loss = 1.63595935\n",
      "Iteration 52685, loss = 1.41149778\n",
      "Iteration 52686, loss = 1.33163504\n",
      "Iteration 52687, loss = 1.50307488\n",
      "Iteration 52688, loss = 1.44648952\n",
      "Iteration 52689, loss = 1.45863328\n",
      "Iteration 52690, loss = 1.58068026\n",
      "Iteration 52691, loss = 1.53382700\n",
      "Iteration 52692, loss = 1.57423350\n",
      "Iteration 52693, loss = 1.34709093\n",
      "Iteration 52694, loss = 1.48085845\n",
      "Iteration 52695, loss = 1.45266049\n",
      "Iteration 52696, loss = 1.55134829\n",
      "Iteration 52697, loss = 1.32362431\n",
      "Iteration 52698, loss = 1.51827564\n",
      "Iteration 52699, loss = 1.31933726\n",
      "Iteration 52700, loss = 1.28587252\n",
      "Iteration 52701, loss = 1.41670171\n",
      "Iteration 52702, loss = 1.50303225\n",
      "Iteration 52703, loss = 1.35894521\n",
      "Iteration 52704, loss = 1.52489485\n",
      "Iteration 52705, loss = 1.32887313\n",
      "Iteration 52706, loss = 1.32744096\n",
      "Iteration 52707, loss = 1.32361439\n",
      "Iteration 52708, loss = 1.40728312\n",
      "Iteration 52709, loss = 1.35276793\n",
      "Iteration 52710, loss = 1.30567306\n",
      "Iteration 52711, loss = 1.36742989\n",
      "Iteration 52712, loss = 1.43639952\n",
      "Iteration 52713, loss = 1.34852091\n",
      "Iteration 52714, loss = 1.44832074\n",
      "Iteration 52715, loss = 1.59867442\n",
      "Iteration 52716, loss = 1.85113361\n",
      "Iteration 52717, loss = 2.16859682\n",
      "Iteration 52718, loss = 2.05367028\n",
      "Iteration 52719, loss = 2.71470483\n",
      "Iteration 52720, loss = 2.15339520\n",
      "Iteration 52721, loss = 1.92317911\n",
      "Iteration 52722, loss = 1.72224304\n",
      "Iteration 52723, loss = 1.84813908\n",
      "Iteration 52724, loss = 2.04947372\n",
      "Iteration 52725, loss = 2.00591622\n",
      "Iteration 52726, loss = 2.07777905\n",
      "Iteration 52727, loss = 2.31886952\n",
      "Iteration 52728, loss = 1.73129830\n",
      "Iteration 52729, loss = 1.48507749\n",
      "Iteration 52730, loss = 1.64201231\n",
      "Iteration 52731, loss = 1.45923452\n",
      "Iteration 52732, loss = 1.44648541\n",
      "Iteration 52733, loss = 1.56295524\n",
      "Iteration 52734, loss = 1.38391702\n",
      "Iteration 52735, loss = 1.41311516\n",
      "Iteration 52736, loss = 1.63289290\n",
      "Iteration 52737, loss = 1.98111138\n",
      "Iteration 52738, loss = 1.86532772\n",
      "Iteration 52739, loss = 1.74284976\n",
      "Iteration 52740, loss = 1.82219321\n",
      "Iteration 52741, loss = 1.98501306\n",
      "Iteration 52742, loss = 1.88288005\n",
      "Iteration 52743, loss = 1.52743441\n",
      "Iteration 52744, loss = 1.65419877\n",
      "Iteration 52745, loss = 1.26846420\n",
      "Iteration 52746, loss = 1.34889861\n",
      "Iteration 52747, loss = 1.29799595\n",
      "Iteration 52748, loss = 1.50858493\n",
      "Iteration 52749, loss = 1.54136422\n",
      "Iteration 52750, loss = 1.56998711\n",
      "Iteration 52751, loss = 1.37615581\n",
      "Iteration 52752, loss = 1.27812911\n",
      "Iteration 52753, loss = 1.39268358\n",
      "Iteration 52754, loss = 1.35963258\n",
      "Iteration 52755, loss = 1.52321157\n",
      "Iteration 52756, loss = 2.28162854\n",
      "Iteration 52757, loss = 1.88963393\n",
      "Iteration 52758, loss = 1.83103498\n",
      "Iteration 52759, loss = 1.86037887\n",
      "Iteration 52760, loss = 1.68244449\n",
      "Iteration 52761, loss = 1.51848383\n",
      "Iteration 52762, loss = 1.37161015\n",
      "Iteration 52763, loss = 1.59806422\n",
      "Iteration 52764, loss = 1.61208867\n",
      "Iteration 52765, loss = 1.67788607\n",
      "Iteration 52766, loss = 1.47374114\n",
      "Iteration 52767, loss = 1.63811519\n",
      "Iteration 52768, loss = 1.47805040\n",
      "Iteration 52769, loss = 1.39560640\n",
      "Iteration 52770, loss = 1.37575601\n",
      "Iteration 52771, loss = 1.31707967\n",
      "Iteration 52772, loss = 1.37876324\n",
      "Iteration 52773, loss = 1.30583159\n",
      "Iteration 52774, loss = 1.38677628\n",
      "Iteration 52775, loss = 1.38839670\n",
      "Iteration 52776, loss = 1.44503586\n",
      "Iteration 52777, loss = 1.64890508\n",
      "Iteration 52778, loss = 2.42245234\n",
      "Iteration 52779, loss = 2.77000543\n",
      "Iteration 52780, loss = 1.79820766\n",
      "Iteration 52781, loss = 1.47606691\n",
      "Iteration 52782, loss = 1.37530340\n",
      "Iteration 52783, loss = 1.30125277\n",
      "Iteration 52784, loss = 1.44728478\n",
      "Iteration 52785, loss = 1.31883324\n",
      "Iteration 52786, loss = 1.28566827\n",
      "Iteration 52787, loss = 1.35522375\n",
      "Iteration 52788, loss = 1.33936717\n",
      "Iteration 52789, loss = 1.39994200\n",
      "Iteration 52790, loss = 1.53704413\n",
      "Iteration 52791, loss = 1.42035934\n",
      "Iteration 52792, loss = 1.69736913\n",
      "Iteration 52793, loss = 1.38969524\n",
      "Iteration 52794, loss = 1.55731510\n",
      "Iteration 52795, loss = 1.64805068\n",
      "Iteration 52796, loss = 1.82881553\n",
      "Iteration 52797, loss = 1.57943326\n",
      "Iteration 52798, loss = 1.91050400\n",
      "Iteration 52799, loss = 1.68193988\n",
      "Iteration 52800, loss = 1.38746544\n",
      "Iteration 52801, loss = 1.47603274\n",
      "Iteration 52802, loss = 2.02320860\n",
      "Iteration 52803, loss = 1.74390296\n",
      "Iteration 52804, loss = 1.64262691\n",
      "Iteration 52805, loss = 1.72650119\n",
      "Iteration 52806, loss = 1.87002902\n",
      "Iteration 52807, loss = 1.91082232\n",
      "Iteration 52808, loss = 1.70553284\n",
      "Iteration 52809, loss = 2.15222618\n",
      "Iteration 52810, loss = 2.37090715\n",
      "Iteration 52811, loss = 3.05727872\n",
      "Iteration 52812, loss = 2.61241384\n",
      "Iteration 52813, loss = 2.56744322\n",
      "Iteration 52814, loss = 2.29241882\n",
      "Iteration 52815, loss = 3.46581346\n",
      "Iteration 52816, loss = 3.90399419\n",
      "Iteration 52817, loss = 3.74560290\n",
      "Iteration 52818, loss = 3.60452826\n",
      "Iteration 52819, loss = 2.67301259\n",
      "Iteration 52820, loss = 2.71051716\n",
      "Iteration 52821, loss = 1.86816429\n",
      "Iteration 52822, loss = 1.93624595\n",
      "Iteration 52823, loss = 1.64313640\n",
      "Iteration 52824, loss = 1.53247197\n",
      "Iteration 52825, loss = 1.48062560\n",
      "Iteration 52826, loss = 1.39990671\n",
      "Iteration 52827, loss = 1.45218262\n",
      "Iteration 52828, loss = 1.39664850\n",
      "Iteration 52829, loss = 1.42963850\n",
      "Iteration 52830, loss = 1.67540774\n",
      "Iteration 52831, loss = 1.71391976\n",
      "Iteration 52832, loss = 1.71764914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 52833, loss = 1.71701058\n",
      "Iteration 52834, loss = 1.91275599\n",
      "Iteration 52835, loss = 2.02660532\n",
      "Iteration 52836, loss = 2.30476201\n",
      "Iteration 52837, loss = 1.91758052\n",
      "Iteration 52838, loss = 1.87500527\n",
      "Iteration 52839, loss = 2.05092279\n",
      "Iteration 52840, loss = 1.47658757\n",
      "Iteration 52841, loss = 1.57512058\n",
      "Iteration 52842, loss = 1.48728480\n",
      "Iteration 52843, loss = 1.41262832\n",
      "Iteration 52844, loss = 1.43800282\n",
      "Iteration 52845, loss = 1.49667455\n",
      "Iteration 52846, loss = 1.51090117\n",
      "Iteration 52847, loss = 1.49402057\n",
      "Iteration 52848, loss = 1.72945447\n",
      "Iteration 52849, loss = 1.66184557\n",
      "Iteration 52850, loss = 1.87254653\n",
      "Iteration 52851, loss = 2.09409948\n",
      "Iteration 52852, loss = 2.79725677\n",
      "Iteration 52853, loss = 2.79063609\n",
      "Iteration 52854, loss = 2.36051150\n",
      "Iteration 52855, loss = 2.32845640\n",
      "Iteration 52856, loss = 2.80322901\n",
      "Iteration 52857, loss = 2.61593126\n",
      "Iteration 52858, loss = 2.38692875\n",
      "Iteration 52859, loss = 2.97279858\n",
      "Iteration 52860, loss = 2.22987427\n",
      "Iteration 52861, loss = 1.64250085\n",
      "Iteration 52862, loss = 1.59793917\n",
      "Iteration 52863, loss = 1.55051355\n",
      "Iteration 52864, loss = 1.44581903\n",
      "Iteration 52865, loss = 1.69565589\n",
      "Iteration 52866, loss = 1.59676337\n",
      "Iteration 52867, loss = 1.49924152\n",
      "Iteration 52868, loss = 1.63179577\n",
      "Iteration 52869, loss = 1.54564119\n",
      "Iteration 52870, loss = 1.72205409\n",
      "Iteration 52871, loss = 1.65435842\n",
      "Iteration 52872, loss = 1.65645601\n",
      "Iteration 52873, loss = 1.69333519\n",
      "Iteration 52874, loss = 1.57719870\n",
      "Iteration 52875, loss = 1.78016553\n",
      "Iteration 52876, loss = 1.42855801\n",
      "Iteration 52877, loss = 1.51800824\n",
      "Iteration 52878, loss = 1.38550634\n",
      "Iteration 52879, loss = 1.29097998\n",
      "Iteration 52880, loss = 1.50245428\n",
      "Iteration 52881, loss = 1.58035408\n",
      "Iteration 52882, loss = 1.80320884\n",
      "Iteration 52883, loss = 1.90314825\n",
      "Iteration 52884, loss = 1.68569576\n",
      "Iteration 52885, loss = 1.70955864\n",
      "Iteration 52886, loss = 1.77760021\n",
      "Iteration 52887, loss = 1.83007609\n",
      "Iteration 52888, loss = 1.50817176\n",
      "Iteration 52889, loss = 1.58599360\n",
      "Iteration 52890, loss = 1.47809122\n",
      "Iteration 52891, loss = 1.47154892\n",
      "Iteration 52892, loss = 1.24587516\n",
      "Iteration 52893, loss = 1.45965902\n",
      "Iteration 52894, loss = 1.46770402\n",
      "Iteration 52895, loss = 1.42707198\n",
      "Iteration 52896, loss = 1.45257343\n",
      "Iteration 52897, loss = 1.63138555\n",
      "Iteration 52898, loss = 1.55235037\n",
      "Iteration 52899, loss = 1.70721164\n",
      "Iteration 52900, loss = 1.39129664\n",
      "Iteration 52901, loss = 1.44814302\n",
      "Iteration 52902, loss = 1.39988801\n",
      "Iteration 52903, loss = 1.39601635\n",
      "Iteration 52904, loss = 1.50576234\n",
      "Iteration 52905, loss = 1.68656386\n",
      "Iteration 52906, loss = 1.86495339\n",
      "Iteration 52907, loss = 1.59855028\n",
      "Iteration 52908, loss = 1.55603157\n",
      "Iteration 52909, loss = 1.99867129\n",
      "Iteration 52910, loss = 1.69784272\n",
      "Iteration 52911, loss = 1.58262937\n",
      "Iteration 52912, loss = 1.42038926\n",
      "Iteration 52913, loss = 1.80854111\n",
      "Iteration 52914, loss = 1.91151721\n",
      "Iteration 52915, loss = 1.88578396\n",
      "Iteration 52916, loss = 1.51221345\n",
      "Iteration 52917, loss = 1.78365943\n",
      "Iteration 52918, loss = 1.68637550\n",
      "Iteration 52919, loss = 1.55896014\n",
      "Iteration 52920, loss = 1.35895230\n",
      "Iteration 52921, loss = 1.34100172\n",
      "Iteration 52922, loss = 1.39923083\n",
      "Iteration 52923, loss = 1.40100609\n",
      "Iteration 52924, loss = 1.51394755\n",
      "Iteration 52925, loss = 1.45667388\n",
      "Iteration 52926, loss = 1.65453175\n",
      "Iteration 52927, loss = 1.53844641\n",
      "Iteration 52928, loss = 1.41476985\n",
      "Iteration 52929, loss = 1.51907426\n",
      "Iteration 52930, loss = 1.55307783\n",
      "Iteration 52931, loss = 1.41762796\n",
      "Iteration 52932, loss = 1.44278039\n",
      "Iteration 52933, loss = 1.41233499\n",
      "Iteration 52934, loss = 1.72787345\n",
      "Iteration 52935, loss = 1.35556007\n",
      "Iteration 52936, loss = 1.35896161\n",
      "Iteration 52937, loss = 1.29186321\n",
      "Iteration 52938, loss = 1.41419437\n",
      "Iteration 52939, loss = 1.26738509\n",
      "Iteration 52940, loss = 1.53908848\n",
      "Iteration 52941, loss = 1.87193399\n",
      "Iteration 52942, loss = 1.75015309\n",
      "Iteration 52943, loss = 2.00062519\n",
      "Iteration 52944, loss = 1.94940628\n",
      "Iteration 52945, loss = 1.85828163\n",
      "Iteration 52946, loss = 1.56713740\n",
      "Iteration 52947, loss = 1.98522891\n",
      "Iteration 52948, loss = 2.37165156\n",
      "Iteration 52949, loss = 2.32275367\n",
      "Iteration 52950, loss = 2.47448348\n",
      "Iteration 52951, loss = 2.26809557\n",
      "Iteration 52952, loss = 1.65487529\n",
      "Iteration 52953, loss = 1.46326656\n",
      "Iteration 52954, loss = 1.61349154\n",
      "Iteration 52955, loss = 1.98493471\n",
      "Iteration 52956, loss = 2.27933511\n",
      "Iteration 52957, loss = 2.09743708\n",
      "Iteration 52958, loss = 2.83883489\n",
      "Iteration 52959, loss = 4.74915148\n",
      "Iteration 52960, loss = 3.03555350\n",
      "Iteration 52961, loss = 5.27262822\n",
      "Iteration 52962, loss = 3.48097035\n",
      "Iteration 52963, loss = 2.84174434\n",
      "Iteration 52964, loss = 1.77559236\n",
      "Iteration 52965, loss = 1.87227686\n",
      "Iteration 52966, loss = 2.08842111\n",
      "Iteration 52967, loss = 2.64713234\n",
      "Iteration 52968, loss = 2.59835421\n",
      "Iteration 52969, loss = 2.26215745\n",
      "Iteration 52970, loss = 1.80932685\n",
      "Iteration 52971, loss = 2.21062070\n",
      "Iteration 52972, loss = 2.13035232\n",
      "Iteration 52973, loss = 2.07319973\n",
      "Iteration 52974, loss = 2.06992286\n",
      "Iteration 52975, loss = 1.83218908\n",
      "Iteration 52976, loss = 2.56801128\n",
      "Iteration 52977, loss = 2.55859392\n",
      "Iteration 52978, loss = 2.30079703\n",
      "Iteration 52979, loss = 1.92716598\n",
      "Iteration 52980, loss = 1.69090553\n",
      "Iteration 52981, loss = 2.14835632\n",
      "Iteration 52982, loss = 2.47823289\n",
      "Iteration 52983, loss = 2.52564980\n",
      "Iteration 52984, loss = 2.24159701\n",
      "Iteration 52985, loss = 1.74831003\n",
      "Iteration 52986, loss = 1.56305953\n",
      "Iteration 52987, loss = 1.63406561\n",
      "Iteration 52988, loss = 1.62209815\n",
      "Iteration 52989, loss = 1.36261122\n",
      "Iteration 52990, loss = 1.51071418\n",
      "Iteration 52991, loss = 1.41398896\n",
      "Iteration 52992, loss = 1.67152506\n",
      "Iteration 52993, loss = 1.64158557\n",
      "Iteration 52994, loss = 1.76438968\n",
      "Iteration 52995, loss = 1.40644284\n",
      "Iteration 52996, loss = 1.73630130\n",
      "Iteration 52997, loss = 1.68265412\n",
      "Iteration 52998, loss = 1.37670810\n",
      "Iteration 52999, loss = 1.30920083\n",
      "Iteration 53000, loss = 1.43565331\n",
      "Iteration 53001, loss = 1.38126123\n",
      "Iteration 53002, loss = 1.77161043\n",
      "Iteration 53003, loss = 1.67475834\n",
      "Iteration 53004, loss = 1.91312166\n",
      "Iteration 53005, loss = 2.11634260\n",
      "Iteration 53006, loss = 1.85819020\n",
      "Iteration 53007, loss = 1.92657353\n",
      "Iteration 53008, loss = 2.60689491\n",
      "Iteration 53009, loss = 1.85884127\n",
      "Iteration 53010, loss = 1.72749244\n",
      "Iteration 53011, loss = 1.82858398\n",
      "Iteration 53012, loss = 1.60616705\n",
      "Iteration 53013, loss = 1.78242502\n",
      "Iteration 53014, loss = 1.93972794\n",
      "Iteration 53015, loss = 1.94850684\n",
      "Iteration 53016, loss = 1.72616549\n",
      "Iteration 53017, loss = 2.29631386\n",
      "Iteration 53018, loss = 1.88729978\n",
      "Iteration 53019, loss = 2.25531149\n",
      "Iteration 53020, loss = 2.82011343\n",
      "Iteration 53021, loss = 2.15730275\n",
      "Iteration 53022, loss = 2.41569782\n",
      "Iteration 53023, loss = 3.01707276\n",
      "Iteration 53024, loss = 2.92222030\n",
      "Iteration 53025, loss = 2.59057299\n",
      "Iteration 53026, loss = 2.38304457\n",
      "Iteration 53027, loss = 2.26483312\n",
      "Iteration 53028, loss = 1.91514259\n",
      "Iteration 53029, loss = 1.83525846\n",
      "Iteration 53030, loss = 1.74557304\n",
      "Iteration 53031, loss = 1.90512634\n",
      "Iteration 53032, loss = 1.73931261\n",
      "Iteration 53033, loss = 1.55651177\n",
      "Iteration 53034, loss = 1.84163842\n",
      "Iteration 53035, loss = 1.76740738\n",
      "Iteration 53036, loss = 1.79284493\n",
      "Iteration 53037, loss = 1.55347521\n",
      "Iteration 53038, loss = 1.68418396\n",
      "Iteration 53039, loss = 1.70276614\n",
      "Iteration 53040, loss = 1.83180123\n",
      "Iteration 53041, loss = 2.07818005\n",
      "Iteration 53042, loss = 1.98257249\n",
      "Iteration 53043, loss = 1.66694579\n",
      "Iteration 53044, loss = 1.73389276\n",
      "Iteration 53045, loss = 2.07023932\n",
      "Iteration 53046, loss = 3.24779691\n",
      "Iteration 53047, loss = 2.62947441\n",
      "Iteration 53048, loss = 2.33319480\n",
      "Iteration 53049, loss = 2.24971996\n",
      "Iteration 53050, loss = 2.09624332\n",
      "Iteration 53051, loss = 2.28245843\n",
      "Iteration 53052, loss = 2.20975187\n",
      "Iteration 53053, loss = 2.06776823\n",
      "Iteration 53054, loss = 1.81535147\n",
      "Iteration 53055, loss = 1.87831288\n",
      "Iteration 53056, loss = 1.80494531\n",
      "Iteration 53057, loss = 1.67560437\n",
      "Iteration 53058, loss = 1.88029995\n",
      "Iteration 53059, loss = 1.69057245\n",
      "Iteration 53060, loss = 1.96345183\n",
      "Iteration 53061, loss = 1.64212666\n",
      "Iteration 53062, loss = 1.83709438\n",
      "Iteration 53063, loss = 2.10510871\n",
      "Iteration 53064, loss = 2.69852748\n",
      "Iteration 53065, loss = 2.47050894\n",
      "Iteration 53066, loss = 2.17608718\n",
      "Iteration 53067, loss = 1.65746094\n",
      "Iteration 53068, loss = 1.58680888\n",
      "Iteration 53069, loss = 1.60004510\n",
      "Iteration 53070, loss = 1.51817715\n",
      "Iteration 53071, loss = 1.51069298\n",
      "Iteration 53072, loss = 1.62939826\n",
      "Iteration 53073, loss = 1.54062748\n",
      "Iteration 53074, loss = 1.58636894\n",
      "Iteration 53075, loss = 1.56741695\n",
      "Iteration 53076, loss = 1.65433308\n",
      "Iteration 53077, loss = 1.52425735\n",
      "Iteration 53078, loss = 1.65511214\n",
      "Iteration 53079, loss = 1.67277075\n",
      "Iteration 53080, loss = 1.66654925\n",
      "Iteration 53081, loss = 1.53491444\n",
      "Iteration 53082, loss = 1.83604167\n",
      "Iteration 53083, loss = 1.31837102\n",
      "Iteration 53084, loss = 1.67058747\n",
      "Iteration 53085, loss = 1.51425424\n",
      "Iteration 53086, loss = 1.55190425\n",
      "Iteration 53087, loss = 1.45117558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 53088, loss = 1.61464451\n",
      "Iteration 53089, loss = 1.36858034\n",
      "Iteration 53090, loss = 1.31995715\n",
      "Iteration 53091, loss = 1.29240689\n",
      "Iteration 53092, loss = 1.51712114\n",
      "Iteration 53093, loss = 1.90657158\n",
      "Iteration 53094, loss = 1.78786302\n",
      "Iteration 53095, loss = 2.02433009\n",
      "Iteration 53096, loss = 1.60551768\n",
      "Iteration 53097, loss = 1.34463959\n",
      "Iteration 53098, loss = 1.46701934\n",
      "Iteration 53099, loss = 1.78730644\n",
      "Iteration 53100, loss = 1.70498544\n",
      "Iteration 53101, loss = 1.53219695\n",
      "Iteration 53102, loss = 1.76694791\n",
      "Iteration 53103, loss = 1.80703247\n",
      "Iteration 53104, loss = 1.77078785\n",
      "Iteration 53105, loss = 1.49514815\n",
      "Iteration 53106, loss = 1.48478739\n",
      "Iteration 53107, loss = 1.30204298\n",
      "Iteration 53108, loss = 1.71716515\n",
      "Iteration 53109, loss = 1.81443391\n",
      "Iteration 53110, loss = 1.40415757\n",
      "Iteration 53111, loss = 1.87134488\n",
      "Iteration 53112, loss = 1.62313260\n",
      "Iteration 53113, loss = 1.69746439\n",
      "Iteration 53114, loss = 1.72022408\n",
      "Iteration 53115, loss = 1.59859687\n",
      "Iteration 53116, loss = 1.79582465\n",
      "Iteration 53117, loss = 1.51238166\n",
      "Iteration 53118, loss = 1.41017056\n",
      "Iteration 53119, loss = 1.54793750\n",
      "Iteration 53120, loss = 1.47048062\n",
      "Iteration 53121, loss = 1.70058687\n",
      "Iteration 53122, loss = 1.58628589\n",
      "Iteration 53123, loss = 1.59990012\n",
      "Iteration 53124, loss = 1.97646292\n",
      "Iteration 53125, loss = 2.36436881\n",
      "Iteration 53126, loss = 2.07969309\n",
      "Iteration 53127, loss = 2.03194301\n",
      "Iteration 53128, loss = 1.79834696\n",
      "Iteration 53129, loss = 1.74358366\n",
      "Iteration 53130, loss = 1.84776151\n",
      "Iteration 53131, loss = 2.18395187\n",
      "Iteration 53132, loss = 1.99616152\n",
      "Iteration 53133, loss = 3.07985675\n",
      "Iteration 53134, loss = 3.23276816\n",
      "Iteration 53135, loss = 2.43786178\n",
      "Iteration 53136, loss = 2.28630969\n",
      "Iteration 53137, loss = 2.01058034\n",
      "Iteration 53138, loss = 1.88836209\n",
      "Iteration 53139, loss = 2.07433260\n",
      "Iteration 53140, loss = 2.03030960\n",
      "Iteration 53141, loss = 1.83045430\n",
      "Iteration 53142, loss = 1.34530961\n",
      "Iteration 53143, loss = 1.78931723\n",
      "Iteration 53144, loss = 1.66520514\n",
      "Iteration 53145, loss = 2.00941856\n",
      "Iteration 53146, loss = 2.22340406\n",
      "Iteration 53147, loss = 2.12113668\n",
      "Iteration 53148, loss = 1.63648616\n",
      "Iteration 53149, loss = 1.61866766\n",
      "Iteration 53150, loss = 1.43433260\n",
      "Iteration 53151, loss = 1.35990182\n",
      "Iteration 53152, loss = 1.49528468\n",
      "Iteration 53153, loss = 1.42012324\n",
      "Iteration 53154, loss = 1.37410984\n",
      "Iteration 53155, loss = 1.49967828\n",
      "Iteration 53156, loss = 1.88683475\n",
      "Iteration 53157, loss = 2.22210056\n",
      "Iteration 53158, loss = 1.90159610\n",
      "Iteration 53159, loss = 1.92307281\n",
      "Iteration 53160, loss = 1.68947989\n",
      "Iteration 53161, loss = 2.45414239\n",
      "Iteration 53162, loss = 1.89830010\n",
      "Iteration 53163, loss = 2.13824782\n",
      "Iteration 53164, loss = 1.77873550\n",
      "Iteration 53165, loss = 1.52194991\n",
      "Iteration 53166, loss = 1.53186174\n",
      "Iteration 53167, loss = 1.47410963\n",
      "Iteration 53168, loss = 1.39028153\n",
      "Iteration 53169, loss = 1.41762211\n",
      "Iteration 53170, loss = 1.45134580\n",
      "Iteration 53171, loss = 1.44997081\n",
      "Iteration 53172, loss = 1.44437616\n",
      "Iteration 53173, loss = 1.45883251\n",
      "Iteration 53174, loss = 1.49299730\n",
      "Iteration 53175, loss = 2.11864524\n",
      "Iteration 53176, loss = 2.20386807\n",
      "Iteration 53177, loss = 1.93959521\n",
      "Iteration 53178, loss = 2.18247830\n",
      "Iteration 53179, loss = 1.97738520\n",
      "Iteration 53180, loss = 2.27785160\n",
      "Iteration 53181, loss = 2.37169225\n",
      "Iteration 53182, loss = 1.61425089\n",
      "Iteration 53183, loss = 1.59255879\n",
      "Iteration 53184, loss = 1.83547632\n",
      "Iteration 53185, loss = 1.53826935\n",
      "Iteration 53186, loss = 1.52146931\n",
      "Iteration 53187, loss = 1.88150170\n",
      "Iteration 53188, loss = 1.52422335\n",
      "Iteration 53189, loss = 1.31843342\n",
      "Iteration 53190, loss = 1.34580612\n",
      "Iteration 53191, loss = 1.34363309\n",
      "Iteration 53192, loss = 1.31901879\n",
      "Iteration 53193, loss = 1.53920941\n",
      "Iteration 53194, loss = 1.66209694\n",
      "Iteration 53195, loss = 1.76470764\n",
      "Iteration 53196, loss = 1.67621307\n",
      "Iteration 53197, loss = 1.64776512\n",
      "Iteration 53198, loss = 1.89495171\n",
      "Iteration 53199, loss = 1.54703478\n",
      "Iteration 53200, loss = 1.54001717\n",
      "Iteration 53201, loss = 1.66280457\n",
      "Iteration 53202, loss = 1.61492788\n",
      "Iteration 53203, loss = 1.50667078\n",
      "Iteration 53204, loss = 1.57007194\n",
      "Iteration 53205, loss = 1.76180995\n",
      "Iteration 53206, loss = 1.76521475\n",
      "Iteration 53207, loss = 1.43132642\n",
      "Iteration 53208, loss = 1.69933077\n",
      "Iteration 53209, loss = 1.71549411\n",
      "Iteration 53210, loss = 1.93081401\n",
      "Iteration 53211, loss = 1.49731773\n",
      "Iteration 53212, loss = 1.76849630\n",
      "Iteration 53213, loss = 1.82352625\n",
      "Iteration 53214, loss = 1.60955092\n",
      "Iteration 53215, loss = 1.92149226\n",
      "Iteration 53216, loss = 2.11143802\n",
      "Iteration 53217, loss = 3.03413766\n",
      "Iteration 53218, loss = 2.65072880\n",
      "Iteration 53219, loss = 1.67280771\n",
      "Iteration 53220, loss = 1.71475104\n",
      "Iteration 53221, loss = 1.80725119\n",
      "Iteration 53222, loss = 2.00674920\n",
      "Iteration 53223, loss = 1.88894196\n",
      "Iteration 53224, loss = 1.73053004\n",
      "Iteration 53225, loss = 1.88509699\n",
      "Iteration 53226, loss = 1.95025101\n",
      "Iteration 53227, loss = 1.66834484\n",
      "Iteration 53228, loss = 1.53672779\n",
      "Iteration 53229, loss = 1.65672997\n",
      "Iteration 53230, loss = 1.53536008\n",
      "Iteration 53231, loss = 1.53364132\n",
      "Iteration 53232, loss = 1.48316803\n",
      "Iteration 53233, loss = 1.84107523\n",
      "Iteration 53234, loss = 1.58481159\n",
      "Iteration 53235, loss = 1.61809820\n",
      "Iteration 53236, loss = 1.81977583\n",
      "Iteration 53237, loss = 1.78067125\n",
      "Iteration 53238, loss = 1.97981009\n",
      "Iteration 53239, loss = 1.87224075\n",
      "Iteration 53240, loss = 1.68157391\n",
      "Iteration 53241, loss = 1.60606789\n",
      "Iteration 53242, loss = 1.58987610\n",
      "Iteration 53243, loss = 1.47556948\n",
      "Iteration 53244, loss = 1.69413073\n",
      "Iteration 53245, loss = 2.00633292\n",
      "Iteration 53246, loss = 2.43106307\n",
      "Iteration 53247, loss = 3.00040420\n",
      "Iteration 53248, loss = 2.78115766\n",
      "Iteration 53249, loss = 2.45638297\n",
      "Iteration 53250, loss = 2.99416886\n",
      "Iteration 53251, loss = 2.27636524\n",
      "Iteration 53252, loss = 1.76221805\n",
      "Iteration 53253, loss = 1.69270445\n",
      "Iteration 53254, loss = 1.76876779\n",
      "Iteration 53255, loss = 1.83595298\n",
      "Iteration 53256, loss = 1.92614037\n",
      "Iteration 53257, loss = 2.14874668\n",
      "Iteration 53258, loss = 2.18450191\n",
      "Iteration 53259, loss = 1.69928503\n",
      "Iteration 53260, loss = 2.50467953\n",
      "Iteration 53261, loss = 2.34139574\n",
      "Iteration 53262, loss = 2.10105793\n",
      "Iteration 53263, loss = 1.95766128\n",
      "Iteration 53264, loss = 2.04402509\n",
      "Iteration 53265, loss = 1.48885634\n",
      "Iteration 53266, loss = 1.38418894\n",
      "Iteration 53267, loss = 1.28893844\n",
      "Iteration 53268, loss = 1.33574754\n",
      "Iteration 53269, loss = 1.50341552\n",
      "Iteration 53270, loss = 1.51245084\n",
      "Iteration 53271, loss = 1.59149707\n",
      "Iteration 53272, loss = 1.28501089\n",
      "Iteration 53273, loss = 1.30026278\n",
      "Iteration 53274, loss = 1.29194613\n",
      "Iteration 53275, loss = 1.58397000\n",
      "Iteration 53276, loss = 1.75675553\n",
      "Iteration 53277, loss = 1.95545337\n",
      "Iteration 53278, loss = 2.33470495\n",
      "Iteration 53279, loss = 1.99682704\n",
      "Iteration 53280, loss = 1.58540377\n",
      "Iteration 53281, loss = 1.42883743\n",
      "Iteration 53282, loss = 1.54457876\n",
      "Iteration 53283, loss = 1.45853038\n",
      "Iteration 53284, loss = 1.69459466\n",
      "Iteration 53285, loss = 1.32588432\n",
      "Iteration 53286, loss = 1.31739422\n",
      "Iteration 53287, loss = 1.64825033\n",
      "Iteration 53288, loss = 1.57119583\n",
      "Iteration 53289, loss = 1.39168413\n",
      "Iteration 53290, loss = 1.63938709\n",
      "Iteration 53291, loss = 1.75027740\n",
      "Iteration 53292, loss = 1.67731841\n",
      "Iteration 53293, loss = 1.72257221\n",
      "Iteration 53294, loss = 1.45399977\n",
      "Iteration 53295, loss = 1.42261969\n",
      "Iteration 53296, loss = 1.37467248\n",
      "Iteration 53297, loss = 1.40813085\n",
      "Iteration 53298, loss = 1.37191995\n",
      "Iteration 53299, loss = 1.38799610\n",
      "Iteration 53300, loss = 1.32221936\n",
      "Iteration 53301, loss = 1.31654926\n",
      "Iteration 53302, loss = 1.41909533\n",
      "Iteration 53303, loss = 1.37821614\n",
      "Iteration 53304, loss = 1.41699706\n",
      "Iteration 53305, loss = 1.57324594\n",
      "Iteration 53306, loss = 1.72651464\n",
      "Iteration 53307, loss = 1.59188820\n",
      "Iteration 53308, loss = 1.85571179\n",
      "Iteration 53309, loss = 2.01838369\n",
      "Iteration 53310, loss = 1.74200740\n",
      "Iteration 53311, loss = 2.23828864\n",
      "Iteration 53312, loss = 1.60376412\n",
      "Iteration 53313, loss = 1.48007270\n",
      "Iteration 53314, loss = 1.32569828\n",
      "Iteration 53315, loss = 1.33760771\n",
      "Iteration 53316, loss = 1.59363307\n",
      "Iteration 53317, loss = 1.35468529\n",
      "Iteration 53318, loss = 1.52852323\n",
      "Iteration 53319, loss = 1.57643024\n",
      "Iteration 53320, loss = 1.47977385\n",
      "Iteration 53321, loss = 1.80556152\n",
      "Iteration 53322, loss = 1.44521313\n",
      "Iteration 53323, loss = 1.32431598\n",
      "Iteration 53324, loss = 1.43385255\n",
      "Iteration 53325, loss = 1.43277482\n",
      "Iteration 53326, loss = 1.52270570\n",
      "Iteration 53327, loss = 1.41993298\n",
      "Iteration 53328, loss = 1.37980548\n",
      "Iteration 53329, loss = 1.55107054\n",
      "Iteration 53330, loss = 2.04691825\n",
      "Iteration 53331, loss = 1.83288931\n",
      "Iteration 53332, loss = 1.35353011\n",
      "Iteration 53333, loss = 1.36332637\n",
      "Iteration 53334, loss = 1.42733282\n",
      "Iteration 53335, loss = 2.37681773\n",
      "Iteration 53336, loss = 2.84242966\n",
      "Iteration 53337, loss = 2.45092041\n",
      "Iteration 53338, loss = 1.77954257\n",
      "Iteration 53339, loss = 1.70895181\n",
      "Iteration 53340, loss = 1.81321329\n",
      "Iteration 53341, loss = 1.52880353\n",
      "Iteration 53342, loss = 1.56791017\n",
      "Iteration 53343, loss = 1.66592660\n",
      "Iteration 53344, loss = 2.41896675\n",
      "Iteration 53345, loss = 2.13569487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 53346, loss = 1.80554233\n",
      "Iteration 53347, loss = 1.63063899\n",
      "Iteration 53348, loss = 1.54807814\n",
      "Iteration 53349, loss = 1.48961703\n",
      "Iteration 53350, loss = 1.51769909\n",
      "Iteration 53351, loss = 1.66071786\n",
      "Iteration 53352, loss = 1.50374683\n",
      "Iteration 53353, loss = 1.47683038\n",
      "Iteration 53354, loss = 1.28323461\n",
      "Iteration 53355, loss = 1.32803979\n",
      "Iteration 53356, loss = 1.32039115\n",
      "Iteration 53357, loss = 1.39741792\n",
      "Iteration 53358, loss = 1.47681131\n",
      "Iteration 53359, loss = 1.85026926\n",
      "Iteration 53360, loss = 1.54128388\n",
      "Iteration 53361, loss = 1.80951152\n",
      "Iteration 53362, loss = 2.26975649\n",
      "Iteration 53363, loss = 2.20315871\n",
      "Iteration 53364, loss = 2.13907115\n",
      "Iteration 53365, loss = 4.55125549\n",
      "Iteration 53366, loss = 3.42162705\n",
      "Iteration 53367, loss = 2.29861856\n",
      "Iteration 53368, loss = 1.95846600\n",
      "Iteration 53369, loss = 2.07747740\n",
      "Iteration 53370, loss = 1.84623584\n",
      "Iteration 53371, loss = 1.68372486\n",
      "Iteration 53372, loss = 1.44089810\n",
      "Iteration 53373, loss = 1.55626489\n",
      "Iteration 53374, loss = 2.53048160\n",
      "Iteration 53375, loss = 2.22628802\n",
      "Iteration 53376, loss = 2.05025132\n",
      "Iteration 53377, loss = 1.80897672\n",
      "Iteration 53378, loss = 1.47140731\n",
      "Iteration 53379, loss = 1.51413828\n",
      "Iteration 53380, loss = 1.60299378\n",
      "Iteration 53381, loss = 1.54605215\n",
      "Iteration 53382, loss = 1.56650936\n",
      "Iteration 53383, loss = 1.46179560\n",
      "Iteration 53384, loss = 1.47052320\n",
      "Iteration 53385, loss = 1.48613267\n",
      "Iteration 53386, loss = 1.40441803\n",
      "Iteration 53387, loss = 1.69690127\n",
      "Iteration 53388, loss = 1.37907310\n",
      "Iteration 53389, loss = 1.29926417\n",
      "Iteration 53390, loss = 1.54251320\n",
      "Iteration 53391, loss = 1.37142802\n",
      "Iteration 53392, loss = 1.43775868\n",
      "Iteration 53393, loss = 1.32471001\n",
      "Iteration 53394, loss = 1.40274078\n",
      "Iteration 53395, loss = 1.37570937\n",
      "Iteration 53396, loss = 1.63828896\n",
      "Iteration 53397, loss = 1.67189216\n",
      "Iteration 53398, loss = 1.64446951\n",
      "Iteration 53399, loss = 1.75293419\n",
      "Iteration 53400, loss = 1.90804517\n",
      "Iteration 53401, loss = 1.94641894\n",
      "Iteration 53402, loss = 2.02764900\n",
      "Iteration 53403, loss = 1.87192692\n",
      "Iteration 53404, loss = 1.58809949\n",
      "Iteration 53405, loss = 1.53797759\n",
      "Iteration 53406, loss = 1.60816098\n",
      "Iteration 53407, loss = 1.46360154\n",
      "Iteration 53408, loss = 1.53588754\n",
      "Iteration 53409, loss = 1.83486270\n",
      "Iteration 53410, loss = 1.88407079\n",
      "Iteration 53411, loss = 1.78131141\n",
      "Iteration 53412, loss = 2.27306996\n",
      "Iteration 53413, loss = 2.04512610\n",
      "Iteration 53414, loss = 2.78580775\n",
      "Iteration 53415, loss = 2.73103868\n",
      "Iteration 53416, loss = 2.13292426\n",
      "Iteration 53417, loss = 1.56483895\n",
      "Iteration 53418, loss = 2.05355730\n",
      "Iteration 53419, loss = 1.87801422\n",
      "Iteration 53420, loss = 2.25360766\n",
      "Iteration 53421, loss = 2.03830623\n",
      "Iteration 53422, loss = 1.67868162\n",
      "Iteration 53423, loss = 1.61548725\n",
      "Iteration 53424, loss = 1.85097907\n",
      "Iteration 53425, loss = 1.80720973\n",
      "Iteration 53426, loss = 2.20067132\n",
      "Iteration 53427, loss = 1.65609527\n",
      "Iteration 53428, loss = 1.40919024\n",
      "Iteration 53429, loss = 1.48559014\n",
      "Iteration 53430, loss = 1.30834449\n",
      "Iteration 53431, loss = 1.41278026\n",
      "Iteration 53432, loss = 2.12847866\n",
      "Iteration 53433, loss = 1.77721707\n",
      "Iteration 53434, loss = 1.68939424\n",
      "Iteration 53435, loss = 1.86938149\n",
      "Iteration 53436, loss = 1.88905663\n",
      "Iteration 53437, loss = 1.96530623\n",
      "Iteration 53438, loss = 1.76379531\n",
      "Iteration 53439, loss = 1.43631106\n",
      "Iteration 53440, loss = 1.45040654\n",
      "Iteration 53441, loss = 1.52621721\n",
      "Iteration 53442, loss = 1.46171506\n",
      "Iteration 53443, loss = 1.66631142\n",
      "Iteration 53444, loss = 1.37925932\n",
      "Iteration 53445, loss = 1.39420115\n",
      "Iteration 53446, loss = 1.37959289\n",
      "Iteration 53447, loss = 1.36260682\n",
      "Iteration 53448, loss = 1.52329448\n",
      "Iteration 53449, loss = 1.45900086\n",
      "Iteration 53450, loss = 1.39949793\n",
      "Iteration 53451, loss = 1.52245568\n",
      "Iteration 53452, loss = 1.42739998\n",
      "Iteration 53453, loss = 1.53546067\n",
      "Iteration 53454, loss = 1.75972917\n",
      "Iteration 53455, loss = 1.87511417\n",
      "Iteration 53456, loss = 1.90553341\n",
      "Iteration 53457, loss = 1.64986170\n",
      "Iteration 53458, loss = 1.50076042\n",
      "Iteration 53459, loss = 1.53396965\n",
      "Iteration 53460, loss = 1.61472984\n",
      "Iteration 53461, loss = 1.48839775\n",
      "Iteration 53462, loss = 1.58776327\n",
      "Iteration 53463, loss = 1.43035210\n",
      "Iteration 53464, loss = 1.69699425\n",
      "Iteration 53465, loss = 1.34732552\n",
      "Iteration 53466, loss = 1.31922777\n",
      "Iteration 53467, loss = 1.27574824\n",
      "Iteration 53468, loss = 1.42388492\n",
      "Iteration 53469, loss = 1.32778327\n",
      "Iteration 53470, loss = 1.43739874\n",
      "Iteration 53471, loss = 1.34001911\n",
      "Iteration 53472, loss = 1.42483872\n",
      "Iteration 53473, loss = 1.64017303\n",
      "Iteration 53474, loss = 1.80264490\n",
      "Iteration 53475, loss = 1.40338557\n",
      "Iteration 53476, loss = 1.94615371\n",
      "Iteration 53477, loss = 2.05949747\n",
      "Iteration 53478, loss = 1.66452007\n",
      "Iteration 53479, loss = 1.77006723\n",
      "Iteration 53480, loss = 1.84888061\n",
      "Iteration 53481, loss = 1.65646192\n",
      "Iteration 53482, loss = 1.49071164\n",
      "Iteration 53483, loss = 1.65554132\n",
      "Iteration 53484, loss = 1.60799946\n",
      "Iteration 53485, loss = 1.83514986\n",
      "Iteration 53486, loss = 1.50616037\n",
      "Iteration 53487, loss = 1.44754129\n",
      "Iteration 53488, loss = 1.46564956\n",
      "Iteration 53489, loss = 1.41598779\n",
      "Iteration 53490, loss = 1.48606580\n",
      "Iteration 53491, loss = 1.46069476\n",
      "Iteration 53492, loss = 1.83789851\n",
      "Iteration 53493, loss = 1.64665359\n",
      "Iteration 53494, loss = 1.51991466\n",
      "Iteration 53495, loss = 1.44600137\n",
      "Iteration 53496, loss = 1.33643648\n",
      "Iteration 53497, loss = 1.36820037\n",
      "Iteration 53498, loss = 1.91706329\n",
      "Iteration 53499, loss = 2.09812836\n",
      "Iteration 53500, loss = 1.78753893\n",
      "Iteration 53501, loss = 1.44229867\n",
      "Iteration 53502, loss = 1.37911589\n",
      "Iteration 53503, loss = 1.36733984\n",
      "Iteration 53504, loss = 1.69952412\n",
      "Iteration 53505, loss = 1.50849135\n",
      "Iteration 53506, loss = 1.68446227\n",
      "Iteration 53507, loss = 1.56544270\n",
      "Iteration 53508, loss = 1.77071675\n",
      "Iteration 53509, loss = 1.49370618\n",
      "Iteration 53510, loss = 1.79191523\n",
      "Iteration 53511, loss = 2.02109251\n",
      "Iteration 53512, loss = 1.77225999\n",
      "Iteration 53513, loss = 1.62584665\n",
      "Iteration 53514, loss = 1.61712188\n",
      "Iteration 53515, loss = 1.91006737\n",
      "Iteration 53516, loss = 1.45778829\n",
      "Iteration 53517, loss = 1.60885147\n",
      "Iteration 53518, loss = 1.63332207\n",
      "Iteration 53519, loss = 1.45783714\n",
      "Iteration 53520, loss = 1.56641591\n",
      "Iteration 53521, loss = 1.59373905\n",
      "Iteration 53522, loss = 1.49936529\n",
      "Iteration 53523, loss = 1.35552201\n",
      "Iteration 53524, loss = 1.62918804\n",
      "Iteration 53525, loss = 2.36688852\n",
      "Iteration 53526, loss = 1.86947746\n",
      "Iteration 53527, loss = 2.31208649\n",
      "Iteration 53528, loss = 2.02205952\n",
      "Iteration 53529, loss = 1.68330188\n",
      "Iteration 53530, loss = 1.70088536\n",
      "Iteration 53531, loss = 1.79528087\n",
      "Iteration 53532, loss = 1.72562643\n",
      "Iteration 53533, loss = 1.50833851\n",
      "Iteration 53534, loss = 1.70463841\n",
      "Iteration 53535, loss = 1.55019330\n",
      "Iteration 53536, loss = 1.58605888\n",
      "Iteration 53537, loss = 1.55741022\n",
      "Iteration 53538, loss = 1.81220100\n",
      "Iteration 53539, loss = 1.52536728\n",
      "Iteration 53540, loss = 1.85632631\n",
      "Iteration 53541, loss = 2.30061975\n",
      "Iteration 53542, loss = 1.84986571\n",
      "Iteration 53543, loss = 1.57076957\n",
      "Iteration 53544, loss = 1.90965940\n",
      "Iteration 53545, loss = 2.30332597\n",
      "Iteration 53546, loss = 1.57341208\n",
      "Iteration 53547, loss = 1.71984645\n",
      "Iteration 53548, loss = 1.99133395\n",
      "Iteration 53549, loss = 2.60858209\n",
      "Iteration 53550, loss = 2.36282677\n",
      "Iteration 53551, loss = 1.55514899\n",
      "Iteration 53552, loss = 1.34593177\n",
      "Iteration 53553, loss = 1.57657802\n",
      "Iteration 53554, loss = 1.34004315\n",
      "Iteration 53555, loss = 1.63706567\n",
      "Iteration 53556, loss = 1.56501656\n",
      "Iteration 53557, loss = 2.10010909\n",
      "Iteration 53558, loss = 1.97363857\n",
      "Iteration 53559, loss = 1.89352716\n",
      "Iteration 53560, loss = 2.20845672\n",
      "Iteration 53561, loss = 2.00303794\n",
      "Iteration 53562, loss = 2.45876364\n",
      "Iteration 53563, loss = 3.05452795\n",
      "Iteration 53564, loss = 2.57543010\n",
      "Iteration 53565, loss = 1.98904713\n",
      "Iteration 53566, loss = 1.86293246\n",
      "Iteration 53567, loss = 1.93423115\n",
      "Iteration 53568, loss = 2.72937182\n",
      "Iteration 53569, loss = 4.16066934\n",
      "Iteration 53570, loss = 2.87813372\n",
      "Iteration 53571, loss = 3.19577931\n",
      "Iteration 53572, loss = 2.18665718\n",
      "Iteration 53573, loss = 2.60323103\n",
      "Iteration 53574, loss = 2.74226463\n",
      "Iteration 53575, loss = 3.67308972\n",
      "Iteration 53576, loss = 2.16423850\n",
      "Iteration 53577, loss = 2.14651463\n",
      "Iteration 53578, loss = 2.37170302\n",
      "Iteration 53579, loss = 1.68836721\n",
      "Iteration 53580, loss = 2.13755717\n",
      "Iteration 53581, loss = 2.02553118\n",
      "Iteration 53582, loss = 2.44174786\n",
      "Iteration 53583, loss = 2.13867531\n",
      "Iteration 53584, loss = 1.60507767\n",
      "Iteration 53585, loss = 1.71452985\n",
      "Iteration 53586, loss = 1.80303192\n",
      "Iteration 53587, loss = 1.86405059\n",
      "Iteration 53588, loss = 2.04516843\n",
      "Iteration 53589, loss = 1.71182099\n",
      "Iteration 53590, loss = 2.22540582\n",
      "Iteration 53591, loss = 2.82113694\n",
      "Iteration 53592, loss = 1.71594359\n",
      "Iteration 53593, loss = 1.69767382\n",
      "Iteration 53594, loss = 1.73564583\n",
      "Iteration 53595, loss = 1.64751596\n",
      "Iteration 53596, loss = 1.57707111\n",
      "Iteration 53597, loss = 1.68143756\n",
      "Iteration 53598, loss = 1.60648526\n",
      "Iteration 53599, loss = 1.45474808\n",
      "Iteration 53600, loss = 1.99741055\n",
      "Iteration 53601, loss = 2.18379332\n",
      "Iteration 53602, loss = 1.82779039\n",
      "Iteration 53603, loss = 1.62417919\n",
      "Iteration 53604, loss = 1.56863177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 53605, loss = 1.69621551\n",
      "Iteration 53606, loss = 1.42828149\n",
      "Iteration 53607, loss = 1.49024387\n",
      "Iteration 53608, loss = 1.46594224\n",
      "Iteration 53609, loss = 1.54504337\n",
      "Iteration 53610, loss = 1.59146153\n",
      "Iteration 53611, loss = 1.45811875\n",
      "Iteration 53612, loss = 1.67444635\n",
      "Iteration 53613, loss = 1.68531935\n",
      "Iteration 53614, loss = 1.49092189\n",
      "Iteration 53615, loss = 1.44474770\n",
      "Iteration 53616, loss = 1.42270166\n",
      "Iteration 53617, loss = 1.63717530\n",
      "Iteration 53618, loss = 1.99187478\n",
      "Iteration 53619, loss = 2.07935074\n",
      "Iteration 53620, loss = 1.68689597\n",
      "Iteration 53621, loss = 1.79320331\n",
      "Iteration 53622, loss = 1.53139714\n",
      "Iteration 53623, loss = 1.87896299\n",
      "Iteration 53624, loss = 1.86795997\n",
      "Iteration 53625, loss = 1.52326825\n",
      "Iteration 53626, loss = 1.60171521\n",
      "Iteration 53627, loss = 1.73814135\n",
      "Iteration 53628, loss = 1.77225631\n",
      "Iteration 53629, loss = 1.67222021\n",
      "Iteration 53630, loss = 1.71718310\n",
      "Iteration 53631, loss = 1.67410316\n",
      "Iteration 53632, loss = 1.59646930\n",
      "Iteration 53633, loss = 1.62754654\n",
      "Iteration 53634, loss = 1.51110320\n",
      "Iteration 53635, loss = 1.58440683\n",
      "Iteration 53636, loss = 1.44026174\n",
      "Iteration 53637, loss = 1.30068030\n",
      "Iteration 53638, loss = 1.52860419\n",
      "Iteration 53639, loss = 1.68180789\n",
      "Iteration 53640, loss = 1.65564736\n",
      "Iteration 53641, loss = 1.61039764\n",
      "Iteration 53642, loss = 1.63010715\n",
      "Iteration 53643, loss = 2.03052744\n",
      "Iteration 53644, loss = 1.68469097\n",
      "Iteration 53645, loss = 1.50914573\n",
      "Iteration 53646, loss = 1.49587278\n",
      "Iteration 53647, loss = 1.67593558\n",
      "Iteration 53648, loss = 1.92331145\n",
      "Iteration 53649, loss = 1.93330779\n",
      "Iteration 53650, loss = 1.84501341\n",
      "Iteration 53651, loss = 1.47569086\n",
      "Iteration 53652, loss = 1.70170885\n",
      "Iteration 53653, loss = 1.54314675\n",
      "Iteration 53654, loss = 1.81118752\n",
      "Iteration 53655, loss = 1.62110213\n",
      "Iteration 53656, loss = 1.76392136\n",
      "Iteration 53657, loss = 1.77084935\n",
      "Iteration 53658, loss = 1.47899042\n",
      "Iteration 53659, loss = 1.71042466\n",
      "Iteration 53660, loss = 1.51951723\n",
      "Iteration 53661, loss = 1.95729175\n",
      "Iteration 53662, loss = 1.66163577\n",
      "Iteration 53663, loss = 1.48747135\n",
      "Iteration 53664, loss = 1.46278750\n",
      "Iteration 53665, loss = 1.94097639\n",
      "Iteration 53666, loss = 1.87851575\n",
      "Iteration 53667, loss = 1.52173653\n",
      "Iteration 53668, loss = 1.60595910\n",
      "Iteration 53669, loss = 1.54553791\n",
      "Iteration 53670, loss = 1.46568868\n",
      "Iteration 53671, loss = 1.71421892\n",
      "Iteration 53672, loss = 2.13335272\n",
      "Iteration 53673, loss = 2.04388138\n",
      "Iteration 53674, loss = 1.90365158\n",
      "Iteration 53675, loss = 1.78223311\n",
      "Iteration 53676, loss = 2.58478119\n",
      "Iteration 53677, loss = 2.54381605\n",
      "Iteration 53678, loss = 2.50915040\n",
      "Iteration 53679, loss = 2.44993997\n",
      "Iteration 53680, loss = 1.74516765\n",
      "Iteration 53681, loss = 1.63475313\n",
      "Iteration 53682, loss = 1.48575491\n",
      "Iteration 53683, loss = 1.56201754\n",
      "Iteration 53684, loss = 1.63976749\n",
      "Iteration 53685, loss = 1.48944528\n",
      "Iteration 53686, loss = 2.06312966\n",
      "Iteration 53687, loss = 1.56835854\n",
      "Iteration 53688, loss = 1.59339177\n",
      "Iteration 53689, loss = 1.73428612\n",
      "Iteration 53690, loss = 1.57282342\n",
      "Iteration 53691, loss = 1.38380039\n",
      "Iteration 53692, loss = 1.47219710\n",
      "Iteration 53693, loss = 1.30571664\n",
      "Iteration 53694, loss = 1.50756163\n",
      "Iteration 53695, loss = 1.61043148\n",
      "Iteration 53696, loss = 1.70348731\n",
      "Iteration 53697, loss = 1.48783273\n",
      "Iteration 53698, loss = 1.39599893\n",
      "Iteration 53699, loss = 1.50728847\n",
      "Iteration 53700, loss = 1.87963026\n",
      "Iteration 53701, loss = 1.81185149\n",
      "Iteration 53702, loss = 1.51307313\n",
      "Iteration 53703, loss = 1.63205767\n",
      "Iteration 53704, loss = 1.90188423\n",
      "Iteration 53705, loss = 1.67788823\n",
      "Iteration 53706, loss = 2.05267086\n",
      "Iteration 53707, loss = 1.59954813\n",
      "Iteration 53708, loss = 2.02796099\n",
      "Iteration 53709, loss = 1.81833541\n",
      "Iteration 53710, loss = 1.85933084\n",
      "Iteration 53711, loss = 1.52541353\n",
      "Iteration 53712, loss = 1.48079010\n",
      "Iteration 53713, loss = 1.39230332\n",
      "Iteration 53714, loss = 1.62834033\n",
      "Iteration 53715, loss = 1.73592364\n",
      "Iteration 53716, loss = 1.85328573\n",
      "Iteration 53717, loss = 1.61526879\n",
      "Iteration 53718, loss = 1.71152201\n",
      "Iteration 53719, loss = 1.78807870\n",
      "Iteration 53720, loss = 1.68750363\n",
      "Iteration 53721, loss = 1.75762808\n",
      "Iteration 53722, loss = 1.63571575\n",
      "Iteration 53723, loss = 1.89865957\n",
      "Iteration 53724, loss = 1.63862652\n",
      "Iteration 53725, loss = 1.44167433\n",
      "Iteration 53726, loss = 1.52197688\n",
      "Iteration 53727, loss = 1.44927793\n",
      "Iteration 53728, loss = 1.59713086\n",
      "Iteration 53729, loss = 1.38384377\n",
      "Iteration 53730, loss = 1.57098023\n",
      "Iteration 53731, loss = 1.36463362\n",
      "Iteration 53732, loss = 1.55788295\n",
      "Iteration 53733, loss = 1.68704037\n",
      "Iteration 53734, loss = 1.32642178\n",
      "Iteration 53735, loss = 1.31858228\n",
      "Iteration 53736, loss = 1.41428278\n",
      "Iteration 53737, loss = 1.37526455\n",
      "Iteration 53738, loss = 1.30308220\n",
      "Iteration 53739, loss = 1.97526746\n",
      "Iteration 53740, loss = 1.76745985\n",
      "Iteration 53741, loss = 1.98659009\n",
      "Iteration 53742, loss = 2.33107284\n",
      "Iteration 53743, loss = 1.55118977\n",
      "Iteration 53744, loss = 1.64456129\n",
      "Iteration 53745, loss = 1.80909629\n",
      "Iteration 53746, loss = 2.27811940\n",
      "Iteration 53747, loss = 1.58394298\n",
      "Iteration 53748, loss = 1.79433220\n",
      "Iteration 53749, loss = 1.76948025\n",
      "Iteration 53750, loss = 2.14686599\n",
      "Iteration 53751, loss = 2.27440921\n",
      "Iteration 53752, loss = 2.14625006\n",
      "Iteration 53753, loss = 2.25829001\n",
      "Iteration 53754, loss = 1.74679199\n",
      "Iteration 53755, loss = 1.47671158\n",
      "Iteration 53756, loss = 1.56304781\n",
      "Iteration 53757, loss = 1.44532588\n",
      "Iteration 53758, loss = 1.48505018\n",
      "Iteration 53759, loss = 1.79749851\n",
      "Iteration 53760, loss = 1.78619984\n",
      "Iteration 53761, loss = 1.96366149\n",
      "Iteration 53762, loss = 2.42124493\n",
      "Iteration 53763, loss = 1.65004586\n",
      "Iteration 53764, loss = 1.80426319\n",
      "Iteration 53765, loss = 1.93318486\n",
      "Iteration 53766, loss = 2.14244507\n",
      "Iteration 53767, loss = 3.02148090\n",
      "Iteration 53768, loss = 2.03560689\n",
      "Iteration 53769, loss = 1.85194483\n",
      "Iteration 53770, loss = 1.47298329\n",
      "Iteration 53771, loss = 1.88209336\n",
      "Iteration 53772, loss = 1.67220094\n",
      "Iteration 53773, loss = 1.63853828\n",
      "Iteration 53774, loss = 1.57408912\n",
      "Iteration 53775, loss = 1.29816062\n",
      "Iteration 53776, loss = 1.70202851\n",
      "Iteration 53777, loss = 2.05200720\n",
      "Iteration 53778, loss = 2.02979882\n",
      "Iteration 53779, loss = 1.66519567\n",
      "Iteration 53780, loss = 1.57331322\n",
      "Iteration 53781, loss = 1.42819408\n",
      "Iteration 53782, loss = 1.41649668\n",
      "Iteration 53783, loss = 1.66876288\n",
      "Iteration 53784, loss = 1.61357873\n",
      "Iteration 53785, loss = 1.50476457\n",
      "Iteration 53786, loss = 1.54166575\n",
      "Iteration 53787, loss = 1.67802790\n",
      "Iteration 53788, loss = 1.59368976\n",
      "Iteration 53789, loss = 1.30467586\n",
      "Iteration 53790, loss = 1.62522010\n",
      "Iteration 53791, loss = 1.61194946\n",
      "Iteration 53792, loss = 1.64310138\n",
      "Iteration 53793, loss = 1.38693893\n",
      "Iteration 53794, loss = 1.34610161\n",
      "Iteration 53795, loss = 1.71249386\n",
      "Iteration 53796, loss = 1.53252132\n",
      "Iteration 53797, loss = 1.61239287\n",
      "Iteration 53798, loss = 1.63087989\n",
      "Iteration 53799, loss = 2.43673468\n",
      "Iteration 53800, loss = 1.95452544\n",
      "Iteration 53801, loss = 1.75371621\n",
      "Iteration 53802, loss = 1.95108165\n",
      "Iteration 53803, loss = 2.10871515\n",
      "Iteration 53804, loss = 1.71155544\n",
      "Iteration 53805, loss = 2.01624061\n",
      "Iteration 53806, loss = 2.95062491\n",
      "Iteration 53807, loss = 2.36461778\n",
      "Iteration 53808, loss = 3.26507832\n",
      "Iteration 53809, loss = 2.81611789\n",
      "Iteration 53810, loss = 2.40232353\n",
      "Iteration 53811, loss = 2.22960842\n",
      "Iteration 53812, loss = 2.06292519\n",
      "Iteration 53813, loss = 1.52284702\n",
      "Iteration 53814, loss = 1.53231677\n",
      "Iteration 53815, loss = 1.42459387\n",
      "Iteration 53816, loss = 1.32362511\n",
      "Iteration 53817, loss = 1.45478307\n",
      "Iteration 53818, loss = 1.43973237\n",
      "Iteration 53819, loss = 1.72239215\n",
      "Iteration 53820, loss = 1.63830509\n",
      "Iteration 53821, loss = 1.55424290\n",
      "Iteration 53822, loss = 1.46629034\n",
      "Iteration 53823, loss = 1.31757656\n",
      "Iteration 53824, loss = 1.52477740\n",
      "Iteration 53825, loss = 1.45715643\n",
      "Iteration 53826, loss = 1.36792215\n",
      "Iteration 53827, loss = 1.23825638\n",
      "Iteration 53828, loss = 1.47800928\n",
      "Iteration 53829, loss = 1.35089509\n",
      "Iteration 53830, loss = 1.34688848\n",
      "Iteration 53831, loss = 1.44307242\n",
      "Iteration 53832, loss = 1.42915573\n",
      "Iteration 53833, loss = 1.87636245\n",
      "Iteration 53834, loss = 1.93555195\n",
      "Iteration 53835, loss = 1.92586212\n",
      "Iteration 53836, loss = 1.65849030\n",
      "Iteration 53837, loss = 1.99607653\n",
      "Iteration 53838, loss = 1.94252390\n",
      "Iteration 53839, loss = 1.64920328\n",
      "Iteration 53840, loss = 1.41105224\n",
      "Iteration 53841, loss = 1.41617172\n",
      "Iteration 53842, loss = 1.42846883\n",
      "Iteration 53843, loss = 1.39278978\n",
      "Iteration 53844, loss = 1.27003544\n",
      "Iteration 53845, loss = 1.35192477\n",
      "Iteration 53846, loss = 1.31382802\n",
      "Iteration 53847, loss = 1.52194248\n",
      "Iteration 53848, loss = 1.71091129\n",
      "Iteration 53849, loss = 1.75093115\n",
      "Iteration 53850, loss = 1.91178800\n",
      "Iteration 53851, loss = 2.61465256\n",
      "Iteration 53852, loss = 2.35457660\n",
      "Iteration 53853, loss = 3.29153902\n",
      "Iteration 53854, loss = 2.39300475\n",
      "Iteration 53855, loss = 1.79107993\n",
      "Iteration 53856, loss = 1.82675265\n",
      "Iteration 53857, loss = 1.66158352\n",
      "Iteration 53858, loss = 1.78328865\n",
      "Iteration 53859, loss = 1.97861851\n",
      "Iteration 53860, loss = 1.60668085\n",
      "Iteration 53861, loss = 1.81679815\n",
      "Iteration 53862, loss = 2.17450737\n",
      "Iteration 53863, loss = 2.01182514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 53864, loss = 1.63835393\n",
      "Iteration 53865, loss = 1.48669472\n",
      "Iteration 53866, loss = 1.66918859\n",
      "Iteration 53867, loss = 1.67496650\n",
      "Iteration 53868, loss = 1.73799509\n",
      "Iteration 53869, loss = 1.77434378\n",
      "Iteration 53870, loss = 1.68666507\n",
      "Iteration 53871, loss = 1.34456848\n",
      "Iteration 53872, loss = 1.56654448\n",
      "Iteration 53873, loss = 1.39380345\n",
      "Iteration 53874, loss = 1.31545399\n",
      "Iteration 53875, loss = 1.40928741\n",
      "Iteration 53876, loss = 1.31128191\n",
      "Iteration 53877, loss = 1.38892948\n",
      "Iteration 53878, loss = 1.42129530\n",
      "Iteration 53879, loss = 1.66797375\n",
      "Iteration 53880, loss = 1.41939510\n",
      "Iteration 53881, loss = 1.33039439\n",
      "Iteration 53882, loss = 1.64663186\n",
      "Iteration 53883, loss = 1.64720212\n",
      "Iteration 53884, loss = 1.61718745\n",
      "Iteration 53885, loss = 1.73074036\n",
      "Iteration 53886, loss = 1.71585426\n",
      "Iteration 53887, loss = 1.78201176\n",
      "Iteration 53888, loss = 1.50678713\n",
      "Iteration 53889, loss = 1.85968443\n",
      "Iteration 53890, loss = 1.50077557\n",
      "Iteration 53891, loss = 1.31025979\n",
      "Iteration 53892, loss = 1.38919341\n",
      "Iteration 53893, loss = 1.67278903\n",
      "Iteration 53894, loss = 1.75180862\n",
      "Iteration 53895, loss = 1.62771871\n",
      "Iteration 53896, loss = 1.64669698\n",
      "Iteration 53897, loss = 1.66831776\n",
      "Iteration 53898, loss = 1.49136655\n",
      "Iteration 53899, loss = 1.64006904\n",
      "Iteration 53900, loss = 1.57811839\n",
      "Iteration 53901, loss = 1.83416680\n",
      "Iteration 53902, loss = 2.08257006\n",
      "Iteration 53903, loss = 2.06396836\n",
      "Iteration 53904, loss = 2.03409386\n",
      "Iteration 53905, loss = 2.03429564\n",
      "Iteration 53906, loss = 2.12890617\n",
      "Iteration 53907, loss = 2.31100831\n",
      "Iteration 53908, loss = 2.44460410\n",
      "Iteration 53909, loss = 2.21402106\n",
      "Iteration 53910, loss = 2.22250487\n",
      "Iteration 53911, loss = 1.45952603\n",
      "Iteration 53912, loss = 1.36450868\n",
      "Iteration 53913, loss = 1.48599551\n",
      "Iteration 53914, loss = 1.48480210\n",
      "Iteration 53915, loss = 1.44552017\n",
      "Iteration 53916, loss = 1.41679680\n",
      "Iteration 53917, loss = 1.35398277\n",
      "Iteration 53918, loss = 1.44375383\n",
      "Iteration 53919, loss = 1.40833735\n",
      "Iteration 53920, loss = 1.72110040\n",
      "Iteration 53921, loss = 1.79735600\n",
      "Iteration 53922, loss = 2.15041044\n",
      "Iteration 53923, loss = 1.93190703\n",
      "Iteration 53924, loss = 1.48400081\n",
      "Iteration 53925, loss = 1.35104217\n",
      "Iteration 53926, loss = 1.50228148\n",
      "Iteration 53927, loss = 1.77246654\n",
      "Iteration 53928, loss = 1.81186142\n",
      "Iteration 53929, loss = 2.42818732\n",
      "Iteration 53930, loss = 2.27072319\n",
      "Iteration 53931, loss = 1.92431634\n",
      "Iteration 53932, loss = 3.36957764\n",
      "Iteration 53933, loss = 2.74367374\n",
      "Iteration 53934, loss = 2.48818052\n",
      "Iteration 53935, loss = 1.92229706\n",
      "Iteration 53936, loss = 2.44677480\n",
      "Iteration 53937, loss = 2.16718850\n",
      "Iteration 53938, loss = 2.04276221\n",
      "Iteration 53939, loss = 2.20819165\n",
      "Iteration 53940, loss = 2.07948111\n",
      "Iteration 53941, loss = 1.95521457\n",
      "Iteration 53942, loss = 1.52670621\n",
      "Iteration 53943, loss = 1.41702681\n",
      "Iteration 53944, loss = 1.44039893\n",
      "Iteration 53945, loss = 1.46280796\n",
      "Iteration 53946, loss = 1.32179277\n",
      "Iteration 53947, loss = 1.31111689\n",
      "Iteration 53948, loss = 1.32315280\n",
      "Iteration 53949, loss = 1.42373700\n",
      "Iteration 53950, loss = 1.43518366\n",
      "Iteration 53951, loss = 1.47281578\n",
      "Iteration 53952, loss = 1.57555677\n",
      "Iteration 53953, loss = 1.48540748\n",
      "Iteration 53954, loss = 1.33827435\n",
      "Iteration 53955, loss = 1.54298314\n",
      "Iteration 53956, loss = 1.51761440\n",
      "Iteration 53957, loss = 2.26414444\n",
      "Iteration 53958, loss = 1.98992306\n",
      "Iteration 53959, loss = 2.22159976\n",
      "Iteration 53960, loss = 1.84620046\n",
      "Iteration 53961, loss = 2.05131492\n",
      "Iteration 53962, loss = 2.54995655\n",
      "Iteration 53963, loss = 2.52332080\n",
      "Iteration 53964, loss = 2.77203751\n",
      "Iteration 53965, loss = 2.04880099\n",
      "Iteration 53966, loss = 1.79554149\n",
      "Iteration 53967, loss = 1.67398989\n",
      "Iteration 53968, loss = 1.48630438\n",
      "Iteration 53969, loss = 1.42258169\n",
      "Iteration 53970, loss = 1.42302279\n",
      "Iteration 53971, loss = 1.75487981\n",
      "Iteration 53972, loss = 1.36530130\n",
      "Iteration 53973, loss = 1.48643677\n",
      "Iteration 53974, loss = 1.30840112\n",
      "Iteration 53975, loss = 1.47932169\n",
      "Iteration 53976, loss = 1.49631413\n",
      "Iteration 53977, loss = 1.50780249\n",
      "Iteration 53978, loss = 1.34188335\n",
      "Iteration 53979, loss = 1.45615785\n",
      "Iteration 53980, loss = 1.92159078\n",
      "Iteration 53981, loss = 1.77739108\n",
      "Iteration 53982, loss = 1.87917879\n",
      "Iteration 53983, loss = 1.74375429\n",
      "Iteration 53984, loss = 1.84814202\n",
      "Iteration 53985, loss = 2.19971930\n",
      "Iteration 53986, loss = 2.20090160\n",
      "Iteration 53987, loss = 2.07961985\n",
      "Iteration 53988, loss = 2.02153349\n",
      "Iteration 53989, loss = 2.01445343\n",
      "Iteration 53990, loss = 1.89581199\n",
      "Iteration 53991, loss = 1.87254774\n",
      "Iteration 53992, loss = 1.79230407\n",
      "Iteration 53993, loss = 1.97619576\n",
      "Iteration 53994, loss = 1.81826110\n",
      "Iteration 53995, loss = 1.88413202\n",
      "Iteration 53996, loss = 2.25509526\n",
      "Iteration 53997, loss = 2.76406748\n",
      "Iteration 53998, loss = 2.64636283\n",
      "Iteration 53999, loss = 2.35893780\n",
      "Iteration 54000, loss = 2.31348900\n",
      "Iteration 54001, loss = 2.03143331\n",
      "Iteration 54002, loss = 2.45021315\n",
      "Iteration 54003, loss = 2.83240549\n",
      "Iteration 54004, loss = 1.95942897\n",
      "Iteration 54005, loss = 2.05001081\n",
      "Iteration 54006, loss = 1.77884782\n",
      "Iteration 54007, loss = 1.38493412\n",
      "Iteration 54008, loss = 1.47955541\n",
      "Iteration 54009, loss = 1.45671404\n",
      "Iteration 54010, loss = 1.35091249\n",
      "Iteration 54011, loss = 1.27044569\n",
      "Iteration 54012, loss = 1.72558540\n",
      "Iteration 54013, loss = 2.28998879\n",
      "Iteration 54014, loss = 1.99068225\n",
      "Iteration 54015, loss = 1.85818666\n",
      "Iteration 54016, loss = 2.03507923\n",
      "Iteration 54017, loss = 2.68729146\n",
      "Iteration 54018, loss = 2.80811068\n",
      "Iteration 54019, loss = 1.67702864\n",
      "Iteration 54020, loss = 2.08792630\n",
      "Iteration 54021, loss = 2.42606073\n",
      "Iteration 54022, loss = 2.65005330\n",
      "Iteration 54023, loss = 2.06684891\n",
      "Iteration 54024, loss = 2.29328068\n",
      "Iteration 54025, loss = 2.35541764\n",
      "Iteration 54026, loss = 2.21434202\n",
      "Iteration 54027, loss = 1.87889225\n",
      "Iteration 54028, loss = 1.77372829\n",
      "Iteration 54029, loss = 2.52816426\n",
      "Iteration 54030, loss = 2.50336495\n",
      "Iteration 54031, loss = 2.21628799\n",
      "Iteration 54032, loss = 2.48264604\n",
      "Iteration 54033, loss = 2.55730829\n",
      "Iteration 54034, loss = 2.45406063\n",
      "Iteration 54035, loss = 2.97795844\n",
      "Iteration 54036, loss = 2.20871455\n",
      "Iteration 54037, loss = 1.91594815\n",
      "Iteration 54038, loss = 1.72825611\n",
      "Iteration 54039, loss = 1.86411767\n",
      "Iteration 54040, loss = 1.82839515\n",
      "Iteration 54041, loss = 2.20167123\n",
      "Iteration 54042, loss = 1.89164592\n",
      "Iteration 54043, loss = 2.07392285\n",
      "Iteration 54044, loss = 1.57413170\n",
      "Iteration 54045, loss = 1.52400476\n",
      "Iteration 54046, loss = 1.52086779\n",
      "Iteration 54047, loss = 1.46676128\n",
      "Iteration 54048, loss = 1.77816161\n",
      "Iteration 54049, loss = 1.55615201\n",
      "Iteration 54050, loss = 1.79217154\n",
      "Iteration 54051, loss = 1.86822316\n",
      "Iteration 54052, loss = 1.66725873\n",
      "Iteration 54053, loss = 1.48432734\n",
      "Iteration 54054, loss = 1.72681826\n",
      "Iteration 54055, loss = 1.78287506\n",
      "Iteration 54056, loss = 1.76152989\n",
      "Iteration 54057, loss = 1.94448228\n",
      "Iteration 54058, loss = 1.96208560\n",
      "Iteration 54059, loss = 2.07889901\n",
      "Iteration 54060, loss = 1.66255238\n",
      "Iteration 54061, loss = 2.16438956\n",
      "Iteration 54062, loss = 2.50935805\n",
      "Iteration 54063, loss = 1.88840586\n",
      "Iteration 54064, loss = 2.16785363\n",
      "Iteration 54065, loss = 1.72946913\n",
      "Iteration 54066, loss = 1.80586464\n",
      "Iteration 54067, loss = 1.90330350\n",
      "Iteration 54068, loss = 2.14454766\n",
      "Iteration 54069, loss = 1.56442186\n",
      "Iteration 54070, loss = 1.45351789\n",
      "Iteration 54071, loss = 1.65780100\n",
      "Iteration 54072, loss = 1.66593981\n",
      "Iteration 54073, loss = 1.82049350\n",
      "Iteration 54074, loss = 2.27470632\n",
      "Iteration 54075, loss = 1.80885119\n",
      "Iteration 54076, loss = 1.43478772\n",
      "Iteration 54077, loss = 1.64027956\n",
      "Iteration 54078, loss = 1.64557904\n",
      "Iteration 54079, loss = 1.50836691\n",
      "Iteration 54080, loss = 1.77632413\n",
      "Iteration 54081, loss = 1.49485255\n",
      "Iteration 54082, loss = 1.86156802\n",
      "Iteration 54083, loss = 2.02232245\n",
      "Iteration 54084, loss = 1.92829231\n",
      "Iteration 54085, loss = 1.64770965\n",
      "Iteration 54086, loss = 2.17631129\n",
      "Iteration 54087, loss = 1.81381859\n",
      "Iteration 54088, loss = 1.71066063\n",
      "Iteration 54089, loss = 1.63076489\n",
      "Iteration 54090, loss = 1.50322166\n",
      "Iteration 54091, loss = 1.25148883\n",
      "Iteration 54092, loss = 1.41948618\n",
      "Iteration 54093, loss = 1.59765086\n",
      "Iteration 54094, loss = 1.56507563\n",
      "Iteration 54095, loss = 1.69467397\n",
      "Iteration 54096, loss = 1.71521297\n",
      "Iteration 54097, loss = 1.65219223\n",
      "Iteration 54098, loss = 1.47180752\n",
      "Iteration 54099, loss = 1.35054980\n",
      "Iteration 54100, loss = 1.41984999\n",
      "Iteration 54101, loss = 1.36855481\n",
      "Iteration 54102, loss = 1.34836485\n",
      "Iteration 54103, loss = 1.39858928\n",
      "Iteration 54104, loss = 1.35285909\n",
      "Iteration 54105, loss = 1.28065019\n",
      "Iteration 54106, loss = 1.32517880\n",
      "Iteration 54107, loss = 1.34143389\n",
      "Iteration 54108, loss = 1.63233795\n",
      "Iteration 54109, loss = 1.54937813\n",
      "Iteration 54110, loss = 1.44738180\n",
      "Iteration 54111, loss = 1.37782052\n",
      "Iteration 54112, loss = 1.37730209\n",
      "Iteration 54113, loss = 1.36940501\n",
      "Iteration 54114, loss = 1.45119515\n",
      "Iteration 54115, loss = 1.66986108\n",
      "Iteration 54116, loss = 1.87869346\n",
      "Iteration 54117, loss = 2.02998077\n",
      "Iteration 54118, loss = 1.70749522\n",
      "Iteration 54119, loss = 1.62056938\n",
      "Iteration 54120, loss = 1.76031308\n",
      "Iteration 54121, loss = 1.80865704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 54122, loss = 2.07358808\n",
      "Iteration 54123, loss = 1.61188719\n",
      "Iteration 54124, loss = 1.95192441\n",
      "Iteration 54125, loss = 1.84235768\n",
      "Iteration 54126, loss = 1.59758302\n",
      "Iteration 54127, loss = 1.58557100\n",
      "Iteration 54128, loss = 1.97660921\n",
      "Iteration 54129, loss = 2.40302132\n",
      "Iteration 54130, loss = 1.91663181\n",
      "Iteration 54131, loss = 1.78182766\n",
      "Iteration 54132, loss = 1.93967583\n",
      "Iteration 54133, loss = 1.79538387\n",
      "Iteration 54134, loss = 1.81900206\n",
      "Iteration 54135, loss = 1.46452753\n",
      "Iteration 54136, loss = 1.94078467\n",
      "Iteration 54137, loss = 2.82556975\n",
      "Iteration 54138, loss = 2.47618567\n",
      "Iteration 54139, loss = 1.91010887\n",
      "Iteration 54140, loss = 2.32110672\n",
      "Iteration 54141, loss = 2.60576275\n",
      "Iteration 54142, loss = 1.72838859\n",
      "Iteration 54143, loss = 2.27330429\n",
      "Iteration 54144, loss = 1.57042782\n",
      "Iteration 54145, loss = 1.35668435\n",
      "Iteration 54146, loss = 1.72809467\n",
      "Iteration 54147, loss = 1.68908657\n",
      "Iteration 54148, loss = 1.50666023\n",
      "Iteration 54149, loss = 1.56850025\n",
      "Iteration 54150, loss = 1.60321135\n",
      "Iteration 54151, loss = 1.49975502\n",
      "Iteration 54152, loss = 1.44886640\n",
      "Iteration 54153, loss = 1.40132697\n",
      "Iteration 54154, loss = 1.60560276\n",
      "Iteration 54155, loss = 1.56473286\n",
      "Iteration 54156, loss = 1.31026117\n",
      "Iteration 54157, loss = 1.37919646\n",
      "Iteration 54158, loss = 1.40480779\n",
      "Iteration 54159, loss = 1.41126560\n",
      "Iteration 54160, loss = 1.50566268\n",
      "Iteration 54161, loss = 1.52317234\n",
      "Iteration 54162, loss = 1.41980952\n",
      "Iteration 54163, loss = 1.44299338\n",
      "Iteration 54164, loss = 1.28918284\n",
      "Iteration 54165, loss = 1.34720810\n",
      "Iteration 54166, loss = 1.38344922\n",
      "Iteration 54167, loss = 1.44008959\n",
      "Iteration 54168, loss = 1.41503105\n",
      "Iteration 54169, loss = 1.44922111\n",
      "Iteration 54170, loss = 1.30137165\n",
      "Iteration 54171, loss = 1.26202769\n",
      "Iteration 54172, loss = 1.37885723\n",
      "Iteration 54173, loss = 1.33135476\n",
      "Iteration 54174, loss = 1.43522360\n",
      "Iteration 54175, loss = 1.60738260\n",
      "Iteration 54176, loss = 1.91909995\n",
      "Iteration 54177, loss = 1.97761377\n",
      "Iteration 54178, loss = 2.05030192\n",
      "Iteration 54179, loss = 2.06653236\n",
      "Iteration 54180, loss = 1.82349248\n",
      "Iteration 54181, loss = 1.45311934\n",
      "Iteration 54182, loss = 1.33968283\n",
      "Iteration 54183, loss = 1.32011690\n",
      "Iteration 54184, loss = 1.47167812\n",
      "Iteration 54185, loss = 2.55239565\n",
      "Iteration 54186, loss = 2.91331838\n",
      "Iteration 54187, loss = 2.97906467\n",
      "Iteration 54188, loss = 2.28858237\n",
      "Iteration 54189, loss = 2.63655253\n",
      "Iteration 54190, loss = 3.23556684\n",
      "Iteration 54191, loss = 2.89439144\n",
      "Iteration 54192, loss = 2.15498865\n",
      "Iteration 54193, loss = 1.84443397\n",
      "Iteration 54194, loss = 1.44970879\n",
      "Iteration 54195, loss = 1.44417382\n",
      "Iteration 54196, loss = 1.51764460\n",
      "Iteration 54197, loss = 1.33012816\n",
      "Iteration 54198, loss = 1.36884372\n",
      "Iteration 54199, loss = 1.30370778\n",
      "Iteration 54200, loss = 1.27908495\n",
      "Iteration 54201, loss = 1.45218798\n",
      "Iteration 54202, loss = 1.47038302\n",
      "Iteration 54203, loss = 1.42221972\n",
      "Iteration 54204, loss = 1.56853783\n",
      "Iteration 54205, loss = 1.48101853\n",
      "Iteration 54206, loss = 1.50382491\n",
      "Iteration 54207, loss = 1.48933550\n",
      "Iteration 54208, loss = 1.58103953\n",
      "Iteration 54209, loss = 1.96148798\n",
      "Iteration 54210, loss = 1.37390219\n",
      "Iteration 54211, loss = 1.36198368\n",
      "Iteration 54212, loss = 1.33045554\n",
      "Iteration 54213, loss = 1.63471626\n",
      "Iteration 54214, loss = 2.25964966\n",
      "Iteration 54215, loss = 1.81764456\n",
      "Iteration 54216, loss = 1.75979486\n",
      "Iteration 54217, loss = 1.92117613\n",
      "Iteration 54218, loss = 1.60065291\n",
      "Iteration 54219, loss = 1.31797382\n",
      "Iteration 54220, loss = 1.32138606\n",
      "Iteration 54221, loss = 1.33476545\n",
      "Iteration 54222, loss = 1.43080129\n",
      "Iteration 54223, loss = 1.51743890\n",
      "Iteration 54224, loss = 1.52779514\n",
      "Iteration 54225, loss = 1.84603632\n",
      "Iteration 54226, loss = 1.54257845\n",
      "Iteration 54227, loss = 1.74153835\n",
      "Iteration 54228, loss = 1.53654117\n",
      "Iteration 54229, loss = 1.44088999\n",
      "Iteration 54230, loss = 1.35007231\n",
      "Iteration 54231, loss = 1.34688305\n",
      "Iteration 54232, loss = 1.29783087\n",
      "Iteration 54233, loss = 1.40617011\n",
      "Iteration 54234, loss = 1.48415969\n",
      "Iteration 54235, loss = 1.70959060\n",
      "Iteration 54236, loss = 2.03438987\n",
      "Iteration 54237, loss = 2.40936774\n",
      "Iteration 54238, loss = 2.05929588\n",
      "Iteration 54239, loss = 2.08088491\n",
      "Iteration 54240, loss = 2.05828038\n",
      "Iteration 54241, loss = 1.97375611\n",
      "Iteration 54242, loss = 2.08580146\n",
      "Iteration 54243, loss = 1.73940910\n",
      "Iteration 54244, loss = 2.24066900\n",
      "Iteration 54245, loss = 1.75245247\n",
      "Iteration 54246, loss = 2.19147488\n",
      "Iteration 54247, loss = 1.67645907\n",
      "Iteration 54248, loss = 1.69557371\n",
      "Iteration 54249, loss = 1.48979587\n",
      "Iteration 54250, loss = 1.54855655\n",
      "Iteration 54251, loss = 1.53910607\n",
      "Iteration 54252, loss = 1.72443107\n",
      "Iteration 54253, loss = 1.65125404\n",
      "Iteration 54254, loss = 1.45967183\n",
      "Iteration 54255, loss = 1.74101032\n",
      "Iteration 54256, loss = 1.46360076\n",
      "Iteration 54257, loss = 1.39445814\n",
      "Iteration 54258, loss = 1.46054607\n",
      "Iteration 54259, loss = 1.53288721\n",
      "Iteration 54260, loss = 1.50102874\n",
      "Iteration 54261, loss = 1.41627064\n",
      "Iteration 54262, loss = 1.41863571\n",
      "Iteration 54263, loss = 1.44881706\n",
      "Iteration 54264, loss = 1.46384534\n",
      "Iteration 54265, loss = 1.72136094\n",
      "Iteration 54266, loss = 1.56600596\n",
      "Iteration 54267, loss = 1.73320577\n",
      "Iteration 54268, loss = 1.42000630\n",
      "Iteration 54269, loss = 1.50194090\n",
      "Iteration 54270, loss = 1.34985334\n",
      "Iteration 54271, loss = 1.39535201\n",
      "Iteration 54272, loss = 1.37622519\n",
      "Iteration 54273, loss = 1.72186214\n",
      "Iteration 54274, loss = 1.83153996\n",
      "Iteration 54275, loss = 1.45698330\n",
      "Iteration 54276, loss = 1.28304143\n",
      "Iteration 54277, loss = 1.40071345\n",
      "Iteration 54278, loss = 1.63989037\n",
      "Iteration 54279, loss = 1.98273398\n",
      "Iteration 54280, loss = 2.24407143\n",
      "Iteration 54281, loss = 2.15255193\n",
      "Iteration 54282, loss = 1.52450408\n",
      "Iteration 54283, loss = 1.42825843\n",
      "Iteration 54284, loss = 1.41369664\n",
      "Iteration 54285, loss = 1.67453350\n",
      "Iteration 54286, loss = 1.56832082\n",
      "Iteration 54287, loss = 1.70097821\n",
      "Iteration 54288, loss = 1.61438003\n",
      "Iteration 54289, loss = 1.64519734\n",
      "Iteration 54290, loss = 1.59805957\n",
      "Iteration 54291, loss = 1.79966155\n",
      "Iteration 54292, loss = 1.90354163\n",
      "Iteration 54293, loss = 1.85726458\n",
      "Iteration 54294, loss = 2.29157783\n",
      "Iteration 54295, loss = 1.65127277\n",
      "Iteration 54296, loss = 1.45435427\n",
      "Iteration 54297, loss = 1.41350617\n",
      "Iteration 54298, loss = 1.40812519\n",
      "Iteration 54299, loss = 1.53736245\n",
      "Iteration 54300, loss = 1.40558343\n",
      "Iteration 54301, loss = 1.35184714\n",
      "Iteration 54302, loss = 1.40492844\n",
      "Iteration 54303, loss = 1.56869348\n",
      "Iteration 54304, loss = 1.47520920\n",
      "Iteration 54305, loss = 1.42416229\n",
      "Iteration 54306, loss = 1.25565476\n",
      "Iteration 54307, loss = 1.37083332\n",
      "Iteration 54308, loss = 1.42068691\n",
      "Iteration 54309, loss = 1.50230208\n",
      "Iteration 54310, loss = 1.60878479\n",
      "Iteration 54311, loss = 1.52852810\n",
      "Iteration 54312, loss = 1.51400585\n",
      "Iteration 54313, loss = 1.33525450\n",
      "Iteration 54314, loss = 1.49981649\n",
      "Iteration 54315, loss = 1.57299132\n",
      "Iteration 54316, loss = 1.59981550\n",
      "Iteration 54317, loss = 1.58057072\n",
      "Iteration 54318, loss = 1.56054552\n",
      "Iteration 54319, loss = 1.75264106\n",
      "Iteration 54320, loss = 1.53569650\n",
      "Iteration 54321, loss = 2.11191204\n",
      "Iteration 54322, loss = 1.70660725\n",
      "Iteration 54323, loss = 1.41925162\n",
      "Iteration 54324, loss = 1.75279713\n",
      "Iteration 54325, loss = 1.58849888\n",
      "Iteration 54326, loss = 1.45093551\n",
      "Iteration 54327, loss = 1.41106464\n",
      "Iteration 54328, loss = 1.36957654\n",
      "Iteration 54329, loss = 1.44347171\n",
      "Iteration 54330, loss = 1.38919928\n",
      "Iteration 54331, loss = 2.70136431\n",
      "Iteration 54332, loss = 2.54783603\n",
      "Iteration 54333, loss = 2.01525156\n",
      "Iteration 54334, loss = 2.03530479\n",
      "Iteration 54335, loss = 1.98576109\n",
      "Iteration 54336, loss = 1.78808978\n",
      "Iteration 54337, loss = 1.82952422\n",
      "Iteration 54338, loss = 1.58150743\n",
      "Iteration 54339, loss = 1.46458033\n",
      "Iteration 54340, loss = 1.45656180\n",
      "Iteration 54341, loss = 1.52770129\n",
      "Iteration 54342, loss = 1.43922542\n",
      "Iteration 54343, loss = 1.51676429\n",
      "Iteration 54344, loss = 1.67279105\n",
      "Iteration 54345, loss = 1.70190192\n",
      "Iteration 54346, loss = 2.04327219\n",
      "Iteration 54347, loss = 1.55077322\n",
      "Iteration 54348, loss = 1.93116558\n",
      "Iteration 54349, loss = 1.96625413\n",
      "Iteration 54350, loss = 2.03125925\n",
      "Iteration 54351, loss = 2.65052358\n",
      "Iteration 54352, loss = 2.22669119\n",
      "Iteration 54353, loss = 1.77625170\n",
      "Iteration 54354, loss = 1.91443536\n",
      "Iteration 54355, loss = 1.51552792\n",
      "Iteration 54356, loss = 2.01148680\n",
      "Iteration 54357, loss = 1.78044347\n",
      "Iteration 54358, loss = 1.57016186\n",
      "Iteration 54359, loss = 1.79574441\n",
      "Iteration 54360, loss = 1.55942966\n",
      "Iteration 54361, loss = 1.57524372\n",
      "Iteration 54362, loss = 1.41236644\n",
      "Iteration 54363, loss = 1.32279945\n",
      "Iteration 54364, loss = 1.54929246\n",
      "Iteration 54365, loss = 1.36587953\n",
      "Iteration 54366, loss = 1.30005652\n",
      "Iteration 54367, loss = 1.34770447\n",
      "Iteration 54368, loss = 1.52064357\n",
      "Iteration 54369, loss = 1.36157444\n",
      "Iteration 54370, loss = 1.49447605\n",
      "Iteration 54371, loss = 1.57968669\n",
      "Iteration 54372, loss = 1.42764478\n",
      "Iteration 54373, loss = 1.85518830\n",
      "Iteration 54374, loss = 2.10682759\n",
      "Iteration 54375, loss = 2.15250389\n",
      "Iteration 54376, loss = 2.35964281\n",
      "Iteration 54377, loss = 1.94646993\n",
      "Iteration 54378, loss = 1.74376697\n",
      "Iteration 54379, loss = 1.59042064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 54380, loss = 1.64499587\n",
      "Iteration 54381, loss = 1.90506469\n",
      "Iteration 54382, loss = 1.44429518\n",
      "Iteration 54383, loss = 1.52533366\n",
      "Iteration 54384, loss = 1.45933566\n",
      "Iteration 54385, loss = 2.10267191\n",
      "Iteration 54386, loss = 2.23374162\n",
      "Iteration 54387, loss = 1.61486348\n",
      "Iteration 54388, loss = 1.44270785\n",
      "Iteration 54389, loss = 1.65221468\n",
      "Iteration 54390, loss = 1.43572701\n",
      "Iteration 54391, loss = 1.90974440\n",
      "Iteration 54392, loss = 1.54028861\n",
      "Iteration 54393, loss = 1.56326779\n",
      "Iteration 54394, loss = 1.52951028\n",
      "Iteration 54395, loss = 1.38016528\n",
      "Iteration 54396, loss = 1.49839028\n",
      "Iteration 54397, loss = 1.49965108\n",
      "Iteration 54398, loss = 1.48951127\n",
      "Iteration 54399, loss = 1.47987448\n",
      "Iteration 54400, loss = 1.49663367\n",
      "Iteration 54401, loss = 1.48063666\n",
      "Iteration 54402, loss = 1.40566256\n",
      "Iteration 54403, loss = 1.57784421\n",
      "Iteration 54404, loss = 2.03533265\n",
      "Iteration 54405, loss = 1.71812153\n",
      "Iteration 54406, loss = 1.52502547\n",
      "Iteration 54407, loss = 1.55887520\n",
      "Iteration 54408, loss = 1.65540310\n",
      "Iteration 54409, loss = 1.59941479\n",
      "Iteration 54410, loss = 1.96168788\n",
      "Iteration 54411, loss = 2.27517439\n",
      "Iteration 54412, loss = 2.13039312\n",
      "Iteration 54413, loss = 1.90375505\n",
      "Iteration 54414, loss = 2.02279925\n",
      "Iteration 54415, loss = 1.56968842\n",
      "Iteration 54416, loss = 1.79452564\n",
      "Iteration 54417, loss = 2.01819857\n",
      "Iteration 54418, loss = 2.76172013\n",
      "Iteration 54419, loss = 4.63402661\n",
      "Iteration 54420, loss = 4.07688426\n",
      "Iteration 54421, loss = 3.30208970\n",
      "Iteration 54422, loss = 3.44138445\n",
      "Iteration 54423, loss = 3.10056049\n",
      "Iteration 54424, loss = 1.95234591\n",
      "Iteration 54425, loss = 1.91998017\n",
      "Iteration 54426, loss = 1.85589241\n",
      "Iteration 54427, loss = 2.59511809\n",
      "Iteration 54428, loss = 2.38038981\n",
      "Iteration 54429, loss = 1.62430881\n",
      "Iteration 54430, loss = 1.40119108\n",
      "Iteration 54431, loss = 1.74346871\n",
      "Iteration 54432, loss = 1.62511992\n",
      "Iteration 54433, loss = 2.08946400\n",
      "Iteration 54434, loss = 1.68468296\n",
      "Iteration 54435, loss = 1.74888263\n",
      "Iteration 54436, loss = 1.57272693\n",
      "Iteration 54437, loss = 1.51501372\n",
      "Iteration 54438, loss = 1.60998536\n",
      "Iteration 54439, loss = 1.56542634\n",
      "Iteration 54440, loss = 1.30589239\n",
      "Iteration 54441, loss = 1.38724486\n",
      "Iteration 54442, loss = 1.34915019\n",
      "Iteration 54443, loss = 1.34917037\n",
      "Iteration 54444, loss = 1.45980830\n",
      "Iteration 54445, loss = 1.37994434\n",
      "Iteration 54446, loss = 1.35362162\n",
      "Iteration 54447, loss = 1.29685320\n",
      "Iteration 54448, loss = 1.26684826\n",
      "Iteration 54449, loss = 1.30709395\n",
      "Iteration 54450, loss = 1.37494928\n",
      "Iteration 54451, loss = 1.29015754\n",
      "Iteration 54452, loss = 1.31438921\n",
      "Iteration 54453, loss = 1.25342443\n",
      "Iteration 54454, loss = 1.29496510\n",
      "Iteration 54455, loss = 1.36884603\n",
      "Iteration 54456, loss = 1.35734111\n",
      "Iteration 54457, loss = 1.45640018\n",
      "Iteration 54458, loss = 1.34590236\n",
      "Iteration 54459, loss = 1.33202644\n",
      "Iteration 54460, loss = 1.38836610\n",
      "Iteration 54461, loss = 1.32752035\n",
      "Iteration 54462, loss = 1.52195859\n",
      "Iteration 54463, loss = 1.78992251\n",
      "Iteration 54464, loss = 2.16598669\n",
      "Iteration 54465, loss = 2.31176883\n",
      "Iteration 54466, loss = 2.20644764\n",
      "Iteration 54467, loss = 1.74291319\n",
      "Iteration 54468, loss = 1.61061000\n",
      "Iteration 54469, loss = 1.35905964\n",
      "Iteration 54470, loss = 1.41632330\n",
      "Iteration 54471, loss = 1.51800333\n",
      "Iteration 54472, loss = 1.54519139\n",
      "Iteration 54473, loss = 1.53560953\n",
      "Iteration 54474, loss = 1.33690157\n",
      "Iteration 54475, loss = 1.35835400\n",
      "Iteration 54476, loss = 1.35814315\n",
      "Iteration 54477, loss = 1.40718658\n",
      "Iteration 54478, loss = 1.77827121\n",
      "Iteration 54479, loss = 1.53370808\n",
      "Iteration 54480, loss = 1.49089217\n",
      "Iteration 54481, loss = 1.97261211\n",
      "Iteration 54482, loss = 1.50836201\n",
      "Iteration 54483, loss = 1.29207678\n",
      "Iteration 54484, loss = 1.54218997\n",
      "Iteration 54485, loss = 1.68172796\n",
      "Iteration 54486, loss = 1.60069258\n",
      "Iteration 54487, loss = 1.44061324\n",
      "Iteration 54488, loss = 1.41386145\n",
      "Iteration 54489, loss = 1.52262410\n",
      "Iteration 54490, loss = 1.59663204\n",
      "Iteration 54491, loss = 1.47528610\n",
      "Iteration 54492, loss = 1.41460153\n",
      "Iteration 54493, loss = 1.64456079\n",
      "Iteration 54494, loss = 1.65637065\n",
      "Iteration 54495, loss = 1.70138716\n",
      "Iteration 54496, loss = 1.48142090\n",
      "Iteration 54497, loss = 1.33527180\n",
      "Iteration 54498, loss = 1.47049677\n",
      "Iteration 54499, loss = 2.31010005\n",
      "Iteration 54500, loss = 1.94409088\n",
      "Iteration 54501, loss = 2.30045507\n",
      "Iteration 54502, loss = 2.61274448\n",
      "Iteration 54503, loss = 3.67601649\n",
      "Iteration 54504, loss = 3.22003004\n",
      "Iteration 54505, loss = 3.20939580\n",
      "Iteration 54506, loss = 2.84612053\n",
      "Iteration 54507, loss = 2.31019653\n",
      "Iteration 54508, loss = 2.44549316\n",
      "Iteration 54509, loss = 2.28284414\n",
      "Iteration 54510, loss = 2.06880533\n",
      "Iteration 54511, loss = 1.89870122\n",
      "Iteration 54512, loss = 2.07356350\n",
      "Iteration 54513, loss = 1.66842211\n",
      "Iteration 54514, loss = 1.79298956\n",
      "Iteration 54515, loss = 1.98537485\n",
      "Iteration 54516, loss = 1.95015060\n",
      "Iteration 54517, loss = 1.98435306\n",
      "Iteration 54518, loss = 2.17781111\n",
      "Iteration 54519, loss = 2.70604966\n",
      "Iteration 54520, loss = 1.72177466\n",
      "Iteration 54521, loss = 1.36910579\n",
      "Iteration 54522, loss = 1.37459694\n",
      "Iteration 54523, loss = 1.38156883\n",
      "Iteration 54524, loss = 1.93665093\n",
      "Iteration 54525, loss = 1.72753252\n",
      "Iteration 54526, loss = 1.87244539\n",
      "Iteration 54527, loss = 1.59208071\n",
      "Iteration 54528, loss = 1.46842091\n",
      "Iteration 54529, loss = 1.56814632\n",
      "Iteration 54530, loss = 1.38283460\n",
      "Iteration 54531, loss = 1.34434423\n",
      "Iteration 54532, loss = 1.59199200\n",
      "Iteration 54533, loss = 2.01961836\n",
      "Iteration 54534, loss = 1.54046706\n",
      "Iteration 54535, loss = 1.33586215\n",
      "Iteration 54536, loss = 1.36689769\n",
      "Iteration 54537, loss = 1.71892582\n",
      "Iteration 54538, loss = 1.51070669\n",
      "Iteration 54539, loss = 1.29740076\n",
      "Iteration 54540, loss = 1.32386296\n",
      "Iteration 54541, loss = 1.46756655\n",
      "Iteration 54542, loss = 1.73820862\n",
      "Iteration 54543, loss = 1.69591426\n",
      "Iteration 54544, loss = 2.70762678\n",
      "Iteration 54545, loss = 2.44049396\n",
      "Iteration 54546, loss = 2.38321596\n",
      "Iteration 54547, loss = 1.88104092\n",
      "Iteration 54548, loss = 1.63970944\n",
      "Iteration 54549, loss = 1.53432676\n",
      "Iteration 54550, loss = 1.45332634\n",
      "Iteration 54551, loss = 1.82519759\n",
      "Iteration 54552, loss = 1.46346106\n",
      "Iteration 54553, loss = 1.46624663\n",
      "Iteration 54554, loss = 1.40129608\n",
      "Iteration 54555, loss = 1.50950562\n",
      "Iteration 54556, loss = 1.38960873\n",
      "Iteration 54557, loss = 1.56924409\n",
      "Iteration 54558, loss = 1.48937258\n",
      "Iteration 54559, loss = 1.57461814\n",
      "Iteration 54560, loss = 1.51050077\n",
      "Iteration 54561, loss = 1.36956878\n",
      "Iteration 54562, loss = 1.31788696\n",
      "Iteration 54563, loss = 1.25892978\n",
      "Iteration 54564, loss = 1.57900981\n",
      "Iteration 54565, loss = 1.66394588\n",
      "Iteration 54566, loss = 1.51517753\n",
      "Iteration 54567, loss = 1.42892339\n",
      "Iteration 54568, loss = 1.55998983\n",
      "Iteration 54569, loss = 1.72891679\n",
      "Iteration 54570, loss = 1.56115571\n",
      "Iteration 54571, loss = 1.58700647\n",
      "Iteration 54572, loss = 1.42905149\n",
      "Iteration 54573, loss = 1.88212613\n",
      "Iteration 54574, loss = 1.38747464\n",
      "Iteration 54575, loss = 1.61877208\n",
      "Iteration 54576, loss = 1.63568269\n",
      "Iteration 54577, loss = 1.97551943\n",
      "Iteration 54578, loss = 1.38340173\n",
      "Iteration 54579, loss = 1.56986276\n",
      "Iteration 54580, loss = 1.69055771\n",
      "Iteration 54581, loss = 2.31616447\n",
      "Iteration 54582, loss = 2.10390870\n",
      "Iteration 54583, loss = 2.38402102\n",
      "Iteration 54584, loss = 2.26140097\n",
      "Iteration 54585, loss = 1.97897963\n",
      "Iteration 54586, loss = 1.93132514\n",
      "Iteration 54587, loss = 1.60796986\n",
      "Iteration 54588, loss = 1.51420048\n",
      "Iteration 54589, loss = 1.53986624\n",
      "Iteration 54590, loss = 1.61052797\n",
      "Iteration 54591, loss = 1.45440635\n",
      "Iteration 54592, loss = 1.32312826\n",
      "Iteration 54593, loss = 1.44051635\n",
      "Iteration 54594, loss = 1.57524536\n",
      "Iteration 54595, loss = 1.48268030\n",
      "Iteration 54596, loss = 1.58246335\n",
      "Iteration 54597, loss = 1.38806296\n",
      "Iteration 54598, loss = 1.26554750\n",
      "Iteration 54599, loss = 1.23337063\n",
      "Iteration 54600, loss = 1.32794783\n",
      "Iteration 54601, loss = 1.45458187\n",
      "Iteration 54602, loss = 1.62370997\n",
      "Iteration 54603, loss = 1.61699777\n",
      "Iteration 54604, loss = 1.74638451\n",
      "Iteration 54605, loss = 1.71949959\n",
      "Iteration 54606, loss = 2.13756738\n",
      "Iteration 54607, loss = 2.44313969\n",
      "Iteration 54608, loss = 2.96681137\n",
      "Iteration 54609, loss = 3.21721681\n",
      "Iteration 54610, loss = 3.17486274\n",
      "Iteration 54611, loss = 2.11889156\n",
      "Iteration 54612, loss = 1.68317356\n",
      "Iteration 54613, loss = 1.86177368\n",
      "Iteration 54614, loss = 1.96583373\n",
      "Iteration 54615, loss = 1.80754191\n",
      "Iteration 54616, loss = 1.52942020\n",
      "Iteration 54617, loss = 1.56498506\n",
      "Iteration 54618, loss = 1.88445692\n",
      "Iteration 54619, loss = 1.78040859\n",
      "Iteration 54620, loss = 1.69292492\n",
      "Iteration 54621, loss = 1.68084702\n",
      "Iteration 54622, loss = 1.93920084\n",
      "Iteration 54623, loss = 1.76796478\n",
      "Iteration 54624, loss = 1.85443718\n",
      "Iteration 54625, loss = 1.81467049\n",
      "Iteration 54626, loss = 2.17038019\n",
      "Iteration 54627, loss = 2.31486976\n",
      "Iteration 54628, loss = 1.74416776\n",
      "Iteration 54629, loss = 1.48577321\n",
      "Iteration 54630, loss = 1.33778481\n",
      "Iteration 54631, loss = 1.25480061\n",
      "Iteration 54632, loss = 1.34458774\n",
      "Iteration 54633, loss = 1.56866546\n",
      "Iteration 54634, loss = 1.70520655\n",
      "Iteration 54635, loss = 2.11657531\n",
      "Iteration 54636, loss = 1.69332470\n",
      "Iteration 54637, loss = 1.79203472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 54638, loss = 1.99809434\n",
      "Iteration 54639, loss = 2.47631420\n",
      "Iteration 54640, loss = 2.68764074\n",
      "Iteration 54641, loss = 2.26600423\n",
      "Iteration 54642, loss = 1.77096231\n",
      "Iteration 54643, loss = 1.73229368\n",
      "Iteration 54644, loss = 1.76321824\n",
      "Iteration 54645, loss = 1.90656053\n",
      "Iteration 54646, loss = 2.10343198\n",
      "Iteration 54647, loss = 2.65787956\n",
      "Iteration 54648, loss = 2.35410350\n",
      "Iteration 54649, loss = 3.20449472\n",
      "Iteration 54650, loss = 2.46510981\n",
      "Iteration 54651, loss = 2.56085928\n",
      "Iteration 54652, loss = 2.67607868\n",
      "Iteration 54653, loss = 4.37256793\n",
      "Iteration 54654, loss = 4.52105655\n",
      "Iteration 54655, loss = 2.50869175\n",
      "Iteration 54656, loss = 2.76010630\n",
      "Iteration 54657, loss = 1.74822851\n",
      "Iteration 54658, loss = 1.81263003\n",
      "Iteration 54659, loss = 1.46508035\n",
      "Iteration 54660, loss = 1.34876428\n",
      "Iteration 54661, loss = 1.75663909\n",
      "Iteration 54662, loss = 1.86270382\n",
      "Iteration 54663, loss = 1.74980930\n",
      "Iteration 54664, loss = 2.00035767\n",
      "Iteration 54665, loss = 1.75426902\n",
      "Iteration 54666, loss = 1.96432711\n",
      "Iteration 54667, loss = 2.19564622\n",
      "Iteration 54668, loss = 2.14771156\n",
      "Iteration 54669, loss = 1.60413658\n",
      "Iteration 54670, loss = 1.67096967\n",
      "Iteration 54671, loss = 2.28255844\n",
      "Iteration 54672, loss = 2.61361801\n",
      "Iteration 54673, loss = 2.85657030\n",
      "Iteration 54674, loss = 1.88742649\n",
      "Iteration 54675, loss = 1.68233406\n",
      "Iteration 54676, loss = 1.52056109\n",
      "Iteration 54677, loss = 1.34445830\n",
      "Iteration 54678, loss = 1.60520855\n",
      "Iteration 54679, loss = 1.71322298\n",
      "Iteration 54680, loss = 1.80221346\n",
      "Iteration 54681, loss = 2.68147855\n",
      "Iteration 54682, loss = 2.23459769\n",
      "Iteration 54683, loss = 2.29506356\n",
      "Iteration 54684, loss = 2.62376858\n",
      "Iteration 54685, loss = 2.18464647\n",
      "Iteration 54686, loss = 2.10473556\n",
      "Iteration 54687, loss = 1.43849829\n",
      "Iteration 54688, loss = 1.43062049\n",
      "Iteration 54689, loss = 1.50413467\n",
      "Iteration 54690, loss = 1.76719864\n",
      "Iteration 54691, loss = 1.67938085\n",
      "Iteration 54692, loss = 1.84687315\n",
      "Iteration 54693, loss = 1.69729469\n",
      "Iteration 54694, loss = 1.69155455\n",
      "Iteration 54695, loss = 1.93587442\n",
      "Iteration 54696, loss = 2.05638498\n",
      "Iteration 54697, loss = 2.37780747\n",
      "Iteration 54698, loss = 1.87612364\n",
      "Iteration 54699, loss = 1.82235574\n",
      "Iteration 54700, loss = 1.60247452\n",
      "Iteration 54701, loss = 1.54158056\n",
      "Iteration 54702, loss = 1.63147509\n",
      "Iteration 54703, loss = 1.93537062\n",
      "Iteration 54704, loss = 2.50027064\n",
      "Iteration 54705, loss = 3.05988639\n",
      "Iteration 54706, loss = 3.84010392\n",
      "Iteration 54707, loss = 3.31811896\n",
      "Iteration 54708, loss = 2.51162799\n",
      "Iteration 54709, loss = 2.54166562\n",
      "Iteration 54710, loss = 2.94086019\n",
      "Iteration 54711, loss = 2.34462542\n",
      "Iteration 54712, loss = 2.50290237\n",
      "Iteration 54713, loss = 2.44966655\n",
      "Iteration 54714, loss = 2.17790700\n",
      "Iteration 54715, loss = 1.74326172\n",
      "Iteration 54716, loss = 1.72613137\n",
      "Iteration 54717, loss = 1.81363361\n",
      "Iteration 54718, loss = 1.85503490\n",
      "Iteration 54719, loss = 1.84760139\n",
      "Iteration 54720, loss = 1.64665406\n",
      "Iteration 54721, loss = 1.94824361\n",
      "Iteration 54722, loss = 1.40499610\n",
      "Iteration 54723, loss = 1.36732439\n",
      "Iteration 54724, loss = 1.40333680\n",
      "Iteration 54725, loss = 1.34824377\n",
      "Iteration 54726, loss = 1.31115450\n",
      "Iteration 54727, loss = 1.46331536\n",
      "Iteration 54728, loss = 1.46309511\n",
      "Iteration 54729, loss = 1.31212444\n",
      "Iteration 54730, loss = 1.37920300\n",
      "Iteration 54731, loss = 1.41101666\n",
      "Iteration 54732, loss = 1.45625851\n",
      "Iteration 54733, loss = 1.63330737\n",
      "Iteration 54734, loss = 1.56471069\n",
      "Iteration 54735, loss = 1.26685430\n",
      "Iteration 54736, loss = 1.32275291\n",
      "Iteration 54737, loss = 1.48831402\n",
      "Iteration 54738, loss = 1.54185652\n",
      "Iteration 54739, loss = 1.53032433\n",
      "Iteration 54740, loss = 1.41887983\n",
      "Iteration 54741, loss = 1.60374367\n",
      "Iteration 54742, loss = 1.74218412\n",
      "Iteration 54743, loss = 1.60948799\n",
      "Iteration 54744, loss = 1.61229093\n",
      "Iteration 54745, loss = 1.56041564\n",
      "Iteration 54746, loss = 1.35737391\n",
      "Iteration 54747, loss = 1.30363828\n",
      "Iteration 54748, loss = 1.41412013\n",
      "Iteration 54749, loss = 1.33715729\n",
      "Iteration 54750, loss = 1.37127150\n",
      "Iteration 54751, loss = 1.47322511\n",
      "Iteration 54752, loss = 1.55289324\n",
      "Iteration 54753, loss = 1.56876677\n",
      "Iteration 54754, loss = 1.49324288\n",
      "Iteration 54755, loss = 1.33248642\n",
      "Iteration 54756, loss = 1.34965270\n",
      "Iteration 54757, loss = 1.33345617\n",
      "Iteration 54758, loss = 1.41677968\n",
      "Iteration 54759, loss = 1.28069656\n",
      "Iteration 54760, loss = 1.27260733\n",
      "Iteration 54761, loss = 1.28507431\n",
      "Iteration 54762, loss = 1.34790623\n",
      "Iteration 54763, loss = 1.26534037\n",
      "Iteration 54764, loss = 1.31471864\n",
      "Iteration 54765, loss = 1.37352757\n",
      "Iteration 54766, loss = 1.34738626\n",
      "Iteration 54767, loss = 1.23970417\n",
      "Iteration 54768, loss = 1.37681404\n",
      "Iteration 54769, loss = 1.60406670\n",
      "Iteration 54770, loss = 1.36782261\n",
      "Iteration 54771, loss = 1.41578656\n",
      "Iteration 54772, loss = 1.48536917\n",
      "Iteration 54773, loss = 1.53064534\n",
      "Iteration 54774, loss = 1.67160515\n",
      "Iteration 54775, loss = 1.47889617\n",
      "Iteration 54776, loss = 1.33609226\n",
      "Iteration 54777, loss = 1.29870370\n",
      "Iteration 54778, loss = 1.46409964\n",
      "Iteration 54779, loss = 1.55804394\n",
      "Iteration 54780, loss = 1.48719355\n",
      "Iteration 54781, loss = 1.24045494\n",
      "Iteration 54782, loss = 1.22640943\n",
      "Iteration 54783, loss = 1.21142001\n",
      "Iteration 54784, loss = 1.36551425\n",
      "Iteration 54785, loss = 1.40862466\n",
      "Iteration 54786, loss = 1.35750817\n",
      "Iteration 54787, loss = 1.48582212\n",
      "Iteration 54788, loss = 1.57880856\n",
      "Iteration 54789, loss = 1.31812224\n",
      "Iteration 54790, loss = 1.30077786\n",
      "Iteration 54791, loss = 1.34744320\n",
      "Iteration 54792, loss = 1.51294050\n",
      "Iteration 54793, loss = 1.54299970\n",
      "Iteration 54794, loss = 1.66399184\n",
      "Iteration 54795, loss = 1.81097040\n",
      "Iteration 54796, loss = 1.75500438\n",
      "Iteration 54797, loss = 1.89026883\n",
      "Iteration 54798, loss = 1.73249934\n",
      "Iteration 54799, loss = 1.69689111\n",
      "Iteration 54800, loss = 1.50527653\n",
      "Iteration 54801, loss = 1.53342303\n",
      "Iteration 54802, loss = 1.32246763\n",
      "Iteration 54803, loss = 1.58411944\n",
      "Iteration 54804, loss = 1.39057774\n",
      "Iteration 54805, loss = 1.53651542\n",
      "Iteration 54806, loss = 1.60530729\n",
      "Iteration 54807, loss = 1.43229918\n",
      "Iteration 54808, loss = 1.66338800\n",
      "Iteration 54809, loss = 1.53883108\n",
      "Iteration 54810, loss = 1.47378817\n",
      "Iteration 54811, loss = 1.74122178\n",
      "Iteration 54812, loss = 1.53107075\n",
      "Iteration 54813, loss = 1.72699430\n",
      "Iteration 54814, loss = 1.69495186\n",
      "Iteration 54815, loss = 2.20734003\n",
      "Iteration 54816, loss = 2.24053401\n",
      "Iteration 54817, loss = 1.85674935\n",
      "Iteration 54818, loss = 1.66090919\n",
      "Iteration 54819, loss = 1.73280824\n",
      "Iteration 54820, loss = 1.76009156\n",
      "Iteration 54821, loss = 1.75386878\n",
      "Iteration 54822, loss = 1.39606170\n",
      "Iteration 54823, loss = 1.54881420\n",
      "Iteration 54824, loss = 1.74441116\n",
      "Iteration 54825, loss = 1.53560648\n",
      "Iteration 54826, loss = 1.61176050\n",
      "Iteration 54827, loss = 1.63690025\n",
      "Iteration 54828, loss = 1.75431473\n",
      "Iteration 54829, loss = 1.50948932\n",
      "Iteration 54830, loss = 1.34590614\n",
      "Iteration 54831, loss = 1.60128800\n",
      "Iteration 54832, loss = 1.31592212\n",
      "Iteration 54833, loss = 1.33216620\n",
      "Iteration 54834, loss = 1.50219145\n",
      "Iteration 54835, loss = 1.56176501\n",
      "Iteration 54836, loss = 1.53107378\n",
      "Iteration 54837, loss = 1.77890437\n",
      "Iteration 54838, loss = 1.34255995\n",
      "Iteration 54839, loss = 1.45957682\n",
      "Iteration 54840, loss = 1.50090372\n",
      "Iteration 54841, loss = 1.56071409\n",
      "Iteration 54842, loss = 1.40926897\n",
      "Iteration 54843, loss = 1.74921677\n",
      "Iteration 54844, loss = 2.41957999\n",
      "Iteration 54845, loss = 2.37717064\n",
      "Iteration 54846, loss = 1.80749881\n",
      "Iteration 54847, loss = 1.75911353\n",
      "Iteration 54848, loss = 2.28947309\n",
      "Iteration 54849, loss = 2.13619552\n",
      "Iteration 54850, loss = 1.68719009\n",
      "Iteration 54851, loss = 1.82338663\n",
      "Iteration 54852, loss = 1.64142875\n",
      "Iteration 54853, loss = 2.11962888\n",
      "Iteration 54854, loss = 2.09370932\n",
      "Iteration 54855, loss = 2.59895893\n",
      "Iteration 54856, loss = 2.58076342\n",
      "Iteration 54857, loss = 3.62174014\n",
      "Iteration 54858, loss = 2.73562241\n",
      "Iteration 54859, loss = 2.16685242\n",
      "Iteration 54860, loss = 2.02788030\n",
      "Iteration 54861, loss = 2.14852101\n",
      "Iteration 54862, loss = 1.88958921\n",
      "Iteration 54863, loss = 2.08974924\n",
      "Iteration 54864, loss = 1.84172975\n",
      "Iteration 54865, loss = 1.48950116\n",
      "Iteration 54866, loss = 1.38503849\n",
      "Iteration 54867, loss = 1.51373566\n",
      "Iteration 54868, loss = 1.46234912\n",
      "Iteration 54869, loss = 1.53038439\n",
      "Iteration 54870, loss = 1.75763647\n",
      "Iteration 54871, loss = 1.70450321\n",
      "Iteration 54872, loss = 1.59532599\n",
      "Iteration 54873, loss = 1.34399240\n",
      "Iteration 54874, loss = 2.07304983\n",
      "Iteration 54875, loss = 1.73566342\n",
      "Iteration 54876, loss = 2.22319978\n",
      "Iteration 54877, loss = 2.20155426\n",
      "Iteration 54878, loss = 2.22816552\n",
      "Iteration 54879, loss = 2.99201308\n",
      "Iteration 54880, loss = 3.33538613\n",
      "Iteration 54881, loss = 2.68818713\n",
      "Iteration 54882, loss = 1.87128531\n",
      "Iteration 54883, loss = 2.15374668\n",
      "Iteration 54884, loss = 2.34145108\n",
      "Iteration 54885, loss = 3.19625752\n",
      "Iteration 54886, loss = 2.22189159\n",
      "Iteration 54887, loss = 2.35148695\n",
      "Iteration 54888, loss = 2.35970793\n",
      "Iteration 54889, loss = 2.23803731\n",
      "Iteration 54890, loss = 1.74406215\n",
      "Iteration 54891, loss = 1.64362988\n",
      "Iteration 54892, loss = 1.84076817\n",
      "Iteration 54893, loss = 1.53419364\n",
      "Iteration 54894, loss = 1.50555041\n",
      "Iteration 54895, loss = 1.83498029\n",
      "Iteration 54896, loss = 1.33946749\n",
      "Iteration 54897, loss = 1.57157248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 54898, loss = 1.43916611\n",
      "Iteration 54899, loss = 1.36243088\n",
      "Iteration 54900, loss = 1.59308171\n",
      "Iteration 54901, loss = 1.66137590\n",
      "Iteration 54902, loss = 1.57589845\n",
      "Iteration 54903, loss = 1.42868497\n",
      "Iteration 54904, loss = 1.69346947\n",
      "Iteration 54905, loss = 1.62083688\n",
      "Iteration 54906, loss = 1.60179645\n",
      "Iteration 54907, loss = 1.60661440\n",
      "Iteration 54908, loss = 1.40645903\n",
      "Iteration 54909, loss = 1.46824133\n",
      "Iteration 54910, loss = 1.56098922\n",
      "Iteration 54911, loss = 1.72695762\n",
      "Iteration 54912, loss = 1.56492788\n",
      "Iteration 54913, loss = 1.69070389\n",
      "Iteration 54914, loss = 1.59244971\n",
      "Iteration 54915, loss = 1.49424896\n",
      "Iteration 54916, loss = 1.25181167\n",
      "Iteration 54917, loss = 1.40177993\n",
      "Iteration 54918, loss = 1.50892833\n",
      "Iteration 54919, loss = 1.61706532\n",
      "Iteration 54920, loss = 1.71494959\n",
      "Iteration 54921, loss = 1.90563004\n",
      "Iteration 54922, loss = 1.95381947\n",
      "Iteration 54923, loss = 2.03570130\n",
      "Iteration 54924, loss = 2.20349288\n",
      "Iteration 54925, loss = 1.86855430\n",
      "Iteration 54926, loss = 1.44718827\n",
      "Iteration 54927, loss = 1.43838028\n",
      "Iteration 54928, loss = 1.58561483\n",
      "Iteration 54929, loss = 1.56880501\n",
      "Iteration 54930, loss = 1.56004142\n",
      "Iteration 54931, loss = 2.05950362\n",
      "Iteration 54932, loss = 2.21112851\n",
      "Iteration 54933, loss = 2.08715552\n",
      "Iteration 54934, loss = 2.53962377\n",
      "Iteration 54935, loss = 2.91477414\n",
      "Iteration 54936, loss = 2.68834360\n",
      "Iteration 54937, loss = 2.16165771\n",
      "Iteration 54938, loss = 2.57983675\n",
      "Iteration 54939, loss = 1.75164189\n",
      "Iteration 54940, loss = 1.68916948\n",
      "Iteration 54941, loss = 2.13002673\n",
      "Iteration 54942, loss = 1.94294954\n",
      "Iteration 54943, loss = 1.80842967\n",
      "Iteration 54944, loss = 1.65717236\n",
      "Iteration 54945, loss = 1.49016697\n",
      "Iteration 54946, loss = 1.59329638\n",
      "Iteration 54947, loss = 1.36699643\n",
      "Iteration 54948, loss = 1.27656082\n",
      "Iteration 54949, loss = 1.36090775\n",
      "Iteration 54950, loss = 1.42712983\n",
      "Iteration 54951, loss = 1.40021208\n",
      "Iteration 54952, loss = 1.48260083\n",
      "Iteration 54953, loss = 1.31618683\n",
      "Iteration 54954, loss = 1.68439813\n",
      "Iteration 54955, loss = 1.89166270\n",
      "Iteration 54956, loss = 1.89870777\n",
      "Iteration 54957, loss = 2.20645645\n",
      "Iteration 54958, loss = 2.08885358\n",
      "Iteration 54959, loss = 2.43040209\n",
      "Iteration 54960, loss = 2.11190315\n",
      "Iteration 54961, loss = 2.22001100\n",
      "Iteration 54962, loss = 2.06735638\n",
      "Iteration 54963, loss = 2.67177429\n",
      "Iteration 54964, loss = 3.11420454\n",
      "Iteration 54965, loss = 4.57454704\n",
      "Iteration 54966, loss = 5.80005862\n",
      "Iteration 54967, loss = 5.45838014\n",
      "Iteration 54968, loss = 3.78292469\n",
      "Iteration 54969, loss = 5.70843628\n",
      "Iteration 54970, loss = 4.36741188\n",
      "Iteration 54971, loss = 4.90452130\n",
      "Iteration 54972, loss = 3.57512393\n",
      "Iteration 54973, loss = 1.99570871\n",
      "Iteration 54974, loss = 2.29405290\n",
      "Iteration 54975, loss = 1.58615526\n",
      "Iteration 54976, loss = 1.82664777\n",
      "Iteration 54977, loss = 2.14720435\n",
      "Iteration 54978, loss = 1.72973493\n",
      "Iteration 54979, loss = 1.59536657\n",
      "Iteration 54980, loss = 1.67046325\n",
      "Iteration 54981, loss = 1.34043004\n",
      "Iteration 54982, loss = 1.45612848\n",
      "Iteration 54983, loss = 1.49091797\n",
      "Iteration 54984, loss = 1.59280528\n",
      "Iteration 54985, loss = 2.00010209\n",
      "Iteration 54986, loss = 1.57425054\n",
      "Iteration 54987, loss = 1.63000621\n",
      "Iteration 54988, loss = 1.45145482\n",
      "Iteration 54989, loss = 1.32096542\n",
      "Iteration 54990, loss = 1.48050548\n",
      "Iteration 54991, loss = 1.48007131\n",
      "Iteration 54992, loss = 1.52551016\n",
      "Iteration 54993, loss = 1.47795864\n",
      "Iteration 54994, loss = 1.46470696\n",
      "Iteration 54995, loss = 1.95406839\n",
      "Iteration 54996, loss = 2.04198670\n",
      "Iteration 54997, loss = 1.77352282\n",
      "Iteration 54998, loss = 2.25628812\n",
      "Iteration 54999, loss = 2.08510553\n",
      "Iteration 55000, loss = 1.45455843\n",
      "Iteration 55001, loss = 1.33378131\n",
      "Iteration 55002, loss = 1.37334358\n",
      "Iteration 55003, loss = 1.78623164\n",
      "Iteration 55004, loss = 1.51005990\n",
      "Iteration 55005, loss = 1.42981677\n",
      "Iteration 55006, loss = 1.50470281\n",
      "Iteration 55007, loss = 1.51968222\n",
      "Iteration 55008, loss = 1.57198514\n",
      "Iteration 55009, loss = 1.63654741\n",
      "Iteration 55010, loss = 1.75257060\n",
      "Iteration 55011, loss = 1.45932655\n",
      "Iteration 55012, loss = 1.73122654\n",
      "Iteration 55013, loss = 1.83827693\n",
      "Iteration 55014, loss = 1.60537124\n",
      "Iteration 55015, loss = 1.69982764\n",
      "Iteration 55016, loss = 1.35883243\n",
      "Iteration 55017, loss = 1.51501199\n",
      "Iteration 55018, loss = 1.60826485\n",
      "Iteration 55019, loss = 1.60223319\n",
      "Iteration 55020, loss = 1.63953051\n",
      "Iteration 55021, loss = 1.80909582\n",
      "Iteration 55022, loss = 2.16688524\n",
      "Iteration 55023, loss = 1.83487868\n",
      "Iteration 55024, loss = 1.82530685\n",
      "Iteration 55025, loss = 2.22388468\n",
      "Iteration 55026, loss = 1.93818349\n",
      "Iteration 55027, loss = 2.17404308\n",
      "Iteration 55028, loss = 1.73997088\n",
      "Iteration 55029, loss = 2.11164891\n",
      "Iteration 55030, loss = 3.11874963\n",
      "Iteration 55031, loss = 2.60455902\n",
      "Iteration 55032, loss = 1.61411008\n",
      "Iteration 55033, loss = 1.48467074\n",
      "Iteration 55034, loss = 1.73355301\n",
      "Iteration 55035, loss = 1.74712107\n",
      "Iteration 55036, loss = 1.83993311\n",
      "Iteration 55037, loss = 1.90406586\n",
      "Iteration 55038, loss = 1.61614365\n",
      "Iteration 55039, loss = 1.40078537\n",
      "Iteration 55040, loss = 1.36100753\n",
      "Iteration 55041, loss = 1.41421579\n",
      "Iteration 55042, loss = 1.49138862\n",
      "Iteration 55043, loss = 1.55401063\n",
      "Iteration 55044, loss = 1.41824584\n",
      "Iteration 55045, loss = 1.49518277\n",
      "Iteration 55046, loss = 1.28779901\n",
      "Iteration 55047, loss = 1.36553265\n",
      "Iteration 55048, loss = 1.43185435\n",
      "Iteration 55049, loss = 1.48041213\n",
      "Iteration 55050, loss = 1.62764206\n",
      "Iteration 55051, loss = 1.47552264\n",
      "Iteration 55052, loss = 1.40256157\n",
      "Iteration 55053, loss = 1.56309681\n",
      "Iteration 55054, loss = 1.74933706\n",
      "Iteration 55055, loss = 1.69605225\n",
      "Iteration 55056, loss = 1.54740952\n",
      "Iteration 55057, loss = 1.75618604\n",
      "Iteration 55058, loss = 1.87348738\n",
      "Iteration 55059, loss = 1.65920708\n",
      "Iteration 55060, loss = 1.91945054\n",
      "Iteration 55061, loss = 1.90545868\n",
      "Iteration 55062, loss = 1.81819774\n",
      "Iteration 55063, loss = 2.02961033\n",
      "Iteration 55064, loss = 1.72627797\n",
      "Iteration 55065, loss = 1.39602039\n",
      "Iteration 55066, loss = 1.23136778\n",
      "Iteration 55067, loss = 1.26811892\n",
      "Iteration 55068, loss = 1.58730288\n",
      "Iteration 55069, loss = 1.72089332\n",
      "Iteration 55070, loss = 1.76949964\n",
      "Iteration 55071, loss = 1.56718342\n",
      "Iteration 55072, loss = 1.40259412\n",
      "Iteration 55073, loss = 1.49041087\n",
      "Iteration 55074, loss = 1.57516818\n",
      "Iteration 55075, loss = 1.46899645\n",
      "Iteration 55076, loss = 1.52508028\n",
      "Iteration 55077, loss = 1.50993446\n",
      "Iteration 55078, loss = 1.54899418\n",
      "Iteration 55079, loss = 1.39221901\n",
      "Iteration 55080, loss = 1.29022258\n",
      "Iteration 55081, loss = 1.68065238\n",
      "Iteration 55082, loss = 1.96258086\n",
      "Iteration 55083, loss = 1.72429192\n",
      "Iteration 55084, loss = 1.73962101\n",
      "Iteration 55085, loss = 1.96026315\n",
      "Iteration 55086, loss = 1.63495951\n",
      "Iteration 55087, loss = 1.40924909\n",
      "Iteration 55088, loss = 1.50421106\n",
      "Iteration 55089, loss = 1.38713603\n",
      "Iteration 55090, loss = 1.46685638\n",
      "Iteration 55091, loss = 1.53073382\n",
      "Iteration 55092, loss = 1.88333458\n",
      "Iteration 55093, loss = 1.78449044\n",
      "Iteration 55094, loss = 1.43256347\n",
      "Iteration 55095, loss = 1.62387984\n",
      "Iteration 55096, loss = 1.42387340\n",
      "Iteration 55097, loss = 1.53616501\n",
      "Iteration 55098, loss = 1.94102036\n",
      "Iteration 55099, loss = 2.06520150\n",
      "Iteration 55100, loss = 1.89911837\n",
      "Iteration 55101, loss = 1.65103159\n",
      "Iteration 55102, loss = 1.94300689\n",
      "Iteration 55103, loss = 2.52117867\n",
      "Iteration 55104, loss = 2.63075694\n",
      "Iteration 55105, loss = 3.73966179\n",
      "Iteration 55106, loss = 6.06554975\n",
      "Iteration 55107, loss = 4.87567048\n",
      "Iteration 55108, loss = 5.60677452\n",
      "Iteration 55109, loss = 4.64102730\n",
      "Iteration 55110, loss = 3.47479066\n",
      "Iteration 55111, loss = 2.64159334\n",
      "Iteration 55112, loss = 2.25607751\n",
      "Iteration 55113, loss = 1.79392241\n",
      "Iteration 55114, loss = 1.57207692\n",
      "Iteration 55115, loss = 1.78353230\n",
      "Iteration 55116, loss = 1.46851221\n",
      "Iteration 55117, loss = 1.38684091\n",
      "Iteration 55118, loss = 1.34738680\n",
      "Iteration 55119, loss = 1.68573499\n",
      "Iteration 55120, loss = 1.51033150\n",
      "Iteration 55121, loss = 1.35247469\n",
      "Iteration 55122, loss = 1.39294106\n",
      "Iteration 55123, loss = 1.56847942\n",
      "Iteration 55124, loss = 1.42443417\n",
      "Iteration 55125, loss = 1.47706414\n",
      "Iteration 55126, loss = 1.29635900\n",
      "Iteration 55127, loss = 1.61312332\n",
      "Iteration 55128, loss = 1.43704133\n",
      "Iteration 55129, loss = 1.28502011\n",
      "Iteration 55130, loss = 1.98848905\n",
      "Iteration 55131, loss = 2.22517049\n",
      "Iteration 55132, loss = 2.35245417\n",
      "Iteration 55133, loss = 2.82081566\n",
      "Iteration 55134, loss = 2.97131362\n",
      "Iteration 55135, loss = 2.52000392\n",
      "Iteration 55136, loss = 2.32731079\n",
      "Iteration 55137, loss = 2.06782106\n",
      "Iteration 55138, loss = 1.90752475\n",
      "Iteration 55139, loss = 2.95357351\n",
      "Iteration 55140, loss = 2.83587259\n",
      "Iteration 55141, loss = 2.66786629\n",
      "Iteration 55142, loss = 1.99878568\n",
      "Iteration 55143, loss = 1.57285337\n",
      "Iteration 55144, loss = 1.86083598\n",
      "Iteration 55145, loss = 1.32702334\n",
      "Iteration 55146, loss = 1.46670452\n",
      "Iteration 55147, loss = 1.48933681\n",
      "Iteration 55148, loss = 1.50981810\n",
      "Iteration 55149, loss = 1.42831946\n",
      "Iteration 55150, loss = 1.40158570\n",
      "Iteration 55151, loss = 1.70742489\n",
      "Iteration 55152, loss = 1.45884924\n",
      "Iteration 55153, loss = 1.64964192\n",
      "Iteration 55154, loss = 1.44257880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 55155, loss = 1.78168108\n",
      "Iteration 55156, loss = 1.45414956\n",
      "Iteration 55157, loss = 1.55721543\n",
      "Iteration 55158, loss = 1.32322456\n",
      "Iteration 55159, loss = 1.44630257\n",
      "Iteration 55160, loss = 1.37475048\n",
      "Iteration 55161, loss = 1.28734051\n",
      "Iteration 55162, loss = 1.52805791\n",
      "Iteration 55163, loss = 1.35477853\n",
      "Iteration 55164, loss = 1.97712459\n",
      "Iteration 55165, loss = 1.64334922\n",
      "Iteration 55166, loss = 1.82370916\n",
      "Iteration 55167, loss = 2.19617158\n",
      "Iteration 55168, loss = 1.53185760\n",
      "Iteration 55169, loss = 1.55029081\n",
      "Iteration 55170, loss = 1.51895325\n",
      "Iteration 55171, loss = 1.39114319\n",
      "Iteration 55172, loss = 1.38983229\n",
      "Iteration 55173, loss = 1.40853196\n",
      "Iteration 55174, loss = 1.36646047\n",
      "Iteration 55175, loss = 1.39760151\n",
      "Iteration 55176, loss = 1.56657873\n",
      "Iteration 55177, loss = 1.65762877\n",
      "Iteration 55178, loss = 1.36788679\n",
      "Iteration 55179, loss = 1.41640996\n",
      "Iteration 55180, loss = 1.57324773\n",
      "Iteration 55181, loss = 1.70266328\n",
      "Iteration 55182, loss = 1.57308729\n",
      "Iteration 55183, loss = 1.63895314\n",
      "Iteration 55184, loss = 1.47399206\n",
      "Iteration 55185, loss = 1.32312890\n",
      "Iteration 55186, loss = 1.59160264\n",
      "Iteration 55187, loss = 1.32848371\n",
      "Iteration 55188, loss = 1.38603495\n",
      "Iteration 55189, loss = 2.02416391\n",
      "Iteration 55190, loss = 1.90786805\n",
      "Iteration 55191, loss = 1.50010286\n",
      "Iteration 55192, loss = 1.22542131\n",
      "Iteration 55193, loss = 1.33327529\n",
      "Iteration 55194, loss = 1.37460055\n",
      "Iteration 55195, loss = 1.42964515\n",
      "Iteration 55196, loss = 1.51398398\n",
      "Iteration 55197, loss = 1.63260112\n",
      "Iteration 55198, loss = 1.46143366\n",
      "Iteration 55199, loss = 1.25865339\n",
      "Iteration 55200, loss = 1.18752198\n",
      "Iteration 55201, loss = 1.30955753\n",
      "Iteration 55202, loss = 1.31053077\n",
      "Iteration 55203, loss = 1.26936379\n",
      "Iteration 55204, loss = 1.26792264\n",
      "Iteration 55205, loss = 1.29323446\n",
      "Iteration 55206, loss = 1.31102025\n",
      "Iteration 55207, loss = 1.25080377\n",
      "Iteration 55208, loss = 1.39284747\n",
      "Iteration 55209, loss = 1.47850582\n",
      "Iteration 55210, loss = 1.87488081\n",
      "Iteration 55211, loss = 1.69747374\n",
      "Iteration 55212, loss = 1.48768049\n",
      "Iteration 55213, loss = 1.36176313\n",
      "Iteration 55214, loss = 1.30731090\n",
      "Iteration 55215, loss = 1.62756849\n",
      "Iteration 55216, loss = 1.53377176\n",
      "Iteration 55217, loss = 1.22736156\n",
      "Iteration 55218, loss = 1.27773232\n",
      "Iteration 55219, loss = 1.38803602\n",
      "Iteration 55220, loss = 1.44156144\n",
      "Iteration 55221, loss = 1.44405106\n",
      "Iteration 55222, loss = 1.49957642\n",
      "Iteration 55223, loss = 1.36617681\n",
      "Iteration 55224, loss = 1.41677457\n",
      "Iteration 55225, loss = 1.48949521\n",
      "Iteration 55226, loss = 1.72673972\n",
      "Iteration 55227, loss = 1.66550010\n",
      "Iteration 55228, loss = 1.41405523\n",
      "Iteration 55229, loss = 1.50203424\n",
      "Iteration 55230, loss = 1.41637194\n",
      "Iteration 55231, loss = 1.25415758\n",
      "Iteration 55232, loss = 1.32090734\n",
      "Iteration 55233, loss = 1.41853630\n",
      "Iteration 55234, loss = 1.54165959\n",
      "Iteration 55235, loss = 2.00915780\n",
      "Iteration 55236, loss = 2.06210350\n",
      "Iteration 55237, loss = 1.86080251\n",
      "Iteration 55238, loss = 1.80392280\n",
      "Iteration 55239, loss = 1.61713128\n",
      "Iteration 55240, loss = 1.66817483\n",
      "Iteration 55241, loss = 1.57615544\n",
      "Iteration 55242, loss = 1.43780008\n",
      "Iteration 55243, loss = 1.29430179\n",
      "Iteration 55244, loss = 1.49091094\n",
      "Iteration 55245, loss = 1.71549070\n",
      "Iteration 55246, loss = 1.79737743\n",
      "Iteration 55247, loss = 1.85528865\n",
      "Iteration 55248, loss = 2.25583372\n",
      "Iteration 55249, loss = 1.73121792\n",
      "Iteration 55250, loss = 1.50306981\n",
      "Iteration 55251, loss = 1.35183049\n",
      "Iteration 55252, loss = 1.45338472\n",
      "Iteration 55253, loss = 1.50437044\n",
      "Iteration 55254, loss = 1.71391191\n",
      "Iteration 55255, loss = 1.50119731\n",
      "Iteration 55256, loss = 1.45489773\n",
      "Iteration 55257, loss = 1.42974660\n",
      "Iteration 55258, loss = 1.56144482\n",
      "Iteration 55259, loss = 1.56363742\n",
      "Iteration 55260, loss = 1.59337457\n",
      "Iteration 55261, loss = 1.34287050\n",
      "Iteration 55262, loss = 1.76572511\n",
      "Iteration 55263, loss = 2.34028769\n",
      "Iteration 55264, loss = 2.39811789\n",
      "Iteration 55265, loss = 1.67596433\n",
      "Iteration 55266, loss = 1.58184604\n",
      "Iteration 55267, loss = 2.17808334\n",
      "Iteration 55268, loss = 1.97735110\n",
      "Iteration 55269, loss = 1.72269676\n",
      "Iteration 55270, loss = 2.03633338\n",
      "Iteration 55271, loss = 1.92048019\n",
      "Iteration 55272, loss = 1.85293530\n",
      "Iteration 55273, loss = 2.12753132\n",
      "Iteration 55274, loss = 1.95215686\n",
      "Iteration 55275, loss = 1.76345196\n",
      "Iteration 55276, loss = 1.66441019\n",
      "Iteration 55277, loss = 1.56925772\n",
      "Iteration 55278, loss = 1.69725138\n",
      "Iteration 55279, loss = 1.48284908\n",
      "Iteration 55280, loss = 1.30109652\n",
      "Iteration 55281, loss = 1.40990169\n",
      "Iteration 55282, loss = 1.39285358\n",
      "Iteration 55283, loss = 1.33693997\n",
      "Iteration 55284, loss = 1.32751186\n",
      "Iteration 55285, loss = 1.21981653\n",
      "Iteration 55286, loss = 1.43670704\n",
      "Iteration 55287, loss = 1.40684665\n",
      "Iteration 55288, loss = 1.45123597\n",
      "Iteration 55289, loss = 1.49938985\n",
      "Iteration 55290, loss = 1.60112000\n",
      "Iteration 55291, loss = 1.43566076\n",
      "Iteration 55292, loss = 1.40688132\n",
      "Iteration 55293, loss = 1.41311165\n",
      "Iteration 55294, loss = 1.46756223\n",
      "Iteration 55295, loss = 1.60644840\n",
      "Iteration 55296, loss = 1.39048218\n",
      "Iteration 55297, loss = 1.46293141\n",
      "Iteration 55298, loss = 1.42659333\n",
      "Iteration 55299, loss = 1.45787427\n",
      "Iteration 55300, loss = 1.46304045\n",
      "Iteration 55301, loss = 1.44862181\n",
      "Iteration 55302, loss = 1.49672162\n",
      "Iteration 55303, loss = 1.54459932\n",
      "Iteration 55304, loss = 1.69563930\n",
      "Iteration 55305, loss = 1.70932908\n",
      "Iteration 55306, loss = 1.43760649\n",
      "Iteration 55307, loss = 1.48956676\n",
      "Iteration 55308, loss = 1.56822010\n",
      "Iteration 55309, loss = 1.84412943\n",
      "Iteration 55310, loss = 1.69121089\n",
      "Iteration 55311, loss = 1.72264644\n",
      "Iteration 55312, loss = 1.59521678\n",
      "Iteration 55313, loss = 1.51054326\n",
      "Iteration 55314, loss = 1.38541088\n",
      "Iteration 55315, loss = 1.46764468\n",
      "Iteration 55316, loss = 1.45229068\n",
      "Iteration 55317, loss = 1.32797674\n",
      "Iteration 55318, loss = 1.32677952\n",
      "Iteration 55319, loss = 1.33589634\n",
      "Iteration 55320, loss = 1.27888308\n",
      "Iteration 55321, loss = 1.43296605\n",
      "Iteration 55322, loss = 1.53900658\n",
      "Iteration 55323, loss = 1.44179815\n",
      "Iteration 55324, loss = 1.28252282\n",
      "Iteration 55325, loss = 1.36986662\n",
      "Iteration 55326, loss = 1.29254815\n",
      "Iteration 55327, loss = 1.37969240\n",
      "Iteration 55328, loss = 1.47897682\n",
      "Iteration 55329, loss = 1.69383829\n",
      "Iteration 55330, loss = 1.50481790\n",
      "Iteration 55331, loss = 1.55684243\n",
      "Iteration 55332, loss = 1.44377868\n",
      "Iteration 55333, loss = 1.82160773\n",
      "Iteration 55334, loss = 2.20377239\n",
      "Iteration 55335, loss = 2.68912051\n",
      "Iteration 55336, loss = 2.64009512\n",
      "Iteration 55337, loss = 3.42522528\n",
      "Iteration 55338, loss = 3.88321378\n",
      "Iteration 55339, loss = 4.08873226\n",
      "Iteration 55340, loss = 2.80157609\n",
      "Iteration 55341, loss = 2.56161503\n",
      "Iteration 55342, loss = 4.76993094\n",
      "Iteration 55343, loss = 4.88385825\n",
      "Iteration 55344, loss = 3.32486395\n",
      "Iteration 55345, loss = 2.75930868\n",
      "Iteration 55346, loss = 2.94161564\n",
      "Iteration 55347, loss = 2.89649725\n",
      "Iteration 55348, loss = 2.51430134\n",
      "Iteration 55349, loss = 1.96687186\n",
      "Iteration 55350, loss = 2.67364160\n",
      "Iteration 55351, loss = 2.12896380\n",
      "Iteration 55352, loss = 2.44510771\n",
      "Iteration 55353, loss = 2.01949433\n",
      "Iteration 55354, loss = 1.85465616\n",
      "Iteration 55355, loss = 1.45147359\n",
      "Iteration 55356, loss = 1.63095426\n",
      "Iteration 55357, loss = 2.44506784\n",
      "Iteration 55358, loss = 1.67806865\n",
      "Iteration 55359, loss = 1.82169612\n",
      "Iteration 55360, loss = 1.99806289\n",
      "Iteration 55361, loss = 2.15092070\n",
      "Iteration 55362, loss = 2.12226050\n",
      "Iteration 55363, loss = 2.21015372\n",
      "Iteration 55364, loss = 2.15275442\n",
      "Iteration 55365, loss = 1.94188985\n",
      "Iteration 55366, loss = 1.99951894\n",
      "Iteration 55367, loss = 1.93071602\n",
      "Iteration 55368, loss = 1.56351883\n",
      "Iteration 55369, loss = 2.02394330\n",
      "Iteration 55370, loss = 1.80095866\n",
      "Iteration 55371, loss = 1.96898118\n",
      "Iteration 55372, loss = 1.49595335\n",
      "Iteration 55373, loss = 1.65845060\n",
      "Iteration 55374, loss = 1.52552509\n",
      "Iteration 55375, loss = 1.69692499\n",
      "Iteration 55376, loss = 1.88159497\n",
      "Iteration 55377, loss = 1.83947346\n",
      "Iteration 55378, loss = 1.73205655\n",
      "Iteration 55379, loss = 1.48998065\n",
      "Iteration 55380, loss = 1.59626573\n",
      "Iteration 55381, loss = 1.35474873\n",
      "Iteration 55382, loss = 1.27262129\n",
      "Iteration 55383, loss = 1.33265630\n",
      "Iteration 55384, loss = 1.31825733\n",
      "Iteration 55385, loss = 1.32286711\n",
      "Iteration 55386, loss = 1.27640715\n",
      "Iteration 55387, loss = 1.33671830\n",
      "Iteration 55388, loss = 1.39919975\n",
      "Iteration 55389, loss = 2.07450506\n",
      "Iteration 55390, loss = 1.83352120\n",
      "Iteration 55391, loss = 1.80463786\n",
      "Iteration 55392, loss = 3.18395910\n",
      "Iteration 55393, loss = 2.87764895\n",
      "Iteration 55394, loss = 2.56956265\n",
      "Iteration 55395, loss = 3.08315911\n",
      "Iteration 55396, loss = 3.13462876\n",
      "Iteration 55397, loss = 5.04500910\n",
      "Iteration 55398, loss = 4.35492904\n",
      "Iteration 55399, loss = 3.73233199\n",
      "Iteration 55400, loss = 2.02824159\n",
      "Iteration 55401, loss = 1.69483368\n",
      "Iteration 55402, loss = 1.95363127\n",
      "Iteration 55403, loss = 1.80421743\n",
      "Iteration 55404, loss = 2.04802202\n",
      "Iteration 55405, loss = 2.01921384\n",
      "Iteration 55406, loss = 2.21626114\n",
      "Iteration 55407, loss = 2.15864415\n",
      "Iteration 55408, loss = 1.65363522\n",
      "Iteration 55409, loss = 1.50579114\n",
      "Iteration 55410, loss = 1.73351738\n",
      "Iteration 55411, loss = 1.41371861\n",
      "Iteration 55412, loss = 1.26978500\n",
      "Iteration 55413, loss = 1.40924016\n",
      "Iteration 55414, loss = 1.46689805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 55415, loss = 1.51667440\n",
      "Iteration 55416, loss = 1.59210190\n",
      "Iteration 55417, loss = 1.68407226\n",
      "Iteration 55418, loss = 2.02555381\n",
      "Iteration 55419, loss = 1.58920297\n",
      "Iteration 55420, loss = 1.93879443\n",
      "Iteration 55421, loss = 1.76348419\n",
      "Iteration 55422, loss = 1.75306574\n",
      "Iteration 55423, loss = 1.62626120\n",
      "Iteration 55424, loss = 1.56080899\n",
      "Iteration 55425, loss = 1.55414766\n",
      "Iteration 55426, loss = 1.44015117\n",
      "Iteration 55427, loss = 1.35735232\n",
      "Iteration 55428, loss = 1.26281909\n",
      "Iteration 55429, loss = 1.43318235\n",
      "Iteration 55430, loss = 1.38297457\n",
      "Iteration 55431, loss = 1.27616015\n",
      "Iteration 55432, loss = 1.39769462\n",
      "Iteration 55433, loss = 1.43219532\n",
      "Iteration 55434, loss = 1.55486439\n",
      "Iteration 55435, loss = 1.39599405\n",
      "Iteration 55436, loss = 1.28327820\n",
      "Iteration 55437, loss = 1.48710003\n",
      "Iteration 55438, loss = 1.25660231\n",
      "Iteration 55439, loss = 1.43679687\n",
      "Iteration 55440, loss = 1.43193398\n",
      "Iteration 55441, loss = 1.42114604\n",
      "Iteration 55442, loss = 1.40575271\n",
      "Iteration 55443, loss = 1.28785560\n",
      "Iteration 55444, loss = 1.39450995\n",
      "Iteration 55445, loss = 1.31431500\n",
      "Iteration 55446, loss = 1.39615123\n",
      "Iteration 55447, loss = 1.49375308\n",
      "Iteration 55448, loss = 1.37248494\n",
      "Iteration 55449, loss = 1.44188665\n",
      "Iteration 55450, loss = 1.55595104\n",
      "Iteration 55451, loss = 1.90857155\n",
      "Iteration 55452, loss = 2.31504035\n",
      "Iteration 55453, loss = 1.92297039\n",
      "Iteration 55454, loss = 1.66201013\n",
      "Iteration 55455, loss = 1.77196855\n",
      "Iteration 55456, loss = 2.07799852\n",
      "Iteration 55457, loss = 1.56445240\n",
      "Iteration 55458, loss = 1.34142859\n",
      "Iteration 55459, loss = 1.24266939\n",
      "Iteration 55460, loss = 1.31836854\n",
      "Iteration 55461, loss = 1.35170122\n",
      "Iteration 55462, loss = 1.50493848\n",
      "Iteration 55463, loss = 1.27439284\n",
      "Iteration 55464, loss = 1.49827430\n",
      "Iteration 55465, loss = 1.78247875\n",
      "Iteration 55466, loss = 1.50785991\n",
      "Iteration 55467, loss = 2.11878832\n",
      "Iteration 55468, loss = 2.50429701\n",
      "Iteration 55469, loss = 2.41295350\n",
      "Iteration 55470, loss = 2.94569615\n",
      "Iteration 55471, loss = 3.51163824\n",
      "Iteration 55472, loss = 3.35976449\n",
      "Iteration 55473, loss = 2.77718710\n",
      "Iteration 55474, loss = 2.02171185\n",
      "Iteration 55475, loss = 2.13963882\n",
      "Iteration 55476, loss = 1.59457975\n",
      "Iteration 55477, loss = 1.64004954\n",
      "Iteration 55478, loss = 1.56832828\n",
      "Iteration 55479, loss = 1.66135977\n",
      "Iteration 55480, loss = 1.84746932\n",
      "Iteration 55481, loss = 1.67408832\n",
      "Iteration 55482, loss = 2.06441813\n",
      "Iteration 55483, loss = 2.00389665\n",
      "Iteration 55484, loss = 1.99046939\n",
      "Iteration 55485, loss = 1.86926888\n",
      "Iteration 55486, loss = 1.70766105\n",
      "Iteration 55487, loss = 1.42212568\n",
      "Iteration 55488, loss = 1.50888091\n",
      "Iteration 55489, loss = 1.55288867\n",
      "Iteration 55490, loss = 1.64629139\n",
      "Iteration 55491, loss = 1.76461065\n",
      "Iteration 55492, loss = 1.56004524\n",
      "Iteration 55493, loss = 1.39187572\n",
      "Iteration 55494, loss = 1.27056702\n",
      "Iteration 55495, loss = 1.36646814\n",
      "Iteration 55496, loss = 1.40709751\n",
      "Iteration 55497, loss = 1.44724819\n",
      "Iteration 55498, loss = 1.53998866\n",
      "Iteration 55499, loss = 1.42187266\n",
      "Iteration 55500, loss = 1.53116046\n",
      "Iteration 55501, loss = 1.64668907\n",
      "Iteration 55502, loss = 1.49865292\n",
      "Iteration 55503, loss = 1.52208112\n",
      "Iteration 55504, loss = 1.42481529\n",
      "Iteration 55505, loss = 1.27631719\n",
      "Iteration 55506, loss = 1.33111121\n",
      "Iteration 55507, loss = 1.55418671\n",
      "Iteration 55508, loss = 1.81640239\n",
      "Iteration 55509, loss = 2.08608447\n",
      "Iteration 55510, loss = 1.78201044\n",
      "Iteration 55511, loss = 1.30147177\n",
      "Iteration 55512, loss = 1.54991956\n",
      "Iteration 55513, loss = 1.56097890\n",
      "Iteration 55514, loss = 2.09722033\n",
      "Iteration 55515, loss = 1.88335576\n",
      "Iteration 55516, loss = 1.86878600\n",
      "Iteration 55517, loss = 1.71330854\n",
      "Iteration 55518, loss = 1.85625591\n",
      "Iteration 55519, loss = 1.83730100\n",
      "Iteration 55520, loss = 1.83029863\n",
      "Iteration 55521, loss = 1.51788583\n",
      "Iteration 55522, loss = 1.26883557\n",
      "Iteration 55523, loss = 1.37587938\n",
      "Iteration 55524, loss = 1.21321299\n",
      "Iteration 55525, loss = 1.36143418\n",
      "Iteration 55526, loss = 1.69423716\n",
      "Iteration 55527, loss = 2.00065734\n",
      "Iteration 55528, loss = 1.68115127\n",
      "Iteration 55529, loss = 1.44885088\n",
      "Iteration 55530, loss = 1.58036098\n",
      "Iteration 55531, loss = 1.72128975\n",
      "Iteration 55532, loss = 1.64812141\n",
      "Iteration 55533, loss = 1.86474003\n",
      "Iteration 55534, loss = 1.68129468\n",
      "Iteration 55535, loss = 1.87236221\n",
      "Iteration 55536, loss = 1.56518726\n",
      "Iteration 55537, loss = 1.48605631\n",
      "Iteration 55538, loss = 1.50641334\n",
      "Iteration 55539, loss = 1.43089512\n",
      "Iteration 55540, loss = 1.37208633\n",
      "Iteration 55541, loss = 1.42872163\n",
      "Iteration 55542, loss = 1.37426118\n",
      "Iteration 55543, loss = 1.57121944\n",
      "Iteration 55544, loss = 1.61328803\n",
      "Iteration 55545, loss = 1.61698385\n",
      "Iteration 55546, loss = 1.82534785\n",
      "Iteration 55547, loss = 1.59072975\n",
      "Iteration 55548, loss = 1.54538285\n",
      "Iteration 55549, loss = 1.63331591\n",
      "Iteration 55550, loss = 1.86833488\n",
      "Iteration 55551, loss = 2.24000482\n",
      "Iteration 55552, loss = 2.27241699\n",
      "Iteration 55553, loss = 3.51091403\n",
      "Iteration 55554, loss = 2.56462619\n",
      "Iteration 55555, loss = 1.52883472\n",
      "Iteration 55556, loss = 1.55944107\n",
      "Iteration 55557, loss = 1.56864411\n",
      "Iteration 55558, loss = 1.54895728\n",
      "Iteration 55559, loss = 1.77842622\n",
      "Iteration 55560, loss = 1.74870072\n",
      "Iteration 55561, loss = 1.98943271\n",
      "Iteration 55562, loss = 1.94852205\n",
      "Iteration 55563, loss = 1.61225633\n",
      "Iteration 55564, loss = 1.36425292\n",
      "Iteration 55565, loss = 1.52188214\n",
      "Iteration 55566, loss = 1.36121215\n",
      "Iteration 55567, loss = 1.52052212\n",
      "Iteration 55568, loss = 1.51852154\n",
      "Iteration 55569, loss = 1.52250082\n",
      "Iteration 55570, loss = 1.85105602\n",
      "Iteration 55571, loss = 1.49273838\n",
      "Iteration 55572, loss = 1.47718816\n",
      "Iteration 55573, loss = 1.47841113\n",
      "Iteration 55574, loss = 1.50208689\n",
      "Iteration 55575, loss = 1.66102644\n",
      "Iteration 55576, loss = 1.62073458\n",
      "Iteration 55577, loss = 1.56076409\n",
      "Iteration 55578, loss = 1.49133008\n",
      "Iteration 55579, loss = 1.52379117\n",
      "Iteration 55580, loss = 1.53908476\n",
      "Iteration 55581, loss = 1.39136606\n",
      "Iteration 55582, loss = 1.56353215\n",
      "Iteration 55583, loss = 1.50389116\n",
      "Iteration 55584, loss = 1.28811382\n",
      "Iteration 55585, loss = 1.36310565\n",
      "Iteration 55586, loss = 1.28208260\n",
      "Iteration 55587, loss = 1.27048074\n",
      "Iteration 55588, loss = 1.38886757\n",
      "Iteration 55589, loss = 1.35255119\n",
      "Iteration 55590, loss = 1.84220088\n",
      "Iteration 55591, loss = 1.99578387\n",
      "Iteration 55592, loss = 2.25809768\n",
      "Iteration 55593, loss = 1.69490379\n",
      "Iteration 55594, loss = 1.60944144\n",
      "Iteration 55595, loss = 1.37512674\n",
      "Iteration 55596, loss = 1.49037343\n",
      "Iteration 55597, loss = 2.15328215\n",
      "Iteration 55598, loss = 2.18225212\n",
      "Iteration 55599, loss = 1.54595368\n",
      "Iteration 55600, loss = 1.85604794\n",
      "Iteration 55601, loss = 1.48645042\n",
      "Iteration 55602, loss = 1.62201870\n",
      "Iteration 55603, loss = 1.65442242\n",
      "Iteration 55604, loss = 1.50938120\n",
      "Iteration 55605, loss = 1.52941895\n",
      "Iteration 55606, loss = 1.69079558\n",
      "Iteration 55607, loss = 1.85591638\n",
      "Iteration 55608, loss = 2.18269447\n",
      "Iteration 55609, loss = 1.82477298\n",
      "Iteration 55610, loss = 1.69220189\n",
      "Iteration 55611, loss = 1.57379956\n",
      "Iteration 55612, loss = 1.33458730\n",
      "Iteration 55613, loss = 1.38017752\n",
      "Iteration 55614, loss = 1.37351880\n",
      "Iteration 55615, loss = 1.35216840\n",
      "Iteration 55616, loss = 1.58394273\n",
      "Iteration 55617, loss = 1.47473477\n",
      "Iteration 55618, loss = 1.89615587\n",
      "Iteration 55619, loss = 2.24467020\n",
      "Iteration 55620, loss = 1.81867492\n",
      "Iteration 55621, loss = 1.97333102\n",
      "Iteration 55622, loss = 1.61515424\n",
      "Iteration 55623, loss = 1.75894129\n",
      "Iteration 55624, loss = 1.63503653\n",
      "Iteration 55625, loss = 1.55899474\n",
      "Iteration 55626, loss = 1.46661362\n",
      "Iteration 55627, loss = 1.39930051\n",
      "Iteration 55628, loss = 1.59452654\n",
      "Iteration 55629, loss = 1.44517147\n",
      "Iteration 55630, loss = 1.32292816\n",
      "Iteration 55631, loss = 1.23830702\n",
      "Iteration 55632, loss = 1.45287699\n",
      "Iteration 55633, loss = 1.56061213\n",
      "Iteration 55634, loss = 1.38283926\n",
      "Iteration 55635, loss = 1.57923693\n",
      "Iteration 55636, loss = 1.38045169\n",
      "Iteration 55637, loss = 1.52391437\n",
      "Iteration 55638, loss = 1.59191368\n",
      "Iteration 55639, loss = 1.48604113\n",
      "Iteration 55640, loss = 1.54427587\n",
      "Iteration 55641, loss = 1.60869574\n",
      "Iteration 55642, loss = 1.66914667\n",
      "Iteration 55643, loss = 1.94071363\n",
      "Iteration 55644, loss = 1.73080220\n",
      "Iteration 55645, loss = 1.50850488\n",
      "Iteration 55646, loss = 1.39986091\n",
      "Iteration 55647, loss = 1.38571184\n",
      "Iteration 55648, loss = 1.28416237\n",
      "Iteration 55649, loss = 1.24380459\n",
      "Iteration 55650, loss = 1.64445662\n",
      "Iteration 55651, loss = 2.09766626\n",
      "Iteration 55652, loss = 1.95479909\n",
      "Iteration 55653, loss = 1.87429934\n",
      "Iteration 55654, loss = 1.95671379\n",
      "Iteration 55655, loss = 2.56289697\n",
      "Iteration 55656, loss = 2.27368308\n",
      "Iteration 55657, loss = 2.33177273\n",
      "Iteration 55658, loss = 2.71139023\n",
      "Iteration 55659, loss = 1.77202279\n",
      "Iteration 55660, loss = 1.69337249\n",
      "Iteration 55661, loss = 1.89641958\n",
      "Iteration 55662, loss = 1.78419615\n",
      "Iteration 55663, loss = 1.92314029\n",
      "Iteration 55664, loss = 1.71778822\n",
      "Iteration 55665, loss = 1.55560801\n",
      "Iteration 55666, loss = 1.74733805\n",
      "Iteration 55667, loss = 1.97034071\n",
      "Iteration 55668, loss = 1.63461269\n",
      "Iteration 55669, loss = 2.12044315\n",
      "Iteration 55670, loss = 2.01293602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 55671, loss = 1.89906184\n",
      "Iteration 55672, loss = 1.81135010\n",
      "Iteration 55673, loss = 1.84664397\n",
      "Iteration 55674, loss = 1.71262562\n",
      "Iteration 55675, loss = 1.52081186\n",
      "Iteration 55676, loss = 1.68028753\n",
      "Iteration 55677, loss = 1.75497075\n",
      "Iteration 55678, loss = 1.78650463\n",
      "Iteration 55679, loss = 1.59146169\n",
      "Iteration 55680, loss = 1.64588983\n",
      "Iteration 55681, loss = 1.48705844\n",
      "Iteration 55682, loss = 1.44719180\n",
      "Iteration 55683, loss = 1.63639610\n",
      "Iteration 55684, loss = 1.35594228\n",
      "Iteration 55685, loss = 1.71675961\n",
      "Iteration 55686, loss = 1.55409831\n",
      "Iteration 55687, loss = 1.47093539\n",
      "Iteration 55688, loss = 1.35368269\n",
      "Iteration 55689, loss = 1.35803300\n",
      "Iteration 55690, loss = 1.28294505\n",
      "Iteration 55691, loss = 1.28628090\n",
      "Iteration 55692, loss = 1.36944853\n",
      "Iteration 55693, loss = 1.41466498\n",
      "Iteration 55694, loss = 1.35883947\n",
      "Iteration 55695, loss = 1.48932637\n",
      "Iteration 55696, loss = 1.35234329\n",
      "Iteration 55697, loss = 1.45609597\n",
      "Iteration 55698, loss = 1.47259295\n",
      "Iteration 55699, loss = 1.56471002\n",
      "Iteration 55700, loss = 1.61977393\n",
      "Iteration 55701, loss = 1.63146433\n",
      "Iteration 55702, loss = 1.55624053\n",
      "Iteration 55703, loss = 1.77373528\n",
      "Iteration 55704, loss = 1.71804040\n",
      "Iteration 55705, loss = 2.29591676\n",
      "Iteration 55706, loss = 2.13586468\n",
      "Iteration 55707, loss = 1.70765336\n",
      "Iteration 55708, loss = 1.80983003\n",
      "Iteration 55709, loss = 1.99121699\n",
      "Iteration 55710, loss = 2.08258284\n",
      "Iteration 55711, loss = 1.61349335\n",
      "Iteration 55712, loss = 1.53648613\n",
      "Iteration 55713, loss = 1.43325952\n",
      "Iteration 55714, loss = 1.47219082\n",
      "Iteration 55715, loss = 1.73982029\n",
      "Iteration 55716, loss = 1.54918792\n",
      "Iteration 55717, loss = 1.42696827\n",
      "Iteration 55718, loss = 1.24692815\n",
      "Iteration 55719, loss = 1.35257325\n",
      "Iteration 55720, loss = 1.46044946\n",
      "Iteration 55721, loss = 1.57622086\n",
      "Iteration 55722, loss = 1.45688501\n",
      "Iteration 55723, loss = 1.62662070\n",
      "Iteration 55724, loss = 1.74129694\n",
      "Iteration 55725, loss = 1.54732176\n",
      "Iteration 55726, loss = 1.29385588\n",
      "Iteration 55727, loss = 1.32810857\n",
      "Iteration 55728, loss = 1.35364576\n",
      "Iteration 55729, loss = 1.45172589\n",
      "Iteration 55730, loss = 1.45649777\n",
      "Iteration 55731, loss = 1.41980152\n",
      "Iteration 55732, loss = 1.40514639\n",
      "Iteration 55733, loss = 1.40549408\n",
      "Iteration 55734, loss = 1.62411030\n",
      "Iteration 55735, loss = 1.88642215\n",
      "Iteration 55736, loss = 1.49448559\n",
      "Iteration 55737, loss = 1.40886922\n",
      "Iteration 55738, loss = 1.24072060\n",
      "Iteration 55739, loss = 1.30617257\n",
      "Iteration 55740, loss = 1.42142836\n",
      "Iteration 55741, loss = 1.56962765\n",
      "Iteration 55742, loss = 1.70370262\n",
      "Iteration 55743, loss = 1.77236474\n",
      "Iteration 55744, loss = 1.92502650\n",
      "Iteration 55745, loss = 1.94902768\n",
      "Iteration 55746, loss = 2.21497359\n",
      "Iteration 55747, loss = 1.48376562\n",
      "Iteration 55748, loss = 1.39802037\n",
      "Iteration 55749, loss = 1.71573889\n",
      "Iteration 55750, loss = 1.57717904\n",
      "Iteration 55751, loss = 1.55965901\n",
      "Iteration 55752, loss = 1.55478779\n",
      "Iteration 55753, loss = 1.73040791\n",
      "Iteration 55754, loss = 1.84105049\n",
      "Iteration 55755, loss = 1.68820978\n",
      "Iteration 55756, loss = 1.71284330\n",
      "Iteration 55757, loss = 1.87322096\n",
      "Iteration 55758, loss = 1.54234804\n",
      "Iteration 55759, loss = 1.33545346\n",
      "Iteration 55760, loss = 1.37083758\n",
      "Iteration 55761, loss = 1.36871741\n",
      "Iteration 55762, loss = 1.38464010\n",
      "Iteration 55763, loss = 1.32854271\n",
      "Iteration 55764, loss = 1.37541345\n",
      "Iteration 55765, loss = 1.27370929\n",
      "Iteration 55766, loss = 1.47931914\n",
      "Iteration 55767, loss = 1.53269316\n",
      "Iteration 55768, loss = 1.77952560\n",
      "Iteration 55769, loss = 1.71395224\n",
      "Iteration 55770, loss = 1.63778685\n",
      "Iteration 55771, loss = 1.74797461\n",
      "Iteration 55772, loss = 2.39743462\n",
      "Iteration 55773, loss = 2.43879329\n",
      "Iteration 55774, loss = 1.66582725\n",
      "Iteration 55775, loss = 1.62312674\n",
      "Iteration 55776, loss = 1.46286177\n",
      "Iteration 55777, loss = 1.51780647\n",
      "Iteration 55778, loss = 1.41629454\n",
      "Iteration 55779, loss = 1.48586473\n",
      "Iteration 55780, loss = 1.46172933\n",
      "Iteration 55781, loss = 1.35629683\n",
      "Iteration 55782, loss = 1.82555458\n",
      "Iteration 55783, loss = 2.02243074\n",
      "Iteration 55784, loss = 2.02791330\n",
      "Iteration 55785, loss = 2.38001582\n",
      "Iteration 55786, loss = 2.28601831\n",
      "Iteration 55787, loss = 2.56802274\n",
      "Iteration 55788, loss = 2.75289530\n",
      "Iteration 55789, loss = 2.67968825\n",
      "Iteration 55790, loss = 2.79340796\n",
      "Iteration 55791, loss = 2.92304715\n",
      "Iteration 55792, loss = 2.21889415\n",
      "Iteration 55793, loss = 2.24505816\n",
      "Iteration 55794, loss = 2.45304920\n",
      "Iteration 55795, loss = 1.95089112\n",
      "Iteration 55796, loss = 2.61853738\n",
      "Iteration 55797, loss = 2.48199029\n",
      "Iteration 55798, loss = 2.36354866\n",
      "Iteration 55799, loss = 2.82554705\n",
      "Iteration 55800, loss = 2.72087078\n",
      "Iteration 55801, loss = 2.04648722\n",
      "Iteration 55802, loss = 1.75030962\n",
      "Iteration 55803, loss = 1.70256561\n",
      "Iteration 55804, loss = 1.60481884\n",
      "Iteration 55805, loss = 1.52529348\n",
      "Iteration 55806, loss = 1.59972652\n",
      "Iteration 55807, loss = 1.96569720\n",
      "Iteration 55808, loss = 2.00784849\n",
      "Iteration 55809, loss = 3.08716005\n",
      "Iteration 55810, loss = 2.73443885\n",
      "Iteration 55811, loss = 2.55323127\n",
      "Iteration 55812, loss = 2.29989357\n",
      "Iteration 55813, loss = 1.68012744\n",
      "Iteration 55814, loss = 1.46139660\n",
      "Iteration 55815, loss = 1.65133150\n",
      "Iteration 55816, loss = 1.68196316\n",
      "Iteration 55817, loss = 1.68607699\n",
      "Iteration 55818, loss = 1.67209083\n",
      "Iteration 55819, loss = 1.71493001\n",
      "Iteration 55820, loss = 1.70272526\n",
      "Iteration 55821, loss = 1.45624929\n",
      "Iteration 55822, loss = 1.46866059\n",
      "Iteration 55823, loss = 1.42423675\n",
      "Iteration 55824, loss = 1.42282154\n",
      "Iteration 55825, loss = 1.34448833\n",
      "Iteration 55826, loss = 1.32211873\n",
      "Iteration 55827, loss = 1.60750117\n",
      "Iteration 55828, loss = 1.66521139\n",
      "Iteration 55829, loss = 1.43369926\n",
      "Iteration 55830, loss = 1.64824143\n",
      "Iteration 55831, loss = 1.60268143\n",
      "Iteration 55832, loss = 1.58532309\n",
      "Iteration 55833, loss = 1.92098762\n",
      "Iteration 55834, loss = 1.80944117\n",
      "Iteration 55835, loss = 1.53565252\n",
      "Iteration 55836, loss = 1.80490422\n",
      "Iteration 55837, loss = 2.03808825\n",
      "Iteration 55838, loss = 1.94777139\n",
      "Iteration 55839, loss = 1.56026857\n",
      "Iteration 55840, loss = 1.42334311\n",
      "Iteration 55841, loss = 1.60837506\n",
      "Iteration 55842, loss = 1.40761904\n",
      "Iteration 55843, loss = 1.34714540\n",
      "Iteration 55844, loss = 1.55241805\n",
      "Iteration 55845, loss = 1.65362343\n",
      "Iteration 55846, loss = 2.03081945\n",
      "Iteration 55847, loss = 1.66733667\n",
      "Iteration 55848, loss = 2.12821291\n",
      "Iteration 55849, loss = 1.52998849\n",
      "Iteration 55850, loss = 1.59857609\n",
      "Iteration 55851, loss = 1.48516320\n",
      "Iteration 55852, loss = 2.05979814\n",
      "Iteration 55853, loss = 2.03139775\n",
      "Iteration 55854, loss = 3.13251622\n",
      "Iteration 55855, loss = 2.03431898\n",
      "Iteration 55856, loss = 2.43431786\n",
      "Iteration 55857, loss = 1.92011091\n",
      "Iteration 55858, loss = 1.70283814\n",
      "Iteration 55859, loss = 1.57191708\n",
      "Iteration 55860, loss = 1.47480797\n",
      "Iteration 55861, loss = 1.40561735\n",
      "Iteration 55862, loss = 1.25800444\n",
      "Iteration 55863, loss = 1.24376230\n",
      "Iteration 55864, loss = 1.27110377\n",
      "Iteration 55865, loss = 1.25949613\n",
      "Iteration 55866, loss = 1.26862520\n",
      "Iteration 55867, loss = 1.31281973\n",
      "Iteration 55868, loss = 1.34082728\n",
      "Iteration 55869, loss = 1.47112870\n",
      "Iteration 55870, loss = 1.39422724\n",
      "Iteration 55871, loss = 1.37859973\n",
      "Iteration 55872, loss = 1.22879245\n",
      "Iteration 55873, loss = 1.24222696\n",
      "Iteration 55874, loss = 1.46467669\n",
      "Iteration 55875, loss = 1.89289672\n",
      "Iteration 55876, loss = 1.72958909\n",
      "Iteration 55877, loss = 1.56507663\n",
      "Iteration 55878, loss = 2.14065723\n",
      "Iteration 55879, loss = 2.09787890\n",
      "Iteration 55880, loss = 3.08643973\n",
      "Iteration 55881, loss = 2.52671631\n",
      "Iteration 55882, loss = 2.62872833\n",
      "Iteration 55883, loss = 1.89671231\n",
      "Iteration 55884, loss = 2.51618101\n",
      "Iteration 55885, loss = 2.44133359\n",
      "Iteration 55886, loss = 1.71518092\n",
      "Iteration 55887, loss = 1.50636893\n",
      "Iteration 55888, loss = 1.75775419\n",
      "Iteration 55889, loss = 1.55248548\n",
      "Iteration 55890, loss = 1.63979415\n",
      "Iteration 55891, loss = 1.54494489\n",
      "Iteration 55892, loss = 1.46176320\n",
      "Iteration 55893, loss = 1.47297073\n",
      "Iteration 55894, loss = 1.43047800\n",
      "Iteration 55895, loss = 1.32439652\n",
      "Iteration 55896, loss = 1.32242595\n",
      "Iteration 55897, loss = 1.29954194\n",
      "Iteration 55898, loss = 1.35843771\n",
      "Iteration 55899, loss = 1.65099451\n",
      "Iteration 55900, loss = 1.64453634\n",
      "Iteration 55901, loss = 1.38970099\n",
      "Iteration 55902, loss = 1.39464231\n",
      "Iteration 55903, loss = 1.45251763\n",
      "Iteration 55904, loss = 1.39033011\n",
      "Iteration 55905, loss = 1.26019144\n",
      "Iteration 55906, loss = 1.43463635\n",
      "Iteration 55907, loss = 1.53964708\n",
      "Iteration 55908, loss = 1.25197659\n",
      "Iteration 55909, loss = 1.51111689\n",
      "Iteration 55910, loss = 1.64975657\n",
      "Iteration 55911, loss = 1.89897567\n",
      "Iteration 55912, loss = 1.88862674\n",
      "Iteration 55913, loss = 1.67518563\n",
      "Iteration 55914, loss = 1.46270843\n",
      "Iteration 55915, loss = 1.55080731\n",
      "Iteration 55916, loss = 1.84786845\n",
      "Iteration 55917, loss = 2.12176321\n",
      "Iteration 55918, loss = 2.99329277\n",
      "Iteration 55919, loss = 2.13983185\n",
      "Iteration 55920, loss = 4.55174806\n",
      "Iteration 55921, loss = 2.74876270\n",
      "Iteration 55922, loss = 2.70960165\n",
      "Iteration 55923, loss = 3.27095245\n",
      "Iteration 55924, loss = 3.16010041\n",
      "Iteration 55925, loss = 3.79104945\n",
      "Iteration 55926, loss = 4.20178630\n",
      "Iteration 55927, loss = 3.78238769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 55928, loss = 3.64855401\n",
      "Iteration 55929, loss = 3.29758131\n",
      "Iteration 55930, loss = 4.20924060\n",
      "Iteration 55931, loss = 4.00293397\n",
      "Iteration 55932, loss = 5.52811257\n",
      "Iteration 55933, loss = 4.79418383\n",
      "Iteration 55934, loss = 3.91759470\n",
      "Iteration 55935, loss = 3.51507334\n",
      "Iteration 55936, loss = 3.17054250\n",
      "Iteration 55937, loss = 2.70535305\n",
      "Iteration 55938, loss = 2.54170427\n",
      "Iteration 55939, loss = 2.09576495\n",
      "Iteration 55940, loss = 1.99524317\n",
      "Iteration 55941, loss = 1.50156372\n",
      "Iteration 55942, loss = 1.46397876\n",
      "Iteration 55943, loss = 1.54404066\n",
      "Iteration 55944, loss = 1.54439663\n",
      "Iteration 55945, loss = 1.38345801\n",
      "Iteration 55946, loss = 1.43766371\n",
      "Iteration 55947, loss = 1.86940879\n",
      "Iteration 55948, loss = 1.73762652\n",
      "Iteration 55949, loss = 1.36102375\n",
      "Iteration 55950, loss = 1.28662701\n",
      "Iteration 55951, loss = 1.46161048\n",
      "Iteration 55952, loss = 1.51273977\n",
      "Iteration 55953, loss = 1.61608228\n",
      "Iteration 55954, loss = 1.33330106\n",
      "Iteration 55955, loss = 1.29447606\n",
      "Iteration 55956, loss = 1.37705366\n",
      "Iteration 55957, loss = 1.25760303\n",
      "Iteration 55958, loss = 1.22399716\n",
      "Iteration 55959, loss = 1.24737137\n",
      "Iteration 55960, loss = 1.23168197\n",
      "Iteration 55961, loss = 1.25483775\n",
      "Iteration 55962, loss = 1.53262462\n",
      "Iteration 55963, loss = 1.46188231\n",
      "Iteration 55964, loss = 1.74277117\n",
      "Iteration 55965, loss = 1.69867967\n",
      "Iteration 55966, loss = 1.45229750\n",
      "Iteration 55967, loss = 1.36851132\n",
      "Iteration 55968, loss = 1.42321647\n",
      "Iteration 55969, loss = 1.21026139\n",
      "Iteration 55970, loss = 1.32550413\n",
      "Iteration 55971, loss = 1.39069316\n",
      "Iteration 55972, loss = 1.65397107\n",
      "Iteration 55973, loss = 1.82149902\n",
      "Iteration 55974, loss = 1.47882811\n",
      "Iteration 55975, loss = 1.60418706\n",
      "Iteration 55976, loss = 1.42182900\n",
      "Iteration 55977, loss = 1.65454403\n",
      "Iteration 55978, loss = 1.72971726\n",
      "Iteration 55979, loss = 1.72004058\n",
      "Iteration 55980, loss = 1.75699943\n",
      "Iteration 55981, loss = 1.71931495\n",
      "Iteration 55982, loss = 2.27040612\n",
      "Iteration 55983, loss = 2.02873066\n",
      "Iteration 55984, loss = 2.22579850\n",
      "Iteration 55985, loss = 1.88377898\n",
      "Iteration 55986, loss = 1.81939813\n",
      "Iteration 55987, loss = 1.70778629\n",
      "Iteration 55988, loss = 1.42610683\n",
      "Iteration 55989, loss = 1.38815425\n",
      "Iteration 55990, loss = 1.82069078\n",
      "Iteration 55991, loss = 1.58425596\n",
      "Iteration 55992, loss = 1.64291579\n",
      "Iteration 55993, loss = 1.54995229\n",
      "Iteration 55994, loss = 1.43130952\n",
      "Iteration 55995, loss = 1.46874021\n",
      "Iteration 55996, loss = 1.44475816\n",
      "Iteration 55997, loss = 1.60107499\n",
      "Iteration 55998, loss = 1.38425465\n",
      "Iteration 55999, loss = 1.48770319\n",
      "Iteration 56000, loss = 1.40016659\n",
      "Iteration 56001, loss = 1.38843683\n",
      "Iteration 56002, loss = 1.42428417\n",
      "Iteration 56003, loss = 1.31729325\n",
      "Iteration 56004, loss = 1.38966747\n",
      "Iteration 56005, loss = 1.58762919\n",
      "Iteration 56006, loss = 1.67347022\n",
      "Iteration 56007, loss = 1.34256512\n",
      "Iteration 56008, loss = 1.28170731\n",
      "Iteration 56009, loss = 1.44833063\n",
      "Iteration 56010, loss = 1.39402922\n",
      "Iteration 56011, loss = 1.32658111\n",
      "Iteration 56012, loss = 1.41931574\n",
      "Iteration 56013, loss = 1.64112362\n",
      "Iteration 56014, loss = 1.45895645\n",
      "Iteration 56015, loss = 1.30794041\n",
      "Iteration 56016, loss = 1.47566576\n",
      "Iteration 56017, loss = 1.56390670\n",
      "Iteration 56018, loss = 1.45054563\n",
      "Iteration 56019, loss = 1.39451175\n",
      "Iteration 56020, loss = 1.40181227\n",
      "Iteration 56021, loss = 1.56254543\n",
      "Iteration 56022, loss = 1.55464785\n",
      "Iteration 56023, loss = 1.40627755\n",
      "Iteration 56024, loss = 1.33406434\n",
      "Iteration 56025, loss = 1.52721912\n",
      "Iteration 56026, loss = 1.83135531\n",
      "Iteration 56027, loss = 1.42451397\n",
      "Iteration 56028, loss = 1.51588694\n",
      "Iteration 56029, loss = 1.52243234\n",
      "Iteration 56030, loss = 1.30134502\n",
      "Iteration 56031, loss = 1.46013395\n",
      "Iteration 56032, loss = 1.36225378\n",
      "Iteration 56033, loss = 1.35958359\n",
      "Iteration 56034, loss = 1.60348466\n",
      "Iteration 56035, loss = 1.69232522\n",
      "Iteration 56036, loss = 1.44622257\n",
      "Iteration 56037, loss = 1.61891835\n",
      "Iteration 56038, loss = 1.46917282\n",
      "Iteration 56039, loss = 1.42625180\n",
      "Iteration 56040, loss = 1.63436449\n",
      "Iteration 56041, loss = 2.00090784\n",
      "Iteration 56042, loss = 2.27445348\n",
      "Iteration 56043, loss = 2.98999320\n",
      "Iteration 56044, loss = 1.91815653\n",
      "Iteration 56045, loss = 1.75230419\n",
      "Iteration 56046, loss = 1.50030408\n",
      "Iteration 56047, loss = 1.80977778\n",
      "Iteration 56048, loss = 1.75000513\n",
      "Iteration 56049, loss = 1.85579214\n",
      "Iteration 56050, loss = 1.89712373\n",
      "Iteration 56051, loss = 1.76211978\n",
      "Iteration 56052, loss = 1.40538747\n",
      "Iteration 56053, loss = 1.41862172\n",
      "Iteration 56054, loss = 1.27473865\n",
      "Iteration 56055, loss = 1.37871598\n",
      "Iteration 56056, loss = 1.58327471\n",
      "Iteration 56057, loss = 1.45116665\n",
      "Iteration 56058, loss = 1.53158609\n",
      "Iteration 56059, loss = 1.86201340\n",
      "Iteration 56060, loss = 2.44421095\n",
      "Iteration 56061, loss = 1.99101050\n",
      "Iteration 56062, loss = 2.77940313\n",
      "Iteration 56063, loss = 2.41971164\n",
      "Iteration 56064, loss = 2.08742595\n",
      "Iteration 56065, loss = 1.89679790\n",
      "Iteration 56066, loss = 2.18897679\n",
      "Iteration 56067, loss = 1.87453928\n",
      "Iteration 56068, loss = 1.96775562\n",
      "Iteration 56069, loss = 2.38021442\n",
      "Iteration 56070, loss = 2.37337692\n",
      "Iteration 56071, loss = 1.84769218\n",
      "Iteration 56072, loss = 2.41433859\n",
      "Iteration 56073, loss = 2.07838445\n",
      "Iteration 56074, loss = 2.19312522\n",
      "Iteration 56075, loss = 1.88462841\n",
      "Iteration 56076, loss = 2.77388588\n",
      "Iteration 56077, loss = 2.88080354\n",
      "Iteration 56078, loss = 1.90867242\n",
      "Iteration 56079, loss = 1.45485824\n",
      "Iteration 56080, loss = 1.55836454\n",
      "Iteration 56081, loss = 1.36483112\n",
      "Iteration 56082, loss = 1.33345567\n",
      "Iteration 56083, loss = 1.46732696\n",
      "Iteration 56084, loss = 1.62256119\n",
      "Iteration 56085, loss = 1.45988204\n",
      "Iteration 56086, loss = 1.51655287\n",
      "Iteration 56087, loss = 1.39800398\n",
      "Iteration 56088, loss = 1.34016448\n",
      "Iteration 56089, loss = 1.59490179\n",
      "Iteration 56090, loss = 2.16341764\n",
      "Iteration 56091, loss = 2.25085242\n",
      "Iteration 56092, loss = 1.98112839\n",
      "Iteration 56093, loss = 1.84807225\n",
      "Iteration 56094, loss = 1.82774843\n",
      "Iteration 56095, loss = 1.75692751\n",
      "Iteration 56096, loss = 1.56444641\n",
      "Iteration 56097, loss = 2.09237882\n",
      "Iteration 56098, loss = 1.50575597\n",
      "Iteration 56099, loss = 1.62035338\n",
      "Iteration 56100, loss = 1.61294456\n",
      "Iteration 56101, loss = 1.57912174\n",
      "Iteration 56102, loss = 1.74625502\n",
      "Iteration 56103, loss = 1.60997276\n",
      "Iteration 56104, loss = 1.42784754\n",
      "Iteration 56105, loss = 1.87844098\n",
      "Iteration 56106, loss = 2.57833345\n",
      "Iteration 56107, loss = 2.86345519\n",
      "Iteration 56108, loss = 2.54902046\n",
      "Iteration 56109, loss = 1.78976715\n",
      "Iteration 56110, loss = 1.62981853\n",
      "Iteration 56111, loss = 1.86183352\n",
      "Iteration 56112, loss = 1.79101635\n",
      "Iteration 56113, loss = 1.45609000\n",
      "Iteration 56114, loss = 1.37058780\n",
      "Iteration 56115, loss = 1.37119406\n",
      "Iteration 56116, loss = 1.38813358\n",
      "Iteration 56117, loss = 1.55449904\n",
      "Iteration 56118, loss = 1.53073174\n",
      "Iteration 56119, loss = 1.53390442\n",
      "Iteration 56120, loss = 1.60226250\n",
      "Iteration 56121, loss = 1.42715253\n",
      "Iteration 56122, loss = 1.38740921\n",
      "Iteration 56123, loss = 1.57023619\n",
      "Iteration 56124, loss = 1.81771941\n",
      "Iteration 56125, loss = 1.63596496\n",
      "Iteration 56126, loss = 2.66922183\n",
      "Iteration 56127, loss = 2.57263016\n",
      "Iteration 56128, loss = 2.65825533\n",
      "Iteration 56129, loss = 2.22734339\n",
      "Iteration 56130, loss = 1.88565578\n",
      "Iteration 56131, loss = 1.88930466\n",
      "Iteration 56132, loss = 1.83093060\n",
      "Iteration 56133, loss = 2.03430099\n",
      "Iteration 56134, loss = 1.92461698\n",
      "Iteration 56135, loss = 1.81486803\n",
      "Iteration 56136, loss = 1.47564579\n",
      "Iteration 56137, loss = 1.57344188\n",
      "Iteration 56138, loss = 1.36538874\n",
      "Iteration 56139, loss = 1.38442103\n",
      "Iteration 56140, loss = 1.38738589\n",
      "Iteration 56141, loss = 1.35550444\n",
      "Iteration 56142, loss = 1.34944802\n",
      "Iteration 56143, loss = 1.47599563\n",
      "Iteration 56144, loss = 1.77362683\n",
      "Iteration 56145, loss = 1.73063937\n",
      "Iteration 56146, loss = 2.00484750\n",
      "Iteration 56147, loss = 2.05740524\n",
      "Iteration 56148, loss = 1.63512644\n",
      "Iteration 56149, loss = 1.91290691\n",
      "Iteration 56150, loss = 1.78667687\n",
      "Iteration 56151, loss = 1.43619492\n",
      "Iteration 56152, loss = 1.37707621\n",
      "Iteration 56153, loss = 1.32223517\n",
      "Iteration 56154, loss = 1.40352590\n",
      "Iteration 56155, loss = 1.53430307\n",
      "Iteration 56156, loss = 1.62902316\n",
      "Iteration 56157, loss = 1.51936783\n",
      "Iteration 56158, loss = 1.65425215\n",
      "Iteration 56159, loss = 2.26258473\n",
      "Iteration 56160, loss = 2.73054162\n",
      "Iteration 56161, loss = 2.80279311\n",
      "Iteration 56162, loss = 2.48790939\n",
      "Iteration 56163, loss = 2.15388581\n",
      "Iteration 56164, loss = 1.69586842\n",
      "Iteration 56165, loss = 1.60977554\n",
      "Iteration 56166, loss = 1.99200980\n",
      "Iteration 56167, loss = 1.70021863\n",
      "Iteration 56168, loss = 1.85540143\n",
      "Iteration 56169, loss = 1.77835896\n",
      "Iteration 56170, loss = 1.65579360\n",
      "Iteration 56171, loss = 1.47326379\n",
      "Iteration 56172, loss = 1.59256529\n",
      "Iteration 56173, loss = 1.79062774\n",
      "Iteration 56174, loss = 2.72841215\n",
      "Iteration 56175, loss = 2.66850724\n",
      "Iteration 56176, loss = 1.92039938\n",
      "Iteration 56177, loss = 1.86339385\n",
      "Iteration 56178, loss = 1.75647017\n",
      "Iteration 56179, loss = 1.91690655\n",
      "Iteration 56180, loss = 1.76459876\n",
      "Iteration 56181, loss = 1.57042772\n",
      "Iteration 56182, loss = 1.50260864\n",
      "Iteration 56183, loss = 1.50366055\n",
      "Iteration 56184, loss = 1.63665218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 56185, loss = 1.61212480\n",
      "Iteration 56186, loss = 1.92020824\n",
      "Iteration 56187, loss = 1.76502626\n",
      "Iteration 56188, loss = 1.87332550\n",
      "Iteration 56189, loss = 1.76302204\n",
      "Iteration 56190, loss = 1.67445884\n",
      "Iteration 56191, loss = 1.59397623\n",
      "Iteration 56192, loss = 2.15006624\n",
      "Iteration 56193, loss = 1.53548112\n",
      "Iteration 56194, loss = 1.43176663\n",
      "Iteration 56195, loss = 1.79376341\n",
      "Iteration 56196, loss = 1.81113431\n",
      "Iteration 56197, loss = 1.38867677\n",
      "Iteration 56198, loss = 1.30862384\n",
      "Iteration 56199, loss = 1.46030355\n",
      "Iteration 56200, loss = 1.57457320\n",
      "Iteration 56201, loss = 1.42587129\n",
      "Iteration 56202, loss = 1.37036493\n",
      "Iteration 56203, loss = 1.39274426\n",
      "Iteration 56204, loss = 1.51219367\n",
      "Iteration 56205, loss = 1.76180866\n",
      "Iteration 56206, loss = 1.68359273\n",
      "Iteration 56207, loss = 1.88292735\n",
      "Iteration 56208, loss = 1.83928339\n",
      "Iteration 56209, loss = 1.84119692\n",
      "Iteration 56210, loss = 2.32707666\n",
      "Iteration 56211, loss = 2.00100170\n",
      "Iteration 56212, loss = 1.77566367\n",
      "Iteration 56213, loss = 1.51227534\n",
      "Iteration 56214, loss = 1.31787759\n",
      "Iteration 56215, loss = 1.54821295\n",
      "Iteration 56216, loss = 1.28995198\n",
      "Iteration 56217, loss = 1.46376508\n",
      "Iteration 56218, loss = 1.34969216\n",
      "Iteration 56219, loss = 1.28230888\n",
      "Iteration 56220, loss = 1.51847927\n",
      "Iteration 56221, loss = 1.51548909\n",
      "Iteration 56222, loss = 1.66690973\n",
      "Iteration 56223, loss = 1.65343855\n",
      "Iteration 56224, loss = 1.75500897\n",
      "Iteration 56225, loss = 1.92017481\n",
      "Iteration 56226, loss = 1.77556179\n",
      "Iteration 56227, loss = 1.54473530\n",
      "Iteration 56228, loss = 1.42977605\n",
      "Iteration 56229, loss = 1.30659334\n",
      "Iteration 56230, loss = 1.45373011\n",
      "Iteration 56231, loss = 1.56296254\n",
      "Iteration 56232, loss = 1.82599525\n",
      "Iteration 56233, loss = 1.55395858\n",
      "Iteration 56234, loss = 1.60292487\n",
      "Iteration 56235, loss = 1.69225290\n",
      "Iteration 56236, loss = 1.41349164\n",
      "Iteration 56237, loss = 1.37292899\n",
      "Iteration 56238, loss = 1.44530932\n",
      "Iteration 56239, loss = 1.45059541\n",
      "Iteration 56240, loss = 1.28953341\n",
      "Iteration 56241, loss = 1.56443650\n",
      "Iteration 56242, loss = 1.49597454\n",
      "Iteration 56243, loss = 1.62175166\n",
      "Iteration 56244, loss = 1.58293966\n",
      "Iteration 56245, loss = 1.75510783\n",
      "Iteration 56246, loss = 1.42033451\n",
      "Iteration 56247, loss = 1.37435076\n",
      "Iteration 56248, loss = 1.26124620\n",
      "Iteration 56249, loss = 1.43477649\n",
      "Iteration 56250, loss = 1.69491461\n",
      "Iteration 56251, loss = 1.64258360\n",
      "Iteration 56252, loss = 1.49851813\n",
      "Iteration 56253, loss = 1.64733449\n",
      "Iteration 56254, loss = 1.67064230\n",
      "Iteration 56255, loss = 1.90533640\n",
      "Iteration 56256, loss = 2.32612595\n",
      "Iteration 56257, loss = 2.57576701\n",
      "Iteration 56258, loss = 2.33290064\n",
      "Iteration 56259, loss = 1.99054199\n",
      "Iteration 56260, loss = 1.75760209\n",
      "Iteration 56261, loss = 1.47226995\n",
      "Iteration 56262, loss = 1.67577002\n",
      "Iteration 56263, loss = 1.53011064\n",
      "Iteration 56264, loss = 1.36087146\n",
      "Iteration 56265, loss = 1.49197279\n",
      "Iteration 56266, loss = 1.56193520\n",
      "Iteration 56267, loss = 1.67164884\n",
      "Iteration 56268, loss = 1.56315338\n",
      "Iteration 56269, loss = 1.49318327\n",
      "Iteration 56270, loss = 1.54412691\n",
      "Iteration 56271, loss = 1.50537129\n",
      "Iteration 56272, loss = 1.64659207\n",
      "Iteration 56273, loss = 1.28549060\n",
      "Iteration 56274, loss = 1.50603137\n",
      "Iteration 56275, loss = 1.34650525\n",
      "Iteration 56276, loss = 1.33785526\n",
      "Iteration 56277, loss = 1.36884940\n",
      "Iteration 56278, loss = 1.41657672\n",
      "Iteration 56279, loss = 1.53159794\n",
      "Iteration 56280, loss = 1.50271288\n",
      "Iteration 56281, loss = 1.60119077\n",
      "Iteration 56282, loss = 1.55955693\n",
      "Iteration 56283, loss = 1.62699577\n",
      "Iteration 56284, loss = 1.52852883\n",
      "Iteration 56285, loss = 1.46709855\n",
      "Iteration 56286, loss = 1.54154838\n",
      "Iteration 56287, loss = 1.49867767\n",
      "Iteration 56288, loss = 1.51776377\n",
      "Iteration 56289, loss = 1.90759011\n",
      "Iteration 56290, loss = 2.04172792\n",
      "Iteration 56291, loss = 1.86295025\n",
      "Iteration 56292, loss = 1.77043747\n",
      "Iteration 56293, loss = 1.69925456\n",
      "Iteration 56294, loss = 1.68196283\n",
      "Iteration 56295, loss = 1.82685043\n",
      "Iteration 56296, loss = 2.74600387\n",
      "Iteration 56297, loss = 1.72109471\n",
      "Iteration 56298, loss = 1.43565314\n",
      "Iteration 56299, loss = 1.35525022\n",
      "Iteration 56300, loss = 1.40787181\n",
      "Iteration 56301, loss = 1.36539408\n",
      "Iteration 56302, loss = 1.26910808\n",
      "Iteration 56303, loss = 1.35841255\n",
      "Iteration 56304, loss = 1.37475030\n",
      "Iteration 56305, loss = 1.36353734\n",
      "Iteration 56306, loss = 1.32573520\n",
      "Iteration 56307, loss = 1.37431819\n",
      "Iteration 56308, loss = 1.31953856\n",
      "Iteration 56309, loss = 1.37239353\n",
      "Iteration 56310, loss = 1.26718450\n",
      "Iteration 56311, loss = 1.42625170\n",
      "Iteration 56312, loss = 1.25542547\n",
      "Iteration 56313, loss = 1.39214916\n",
      "Iteration 56314, loss = 1.63571818\n",
      "Iteration 56315, loss = 2.30829604\n",
      "Iteration 56316, loss = 2.72758331\n",
      "Iteration 56317, loss = 2.02011917\n",
      "Iteration 56318, loss = 2.53885095\n",
      "Iteration 56319, loss = 1.97652607\n",
      "Iteration 56320, loss = 2.02317738\n",
      "Iteration 56321, loss = 2.05582028\n",
      "Iteration 56322, loss = 2.45068286\n",
      "Iteration 56323, loss = 2.06549548\n",
      "Iteration 56324, loss = 1.78074972\n",
      "Iteration 56325, loss = 1.95995832\n",
      "Iteration 56326, loss = 1.73790490\n",
      "Iteration 56327, loss = 1.51100775\n",
      "Iteration 56328, loss = 1.60098529\n",
      "Iteration 56329, loss = 1.46190399\n",
      "Iteration 56330, loss = 2.10292507\n",
      "Iteration 56331, loss = 2.32897882\n",
      "Iteration 56332, loss = 2.70012965\n",
      "Iteration 56333, loss = 1.90894666\n",
      "Iteration 56334, loss = 1.66925701\n",
      "Iteration 56335, loss = 1.42093300\n",
      "Iteration 56336, loss = 1.84173186\n",
      "Iteration 56337, loss = 1.46990498\n",
      "Iteration 56338, loss = 1.54406986\n",
      "Iteration 56339, loss = 1.40398234\n",
      "Iteration 56340, loss = 1.96388750\n",
      "Iteration 56341, loss = 1.76659784\n",
      "Iteration 56342, loss = 1.88275569\n",
      "Iteration 56343, loss = 2.30248064\n",
      "Iteration 56344, loss = 1.84262413\n",
      "Iteration 56345, loss = 1.39802189\n",
      "Iteration 56346, loss = 1.60577922\n",
      "Iteration 56347, loss = 2.02096934\n",
      "Iteration 56348, loss = 1.68456047\n",
      "Iteration 56349, loss = 1.67515670\n",
      "Iteration 56350, loss = 2.46319132\n",
      "Iteration 56351, loss = 2.17654173\n",
      "Iteration 56352, loss = 2.19368793\n",
      "Iteration 56353, loss = 1.85622578\n",
      "Iteration 56354, loss = 1.42937446\n",
      "Iteration 56355, loss = 1.38558760\n",
      "Iteration 56356, loss = 1.49824874\n",
      "Iteration 56357, loss = 1.52523808\n",
      "Iteration 56358, loss = 1.37045670\n",
      "Iteration 56359, loss = 1.50353934\n",
      "Iteration 56360, loss = 1.59149503\n",
      "Iteration 56361, loss = 1.43056568\n",
      "Iteration 56362, loss = 1.73538290\n",
      "Iteration 56363, loss = 2.15975687\n",
      "Iteration 56364, loss = 1.64848146\n",
      "Iteration 56365, loss = 1.57097639\n",
      "Iteration 56366, loss = 1.41790109\n",
      "Iteration 56367, loss = 1.76761847\n",
      "Iteration 56368, loss = 2.17826786\n",
      "Iteration 56369, loss = 2.40815288\n",
      "Iteration 56370, loss = 2.34135794\n",
      "Iteration 56371, loss = 2.13877987\n",
      "Iteration 56372, loss = 1.99297192\n",
      "Iteration 56373, loss = 2.23118568\n",
      "Iteration 56374, loss = 1.73349433\n",
      "Iteration 56375, loss = 1.70950563\n",
      "Iteration 56376, loss = 1.70830181\n",
      "Iteration 56377, loss = 1.69588653\n",
      "Iteration 56378, loss = 2.34097033\n",
      "Iteration 56379, loss = 1.90693329\n",
      "Iteration 56380, loss = 1.91511304\n",
      "Iteration 56381, loss = 1.52326007\n",
      "Iteration 56382, loss = 1.69830329\n",
      "Iteration 56383, loss = 2.05165568\n",
      "Iteration 56384, loss = 2.16529357\n",
      "Iteration 56385, loss = 2.47277039\n",
      "Iteration 56386, loss = 3.12201529\n",
      "Iteration 56387, loss = 3.08080543\n",
      "Iteration 56388, loss = 2.40274773\n",
      "Iteration 56389, loss = 2.37984268\n",
      "Iteration 56390, loss = 2.37108861\n",
      "Iteration 56391, loss = 2.38632145\n",
      "Iteration 56392, loss = 1.75304838\n",
      "Iteration 56393, loss = 1.74701936\n",
      "Iteration 56394, loss = 1.51195726\n",
      "Iteration 56395, loss = 1.60393651\n",
      "Iteration 56396, loss = 1.46338507\n",
      "Iteration 56397, loss = 1.38259717\n",
      "Iteration 56398, loss = 1.58562652\n",
      "Iteration 56399, loss = 1.44197728\n",
      "Iteration 56400, loss = 1.44116354\n",
      "Iteration 56401, loss = 1.45442738\n",
      "Iteration 56402, loss = 1.67467966\n",
      "Iteration 56403, loss = 1.44092203\n",
      "Iteration 56404, loss = 1.36091292\n",
      "Iteration 56405, loss = 1.46047735\n",
      "Iteration 56406, loss = 1.28174658\n",
      "Iteration 56407, loss = 1.44116347\n",
      "Iteration 56408, loss = 1.38287795\n",
      "Iteration 56409, loss = 1.44773349\n",
      "Iteration 56410, loss = 1.38657365\n",
      "Iteration 56411, loss = 1.37901047\n",
      "Iteration 56412, loss = 1.42883814\n",
      "Iteration 56413, loss = 1.53324050\n",
      "Iteration 56414, loss = 1.79872730\n",
      "Iteration 56415, loss = 1.54958171\n",
      "Iteration 56416, loss = 1.63596310\n",
      "Iteration 56417, loss = 1.43411271\n",
      "Iteration 56418, loss = 1.36179748\n",
      "Iteration 56419, loss = 1.49575753\n",
      "Iteration 56420, loss = 1.74073201\n",
      "Iteration 56421, loss = 1.82942497\n",
      "Iteration 56422, loss = 1.63113525\n",
      "Iteration 56423, loss = 2.12189017\n",
      "Iteration 56424, loss = 2.03329401\n",
      "Iteration 56425, loss = 2.15204252\n",
      "Iteration 56426, loss = 2.01427199\n",
      "Iteration 56427, loss = 2.74568021\n",
      "Iteration 56428, loss = 2.15677446\n",
      "Iteration 56429, loss = 1.84072871\n",
      "Iteration 56430, loss = 1.63401257\n",
      "Iteration 56431, loss = 1.38030791\n",
      "Iteration 56432, loss = 1.33929338\n",
      "Iteration 56433, loss = 1.36420243\n",
      "Iteration 56434, loss = 1.65171463\n",
      "Iteration 56435, loss = 1.61257150\n",
      "Iteration 56436, loss = 1.69387407\n",
      "Iteration 56437, loss = 1.84141371\n",
      "Iteration 56438, loss = 1.52231300\n",
      "Iteration 56439, loss = 1.44254929\n",
      "Iteration 56440, loss = 1.59316992\n",
      "Iteration 56441, loss = 1.51577184\n",
      "Iteration 56442, loss = 1.55367219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 56443, loss = 1.48355447\n",
      "Iteration 56444, loss = 2.29255368\n",
      "Iteration 56445, loss = 2.96507088\n",
      "Iteration 56446, loss = 2.96606280\n",
      "Iteration 56447, loss = 1.68062893\n",
      "Iteration 56448, loss = 1.60731216\n",
      "Iteration 56449, loss = 1.37206175\n",
      "Iteration 56450, loss = 1.44428120\n",
      "Iteration 56451, loss = 1.71105555\n",
      "Iteration 56452, loss = 1.75084017\n",
      "Iteration 56453, loss = 1.30315209\n",
      "Iteration 56454, loss = 1.33715073\n",
      "Iteration 56455, loss = 1.35148587\n",
      "Iteration 56456, loss = 1.32940708\n",
      "Iteration 56457, loss = 1.29224707\n",
      "Iteration 56458, loss = 1.29025168\n",
      "Iteration 56459, loss = 1.38287901\n",
      "Iteration 56460, loss = 1.48270346\n",
      "Iteration 56461, loss = 1.77201141\n",
      "Iteration 56462, loss = 1.50587041\n",
      "Iteration 56463, loss = 1.47210394\n",
      "Iteration 56464, loss = 1.38852828\n",
      "Iteration 56465, loss = 1.44929978\n",
      "Iteration 56466, loss = 1.60920172\n",
      "Iteration 56467, loss = 1.42126984\n",
      "Iteration 56468, loss = 1.43587044\n",
      "Iteration 56469, loss = 1.44613420\n",
      "Iteration 56470, loss = 1.28105990\n",
      "Iteration 56471, loss = 1.23877366\n",
      "Iteration 56472, loss = 1.39651129\n",
      "Iteration 56473, loss = 1.29988056\n",
      "Iteration 56474, loss = 1.37075392\n",
      "Iteration 56475, loss = 1.39099571\n",
      "Iteration 56476, loss = 1.42510402\n",
      "Iteration 56477, loss = 1.32267981\n",
      "Iteration 56478, loss = 1.42927779\n",
      "Iteration 56479, loss = 1.70773901\n",
      "Iteration 56480, loss = 1.66321943\n",
      "Iteration 56481, loss = 1.66523381\n",
      "Iteration 56482, loss = 1.90115675\n",
      "Iteration 56483, loss = 1.64681204\n",
      "Iteration 56484, loss = 1.52600487\n",
      "Iteration 56485, loss = 1.64192190\n",
      "Iteration 56486, loss = 1.85535441\n",
      "Iteration 56487, loss = 1.79470679\n",
      "Iteration 56488, loss = 1.89564904\n",
      "Iteration 56489, loss = 1.62941060\n",
      "Iteration 56490, loss = 1.65537464\n",
      "Iteration 56491, loss = 1.70504744\n",
      "Iteration 56492, loss = 1.61896281\n",
      "Iteration 56493, loss = 1.43940477\n",
      "Iteration 56494, loss = 1.35821905\n",
      "Iteration 56495, loss = 1.29010577\n",
      "Iteration 56496, loss = 1.45144570\n",
      "Iteration 56497, loss = 1.43086026\n",
      "Iteration 56498, loss = 1.92369822\n",
      "Iteration 56499, loss = 1.45448379\n",
      "Iteration 56500, loss = 1.54468130\n",
      "Iteration 56501, loss = 1.57781190\n",
      "Iteration 56502, loss = 1.74108589\n",
      "Iteration 56503, loss = 1.42428588\n",
      "Iteration 56504, loss = 1.45966110\n",
      "Iteration 56505, loss = 1.50128049\n",
      "Iteration 56506, loss = 1.34941196\n",
      "Iteration 56507, loss = 1.27080411\n",
      "Iteration 56508, loss = 1.35322944\n",
      "Iteration 56509, loss = 1.32766084\n",
      "Iteration 56510, loss = 1.34400791\n",
      "Iteration 56511, loss = 1.51593016\n",
      "Iteration 56512, loss = 1.45817126\n",
      "Iteration 56513, loss = 1.42113575\n",
      "Iteration 56514, loss = 1.37204650\n",
      "Iteration 56515, loss = 1.41217007\n",
      "Iteration 56516, loss = 1.30428010\n",
      "Iteration 56517, loss = 1.25875604\n",
      "Iteration 56518, loss = 1.43010859\n",
      "Iteration 56519, loss = 1.37026019\n",
      "Iteration 56520, loss = 1.46996343\n",
      "Iteration 56521, loss = 2.07007047\n",
      "Iteration 56522, loss = 2.04734880\n",
      "Iteration 56523, loss = 1.64215093\n",
      "Iteration 56524, loss = 1.41979484\n",
      "Iteration 56525, loss = 1.82629574\n",
      "Iteration 56526, loss = 1.49098433\n",
      "Iteration 56527, loss = 1.78078500\n",
      "Iteration 56528, loss = 1.70875757\n",
      "Iteration 56529, loss = 1.57740773\n",
      "Iteration 56530, loss = 1.40786327\n",
      "Iteration 56531, loss = 1.57895474\n",
      "Iteration 56532, loss = 1.48497748\n",
      "Iteration 56533, loss = 1.63910070\n",
      "Iteration 56534, loss = 1.52857684\n",
      "Iteration 56535, loss = 1.34966896\n",
      "Iteration 56536, loss = 1.33564823\n",
      "Iteration 56537, loss = 1.21005114\n",
      "Iteration 56538, loss = 1.31446231\n",
      "Iteration 56539, loss = 1.61695379\n",
      "Iteration 56540, loss = 1.47566606\n",
      "Iteration 56541, loss = 1.72499679\n",
      "Iteration 56542, loss = 1.86609657\n",
      "Iteration 56543, loss = 1.50823381\n",
      "Iteration 56544, loss = 1.84541609\n",
      "Iteration 56545, loss = 1.62270291\n",
      "Iteration 56546, loss = 1.38859613\n",
      "Iteration 56547, loss = 1.66557937\n",
      "Iteration 56548, loss = 1.67267612\n",
      "Iteration 56549, loss = 1.43468763\n",
      "Iteration 56550, loss = 1.29175596\n",
      "Iteration 56551, loss = 1.38726550\n",
      "Iteration 56552, loss = 1.36606904\n",
      "Iteration 56553, loss = 1.43793954\n",
      "Iteration 56554, loss = 1.74795559\n",
      "Iteration 56555, loss = 1.63532087\n",
      "Iteration 56556, loss = 2.22442927\n",
      "Iteration 56557, loss = 1.74464988\n",
      "Iteration 56558, loss = 1.50663564\n",
      "Iteration 56559, loss = 1.57308799\n",
      "Iteration 56560, loss = 1.78349145\n",
      "Iteration 56561, loss = 1.69838027\n",
      "Iteration 56562, loss = 1.89598869\n",
      "Iteration 56563, loss = 2.01449706\n",
      "Iteration 56564, loss = 2.18681927\n",
      "Iteration 56565, loss = 3.01448246\n",
      "Iteration 56566, loss = 2.45201060\n",
      "Iteration 56567, loss = 3.47867197\n",
      "Iteration 56568, loss = 3.08767970\n",
      "Iteration 56569, loss = 3.12544306\n",
      "Iteration 56570, loss = 1.95076659\n",
      "Iteration 56571, loss = 1.66607736\n",
      "Iteration 56572, loss = 1.54015742\n",
      "Iteration 56573, loss = 1.43418291\n",
      "Iteration 56574, loss = 1.43769533\n",
      "Iteration 56575, loss = 1.42685191\n",
      "Iteration 56576, loss = 1.59959167\n",
      "Iteration 56577, loss = 1.51309038\n",
      "Iteration 56578, loss = 1.56246447\n",
      "Iteration 56579, loss = 1.55000348\n",
      "Iteration 56580, loss = 1.30457742\n",
      "Iteration 56581, loss = 1.27291544\n",
      "Iteration 56582, loss = 1.17342938\n",
      "Iteration 56583, loss = 1.47235213\n",
      "Iteration 56584, loss = 1.48745185\n",
      "Iteration 56585, loss = 1.62677962\n",
      "Iteration 56586, loss = 1.56990424\n",
      "Iteration 56587, loss = 1.64093801\n",
      "Iteration 56588, loss = 1.74814155\n",
      "Iteration 56589, loss = 1.36221743\n",
      "Iteration 56590, loss = 1.67928100\n",
      "Iteration 56591, loss = 1.79983300\n",
      "Iteration 56592, loss = 1.89758152\n",
      "Iteration 56593, loss = 2.03064537\n",
      "Iteration 56594, loss = 1.72810274\n",
      "Iteration 56595, loss = 1.77436473\n",
      "Iteration 56596, loss = 1.71759797\n",
      "Iteration 56597, loss = 1.58931510\n",
      "Iteration 56598, loss = 1.71439499\n",
      "Iteration 56599, loss = 2.68651936\n",
      "Iteration 56600, loss = 2.33094149\n",
      "Iteration 56601, loss = 1.65452713\n",
      "Iteration 56602, loss = 1.58839633\n",
      "Iteration 56603, loss = 1.49947853\n",
      "Iteration 56604, loss = 1.52113058\n",
      "Iteration 56605, loss = 1.58967039\n",
      "Iteration 56606, loss = 1.52586112\n",
      "Iteration 56607, loss = 1.51455427\n",
      "Iteration 56608, loss = 1.52939526\n",
      "Iteration 56609, loss = 1.38223670\n",
      "Iteration 56610, loss = 1.23489795\n",
      "Iteration 56611, loss = 1.49868605\n",
      "Iteration 56612, loss = 1.90775262\n",
      "Iteration 56613, loss = 1.72566445\n",
      "Iteration 56614, loss = 1.47781746\n",
      "Iteration 56615, loss = 1.65942559\n",
      "Iteration 56616, loss = 1.73537295\n",
      "Iteration 56617, loss = 1.88337078\n",
      "Iteration 56618, loss = 1.62900832\n",
      "Iteration 56619, loss = 1.74844616\n",
      "Iteration 56620, loss = 1.94132535\n",
      "Iteration 56621, loss = 2.03510143\n",
      "Iteration 56622, loss = 1.90287475\n",
      "Iteration 56623, loss = 1.94269723\n",
      "Iteration 56624, loss = 1.68206057\n",
      "Iteration 56625, loss = 1.76402576\n",
      "Iteration 56626, loss = 1.54446156\n",
      "Iteration 56627, loss = 1.76267290\n",
      "Iteration 56628, loss = 1.79225853\n",
      "Iteration 56629, loss = 1.96975920\n",
      "Iteration 56630, loss = 1.58822739\n",
      "Iteration 56631, loss = 1.54131596\n",
      "Iteration 56632, loss = 1.50585694\n",
      "Iteration 56633, loss = 1.54758177\n",
      "Iteration 56634, loss = 1.53109381\n",
      "Iteration 56635, loss = 1.66152814\n",
      "Iteration 56636, loss = 1.53955678\n",
      "Iteration 56637, loss = 1.56429817\n",
      "Iteration 56638, loss = 1.72965833\n",
      "Iteration 56639, loss = 1.75219973\n",
      "Iteration 56640, loss = 3.24557530\n",
      "Iteration 56641, loss = 2.80201883\n",
      "Iteration 56642, loss = 2.74922660\n",
      "Iteration 56643, loss = 1.89229105\n",
      "Iteration 56644, loss = 1.84597215\n",
      "Iteration 56645, loss = 1.52998923\n",
      "Iteration 56646, loss = 1.52735412\n",
      "Iteration 56647, loss = 1.40345991\n",
      "Iteration 56648, loss = 1.32054976\n",
      "Iteration 56649, loss = 1.26813535\n",
      "Iteration 56650, loss = 1.76676336\n",
      "Iteration 56651, loss = 2.12298084\n",
      "Iteration 56652, loss = 1.69110381\n",
      "Iteration 56653, loss = 1.38594627\n",
      "Iteration 56654, loss = 1.54101237\n",
      "Iteration 56655, loss = 1.45039449\n",
      "Iteration 56656, loss = 1.52133791\n",
      "Iteration 56657, loss = 1.77303063\n",
      "Iteration 56658, loss = 1.57578494\n",
      "Iteration 56659, loss = 1.67111364\n",
      "Iteration 56660, loss = 1.45561920\n",
      "Iteration 56661, loss = 1.46021206\n",
      "Iteration 56662, loss = 1.56134092\n",
      "Iteration 56663, loss = 1.47068572\n",
      "Iteration 56664, loss = 1.33341130\n",
      "Iteration 56665, loss = 1.40956348\n",
      "Iteration 56666, loss = 1.43380006\n",
      "Iteration 56667, loss = 1.80614890\n",
      "Iteration 56668, loss = 1.90333968\n",
      "Iteration 56669, loss = 2.28842473\n",
      "Iteration 56670, loss = 1.93359286\n",
      "Iteration 56671, loss = 1.74100472\n",
      "Iteration 56672, loss = 1.37504560\n",
      "Iteration 56673, loss = 1.49465251\n",
      "Iteration 56674, loss = 1.51394992\n",
      "Iteration 56675, loss = 1.51888843\n",
      "Iteration 56676, loss = 1.62698651\n",
      "Iteration 56677, loss = 1.37568981\n",
      "Iteration 56678, loss = 1.71796888\n",
      "Iteration 56679, loss = 1.59400628\n",
      "Iteration 56680, loss = 1.57943492\n",
      "Iteration 56681, loss = 1.46995974\n",
      "Iteration 56682, loss = 1.35224297\n",
      "Iteration 56683, loss = 1.33705296\n",
      "Iteration 56684, loss = 1.36512608\n",
      "Iteration 56685, loss = 1.25963228\n",
      "Iteration 56686, loss = 1.32399919\n",
      "Iteration 56687, loss = 1.30683116\n",
      "Iteration 56688, loss = 1.60432247\n",
      "Iteration 56689, loss = 1.81817978\n",
      "Iteration 56690, loss = 2.09658559\n",
      "Iteration 56691, loss = 1.73462623\n",
      "Iteration 56692, loss = 1.81299394\n",
      "Iteration 56693, loss = 1.89970431\n",
      "Iteration 56694, loss = 1.67039531\n",
      "Iteration 56695, loss = 1.60984708\n",
      "Iteration 56696, loss = 1.55884334\n",
      "Iteration 56697, loss = 1.39423150\n",
      "Iteration 56698, loss = 1.53733807\n",
      "Iteration 56699, loss = 1.72400476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 56700, loss = 1.72705842\n",
      "Iteration 56701, loss = 1.71078486\n",
      "Iteration 56702, loss = 1.70624538\n",
      "Iteration 56703, loss = 1.68408713\n",
      "Iteration 56704, loss = 2.90295869\n",
      "Iteration 56705, loss = 4.00607009\n",
      "Iteration 56706, loss = 3.27454863\n",
      "Iteration 56707, loss = 2.58554090\n",
      "Iteration 56708, loss = 2.20809345\n",
      "Iteration 56709, loss = 2.89694243\n",
      "Iteration 56710, loss = 2.24178091\n",
      "Iteration 56711, loss = 2.06198410\n",
      "Iteration 56712, loss = 2.10648844\n",
      "Iteration 56713, loss = 1.95076548\n",
      "Iteration 56714, loss = 2.11987698\n",
      "Iteration 56715, loss = 1.60680209\n",
      "Iteration 56716, loss = 1.32233675\n",
      "Iteration 56717, loss = 1.30360358\n",
      "Iteration 56718, loss = 1.34597302\n",
      "Iteration 56719, loss = 1.36023297\n",
      "Iteration 56720, loss = 1.50181752\n",
      "Iteration 56721, loss = 1.47741129\n",
      "Iteration 56722, loss = 1.51587645\n",
      "Iteration 56723, loss = 1.39576815\n",
      "Iteration 56724, loss = 1.54007622\n",
      "Iteration 56725, loss = 1.93160150\n",
      "Iteration 56726, loss = 1.62576606\n",
      "Iteration 56727, loss = 1.68313773\n",
      "Iteration 56728, loss = 1.39724588\n",
      "Iteration 56729, loss = 1.92400604\n",
      "Iteration 56730, loss = 1.63875653\n",
      "Iteration 56731, loss = 2.21015648\n",
      "Iteration 56732, loss = 2.37722405\n",
      "Iteration 56733, loss = 2.60451731\n",
      "Iteration 56734, loss = 1.55864373\n",
      "Iteration 56735, loss = 1.91835271\n",
      "Iteration 56736, loss = 1.50190137\n",
      "Iteration 56737, loss = 1.69336694\n",
      "Iteration 56738, loss = 1.50886084\n",
      "Iteration 56739, loss = 1.58666969\n",
      "Iteration 56740, loss = 1.73613550\n",
      "Iteration 56741, loss = 1.84461178\n",
      "Iteration 56742, loss = 1.88873373\n",
      "Iteration 56743, loss = 1.40263505\n",
      "Iteration 56744, loss = 2.04094030\n",
      "Iteration 56745, loss = 1.70715226\n",
      "Iteration 56746, loss = 1.93227453\n",
      "Iteration 56747, loss = 1.48765914\n",
      "Iteration 56748, loss = 1.84111669\n",
      "Iteration 56749, loss = 1.66003221\n",
      "Iteration 56750, loss = 1.74752912\n",
      "Iteration 56751, loss = 1.63187538\n",
      "Iteration 56752, loss = 1.34213998\n",
      "Iteration 56753, loss = 1.69719508\n",
      "Iteration 56754, loss = 1.60561960\n",
      "Iteration 56755, loss = 1.46034984\n",
      "Iteration 56756, loss = 1.54827203\n",
      "Iteration 56757, loss = 1.55740199\n",
      "Iteration 56758, loss = 1.54846956\n",
      "Iteration 56759, loss = 1.38318995\n",
      "Iteration 56760, loss = 1.51727828\n",
      "Iteration 56761, loss = 1.33691749\n",
      "Iteration 56762, loss = 1.35602237\n",
      "Iteration 56763, loss = 1.36097683\n",
      "Iteration 56764, loss = 1.43091320\n",
      "Iteration 56765, loss = 1.65879533\n",
      "Iteration 56766, loss = 1.59652259\n",
      "Iteration 56767, loss = 1.38139408\n",
      "Iteration 56768, loss = 1.57418091\n",
      "Iteration 56769, loss = 1.32857769\n",
      "Iteration 56770, loss = 1.35349569\n",
      "Iteration 56771, loss = 1.31968736\n",
      "Iteration 56772, loss = 1.40399489\n",
      "Iteration 56773, loss = 1.68122264\n",
      "Iteration 56774, loss = 1.45067835\n",
      "Iteration 56775, loss = 1.31781589\n",
      "Iteration 56776, loss = 2.33126512\n",
      "Iteration 56777, loss = 3.72037024\n",
      "Iteration 56778, loss = 2.46707104\n",
      "Iteration 56779, loss = 2.08171035\n",
      "Iteration 56780, loss = 2.02142475\n",
      "Iteration 56781, loss = 1.99913333\n",
      "Iteration 56782, loss = 2.11787351\n",
      "Iteration 56783, loss = 2.06968533\n",
      "Iteration 56784, loss = 2.03819527\n",
      "Iteration 56785, loss = 1.76211605\n",
      "Iteration 56786, loss = 1.75402914\n",
      "Iteration 56787, loss = 2.35542919\n",
      "Iteration 56788, loss = 2.18060894\n",
      "Iteration 56789, loss = 1.75999897\n",
      "Iteration 56790, loss = 1.75527265\n",
      "Iteration 56791, loss = 2.04077079\n",
      "Iteration 56792, loss = 2.20577869\n",
      "Iteration 56793, loss = 1.73617697\n",
      "Iteration 56794, loss = 1.62689989\n",
      "Iteration 56795, loss = 1.65692585\n",
      "Iteration 56796, loss = 1.77924023\n",
      "Iteration 56797, loss = 1.46927616\n",
      "Iteration 56798, loss = 1.54057339\n",
      "Iteration 56799, loss = 1.65915817\n",
      "Iteration 56800, loss = 1.44381721\n",
      "Iteration 56801, loss = 1.46069789\n",
      "Iteration 56802, loss = 1.47045129\n",
      "Iteration 56803, loss = 1.35794863\n",
      "Iteration 56804, loss = 1.28678629\n",
      "Iteration 56805, loss = 1.55954441\n",
      "Iteration 56806, loss = 1.28071213\n",
      "Iteration 56807, loss = 1.38358589\n",
      "Iteration 56808, loss = 1.49638024\n",
      "Iteration 56809, loss = 1.53512878\n",
      "Iteration 56810, loss = 1.35636005\n",
      "Iteration 56811, loss = 1.49070179\n",
      "Iteration 56812, loss = 1.63258228\n",
      "Iteration 56813, loss = 2.05401039\n",
      "Iteration 56814, loss = 1.89365489\n",
      "Iteration 56815, loss = 1.73925224\n",
      "Iteration 56816, loss = 1.62434347\n",
      "Iteration 56817, loss = 1.54511565\n",
      "Iteration 56818, loss = 1.53317627\n",
      "Iteration 56819, loss = 1.61679603\n",
      "Iteration 56820, loss = 1.49414281\n",
      "Iteration 56821, loss = 1.52077440\n",
      "Iteration 56822, loss = 1.82926505\n",
      "Iteration 56823, loss = 1.83058662\n",
      "Iteration 56824, loss = 1.60411646\n",
      "Iteration 56825, loss = 1.41142881\n",
      "Iteration 56826, loss = 1.41832730\n",
      "Iteration 56827, loss = 1.88371044\n",
      "Iteration 56828, loss = 1.46378030\n",
      "Iteration 56829, loss = 1.39921703\n",
      "Iteration 56830, loss = 1.47674863\n",
      "Iteration 56831, loss = 1.47958043\n",
      "Iteration 56832, loss = 1.58990221\n",
      "Iteration 56833, loss = 1.62377560\n",
      "Iteration 56834, loss = 1.47955684\n",
      "Iteration 56835, loss = 1.48021760\n",
      "Iteration 56836, loss = 1.32113855\n",
      "Iteration 56837, loss = 1.36799236\n",
      "Iteration 56838, loss = 1.47508270\n",
      "Iteration 56839, loss = 1.33264276\n",
      "Iteration 56840, loss = 1.50006258\n",
      "Iteration 56841, loss = 1.37049343\n",
      "Iteration 56842, loss = 1.45239134\n",
      "Iteration 56843, loss = 1.35016945\n",
      "Iteration 56844, loss = 1.43480751\n",
      "Iteration 56845, loss = 1.78674999\n",
      "Iteration 56846, loss = 1.77234985\n",
      "Iteration 56847, loss = 1.61779269\n",
      "Iteration 56848, loss = 1.52077557\n",
      "Iteration 56849, loss = 1.42572032\n",
      "Iteration 56850, loss = 1.72939686\n",
      "Iteration 56851, loss = 1.46433466\n",
      "Iteration 56852, loss = 1.57682521\n",
      "Iteration 56853, loss = 1.60803249\n",
      "Iteration 56854, loss = 2.50539113\n",
      "Iteration 56855, loss = 1.82078709\n",
      "Iteration 56856, loss = 1.57591017\n",
      "Iteration 56857, loss = 1.54077869\n",
      "Iteration 56858, loss = 1.96656689\n",
      "Iteration 56859, loss = 2.48968552\n",
      "Iteration 56860, loss = 1.81033593\n",
      "Iteration 56861, loss = 1.92009651\n",
      "Iteration 56862, loss = 1.77687302\n",
      "Iteration 56863, loss = 1.85078747\n",
      "Iteration 56864, loss = 1.64357047\n",
      "Iteration 56865, loss = 1.32751554\n",
      "Iteration 56866, loss = 1.35672821\n",
      "Iteration 56867, loss = 1.30907506\n",
      "Iteration 56868, loss = 1.36949142\n",
      "Iteration 56869, loss = 1.66786753\n",
      "Iteration 56870, loss = 1.62353104\n",
      "Iteration 56871, loss = 2.08651944\n",
      "Iteration 56872, loss = 2.13160743\n",
      "Iteration 56873, loss = 2.22317825\n",
      "Iteration 56874, loss = 2.46502487\n",
      "Iteration 56875, loss = 1.89032260\n",
      "Iteration 56876, loss = 1.54510698\n",
      "Iteration 56877, loss = 1.66667648\n",
      "Iteration 56878, loss = 1.47005470\n",
      "Iteration 56879, loss = 1.56038269\n",
      "Iteration 56880, loss = 1.77099670\n",
      "Iteration 56881, loss = 2.03821901\n",
      "Iteration 56882, loss = 1.71024323\n",
      "Iteration 56883, loss = 1.43661361\n",
      "Iteration 56884, loss = 1.52100912\n",
      "Iteration 56885, loss = 1.57859892\n",
      "Iteration 56886, loss = 1.55246603\n",
      "Iteration 56887, loss = 1.55490251\n",
      "Iteration 56888, loss = 1.50931616\n",
      "Iteration 56889, loss = 1.48544176\n",
      "Iteration 56890, loss = 1.42836819\n",
      "Iteration 56891, loss = 1.50034925\n",
      "Iteration 56892, loss = 1.63863868\n",
      "Iteration 56893, loss = 1.92976931\n",
      "Iteration 56894, loss = 1.57839843\n",
      "Iteration 56895, loss = 1.27986218\n",
      "Iteration 56896, loss = 1.36292412\n",
      "Iteration 56897, loss = 1.38004227\n",
      "Iteration 56898, loss = 1.40675774\n",
      "Iteration 56899, loss = 1.31065020\n",
      "Iteration 56900, loss = 1.36016056\n",
      "Iteration 56901, loss = 1.35466349\n",
      "Iteration 56902, loss = 1.41772956\n",
      "Iteration 56903, loss = 1.46245337\n",
      "Iteration 56904, loss = 1.70663342\n",
      "Iteration 56905, loss = 1.78441814\n",
      "Iteration 56906, loss = 1.83128286\n",
      "Iteration 56907, loss = 1.76590420\n",
      "Iteration 56908, loss = 1.51838873\n",
      "Iteration 56909, loss = 1.51956164\n",
      "Iteration 56910, loss = 1.45074297\n",
      "Iteration 56911, loss = 1.40028560\n",
      "Iteration 56912, loss = 1.46007139\n",
      "Iteration 56913, loss = 1.58638419\n",
      "Iteration 56914, loss = 1.65164238\n",
      "Iteration 56915, loss = 1.43145099\n",
      "Iteration 56916, loss = 1.67906768\n",
      "Iteration 56917, loss = 1.51061262\n",
      "Iteration 56918, loss = 1.53456074\n",
      "Iteration 56919, loss = 1.50819692\n",
      "Iteration 56920, loss = 1.47376210\n",
      "Iteration 56921, loss = 1.45818803\n",
      "Iteration 56922, loss = 1.74386185\n",
      "Iteration 56923, loss = 1.78329628\n",
      "Iteration 56924, loss = 1.74045143\n",
      "Iteration 56925, loss = 1.64779901\n",
      "Iteration 56926, loss = 1.72075424\n",
      "Iteration 56927, loss = 1.95910451\n",
      "Iteration 56928, loss = 1.49571981\n",
      "Iteration 56929, loss = 1.86121301\n",
      "Iteration 56930, loss = 1.56086249\n",
      "Iteration 56931, loss = 1.61921055\n",
      "Iteration 56932, loss = 1.51534268\n",
      "Iteration 56933, loss = 1.66630047\n",
      "Iteration 56934, loss = 1.80120099\n",
      "Iteration 56935, loss = 1.53514735\n",
      "Iteration 56936, loss = 1.72786101\n",
      "Iteration 56937, loss = 1.95595142\n",
      "Iteration 56938, loss = 2.46494744\n",
      "Iteration 56939, loss = 2.46245741\n",
      "Iteration 56940, loss = 2.56226123\n",
      "Iteration 56941, loss = 2.33310894\n",
      "Iteration 56942, loss = 2.68292092\n",
      "Iteration 56943, loss = 2.22761139\n",
      "Iteration 56944, loss = 2.21856804\n",
      "Iteration 56945, loss = 2.36801495\n",
      "Iteration 56946, loss = 2.38330871\n",
      "Iteration 56947, loss = 2.34721663\n",
      "Iteration 56948, loss = 1.95770072\n",
      "Iteration 56949, loss = 2.35800854\n",
      "Iteration 56950, loss = 2.34154643\n",
      "Iteration 56951, loss = 2.34091172\n",
      "Iteration 56952, loss = 2.84086776\n",
      "Iteration 56953, loss = 2.80046499\n",
      "Iteration 56954, loss = 2.80059703\n",
      "Iteration 56955, loss = 2.24913899\n",
      "Iteration 56956, loss = 1.74924904\n",
      "Iteration 56957, loss = 1.93413935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 56958, loss = 2.15754526\n",
      "Iteration 56959, loss = 2.24565279\n",
      "Iteration 56960, loss = 2.11411684\n",
      "Iteration 56961, loss = 1.64976976\n",
      "Iteration 56962, loss = 1.71317559\n",
      "Iteration 56963, loss = 1.86102125\n",
      "Iteration 56964, loss = 2.04237996\n",
      "Iteration 56965, loss = 1.86429070\n",
      "Iteration 56966, loss = 1.61876011\n",
      "Iteration 56967, loss = 1.68566506\n",
      "Iteration 56968, loss = 1.45801498\n",
      "Iteration 56969, loss = 1.60056359\n",
      "Iteration 56970, loss = 1.67188577\n",
      "Iteration 56971, loss = 1.69407605\n",
      "Iteration 56972, loss = 1.71587022\n",
      "Iteration 56973, loss = 1.89601158\n",
      "Iteration 56974, loss = 1.79414498\n",
      "Iteration 56975, loss = 2.15551932\n",
      "Iteration 56976, loss = 1.95574308\n",
      "Iteration 56977, loss = 1.69720279\n",
      "Iteration 56978, loss = 2.18184476\n",
      "Iteration 56979, loss = 1.94909199\n",
      "Iteration 56980, loss = 1.68549183\n",
      "Iteration 56981, loss = 1.70794863\n",
      "Iteration 56982, loss = 1.85062307\n",
      "Iteration 56983, loss = 1.72799673\n",
      "Iteration 56984, loss = 1.62997265\n",
      "Iteration 56985, loss = 2.02757470\n",
      "Iteration 56986, loss = 1.94758002\n",
      "Iteration 56987, loss = 2.35387942\n",
      "Iteration 56988, loss = 2.84612243\n",
      "Iteration 56989, loss = 2.69377441\n",
      "Iteration 56990, loss = 2.26333804\n",
      "Iteration 56991, loss = 2.86202175\n",
      "Iteration 56992, loss = 2.38239744\n",
      "Iteration 56993, loss = 2.64907804\n",
      "Iteration 56994, loss = 2.83629902\n",
      "Iteration 56995, loss = 2.95415442\n",
      "Iteration 56996, loss = 2.18714081\n",
      "Iteration 56997, loss = 2.02240754\n",
      "Iteration 56998, loss = 2.18884287\n",
      "Iteration 56999, loss = 2.22655468\n",
      "Iteration 57000, loss = 1.81086700\n",
      "Iteration 57001, loss = 1.85030927\n",
      "Iteration 57002, loss = 1.64833921\n",
      "Iteration 57003, loss = 1.84958774\n",
      "Iteration 57004, loss = 1.90817437\n",
      "Iteration 57005, loss = 2.01584896\n",
      "Iteration 57006, loss = 1.75370531\n",
      "Iteration 57007, loss = 1.64693385\n",
      "Iteration 57008, loss = 1.22197671\n",
      "Iteration 57009, loss = 1.35542690\n",
      "Iteration 57010, loss = 1.45624528\n",
      "Iteration 57011, loss = 1.51607525\n",
      "Iteration 57012, loss = 1.54115547\n",
      "Iteration 57013, loss = 1.74162754\n",
      "Iteration 57014, loss = 1.45729737\n",
      "Iteration 57015, loss = 1.39731125\n",
      "Iteration 57016, loss = 1.57031185\n",
      "Iteration 57017, loss = 1.64211701\n",
      "Iteration 57018, loss = 1.62466777\n",
      "Iteration 57019, loss = 1.55564301\n",
      "Iteration 57020, loss = 1.49205026\n",
      "Iteration 57021, loss = 1.50004263\n",
      "Iteration 57022, loss = 1.60647724\n",
      "Iteration 57023, loss = 1.75157618\n",
      "Iteration 57024, loss = 1.76374745\n",
      "Iteration 57025, loss = 1.87741859\n",
      "Iteration 57026, loss = 1.84851926\n",
      "Iteration 57027, loss = 1.74475857\n",
      "Iteration 57028, loss = 1.82094932\n",
      "Iteration 57029, loss = 1.46912093\n",
      "Iteration 57030, loss = 1.24908264\n",
      "Iteration 57031, loss = 1.34788314\n",
      "Iteration 57032, loss = 1.60691957\n",
      "Iteration 57033, loss = 1.72822983\n",
      "Iteration 57034, loss = 1.73165623\n",
      "Iteration 57035, loss = 1.35022307\n",
      "Iteration 57036, loss = 1.24153580\n",
      "Iteration 57037, loss = 1.32843028\n",
      "Iteration 57038, loss = 1.38205901\n",
      "Iteration 57039, loss = 1.63620691\n",
      "Iteration 57040, loss = 1.69956809\n",
      "Iteration 57041, loss = 1.79675260\n",
      "Iteration 57042, loss = 3.88559983\n",
      "Iteration 57043, loss = 2.95993722\n",
      "Iteration 57044, loss = 2.28482774\n",
      "Iteration 57045, loss = 1.66694124\n",
      "Iteration 57046, loss = 1.55730285\n",
      "Iteration 57047, loss = 1.75821176\n",
      "Iteration 57048, loss = 1.92280557\n",
      "Iteration 57049, loss = 1.83952388\n",
      "Iteration 57050, loss = 1.77974825\n",
      "Iteration 57051, loss = 1.69100064\n",
      "Iteration 57052, loss = 1.94362195\n",
      "Iteration 57053, loss = 1.78328485\n",
      "Iteration 57054, loss = 1.72102051\n",
      "Iteration 57055, loss = 1.72103773\n",
      "Iteration 57056, loss = 1.56661327\n",
      "Iteration 57057, loss = 1.29227878\n",
      "Iteration 57058, loss = 1.91011865\n",
      "Iteration 57059, loss = 1.99154971\n",
      "Iteration 57060, loss = 2.30873701\n",
      "Iteration 57061, loss = 1.69445401\n",
      "Iteration 57062, loss = 1.97033784\n",
      "Iteration 57063, loss = 1.91781131\n",
      "Iteration 57064, loss = 2.00584754\n",
      "Iteration 57065, loss = 1.81372780\n",
      "Iteration 57066, loss = 1.84231012\n",
      "Iteration 57067, loss = 1.55126729\n",
      "Iteration 57068, loss = 1.24954174\n",
      "Iteration 57069, loss = 1.60487854\n",
      "Iteration 57070, loss = 1.63350255\n",
      "Iteration 57071, loss = 1.69092633\n",
      "Iteration 57072, loss = 1.83364557\n",
      "Iteration 57073, loss = 1.52514143\n",
      "Iteration 57074, loss = 1.57633721\n",
      "Iteration 57075, loss = 1.40210847\n",
      "Iteration 57076, loss = 1.54800549\n",
      "Iteration 57077, loss = 1.23333704\n",
      "Iteration 57078, loss = 1.34779728\n",
      "Iteration 57079, loss = 1.35721048\n",
      "Iteration 57080, loss = 1.47306135\n",
      "Iteration 57081, loss = 1.47923686\n",
      "Iteration 57082, loss = 1.49970222\n",
      "Iteration 57083, loss = 1.35497939\n",
      "Iteration 57084, loss = 1.68258033\n",
      "Iteration 57085, loss = 1.66817607\n",
      "Iteration 57086, loss = 1.45641622\n",
      "Iteration 57087, loss = 2.22009903\n",
      "Iteration 57088, loss = 2.81321749\n",
      "Iteration 57089, loss = 2.54649228\n",
      "Iteration 57090, loss = 2.12953239\n",
      "Iteration 57091, loss = 2.77807095\n",
      "Iteration 57092, loss = 2.38986615\n",
      "Iteration 57093, loss = 2.78513183\n",
      "Iteration 57094, loss = 3.58703583\n",
      "Iteration 57095, loss = 3.21289520\n",
      "Iteration 57096, loss = 3.20872153\n",
      "Iteration 57097, loss = 2.33550270\n",
      "Iteration 57098, loss = 2.00126540\n",
      "Iteration 57099, loss = 2.19683076\n",
      "Iteration 57100, loss = 1.86383925\n",
      "Iteration 57101, loss = 2.07808455\n",
      "Iteration 57102, loss = 1.94217769\n",
      "Iteration 57103, loss = 1.44399761\n",
      "Iteration 57104, loss = 1.36547350\n",
      "Iteration 57105, loss = 1.45516720\n",
      "Iteration 57106, loss = 1.53634018\n",
      "Iteration 57107, loss = 1.89978863\n",
      "Iteration 57108, loss = 1.79392193\n",
      "Iteration 57109, loss = 2.51487932\n",
      "Iteration 57110, loss = 2.49988866\n",
      "Iteration 57111, loss = 3.43360606\n",
      "Iteration 57112, loss = 3.44936977\n",
      "Iteration 57113, loss = 5.52718907\n",
      "Iteration 57114, loss = 3.85804732\n",
      "Iteration 57115, loss = 2.25637350\n",
      "Iteration 57116, loss = 3.45817816\n",
      "Iteration 57117, loss = 2.37590593\n",
      "Iteration 57118, loss = 2.52131999\n",
      "Iteration 57119, loss = 1.83855487\n",
      "Iteration 57120, loss = 2.88465162\n",
      "Iteration 57121, loss = 2.66990163\n",
      "Iteration 57122, loss = 1.85676553\n",
      "Iteration 57123, loss = 1.37241386\n",
      "Iteration 57124, loss = 1.80979081\n",
      "Iteration 57125, loss = 1.55686432\n",
      "Iteration 57126, loss = 1.45484532\n",
      "Iteration 57127, loss = 1.50639832\n",
      "Iteration 57128, loss = 1.60029879\n",
      "Iteration 57129, loss = 1.33475946\n",
      "Iteration 57130, loss = 1.56389790\n",
      "Iteration 57131, loss = 1.24248668\n",
      "Iteration 57132, loss = 1.49642372\n",
      "Iteration 57133, loss = 1.60768000\n",
      "Iteration 57134, loss = 1.43929453\n",
      "Iteration 57135, loss = 1.51860773\n",
      "Iteration 57136, loss = 1.56244990\n",
      "Iteration 57137, loss = 1.64008193\n",
      "Iteration 57138, loss = 1.62664381\n",
      "Iteration 57139, loss = 1.46603986\n",
      "Iteration 57140, loss = 1.41207584\n",
      "Iteration 57141, loss = 1.57377700\n",
      "Iteration 57142, loss = 1.61670960\n",
      "Iteration 57143, loss = 1.72357999\n",
      "Iteration 57144, loss = 1.55433151\n",
      "Iteration 57145, loss = 1.57071369\n",
      "Iteration 57146, loss = 1.59702576\n",
      "Iteration 57147, loss = 1.65229842\n",
      "Iteration 57148, loss = 1.38028004\n",
      "Iteration 57149, loss = 1.80206821\n",
      "Iteration 57150, loss = 1.73529200\n",
      "Iteration 57151, loss = 1.60400621\n",
      "Iteration 57152, loss = 1.44547426\n",
      "Iteration 57153, loss = 1.71059495\n",
      "Iteration 57154, loss = 2.03374406\n",
      "Iteration 57155, loss = 1.63202964\n",
      "Iteration 57156, loss = 1.82069596\n",
      "Iteration 57157, loss = 1.89787855\n",
      "Iteration 57158, loss = 2.29741050\n",
      "Iteration 57159, loss = 1.87394559\n",
      "Iteration 57160, loss = 1.40973141\n",
      "Iteration 57161, loss = 1.28864007\n",
      "Iteration 57162, loss = 1.29899841\n",
      "Iteration 57163, loss = 1.37377327\n",
      "Iteration 57164, loss = 1.32783207\n",
      "Iteration 57165, loss = 1.31235605\n",
      "Iteration 57166, loss = 1.51649264\n",
      "Iteration 57167, loss = 1.63285314\n",
      "Iteration 57168, loss = 1.48646106\n",
      "Iteration 57169, loss = 1.39189337\n",
      "Iteration 57170, loss = 1.40685828\n",
      "Iteration 57171, loss = 1.20989328\n",
      "Iteration 57172, loss = 1.30855582\n",
      "Iteration 57173, loss = 1.41888250\n",
      "Iteration 57174, loss = 1.32776522\n",
      "Iteration 57175, loss = 1.48686808\n",
      "Iteration 57176, loss = 1.68585580\n",
      "Iteration 57177, loss = 2.14644324\n",
      "Iteration 57178, loss = 2.22005405\n",
      "Iteration 57179, loss = 1.94144488\n",
      "Iteration 57180, loss = 3.63744366\n",
      "Iteration 57181, loss = 3.14909285\n",
      "Iteration 57182, loss = 4.22723507\n",
      "Iteration 57183, loss = 5.41969341\n",
      "Iteration 57184, loss = 5.51709605\n",
      "Iteration 57185, loss = 4.51978684\n",
      "Iteration 57186, loss = 3.10217628\n",
      "Iteration 57187, loss = 2.47814333\n",
      "Iteration 57188, loss = 1.89147124\n",
      "Iteration 57189, loss = 2.02269968\n",
      "Iteration 57190, loss = 1.73299167\n",
      "Iteration 57191, loss = 1.94067826\n",
      "Iteration 57192, loss = 1.70948424\n",
      "Iteration 57193, loss = 1.31046609\n",
      "Iteration 57194, loss = 1.42368840\n",
      "Iteration 57195, loss = 1.45059279\n",
      "Iteration 57196, loss = 1.60345629\n",
      "Iteration 57197, loss = 1.71057398\n",
      "Iteration 57198, loss = 1.68439059\n",
      "Iteration 57199, loss = 1.69939486\n",
      "Iteration 57200, loss = 1.56234946\n",
      "Iteration 57201, loss = 1.66058673\n",
      "Iteration 57202, loss = 1.69520734\n",
      "Iteration 57203, loss = 1.57532576\n",
      "Iteration 57204, loss = 1.60418806\n",
      "Iteration 57205, loss = 1.67025491\n",
      "Iteration 57206, loss = 1.88970790\n",
      "Iteration 57207, loss = 1.47211618\n",
      "Iteration 57208, loss = 1.32600164\n",
      "Iteration 57209, loss = 1.41631657\n",
      "Iteration 57210, loss = 1.35787346\n",
      "Iteration 57211, loss = 1.44782664\n",
      "Iteration 57212, loss = 1.67623517\n",
      "Iteration 57213, loss = 1.66907390\n",
      "Iteration 57214, loss = 1.51570237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 57215, loss = 1.84547660\n",
      "Iteration 57216, loss = 1.74743536\n",
      "Iteration 57217, loss = 1.76959391\n",
      "Iteration 57218, loss = 1.41783867\n",
      "Iteration 57219, loss = 1.35084367\n",
      "Iteration 57220, loss = 1.36984386\n",
      "Iteration 57221, loss = 1.58893954\n",
      "Iteration 57222, loss = 1.38339681\n",
      "Iteration 57223, loss = 1.66532864\n",
      "Iteration 57224, loss = 1.42023225\n",
      "Iteration 57225, loss = 1.34189559\n",
      "Iteration 57226, loss = 1.43863901\n",
      "Iteration 57227, loss = 1.46611882\n",
      "Iteration 57228, loss = 1.41291466\n",
      "Iteration 57229, loss = 1.33399400\n",
      "Iteration 57230, loss = 1.37969701\n",
      "Iteration 57231, loss = 1.43421710\n",
      "Iteration 57232, loss = 1.25486385\n",
      "Iteration 57233, loss = 1.25811958\n",
      "Iteration 57234, loss = 1.29341726\n",
      "Iteration 57235, loss = 1.33206445\n",
      "Iteration 57236, loss = 1.28669565\n",
      "Iteration 57237, loss = 1.38796885\n",
      "Iteration 57238, loss = 1.58060960\n",
      "Iteration 57239, loss = 1.44722686\n",
      "Iteration 57240, loss = 1.78120833\n",
      "Iteration 57241, loss = 1.64869976\n",
      "Iteration 57242, loss = 1.45482650\n",
      "Iteration 57243, loss = 1.68180339\n",
      "Iteration 57244, loss = 1.84420110\n",
      "Iteration 57245, loss = 2.61126204\n",
      "Iteration 57246, loss = 1.98956876\n",
      "Iteration 57247, loss = 1.86934702\n",
      "Iteration 57248, loss = 1.75068884\n",
      "Iteration 57249, loss = 1.57006614\n",
      "Iteration 57250, loss = 1.45884732\n",
      "Iteration 57251, loss = 1.48670294\n",
      "Iteration 57252, loss = 1.28764471\n",
      "Iteration 57253, loss = 1.33730135\n",
      "Iteration 57254, loss = 1.45737072\n",
      "Iteration 57255, loss = 1.33064585\n",
      "Iteration 57256, loss = 1.21583222\n",
      "Iteration 57257, loss = 1.29067313\n",
      "Iteration 57258, loss = 1.28866496\n",
      "Iteration 57259, loss = 1.18055734\n",
      "Iteration 57260, loss = 1.25970679\n",
      "Iteration 57261, loss = 1.37696977\n",
      "Iteration 57262, loss = 1.22976955\n",
      "Iteration 57263, loss = 1.51587699\n",
      "Iteration 57264, loss = 1.66000690\n",
      "Iteration 57265, loss = 1.63355860\n",
      "Iteration 57266, loss = 1.61517205\n",
      "Iteration 57267, loss = 1.51913782\n",
      "Iteration 57268, loss = 1.88738834\n",
      "Iteration 57269, loss = 1.47784187\n",
      "Iteration 57270, loss = 1.29029168\n",
      "Iteration 57271, loss = 1.22869978\n",
      "Iteration 57272, loss = 1.24201256\n",
      "Iteration 57273, loss = 1.28428985\n",
      "Iteration 57274, loss = 1.30194414\n",
      "Iteration 57275, loss = 1.22162826\n",
      "Iteration 57276, loss = 1.19927372\n",
      "Iteration 57277, loss = 1.32476271\n",
      "Iteration 57278, loss = 1.34323897\n",
      "Iteration 57279, loss = 1.33078041\n",
      "Iteration 57280, loss = 1.64714414\n",
      "Iteration 57281, loss = 1.37520379\n",
      "Iteration 57282, loss = 1.65664114\n",
      "Iteration 57283, loss = 1.72776169\n",
      "Iteration 57284, loss = 2.16371987\n",
      "Iteration 57285, loss = 1.94966103\n",
      "Iteration 57286, loss = 2.19196407\n",
      "Iteration 57287, loss = 1.67107056\n",
      "Iteration 57288, loss = 1.46914730\n",
      "Iteration 57289, loss = 1.40018495\n",
      "Iteration 57290, loss = 1.23979254\n",
      "Iteration 57291, loss = 1.31261556\n",
      "Iteration 57292, loss = 1.39435192\n",
      "Iteration 57293, loss = 1.34347154\n",
      "Iteration 57294, loss = 1.37673654\n",
      "Iteration 57295, loss = 1.50417392\n",
      "Iteration 57296, loss = 1.40686712\n",
      "Iteration 57297, loss = 1.36596415\n",
      "Iteration 57298, loss = 1.81026971\n",
      "Iteration 57299, loss = 1.76501274\n",
      "Iteration 57300, loss = 1.68160276\n",
      "Iteration 57301, loss = 1.60504182\n",
      "Iteration 57302, loss = 1.62574954\n",
      "Iteration 57303, loss = 1.48837147\n",
      "Iteration 57304, loss = 1.23425626\n",
      "Iteration 57305, loss = 1.34589198\n",
      "Iteration 57306, loss = 1.51922399\n",
      "Iteration 57307, loss = 1.33104671\n",
      "Iteration 57308, loss = 1.71545515\n",
      "Iteration 57309, loss = 2.14787998\n",
      "Iteration 57310, loss = 1.77450754\n",
      "Iteration 57311, loss = 1.63465046\n",
      "Iteration 57312, loss = 1.79307077\n",
      "Iteration 57313, loss = 1.34505222\n",
      "Iteration 57314, loss = 1.46316955\n",
      "Iteration 57315, loss = 1.48164265\n",
      "Iteration 57316, loss = 1.94035343\n",
      "Iteration 57317, loss = 2.00223284\n",
      "Iteration 57318, loss = 1.57120571\n",
      "Iteration 57319, loss = 2.06890056\n",
      "Iteration 57320, loss = 1.78391743\n",
      "Iteration 57321, loss = 1.47249318\n",
      "Iteration 57322, loss = 1.51095616\n",
      "Iteration 57323, loss = 1.75901139\n",
      "Iteration 57324, loss = 1.94436961\n",
      "Iteration 57325, loss = 1.42773397\n",
      "Iteration 57326, loss = 1.52599001\n",
      "Iteration 57327, loss = 1.48917070\n",
      "Iteration 57328, loss = 2.11654997\n",
      "Iteration 57329, loss = 1.90845361\n",
      "Iteration 57330, loss = 1.39664031\n",
      "Iteration 57331, loss = 1.65518152\n",
      "Iteration 57332, loss = 2.47426733\n",
      "Iteration 57333, loss = 2.32797029\n",
      "Iteration 57334, loss = 2.13331344\n",
      "Iteration 57335, loss = 2.17256362\n",
      "Iteration 57336, loss = 1.67189983\n",
      "Iteration 57337, loss = 2.09113033\n",
      "Iteration 57338, loss = 2.02002594\n",
      "Iteration 57339, loss = 2.07458160\n",
      "Iteration 57340, loss = 1.97677024\n",
      "Iteration 57341, loss = 2.55230445\n",
      "Iteration 57342, loss = 2.90680801\n",
      "Iteration 57343, loss = 1.79578208\n",
      "Iteration 57344, loss = 2.08098324\n",
      "Iteration 57345, loss = 2.39009291\n",
      "Iteration 57346, loss = 3.11524550\n",
      "Iteration 57347, loss = 2.11120792\n",
      "Iteration 57348, loss = 2.11196965\n",
      "Iteration 57349, loss = 1.99868178\n",
      "Iteration 57350, loss = 2.29732841\n",
      "Iteration 57351, loss = 2.80778830\n",
      "Iteration 57352, loss = 2.49545207\n",
      "Iteration 57353, loss = 1.86874084\n",
      "Iteration 57354, loss = 1.47877719\n",
      "Iteration 57355, loss = 1.51997729\n",
      "Iteration 57356, loss = 1.50968052\n",
      "Iteration 57357, loss = 1.46864153\n",
      "Iteration 57358, loss = 1.51227332\n",
      "Iteration 57359, loss = 1.45124483\n",
      "Iteration 57360, loss = 1.57899521\n",
      "Iteration 57361, loss = 1.67084931\n",
      "Iteration 57362, loss = 1.50685886\n",
      "Iteration 57363, loss = 1.35711589\n",
      "Iteration 57364, loss = 1.33620732\n",
      "Iteration 57365, loss = 1.36528657\n",
      "Iteration 57366, loss = 1.66347304\n",
      "Iteration 57367, loss = 2.08392472\n",
      "Iteration 57368, loss = 1.74220088\n",
      "Iteration 57369, loss = 2.09645667\n",
      "Iteration 57370, loss = 1.43295670\n",
      "Iteration 57371, loss = 1.39337765\n",
      "Iteration 57372, loss = 1.45415491\n",
      "Iteration 57373, loss = 1.42059475\n",
      "Iteration 57374, loss = 1.34779584\n",
      "Iteration 57375, loss = 1.33166978\n",
      "Iteration 57376, loss = 1.60160962\n",
      "Iteration 57377, loss = 1.82794758\n",
      "Iteration 57378, loss = 1.41990246\n",
      "Iteration 57379, loss = 1.75806938\n",
      "Iteration 57380, loss = 1.68317508\n",
      "Iteration 57381, loss = 1.52055107\n",
      "Iteration 57382, loss = 1.67939313\n",
      "Iteration 57383, loss = 1.42376340\n",
      "Iteration 57384, loss = 1.37410430\n",
      "Iteration 57385, loss = 1.39555819\n",
      "Iteration 57386, loss = 1.44378173\n",
      "Iteration 57387, loss = 1.42752414\n",
      "Iteration 57388, loss = 1.26842237\n",
      "Iteration 57389, loss = 1.64656223\n",
      "Iteration 57390, loss = 1.59358929\n",
      "Iteration 57391, loss = 2.50114556\n",
      "Iteration 57392, loss = 1.76528695\n",
      "Iteration 57393, loss = 1.43401069\n",
      "Iteration 57394, loss = 1.78724024\n",
      "Iteration 57395, loss = 1.61386394\n",
      "Iteration 57396, loss = 1.74124259\n",
      "Iteration 57397, loss = 1.67920332\n",
      "Iteration 57398, loss = 1.87817397\n",
      "Iteration 57399, loss = 1.78343764\n",
      "Iteration 57400, loss = 1.60380602\n",
      "Iteration 57401, loss = 1.45126850\n",
      "Iteration 57402, loss = 1.85673794\n",
      "Iteration 57403, loss = 2.11163905\n",
      "Iteration 57404, loss = 1.68210219\n",
      "Iteration 57405, loss = 1.48781536\n",
      "Iteration 57406, loss = 1.28672987\n",
      "Iteration 57407, loss = 1.53776994\n",
      "Iteration 57408, loss = 1.23148613\n",
      "Iteration 57409, loss = 1.46273633\n",
      "Iteration 57410, loss = 2.18291090\n",
      "Iteration 57411, loss = 2.23758359\n",
      "Iteration 57412, loss = 1.56403130\n",
      "Iteration 57413, loss = 1.52219720\n",
      "Iteration 57414, loss = 1.59638507\n",
      "Iteration 57415, loss = 1.80901137\n",
      "Iteration 57416, loss = 1.80265098\n",
      "Iteration 57417, loss = 2.03131806\n",
      "Iteration 57418, loss = 1.69775836\n",
      "Iteration 57419, loss = 1.58514643\n",
      "Iteration 57420, loss = 1.40659188\n",
      "Iteration 57421, loss = 1.42718414\n",
      "Iteration 57422, loss = 1.82247015\n",
      "Iteration 57423, loss = 1.80389889\n",
      "Iteration 57424, loss = 1.95292061\n",
      "Iteration 57425, loss = 1.87082784\n",
      "Iteration 57426, loss = 1.97861204\n",
      "Iteration 57427, loss = 1.86210378\n",
      "Iteration 57428, loss = 1.74105406\n",
      "Iteration 57429, loss = 1.65773949\n",
      "Iteration 57430, loss = 1.71572634\n",
      "Iteration 57431, loss = 1.72933555\n",
      "Iteration 57432, loss = 1.62463181\n",
      "Iteration 57433, loss = 1.42211011\n",
      "Iteration 57434, loss = 1.40513084\n",
      "Iteration 57435, loss = 1.29159353\n",
      "Iteration 57436, loss = 1.39958681\n",
      "Iteration 57437, loss = 1.28326636\n",
      "Iteration 57438, loss = 1.35803900\n",
      "Iteration 57439, loss = 1.36693363\n",
      "Iteration 57440, loss = 1.40269862\n",
      "Iteration 57441, loss = 1.36378314\n",
      "Iteration 57442, loss = 1.24932605\n",
      "Iteration 57443, loss = 1.20752152\n",
      "Iteration 57444, loss = 1.23549355\n",
      "Iteration 57445, loss = 1.32380654\n",
      "Iteration 57446, loss = 1.36774267\n",
      "Iteration 57447, loss = 1.49754362\n",
      "Iteration 57448, loss = 1.81388752\n",
      "Iteration 57449, loss = 1.36933591\n",
      "Iteration 57450, loss = 1.27842672\n",
      "Iteration 57451, loss = 1.28566627\n",
      "Iteration 57452, loss = 1.60729639\n",
      "Iteration 57453, loss = 1.79110481\n",
      "Iteration 57454, loss = 1.77210612\n",
      "Iteration 57455, loss = 2.27261051\n",
      "Iteration 57456, loss = 2.36970091\n",
      "Iteration 57457, loss = 2.30859358\n",
      "Iteration 57458, loss = 1.99782407\n",
      "Iteration 57459, loss = 1.65051899\n",
      "Iteration 57460, loss = 1.58261965\n",
      "Iteration 57461, loss = 1.37578159\n",
      "Iteration 57462, loss = 1.67939710\n",
      "Iteration 57463, loss = 1.39037908\n",
      "Iteration 57464, loss = 1.47561991\n",
      "Iteration 57465, loss = 1.55338810\n",
      "Iteration 57466, loss = 1.49351064\n",
      "Iteration 57467, loss = 1.67573753\n",
      "Iteration 57468, loss = 1.71815253\n",
      "Iteration 57469, loss = 1.66089041\n",
      "Iteration 57470, loss = 1.41040648\n",
      "Iteration 57471, loss = 1.81077531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 57472, loss = 1.55006028\n",
      "Iteration 57473, loss = 1.65472601\n",
      "Iteration 57474, loss = 2.00292570\n",
      "Iteration 57475, loss = 1.91970427\n",
      "Iteration 57476, loss = 1.64800799\n",
      "Iteration 57477, loss = 1.58197572\n",
      "Iteration 57478, loss = 1.36167201\n",
      "Iteration 57479, loss = 1.35752270\n",
      "Iteration 57480, loss = 1.59319984\n",
      "Iteration 57481, loss = 1.51887543\n",
      "Iteration 57482, loss = 1.41980363\n",
      "Iteration 57483, loss = 1.50452422\n",
      "Iteration 57484, loss = 1.53457746\n",
      "Iteration 57485, loss = 1.71052839\n",
      "Iteration 57486, loss = 1.62631676\n",
      "Iteration 57487, loss = 2.11532977\n",
      "Iteration 57488, loss = 1.67781255\n",
      "Iteration 57489, loss = 1.54625837\n",
      "Iteration 57490, loss = 1.51595218\n",
      "Iteration 57491, loss = 1.78484441\n",
      "Iteration 57492, loss = 2.40338249\n",
      "Iteration 57493, loss = 1.91370994\n",
      "Iteration 57494, loss = 2.28350305\n",
      "Iteration 57495, loss = 1.76695461\n",
      "Iteration 57496, loss = 1.86068066\n",
      "Iteration 57497, loss = 1.84360738\n",
      "Iteration 57498, loss = 1.75412896\n",
      "Iteration 57499, loss = 1.42469047\n",
      "Iteration 57500, loss = 1.33697330\n",
      "Iteration 57501, loss = 1.71675755\n",
      "Iteration 57502, loss = 2.31839199\n",
      "Iteration 57503, loss = 2.46776423\n",
      "Iteration 57504, loss = 2.40941957\n",
      "Iteration 57505, loss = 2.40685334\n",
      "Iteration 57506, loss = 2.21485373\n",
      "Iteration 57507, loss = 2.60861884\n",
      "Iteration 57508, loss = 2.90878028\n",
      "Iteration 57509, loss = 2.23595592\n",
      "Iteration 57510, loss = 1.35453962\n",
      "Iteration 57511, loss = 1.38728851\n",
      "Iteration 57512, loss = 1.48637317\n",
      "Iteration 57513, loss = 1.42514625\n",
      "Iteration 57514, loss = 1.30481425\n",
      "Iteration 57515, loss = 1.35430325\n",
      "Iteration 57516, loss = 1.26175936\n",
      "Iteration 57517, loss = 1.29930594\n",
      "Iteration 57518, loss = 1.31035702\n",
      "Iteration 57519, loss = 1.47728753\n",
      "Iteration 57520, loss = 1.45792170\n",
      "Iteration 57521, loss = 1.48605845\n",
      "Iteration 57522, loss = 1.39394223\n",
      "Iteration 57523, loss = 1.52690976\n",
      "Iteration 57524, loss = 1.46324145\n",
      "Iteration 57525, loss = 1.28344808\n",
      "Iteration 57526, loss = 1.53938188\n",
      "Iteration 57527, loss = 1.59482863\n",
      "Iteration 57528, loss = 1.39235506\n",
      "Iteration 57529, loss = 1.40176243\n",
      "Iteration 57530, loss = 1.30669809\n",
      "Iteration 57531, loss = 1.45781252\n",
      "Iteration 57532, loss = 1.48201210\n",
      "Iteration 57533, loss = 1.22342883\n",
      "Iteration 57534, loss = 1.32142747\n",
      "Iteration 57535, loss = 1.40725251\n",
      "Iteration 57536, loss = 1.52323468\n",
      "Iteration 57537, loss = 1.56288696\n",
      "Iteration 57538, loss = 1.62249502\n",
      "Iteration 57539, loss = 1.82843415\n",
      "Iteration 57540, loss = 2.10317959\n",
      "Iteration 57541, loss = 1.58658045\n",
      "Iteration 57542, loss = 1.37410497\n",
      "Iteration 57543, loss = 1.47898531\n",
      "Iteration 57544, loss = 1.39213867\n",
      "Iteration 57545, loss = 1.33408974\n",
      "Iteration 57546, loss = 1.53602168\n",
      "Iteration 57547, loss = 1.72285664\n",
      "Iteration 57548, loss = 1.53106316\n",
      "Iteration 57549, loss = 2.15175092\n",
      "Iteration 57550, loss = 1.77329995\n",
      "Iteration 57551, loss = 1.52138516\n",
      "Iteration 57552, loss = 1.65883652\n",
      "Iteration 57553, loss = 2.07188420\n",
      "Iteration 57554, loss = 1.65677492\n",
      "Iteration 57555, loss = 2.02764431\n",
      "Iteration 57556, loss = 1.87964739\n",
      "Iteration 57557, loss = 1.64249281\n",
      "Iteration 57558, loss = 1.63874324\n",
      "Iteration 57559, loss = 1.96335043\n",
      "Iteration 57560, loss = 1.93941907\n",
      "Iteration 57561, loss = 2.08108309\n",
      "Iteration 57562, loss = 1.84087794\n",
      "Iteration 57563, loss = 1.49153593\n",
      "Iteration 57564, loss = 1.50672321\n",
      "Iteration 57565, loss = 1.59887630\n",
      "Iteration 57566, loss = 1.52750831\n",
      "Iteration 57567, loss = 1.63655286\n",
      "Iteration 57568, loss = 1.66831792\n",
      "Iteration 57569, loss = 2.06681030\n",
      "Iteration 57570, loss = 1.41301385\n",
      "Iteration 57571, loss = 1.80450010\n",
      "Iteration 57572, loss = 1.81774360\n",
      "Iteration 57573, loss = 1.71171326\n",
      "Iteration 57574, loss = 1.42750608\n",
      "Iteration 57575, loss = 1.56228409\n",
      "Iteration 57576, loss = 1.39055431\n",
      "Iteration 57577, loss = 1.46403465\n",
      "Iteration 57578, loss = 1.32944849\n",
      "Iteration 57579, loss = 1.33279910\n",
      "Iteration 57580, loss = 1.24658732\n",
      "Iteration 57581, loss = 1.36322801\n",
      "Iteration 57582, loss = 1.28719360\n",
      "Iteration 57583, loss = 1.48399722\n",
      "Iteration 57584, loss = 1.50207206\n",
      "Iteration 57585, loss = 1.32091385\n",
      "Iteration 57586, loss = 1.48871239\n",
      "Iteration 57587, loss = 1.72614209\n",
      "Iteration 57588, loss = 1.46279058\n",
      "Iteration 57589, loss = 1.41565743\n",
      "Iteration 57590, loss = 1.52193965\n",
      "Iteration 57591, loss = 1.42478628\n",
      "Iteration 57592, loss = 1.55679291\n",
      "Iteration 57593, loss = 1.32194792\n",
      "Iteration 57594, loss = 1.28488604\n",
      "Iteration 57595, loss = 1.31522599\n",
      "Iteration 57596, loss = 1.41246911\n",
      "Iteration 57597, loss = 1.41603277\n",
      "Iteration 57598, loss = 1.31226488\n",
      "Iteration 57599, loss = 1.37706133\n",
      "Iteration 57600, loss = 1.40527691\n",
      "Iteration 57601, loss = 1.40175457\n",
      "Iteration 57602, loss = 1.60541085\n",
      "Iteration 57603, loss = 1.47750919\n",
      "Iteration 57604, loss = 1.40657008\n",
      "Iteration 57605, loss = 1.38840765\n",
      "Iteration 57606, loss = 1.88635470\n",
      "Iteration 57607, loss = 2.05451899\n",
      "Iteration 57608, loss = 3.12724148\n",
      "Iteration 57609, loss = 3.49979705\n",
      "Iteration 57610, loss = 4.16934017\n",
      "Iteration 57611, loss = 2.58909709\n",
      "Iteration 57612, loss = 1.97733029\n",
      "Iteration 57613, loss = 1.82614337\n",
      "Iteration 57614, loss = 2.49310322\n",
      "Iteration 57615, loss = 1.95388661\n",
      "Iteration 57616, loss = 2.26415471\n",
      "Iteration 57617, loss = 2.36157502\n",
      "Iteration 57618, loss = 2.50265041\n",
      "Iteration 57619, loss = 2.35482646\n",
      "Iteration 57620, loss = 2.29111309\n",
      "Iteration 57621, loss = 1.67869986\n",
      "Iteration 57622, loss = 1.64964673\n",
      "Iteration 57623, loss = 1.80440569\n",
      "Iteration 57624, loss = 1.60520012\n",
      "Iteration 57625, loss = 1.56813631\n",
      "Iteration 57626, loss = 1.47960243\n",
      "Iteration 57627, loss = 1.49544679\n",
      "Iteration 57628, loss = 1.62028579\n",
      "Iteration 57629, loss = 1.90537915\n",
      "Iteration 57630, loss = 1.59733630\n",
      "Iteration 57631, loss = 1.77536841\n",
      "Iteration 57632, loss = 1.75841828\n",
      "Iteration 57633, loss = 1.95635777\n",
      "Iteration 57634, loss = 1.58313463\n",
      "Iteration 57635, loss = 1.46008308\n",
      "Iteration 57636, loss = 1.68691244\n",
      "Iteration 57637, loss = 1.72450079\n",
      "Iteration 57638, loss = 1.43430713\n",
      "Iteration 57639, loss = 1.30893300\n",
      "Iteration 57640, loss = 1.28292388\n",
      "Iteration 57641, loss = 1.72457024\n",
      "Iteration 57642, loss = 1.67678654\n",
      "Iteration 57643, loss = 1.75612311\n",
      "Iteration 57644, loss = 2.39134304\n",
      "Iteration 57645, loss = 1.90044222\n",
      "Iteration 57646, loss = 1.55640379\n",
      "Iteration 57647, loss = 1.73715584\n",
      "Iteration 57648, loss = 1.66342357\n",
      "Iteration 57649, loss = 1.87774758\n",
      "Iteration 57650, loss = 1.58765275\n",
      "Iteration 57651, loss = 1.58292589\n",
      "Iteration 57652, loss = 1.53624543\n",
      "Iteration 57653, loss = 1.64156550\n",
      "Iteration 57654, loss = 1.68883406\n",
      "Iteration 57655, loss = 1.77196130\n",
      "Iteration 57656, loss = 1.90976439\n",
      "Iteration 57657, loss = 1.65353539\n",
      "Iteration 57658, loss = 2.15189670\n",
      "Iteration 57659, loss = 1.77178748\n",
      "Iteration 57660, loss = 1.91816142\n",
      "Iteration 57661, loss = 1.64096979\n",
      "Iteration 57662, loss = 1.64835859\n",
      "Iteration 57663, loss = 1.33151639\n",
      "Iteration 57664, loss = 1.40388662\n",
      "Iteration 57665, loss = 1.23140402\n",
      "Iteration 57666, loss = 1.40993030\n",
      "Iteration 57667, loss = 1.41163031\n",
      "Iteration 57668, loss = 1.85180624\n",
      "Iteration 57669, loss = 1.64317943\n",
      "Iteration 57670, loss = 1.52660039\n",
      "Iteration 57671, loss = 1.53080129\n",
      "Iteration 57672, loss = 1.74388829\n",
      "Iteration 57673, loss = 1.36216949\n",
      "Iteration 57674, loss = 1.24535053\n",
      "Iteration 57675, loss = 1.27105211\n",
      "Iteration 57676, loss = 1.24826083\n",
      "Iteration 57677, loss = 1.45661347\n",
      "Iteration 57678, loss = 1.54737710\n",
      "Iteration 57679, loss = 1.32084630\n",
      "Iteration 57680, loss = 1.50354546\n",
      "Iteration 57681, loss = 1.46308497\n",
      "Iteration 57682, loss = 1.61180880\n",
      "Iteration 57683, loss = 1.60789946\n",
      "Iteration 57684, loss = 1.38185469\n",
      "Iteration 57685, loss = 1.44628130\n",
      "Iteration 57686, loss = 1.37196003\n",
      "Iteration 57687, loss = 1.42281348\n",
      "Iteration 57688, loss = 1.33706471\n",
      "Iteration 57689, loss = 1.33157755\n",
      "Iteration 57690, loss = 1.37544224\n",
      "Iteration 57691, loss = 1.25692029\n",
      "Iteration 57692, loss = 1.34432308\n",
      "Iteration 57693, loss = 1.37221605\n",
      "Iteration 57694, loss = 1.40497696\n",
      "Iteration 57695, loss = 1.80558276\n",
      "Iteration 57696, loss = 1.40163105\n",
      "Iteration 57697, loss = 1.28269975\n",
      "Iteration 57698, loss = 3.13822147\n",
      "Iteration 57699, loss = 4.32109562\n",
      "Iteration 57700, loss = 2.62685924\n",
      "Iteration 57701, loss = 2.55510459\n",
      "Iteration 57702, loss = 3.01001280\n",
      "Iteration 57703, loss = 2.18372539\n",
      "Iteration 57704, loss = 2.24413345\n",
      "Iteration 57705, loss = 1.80194834\n",
      "Iteration 57706, loss = 2.00377349\n",
      "Iteration 57707, loss = 1.95508113\n",
      "Iteration 57708, loss = 2.25845211\n",
      "Iteration 57709, loss = 1.73852041\n",
      "Iteration 57710, loss = 1.50070442\n",
      "Iteration 57711, loss = 2.11058748\n",
      "Iteration 57712, loss = 1.81815766\n",
      "Iteration 57713, loss = 1.66042826\n",
      "Iteration 57714, loss = 1.55713442\n",
      "Iteration 57715, loss = 1.57092901\n",
      "Iteration 57716, loss = 1.69228878\n",
      "Iteration 57717, loss = 1.74011782\n",
      "Iteration 57718, loss = 1.36812483\n",
      "Iteration 57719, loss = 1.54549005\n",
      "Iteration 57720, loss = 1.69688733\n",
      "Iteration 57721, loss = 1.61404882\n",
      "Iteration 57722, loss = 1.49985942\n",
      "Iteration 57723, loss = 1.46367722\n",
      "Iteration 57724, loss = 1.32720343\n",
      "Iteration 57725, loss = 1.27621149\n",
      "Iteration 57726, loss = 1.28150655\n",
      "Iteration 57727, loss = 1.28476130\n",
      "Iteration 57728, loss = 1.40394525\n",
      "Iteration 57729, loss = 1.58520332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 57730, loss = 1.77942397\n",
      "Iteration 57731, loss = 1.65431788\n",
      "Iteration 57732, loss = 1.77001223\n",
      "Iteration 57733, loss = 1.36689596\n",
      "Iteration 57734, loss = 1.36008085\n",
      "Iteration 57735, loss = 1.29179820\n",
      "Iteration 57736, loss = 1.40577463\n",
      "Iteration 57737, loss = 1.63735784\n",
      "Iteration 57738, loss = 1.71203689\n",
      "Iteration 57739, loss = 1.68982258\n",
      "Iteration 57740, loss = 1.59539076\n",
      "Iteration 57741, loss = 1.63916801\n",
      "Iteration 57742, loss = 1.80354285\n",
      "Iteration 57743, loss = 1.37728895\n",
      "Iteration 57744, loss = 1.41645133\n",
      "Iteration 57745, loss = 2.04790871\n",
      "Iteration 57746, loss = 1.85147493\n",
      "Iteration 57747, loss = 1.56072588\n",
      "Iteration 57748, loss = 2.43129838\n",
      "Iteration 57749, loss = 3.06121320\n",
      "Iteration 57750, loss = 2.83816977\n",
      "Iteration 57751, loss = 2.75966636\n",
      "Iteration 57752, loss = 1.91887173\n",
      "Iteration 57753, loss = 1.85197189\n",
      "Iteration 57754, loss = 1.64157690\n",
      "Iteration 57755, loss = 1.68553477\n",
      "Iteration 57756, loss = 1.57567673\n",
      "Iteration 57757, loss = 1.64401038\n",
      "Iteration 57758, loss = 1.61075743\n",
      "Iteration 57759, loss = 1.65262762\n",
      "Iteration 57760, loss = 1.45497379\n",
      "Iteration 57761, loss = 1.45282972\n",
      "Iteration 57762, loss = 1.21500196\n",
      "Iteration 57763, loss = 1.69640614\n",
      "Iteration 57764, loss = 1.54207093\n",
      "Iteration 57765, loss = 1.49921114\n",
      "Iteration 57766, loss = 1.45472145\n",
      "Iteration 57767, loss = 1.49618099\n",
      "Iteration 57768, loss = 1.66026187\n",
      "Iteration 57769, loss = 1.88228917\n",
      "Iteration 57770, loss = 1.40880399\n",
      "Iteration 57771, loss = 1.56185324\n",
      "Iteration 57772, loss = 1.29434013\n",
      "Iteration 57773, loss = 1.27920385\n",
      "Iteration 57774, loss = 1.37970533\n",
      "Iteration 57775, loss = 1.35267514\n",
      "Iteration 57776, loss = 1.58009561\n",
      "Iteration 57777, loss = 1.79827948\n",
      "Iteration 57778, loss = 1.69105999\n",
      "Iteration 57779, loss = 1.32098269\n",
      "Iteration 57780, loss = 1.55403075\n",
      "Iteration 57781, loss = 1.48969708\n",
      "Iteration 57782, loss = 1.40683857\n",
      "Iteration 57783, loss = 1.52942052\n",
      "Iteration 57784, loss = 1.43402998\n",
      "Iteration 57785, loss = 1.46338857\n",
      "Iteration 57786, loss = 2.46091814\n",
      "Iteration 57787, loss = 2.63281849\n",
      "Iteration 57788, loss = 2.72900321\n",
      "Iteration 57789, loss = 2.61184191\n",
      "Iteration 57790, loss = 2.28339367\n",
      "Iteration 57791, loss = 1.67115263\n",
      "Iteration 57792, loss = 2.21866843\n",
      "Iteration 57793, loss = 2.16670383\n",
      "Iteration 57794, loss = 2.81541664\n",
      "Iteration 57795, loss = 2.73164822\n",
      "Iteration 57796, loss = 2.80188310\n",
      "Iteration 57797, loss = 1.88773385\n",
      "Iteration 57798, loss = 1.75705640\n",
      "Iteration 57799, loss = 2.31815292\n",
      "Iteration 57800, loss = 2.27040797\n",
      "Iteration 57801, loss = 2.14600052\n",
      "Iteration 57802, loss = 2.03888491\n",
      "Iteration 57803, loss = 1.91181622\n",
      "Iteration 57804, loss = 1.80418636\n",
      "Iteration 57805, loss = 1.71599520\n",
      "Iteration 57806, loss = 1.95300290\n",
      "Iteration 57807, loss = 1.90094429\n",
      "Iteration 57808, loss = 2.09169036\n",
      "Iteration 57809, loss = 2.37640741\n",
      "Iteration 57810, loss = 1.60786757\n",
      "Iteration 57811, loss = 1.86487043\n",
      "Iteration 57812, loss = 1.77721264\n",
      "Iteration 57813, loss = 1.74825427\n",
      "Iteration 57814, loss = 1.67282245\n",
      "Iteration 57815, loss = 1.81646529\n",
      "Iteration 57816, loss = 1.42996584\n",
      "Iteration 57817, loss = 1.37907742\n",
      "Iteration 57818, loss = 1.38215322\n",
      "Iteration 57819, loss = 1.37192729\n",
      "Iteration 57820, loss = 1.45007710\n",
      "Iteration 57821, loss = 1.26061359\n",
      "Iteration 57822, loss = 1.45285025\n",
      "Iteration 57823, loss = 1.69691507\n",
      "Iteration 57824, loss = 2.16357105\n",
      "Iteration 57825, loss = 1.70274881\n",
      "Iteration 57826, loss = 1.33401316\n",
      "Iteration 57827, loss = 1.36872385\n",
      "Iteration 57828, loss = 1.37797860\n",
      "Iteration 57829, loss = 1.48524274\n",
      "Iteration 57830, loss = 1.55582340\n",
      "Iteration 57831, loss = 1.86971968\n",
      "Iteration 57832, loss = 2.09921363\n",
      "Iteration 57833, loss = 1.46281214\n",
      "Iteration 57834, loss = 1.69025570\n",
      "Iteration 57835, loss = 1.51738459\n",
      "Iteration 57836, loss = 1.39973563\n",
      "Iteration 57837, loss = 1.53465659\n",
      "Iteration 57838, loss = 1.59540484\n",
      "Iteration 57839, loss = 1.58179413\n",
      "Iteration 57840, loss = 1.64153416\n",
      "Iteration 57841, loss = 1.57606055\n",
      "Iteration 57842, loss = 1.37456261\n",
      "Iteration 57843, loss = 1.36383905\n",
      "Iteration 57844, loss = 1.45805177\n",
      "Iteration 57845, loss = 1.47045004\n",
      "Iteration 57846, loss = 1.46906862\n",
      "Iteration 57847, loss = 1.39086574\n",
      "Iteration 57848, loss = 1.38665327\n",
      "Iteration 57849, loss = 1.54846272\n",
      "Iteration 57850, loss = 1.83702646\n",
      "Iteration 57851, loss = 2.47331619\n",
      "Iteration 57852, loss = 2.16570494\n",
      "Iteration 57853, loss = 1.80424991\n",
      "Iteration 57854, loss = 1.63224357\n",
      "Iteration 57855, loss = 1.71062580\n",
      "Iteration 57856, loss = 1.53263805\n",
      "Iteration 57857, loss = 1.70104090\n",
      "Iteration 57858, loss = 1.61561188\n",
      "Iteration 57859, loss = 1.50556244\n",
      "Iteration 57860, loss = 1.42638228\n",
      "Iteration 57861, loss = 1.31472707\n",
      "Iteration 57862, loss = 1.30575778\n",
      "Iteration 57863, loss = 1.27838299\n",
      "Iteration 57864, loss = 1.29318084\n",
      "Iteration 57865, loss = 1.44947256\n",
      "Iteration 57866, loss = 1.40005817\n",
      "Iteration 57867, loss = 1.43916985\n",
      "Iteration 57868, loss = 1.49512472\n",
      "Iteration 57869, loss = 1.46011360\n",
      "Iteration 57870, loss = 1.39289336\n",
      "Iteration 57871, loss = 1.33648176\n",
      "Iteration 57872, loss = 1.57559375\n",
      "Iteration 57873, loss = 1.48562881\n",
      "Iteration 57874, loss = 1.51379376\n",
      "Iteration 57875, loss = 1.42084104\n",
      "Iteration 57876, loss = 1.54393345\n",
      "Iteration 57877, loss = 1.45416193\n",
      "Iteration 57878, loss = 1.47242781\n",
      "Iteration 57879, loss = 1.61498043\n",
      "Iteration 57880, loss = 1.81478584\n",
      "Iteration 57881, loss = 1.78366456\n",
      "Iteration 57882, loss = 1.65218215\n",
      "Iteration 57883, loss = 1.31191111\n",
      "Iteration 57884, loss = 1.63564319\n",
      "Iteration 57885, loss = 1.34149414\n",
      "Iteration 57886, loss = 1.32097663\n",
      "Iteration 57887, loss = 1.57561821\n",
      "Iteration 57888, loss = 1.82052744\n",
      "Iteration 57889, loss = 1.70484767\n",
      "Iteration 57890, loss = 1.63842248\n",
      "Iteration 57891, loss = 1.42980073\n",
      "Iteration 57892, loss = 1.30037009\n",
      "Iteration 57893, loss = 1.38546021\n",
      "Iteration 57894, loss = 1.19619368\n",
      "Iteration 57895, loss = 1.28228831\n",
      "Iteration 57896, loss = 1.36836169\n",
      "Iteration 57897, loss = 1.47567133\n",
      "Iteration 57898, loss = 1.53900634\n",
      "Iteration 57899, loss = 1.92425264\n",
      "Iteration 57900, loss = 1.51485253\n",
      "Iteration 57901, loss = 1.60678996\n",
      "Iteration 57902, loss = 1.52399883\n",
      "Iteration 57903, loss = 1.77082865\n",
      "Iteration 57904, loss = 1.51718116\n",
      "Iteration 57905, loss = 1.77920959\n",
      "Iteration 57906, loss = 1.71195035\n",
      "Iteration 57907, loss = 1.65346539\n",
      "Iteration 57908, loss = 1.73429594\n",
      "Iteration 57909, loss = 1.33817079\n",
      "Iteration 57910, loss = 1.42787561\n",
      "Iteration 57911, loss = 1.60123039\n",
      "Iteration 57912, loss = 1.63664429\n",
      "Iteration 57913, loss = 1.59060596\n",
      "Iteration 57914, loss = 1.44064090\n",
      "Iteration 57915, loss = 1.31880134\n",
      "Iteration 57916, loss = 1.26859488\n",
      "Iteration 57917, loss = 1.30324450\n",
      "Iteration 57918, loss = 1.39029927\n",
      "Iteration 57919, loss = 1.36945728\n",
      "Iteration 57920, loss = 1.76880146\n",
      "Iteration 57921, loss = 1.79191770\n",
      "Iteration 57922, loss = 2.07306292\n",
      "Iteration 57923, loss = 1.69037635\n",
      "Iteration 57924, loss = 2.52258638\n",
      "Iteration 57925, loss = 3.33216937\n",
      "Iteration 57926, loss = 3.19787291\n",
      "Iteration 57927, loss = 2.98207862\n",
      "Iteration 57928, loss = 2.36580248\n",
      "Iteration 57929, loss = 1.79136266\n",
      "Iteration 57930, loss = 2.14215808\n",
      "Iteration 57931, loss = 1.87472225\n",
      "Iteration 57932, loss = 2.28711074\n",
      "Iteration 57933, loss = 1.75175185\n",
      "Iteration 57934, loss = 1.61777390\n",
      "Iteration 57935, loss = 2.02098484\n",
      "Iteration 57936, loss = 1.78716502\n",
      "Iteration 57937, loss = 1.93865284\n",
      "Iteration 57938, loss = 1.73128463\n",
      "Iteration 57939, loss = 1.50212150\n",
      "Iteration 57940, loss = 1.77953180\n",
      "Iteration 57941, loss = 1.73598323\n",
      "Iteration 57942, loss = 1.77537928\n",
      "Iteration 57943, loss = 1.69281305\n",
      "Iteration 57944, loss = 1.67184304\n",
      "Iteration 57945, loss = 1.79621336\n",
      "Iteration 57946, loss = 1.63511039\n",
      "Iteration 57947, loss = 1.76878073\n",
      "Iteration 57948, loss = 1.35055923\n",
      "Iteration 57949, loss = 1.54184052\n",
      "Iteration 57950, loss = 1.40846039\n",
      "Iteration 57951, loss = 1.37650271\n",
      "Iteration 57952, loss = 1.42698476\n",
      "Iteration 57953, loss = 1.29136231\n",
      "Iteration 57954, loss = 1.35164928\n",
      "Iteration 57955, loss = 1.25782831\n",
      "Iteration 57956, loss = 1.28142991\n",
      "Iteration 57957, loss = 1.25937965\n",
      "Iteration 57958, loss = 1.49476552\n",
      "Iteration 57959, loss = 1.53820661\n",
      "Iteration 57960, loss = 1.61547336\n",
      "Iteration 57961, loss = 1.61613255\n",
      "Iteration 57962, loss = 1.52313398\n",
      "Iteration 57963, loss = 1.46566705\n",
      "Iteration 57964, loss = 1.19600042\n",
      "Iteration 57965, loss = 1.22970957\n",
      "Iteration 57966, loss = 1.20382150\n",
      "Iteration 57967, loss = 1.17416601\n",
      "Iteration 57968, loss = 1.20534289\n",
      "Iteration 57969, loss = 1.27964712\n",
      "Iteration 57970, loss = 1.17359537\n",
      "Iteration 57971, loss = 1.14854662\n",
      "Iteration 57972, loss = 1.19936848\n",
      "Iteration 57973, loss = 1.27338839\n",
      "Iteration 57974, loss = 1.39119931\n",
      "Iteration 57975, loss = 1.26737309\n",
      "Iteration 57976, loss = 1.46586920\n",
      "Iteration 57977, loss = 1.50423703\n",
      "Iteration 57978, loss = 1.28110373\n",
      "Iteration 57979, loss = 1.53043805\n",
      "Iteration 57980, loss = 1.56558457\n",
      "Iteration 57981, loss = 1.40469029\n",
      "Iteration 57982, loss = 1.41759585\n",
      "Iteration 57983, loss = 1.45165147\n",
      "Iteration 57984, loss = 1.60050246\n",
      "Iteration 57985, loss = 2.21117314\n",
      "Iteration 57986, loss = 2.27841104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 57987, loss = 1.54036437\n",
      "Iteration 57988, loss = 1.35339140\n",
      "Iteration 57989, loss = 1.73020254\n",
      "Iteration 57990, loss = 1.84324347\n",
      "Iteration 57991, loss = 1.94209007\n",
      "Iteration 57992, loss = 1.61233149\n",
      "Iteration 57993, loss = 1.58528550\n",
      "Iteration 57994, loss = 1.58916631\n",
      "Iteration 57995, loss = 1.81300245\n",
      "Iteration 57996, loss = 1.38667178\n",
      "Iteration 57997, loss = 1.40910014\n",
      "Iteration 57998, loss = 1.56215382\n",
      "Iteration 57999, loss = 1.48798557\n",
      "Iteration 58000, loss = 1.56153854\n",
      "Iteration 58001, loss = 1.60088638\n",
      "Iteration 58002, loss = 1.42917197\n",
      "Iteration 58003, loss = 1.37431959\n",
      "Iteration 58004, loss = 1.62914075\n",
      "Iteration 58005, loss = 1.42274703\n",
      "Iteration 58006, loss = 1.41740025\n",
      "Iteration 58007, loss = 1.40850068\n",
      "Iteration 58008, loss = 1.34856882\n",
      "Iteration 58009, loss = 1.34872615\n",
      "Iteration 58010, loss = 1.47882322\n",
      "Iteration 58011, loss = 1.47761866\n",
      "Iteration 58012, loss = 1.42611385\n",
      "Iteration 58013, loss = 1.37541498\n",
      "Iteration 58014, loss = 1.28148058\n",
      "Iteration 58015, loss = 1.44383597\n",
      "Iteration 58016, loss = 1.71674921\n",
      "Iteration 58017, loss = 1.64389906\n",
      "Iteration 58018, loss = 1.45238615\n",
      "Iteration 58019, loss = 1.65102184\n",
      "Iteration 58020, loss = 1.36222247\n",
      "Iteration 58021, loss = 1.81686710\n",
      "Iteration 58022, loss = 1.78902934\n",
      "Iteration 58023, loss = 1.95592889\n",
      "Iteration 58024, loss = 1.82958189\n",
      "Iteration 58025, loss = 1.65286120\n",
      "Iteration 58026, loss = 1.72984856\n",
      "Iteration 58027, loss = 2.03048999\n",
      "Iteration 58028, loss = 1.53582164\n",
      "Iteration 58029, loss = 1.45916885\n",
      "Iteration 58030, loss = 1.66864087\n",
      "Iteration 58031, loss = 1.51584156\n",
      "Iteration 58032, loss = 1.82671475\n",
      "Iteration 58033, loss = 2.73432573\n",
      "Iteration 58034, loss = 3.17724526\n",
      "Iteration 58035, loss = 2.68938957\n",
      "Iteration 58036, loss = 2.30554142\n",
      "Iteration 58037, loss = 2.09120572\n",
      "Iteration 58038, loss = 3.23111077\n",
      "Iteration 58039, loss = 1.94279035\n",
      "Iteration 58040, loss = 1.86056403\n",
      "Iteration 58041, loss = 1.60724530\n",
      "Iteration 58042, loss = 2.24661185\n",
      "Iteration 58043, loss = 2.22667010\n",
      "Iteration 58044, loss = 1.78510308\n",
      "Iteration 58045, loss = 2.07707891\n",
      "Iteration 58046, loss = 1.68641213\n",
      "Iteration 58047, loss = 1.72529103\n",
      "Iteration 58048, loss = 1.67429266\n",
      "Iteration 58049, loss = 1.36373448\n",
      "Iteration 58050, loss = 1.53398007\n",
      "Iteration 58051, loss = 1.97305496\n",
      "Iteration 58052, loss = 1.60420345\n",
      "Iteration 58053, loss = 1.60011430\n",
      "Iteration 58054, loss = 1.45334738\n",
      "Iteration 58055, loss = 1.38393634\n",
      "Iteration 58056, loss = 1.50870913\n",
      "Iteration 58057, loss = 1.27007974\n",
      "Iteration 58058, loss = 1.49031849\n",
      "Iteration 58059, loss = 1.31708551\n",
      "Iteration 58060, loss = 1.27572941\n",
      "Iteration 58061, loss = 1.34357995\n",
      "Iteration 58062, loss = 1.30747716\n",
      "Iteration 58063, loss = 1.35495657\n",
      "Iteration 58064, loss = 1.32497742\n",
      "Iteration 58065, loss = 1.52198575\n",
      "Iteration 58066, loss = 1.44879774\n",
      "Iteration 58067, loss = 1.46948506\n",
      "Iteration 58068, loss = 1.36107094\n",
      "Iteration 58069, loss = 1.40938284\n",
      "Iteration 58070, loss = 1.32603365\n",
      "Iteration 58071, loss = 1.29794742\n",
      "Iteration 58072, loss = 1.25038131\n",
      "Iteration 58073, loss = 1.29928017\n",
      "Iteration 58074, loss = 1.54769790\n",
      "Iteration 58075, loss = 1.38180580\n",
      "Iteration 58076, loss = 1.48877980\n",
      "Iteration 58077, loss = 1.56948337\n",
      "Iteration 58078, loss = 1.41824952\n",
      "Iteration 58079, loss = 1.65624098\n",
      "Iteration 58080, loss = 1.78140506\n",
      "Iteration 58081, loss = 1.87374316\n",
      "Iteration 58082, loss = 2.04124280\n",
      "Iteration 58083, loss = 1.69762028\n",
      "Iteration 58084, loss = 1.45129175\n",
      "Iteration 58085, loss = 1.22150351\n",
      "Iteration 58086, loss = 1.21713630\n",
      "Iteration 58087, loss = 1.35399803\n",
      "Iteration 58088, loss = 1.47170024\n",
      "Iteration 58089, loss = 1.30322929\n",
      "Iteration 58090, loss = 1.26514184\n",
      "Iteration 58091, loss = 1.49786665\n",
      "Iteration 58092, loss = 1.53421617\n",
      "Iteration 58093, loss = 1.73474834\n",
      "Iteration 58094, loss = 1.94234689\n",
      "Iteration 58095, loss = 1.72550187\n",
      "Iteration 58096, loss = 2.04573037\n",
      "Iteration 58097, loss = 1.94053007\n",
      "Iteration 58098, loss = 2.20018718\n",
      "Iteration 58099, loss = 1.54678910\n",
      "Iteration 58100, loss = 1.42738767\n",
      "Iteration 58101, loss = 1.45800542\n",
      "Iteration 58102, loss = 1.62130832\n",
      "Iteration 58103, loss = 2.06906931\n",
      "Iteration 58104, loss = 1.83253584\n",
      "Iteration 58105, loss = 1.64972376\n",
      "Iteration 58106, loss = 1.27104055\n",
      "Iteration 58107, loss = 1.62356302\n",
      "Iteration 58108, loss = 1.76952437\n",
      "Iteration 58109, loss = 1.81878779\n",
      "Iteration 58110, loss = 1.52793882\n",
      "Iteration 58111, loss = 1.64188478\n",
      "Iteration 58112, loss = 1.41952391\n",
      "Iteration 58113, loss = 1.65990724\n",
      "Iteration 58114, loss = 1.44859406\n",
      "Iteration 58115, loss = 1.37329666\n",
      "Iteration 58116, loss = 1.81508754\n",
      "Iteration 58117, loss = 1.67643143\n",
      "Iteration 58118, loss = 1.91548851\n",
      "Iteration 58119, loss = 1.70381885\n",
      "Iteration 58120, loss = 1.78912961\n",
      "Iteration 58121, loss = 1.46927290\n",
      "Iteration 58122, loss = 1.31572924\n",
      "Iteration 58123, loss = 1.59821163\n",
      "Iteration 58124, loss = 1.37452251\n",
      "Iteration 58125, loss = 1.45447241\n",
      "Iteration 58126, loss = 1.46079985\n",
      "Iteration 58127, loss = 1.62253767\n",
      "Iteration 58128, loss = 1.75451618\n",
      "Iteration 58129, loss = 1.62884842\n",
      "Iteration 58130, loss = 2.70784759\n",
      "Iteration 58131, loss = 1.98312932\n",
      "Iteration 58132, loss = 1.85357147\n",
      "Iteration 58133, loss = 1.50106151\n",
      "Iteration 58134, loss = 1.86886790\n",
      "Iteration 58135, loss = 1.79097665\n",
      "Iteration 58136, loss = 2.11565982\n",
      "Iteration 58137, loss = 2.40773011\n",
      "Iteration 58138, loss = 2.41666461\n",
      "Iteration 58139, loss = 2.05805785\n",
      "Iteration 58140, loss = 2.09554735\n",
      "Iteration 58141, loss = 1.87400793\n",
      "Iteration 58142, loss = 2.23234929\n",
      "Iteration 58143, loss = 2.14561853\n",
      "Iteration 58144, loss = 1.97005534\n",
      "Iteration 58145, loss = 1.77243208\n",
      "Iteration 58146, loss = 1.57391649\n",
      "Iteration 58147, loss = 1.66778663\n",
      "Iteration 58148, loss = 1.80430501\n",
      "Iteration 58149, loss = 1.68431242\n",
      "Iteration 58150, loss = 1.78910420\n",
      "Iteration 58151, loss = 1.53053038\n",
      "Iteration 58152, loss = 1.22956908\n",
      "Iteration 58153, loss = 1.43935670\n",
      "Iteration 58154, loss = 1.28282765\n",
      "Iteration 58155, loss = 1.30216071\n",
      "Iteration 58156, loss = 1.48020065\n",
      "Iteration 58157, loss = 1.41819386\n",
      "Iteration 58158, loss = 1.37030646\n",
      "Iteration 58159, loss = 1.92899043\n",
      "Iteration 58160, loss = 2.08334406\n",
      "Iteration 58161, loss = 2.20544867\n",
      "Iteration 58162, loss = 2.48817785\n",
      "Iteration 58163, loss = 2.08888689\n",
      "Iteration 58164, loss = 1.88713186\n",
      "Iteration 58165, loss = 1.44142924\n",
      "Iteration 58166, loss = 1.56544254\n",
      "Iteration 58167, loss = 1.37361175\n",
      "Iteration 58168, loss = 1.58136362\n",
      "Iteration 58169, loss = 1.38095223\n",
      "Iteration 58170, loss = 1.65080950\n",
      "Iteration 58171, loss = 1.62521024\n",
      "Iteration 58172, loss = 1.31827079\n",
      "Iteration 58173, loss = 1.28082969\n",
      "Iteration 58174, loss = 1.25050537\n",
      "Iteration 58175, loss = 1.21845039\n",
      "Iteration 58176, loss = 1.33482159\n",
      "Iteration 58177, loss = 1.32569385\n",
      "Iteration 58178, loss = 1.71628301\n",
      "Iteration 58179, loss = 1.76888198\n",
      "Iteration 58180, loss = 2.01145970\n",
      "Iteration 58181, loss = 1.73647451\n",
      "Iteration 58182, loss = 1.58446046\n",
      "Iteration 58183, loss = 1.83728304\n",
      "Iteration 58184, loss = 2.40805008\n",
      "Iteration 58185, loss = 1.81144205\n",
      "Iteration 58186, loss = 1.41050986\n",
      "Iteration 58187, loss = 1.39934377\n",
      "Iteration 58188, loss = 1.24847129\n",
      "Iteration 58189, loss = 1.61578326\n",
      "Iteration 58190, loss = 1.65088304\n",
      "Iteration 58191, loss = 1.42722216\n",
      "Iteration 58192, loss = 1.36261145\n",
      "Iteration 58193, loss = 1.41487867\n",
      "Iteration 58194, loss = 1.49644890\n",
      "Iteration 58195, loss = 1.31264994\n",
      "Iteration 58196, loss = 1.30032763\n",
      "Iteration 58197, loss = 1.34960968\n",
      "Iteration 58198, loss = 1.35263046\n",
      "Iteration 58199, loss = 1.49233042\n",
      "Iteration 58200, loss = 1.80106968\n",
      "Iteration 58201, loss = 1.66875774\n",
      "Iteration 58202, loss = 1.39538848\n",
      "Iteration 58203, loss = 1.49442063\n",
      "Iteration 58204, loss = 1.75105745\n",
      "Iteration 58205, loss = 1.71600153\n",
      "Iteration 58206, loss = 1.85373916\n",
      "Iteration 58207, loss = 2.92668126\n",
      "Iteration 58208, loss = 3.33672597\n",
      "Iteration 58209, loss = 2.57114454\n",
      "Iteration 58210, loss = 1.96584035\n",
      "Iteration 58211, loss = 1.40512313\n",
      "Iteration 58212, loss = 1.34073621\n",
      "Iteration 58213, loss = 1.48519459\n",
      "Iteration 58214, loss = 1.38753060\n",
      "Iteration 58215, loss = 1.33058791\n",
      "Iteration 58216, loss = 1.29023918\n",
      "Iteration 58217, loss = 1.30823330\n",
      "Iteration 58218, loss = 1.37844404\n",
      "Iteration 58219, loss = 1.37991577\n",
      "Iteration 58220, loss = 1.46370267\n",
      "Iteration 58221, loss = 1.30470004\n",
      "Iteration 58222, loss = 1.36246496\n",
      "Iteration 58223, loss = 1.41667209\n",
      "Iteration 58224, loss = 1.32594118\n",
      "Iteration 58225, loss = 1.29112865\n",
      "Iteration 58226, loss = 1.57117707\n",
      "Iteration 58227, loss = 1.37471357\n",
      "Iteration 58228, loss = 1.35322249\n",
      "Iteration 58229, loss = 1.42318646\n",
      "Iteration 58230, loss = 1.68787875\n",
      "Iteration 58231, loss = 1.52679910\n",
      "Iteration 58232, loss = 1.60782881\n",
      "Iteration 58233, loss = 1.48634388\n",
      "Iteration 58234, loss = 1.52279971\n",
      "Iteration 58235, loss = 1.39004862\n",
      "Iteration 58236, loss = 1.22608813\n",
      "Iteration 58237, loss = 1.51072040\n",
      "Iteration 58238, loss = 1.64074859\n",
      "Iteration 58239, loss = 1.57769465\n",
      "Iteration 58240, loss = 1.97361562\n",
      "Iteration 58241, loss = 1.48537661\n",
      "Iteration 58242, loss = 1.49586183\n",
      "Iteration 58243, loss = 1.40102356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 58244, loss = 1.37264770\n",
      "Iteration 58245, loss = 1.22694104\n",
      "Iteration 58246, loss = 1.34979789\n",
      "Iteration 58247, loss = 1.39546784\n",
      "Iteration 58248, loss = 1.28012392\n",
      "Iteration 58249, loss = 1.31737990\n",
      "Iteration 58250, loss = 1.37833950\n",
      "Iteration 58251, loss = 1.35666061\n",
      "Iteration 58252, loss = 1.28452977\n",
      "Iteration 58253, loss = 1.33100568\n",
      "Iteration 58254, loss = 1.97018208\n",
      "Iteration 58255, loss = 1.91725735\n",
      "Iteration 58256, loss = 1.65889407\n",
      "Iteration 58257, loss = 1.41676520\n",
      "Iteration 58258, loss = 1.43879450\n",
      "Iteration 58259, loss = 1.58231791\n",
      "Iteration 58260, loss = 1.76199580\n",
      "Iteration 58261, loss = 1.62039683\n",
      "Iteration 58262, loss = 1.62898087\n",
      "Iteration 58263, loss = 1.60137191\n",
      "Iteration 58264, loss = 1.47840748\n",
      "Iteration 58265, loss = 1.37212585\n",
      "Iteration 58266, loss = 1.64908191\n",
      "Iteration 58267, loss = 1.91490157\n",
      "Iteration 58268, loss = 2.63995680\n",
      "Iteration 58269, loss = 3.60137679\n",
      "Iteration 58270, loss = 2.90147339\n",
      "Iteration 58271, loss = 2.81470995\n",
      "Iteration 58272, loss = 2.83258600\n",
      "Iteration 58273, loss = 4.09998642\n",
      "Iteration 58274, loss = 5.53473113\n",
      "Iteration 58275, loss = 3.89096268\n",
      "Iteration 58276, loss = 2.30672676\n",
      "Iteration 58277, loss = 2.23923632\n",
      "Iteration 58278, loss = 2.78118723\n",
      "Iteration 58279, loss = 2.04943022\n",
      "Iteration 58280, loss = 2.27902355\n",
      "Iteration 58281, loss = 2.07789813\n",
      "Iteration 58282, loss = 1.74397915\n",
      "Iteration 58283, loss = 2.04833975\n",
      "Iteration 58284, loss = 1.68674553\n",
      "Iteration 58285, loss = 1.82059262\n",
      "Iteration 58286, loss = 1.51218510\n",
      "Iteration 58287, loss = 1.59930301\n",
      "Iteration 58288, loss = 1.33476213\n",
      "Iteration 58289, loss = 1.35491351\n",
      "Iteration 58290, loss = 1.39921154\n",
      "Iteration 58291, loss = 1.50053295\n",
      "Iteration 58292, loss = 2.41592495\n",
      "Iteration 58293, loss = 2.12543766\n",
      "Iteration 58294, loss = 1.85294353\n",
      "Iteration 58295, loss = 1.48119320\n",
      "Iteration 58296, loss = 1.41283797\n",
      "Iteration 58297, loss = 1.38588146\n",
      "Iteration 58298, loss = 1.43675207\n",
      "Iteration 58299, loss = 1.52532579\n",
      "Iteration 58300, loss = 1.65920508\n",
      "Iteration 58301, loss = 1.53066522\n",
      "Iteration 58302, loss = 1.52038008\n",
      "Iteration 58303, loss = 1.58372981\n",
      "Iteration 58304, loss = 1.41314385\n",
      "Iteration 58305, loss = 1.54397978\n",
      "Iteration 58306, loss = 1.92474814\n",
      "Iteration 58307, loss = 2.05388092\n",
      "Iteration 58308, loss = 1.94667724\n",
      "Iteration 58309, loss = 1.66254460\n",
      "Iteration 58310, loss = 1.55860786\n",
      "Iteration 58311, loss = 1.65011703\n",
      "Iteration 58312, loss = 1.26465639\n",
      "Iteration 58313, loss = 1.37628638\n",
      "Iteration 58314, loss = 1.32203844\n",
      "Iteration 58315, loss = 1.48289906\n",
      "Iteration 58316, loss = 1.44566079\n",
      "Iteration 58317, loss = 1.59098118\n",
      "Iteration 58318, loss = 1.42163467\n",
      "Iteration 58319, loss = 1.59659459\n",
      "Iteration 58320, loss = 1.57273712\n",
      "Iteration 58321, loss = 2.09912836\n",
      "Iteration 58322, loss = 1.82087920\n",
      "Iteration 58323, loss = 2.06778230\n",
      "Iteration 58324, loss = 2.07160439\n",
      "Iteration 58325, loss = 1.90050169\n",
      "Iteration 58326, loss = 1.67455284\n",
      "Iteration 58327, loss = 1.93518428\n",
      "Iteration 58328, loss = 1.46932412\n",
      "Iteration 58329, loss = 1.76275872\n",
      "Iteration 58330, loss = 2.23364680\n",
      "Iteration 58331, loss = 1.98342834\n",
      "Iteration 58332, loss = 1.77129314\n",
      "Iteration 58333, loss = 2.52078698\n",
      "Iteration 58334, loss = 2.13692373\n",
      "Iteration 58335, loss = 2.26003529\n",
      "Iteration 58336, loss = 2.21300502\n",
      "Iteration 58337, loss = 2.28360818\n",
      "Iteration 58338, loss = 1.76716467\n",
      "Iteration 58339, loss = 1.49604100\n",
      "Iteration 58340, loss = 1.77744362\n",
      "Iteration 58341, loss = 1.95739461\n",
      "Iteration 58342, loss = 3.58393782\n",
      "Iteration 58343, loss = 3.78717654\n",
      "Iteration 58344, loss = 2.69710086\n",
      "Iteration 58345, loss = 1.65726731\n",
      "Iteration 58346, loss = 2.34616696\n",
      "Iteration 58347, loss = 2.04644418\n",
      "Iteration 58348, loss = 2.46916853\n",
      "Iteration 58349, loss = 1.93691072\n",
      "Iteration 58350, loss = 1.99873221\n",
      "Iteration 58351, loss = 2.26614757\n",
      "Iteration 58352, loss = 2.10860467\n",
      "Iteration 58353, loss = 2.56495289\n",
      "Iteration 58354, loss = 2.89348418\n",
      "Iteration 58355, loss = 1.82927089\n",
      "Iteration 58356, loss = 1.66206830\n",
      "Iteration 58357, loss = 1.63997656\n",
      "Iteration 58358, loss = 1.46447481\n",
      "Iteration 58359, loss = 1.58901190\n",
      "Iteration 58360, loss = 1.67397066\n",
      "Iteration 58361, loss = 1.77988476\n",
      "Iteration 58362, loss = 1.86151910\n",
      "Iteration 58363, loss = 1.50706310\n",
      "Iteration 58364, loss = 1.70317827\n",
      "Iteration 58365, loss = 1.70583302\n",
      "Iteration 58366, loss = 1.38357628\n",
      "Iteration 58367, loss = 1.39612340\n",
      "Iteration 58368, loss = 1.22553171\n",
      "Iteration 58369, loss = 1.33915899\n",
      "Iteration 58370, loss = 1.46849569\n",
      "Iteration 58371, loss = 1.47306030\n",
      "Iteration 58372, loss = 1.39259875\n",
      "Iteration 58373, loss = 1.26419205\n",
      "Iteration 58374, loss = 1.18809713\n",
      "Iteration 58375, loss = 1.20063082\n",
      "Iteration 58376, loss = 1.18243617\n",
      "Iteration 58377, loss = 1.24232461\n",
      "Iteration 58378, loss = 1.29789450\n",
      "Iteration 58379, loss = 1.20661391\n",
      "Iteration 58380, loss = 1.56687615\n",
      "Iteration 58381, loss = 1.35341958\n",
      "Iteration 58382, loss = 1.35625723\n",
      "Iteration 58383, loss = 1.35561944\n",
      "Iteration 58384, loss = 1.32202864\n",
      "Iteration 58385, loss = 1.26472329\n",
      "Iteration 58386, loss = 1.43847278\n",
      "Iteration 58387, loss = 1.40969878\n",
      "Iteration 58388, loss = 1.52120797\n",
      "Iteration 58389, loss = 1.59765552\n",
      "Iteration 58390, loss = 1.60389785\n",
      "Iteration 58391, loss = 1.63531851\n",
      "Iteration 58392, loss = 1.55553009\n",
      "Iteration 58393, loss = 1.43508186\n",
      "Iteration 58394, loss = 1.24708758\n",
      "Iteration 58395, loss = 1.54094210\n",
      "Iteration 58396, loss = 1.85199998\n",
      "Iteration 58397, loss = 1.77702299\n",
      "Iteration 58398, loss = 1.59394994\n",
      "Iteration 58399, loss = 1.71423367\n",
      "Iteration 58400, loss = 1.65032423\n",
      "Iteration 58401, loss = 1.64844053\n",
      "Iteration 58402, loss = 1.46058544\n",
      "Iteration 58403, loss = 1.39516178\n",
      "Iteration 58404, loss = 1.25237423\n",
      "Iteration 58405, loss = 1.36659442\n",
      "Iteration 58406, loss = 1.32658085\n",
      "Iteration 58407, loss = 1.24264881\n",
      "Iteration 58408, loss = 1.21635624\n",
      "Iteration 58409, loss = 1.28135776\n",
      "Iteration 58410, loss = 1.31619887\n",
      "Iteration 58411, loss = 1.40708533\n",
      "Iteration 58412, loss = 1.34971508\n",
      "Iteration 58413, loss = 1.67174923\n",
      "Iteration 58414, loss = 1.84819721\n",
      "Iteration 58415, loss = 2.23264968\n",
      "Iteration 58416, loss = 1.61929061\n",
      "Iteration 58417, loss = 1.50914016\n",
      "Iteration 58418, loss = 1.62749087\n",
      "Iteration 58419, loss = 1.61014621\n",
      "Iteration 58420, loss = 1.44669687\n",
      "Iteration 58421, loss = 1.19905018\n",
      "Iteration 58422, loss = 1.23380729\n",
      "Iteration 58423, loss = 1.29568241\n",
      "Iteration 58424, loss = 1.34322934\n",
      "Iteration 58425, loss = 1.40921298\n",
      "Iteration 58426, loss = 1.34124786\n",
      "Iteration 58427, loss = 1.30887343\n",
      "Iteration 58428, loss = 1.33953408\n",
      "Iteration 58429, loss = 1.63043635\n",
      "Iteration 58430, loss = 1.62810775\n",
      "Iteration 58431, loss = 1.45931597\n",
      "Iteration 58432, loss = 1.42765260\n",
      "Iteration 58433, loss = 1.50373327\n",
      "Iteration 58434, loss = 1.88835885\n",
      "Iteration 58435, loss = 1.78075598\n",
      "Iteration 58436, loss = 1.54324049\n",
      "Iteration 58437, loss = 1.34548725\n",
      "Iteration 58438, loss = 1.41858390\n",
      "Iteration 58439, loss = 1.55703928\n",
      "Iteration 58440, loss = 1.38244847\n",
      "Iteration 58441, loss = 1.36522690\n",
      "Iteration 58442, loss = 1.35551084\n",
      "Iteration 58443, loss = 1.72621301\n",
      "Iteration 58444, loss = 1.75010083\n",
      "Iteration 58445, loss = 2.33671296\n",
      "Iteration 58446, loss = 2.28944747\n",
      "Iteration 58447, loss = 1.74705938\n",
      "Iteration 58448, loss = 1.55001054\n",
      "Iteration 58449, loss = 1.61483599\n",
      "Iteration 58450, loss = 1.87255487\n",
      "Iteration 58451, loss = 1.67684242\n",
      "Iteration 58452, loss = 1.70694296\n",
      "Iteration 58453, loss = 1.64313501\n",
      "Iteration 58454, loss = 1.59508513\n",
      "Iteration 58455, loss = 1.55465022\n",
      "Iteration 58456, loss = 1.57818124\n",
      "Iteration 58457, loss = 1.41666865\n",
      "Iteration 58458, loss = 1.68365231\n",
      "Iteration 58459, loss = 1.63190854\n",
      "Iteration 58460, loss = 1.58558321\n",
      "Iteration 58461, loss = 1.55746980\n",
      "Iteration 58462, loss = 1.76175540\n",
      "Iteration 58463, loss = 1.70967670\n",
      "Iteration 58464, loss = 1.68347774\n",
      "Iteration 58465, loss = 1.46542753\n",
      "Iteration 58466, loss = 1.32017193\n",
      "Iteration 58467, loss = 1.35587149\n",
      "Iteration 58468, loss = 1.69525064\n",
      "Iteration 58469, loss = 1.73521107\n",
      "Iteration 58470, loss = 1.55865875\n",
      "Iteration 58471, loss = 1.65507638\n",
      "Iteration 58472, loss = 1.61361478\n",
      "Iteration 58473, loss = 1.48944891\n",
      "Iteration 58474, loss = 1.29744454\n",
      "Iteration 58475, loss = 1.36108345\n",
      "Iteration 58476, loss = 1.56141826\n",
      "Iteration 58477, loss = 1.59802363\n",
      "Iteration 58478, loss = 1.38138934\n",
      "Iteration 58479, loss = 1.30999588\n",
      "Iteration 58480, loss = 1.47583419\n",
      "Iteration 58481, loss = 1.69373691\n",
      "Iteration 58482, loss = 1.47007812\n",
      "Iteration 58483, loss = 1.46363775\n",
      "Iteration 58484, loss = 1.28338127\n",
      "Iteration 58485, loss = 1.39977519\n",
      "Iteration 58486, loss = 1.32622248\n",
      "Iteration 58487, loss = 1.49429307\n",
      "Iteration 58488, loss = 1.32799078\n",
      "Iteration 58489, loss = 1.30919184\n",
      "Iteration 58490, loss = 1.35802473\n",
      "Iteration 58491, loss = 1.37954635\n",
      "Iteration 58492, loss = 1.47285812\n",
      "Iteration 58493, loss = 1.40617474\n",
      "Iteration 58494, loss = 1.50007691\n",
      "Iteration 58495, loss = 1.42280929\n",
      "Iteration 58496, loss = 1.58745147\n",
      "Iteration 58497, loss = 1.79955482\n",
      "Iteration 58498, loss = 2.14608682\n",
      "Iteration 58499, loss = 1.91675467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 58500, loss = 1.88505680\n",
      "Iteration 58501, loss = 1.47072358\n",
      "Iteration 58502, loss = 1.33799980\n",
      "Iteration 58503, loss = 1.48331841\n",
      "Iteration 58504, loss = 1.49059640\n",
      "Iteration 58505, loss = 1.59295080\n",
      "Iteration 58506, loss = 1.45128690\n",
      "Iteration 58507, loss = 1.76320672\n",
      "Iteration 58508, loss = 1.49852051\n",
      "Iteration 58509, loss = 1.42572382\n",
      "Iteration 58510, loss = 1.44782642\n",
      "Iteration 58511, loss = 1.43271954\n",
      "Iteration 58512, loss = 1.36865431\n",
      "Iteration 58513, loss = 1.50168087\n",
      "Iteration 58514, loss = 2.02151474\n",
      "Iteration 58515, loss = 1.61699861\n",
      "Iteration 58516, loss = 1.85088900\n",
      "Iteration 58517, loss = 1.93502198\n",
      "Iteration 58518, loss = 2.72486035\n",
      "Iteration 58519, loss = 2.75087843\n",
      "Iteration 58520, loss = 2.75197079\n",
      "Iteration 58521, loss = 1.99631004\n",
      "Iteration 58522, loss = 2.53289875\n",
      "Iteration 58523, loss = 2.08603523\n",
      "Iteration 58524, loss = 2.19521148\n",
      "Iteration 58525, loss = 1.81504946\n",
      "Iteration 58526, loss = 1.71210799\n",
      "Iteration 58527, loss = 1.52491030\n",
      "Iteration 58528, loss = 1.56941529\n",
      "Iteration 58529, loss = 1.28688318\n",
      "Iteration 58530, loss = 1.43629349\n",
      "Iteration 58531, loss = 1.24205753\n",
      "Iteration 58532, loss = 1.28388454\n",
      "Iteration 58533, loss = 1.47595540\n",
      "Iteration 58534, loss = 1.75012534\n",
      "Iteration 58535, loss = 1.56662198\n",
      "Iteration 58536, loss = 1.55496168\n",
      "Iteration 58537, loss = 1.44165110\n",
      "Iteration 58538, loss = 1.65469304\n",
      "Iteration 58539, loss = 2.38931095\n",
      "Iteration 58540, loss = 2.35883470\n",
      "Iteration 58541, loss = 2.32523990\n",
      "Iteration 58542, loss = 2.91771291\n",
      "Iteration 58543, loss = 1.85530841\n",
      "Iteration 58544, loss = 2.53295915\n",
      "Iteration 58545, loss = 2.24913507\n",
      "Iteration 58546, loss = 2.01126365\n",
      "Iteration 58547, loss = 2.16122960\n",
      "Iteration 58548, loss = 2.66358436\n",
      "Iteration 58549, loss = 2.80079121\n",
      "Iteration 58550, loss = 1.97003079\n",
      "Iteration 58551, loss = 1.73140488\n",
      "Iteration 58552, loss = 1.83387042\n",
      "Iteration 58553, loss = 2.15920794\n",
      "Iteration 58554, loss = 2.16934509\n",
      "Iteration 58555, loss = 1.79865475\n",
      "Iteration 58556, loss = 1.70825796\n",
      "Iteration 58557, loss = 1.31421406\n",
      "Iteration 58558, loss = 1.68071724\n",
      "Iteration 58559, loss = 1.29581868\n",
      "Iteration 58560, loss = 1.34218143\n",
      "Iteration 58561, loss = 1.37528981\n",
      "Iteration 58562, loss = 1.50458100\n",
      "Iteration 58563, loss = 1.73365834\n",
      "Iteration 58564, loss = 1.61339842\n",
      "Iteration 58565, loss = 1.54245602\n",
      "Iteration 58566, loss = 1.38306428\n",
      "Iteration 58567, loss = 1.66529594\n",
      "Iteration 58568, loss = 1.58579061\n",
      "Iteration 58569, loss = 1.34127591\n",
      "Iteration 58570, loss = 1.21289291\n",
      "Iteration 58571, loss = 1.22614332\n",
      "Iteration 58572, loss = 1.27265038\n",
      "Iteration 58573, loss = 1.33239267\n",
      "Iteration 58574, loss = 1.38414801\n",
      "Iteration 58575, loss = 1.33390570\n",
      "Iteration 58576, loss = 1.46559764\n",
      "Iteration 58577, loss = 1.63305531\n",
      "Iteration 58578, loss = 1.70323628\n",
      "Iteration 58579, loss = 1.87922240\n",
      "Iteration 58580, loss = 1.50864289\n",
      "Iteration 58581, loss = 1.78773822\n",
      "Iteration 58582, loss = 1.99457320\n",
      "Iteration 58583, loss = 1.57971731\n",
      "Iteration 58584, loss = 1.60807993\n",
      "Iteration 58585, loss = 1.49188582\n",
      "Iteration 58586, loss = 1.88318285\n",
      "Iteration 58587, loss = 1.73456549\n",
      "Iteration 58588, loss = 1.75800061\n",
      "Iteration 58589, loss = 1.68269775\n",
      "Iteration 58590, loss = 1.55823176\n",
      "Iteration 58591, loss = 1.59917198\n",
      "Iteration 58592, loss = 1.74960518\n",
      "Iteration 58593, loss = 1.35366792\n",
      "Iteration 58594, loss = 1.35269520\n",
      "Iteration 58595, loss = 1.31695337\n",
      "Iteration 58596, loss = 1.30835106\n",
      "Iteration 58597, loss = 1.32549064\n",
      "Iteration 58598, loss = 1.35121339\n",
      "Iteration 58599, loss = 1.48252257\n",
      "Iteration 58600, loss = 1.22242365\n",
      "Iteration 58601, loss = 1.19199876\n",
      "Iteration 58602, loss = 1.28642976\n",
      "Iteration 58603, loss = 1.29028334\n",
      "Iteration 58604, loss = 1.58793152\n",
      "Iteration 58605, loss = 1.84930414\n",
      "Iteration 58606, loss = 1.99004844\n",
      "Iteration 58607, loss = 2.18707931\n",
      "Iteration 58608, loss = 1.56622067\n",
      "Iteration 58609, loss = 1.50607605\n",
      "Iteration 58610, loss = 1.54632745\n",
      "Iteration 58611, loss = 1.55748663\n",
      "Iteration 58612, loss = 1.38734385\n",
      "Iteration 58613, loss = 1.49089268\n",
      "Iteration 58614, loss = 1.55362921\n",
      "Iteration 58615, loss = 1.76500001\n",
      "Iteration 58616, loss = 1.56823019\n",
      "Iteration 58617, loss = 1.86280868\n",
      "Iteration 58618, loss = 2.39935360\n",
      "Iteration 58619, loss = 2.19133814\n",
      "Iteration 58620, loss = 2.15681960\n",
      "Iteration 58621, loss = 1.80789984\n",
      "Iteration 58622, loss = 2.06210249\n",
      "Iteration 58623, loss = 2.42824039\n",
      "Iteration 58624, loss = 1.76146515\n",
      "Iteration 58625, loss = 1.37643975\n",
      "Iteration 58626, loss = 1.59348116\n",
      "Iteration 58627, loss = 1.52447840\n",
      "Iteration 58628, loss = 1.63968136\n",
      "Iteration 58629, loss = 1.51321846\n",
      "Iteration 58630, loss = 1.66023507\n",
      "Iteration 58631, loss = 1.66902151\n",
      "Iteration 58632, loss = 1.67221331\n",
      "Iteration 58633, loss = 1.84217296\n",
      "Iteration 58634, loss = 1.77678120\n",
      "Iteration 58635, loss = 1.66785438\n",
      "Iteration 58636, loss = 1.54858542\n",
      "Iteration 58637, loss = 1.63231424\n",
      "Iteration 58638, loss = 2.14946785\n",
      "Iteration 58639, loss = 1.56372687\n",
      "Iteration 58640, loss = 2.04901504\n",
      "Iteration 58641, loss = 2.05971688\n",
      "Iteration 58642, loss = 1.90068782\n",
      "Iteration 58643, loss = 1.99855362\n",
      "Iteration 58644, loss = 2.12735480\n",
      "Iteration 58645, loss = 2.28342870\n",
      "Iteration 58646, loss = 1.59468741\n",
      "Iteration 58647, loss = 1.76620017\n",
      "Iteration 58648, loss = 1.66918437\n",
      "Iteration 58649, loss = 1.47862537\n",
      "Iteration 58650, loss = 1.63554814\n",
      "Iteration 58651, loss = 1.44527313\n",
      "Iteration 58652, loss = 1.43084737\n",
      "Iteration 58653, loss = 1.44407397\n",
      "Iteration 58654, loss = 1.34974911\n",
      "Iteration 58655, loss = 1.32552654\n",
      "Iteration 58656, loss = 1.37648142\n",
      "Iteration 58657, loss = 1.35513246\n",
      "Iteration 58658, loss = 1.43653447\n",
      "Iteration 58659, loss = 1.72603577\n",
      "Iteration 58660, loss = 1.96214397\n",
      "Iteration 58661, loss = 1.82868607\n",
      "Iteration 58662, loss = 1.66867252\n",
      "Iteration 58663, loss = 2.01138257\n",
      "Iteration 58664, loss = 2.02559014\n",
      "Iteration 58665, loss = 1.55653848\n",
      "Iteration 58666, loss = 1.64564642\n",
      "Iteration 58667, loss = 1.69337049\n",
      "Iteration 58668, loss = 1.58801105\n",
      "Iteration 58669, loss = 1.63355445\n",
      "Iteration 58670, loss = 1.65985453\n",
      "Iteration 58671, loss = 1.68827001\n",
      "Iteration 58672, loss = 2.36143950\n",
      "Iteration 58673, loss = 2.79479910\n",
      "Iteration 58674, loss = 1.63783814\n",
      "Iteration 58675, loss = 1.77575071\n",
      "Iteration 58676, loss = 2.09926121\n",
      "Iteration 58677, loss = 1.77459756\n",
      "Iteration 58678, loss = 1.47797074\n",
      "Iteration 58679, loss = 1.54138814\n",
      "Iteration 58680, loss = 1.44873635\n",
      "Iteration 58681, loss = 1.48950346\n",
      "Iteration 58682, loss = 1.42210861\n",
      "Iteration 58683, loss = 1.45941450\n",
      "Iteration 58684, loss = 1.38639278\n",
      "Iteration 58685, loss = 1.24178385\n",
      "Iteration 58686, loss = 1.35881390\n",
      "Iteration 58687, loss = 1.46342429\n",
      "Iteration 58688, loss = 1.64678933\n",
      "Iteration 58689, loss = 1.65554764\n",
      "Iteration 58690, loss = 1.76904958\n",
      "Iteration 58691, loss = 1.57400235\n",
      "Iteration 58692, loss = 1.83923181\n",
      "Iteration 58693, loss = 1.47012939\n",
      "Iteration 58694, loss = 1.38947628\n",
      "Iteration 58695, loss = 1.49633882\n",
      "Iteration 58696, loss = 1.75140788\n",
      "Iteration 58697, loss = 2.11249355\n",
      "Iteration 58698, loss = 1.89486507\n",
      "Iteration 58699, loss = 1.81489073\n",
      "Iteration 58700, loss = 1.65404105\n",
      "Iteration 58701, loss = 1.99066176\n",
      "Iteration 58702, loss = 2.01778581\n",
      "Iteration 58703, loss = 2.80175322\n",
      "Iteration 58704, loss = 2.45994176\n",
      "Iteration 58705, loss = 1.68686409\n",
      "Iteration 58706, loss = 1.57048426\n",
      "Iteration 58707, loss = 1.49749260\n",
      "Iteration 58708, loss = 1.70678766\n",
      "Iteration 58709, loss = 1.52285658\n",
      "Iteration 58710, loss = 1.35875277\n",
      "Iteration 58711, loss = 1.38082883\n",
      "Iteration 58712, loss = 1.56738459\n",
      "Iteration 58713, loss = 1.93317467\n",
      "Iteration 58714, loss = 1.69706675\n",
      "Iteration 58715, loss = 1.52057153\n",
      "Iteration 58716, loss = 1.71438349\n",
      "Iteration 58717, loss = 1.77140369\n",
      "Iteration 58718, loss = 1.38287800\n",
      "Iteration 58719, loss = 1.32110705\n",
      "Iteration 58720, loss = 1.45563706\n",
      "Iteration 58721, loss = 1.42004712\n",
      "Iteration 58722, loss = 1.30649890\n",
      "Iteration 58723, loss = 1.48971202\n",
      "Iteration 58724, loss = 2.11638478\n",
      "Iteration 58725, loss = 2.24138412\n",
      "Iteration 58726, loss = 2.85044365\n",
      "Iteration 58727, loss = 2.37997565\n",
      "Iteration 58728, loss = 2.67808842\n",
      "Iteration 58729, loss = 2.82564489\n",
      "Iteration 58730, loss = 1.91228445\n",
      "Iteration 58731, loss = 2.31288847\n",
      "Iteration 58732, loss = 2.18903998\n",
      "Iteration 58733, loss = 1.78704204\n",
      "Iteration 58734, loss = 2.02590775\n",
      "Iteration 58735, loss = 2.94929084\n",
      "Iteration 58736, loss = 3.00498696\n",
      "Iteration 58737, loss = 2.98785771\n",
      "Iteration 58738, loss = 2.58465222\n",
      "Iteration 58739, loss = 1.90755624\n",
      "Iteration 58740, loss = 1.97031823\n",
      "Iteration 58741, loss = 1.59386405\n",
      "Iteration 58742, loss = 1.50446314\n",
      "Iteration 58743, loss = 1.30772566\n",
      "Iteration 58744, loss = 1.37120168\n",
      "Iteration 58745, loss = 1.47579143\n",
      "Iteration 58746, loss = 1.75621652\n",
      "Iteration 58747, loss = 1.67407761\n",
      "Iteration 58748, loss = 1.57118555\n",
      "Iteration 58749, loss = 1.57440074\n",
      "Iteration 58750, loss = 1.40951800\n",
      "Iteration 58751, loss = 1.43928619\n",
      "Iteration 58752, loss = 1.47650938\n",
      "Iteration 58753, loss = 1.46102786\n",
      "Iteration 58754, loss = 1.30090087\n",
      "Iteration 58755, loss = 1.29256939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 58756, loss = 1.27539748\n",
      "Iteration 58757, loss = 1.62015565\n",
      "Iteration 58758, loss = 2.15394937\n",
      "Iteration 58759, loss = 2.05610805\n",
      "Iteration 58760, loss = 1.78283324\n",
      "Iteration 58761, loss = 1.42549937\n",
      "Iteration 58762, loss = 1.53200165\n",
      "Iteration 58763, loss = 1.74099763\n",
      "Iteration 58764, loss = 1.97443721\n",
      "Iteration 58765, loss = 2.07071050\n",
      "Iteration 58766, loss = 1.81955359\n",
      "Iteration 58767, loss = 1.98501201\n",
      "Iteration 58768, loss = 1.99443360\n",
      "Iteration 58769, loss = 1.92363663\n",
      "Iteration 58770, loss = 2.00086113\n",
      "Iteration 58771, loss = 1.81654830\n",
      "Iteration 58772, loss = 1.34258805\n",
      "Iteration 58773, loss = 1.67142191\n",
      "Iteration 58774, loss = 1.55566251\n",
      "Iteration 58775, loss = 1.98625035\n",
      "Iteration 58776, loss = 1.97661339\n",
      "Iteration 58777, loss = 2.17445470\n",
      "Iteration 58778, loss = 2.44763882\n",
      "Iteration 58779, loss = 2.60344388\n",
      "Iteration 58780, loss = 2.44336535\n",
      "Iteration 58781, loss = 1.94626938\n",
      "Iteration 58782, loss = 2.03339403\n",
      "Iteration 58783, loss = 1.95734957\n",
      "Iteration 58784, loss = 2.07729549\n",
      "Iteration 58785, loss = 1.52963700\n",
      "Iteration 58786, loss = 1.29086208\n",
      "Iteration 58787, loss = 1.38443304\n",
      "Iteration 58788, loss = 1.46711949\n",
      "Iteration 58789, loss = 1.25486424\n",
      "Iteration 58790, loss = 1.34340467\n",
      "Iteration 58791, loss = 1.30176921\n",
      "Iteration 58792, loss = 1.48440329\n",
      "Iteration 58793, loss = 1.64830512\n",
      "Iteration 58794, loss = 1.78828018\n",
      "Iteration 58795, loss = 1.88507482\n",
      "Iteration 58796, loss = 2.11046154\n",
      "Iteration 58797, loss = 2.00813780\n",
      "Iteration 58798, loss = 2.21285263\n",
      "Iteration 58799, loss = 1.48279018\n",
      "Iteration 58800, loss = 1.50282162\n",
      "Iteration 58801, loss = 1.67317693\n",
      "Iteration 58802, loss = 1.54150876\n",
      "Iteration 58803, loss = 1.45831922\n",
      "Iteration 58804, loss = 1.26715016\n",
      "Iteration 58805, loss = 1.33708324\n",
      "Iteration 58806, loss = 1.40170296\n",
      "Iteration 58807, loss = 1.30540183\n",
      "Iteration 58808, loss = 1.23289715\n",
      "Iteration 58809, loss = 1.27759670\n",
      "Iteration 58810, loss = 1.45378147\n",
      "Iteration 58811, loss = 1.32256459\n",
      "Iteration 58812, loss = 1.31601112\n",
      "Iteration 58813, loss = 1.24770110\n",
      "Iteration 58814, loss = 1.23588663\n",
      "Iteration 58815, loss = 1.65246639\n",
      "Iteration 58816, loss = 1.37282112\n",
      "Iteration 58817, loss = 1.33057537\n",
      "Iteration 58818, loss = 1.54943386\n",
      "Iteration 58819, loss = 1.54028542\n",
      "Iteration 58820, loss = 1.69112253\n",
      "Iteration 58821, loss = 1.38994057\n",
      "Iteration 58822, loss = 1.79784380\n",
      "Iteration 58823, loss = 1.29721876\n",
      "Iteration 58824, loss = 1.28776099\n",
      "Iteration 58825, loss = 1.30830776\n",
      "Iteration 58826, loss = 1.26089507\n",
      "Iteration 58827, loss = 1.50918735\n",
      "Iteration 58828, loss = 1.40180150\n",
      "Iteration 58829, loss = 1.33527423\n",
      "Iteration 58830, loss = 1.28067019\n",
      "Iteration 58831, loss = 1.33037807\n",
      "Iteration 58832, loss = 1.33093807\n",
      "Iteration 58833, loss = 1.62886574\n",
      "Iteration 58834, loss = 1.86665340\n",
      "Iteration 58835, loss = 2.12934647\n",
      "Iteration 58836, loss = 1.83076809\n",
      "Iteration 58837, loss = 1.87568256\n",
      "Iteration 58838, loss = 2.20437214\n",
      "Iteration 58839, loss = 2.42374457\n",
      "Iteration 58840, loss = 1.63089550\n",
      "Iteration 58841, loss = 1.73021405\n",
      "Iteration 58842, loss = 1.69906685\n",
      "Iteration 58843, loss = 1.77098589\n",
      "Iteration 58844, loss = 2.50124615\n",
      "Iteration 58845, loss = 2.66645558\n",
      "Iteration 58846, loss = 1.63212684\n",
      "Iteration 58847, loss = 1.63180133\n",
      "Iteration 58848, loss = 1.55432395\n",
      "Iteration 58849, loss = 1.79175038\n",
      "Iteration 58850, loss = 1.46543217\n",
      "Iteration 58851, loss = 1.24557631\n",
      "Iteration 58852, loss = 1.46406196\n",
      "Iteration 58853, loss = 1.55002638\n",
      "Iteration 58854, loss = 1.32365835\n",
      "Iteration 58855, loss = 1.35190930\n",
      "Iteration 58856, loss = 1.25265146\n",
      "Iteration 58857, loss = 1.24760276\n",
      "Iteration 58858, loss = 1.22822759\n",
      "Iteration 58859, loss = 1.51002981\n",
      "Iteration 58860, loss = 1.45409348\n",
      "Iteration 58861, loss = 1.40843089\n",
      "Iteration 58862, loss = 1.45803710\n",
      "Iteration 58863, loss = 1.37399881\n",
      "Iteration 58864, loss = 1.34919853\n",
      "Iteration 58865, loss = 1.30696967\n",
      "Iteration 58866, loss = 1.26506760\n",
      "Iteration 58867, loss = 1.40536600\n",
      "Iteration 58868, loss = 1.55605769\n",
      "Iteration 58869, loss = 1.76371253\n",
      "Iteration 58870, loss = 1.58419760\n",
      "Iteration 58871, loss = 1.51258986\n",
      "Iteration 58872, loss = 1.30102502\n",
      "Iteration 58873, loss = 1.46035697\n",
      "Iteration 58874, loss = 1.63943355\n",
      "Iteration 58875, loss = 1.97669559\n",
      "Iteration 58876, loss = 2.37731956\n",
      "Iteration 58877, loss = 2.17977183\n",
      "Iteration 58878, loss = 2.34891380\n",
      "Iteration 58879, loss = 2.01262838\n",
      "Iteration 58880, loss = 1.47194402\n",
      "Iteration 58881, loss = 1.64163793\n",
      "Iteration 58882, loss = 1.61993889\n",
      "Iteration 58883, loss = 1.85110464\n",
      "Iteration 58884, loss = 1.76419097\n",
      "Iteration 58885, loss = 1.70747637\n",
      "Iteration 58886, loss = 1.98489377\n",
      "Iteration 58887, loss = 1.82171227\n",
      "Iteration 58888, loss = 1.43466854\n",
      "Iteration 58889, loss = 1.46470694\n",
      "Iteration 58890, loss = 1.42935774\n",
      "Iteration 58891, loss = 1.24972387\n",
      "Iteration 58892, loss = 1.35814585\n",
      "Iteration 58893, loss = 1.23790563\n",
      "Iteration 58894, loss = 1.43649266\n",
      "Iteration 58895, loss = 1.51564236\n",
      "Iteration 58896, loss = 1.45613563\n",
      "Iteration 58897, loss = 1.70456647\n",
      "Iteration 58898, loss = 2.07456246\n",
      "Iteration 58899, loss = 2.20704320\n",
      "Iteration 58900, loss = 2.24143235\n",
      "Iteration 58901, loss = 1.99333020\n",
      "Iteration 58902, loss = 1.62669843\n",
      "Iteration 58903, loss = 1.48034278\n",
      "Iteration 58904, loss = 1.51777912\n",
      "Iteration 58905, loss = 1.68211545\n",
      "Iteration 58906, loss = 1.63803361\n",
      "Iteration 58907, loss = 1.47999925\n",
      "Iteration 58908, loss = 1.49416190\n",
      "Iteration 58909, loss = 1.52347643\n",
      "Iteration 58910, loss = 1.55899626\n",
      "Iteration 58911, loss = 1.88132874\n",
      "Iteration 58912, loss = 1.45845921\n",
      "Iteration 58913, loss = 1.70070969\n",
      "Iteration 58914, loss = 1.59280784\n",
      "Iteration 58915, loss = 2.02489847\n",
      "Iteration 58916, loss = 2.40840355\n",
      "Iteration 58917, loss = 3.12850100\n",
      "Iteration 58918, loss = 2.30069226\n",
      "Iteration 58919, loss = 1.65083787\n",
      "Iteration 58920, loss = 1.59067417\n",
      "Iteration 58921, loss = 1.98248331\n",
      "Iteration 58922, loss = 1.66057378\n",
      "Iteration 58923, loss = 1.45339254\n",
      "Iteration 58924, loss = 1.31862588\n",
      "Iteration 58925, loss = 1.29278455\n",
      "Iteration 58926, loss = 1.55400568\n",
      "Iteration 58927, loss = 1.72089777\n",
      "Iteration 58928, loss = 1.61682831\n",
      "Iteration 58929, loss = 1.38416031\n",
      "Iteration 58930, loss = 1.63072882\n",
      "Iteration 58931, loss = 1.74321914\n",
      "Iteration 58932, loss = 1.47936828\n",
      "Iteration 58933, loss = 1.63443433\n",
      "Iteration 58934, loss = 1.69201475\n",
      "Iteration 58935, loss = 1.33666727\n",
      "Iteration 58936, loss = 1.29204775\n",
      "Iteration 58937, loss = 1.41515049\n",
      "Iteration 58938, loss = 1.30250743\n",
      "Iteration 58939, loss = 1.33079759\n",
      "Iteration 58940, loss = 1.35486498\n",
      "Iteration 58941, loss = 1.32536974\n",
      "Iteration 58942, loss = 1.80245135\n",
      "Iteration 58943, loss = 1.75213143\n",
      "Iteration 58944, loss = 1.73437235\n",
      "Iteration 58945, loss = 1.67261777\n",
      "Iteration 58946, loss = 1.62807848\n",
      "Iteration 58947, loss = 1.52276808\n",
      "Iteration 58948, loss = 1.52444335\n",
      "Iteration 58949, loss = 1.62699030\n",
      "Iteration 58950, loss = 1.42359076\n",
      "Iteration 58951, loss = 1.74328460\n",
      "Iteration 58952, loss = 1.67032034\n",
      "Iteration 58953, loss = 1.57943815\n",
      "Iteration 58954, loss = 1.71738558\n",
      "Iteration 58955, loss = 1.74758809\n",
      "Iteration 58956, loss = 1.76876168\n",
      "Iteration 58957, loss = 1.59498779\n",
      "Iteration 58958, loss = 1.61697926\n",
      "Iteration 58959, loss = 1.73830799\n",
      "Iteration 58960, loss = 1.80262592\n",
      "Iteration 58961, loss = 1.51144295\n",
      "Iteration 58962, loss = 1.42870246\n",
      "Iteration 58963, loss = 1.49148099\n",
      "Iteration 58964, loss = 1.42798192\n",
      "Iteration 58965, loss = 1.30411033\n",
      "Iteration 58966, loss = 1.80918983\n",
      "Iteration 58967, loss = 2.28131757\n",
      "Iteration 58968, loss = 3.16330168\n",
      "Iteration 58969, loss = 2.31074031\n",
      "Iteration 58970, loss = 1.97830633\n",
      "Iteration 58971, loss = 1.84638980\n",
      "Iteration 58972, loss = 1.45955761\n",
      "Iteration 58973, loss = 1.30201508\n",
      "Iteration 58974, loss = 1.20724431\n",
      "Iteration 58975, loss = 1.26024060\n",
      "Iteration 58976, loss = 1.52752094\n",
      "Iteration 58977, loss = 2.15401401\n",
      "Iteration 58978, loss = 2.62133159\n",
      "Iteration 58979, loss = 3.07272347\n",
      "Iteration 58980, loss = 2.11629452\n",
      "Iteration 58981, loss = 2.27460362\n",
      "Iteration 58982, loss = 1.82868636\n",
      "Iteration 58983, loss = 1.61026783\n",
      "Iteration 58984, loss = 1.68337950\n",
      "Iteration 58985, loss = 1.86557171\n",
      "Iteration 58986, loss = 1.76285612\n",
      "Iteration 58987, loss = 1.36495610\n",
      "Iteration 58988, loss = 1.63791716\n",
      "Iteration 58989, loss = 1.66958138\n",
      "Iteration 58990, loss = 1.75733670\n",
      "Iteration 58991, loss = 1.94196214\n",
      "Iteration 58992, loss = 2.69982930\n",
      "Iteration 58993, loss = 2.60621141\n",
      "Iteration 58994, loss = 2.34075483\n",
      "Iteration 58995, loss = 2.37620159\n",
      "Iteration 58996, loss = 2.48177323\n",
      "Iteration 58997, loss = 2.30058459\n",
      "Iteration 58998, loss = 1.99995978\n",
      "Iteration 58999, loss = 1.71662325\n",
      "Iteration 59000, loss = 1.56826266\n",
      "Iteration 59001, loss = 1.59862779\n",
      "Iteration 59002, loss = 1.66924001\n",
      "Iteration 59003, loss = 1.67286812\n",
      "Iteration 59004, loss = 1.45508231\n",
      "Iteration 59005, loss = 1.49196958\n",
      "Iteration 59006, loss = 1.45413799\n",
      "Iteration 59007, loss = 1.36192708\n",
      "Iteration 59008, loss = 1.35349687\n",
      "Iteration 59009, loss = 1.50090814\n",
      "Iteration 59010, loss = 1.33775323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 59011, loss = 1.58271716\n",
      "Iteration 59012, loss = 1.48211769\n",
      "Iteration 59013, loss = 1.48377736\n",
      "Iteration 59014, loss = 1.46441991\n",
      "Iteration 59015, loss = 1.52706667\n",
      "Iteration 59016, loss = 1.24350027\n",
      "Iteration 59017, loss = 1.35539517\n",
      "Iteration 59018, loss = 1.43414517\n",
      "Iteration 59019, loss = 1.57781267\n",
      "Iteration 59020, loss = 1.56419259\n",
      "Iteration 59021, loss = 1.66294249\n",
      "Iteration 59022, loss = 1.66139206\n",
      "Iteration 59023, loss = 1.99291739\n",
      "Iteration 59024, loss = 2.42712255\n",
      "Iteration 59025, loss = 1.93995953\n",
      "Iteration 59026, loss = 2.52014528\n",
      "Iteration 59027, loss = 3.00888016\n",
      "Iteration 59028, loss = 3.28855945\n",
      "Iteration 59029, loss = 2.37526035\n",
      "Iteration 59030, loss = 1.98288077\n",
      "Iteration 59031, loss = 2.26431042\n",
      "Iteration 59032, loss = 2.14316523\n",
      "Iteration 59033, loss = 1.73394996\n",
      "Iteration 59034, loss = 1.45215941\n",
      "Iteration 59035, loss = 2.03785075\n",
      "Iteration 59036, loss = 1.75978156\n",
      "Iteration 59037, loss = 1.27813175\n",
      "Iteration 59038, loss = 1.28467140\n",
      "Iteration 59039, loss = 1.20659307\n",
      "Iteration 59040, loss = 1.17699174\n",
      "Iteration 59041, loss = 1.24075446\n",
      "Iteration 59042, loss = 1.30304885\n",
      "Iteration 59043, loss = 1.27787622\n",
      "Iteration 59044, loss = 1.69338014\n",
      "Iteration 59045, loss = 1.69318678\n",
      "Iteration 59046, loss = 1.43087052\n",
      "Iteration 59047, loss = 1.36645380\n",
      "Iteration 59048, loss = 1.25178698\n",
      "Iteration 59049, loss = 1.68029674\n",
      "Iteration 59050, loss = 1.98083264\n",
      "Iteration 59051, loss = 2.06638662\n",
      "Iteration 59052, loss = 1.69577452\n",
      "Iteration 59053, loss = 1.93683351\n",
      "Iteration 59054, loss = 2.11920911\n",
      "Iteration 59055, loss = 1.74561710\n",
      "Iteration 59056, loss = 1.59285532\n",
      "Iteration 59057, loss = 1.75664321\n",
      "Iteration 59058, loss = 1.72448308\n",
      "Iteration 59059, loss = 2.13095655\n",
      "Iteration 59060, loss = 1.87862559\n",
      "Iteration 59061, loss = 2.54685256\n",
      "Iteration 59062, loss = 1.89239186\n",
      "Iteration 59063, loss = 1.98430466\n",
      "Iteration 59064, loss = 2.03029245\n",
      "Iteration 59065, loss = 1.73642290\n",
      "Iteration 59066, loss = 1.76025042\n",
      "Iteration 59067, loss = 1.93353768\n",
      "Iteration 59068, loss = 1.66426055\n",
      "Iteration 59069, loss = 1.80147326\n",
      "Iteration 59070, loss = 1.68465052\n",
      "Iteration 59071, loss = 1.72032745\n",
      "Iteration 59072, loss = 1.64726140\n",
      "Iteration 59073, loss = 1.59864272\n",
      "Iteration 59074, loss = 1.72873808\n",
      "Iteration 59075, loss = 2.05453798\n",
      "Iteration 59076, loss = 1.73957967\n",
      "Iteration 59077, loss = 1.80277330\n",
      "Iteration 59078, loss = 1.68822624\n",
      "Iteration 59079, loss = 1.27154431\n",
      "Iteration 59080, loss = 1.35338161\n",
      "Iteration 59081, loss = 1.39660322\n",
      "Iteration 59082, loss = 1.26005999\n",
      "Iteration 59083, loss = 1.30201386\n",
      "Iteration 59084, loss = 1.34837768\n",
      "Iteration 59085, loss = 1.54766072\n",
      "Iteration 59086, loss = 1.89366480\n",
      "Iteration 59087, loss = 1.72135824\n",
      "Iteration 59088, loss = 1.44121853\n",
      "Iteration 59089, loss = 1.47098795\n",
      "Iteration 59090, loss = 1.49339256\n",
      "Iteration 59091, loss = 1.54179284\n",
      "Iteration 59092, loss = 1.86620983\n",
      "Iteration 59093, loss = 1.72527191\n",
      "Iteration 59094, loss = 1.47116300\n",
      "Iteration 59095, loss = 1.51383122\n",
      "Iteration 59096, loss = 1.41867444\n",
      "Iteration 59097, loss = 1.53921349\n",
      "Iteration 59098, loss = 1.42602428\n",
      "Iteration 59099, loss = 1.22545179\n",
      "Iteration 59100, loss = 1.36570166\n",
      "Iteration 59101, loss = 1.30421167\n",
      "Iteration 59102, loss = 1.37254631\n",
      "Iteration 59103, loss = 1.77546875\n",
      "Iteration 59104, loss = 1.73945994\n",
      "Iteration 59105, loss = 1.54825799\n",
      "Iteration 59106, loss = 1.31130712\n",
      "Iteration 59107, loss = 1.32180026\n",
      "Iteration 59108, loss = 1.37492052\n",
      "Iteration 59109, loss = 1.76669584\n",
      "Iteration 59110, loss = 1.54108657\n",
      "Iteration 59111, loss = 1.60710477\n",
      "Iteration 59112, loss = 1.64653887\n",
      "Iteration 59113, loss = 1.68191746\n",
      "Iteration 59114, loss = 1.43952230\n",
      "Iteration 59115, loss = 1.53076795\n",
      "Iteration 59116, loss = 1.43955796\n",
      "Iteration 59117, loss = 1.32937191\n",
      "Iteration 59118, loss = 1.21929768\n",
      "Iteration 59119, loss = 1.17920790\n",
      "Iteration 59120, loss = 1.18851268\n",
      "Iteration 59121, loss = 1.32052021\n",
      "Iteration 59122, loss = 1.40738032\n",
      "Iteration 59123, loss = 1.35253913\n",
      "Iteration 59124, loss = 1.34626201\n",
      "Iteration 59125, loss = 1.45645016\n",
      "Iteration 59126, loss = 1.47388974\n",
      "Iteration 59127, loss = 1.27340780\n",
      "Iteration 59128, loss = 1.27053863\n",
      "Iteration 59129, loss = 1.36316998\n",
      "Iteration 59130, loss = 1.36760097\n",
      "Iteration 59131, loss = 1.23174388\n",
      "Iteration 59132, loss = 1.28901457\n",
      "Iteration 59133, loss = 1.30575539\n",
      "Iteration 59134, loss = 1.47357661\n",
      "Iteration 59135, loss = 1.42826255\n",
      "Iteration 59136, loss = 1.41102683\n",
      "Iteration 59137, loss = 1.30524737\n",
      "Iteration 59138, loss = 1.19342571\n",
      "Iteration 59139, loss = 1.29522809\n",
      "Iteration 59140, loss = 1.51344971\n",
      "Iteration 59141, loss = 1.45751505\n",
      "Iteration 59142, loss = 1.34406179\n",
      "Iteration 59143, loss = 1.24948468\n",
      "Iteration 59144, loss = 1.23956138\n",
      "Iteration 59145, loss = 1.42311276\n",
      "Iteration 59146, loss = 1.67679080\n",
      "Iteration 59147, loss = 1.93373557\n",
      "Iteration 59148, loss = 1.56050999\n",
      "Iteration 59149, loss = 1.81346698\n",
      "Iteration 59150, loss = 2.63477735\n",
      "Iteration 59151, loss = 1.68279692\n",
      "Iteration 59152, loss = 1.89015918\n",
      "Iteration 59153, loss = 2.35516477\n",
      "Iteration 59154, loss = 2.00732141\n",
      "Iteration 59155, loss = 1.95230519\n",
      "Iteration 59156, loss = 1.77422005\n",
      "Iteration 59157, loss = 1.50342584\n",
      "Iteration 59158, loss = 1.51818521\n",
      "Iteration 59159, loss = 1.54411424\n",
      "Iteration 59160, loss = 1.41380167\n",
      "Iteration 59161, loss = 1.55053996\n",
      "Iteration 59162, loss = 1.44529331\n",
      "Iteration 59163, loss = 1.40802562\n",
      "Iteration 59164, loss = 1.37784823\n",
      "Iteration 59165, loss = 1.53648517\n",
      "Iteration 59166, loss = 1.75902460\n",
      "Iteration 59167, loss = 1.88377719\n",
      "Iteration 59168, loss = 2.10409638\n",
      "Iteration 59169, loss = 1.75166708\n",
      "Iteration 59170, loss = 1.96266532\n",
      "Iteration 59171, loss = 1.56655932\n",
      "Iteration 59172, loss = 1.58688896\n",
      "Iteration 59173, loss = 1.63424684\n",
      "Iteration 59174, loss = 1.75222951\n",
      "Iteration 59175, loss = 2.19599671\n",
      "Iteration 59176, loss = 1.75989974\n",
      "Iteration 59177, loss = 1.60992748\n",
      "Iteration 59178, loss = 1.26637015\n",
      "Iteration 59179, loss = 1.23248161\n",
      "Iteration 59180, loss = 1.28308700\n",
      "Iteration 59181, loss = 1.48898873\n",
      "Iteration 59182, loss = 1.36142190\n",
      "Iteration 59183, loss = 1.28892435\n",
      "Iteration 59184, loss = 1.41541900\n",
      "Iteration 59185, loss = 1.69971066\n",
      "Iteration 59186, loss = 1.84075908\n",
      "Iteration 59187, loss = 1.57720617\n",
      "Iteration 59188, loss = 1.78605872\n",
      "Iteration 59189, loss = 1.98008017\n",
      "Iteration 59190, loss = 1.71690356\n",
      "Iteration 59191, loss = 1.72194972\n",
      "Iteration 59192, loss = 2.00121531\n",
      "Iteration 59193, loss = 1.67087717\n",
      "Iteration 59194, loss = 1.46653175\n",
      "Iteration 59195, loss = 1.83036512\n",
      "Iteration 59196, loss = 2.11194303\n",
      "Iteration 59197, loss = 1.83232551\n",
      "Iteration 59198, loss = 1.67936051\n",
      "Iteration 59199, loss = 1.42745331\n",
      "Iteration 59200, loss = 1.49622754\n",
      "Iteration 59201, loss = 1.36921398\n",
      "Iteration 59202, loss = 1.46331911\n",
      "Iteration 59203, loss = 1.52872513\n",
      "Iteration 59204, loss = 1.54576563\n",
      "Iteration 59205, loss = 1.50911496\n",
      "Iteration 59206, loss = 1.81640185\n",
      "Iteration 59207, loss = 1.62448713\n",
      "Iteration 59208, loss = 1.90013681\n",
      "Iteration 59209, loss = 1.91027208\n",
      "Iteration 59210, loss = 2.29311874\n",
      "Iteration 59211, loss = 1.72479935\n",
      "Iteration 59212, loss = 1.69785423\n",
      "Iteration 59213, loss = 2.18527315\n",
      "Iteration 59214, loss = 3.38079891\n",
      "Iteration 59215, loss = 2.04235361\n",
      "Iteration 59216, loss = 2.13929542\n",
      "Iteration 59217, loss = 1.48358014\n",
      "Iteration 59218, loss = 1.68930436\n",
      "Iteration 59219, loss = 1.88347420\n",
      "Iteration 59220, loss = 1.68099313\n",
      "Iteration 59221, loss = 3.22564104\n",
      "Iteration 59222, loss = 4.65113154\n",
      "Iteration 59223, loss = 2.86889993\n",
      "Iteration 59224, loss = 2.60632819\n",
      "Iteration 59225, loss = 2.42961201\n",
      "Iteration 59226, loss = 2.16470059\n",
      "Iteration 59227, loss = 2.28753948\n",
      "Iteration 59228, loss = 1.78438992\n",
      "Iteration 59229, loss = 1.78804652\n",
      "Iteration 59230, loss = 1.58234293\n",
      "Iteration 59231, loss = 1.89675837\n",
      "Iteration 59232, loss = 1.75524004\n",
      "Iteration 59233, loss = 1.84060321\n",
      "Iteration 59234, loss = 1.64073040\n",
      "Iteration 59235, loss = 1.46536337\n",
      "Iteration 59236, loss = 1.52297727\n",
      "Iteration 59237, loss = 1.41088328\n",
      "Iteration 59238, loss = 1.52985944\n",
      "Iteration 59239, loss = 1.34445046\n",
      "Iteration 59240, loss = 1.47913174\n",
      "Iteration 59241, loss = 1.32507173\n",
      "Iteration 59242, loss = 1.52014345\n",
      "Iteration 59243, loss = 1.77987874\n",
      "Iteration 59244, loss = 1.72848922\n",
      "Iteration 59245, loss = 1.78680370\n",
      "Iteration 59246, loss = 1.72398372\n",
      "Iteration 59247, loss = 1.65397099\n",
      "Iteration 59248, loss = 1.61022202\n",
      "Iteration 59249, loss = 1.86431240\n",
      "Iteration 59250, loss = 1.50337586\n",
      "Iteration 59251, loss = 1.50977969\n",
      "Iteration 59252, loss = 1.31925825\n",
      "Iteration 59253, loss = 1.26799558\n",
      "Iteration 59254, loss = 1.31692901\n",
      "Iteration 59255, loss = 1.66062735\n",
      "Iteration 59256, loss = 2.13699522\n",
      "Iteration 59257, loss = 1.86066451\n",
      "Iteration 59258, loss = 1.68767026\n",
      "Iteration 59259, loss = 1.44037306\n",
      "Iteration 59260, loss = 1.48851947\n",
      "Iteration 59261, loss = 1.51005809\n",
      "Iteration 59262, loss = 1.35009943\n",
      "Iteration 59263, loss = 1.43210389\n",
      "Iteration 59264, loss = 1.41314620\n",
      "Iteration 59265, loss = 1.51893327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 59266, loss = 1.72021067\n",
      "Iteration 59267, loss = 1.50104592\n",
      "Iteration 59268, loss = 1.59944425\n",
      "Iteration 59269, loss = 1.40538988\n",
      "Iteration 59270, loss = 1.51393556\n",
      "Iteration 59271, loss = 1.50581685\n",
      "Iteration 59272, loss = 1.73421088\n",
      "Iteration 59273, loss = 1.31655818\n",
      "Iteration 59274, loss = 1.57244206\n",
      "Iteration 59275, loss = 1.77938480\n",
      "Iteration 59276, loss = 1.68171123\n",
      "Iteration 59277, loss = 1.46432452\n",
      "Iteration 59278, loss = 1.48801251\n",
      "Iteration 59279, loss = 1.37902772\n",
      "Iteration 59280, loss = 1.52414918\n",
      "Iteration 59281, loss = 1.46323089\n",
      "Iteration 59282, loss = 1.30155106\n",
      "Iteration 59283, loss = 1.50956154\n",
      "Iteration 59284, loss = 1.88610917\n",
      "Iteration 59285, loss = 1.48255201\n",
      "Iteration 59286, loss = 1.38853718\n",
      "Iteration 59287, loss = 1.49041926\n",
      "Iteration 59288, loss = 2.17874814\n",
      "Iteration 59289, loss = 3.30500234\n",
      "Iteration 59290, loss = 2.83609509\n",
      "Iteration 59291, loss = 3.40536283\n",
      "Iteration 59292, loss = 2.99991656\n",
      "Iteration 59293, loss = 3.25663214\n",
      "Iteration 59294, loss = 2.76077071\n",
      "Iteration 59295, loss = 1.94304100\n",
      "Iteration 59296, loss = 3.42333204\n",
      "Iteration 59297, loss = 2.91234731\n",
      "Iteration 59298, loss = 1.98112473\n",
      "Iteration 59299, loss = 1.76065023\n",
      "Iteration 59300, loss = 1.73698500\n",
      "Iteration 59301, loss = 1.57709864\n",
      "Iteration 59302, loss = 1.40781872\n",
      "Iteration 59303, loss = 1.51361664\n",
      "Iteration 59304, loss = 1.31755059\n",
      "Iteration 59305, loss = 1.39300797\n",
      "Iteration 59306, loss = 1.39537745\n",
      "Iteration 59307, loss = 1.42756876\n",
      "Iteration 59308, loss = 1.55393195\n",
      "Iteration 59309, loss = 1.72028099\n",
      "Iteration 59310, loss = 2.06495282\n",
      "Iteration 59311, loss = 1.81899768\n",
      "Iteration 59312, loss = 1.54887079\n",
      "Iteration 59313, loss = 1.63820473\n",
      "Iteration 59314, loss = 1.43149362\n",
      "Iteration 59315, loss = 1.71420123\n",
      "Iteration 59316, loss = 1.56311386\n",
      "Iteration 59317, loss = 1.28037971\n",
      "Iteration 59318, loss = 1.35917372\n",
      "Iteration 59319, loss = 1.34491199\n",
      "Iteration 59320, loss = 1.42841376\n",
      "Iteration 59321, loss = 1.47465300\n",
      "Iteration 59322, loss = 1.27174307\n",
      "Iteration 59323, loss = 1.33170465\n",
      "Iteration 59324, loss = 1.29003953\n",
      "Iteration 59325, loss = 1.33419550\n",
      "Iteration 59326, loss = 1.27686707\n",
      "Iteration 59327, loss = 1.36554428\n",
      "Iteration 59328, loss = 1.19308544\n",
      "Iteration 59329, loss = 1.19179688\n",
      "Iteration 59330, loss = 1.56613231\n",
      "Iteration 59331, loss = 1.67975742\n",
      "Iteration 59332, loss = 1.55428345\n",
      "Iteration 59333, loss = 1.92864500\n",
      "Iteration 59334, loss = 1.80446877\n",
      "Iteration 59335, loss = 1.90154705\n",
      "Iteration 59336, loss = 1.77702491\n",
      "Iteration 59337, loss = 1.86265263\n",
      "Iteration 59338, loss = 2.07173547\n",
      "Iteration 59339, loss = 1.67473992\n",
      "Iteration 59340, loss = 1.98994430\n",
      "Iteration 59341, loss = 2.35448995\n",
      "Iteration 59342, loss = 2.05217050\n",
      "Iteration 59343, loss = 1.83976428\n",
      "Iteration 59344, loss = 1.60316996\n",
      "Iteration 59345, loss = 1.47800665\n",
      "Iteration 59346, loss = 1.36985211\n",
      "Iteration 59347, loss = 1.65076770\n",
      "Iteration 59348, loss = 1.42841158\n",
      "Iteration 59349, loss = 1.40861002\n",
      "Iteration 59350, loss = 1.29644940\n",
      "Iteration 59351, loss = 1.25002997\n",
      "Iteration 59352, loss = 1.41196613\n",
      "Iteration 59353, loss = 1.56839491\n",
      "Iteration 59354, loss = 1.44168739\n",
      "Iteration 59355, loss = 1.34847084\n",
      "Iteration 59356, loss = 1.29071527\n",
      "Iteration 59357, loss = 1.33131349\n",
      "Iteration 59358, loss = 1.29813441\n",
      "Iteration 59359, loss = 1.33200561\n",
      "Iteration 59360, loss = 1.51519833\n",
      "Iteration 59361, loss = 1.42452302\n",
      "Iteration 59362, loss = 1.52728076\n",
      "Iteration 59363, loss = 1.28615252\n",
      "Iteration 59364, loss = 1.38699909\n",
      "Iteration 59365, loss = 1.37314556\n",
      "Iteration 59366, loss = 1.27183838\n",
      "Iteration 59367, loss = 1.34155053\n",
      "Iteration 59368, loss = 1.33269285\n",
      "Iteration 59369, loss = 1.32832934\n",
      "Iteration 59370, loss = 1.66626714\n",
      "Iteration 59371, loss = 1.45235410\n",
      "Iteration 59372, loss = 1.31853528\n",
      "Iteration 59373, loss = 1.69958822\n",
      "Iteration 59374, loss = 2.01574500\n",
      "Iteration 59375, loss = 1.79453045\n",
      "Iteration 59376, loss = 1.37381593\n",
      "Iteration 59377, loss = 1.26876486\n",
      "Iteration 59378, loss = 1.34806451\n",
      "Iteration 59379, loss = 1.31710435\n",
      "Iteration 59380, loss = 1.39887920\n",
      "Iteration 59381, loss = 1.33958583\n",
      "Iteration 59382, loss = 1.29089115\n",
      "Iteration 59383, loss = 1.39645330\n",
      "Iteration 59384, loss = 1.41916003\n",
      "Iteration 59385, loss = 1.64079356\n",
      "Iteration 59386, loss = 1.45409702\n",
      "Iteration 59387, loss = 1.48101165\n",
      "Iteration 59388, loss = 1.75644304\n",
      "Iteration 59389, loss = 1.53166449\n",
      "Iteration 59390, loss = 1.43081605\n",
      "Iteration 59391, loss = 1.68332088\n",
      "Iteration 59392, loss = 1.80874598\n",
      "Iteration 59393, loss = 1.72474510\n",
      "Iteration 59394, loss = 1.77316079\n",
      "Iteration 59395, loss = 1.49358296\n",
      "Iteration 59396, loss = 1.36533950\n",
      "Iteration 59397, loss = 1.43272412\n",
      "Iteration 59398, loss = 1.43377537\n",
      "Iteration 59399, loss = 1.61053436\n",
      "Iteration 59400, loss = 1.56147245\n",
      "Iteration 59401, loss = 1.83764069\n",
      "Iteration 59402, loss = 1.66715972\n",
      "Iteration 59403, loss = 1.64290331\n",
      "Iteration 59404, loss = 2.13263867\n",
      "Iteration 59405, loss = 1.78682802\n",
      "Iteration 59406, loss = 1.63379771\n",
      "Iteration 59407, loss = 1.65173456\n",
      "Iteration 59408, loss = 1.31081485\n",
      "Iteration 59409, loss = 1.41600572\n",
      "Iteration 59410, loss = 1.68798739\n",
      "Iteration 59411, loss = 2.24980247\n",
      "Iteration 59412, loss = 1.64365720\n",
      "Iteration 59413, loss = 1.54616036\n",
      "Iteration 59414, loss = 1.54386066\n",
      "Iteration 59415, loss = 1.68334033\n",
      "Iteration 59416, loss = 1.74724774\n",
      "Iteration 59417, loss = 1.89728870\n",
      "Iteration 59418, loss = 2.12523063\n",
      "Iteration 59419, loss = 2.02636026\n",
      "Iteration 59420, loss = 1.54984206\n",
      "Iteration 59421, loss = 1.27568663\n",
      "Iteration 59422, loss = 1.36835214\n",
      "Iteration 59423, loss = 1.32191657\n",
      "Iteration 59424, loss = 1.36297086\n",
      "Iteration 59425, loss = 1.54845641\n",
      "Iteration 59426, loss = 1.56057610\n",
      "Iteration 59427, loss = 1.85356287\n",
      "Iteration 59428, loss = 1.69116413\n",
      "Iteration 59429, loss = 1.40581400\n",
      "Iteration 59430, loss = 1.34494195\n",
      "Iteration 59431, loss = 1.37364407\n",
      "Iteration 59432, loss = 1.27988998\n",
      "Iteration 59433, loss = 1.34656126\n",
      "Iteration 59434, loss = 1.25980202\n",
      "Iteration 59435, loss = 1.31768711\n",
      "Iteration 59436, loss = 1.41990485\n",
      "Iteration 59437, loss = 1.72398927\n",
      "Iteration 59438, loss = 1.50422560\n",
      "Iteration 59439, loss = 1.42501068\n",
      "Iteration 59440, loss = 1.70519131\n",
      "Iteration 59441, loss = 1.46428685\n",
      "Iteration 59442, loss = 2.21058627\n",
      "Iteration 59443, loss = 2.32948111\n",
      "Iteration 59444, loss = 1.85604309\n",
      "Iteration 59445, loss = 1.49122449\n",
      "Iteration 59446, loss = 1.69727354\n",
      "Iteration 59447, loss = 1.79909065\n",
      "Iteration 59448, loss = 1.55578187\n",
      "Iteration 59449, loss = 1.31246751\n",
      "Iteration 59450, loss = 1.30656162\n",
      "Iteration 59451, loss = 1.28673832\n",
      "Iteration 59452, loss = 1.29246334\n",
      "Iteration 59453, loss = 1.28652155\n",
      "Iteration 59454, loss = 1.46374086\n",
      "Iteration 59455, loss = 1.31463985\n",
      "Iteration 59456, loss = 1.24635043\n",
      "Iteration 59457, loss = 1.28853963\n",
      "Iteration 59458, loss = 1.53960821\n",
      "Iteration 59459, loss = 1.54440977\n",
      "Iteration 59460, loss = 1.42397861\n",
      "Iteration 59461, loss = 1.97660604\n",
      "Iteration 59462, loss = 1.91081939\n",
      "Iteration 59463, loss = 1.92193192\n",
      "Iteration 59464, loss = 2.26736199\n",
      "Iteration 59465, loss = 2.23937570\n",
      "Iteration 59466, loss = 2.09926785\n",
      "Iteration 59467, loss = 1.99854869\n",
      "Iteration 59468, loss = 2.77249013\n",
      "Iteration 59469, loss = 2.69328001\n",
      "Iteration 59470, loss = 2.59933348\n",
      "Iteration 59471, loss = 2.62789402\n",
      "Iteration 59472, loss = 2.61672171\n",
      "Iteration 59473, loss = 3.42097463\n",
      "Iteration 59474, loss = 3.31734484\n",
      "Iteration 59475, loss = 2.18134165\n",
      "Iteration 59476, loss = 1.72028279\n",
      "Iteration 59477, loss = 1.58574063\n",
      "Iteration 59478, loss = 1.46734582\n",
      "Iteration 59479, loss = 1.38239488\n",
      "Iteration 59480, loss = 1.46415748\n",
      "Iteration 59481, loss = 1.33606380\n",
      "Iteration 59482, loss = 1.40123745\n",
      "Iteration 59483, loss = 1.32818981\n",
      "Iteration 59484, loss = 1.31639963\n",
      "Iteration 59485, loss = 1.24815048\n",
      "Iteration 59486, loss = 1.48403735\n",
      "Iteration 59487, loss = 1.36218808\n",
      "Iteration 59488, loss = 1.43884136\n",
      "Iteration 59489, loss = 1.28754284\n",
      "Iteration 59490, loss = 1.34759273\n",
      "Iteration 59491, loss = 1.49885932\n",
      "Iteration 59492, loss = 1.52153891\n",
      "Iteration 59493, loss = 1.45823125\n",
      "Iteration 59494, loss = 1.43649962\n",
      "Iteration 59495, loss = 1.32151949\n",
      "Iteration 59496, loss = 2.50287103\n",
      "Iteration 59497, loss = 4.65282378\n",
      "Iteration 59498, loss = 4.25342806\n",
      "Iteration 59499, loss = 3.69831971\n",
      "Iteration 59500, loss = 4.17711150\n",
      "Iteration 59501, loss = 2.57639582\n",
      "Iteration 59502, loss = 1.89269900\n",
      "Iteration 59503, loss = 1.54364523\n",
      "Iteration 59504, loss = 1.47203216\n",
      "Iteration 59505, loss = 1.40602971\n",
      "Iteration 59506, loss = 1.47135285\n",
      "Iteration 59507, loss = 1.26235043\n",
      "Iteration 59508, loss = 1.54499629\n",
      "Iteration 59509, loss = 1.86379344\n",
      "Iteration 59510, loss = 1.44403834\n",
      "Iteration 59511, loss = 1.30956094\n",
      "Iteration 59512, loss = 1.38671483\n",
      "Iteration 59513, loss = 1.32429312\n",
      "Iteration 59514, loss = 1.39691110\n",
      "Iteration 59515, loss = 1.40134473\n",
      "Iteration 59516, loss = 1.54174240\n",
      "Iteration 59517, loss = 1.61362248\n",
      "Iteration 59518, loss = 1.55618286\n",
      "Iteration 59519, loss = 1.56744439\n",
      "Iteration 59520, loss = 1.41255505\n",
      "Iteration 59521, loss = 1.28041468\n",
      "Iteration 59522, loss = 1.36633483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 59523, loss = 1.58356581\n",
      "Iteration 59524, loss = 1.63463188\n",
      "Iteration 59525, loss = 1.55377579\n",
      "Iteration 59526, loss = 1.50653075\n",
      "Iteration 59527, loss = 1.51264540\n",
      "Iteration 59528, loss = 1.53645102\n",
      "Iteration 59529, loss = 1.46309320\n",
      "Iteration 59530, loss = 1.57063137\n",
      "Iteration 59531, loss = 1.70189250\n",
      "Iteration 59532, loss = 1.39370049\n",
      "Iteration 59533, loss = 1.32191026\n",
      "Iteration 59534, loss = 1.48271583\n",
      "Iteration 59535, loss = 1.60710216\n",
      "Iteration 59536, loss = 1.51596847\n",
      "Iteration 59537, loss = 1.70322879\n",
      "Iteration 59538, loss = 1.47278916\n",
      "Iteration 59539, loss = 1.50581368\n",
      "Iteration 59540, loss = 1.51181320\n",
      "Iteration 59541, loss = 1.45726794\n",
      "Iteration 59542, loss = 1.48609986\n",
      "Iteration 59543, loss = 1.35287835\n",
      "Iteration 59544, loss = 1.41661613\n",
      "Iteration 59545, loss = 1.45701136\n",
      "Iteration 59546, loss = 1.58030747\n",
      "Iteration 59547, loss = 2.14516330\n",
      "Iteration 59548, loss = 1.90212643\n",
      "Iteration 59549, loss = 1.66373094\n",
      "Iteration 59550, loss = 1.75872305\n",
      "Iteration 59551, loss = 1.75073285\n",
      "Iteration 59552, loss = 1.78147362\n",
      "Iteration 59553, loss = 1.81891820\n",
      "Iteration 59554, loss = 1.67046585\n",
      "Iteration 59555, loss = 1.35765330\n",
      "Iteration 59556, loss = 1.44438365\n",
      "Iteration 59557, loss = 1.27366953\n",
      "Iteration 59558, loss = 1.33671242\n",
      "Iteration 59559, loss = 1.15871283\n",
      "Iteration 59560, loss = 1.35328746\n",
      "Iteration 59561, loss = 1.36255887\n",
      "Iteration 59562, loss = 1.28541868\n",
      "Iteration 59563, loss = 1.31620964\n",
      "Iteration 59564, loss = 1.39874666\n",
      "Iteration 59565, loss = 1.37414749\n",
      "Iteration 59566, loss = 1.47533501\n",
      "Iteration 59567, loss = 1.31230667\n",
      "Iteration 59568, loss = 1.30318787\n",
      "Iteration 59569, loss = 1.62900415\n",
      "Iteration 59570, loss = 1.37409280\n",
      "Iteration 59571, loss = 1.54266245\n",
      "Iteration 59572, loss = 1.57288654\n",
      "Iteration 59573, loss = 1.69033670\n",
      "Iteration 59574, loss = 1.50724711\n",
      "Iteration 59575, loss = 1.32274638\n",
      "Iteration 59576, loss = 1.42871855\n",
      "Iteration 59577, loss = 1.58495763\n",
      "Iteration 59578, loss = 1.97433426\n",
      "Iteration 59579, loss = 2.90655067\n",
      "Iteration 59580, loss = 1.93344187\n",
      "Iteration 59581, loss = 1.99008173\n",
      "Iteration 59582, loss = 2.09319892\n",
      "Iteration 59583, loss = 1.93658404\n",
      "Iteration 59584, loss = 2.00050815\n",
      "Iteration 59585, loss = 1.79941127\n",
      "Iteration 59586, loss = 1.92385940\n",
      "Iteration 59587, loss = 1.64795639\n",
      "Iteration 59588, loss = 1.79721295\n",
      "Iteration 59589, loss = 1.63555878\n",
      "Iteration 59590, loss = 1.80689246\n",
      "Iteration 59591, loss = 2.73076908\n",
      "Iteration 59592, loss = 1.99650636\n",
      "Iteration 59593, loss = 1.77455690\n",
      "Iteration 59594, loss = 1.63044472\n",
      "Iteration 59595, loss = 3.11118329\n",
      "Iteration 59596, loss = 3.94949908\n",
      "Iteration 59597, loss = 2.78790169\n",
      "Iteration 59598, loss = 2.25745129\n",
      "Iteration 59599, loss = 2.07809215\n",
      "Iteration 59600, loss = 2.54681029\n",
      "Iteration 59601, loss = 1.92285027\n",
      "Iteration 59602, loss = 1.54459151\n",
      "Iteration 59603, loss = 1.73263056\n",
      "Iteration 59604, loss = 1.80058928\n",
      "Iteration 59605, loss = 1.88393265\n",
      "Iteration 59606, loss = 2.81051248\n",
      "Iteration 59607, loss = 2.91008386\n",
      "Iteration 59608, loss = 3.04928571\n",
      "Iteration 59609, loss = 1.82553234\n",
      "Iteration 59610, loss = 1.65027418\n",
      "Iteration 59611, loss = 1.51145274\n",
      "Iteration 59612, loss = 1.88481709\n",
      "Iteration 59613, loss = 1.77515838\n",
      "Iteration 59614, loss = 2.59449499\n",
      "Iteration 59615, loss = 2.70845057\n",
      "Iteration 59616, loss = 2.54569810\n",
      "Iteration 59617, loss = 2.84300515\n",
      "Iteration 59618, loss = 2.90376937\n",
      "Iteration 59619, loss = 3.08216846\n",
      "Iteration 59620, loss = 3.01858055\n",
      "Iteration 59621, loss = 2.20445442\n",
      "Iteration 59622, loss = 2.78644100\n",
      "Iteration 59623, loss = 3.05685290\n",
      "Iteration 59624, loss = 2.48488831\n",
      "Iteration 59625, loss = 2.71872517\n",
      "Iteration 59626, loss = 1.55159953\n",
      "Iteration 59627, loss = 1.88003055\n",
      "Iteration 59628, loss = 1.67665203\n",
      "Iteration 59629, loss = 1.46459571\n",
      "Iteration 59630, loss = 1.73150372\n",
      "Iteration 59631, loss = 1.36665312\n",
      "Iteration 59632, loss = 1.26434888\n",
      "Iteration 59633, loss = 1.20257403\n",
      "Iteration 59634, loss = 1.26564275\n",
      "Iteration 59635, loss = 1.35097841\n",
      "Iteration 59636, loss = 1.20454488\n",
      "Iteration 59637, loss = 1.23116416\n",
      "Iteration 59638, loss = 1.17320265\n",
      "Iteration 59639, loss = 1.30021818\n",
      "Iteration 59640, loss = 1.24532536\n",
      "Iteration 59641, loss = 1.27662957\n",
      "Iteration 59642, loss = 1.27112480\n",
      "Iteration 59643, loss = 1.38463599\n",
      "Iteration 59644, loss = 1.33416596\n",
      "Iteration 59645, loss = 1.40589925\n",
      "Iteration 59646, loss = 1.38909941\n",
      "Iteration 59647, loss = 1.49992187\n",
      "Iteration 59648, loss = 1.26506458\n",
      "Iteration 59649, loss = 1.47560345\n",
      "Iteration 59650, loss = 1.72705383\n",
      "Iteration 59651, loss = 1.48029385\n",
      "Iteration 59652, loss = 1.34442343\n",
      "Iteration 59653, loss = 1.36910699\n",
      "Iteration 59654, loss = 1.47922285\n",
      "Iteration 59655, loss = 1.51707774\n",
      "Iteration 59656, loss = 1.47326161\n",
      "Iteration 59657, loss = 1.63492782\n",
      "Iteration 59658, loss = 1.88879779\n",
      "Iteration 59659, loss = 1.62903250\n",
      "Iteration 59660, loss = 1.35701154\n",
      "Iteration 59661, loss = 1.85958540\n",
      "Iteration 59662, loss = 1.50868026\n",
      "Iteration 59663, loss = 1.28036922\n",
      "Iteration 59664, loss = 1.41249399\n",
      "Iteration 59665, loss = 1.73615868\n",
      "Iteration 59666, loss = 1.62936436\n",
      "Iteration 59667, loss = 1.32042892\n",
      "Iteration 59668, loss = 1.59944191\n",
      "Iteration 59669, loss = 1.64927344\n",
      "Iteration 59670, loss = 1.33242729\n",
      "Iteration 59671, loss = 1.55493378\n",
      "Iteration 59672, loss = 1.58200835\n",
      "Iteration 59673, loss = 1.50743073\n",
      "Iteration 59674, loss = 1.57265844\n",
      "Iteration 59675, loss = 1.41383952\n",
      "Iteration 59676, loss = 1.93154813\n",
      "Iteration 59677, loss = 2.28284143\n",
      "Iteration 59678, loss = 1.58824741\n",
      "Iteration 59679, loss = 1.85747984\n",
      "Iteration 59680, loss = 1.83924389\n",
      "Iteration 59681, loss = 1.50795493\n",
      "Iteration 59682, loss = 1.45976981\n",
      "Iteration 59683, loss = 1.37767968\n",
      "Iteration 59684, loss = 1.50555952\n",
      "Iteration 59685, loss = 1.73599318\n",
      "Iteration 59686, loss = 1.29549669\n",
      "Iteration 59687, loss = 1.44598385\n",
      "Iteration 59688, loss = 1.44865133\n",
      "Iteration 59689, loss = 1.38435560\n",
      "Iteration 59690, loss = 1.60296587\n",
      "Iteration 59691, loss = 1.38881239\n",
      "Iteration 59692, loss = 1.39682413\n",
      "Iteration 59693, loss = 1.36091425\n",
      "Iteration 59694, loss = 1.30415346\n",
      "Iteration 59695, loss = 1.34653466\n",
      "Iteration 59696, loss = 1.32020242\n",
      "Iteration 59697, loss = 1.24372538\n",
      "Iteration 59698, loss = 1.29082568\n",
      "Iteration 59699, loss = 1.28098305\n",
      "Iteration 59700, loss = 1.43377523\n",
      "Iteration 59701, loss = 1.57301495\n",
      "Iteration 59702, loss = 2.11667745\n",
      "Iteration 59703, loss = 1.64564941\n",
      "Iteration 59704, loss = 1.66072951\n",
      "Iteration 59705, loss = 1.70679195\n",
      "Iteration 59706, loss = 1.38800410\n",
      "Iteration 59707, loss = 1.64042350\n",
      "Iteration 59708, loss = 1.20651158\n",
      "Iteration 59709, loss = 1.27113664\n",
      "Iteration 59710, loss = 1.64192045\n",
      "Iteration 59711, loss = 1.35770084\n",
      "Iteration 59712, loss = 1.32432763\n",
      "Iteration 59713, loss = 1.43615436\n",
      "Iteration 59714, loss = 1.69178006\n",
      "Iteration 59715, loss = 2.13956300\n",
      "Iteration 59716, loss = 1.84872687\n",
      "Iteration 59717, loss = 1.47028986\n",
      "Iteration 59718, loss = 1.47239560\n",
      "Iteration 59719, loss = 1.74419335\n",
      "Iteration 59720, loss = 1.86347089\n",
      "Iteration 59721, loss = 1.56550640\n",
      "Iteration 59722, loss = 1.22944242\n",
      "Iteration 59723, loss = 1.36233683\n",
      "Iteration 59724, loss = 1.38040458\n",
      "Iteration 59725, loss = 1.47268985\n",
      "Iteration 59726, loss = 1.29367572\n",
      "Iteration 59727, loss = 1.22678498\n",
      "Iteration 59728, loss = 1.32790500\n",
      "Iteration 59729, loss = 1.39279245\n",
      "Iteration 59730, loss = 1.55084912\n",
      "Iteration 59731, loss = 1.57902939\n",
      "Iteration 59732, loss = 1.27092474\n",
      "Iteration 59733, loss = 1.32955517\n",
      "Iteration 59734, loss = 1.38134471\n",
      "Iteration 59735, loss = 1.51178620\n",
      "Iteration 59736, loss = 1.22533254\n",
      "Iteration 59737, loss = 1.36048420\n",
      "Iteration 59738, loss = 1.57656050\n",
      "Iteration 59739, loss = 1.38546863\n",
      "Iteration 59740, loss = 1.73571584\n",
      "Iteration 59741, loss = 1.76647346\n",
      "Iteration 59742, loss = 1.64443454\n",
      "Iteration 59743, loss = 1.50782530\n",
      "Iteration 59744, loss = 1.33947225\n",
      "Iteration 59745, loss = 1.32317521\n",
      "Iteration 59746, loss = 1.53275936\n",
      "Iteration 59747, loss = 1.85258470\n",
      "Iteration 59748, loss = 1.95831125\n",
      "Iteration 59749, loss = 2.18971685\n",
      "Iteration 59750, loss = 2.31596854\n",
      "Iteration 59751, loss = 2.98750033\n",
      "Iteration 59752, loss = 4.00424592\n",
      "Iteration 59753, loss = 6.81181819\n",
      "Iteration 59754, loss = 7.29900008\n",
      "Iteration 59755, loss = 3.13857907\n",
      "Iteration 59756, loss = 2.85295166\n",
      "Iteration 59757, loss = 1.92663279\n",
      "Iteration 59758, loss = 2.23736690\n",
      "Iteration 59759, loss = 2.98209819\n",
      "Iteration 59760, loss = 2.04586584\n",
      "Iteration 59761, loss = 1.99253124\n",
      "Iteration 59762, loss = 1.94072278\n",
      "Iteration 59763, loss = 2.15335937\n",
      "Iteration 59764, loss = 2.11088860\n",
      "Iteration 59765, loss = 1.99475855\n",
      "Iteration 59766, loss = 1.86354368\n",
      "Iteration 59767, loss = 1.80245613\n",
      "Iteration 59768, loss = 1.94583528\n",
      "Iteration 59769, loss = 1.43239427\n",
      "Iteration 59770, loss = 1.46648859\n",
      "Iteration 59771, loss = 1.39584499\n",
      "Iteration 59772, loss = 1.41450365\n",
      "Iteration 59773, loss = 1.40951397\n",
      "Iteration 59774, loss = 1.54442892\n",
      "Iteration 59775, loss = 1.59788188\n",
      "Iteration 59776, loss = 1.60620893\n",
      "Iteration 59777, loss = 1.50318969\n",
      "Iteration 59778, loss = 1.61591232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 59779, loss = 1.60683753\n",
      "Iteration 59780, loss = 1.36028942\n",
      "Iteration 59781, loss = 1.26620724\n",
      "Iteration 59782, loss = 1.29307455\n",
      "Iteration 59783, loss = 1.22167226\n",
      "Iteration 59784, loss = 1.42122510\n",
      "Iteration 59785, loss = 1.37896294\n",
      "Iteration 59786, loss = 1.48613449\n",
      "Iteration 59787, loss = 1.28205476\n",
      "Iteration 59788, loss = 1.72001529\n",
      "Iteration 59789, loss = 1.61735938\n",
      "Iteration 59790, loss = 1.80233712\n",
      "Iteration 59791, loss = 1.77391035\n",
      "Iteration 59792, loss = 1.59393767\n",
      "Iteration 59793, loss = 3.13666706\n",
      "Iteration 59794, loss = 2.71058296\n",
      "Iteration 59795, loss = 3.28930093\n",
      "Iteration 59796, loss = 3.41999543\n",
      "Iteration 59797, loss = 3.08127227\n",
      "Iteration 59798, loss = 2.79538937\n",
      "Iteration 59799, loss = 2.18572911\n",
      "Iteration 59800, loss = 1.61723706\n",
      "Iteration 59801, loss = 1.64282749\n",
      "Iteration 59802, loss = 1.53909088\n",
      "Iteration 59803, loss = 1.32541873\n",
      "Iteration 59804, loss = 1.39206633\n",
      "Iteration 59805, loss = 1.35661367\n",
      "Iteration 59806, loss = 1.37243002\n",
      "Iteration 59807, loss = 1.31593767\n",
      "Iteration 59808, loss = 1.24236227\n",
      "Iteration 59809, loss = 1.28613373\n",
      "Iteration 59810, loss = 1.35546637\n",
      "Iteration 59811, loss = 1.41533249\n",
      "Iteration 59812, loss = 1.39029783\n",
      "Iteration 59813, loss = 1.55664588\n",
      "Iteration 59814, loss = 1.40918742\n",
      "Iteration 59815, loss = 1.26482810\n",
      "Iteration 59816, loss = 1.28605259\n",
      "Iteration 59817, loss = 1.23629668\n",
      "Iteration 59818, loss = 1.36513492\n",
      "Iteration 59819, loss = 1.33251649\n",
      "Iteration 59820, loss = 1.52954972\n",
      "Iteration 59821, loss = 1.75920907\n",
      "Iteration 59822, loss = 1.59707262\n",
      "Iteration 59823, loss = 1.58847511\n",
      "Iteration 59824, loss = 1.30295351\n",
      "Iteration 59825, loss = 1.49909589\n",
      "Iteration 59826, loss = 2.17994536\n",
      "Iteration 59827, loss = 2.00509073\n",
      "Iteration 59828, loss = 1.61069671\n",
      "Iteration 59829, loss = 1.49145408\n",
      "Iteration 59830, loss = 1.37506233\n",
      "Iteration 59831, loss = 1.37362909\n",
      "Iteration 59832, loss = 1.36847158\n",
      "Iteration 59833, loss = 1.56108050\n",
      "Iteration 59834, loss = 1.48519091\n",
      "Iteration 59835, loss = 1.44034470\n",
      "Iteration 59836, loss = 1.32112557\n",
      "Iteration 59837, loss = 1.72810952\n",
      "Iteration 59838, loss = 1.61897827\n",
      "Iteration 59839, loss = 1.48978501\n",
      "Iteration 59840, loss = 1.32728517\n",
      "Iteration 59841, loss = 1.25995514\n",
      "Iteration 59842, loss = 1.27184380\n",
      "Iteration 59843, loss = 1.19571389\n",
      "Iteration 59844, loss = 1.32669979\n",
      "Iteration 59845, loss = 1.21529690\n",
      "Iteration 59846, loss = 1.33781527\n",
      "Iteration 59847, loss = 1.45878392\n",
      "Iteration 59848, loss = 1.75968215\n",
      "Iteration 59849, loss = 1.67620433\n",
      "Iteration 59850, loss = 1.24684381\n",
      "Iteration 59851, loss = 1.59048535\n",
      "Iteration 59852, loss = 1.30712117\n",
      "Iteration 59853, loss = 1.30292698\n",
      "Iteration 59854, loss = 1.25249795\n",
      "Iteration 59855, loss = 1.14427513\n",
      "Iteration 59856, loss = 1.27210717\n",
      "Iteration 59857, loss = 1.31984690\n",
      "Iteration 59858, loss = 1.46725882\n",
      "Iteration 59859, loss = 1.40323911\n",
      "Iteration 59860, loss = 1.38951957\n",
      "Iteration 59861, loss = 1.61478769\n",
      "Iteration 59862, loss = 1.50227179\n",
      "Iteration 59863, loss = 1.52927702\n",
      "Iteration 59864, loss = 1.71486010\n",
      "Iteration 59865, loss = 1.96116087\n",
      "Iteration 59866, loss = 1.58518702\n",
      "Iteration 59867, loss = 1.41833063\n",
      "Iteration 59868, loss = 1.56899701\n",
      "Iteration 59869, loss = 1.53538947\n",
      "Iteration 59870, loss = 1.53360853\n",
      "Iteration 59871, loss = 1.91405735\n",
      "Iteration 59872, loss = 1.58322954\n",
      "Iteration 59873, loss = 2.00513861\n",
      "Iteration 59874, loss = 1.81987313\n",
      "Iteration 59875, loss = 1.84045727\n",
      "Iteration 59876, loss = 1.64408222\n",
      "Iteration 59877, loss = 1.53086835\n",
      "Iteration 59878, loss = 1.44578859\n",
      "Iteration 59879, loss = 1.67649941\n",
      "Iteration 59880, loss = 1.43282990\n",
      "Iteration 59881, loss = 1.50401004\n",
      "Iteration 59882, loss = 1.38827753\n",
      "Iteration 59883, loss = 1.57733222\n",
      "Iteration 59884, loss = 1.66763874\n",
      "Iteration 59885, loss = 1.43592045\n",
      "Iteration 59886, loss = 1.21660245\n",
      "Iteration 59887, loss = 1.41990751\n",
      "Iteration 59888, loss = 1.34829283\n",
      "Iteration 59889, loss = 1.49369660\n",
      "Iteration 59890, loss = 1.42834695\n",
      "Iteration 59891, loss = 1.56149186\n",
      "Iteration 59892, loss = 1.40427612\n",
      "Iteration 59893, loss = 1.33346998\n",
      "Iteration 59894, loss = 1.48924248\n",
      "Iteration 59895, loss = 1.40783572\n",
      "Iteration 59896, loss = 1.68871311\n",
      "Iteration 59897, loss = 1.81634848\n",
      "Iteration 59898, loss = 1.64248497\n",
      "Iteration 59899, loss = 1.75495306\n",
      "Iteration 59900, loss = 1.35526249\n",
      "Iteration 59901, loss = 1.27653319\n",
      "Iteration 59902, loss = 1.34927809\n",
      "Iteration 59903, loss = 1.53867976\n",
      "Iteration 59904, loss = 1.53745943\n",
      "Iteration 59905, loss = 1.57899910\n",
      "Iteration 59906, loss = 1.51166865\n",
      "Iteration 59907, loss = 1.30599840\n",
      "Iteration 59908, loss = 1.16708220\n",
      "Iteration 59909, loss = 1.44315434\n",
      "Iteration 59910, loss = 1.30565206\n",
      "Iteration 59911, loss = 1.45470079\n",
      "Iteration 59912, loss = 1.50391673\n",
      "Iteration 59913, loss = 1.39566588\n",
      "Iteration 59914, loss = 1.45023530\n",
      "Iteration 59915, loss = 1.56273236\n",
      "Iteration 59916, loss = 1.52389578\n",
      "Iteration 59917, loss = 1.72265556\n",
      "Iteration 59918, loss = 1.40980292\n",
      "Iteration 59919, loss = 1.59224131\n",
      "Iteration 59920, loss = 1.43061609\n",
      "Iteration 59921, loss = 1.25165222\n",
      "Iteration 59922, loss = 1.19952091\n",
      "Iteration 59923, loss = 1.27144863\n",
      "Iteration 59924, loss = 1.35080945\n",
      "Iteration 59925, loss = 1.25614982\n",
      "Iteration 59926, loss = 1.42420854\n",
      "Iteration 59927, loss = 1.52193310\n",
      "Iteration 59928, loss = 1.59937485\n",
      "Iteration 59929, loss = 1.38493804\n",
      "Iteration 59930, loss = 1.41315867\n",
      "Iteration 59931, loss = 1.37185027\n",
      "Iteration 59932, loss = 1.62697724\n",
      "Iteration 59933, loss = 1.80796204\n",
      "Iteration 59934, loss = 1.62600901\n",
      "Iteration 59935, loss = 1.58591046\n",
      "Iteration 59936, loss = 1.65709795\n",
      "Iteration 59937, loss = 2.01963849\n",
      "Iteration 59938, loss = 2.25159286\n",
      "Iteration 59939, loss = 1.95441414\n",
      "Iteration 59940, loss = 1.67765568\n",
      "Iteration 59941, loss = 1.37638660\n",
      "Iteration 59942, loss = 1.48136904\n",
      "Iteration 59943, loss = 1.42873434\n",
      "Iteration 59944, loss = 2.05245035\n",
      "Iteration 59945, loss = 1.70538357\n",
      "Iteration 59946, loss = 1.70503914\n",
      "Iteration 59947, loss = 1.55584538\n",
      "Iteration 59948, loss = 1.85591356\n",
      "Iteration 59949, loss = 1.55429008\n",
      "Iteration 59950, loss = 1.36312589\n",
      "Iteration 59951, loss = 1.28364132\n",
      "Iteration 59952, loss = 1.18626964\n",
      "Iteration 59953, loss = 1.27065155\n",
      "Iteration 59954, loss = 1.40936291\n",
      "Iteration 59955, loss = 1.64344749\n",
      "Iteration 59956, loss = 2.03226794\n",
      "Iteration 59957, loss = 1.47468092\n",
      "Iteration 59958, loss = 1.40175989\n",
      "Iteration 59959, loss = 1.60613719\n",
      "Iteration 59960, loss = 1.38557061\n",
      "Iteration 59961, loss = 1.66406662\n",
      "Iteration 59962, loss = 1.65293881\n",
      "Iteration 59963, loss = 2.02273574\n",
      "Iteration 59964, loss = 1.61363309\n",
      "Iteration 59965, loss = 1.32574499\n",
      "Iteration 59966, loss = 1.24927707\n",
      "Iteration 59967, loss = 1.18502650\n",
      "Iteration 59968, loss = 1.25191202\n",
      "Iteration 59969, loss = 1.46669421\n",
      "Iteration 59970, loss = 1.82189703\n",
      "Iteration 59971, loss = 2.01986867\n",
      "Iteration 59972, loss = 2.33319484\n",
      "Iteration 59973, loss = 3.28913228\n",
      "Iteration 59974, loss = 4.79583370\n",
      "Iteration 59975, loss = 6.42324037\n",
      "Iteration 59976, loss = 6.12760393\n",
      "Iteration 59977, loss = 6.80359408\n",
      "Iteration 59978, loss = 4.31506443\n",
      "Iteration 59979, loss = 3.89478191\n",
      "Iteration 59980, loss = 2.47629147\n",
      "Iteration 59981, loss = 2.44490434\n",
      "Iteration 59982, loss = 2.86152198\n",
      "Iteration 59983, loss = 1.86972815\n",
      "Iteration 59984, loss = 1.59054235\n",
      "Iteration 59985, loss = 1.56807075\n",
      "Iteration 59986, loss = 1.38987804\n",
      "Iteration 59987, loss = 1.29821104\n",
      "Iteration 59988, loss = 1.37062393\n",
      "Iteration 59989, loss = 1.36018681\n",
      "Iteration 59990, loss = 1.50299845\n",
      "Iteration 59991, loss = 1.50994438\n",
      "Iteration 59992, loss = 1.53996809\n",
      "Iteration 59993, loss = 1.37166132\n",
      "Iteration 59994, loss = 1.30969403\n",
      "Iteration 59995, loss = 1.26115815\n",
      "Iteration 59996, loss = 1.28393557\n",
      "Iteration 59997, loss = 1.25836184\n",
      "Iteration 59998, loss = 1.31443789\n",
      "Iteration 59999, loss = 1.29552556\n",
      "Iteration 60000, loss = 1.28257928\n",
      "Iteration 60001, loss = 1.23960479\n",
      "Iteration 60002, loss = 1.27813884\n",
      "Iteration 60003, loss = 1.22430223\n",
      "Iteration 60004, loss = 1.25280474\n",
      "Iteration 60005, loss = 1.30146608\n",
      "Iteration 60006, loss = 1.33355661\n",
      "Iteration 60007, loss = 1.33780538\n",
      "Iteration 60008, loss = 1.28609833\n",
      "Iteration 60009, loss = 1.38850755\n",
      "Iteration 60010, loss = 1.27381076\n",
      "Iteration 60011, loss = 1.57985365\n",
      "Iteration 60012, loss = 1.56088730\n",
      "Iteration 60013, loss = 1.41540702\n",
      "Iteration 60014, loss = 1.33601167\n",
      "Iteration 60015, loss = 1.44851555\n",
      "Iteration 60016, loss = 1.62962444\n",
      "Iteration 60017, loss = 1.29080521\n",
      "Iteration 60018, loss = 1.74981530\n",
      "Iteration 60019, loss = 1.52872304\n",
      "Iteration 60020, loss = 1.58075405\n",
      "Iteration 60021, loss = 1.45900272\n",
      "Iteration 60022, loss = 1.38729392\n",
      "Iteration 60023, loss = 1.23220587\n",
      "Iteration 60024, loss = 1.48897614\n",
      "Iteration 60025, loss = 1.54700136\n",
      "Iteration 60026, loss = 1.42553103\n",
      "Iteration 60027, loss = 1.31906232\n",
      "Iteration 60028, loss = 1.20989562\n",
      "Iteration 60029, loss = 1.21046549\n",
      "Iteration 60030, loss = 1.46967399\n",
      "Iteration 60031, loss = 2.21969269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 60032, loss = 2.12358225\n",
      "Iteration 60033, loss = 1.80282388\n",
      "Iteration 60034, loss = 1.77222302\n",
      "Iteration 60035, loss = 1.30904595\n",
      "Iteration 60036, loss = 1.28371596\n",
      "Iteration 60037, loss = 1.53885807\n",
      "Iteration 60038, loss = 1.57845990\n",
      "Iteration 60039, loss = 1.50747187\n",
      "Iteration 60040, loss = 1.45266679\n",
      "Iteration 60041, loss = 1.61106032\n",
      "Iteration 60042, loss = 1.49562861\n",
      "Iteration 60043, loss = 1.32125551\n",
      "Iteration 60044, loss = 1.36923682\n",
      "Iteration 60045, loss = 1.34562229\n",
      "Iteration 60046, loss = 1.31544046\n",
      "Iteration 60047, loss = 1.25235388\n",
      "Iteration 60048, loss = 1.37481087\n",
      "Iteration 60049, loss = 1.30360780\n",
      "Iteration 60050, loss = 1.52845698\n",
      "Iteration 60051, loss = 1.44246621\n",
      "Iteration 60052, loss = 1.33051839\n",
      "Iteration 60053, loss = 1.63258321\n",
      "Iteration 60054, loss = 2.47585073\n",
      "Iteration 60055, loss = 2.07255764\n",
      "Iteration 60056, loss = 1.62715186\n",
      "Iteration 60057, loss = 1.47803360\n",
      "Iteration 60058, loss = 1.36665828\n",
      "Iteration 60059, loss = 1.46600004\n",
      "Iteration 60060, loss = 1.61458382\n",
      "Iteration 60061, loss = 1.75297137\n",
      "Iteration 60062, loss = 1.95729362\n",
      "Iteration 60063, loss = 1.56763260\n",
      "Iteration 60064, loss = 1.62375989\n",
      "Iteration 60065, loss = 1.47170568\n",
      "Iteration 60066, loss = 1.32498818\n",
      "Iteration 60067, loss = 1.32447328\n",
      "Iteration 60068, loss = 1.19123680\n",
      "Iteration 60069, loss = 1.34427997\n",
      "Iteration 60070, loss = 1.38544432\n",
      "Iteration 60071, loss = 1.22961043\n",
      "Iteration 60072, loss = 1.31973932\n",
      "Iteration 60073, loss = 1.49668478\n",
      "Iteration 60074, loss = 1.36542245\n",
      "Iteration 60075, loss = 1.35823895\n",
      "Iteration 60076, loss = 1.51764872\n",
      "Iteration 60077, loss = 1.79822356\n",
      "Iteration 60078, loss = 1.57184082\n",
      "Iteration 60079, loss = 1.61234477\n",
      "Iteration 60080, loss = 1.68802725\n",
      "Iteration 60081, loss = 1.88896356\n",
      "Iteration 60082, loss = 1.72074288\n",
      "Iteration 60083, loss = 1.50202145\n",
      "Iteration 60084, loss = 1.66412140\n",
      "Iteration 60085, loss = 1.69739788\n",
      "Iteration 60086, loss = 1.71234416\n",
      "Iteration 60087, loss = 1.48055286\n",
      "Iteration 60088, loss = 1.56435257\n",
      "Iteration 60089, loss = 1.66609667\n",
      "Iteration 60090, loss = 1.39605825\n",
      "Iteration 60091, loss = 1.42536697\n",
      "Iteration 60092, loss = 1.51810880\n",
      "Iteration 60093, loss = 1.58641484\n",
      "Iteration 60094, loss = 1.51899544\n",
      "Iteration 60095, loss = 1.58402735\n",
      "Iteration 60096, loss = 1.42443437\n",
      "Iteration 60097, loss = 1.92848008\n",
      "Iteration 60098, loss = 1.81644354\n",
      "Iteration 60099, loss = 1.47757117\n",
      "Iteration 60100, loss = 1.47573998\n",
      "Iteration 60101, loss = 1.55708884\n",
      "Iteration 60102, loss = 1.37895562\n",
      "Iteration 60103, loss = 1.21613149\n",
      "Iteration 60104, loss = 1.18614816\n",
      "Iteration 60105, loss = 1.26094065\n",
      "Iteration 60106, loss = 1.30448045\n",
      "Iteration 60107, loss = 1.50942896\n",
      "Iteration 60108, loss = 1.63682920\n",
      "Iteration 60109, loss = 2.20920192\n",
      "Iteration 60110, loss = 1.73002338\n",
      "Iteration 60111, loss = 2.25837987\n",
      "Iteration 60112, loss = 2.21272117\n",
      "Iteration 60113, loss = 2.26221819\n",
      "Iteration 60114, loss = 1.68764313\n",
      "Iteration 60115, loss = 1.58313570\n",
      "Iteration 60116, loss = 1.35063368\n",
      "Iteration 60117, loss = 1.49492826\n",
      "Iteration 60118, loss = 1.86977954\n",
      "Iteration 60119, loss = 2.17158084\n",
      "Iteration 60120, loss = 1.67160957\n",
      "Iteration 60121, loss = 1.71066775\n",
      "Iteration 60122, loss = 2.10930639\n",
      "Iteration 60123, loss = 2.00171122\n",
      "Iteration 60124, loss = 1.93296967\n",
      "Iteration 60125, loss = 1.53771629\n",
      "Iteration 60126, loss = 1.65878172\n",
      "Iteration 60127, loss = 1.61253515\n",
      "Iteration 60128, loss = 1.45137946\n",
      "Iteration 60129, loss = 1.56226500\n",
      "Iteration 60130, loss = 1.34316249\n",
      "Iteration 60131, loss = 1.43057989\n",
      "Iteration 60132, loss = 1.41023472\n",
      "Iteration 60133, loss = 1.44860249\n",
      "Iteration 60134, loss = 1.31264692\n",
      "Iteration 60135, loss = 1.46358050\n",
      "Iteration 60136, loss = 1.71102230\n",
      "Iteration 60137, loss = 2.52306296\n",
      "Iteration 60138, loss = 3.08635551\n",
      "Iteration 60139, loss = 3.51658211\n",
      "Iteration 60140, loss = 2.75326730\n",
      "Iteration 60141, loss = 2.44682989\n",
      "Iteration 60142, loss = 2.20701903\n",
      "Iteration 60143, loss = 1.55685842\n",
      "Iteration 60144, loss = 1.65392046\n",
      "Iteration 60145, loss = 1.77006226\n",
      "Iteration 60146, loss = 2.23980585\n",
      "Iteration 60147, loss = 2.41560761\n",
      "Iteration 60148, loss = 1.85111808\n",
      "Iteration 60149, loss = 1.44648850\n",
      "Iteration 60150, loss = 1.69538037\n",
      "Iteration 60151, loss = 1.51170167\n",
      "Iteration 60152, loss = 1.55278450\n",
      "Iteration 60153, loss = 1.96972602\n",
      "Iteration 60154, loss = 1.81063898\n",
      "Iteration 60155, loss = 1.96482583\n",
      "Iteration 60156, loss = 3.10303966\n",
      "Iteration 60157, loss = 2.05965785\n",
      "Iteration 60158, loss = 1.87743584\n",
      "Iteration 60159, loss = 1.58508248\n",
      "Iteration 60160, loss = 1.55611749\n",
      "Iteration 60161, loss = 1.31651751\n",
      "Iteration 60162, loss = 1.34175243\n",
      "Iteration 60163, loss = 1.56761325\n",
      "Iteration 60164, loss = 1.75986909\n",
      "Iteration 60165, loss = 1.62848634\n",
      "Iteration 60166, loss = 1.45777233\n",
      "Iteration 60167, loss = 1.89967968\n",
      "Iteration 60168, loss = 1.62476128\n",
      "Iteration 60169, loss = 1.49900913\n",
      "Iteration 60170, loss = 1.35547807\n",
      "Iteration 60171, loss = 1.26884011\n",
      "Iteration 60172, loss = 1.24198636\n",
      "Iteration 60173, loss = 1.36046584\n",
      "Iteration 60174, loss = 1.42415970\n",
      "Iteration 60175, loss = 1.68967739\n",
      "Iteration 60176, loss = 1.73370744\n",
      "Iteration 60177, loss = 1.58522382\n",
      "Iteration 60178, loss = 1.51187212\n",
      "Iteration 60179, loss = 1.69510209\n",
      "Iteration 60180, loss = 2.04455354\n",
      "Iteration 60181, loss = 1.63580181\n",
      "Iteration 60182, loss = 1.80749111\n",
      "Iteration 60183, loss = 1.61272013\n",
      "Iteration 60184, loss = 1.58396768\n",
      "Iteration 60185, loss = 1.44313394\n",
      "Iteration 60186, loss = 1.41938730\n",
      "Iteration 60187, loss = 1.27193851\n",
      "Iteration 60188, loss = 1.32153821\n",
      "Iteration 60189, loss = 1.71588307\n",
      "Iteration 60190, loss = 1.62418779\n",
      "Iteration 60191, loss = 1.41111491\n",
      "Iteration 60192, loss = 1.37134455\n",
      "Iteration 60193, loss = 1.57345243\n",
      "Iteration 60194, loss = 1.63078808\n",
      "Iteration 60195, loss = 1.64367414\n",
      "Iteration 60196, loss = 1.70516282\n",
      "Iteration 60197, loss = 2.02758589\n",
      "Iteration 60198, loss = 2.06225995\n",
      "Iteration 60199, loss = 2.44778442\n",
      "Iteration 60200, loss = 3.50452707\n",
      "Iteration 60201, loss = 2.60330380\n",
      "Iteration 60202, loss = 2.10676855\n",
      "Iteration 60203, loss = 2.07551639\n",
      "Iteration 60204, loss = 1.85058695\n",
      "Iteration 60205, loss = 1.61867598\n",
      "Iteration 60206, loss = 1.61899463\n",
      "Iteration 60207, loss = 1.63066317\n",
      "Iteration 60208, loss = 1.87198081\n",
      "Iteration 60209, loss = 1.91290635\n",
      "Iteration 60210, loss = 1.96533636\n",
      "Iteration 60211, loss = 2.05601612\n",
      "Iteration 60212, loss = 2.63955833\n",
      "Iteration 60213, loss = 3.21569225\n",
      "Iteration 60214, loss = 2.87500598\n",
      "Iteration 60215, loss = 2.41391920\n",
      "Iteration 60216, loss = 1.83491043\n",
      "Iteration 60217, loss = 1.78356126\n",
      "Iteration 60218, loss = 1.56183769\n",
      "Iteration 60219, loss = 1.27979758\n",
      "Iteration 60220, loss = 1.33190815\n",
      "Iteration 60221, loss = 1.52838040\n",
      "Iteration 60222, loss = 1.63741300\n",
      "Iteration 60223, loss = 2.13716174\n",
      "Iteration 60224, loss = 2.02088766\n",
      "Iteration 60225, loss = 1.68170350\n",
      "Iteration 60226, loss = 1.86881175\n",
      "Iteration 60227, loss = 2.08779622\n",
      "Iteration 60228, loss = 1.67953158\n",
      "Iteration 60229, loss = 1.33101241\n",
      "Iteration 60230, loss = 1.36233229\n",
      "Iteration 60231, loss = 1.24296262\n",
      "Iteration 60232, loss = 1.25362939\n",
      "Iteration 60233, loss = 1.35013930\n",
      "Iteration 60234, loss = 1.44303456\n",
      "Iteration 60235, loss = 1.59524852\n",
      "Iteration 60236, loss = 1.57461118\n",
      "Iteration 60237, loss = 1.52662123\n",
      "Iteration 60238, loss = 1.33593721\n",
      "Iteration 60239, loss = 1.39553442\n",
      "Iteration 60240, loss = 1.60400295\n",
      "Iteration 60241, loss = 1.73051007\n",
      "Iteration 60242, loss = 1.52324076\n",
      "Iteration 60243, loss = 1.43162866\n",
      "Iteration 60244, loss = 1.61660351\n",
      "Iteration 60245, loss = 1.62910499\n",
      "Iteration 60246, loss = 1.47300669\n",
      "Iteration 60247, loss = 1.51624118\n",
      "Iteration 60248, loss = 1.37755772\n",
      "Iteration 60249, loss = 1.53076921\n",
      "Iteration 60250, loss = 1.55269256\n",
      "Iteration 60251, loss = 1.66912247\n",
      "Iteration 60252, loss = 1.79488343\n",
      "Iteration 60253, loss = 1.96063154\n",
      "Iteration 60254, loss = 1.51308628\n",
      "Iteration 60255, loss = 1.58300908\n",
      "Iteration 60256, loss = 1.36733013\n",
      "Iteration 60257, loss = 1.29186360\n",
      "Iteration 60258, loss = 1.54755542\n",
      "Iteration 60259, loss = 1.49912463\n",
      "Iteration 60260, loss = 1.38451347\n",
      "Iteration 60261, loss = 1.23352018\n",
      "Iteration 60262, loss = 1.55352582\n",
      "Iteration 60263, loss = 1.57424013\n",
      "Iteration 60264, loss = 1.48615692\n",
      "Iteration 60265, loss = 1.28558911\n",
      "Iteration 60266, loss = 1.35934107\n",
      "Iteration 60267, loss = 1.21176880\n",
      "Iteration 60268, loss = 1.56751420\n",
      "Iteration 60269, loss = 1.51761713\n",
      "Iteration 60270, loss = 1.46584820\n",
      "Iteration 60271, loss = 1.32289889\n",
      "Iteration 60272, loss = 1.31362093\n",
      "Iteration 60273, loss = 1.51187737\n",
      "Iteration 60274, loss = 1.73486459\n",
      "Iteration 60275, loss = 2.04703359\n",
      "Iteration 60276, loss = 2.92308562\n",
      "Iteration 60277, loss = 4.11167483\n",
      "Iteration 60278, loss = 3.15204033\n",
      "Iteration 60279, loss = 3.00128728\n",
      "Iteration 60280, loss = 4.37021694\n",
      "Iteration 60281, loss = 3.76518301\n",
      "Iteration 60282, loss = 3.23887629\n",
      "Iteration 60283, loss = 2.78573716\n",
      "Iteration 60284, loss = 2.23512252\n",
      "Iteration 60285, loss = 1.71284372\n",
      "Iteration 60286, loss = 1.73186639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 60287, loss = 1.87056672\n",
      "Iteration 60288, loss = 2.20410746\n",
      "Iteration 60289, loss = 1.94382326\n",
      "Iteration 60290, loss = 1.46446381\n",
      "Iteration 60291, loss = 1.42272346\n",
      "Iteration 60292, loss = 1.51360664\n",
      "Iteration 60293, loss = 1.83542414\n",
      "Iteration 60294, loss = 1.42933182\n",
      "Iteration 60295, loss = 1.53722395\n",
      "Iteration 60296, loss = 1.31982740\n",
      "Iteration 60297, loss = 1.49960898\n",
      "Iteration 60298, loss = 1.35542900\n",
      "Iteration 60299, loss = 1.46670764\n",
      "Iteration 60300, loss = 1.40933779\n",
      "Iteration 60301, loss = 1.84863275\n",
      "Iteration 60302, loss = 1.97535284\n",
      "Iteration 60303, loss = 2.01848375\n",
      "Iteration 60304, loss = 1.62102063\n",
      "Iteration 60305, loss = 1.69633940\n",
      "Iteration 60306, loss = 2.24458193\n",
      "Iteration 60307, loss = 1.70357706\n",
      "Iteration 60308, loss = 1.73011073\n",
      "Iteration 60309, loss = 1.67904491\n",
      "Iteration 60310, loss = 1.51178840\n",
      "Iteration 60311, loss = 1.40858212\n",
      "Iteration 60312, loss = 1.49545875\n",
      "Iteration 60313, loss = 1.49988499\n",
      "Iteration 60314, loss = 1.40129554\n",
      "Iteration 60315, loss = 1.41181609\n",
      "Iteration 60316, loss = 1.67312595\n",
      "Iteration 60317, loss = 1.83964015\n",
      "Iteration 60318, loss = 1.68263760\n",
      "Iteration 60319, loss = 1.60883835\n",
      "Iteration 60320, loss = 1.53206842\n",
      "Iteration 60321, loss = 1.52667002\n",
      "Iteration 60322, loss = 1.49817554\n",
      "Iteration 60323, loss = 1.51754617\n",
      "Iteration 60324, loss = 2.13850678\n",
      "Iteration 60325, loss = 1.91638584\n",
      "Iteration 60326, loss = 1.84015271\n",
      "Iteration 60327, loss = 1.56343717\n",
      "Iteration 60328, loss = 1.65055918\n",
      "Iteration 60329, loss = 2.19204218\n",
      "Iteration 60330, loss = 2.29779554\n",
      "Iteration 60331, loss = 2.27717352\n",
      "Iteration 60332, loss = 1.62860311\n",
      "Iteration 60333, loss = 1.53732549\n",
      "Iteration 60334, loss = 1.51081459\n",
      "Iteration 60335, loss = 1.74475357\n",
      "Iteration 60336, loss = 1.86702380\n",
      "Iteration 60337, loss = 1.50197608\n",
      "Iteration 60338, loss = 1.78265299\n",
      "Iteration 60339, loss = 1.72174316\n",
      "Iteration 60340, loss = 1.48014247\n",
      "Iteration 60341, loss = 1.63063516\n",
      "Iteration 60342, loss = 1.63351121\n",
      "Iteration 60343, loss = 1.25089030\n",
      "Iteration 60344, loss = 1.24799634\n",
      "Iteration 60345, loss = 1.24285472\n",
      "Iteration 60346, loss = 1.27658128\n",
      "Iteration 60347, loss = 1.28521351\n",
      "Iteration 60348, loss = 1.57752867\n",
      "Iteration 60349, loss = 1.67903987\n",
      "Iteration 60350, loss = 1.49746973\n",
      "Iteration 60351, loss = 1.55196411\n",
      "Iteration 60352, loss = 1.57016442\n",
      "Iteration 60353, loss = 1.87248525\n",
      "Iteration 60354, loss = 1.72217312\n",
      "Iteration 60355, loss = 1.51373291\n",
      "Iteration 60356, loss = 1.43933322\n",
      "Iteration 60357, loss = 1.40989714\n",
      "Iteration 60358, loss = 1.37058829\n",
      "Iteration 60359, loss = 1.34032512\n",
      "Iteration 60360, loss = 1.35751252\n",
      "Iteration 60361, loss = 2.09007445\n",
      "Iteration 60362, loss = 1.59699639\n",
      "Iteration 60363, loss = 1.49678306\n",
      "Iteration 60364, loss = 1.19925448\n",
      "Iteration 60365, loss = 1.71201760\n",
      "Iteration 60366, loss = 2.26221662\n",
      "Iteration 60367, loss = 2.79285078\n",
      "Iteration 60368, loss = 1.68658629\n",
      "Iteration 60369, loss = 1.79138792\n",
      "Iteration 60370, loss = 2.06498591\n",
      "Iteration 60371, loss = 1.68091120\n",
      "Iteration 60372, loss = 1.44544541\n",
      "Iteration 60373, loss = 1.43234220\n",
      "Iteration 60374, loss = 1.83548205\n",
      "Iteration 60375, loss = 1.78982927\n",
      "Iteration 60376, loss = 1.56210285\n",
      "Iteration 60377, loss = 1.48192927\n",
      "Iteration 60378, loss = 1.53060146\n",
      "Iteration 60379, loss = 1.51378579\n",
      "Iteration 60380, loss = 1.93154086\n",
      "Iteration 60381, loss = 1.61526939\n",
      "Iteration 60382, loss = 1.62853844\n",
      "Iteration 60383, loss = 1.49480997\n",
      "Iteration 60384, loss = 1.48128838\n",
      "Iteration 60385, loss = 1.29282949\n",
      "Iteration 60386, loss = 1.51125797\n",
      "Iteration 60387, loss = 1.39141935\n",
      "Iteration 60388, loss = 1.57047663\n",
      "Iteration 60389, loss = 1.42529828\n",
      "Iteration 60390, loss = 1.55447295\n",
      "Iteration 60391, loss = 1.59081192\n",
      "Iteration 60392, loss = 1.54345513\n",
      "Iteration 60393, loss = 1.42415323\n",
      "Iteration 60394, loss = 1.56142470\n",
      "Iteration 60395, loss = 1.38407800\n",
      "Iteration 60396, loss = 1.29008069\n",
      "Iteration 60397, loss = 1.44189478\n",
      "Iteration 60398, loss = 1.29798569\n",
      "Iteration 60399, loss = 1.40776774\n",
      "Iteration 60400, loss = 1.38282895\n",
      "Iteration 60401, loss = 1.34769180\n",
      "Iteration 60402, loss = 1.37039032\n",
      "Iteration 60403, loss = 1.95295017\n",
      "Iteration 60404, loss = 2.23343996\n",
      "Iteration 60405, loss = 2.59293159\n",
      "Iteration 60406, loss = 1.54354016\n",
      "Iteration 60407, loss = 2.11130513\n",
      "Iteration 60408, loss = 1.92003578\n",
      "Iteration 60409, loss = 2.06478056\n",
      "Iteration 60410, loss = 2.14072292\n",
      "Iteration 60411, loss = 2.00689208\n",
      "Iteration 60412, loss = 2.14300057\n",
      "Iteration 60413, loss = 1.64867902\n",
      "Iteration 60414, loss = 1.55893759\n",
      "Iteration 60415, loss = 1.92315989\n",
      "Iteration 60416, loss = 1.56347817\n",
      "Iteration 60417, loss = 1.58448057\n",
      "Iteration 60418, loss = 1.48462396\n",
      "Iteration 60419, loss = 1.57096934\n",
      "Iteration 60420, loss = 2.29351693\n",
      "Iteration 60421, loss = 1.65384868\n",
      "Iteration 60422, loss = 1.56831634\n",
      "Iteration 60423, loss = 1.64772657\n",
      "Iteration 60424, loss = 1.76317200\n",
      "Iteration 60425, loss = 2.42281702\n",
      "Iteration 60426, loss = 3.10705611\n",
      "Iteration 60427, loss = 2.24289259\n",
      "Iteration 60428, loss = 1.91946429\n",
      "Iteration 60429, loss = 2.24410013\n",
      "Iteration 60430, loss = 2.14045931\n",
      "Iteration 60431, loss = 1.91187713\n",
      "Iteration 60432, loss = 1.50017438\n",
      "Iteration 60433, loss = 1.30578382\n",
      "Iteration 60434, loss = 1.47763953\n",
      "Iteration 60435, loss = 1.57149746\n",
      "Iteration 60436, loss = 1.42067376\n",
      "Iteration 60437, loss = 1.31773335\n",
      "Iteration 60438, loss = 1.37341214\n",
      "Iteration 60439, loss = 1.32815739\n",
      "Iteration 60440, loss = 1.38995527\n",
      "Iteration 60441, loss = 1.42703844\n",
      "Iteration 60442, loss = 1.20551523\n",
      "Iteration 60443, loss = 1.27224625\n",
      "Iteration 60444, loss = 1.38621755\n",
      "Iteration 60445, loss = 1.54882894\n",
      "Iteration 60446, loss = 1.52094825\n",
      "Iteration 60447, loss = 1.37215189\n",
      "Iteration 60448, loss = 1.62394907\n",
      "Iteration 60449, loss = 2.04130628\n",
      "Iteration 60450, loss = 1.82591285\n",
      "Iteration 60451, loss = 1.49482637\n",
      "Iteration 60452, loss = 1.53281475\n",
      "Iteration 60453, loss = 1.51256013\n",
      "Iteration 60454, loss = 1.32678155\n",
      "Iteration 60455, loss = 1.23855389\n",
      "Iteration 60456, loss = 1.27619904\n",
      "Iteration 60457, loss = 1.54921029\n",
      "Iteration 60458, loss = 1.39476906\n",
      "Iteration 60459, loss = 1.30699087\n",
      "Iteration 60460, loss = 1.35291286\n",
      "Iteration 60461, loss = 1.47655454\n",
      "Iteration 60462, loss = 1.40133409\n",
      "Iteration 60463, loss = 1.52649081\n",
      "Iteration 60464, loss = 1.61900274\n",
      "Iteration 60465, loss = 1.54107208\n",
      "Iteration 60466, loss = 1.54630468\n",
      "Iteration 60467, loss = 1.41288780\n",
      "Iteration 60468, loss = 1.52125743\n",
      "Iteration 60469, loss = 1.70831813\n",
      "Iteration 60470, loss = 1.33102268\n",
      "Iteration 60471, loss = 1.50398411\n",
      "Iteration 60472, loss = 1.43645164\n",
      "Iteration 60473, loss = 1.25952665\n",
      "Iteration 60474, loss = 1.21498315\n",
      "Iteration 60475, loss = 1.31603887\n",
      "Iteration 60476, loss = 1.31228913\n",
      "Iteration 60477, loss = 1.50368511\n",
      "Iteration 60478, loss = 1.50481165\n",
      "Iteration 60479, loss = 1.26426809\n",
      "Iteration 60480, loss = 1.36531072\n",
      "Iteration 60481, loss = 1.40373697\n",
      "Iteration 60482, loss = 1.57114039\n",
      "Iteration 60483, loss = 1.97873324\n",
      "Iteration 60484, loss = 1.64506420\n",
      "Iteration 60485, loss = 1.63978830\n",
      "Iteration 60486, loss = 1.36685176\n",
      "Iteration 60487, loss = 1.69419351\n",
      "Iteration 60488, loss = 1.38697932\n",
      "Iteration 60489, loss = 1.49601493\n",
      "Iteration 60490, loss = 1.31421946\n",
      "Iteration 60491, loss = 1.22220883\n",
      "Iteration 60492, loss = 1.38863967\n",
      "Iteration 60493, loss = 1.48752199\n",
      "Iteration 60494, loss = 1.50461218\n",
      "Iteration 60495, loss = 1.34903162\n",
      "Iteration 60496, loss = 1.30559369\n",
      "Iteration 60497, loss = 1.30254510\n",
      "Iteration 60498, loss = 1.33728696\n",
      "Iteration 60499, loss = 1.31193424\n",
      "Iteration 60500, loss = 1.26113377\n",
      "Iteration 60501, loss = 1.29289417\n",
      "Iteration 60502, loss = 1.27462060\n",
      "Iteration 60503, loss = 1.26250186\n",
      "Iteration 60504, loss = 1.32567047\n",
      "Iteration 60505, loss = 1.31562505\n",
      "Iteration 60506, loss = 1.27565645\n",
      "Iteration 60507, loss = 1.31455289\n",
      "Iteration 60508, loss = 1.43623710\n",
      "Iteration 60509, loss = 1.72689533\n",
      "Iteration 60510, loss = 1.84533121\n",
      "Iteration 60511, loss = 1.72648234\n",
      "Iteration 60512, loss = 2.16306721\n",
      "Iteration 60513, loss = 5.02507939\n",
      "Iteration 60514, loss = 6.52751191\n",
      "Iteration 60515, loss = 3.74546259\n",
      "Iteration 60516, loss = 4.02773611\n",
      "Iteration 60517, loss = 3.81118323\n",
      "Iteration 60518, loss = 2.76945067\n",
      "Iteration 60519, loss = 2.00224846\n",
      "Iteration 60520, loss = 2.38126756\n",
      "Iteration 60521, loss = 2.31983334\n",
      "Iteration 60522, loss = 2.04727151\n",
      "Iteration 60523, loss = 1.87232887\n",
      "Iteration 60524, loss = 1.93350227\n",
      "Iteration 60525, loss = 1.51194970\n",
      "Iteration 60526, loss = 1.39430312\n",
      "Iteration 60527, loss = 1.41313999\n",
      "Iteration 60528, loss = 1.54152397\n",
      "Iteration 60529, loss = 1.41927818\n",
      "Iteration 60530, loss = 1.34202778\n",
      "Iteration 60531, loss = 1.38290041\n",
      "Iteration 60532, loss = 1.36791624\n",
      "Iteration 60533, loss = 1.30873742\n",
      "Iteration 60534, loss = 1.37362801\n",
      "Iteration 60535, loss = 1.31123470\n",
      "Iteration 60536, loss = 1.37800787\n",
      "Iteration 60537, loss = 1.78393303\n",
      "Iteration 60538, loss = 1.63333788\n",
      "Iteration 60539, loss = 1.72401954\n",
      "Iteration 60540, loss = 1.73162758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 60541, loss = 2.05255633\n",
      "Iteration 60542, loss = 1.57375627\n",
      "Iteration 60543, loss = 1.41904451\n",
      "Iteration 60544, loss = 1.49841032\n",
      "Iteration 60545, loss = 1.60549975\n",
      "Iteration 60546, loss = 1.46224866\n",
      "Iteration 60547, loss = 1.43419741\n",
      "Iteration 60548, loss = 1.29778650\n",
      "Iteration 60549, loss = 1.30333779\n",
      "Iteration 60550, loss = 1.78088833\n",
      "Iteration 60551, loss = 1.83287595\n",
      "Iteration 60552, loss = 1.57400380\n",
      "Iteration 60553, loss = 1.47918164\n",
      "Iteration 60554, loss = 1.42908215\n",
      "Iteration 60555, loss = 1.64287383\n",
      "Iteration 60556, loss = 1.85535175\n",
      "Iteration 60557, loss = 1.25141163\n",
      "Iteration 60558, loss = 1.47936357\n",
      "Iteration 60559, loss = 1.72547904\n",
      "Iteration 60560, loss = 1.71044483\n",
      "Iteration 60561, loss = 1.54578156\n",
      "Iteration 60562, loss = 2.35804115\n",
      "Iteration 60563, loss = 1.80915682\n",
      "Iteration 60564, loss = 1.87374635\n",
      "Iteration 60565, loss = 1.93431533\n",
      "Iteration 60566, loss = 1.70308490\n",
      "Iteration 60567, loss = 1.40919291\n",
      "Iteration 60568, loss = 1.47496511\n",
      "Iteration 60569, loss = 1.42308556\n",
      "Iteration 60570, loss = 1.29852088\n",
      "Iteration 60571, loss = 1.49718567\n",
      "Iteration 60572, loss = 1.65813933\n",
      "Iteration 60573, loss = 1.67889924\n",
      "Iteration 60574, loss = 2.18225231\n",
      "Iteration 60575, loss = 2.89578998\n",
      "Iteration 60576, loss = 3.37605809\n",
      "Iteration 60577, loss = 3.11405266\n",
      "Iteration 60578, loss = 4.92888804\n",
      "Iteration 60579, loss = 5.51691693\n",
      "Iteration 60580, loss = 5.32872577\n",
      "Iteration 60581, loss = 5.12984336\n",
      "Iteration 60582, loss = 6.40804598\n",
      "Iteration 60583, loss = 7.98873608\n",
      "Iteration 60584, loss = 3.83279664\n",
      "Iteration 60585, loss = 2.49814511\n",
      "Iteration 60586, loss = 3.19780452\n",
      "Iteration 60587, loss = 3.17933752\n",
      "Iteration 60588, loss = 3.11398714\n",
      "Iteration 60589, loss = 2.90460891\n",
      "Iteration 60590, loss = 2.32798681\n",
      "Iteration 60591, loss = 2.09220435\n",
      "Iteration 60592, loss = 1.67734565\n",
      "Iteration 60593, loss = 1.53380979\n",
      "Iteration 60594, loss = 1.41399000\n",
      "Iteration 60595, loss = 1.26856187\n",
      "Iteration 60596, loss = 1.27868787\n",
      "Iteration 60597, loss = 1.33770188\n",
      "Iteration 60598, loss = 1.33242393\n",
      "Iteration 60599, loss = 1.36175029\n",
      "Iteration 60600, loss = 1.19533997\n",
      "Iteration 60601, loss = 1.29282819\n",
      "Iteration 60602, loss = 1.27744380\n",
      "Iteration 60603, loss = 1.23035796\n",
      "Iteration 60604, loss = 1.36617864\n",
      "Iteration 60605, loss = 1.40670355\n",
      "Iteration 60606, loss = 1.41779727\n",
      "Iteration 60607, loss = 1.77514585\n",
      "Iteration 60608, loss = 1.77685620\n",
      "Iteration 60609, loss = 1.60851631\n",
      "Iteration 60610, loss = 1.54076046\n",
      "Iteration 60611, loss = 1.41922420\n",
      "Iteration 60612, loss = 1.27541312\n",
      "Iteration 60613, loss = 1.18809882\n",
      "Iteration 60614, loss = 1.23299566\n",
      "Iteration 60615, loss = 1.57608416\n",
      "Iteration 60616, loss = 1.74041695\n",
      "Iteration 60617, loss = 1.33862211\n",
      "Iteration 60618, loss = 1.30365242\n",
      "Iteration 60619, loss = 1.35178494\n",
      "Iteration 60620, loss = 1.62381337\n",
      "Iteration 60621, loss = 1.23339674\n",
      "Iteration 60622, loss = 1.21252956\n",
      "Iteration 60623, loss = 1.33794868\n",
      "Iteration 60624, loss = 1.27490428\n",
      "Iteration 60625, loss = 1.29263360\n",
      "Iteration 60626, loss = 1.26979358\n",
      "Iteration 60627, loss = 1.24414597\n",
      "Iteration 60628, loss = 1.39057667\n",
      "Iteration 60629, loss = 1.28216838\n",
      "Iteration 60630, loss = 1.33268270\n",
      "Iteration 60631, loss = 1.26113104\n",
      "Iteration 60632, loss = 1.25899723\n",
      "Iteration 60633, loss = 1.37912918\n",
      "Iteration 60634, loss = 1.50485748\n",
      "Iteration 60635, loss = 1.58872809\n",
      "Iteration 60636, loss = 1.38282023\n",
      "Iteration 60637, loss = 1.27604215\n",
      "Iteration 60638, loss = 1.59382976\n",
      "Iteration 60639, loss = 1.40671043\n",
      "Iteration 60640, loss = 1.79608997\n",
      "Iteration 60641, loss = 2.06462970\n",
      "Iteration 60642, loss = 2.23602709\n",
      "Iteration 60643, loss = 2.01639673\n",
      "Iteration 60644, loss = 1.85463111\n",
      "Iteration 60645, loss = 2.25990000\n",
      "Iteration 60646, loss = 1.68802766\n",
      "Iteration 60647, loss = 1.63176992\n",
      "Iteration 60648, loss = 1.37985256\n",
      "Iteration 60649, loss = 1.26145597\n",
      "Iteration 60650, loss = 1.91370460\n",
      "Iteration 60651, loss = 1.39541306\n",
      "Iteration 60652, loss = 1.35851155\n",
      "Iteration 60653, loss = 1.53730553\n",
      "Iteration 60654, loss = 1.86809185\n",
      "Iteration 60655, loss = 2.03789424\n",
      "Iteration 60656, loss = 1.61472494\n",
      "Iteration 60657, loss = 1.52345329\n",
      "Iteration 60658, loss = 1.90109946\n",
      "Iteration 60659, loss = 1.64166653\n",
      "Iteration 60660, loss = 1.69973474\n",
      "Iteration 60661, loss = 1.76585427\n",
      "Iteration 60662, loss = 1.93187478\n",
      "Iteration 60663, loss = 2.35217580\n",
      "Iteration 60664, loss = 2.14697555\n",
      "Iteration 60665, loss = 2.25046957\n",
      "Iteration 60666, loss = 1.54746542\n",
      "Iteration 60667, loss = 1.37775456\n",
      "Iteration 60668, loss = 1.27275607\n",
      "Iteration 60669, loss = 1.25003261\n",
      "Iteration 60670, loss = 1.37713691\n",
      "Iteration 60671, loss = 1.88241681\n",
      "Iteration 60672, loss = 1.62385464\n",
      "Iteration 60673, loss = 1.57754872\n",
      "Iteration 60674, loss = 1.55122200\n",
      "Iteration 60675, loss = 1.34905300\n",
      "Iteration 60676, loss = 1.50134921\n",
      "Iteration 60677, loss = 1.51513895\n",
      "Iteration 60678, loss = 1.23805657\n",
      "Iteration 60679, loss = 1.20178292\n",
      "Iteration 60680, loss = 1.21063104\n",
      "Iteration 60681, loss = 1.24386736\n",
      "Iteration 60682, loss = 1.23874886\n",
      "Iteration 60683, loss = 1.45901358\n",
      "Iteration 60684, loss = 1.54908030\n",
      "Iteration 60685, loss = 1.57124290\n",
      "Iteration 60686, loss = 1.79097653\n",
      "Iteration 60687, loss = 1.60833729\n",
      "Iteration 60688, loss = 1.47449554\n",
      "Iteration 60689, loss = 1.76218853\n",
      "Iteration 60690, loss = 1.51420450\n",
      "Iteration 60691, loss = 1.64573587\n",
      "Iteration 60692, loss = 2.10981521\n",
      "Iteration 60693, loss = 1.64326475\n",
      "Iteration 60694, loss = 1.53177837\n",
      "Iteration 60695, loss = 1.54177391\n",
      "Iteration 60696, loss = 1.60310142\n",
      "Iteration 60697, loss = 1.67436413\n",
      "Iteration 60698, loss = 1.63294844\n",
      "Iteration 60699, loss = 1.49286803\n",
      "Iteration 60700, loss = 1.62536848\n",
      "Iteration 60701, loss = 1.83862958\n",
      "Iteration 60702, loss = 1.50974287\n",
      "Iteration 60703, loss = 1.35837064\n",
      "Iteration 60704, loss = 1.34948467\n",
      "Iteration 60705, loss = 1.32723587\n",
      "Iteration 60706, loss = 1.28478095\n",
      "Iteration 60707, loss = 1.40733980\n",
      "Iteration 60708, loss = 1.42872590\n",
      "Iteration 60709, loss = 1.28924307\n",
      "Iteration 60710, loss = 1.37756996\n",
      "Iteration 60711, loss = 1.51321114\n",
      "Iteration 60712, loss = 1.79792411\n",
      "Iteration 60713, loss = 1.87980882\n",
      "Iteration 60714, loss = 1.71491281\n",
      "Iteration 60715, loss = 2.18310790\n",
      "Iteration 60716, loss = 2.07457121\n",
      "Iteration 60717, loss = 2.64555084\n",
      "Iteration 60718, loss = 2.20763093\n",
      "Iteration 60719, loss = 2.64425167\n",
      "Iteration 60720, loss = 1.57847454\n",
      "Iteration 60721, loss = 1.54631784\n",
      "Iteration 60722, loss = 1.71316980\n",
      "Iteration 60723, loss = 1.75934863\n",
      "Iteration 60724, loss = 1.33697128\n",
      "Iteration 60725, loss = 1.53581236\n",
      "Iteration 60726, loss = 1.68328071\n",
      "Iteration 60727, loss = 1.64930913\n",
      "Iteration 60728, loss = 1.63072084\n",
      "Iteration 60729, loss = 1.60223252\n",
      "Iteration 60730, loss = 1.92463006\n",
      "Iteration 60731, loss = 1.82440051\n",
      "Iteration 60732, loss = 2.04263991\n",
      "Iteration 60733, loss = 1.83936328\n",
      "Iteration 60734, loss = 2.17971063\n",
      "Iteration 60735, loss = 2.64420764\n",
      "Iteration 60736, loss = 1.80002640\n",
      "Iteration 60737, loss = 1.80020150\n",
      "Iteration 60738, loss = 1.51634492\n",
      "Iteration 60739, loss = 1.42992046\n",
      "Iteration 60740, loss = 1.32844160\n",
      "Iteration 60741, loss = 1.29177804\n",
      "Iteration 60742, loss = 1.23314378\n",
      "Iteration 60743, loss = 1.28748843\n",
      "Iteration 60744, loss = 1.36506828\n",
      "Iteration 60745, loss = 1.56382276\n",
      "Iteration 60746, loss = 2.82349128\n",
      "Iteration 60747, loss = 1.78844179\n",
      "Iteration 60748, loss = 1.70421205\n",
      "Iteration 60749, loss = 2.05762585\n",
      "Iteration 60750, loss = 2.08226598\n",
      "Iteration 60751, loss = 1.74461657\n",
      "Iteration 60752, loss = 1.32247080\n",
      "Iteration 60753, loss = 1.33389782\n",
      "Iteration 60754, loss = 1.34393157\n",
      "Iteration 60755, loss = 1.41198099\n",
      "Iteration 60756, loss = 1.48358270\n",
      "Iteration 60757, loss = 1.57300947\n",
      "Iteration 60758, loss = 1.91138993\n",
      "Iteration 60759, loss = 1.59190220\n",
      "Iteration 60760, loss = 1.66493596\n",
      "Iteration 60761, loss = 1.64047667\n",
      "Iteration 60762, loss = 1.40425472\n",
      "Iteration 60763, loss = 1.51504677\n",
      "Iteration 60764, loss = 1.64150593\n",
      "Iteration 60765, loss = 1.51457300\n",
      "Iteration 60766, loss = 1.42046556\n",
      "Iteration 60767, loss = 1.58539385\n",
      "Iteration 60768, loss = 1.52943067\n",
      "Iteration 60769, loss = 2.09400965\n",
      "Iteration 60770, loss = 1.85542679\n",
      "Iteration 60771, loss = 1.52335087\n",
      "Iteration 60772, loss = 1.60160938\n",
      "Iteration 60773, loss = 1.75152790\n",
      "Iteration 60774, loss = 1.69966841\n",
      "Iteration 60775, loss = 1.33689607\n",
      "Iteration 60776, loss = 1.53668378\n",
      "Iteration 60777, loss = 1.55891781\n",
      "Iteration 60778, loss = 1.40986915\n",
      "Iteration 60779, loss = 1.74121397\n",
      "Iteration 60780, loss = 1.76192761\n",
      "Iteration 60781, loss = 1.49036791\n",
      "Iteration 60782, loss = 1.39382298\n",
      "Iteration 60783, loss = 1.33970709\n",
      "Iteration 60784, loss = 1.33884318\n",
      "Iteration 60785, loss = 1.32543784\n",
      "Iteration 60786, loss = 1.31954643\n",
      "Iteration 60787, loss = 1.17426123\n",
      "Iteration 60788, loss = 1.31454098\n",
      "Iteration 60789, loss = 1.22666757\n",
      "Iteration 60790, loss = 1.20383514\n",
      "Iteration 60791, loss = 1.26033986\n",
      "Iteration 60792, loss = 1.45218931\n",
      "Iteration 60793, loss = 1.34437707\n",
      "Iteration 60794, loss = 1.30769542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 60795, loss = 1.23583818\n",
      "Iteration 60796, loss = 1.33768228\n",
      "Iteration 60797, loss = 1.25370125\n",
      "Iteration 60798, loss = 1.21157056\n",
      "Iteration 60799, loss = 1.25792676\n",
      "Iteration 60800, loss = 1.13456468\n",
      "Iteration 60801, loss = 1.45634266\n",
      "Iteration 60802, loss = 1.72937900\n",
      "Iteration 60803, loss = 1.72582617\n",
      "Iteration 60804, loss = 1.73808571\n",
      "Iteration 60805, loss = 2.22872655\n",
      "Iteration 60806, loss = 2.07461107\n",
      "Iteration 60807, loss = 1.63193203\n",
      "Iteration 60808, loss = 1.45375805\n",
      "Iteration 60809, loss = 1.38138535\n",
      "Iteration 60810, loss = 1.29212267\n",
      "Iteration 60811, loss = 1.33477291\n",
      "Iteration 60812, loss = 1.38097359\n",
      "Iteration 60813, loss = 1.53666999\n",
      "Iteration 60814, loss = 1.18489282\n",
      "Iteration 60815, loss = 1.29345655\n",
      "Iteration 60816, loss = 1.26077248\n",
      "Iteration 60817, loss = 1.26686053\n",
      "Iteration 60818, loss = 1.25896313\n",
      "Iteration 60819, loss = 1.16992917\n",
      "Iteration 60820, loss = 1.20106862\n",
      "Iteration 60821, loss = 1.13853528\n",
      "Iteration 60822, loss = 1.17914588\n",
      "Iteration 60823, loss = 1.37976641\n",
      "Iteration 60824, loss = 1.39517868\n",
      "Iteration 60825, loss = 1.16620754\n",
      "Iteration 60826, loss = 1.21433892\n",
      "Iteration 60827, loss = 1.23714224\n",
      "Iteration 60828, loss = 1.24241644\n",
      "Iteration 60829, loss = 1.31483585\n",
      "Iteration 60830, loss = 1.44306692\n",
      "Iteration 60831, loss = 1.39162048\n",
      "Iteration 60832, loss = 1.29469083\n",
      "Iteration 60833, loss = 1.12885522\n",
      "Iteration 60834, loss = 1.18245156\n",
      "Iteration 60835, loss = 1.15138600\n",
      "Iteration 60836, loss = 1.45901564\n",
      "Iteration 60837, loss = 1.51112051\n",
      "Iteration 60838, loss = 1.76657621\n",
      "Iteration 60839, loss = 1.30504478\n",
      "Iteration 60840, loss = 1.93755151\n",
      "Iteration 60841, loss = 1.64100526\n",
      "Iteration 60842, loss = 1.54015614\n",
      "Iteration 60843, loss = 1.44325784\n",
      "Iteration 60844, loss = 1.28759607\n",
      "Iteration 60845, loss = 1.30510768\n",
      "Iteration 60846, loss = 1.34966293\n",
      "Iteration 60847, loss = 1.23121461\n",
      "Iteration 60848, loss = 1.26520190\n",
      "Iteration 60849, loss = 1.29883686\n",
      "Iteration 60850, loss = 1.27011455\n",
      "Iteration 60851, loss = 1.30889762\n",
      "Iteration 60852, loss = 1.21393508\n",
      "Iteration 60853, loss = 1.33196779\n",
      "Iteration 60854, loss = 1.34496978\n",
      "Iteration 60855, loss = 1.54553326\n",
      "Iteration 60856, loss = 1.84109470\n",
      "Iteration 60857, loss = 1.84660837\n",
      "Iteration 60858, loss = 1.72364152\n",
      "Iteration 60859, loss = 2.01818426\n",
      "Iteration 60860, loss = 1.45353877\n",
      "Iteration 60861, loss = 1.69435570\n",
      "Iteration 60862, loss = 1.48293808\n",
      "Iteration 60863, loss = 1.53197308\n",
      "Iteration 60864, loss = 1.41616548\n",
      "Iteration 60865, loss = 1.22039487\n",
      "Iteration 60866, loss = 1.35367543\n",
      "Iteration 60867, loss = 1.51952468\n",
      "Iteration 60868, loss = 1.38482434\n",
      "Iteration 60869, loss = 1.22581630\n",
      "Iteration 60870, loss = 1.42154691\n",
      "Iteration 60871, loss = 1.45523535\n",
      "Iteration 60872, loss = 1.46079438\n",
      "Iteration 60873, loss = 1.41546812\n",
      "Iteration 60874, loss = 1.32310772\n",
      "Iteration 60875, loss = 1.33108419\n",
      "Iteration 60876, loss = 1.38101197\n",
      "Iteration 60877, loss = 1.81957764\n",
      "Iteration 60878, loss = 1.88806467\n",
      "Iteration 60879, loss = 1.95641304\n",
      "Iteration 60880, loss = 1.62514665\n",
      "Iteration 60881, loss = 1.60179228\n",
      "Iteration 60882, loss = 1.88180907\n",
      "Iteration 60883, loss = 2.11970183\n",
      "Iteration 60884, loss = 1.81819910\n",
      "Iteration 60885, loss = 1.71057849\n",
      "Iteration 60886, loss = 1.86379504\n",
      "Iteration 60887, loss = 1.33014393\n",
      "Iteration 60888, loss = 1.35292144\n",
      "Iteration 60889, loss = 1.40349329\n",
      "Iteration 60890, loss = 1.39734154\n",
      "Iteration 60891, loss = 1.65291647\n",
      "Iteration 60892, loss = 1.51086949\n",
      "Iteration 60893, loss = 1.28926969\n",
      "Iteration 60894, loss = 1.23490733\n",
      "Iteration 60895, loss = 2.29539206\n",
      "Iteration 60896, loss = 2.28304540\n",
      "Iteration 60897, loss = 1.64488712\n",
      "Iteration 60898, loss = 1.68534811\n",
      "Iteration 60899, loss = 1.48492815\n",
      "Iteration 60900, loss = 1.49130817\n",
      "Iteration 60901, loss = 1.42615677\n",
      "Iteration 60902, loss = 1.24849872\n",
      "Iteration 60903, loss = 1.49036584\n",
      "Iteration 60904, loss = 1.54134185\n",
      "Iteration 60905, loss = 1.43457083\n",
      "Iteration 60906, loss = 1.21370392\n",
      "Iteration 60907, loss = 1.67987871\n",
      "Iteration 60908, loss = 2.19036551\n",
      "Iteration 60909, loss = 3.71843255\n",
      "Iteration 60910, loss = 2.62120891\n",
      "Iteration 60911, loss = 3.16201725\n",
      "Iteration 60912, loss = 3.07427001\n",
      "Iteration 60913, loss = 2.53132987\n",
      "Iteration 60914, loss = 2.23975049\n",
      "Iteration 60915, loss = 1.50486700\n",
      "Iteration 60916, loss = 1.70491672\n",
      "Iteration 60917, loss = 1.61272834\n",
      "Iteration 60918, loss = 1.66629825\n",
      "Iteration 60919, loss = 1.59035309\n",
      "Iteration 60920, loss = 1.54244530\n",
      "Iteration 60921, loss = 1.45035589\n",
      "Iteration 60922, loss = 1.60676732\n",
      "Iteration 60923, loss = 1.41637095\n",
      "Iteration 60924, loss = 1.38252105\n",
      "Iteration 60925, loss = 1.20972442\n",
      "Iteration 60926, loss = 1.19789049\n",
      "Iteration 60927, loss = 1.31109881\n",
      "Iteration 60928, loss = 1.21352432\n",
      "Iteration 60929, loss = 1.38352034\n",
      "Iteration 60930, loss = 1.62916595\n",
      "Iteration 60931, loss = 1.50203506\n",
      "Iteration 60932, loss = 1.39091031\n",
      "Iteration 60933, loss = 1.34145221\n",
      "Iteration 60934, loss = 1.47010567\n",
      "Iteration 60935, loss = 1.30185263\n",
      "Iteration 60936, loss = 1.26083093\n",
      "Iteration 60937, loss = 1.50522696\n",
      "Iteration 60938, loss = 1.43046338\n",
      "Iteration 60939, loss = 1.37427475\n",
      "Iteration 60940, loss = 1.39958866\n",
      "Iteration 60941, loss = 1.30210187\n",
      "Iteration 60942, loss = 1.27942923\n",
      "Iteration 60943, loss = 1.25388288\n",
      "Iteration 60944, loss = 1.18300675\n",
      "Iteration 60945, loss = 1.17140343\n",
      "Iteration 60946, loss = 1.37123853\n",
      "Iteration 60947, loss = 1.46677211\n",
      "Iteration 60948, loss = 1.53381269\n",
      "Iteration 60949, loss = 1.56726007\n",
      "Iteration 60950, loss = 1.78282781\n",
      "Iteration 60951, loss = 1.58496780\n",
      "Iteration 60952, loss = 1.30983028\n",
      "Iteration 60953, loss = 1.29262143\n",
      "Iteration 60954, loss = 1.34136104\n",
      "Iteration 60955, loss = 1.41324025\n",
      "Iteration 60956, loss = 1.74907025\n",
      "Iteration 60957, loss = 1.81815335\n",
      "Iteration 60958, loss = 1.84060196\n",
      "Iteration 60959, loss = 2.07829358\n",
      "Iteration 60960, loss = 1.70629983\n",
      "Iteration 60961, loss = 1.74217467\n",
      "Iteration 60962, loss = 2.02736657\n",
      "Iteration 60963, loss = 1.61156686\n",
      "Iteration 60964, loss = 1.42750644\n",
      "Iteration 60965, loss = 1.55260761\n",
      "Iteration 60966, loss = 1.37326562\n",
      "Iteration 60967, loss = 2.01713968\n",
      "Iteration 60968, loss = 1.72579562\n",
      "Iteration 60969, loss = 1.69787740\n",
      "Iteration 60970, loss = 1.54973485\n",
      "Iteration 60971, loss = 1.31273588\n",
      "Iteration 60972, loss = 1.28887376\n",
      "Iteration 60973, loss = 1.39106124\n",
      "Iteration 60974, loss = 1.27481476\n",
      "Iteration 60975, loss = 1.30459014\n",
      "Iteration 60976, loss = 1.75862527\n",
      "Iteration 60977, loss = 1.67004543\n",
      "Iteration 60978, loss = 1.92995264\n",
      "Iteration 60979, loss = 1.37317974\n",
      "Iteration 60980, loss = 1.29063062\n",
      "Iteration 60981, loss = 1.35040845\n",
      "Iteration 60982, loss = 1.17738187\n",
      "Iteration 60983, loss = 1.28369812\n",
      "Iteration 60984, loss = 1.26840068\n",
      "Iteration 60985, loss = 1.18079759\n",
      "Iteration 60986, loss = 1.33944362\n",
      "Iteration 60987, loss = 1.34434217\n",
      "Iteration 60988, loss = 1.38521530\n",
      "Iteration 60989, loss = 1.36962375\n",
      "Iteration 60990, loss = 1.41287274\n",
      "Iteration 60991, loss = 1.70609707\n",
      "Iteration 60992, loss = 1.78626278\n",
      "Iteration 60993, loss = 1.97663313\n",
      "Iteration 60994, loss = 2.21506320\n",
      "Iteration 60995, loss = 1.91634992\n",
      "Iteration 60996, loss = 2.09821623\n",
      "Iteration 60997, loss = 1.83440104\n",
      "Iteration 60998, loss = 2.13755261\n",
      "Iteration 60999, loss = 3.17068607\n",
      "Iteration 61000, loss = 3.09812842\n",
      "Iteration 61001, loss = 2.97957590\n",
      "Iteration 61002, loss = 1.83017133\n",
      "Iteration 61003, loss = 1.26038080\n",
      "Iteration 61004, loss = 1.68867007\n",
      "Iteration 61005, loss = 1.64387099\n",
      "Iteration 61006, loss = 1.54354445\n",
      "Iteration 61007, loss = 1.35691054\n",
      "Iteration 61008, loss = 1.41111758\n",
      "Iteration 61009, loss = 1.52542233\n",
      "Iteration 61010, loss = 1.39586641\n",
      "Iteration 61011, loss = 1.36419250\n",
      "Iteration 61012, loss = 1.45581289\n",
      "Iteration 61013, loss = 1.58104205\n",
      "Iteration 61014, loss = 1.32684685\n",
      "Iteration 61015, loss = 1.46823996\n",
      "Iteration 61016, loss = 1.26943370\n",
      "Iteration 61017, loss = 1.39374037\n",
      "Iteration 61018, loss = 1.51733203\n",
      "Iteration 61019, loss = 1.55369992\n",
      "Iteration 61020, loss = 1.44005283\n",
      "Iteration 61021, loss = 1.39073657\n",
      "Iteration 61022, loss = 1.24242265\n",
      "Iteration 61023, loss = 1.60932708\n",
      "Iteration 61024, loss = 1.56296630\n",
      "Iteration 61025, loss = 1.47095662\n",
      "Iteration 61026, loss = 1.34297018\n",
      "Iteration 61027, loss = 1.62319105\n",
      "Iteration 61028, loss = 1.32634685\n",
      "Iteration 61029, loss = 1.23415212\n",
      "Iteration 61030, loss = 1.31031341\n",
      "Iteration 61031, loss = 1.39727195\n",
      "Iteration 61032, loss = 1.80170853\n",
      "Iteration 61033, loss = 1.72247843\n",
      "Iteration 61034, loss = 1.43079215\n",
      "Iteration 61035, loss = 1.54000785\n",
      "Iteration 61036, loss = 1.31780363\n",
      "Iteration 61037, loss = 1.38464030\n",
      "Iteration 61038, loss = 1.53199219\n",
      "Iteration 61039, loss = 2.07128625\n",
      "Iteration 61040, loss = 2.07941207\n",
      "Iteration 61041, loss = 2.43587083\n",
      "Iteration 61042, loss = 2.03581110\n",
      "Iteration 61043, loss = 2.29326602\n",
      "Iteration 61044, loss = 1.81761895\n",
      "Iteration 61045, loss = 1.42245644\n",
      "Iteration 61046, loss = 1.74866555\n",
      "Iteration 61047, loss = 1.49778292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 61048, loss = 1.55482783\n",
      "Iteration 61049, loss = 1.50887590\n",
      "Iteration 61050, loss = 1.85131252\n",
      "Iteration 61051, loss = 1.58812296\n",
      "Iteration 61052, loss = 1.58023627\n",
      "Iteration 61053, loss = 2.01959308\n",
      "Iteration 61054, loss = 1.61909214\n",
      "Iteration 61055, loss = 1.48920358\n",
      "Iteration 61056, loss = 1.59200155\n",
      "Iteration 61057, loss = 1.49598514\n",
      "Iteration 61058, loss = 1.55866735\n",
      "Iteration 61059, loss = 2.21516469\n",
      "Iteration 61060, loss = 1.80043359\n",
      "Iteration 61061, loss = 2.43862058\n",
      "Iteration 61062, loss = 2.09656325\n",
      "Iteration 61063, loss = 1.73029044\n",
      "Iteration 61064, loss = 1.35264834\n",
      "Iteration 61065, loss = 1.49703812\n",
      "Iteration 61066, loss = 1.94971804\n",
      "Iteration 61067, loss = 1.54657010\n",
      "Iteration 61068, loss = 1.62331269\n",
      "Iteration 61069, loss = 1.62987181\n",
      "Iteration 61070, loss = 1.57948310\n",
      "Iteration 61071, loss = 1.54001575\n",
      "Iteration 61072, loss = 1.41415504\n",
      "Iteration 61073, loss = 1.37224917\n",
      "Iteration 61074, loss = 1.35074613\n",
      "Iteration 61075, loss = 1.38292983\n",
      "Iteration 61076, loss = 1.30723899\n",
      "Iteration 61077, loss = 1.85164968\n",
      "Iteration 61078, loss = 1.62507535\n",
      "Iteration 61079, loss = 1.77211645\n",
      "Iteration 61080, loss = 3.61880756\n",
      "Iteration 61081, loss = 5.82323344\n",
      "Iteration 61082, loss = 4.57161078\n",
      "Iteration 61083, loss = 3.22487470\n",
      "Iteration 61084, loss = 3.87830721\n",
      "Iteration 61085, loss = 2.66531348\n",
      "Iteration 61086, loss = 1.99285354\n",
      "Iteration 61087, loss = 1.88589776\n",
      "Iteration 61088, loss = 1.84151420\n",
      "Iteration 61089, loss = 2.14108416\n",
      "Iteration 61090, loss = 1.78752905\n",
      "Iteration 61091, loss = 1.51800153\n",
      "Iteration 61092, loss = 1.36691630\n",
      "Iteration 61093, loss = 1.28404950\n",
      "Iteration 61094, loss = 1.68910726\n",
      "Iteration 61095, loss = 1.64500029\n",
      "Iteration 61096, loss = 1.56355867\n",
      "Iteration 61097, loss = 2.13498523\n",
      "Iteration 61098, loss = 1.59077305\n",
      "Iteration 61099, loss = 1.46488752\n",
      "Iteration 61100, loss = 1.36290051\n",
      "Iteration 61101, loss = 1.30043741\n",
      "Iteration 61102, loss = 1.47147424\n",
      "Iteration 61103, loss = 1.55050124\n",
      "Iteration 61104, loss = 1.63775903\n",
      "Iteration 61105, loss = 1.94861902\n",
      "Iteration 61106, loss = 1.85006248\n",
      "Iteration 61107, loss = 1.66509328\n",
      "Iteration 61108, loss = 1.43662554\n",
      "Iteration 61109, loss = 1.41634373\n",
      "Iteration 61110, loss = 1.84730550\n",
      "Iteration 61111, loss = 1.51202093\n",
      "Iteration 61112, loss = 1.46026370\n",
      "Iteration 61113, loss = 1.45222750\n",
      "Iteration 61114, loss = 1.46381303\n",
      "Iteration 61115, loss = 1.29029556\n",
      "Iteration 61116, loss = 1.41817220\n",
      "Iteration 61117, loss = 1.35838746\n",
      "Iteration 61118, loss = 1.33327114\n",
      "Iteration 61119, loss = 1.76675357\n",
      "Iteration 61120, loss = 1.58483615\n",
      "Iteration 61121, loss = 1.32138953\n",
      "Iteration 61122, loss = 1.22391764\n",
      "Iteration 61123, loss = 1.24586374\n",
      "Iteration 61124, loss = 1.38265794\n",
      "Iteration 61125, loss = 1.56747398\n",
      "Iteration 61126, loss = 1.36996940\n",
      "Iteration 61127, loss = 1.49467161\n",
      "Iteration 61128, loss = 1.43805968\n",
      "Iteration 61129, loss = 1.47207233\n",
      "Iteration 61130, loss = 1.83068851\n",
      "Iteration 61131, loss = 1.31535336\n",
      "Iteration 61132, loss = 1.34430762\n",
      "Iteration 61133, loss = 1.17276099\n",
      "Iteration 61134, loss = 1.42928211\n",
      "Iteration 61135, loss = 1.56209807\n",
      "Iteration 61136, loss = 1.57587546\n",
      "Iteration 61137, loss = 2.59901767\n",
      "Iteration 61138, loss = 2.19270282\n",
      "Iteration 61139, loss = 2.12229975\n",
      "Iteration 61140, loss = 2.11261445\n",
      "Iteration 61141, loss = 2.20258343\n",
      "Iteration 61142, loss = 2.82917320\n",
      "Iteration 61143, loss = 1.78736187\n",
      "Iteration 61144, loss = 1.87129498\n",
      "Iteration 61145, loss = 1.80565906\n",
      "Iteration 61146, loss = 1.59432160\n",
      "Iteration 61147, loss = 1.73173593\n",
      "Iteration 61148, loss = 2.41029452\n",
      "Iteration 61149, loss = 2.75294803\n",
      "Iteration 61150, loss = 3.81125581\n",
      "Iteration 61151, loss = 6.08654159\n",
      "Iteration 61152, loss = 5.05079461\n",
      "Iteration 61153, loss = 4.48399000\n",
      "Iteration 61154, loss = 3.62042459\n",
      "Iteration 61155, loss = 3.03583137\n",
      "Iteration 61156, loss = 2.29963208\n",
      "Iteration 61157, loss = 2.02609751\n",
      "Iteration 61158, loss = 1.88108102\n",
      "Iteration 61159, loss = 1.44226875\n",
      "Iteration 61160, loss = 1.39155414\n",
      "Iteration 61161, loss = 1.29903930\n",
      "Iteration 61162, loss = 1.34410488\n",
      "Iteration 61163, loss = 1.81469273\n",
      "Iteration 61164, loss = 1.89484263\n",
      "Iteration 61165, loss = 1.45021554\n",
      "Iteration 61166, loss = 1.64865193\n",
      "Iteration 61167, loss = 1.59550101\n",
      "Iteration 61168, loss = 1.82310191\n",
      "Iteration 61169, loss = 1.47143758\n",
      "Iteration 61170, loss = 1.52960823\n",
      "Iteration 61171, loss = 1.54909384\n",
      "Iteration 61172, loss = 1.51894727\n",
      "Iteration 61173, loss = 1.66979492\n",
      "Iteration 61174, loss = 1.51972103\n",
      "Iteration 61175, loss = 1.46368029\n",
      "Iteration 61176, loss = 1.34271483\n",
      "Iteration 61177, loss = 1.34324903\n",
      "Iteration 61178, loss = 1.29029375\n",
      "Iteration 61179, loss = 1.29326312\n",
      "Iteration 61180, loss = 1.20496206\n",
      "Iteration 61181, loss = 1.27461012\n",
      "Iteration 61182, loss = 1.39543664\n",
      "Iteration 61183, loss = 1.66664176\n",
      "Iteration 61184, loss = 1.62074323\n",
      "Iteration 61185, loss = 1.44481676\n",
      "Iteration 61186, loss = 1.49424703\n",
      "Iteration 61187, loss = 1.20604544\n",
      "Iteration 61188, loss = 1.21593834\n",
      "Iteration 61189, loss = 1.20611220\n",
      "Iteration 61190, loss = 1.19798681\n",
      "Iteration 61191, loss = 1.21471399\n",
      "Iteration 61192, loss = 1.33543534\n",
      "Iteration 61193, loss = 1.64092417\n",
      "Iteration 61194, loss = 1.74591704\n",
      "Iteration 61195, loss = 1.79275145\n",
      "Iteration 61196, loss = 1.90767627\n",
      "Iteration 61197, loss = 1.60913819\n",
      "Iteration 61198, loss = 1.46854601\n",
      "Iteration 61199, loss = 1.49314082\n",
      "Iteration 61200, loss = 1.40103012\n",
      "Iteration 61201, loss = 1.50166283\n",
      "Iteration 61202, loss = 1.48350728\n",
      "Iteration 61203, loss = 1.73996316\n",
      "Iteration 61204, loss = 1.49993985\n",
      "Iteration 61205, loss = 1.38135253\n",
      "Iteration 61206, loss = 1.20203612\n",
      "Iteration 61207, loss = 1.24518323\n",
      "Iteration 61208, loss = 1.32559764\n",
      "Iteration 61209, loss = 1.47184377\n",
      "Iteration 61210, loss = 1.39762151\n",
      "Iteration 61211, loss = 1.32973014\n",
      "Iteration 61212, loss = 1.37109298\n",
      "Iteration 61213, loss = 1.23003719\n",
      "Iteration 61214, loss = 1.30891713\n",
      "Iteration 61215, loss = 1.30277544\n",
      "Iteration 61216, loss = 1.33320210\n",
      "Iteration 61217, loss = 1.25149608\n",
      "Iteration 61218, loss = 1.25393708\n",
      "Iteration 61219, loss = 1.28958692\n",
      "Iteration 61220, loss = 1.20903623\n",
      "Iteration 61221, loss = 1.32334009\n",
      "Iteration 61222, loss = 1.33221553\n",
      "Iteration 61223, loss = 1.46285444\n",
      "Iteration 61224, loss = 1.38246865\n",
      "Iteration 61225, loss = 1.42451357\n",
      "Iteration 61226, loss = 1.27238612\n",
      "Iteration 61227, loss = 1.41762097\n",
      "Iteration 61228, loss = 1.30675981\n",
      "Iteration 61229, loss = 1.26599280\n",
      "Iteration 61230, loss = 1.21019142\n",
      "Iteration 61231, loss = 1.34379498\n",
      "Iteration 61232, loss = 1.38802615\n",
      "Iteration 61233, loss = 1.35521809\n",
      "Iteration 61234, loss = 1.44774131\n",
      "Iteration 61235, loss = 1.29445025\n",
      "Iteration 61236, loss = 1.21540168\n",
      "Iteration 61237, loss = 1.25187631\n",
      "Iteration 61238, loss = 1.36381395\n",
      "Iteration 61239, loss = 1.52571063\n",
      "Iteration 61240, loss = 1.84144483\n",
      "Iteration 61241, loss = 2.49746346\n",
      "Iteration 61242, loss = 2.25926409\n",
      "Iteration 61243, loss = 1.61810080\n",
      "Iteration 61244, loss = 1.36419754\n",
      "Iteration 61245, loss = 1.18016988\n",
      "Iteration 61246, loss = 1.27793809\n",
      "Iteration 61247, loss = 1.19687953\n",
      "Iteration 61248, loss = 1.33756020\n",
      "Iteration 61249, loss = 1.34867347\n",
      "Iteration 61250, loss = 1.52263893\n",
      "Iteration 61251, loss = 1.28485018\n",
      "Iteration 61252, loss = 1.51023328\n",
      "Iteration 61253, loss = 1.44214748\n",
      "Iteration 61254, loss = 1.39945567\n",
      "Iteration 61255, loss = 1.47992266\n",
      "Iteration 61256, loss = 1.50123389\n",
      "Iteration 61257, loss = 1.54972469\n",
      "Iteration 61258, loss = 1.44397184\n",
      "Iteration 61259, loss = 1.71452697\n",
      "Iteration 61260, loss = 1.37804886\n",
      "Iteration 61261, loss = 1.40512124\n",
      "Iteration 61262, loss = 1.25214405\n",
      "Iteration 61263, loss = 1.34194959\n",
      "Iteration 61264, loss = 1.33365379\n",
      "Iteration 61265, loss = 1.31450815\n",
      "Iteration 61266, loss = 1.31966148\n",
      "Iteration 61267, loss = 1.31082941\n",
      "Iteration 61268, loss = 1.47311525\n",
      "Iteration 61269, loss = 1.99766162\n",
      "Iteration 61270, loss = 1.77778026\n",
      "Iteration 61271, loss = 2.05727204\n",
      "Iteration 61272, loss = 1.65238291\n",
      "Iteration 61273, loss = 1.93496695\n",
      "Iteration 61274, loss = 1.58655484\n",
      "Iteration 61275, loss = 1.80735724\n",
      "Iteration 61276, loss = 1.47761606\n",
      "Iteration 61277, loss = 1.54665340\n",
      "Iteration 61278, loss = 1.60355162\n",
      "Iteration 61279, loss = 1.52085504\n",
      "Iteration 61280, loss = 1.66150994\n",
      "Iteration 61281, loss = 1.70733602\n",
      "Iteration 61282, loss = 1.57323330\n",
      "Iteration 61283, loss = 1.72478800\n",
      "Iteration 61284, loss = 1.76705191\n",
      "Iteration 61285, loss = 2.11939181\n",
      "Iteration 61286, loss = 1.92079220\n",
      "Iteration 61287, loss = 1.69105495\n",
      "Iteration 61288, loss = 3.37046640\n",
      "Iteration 61289, loss = 2.92732827\n",
      "Iteration 61290, loss = 2.20867306\n",
      "Iteration 61291, loss = 1.59784982\n",
      "Iteration 61292, loss = 1.55073538\n",
      "Iteration 61293, loss = 1.44275970\n",
      "Iteration 61294, loss = 1.51467906\n",
      "Iteration 61295, loss = 1.42617402\n",
      "Iteration 61296, loss = 1.65311271\n",
      "Iteration 61297, loss = 1.53104642\n",
      "Iteration 61298, loss = 1.45223978\n",
      "Iteration 61299, loss = 1.40424165\n",
      "Iteration 61300, loss = 1.29048614\n",
      "Iteration 61301, loss = 1.38543849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 61302, loss = 1.37212690\n",
      "Iteration 61303, loss = 1.72693200\n",
      "Iteration 61304, loss = 1.33832938\n",
      "Iteration 61305, loss = 1.57676616\n",
      "Iteration 61306, loss = 1.76275655\n",
      "Iteration 61307, loss = 2.12127562\n",
      "Iteration 61308, loss = 1.88773808\n",
      "Iteration 61309, loss = 2.07896216\n",
      "Iteration 61310, loss = 1.41400407\n",
      "Iteration 61311, loss = 1.70931946\n",
      "Iteration 61312, loss = 1.79889503\n",
      "Iteration 61313, loss = 1.36086223\n",
      "Iteration 61314, loss = 1.55363831\n",
      "Iteration 61315, loss = 1.74241371\n",
      "Iteration 61316, loss = 1.59092979\n",
      "Iteration 61317, loss = 1.90387930\n",
      "Iteration 61318, loss = 1.42073449\n",
      "Iteration 61319, loss = 1.50521375\n",
      "Iteration 61320, loss = 1.61927445\n",
      "Iteration 61321, loss = 1.52458295\n",
      "Iteration 61322, loss = 1.67542315\n",
      "Iteration 61323, loss = 1.46191291\n",
      "Iteration 61324, loss = 1.24146175\n",
      "Iteration 61325, loss = 1.87976776\n",
      "Iteration 61326, loss = 1.96834686\n",
      "Iteration 61327, loss = 2.09001921\n",
      "Iteration 61328, loss = 2.26567385\n",
      "Iteration 61329, loss = 1.95574312\n",
      "Iteration 61330, loss = 1.61844555\n",
      "Iteration 61331, loss = 1.45077786\n",
      "Iteration 61332, loss = 1.49982440\n",
      "Iteration 61333, loss = 1.56190394\n",
      "Iteration 61334, loss = 1.62878988\n",
      "Iteration 61335, loss = 1.43963663\n",
      "Iteration 61336, loss = 1.38610873\n",
      "Iteration 61337, loss = 1.37094347\n",
      "Iteration 61338, loss = 1.27816395\n",
      "Iteration 61339, loss = 1.26014474\n",
      "Iteration 61340, loss = 1.43457663\n",
      "Iteration 61341, loss = 2.04361623\n",
      "Iteration 61342, loss = 1.70685429\n",
      "Iteration 61343, loss = 1.27541334\n",
      "Iteration 61344, loss = 1.24219248\n",
      "Iteration 61345, loss = 1.23320620\n",
      "Iteration 61346, loss = 1.28826093\n",
      "Iteration 61347, loss = 1.35240206\n",
      "Iteration 61348, loss = 1.33028156\n",
      "Iteration 61349, loss = 1.37152676\n",
      "Iteration 61350, loss = 1.44172355\n",
      "Iteration 61351, loss = 1.51406173\n",
      "Iteration 61352, loss = 1.44282489\n",
      "Iteration 61353, loss = 1.67721060\n",
      "Iteration 61354, loss = 1.55350404\n",
      "Iteration 61355, loss = 1.44954528\n",
      "Iteration 61356, loss = 1.28380536\n",
      "Iteration 61357, loss = 1.67219815\n",
      "Iteration 61358, loss = 1.72029624\n",
      "Iteration 61359, loss = 1.48491017\n",
      "Iteration 61360, loss = 1.49366054\n",
      "Iteration 61361, loss = 1.41665104\n",
      "Iteration 61362, loss = 1.46918668\n",
      "Iteration 61363, loss = 1.22359458\n",
      "Iteration 61364, loss = 1.53303030\n",
      "Iteration 61365, loss = 1.83459354\n",
      "Iteration 61366, loss = 1.88397079\n",
      "Iteration 61367, loss = 1.68931699\n",
      "Iteration 61368, loss = 1.71551303\n",
      "Iteration 61369, loss = 1.55257026\n",
      "Iteration 61370, loss = 2.51743426\n",
      "Iteration 61371, loss = 2.77108432\n",
      "Iteration 61372, loss = 2.32815264\n",
      "Iteration 61373, loss = 1.71577118\n",
      "Iteration 61374, loss = 2.20277726\n",
      "Iteration 61375, loss = 1.57594542\n",
      "Iteration 61376, loss = 1.37466863\n",
      "Iteration 61377, loss = 1.49228431\n",
      "Iteration 61378, loss = 1.59254728\n",
      "Iteration 61379, loss = 1.63868482\n",
      "Iteration 61380, loss = 1.71950200\n",
      "Iteration 61381, loss = 1.46577824\n",
      "Iteration 61382, loss = 1.53413456\n",
      "Iteration 61383, loss = 1.55638219\n",
      "Iteration 61384, loss = 1.39972512\n",
      "Iteration 61385, loss = 1.47999112\n",
      "Iteration 61386, loss = 1.89773185\n",
      "Iteration 61387, loss = 1.82614688\n",
      "Iteration 61388, loss = 1.92196012\n",
      "Iteration 61389, loss = 1.57042145\n",
      "Iteration 61390, loss = 2.00368704\n",
      "Iteration 61391, loss = 2.06785950\n",
      "Iteration 61392, loss = 1.86471315\n",
      "Iteration 61393, loss = 1.63130515\n",
      "Iteration 61394, loss = 1.31071991\n",
      "Iteration 61395, loss = 1.47237004\n",
      "Iteration 61396, loss = 1.38986917\n",
      "Iteration 61397, loss = 1.37276842\n",
      "Iteration 61398, loss = 1.27574183\n",
      "Iteration 61399, loss = 1.19879390\n",
      "Iteration 61400, loss = 1.26560687\n",
      "Iteration 61401, loss = 1.18453969\n",
      "Iteration 61402, loss = 1.29705239\n",
      "Iteration 61403, loss = 1.50966454\n",
      "Iteration 61404, loss = 1.92175717\n",
      "Iteration 61405, loss = 2.68546956\n",
      "Iteration 61406, loss = 2.59292445\n",
      "Iteration 61407, loss = 1.42644295\n",
      "Iteration 61408, loss = 1.30334608\n",
      "Iteration 61409, loss = 1.40302319\n",
      "Iteration 61410, loss = 1.20680891\n",
      "Iteration 61411, loss = 1.39059364\n",
      "Iteration 61412, loss = 1.47381431\n",
      "Iteration 61413, loss = 1.59858706\n",
      "Iteration 61414, loss = 1.53058358\n",
      "Iteration 61415, loss = 1.57922836\n",
      "Iteration 61416, loss = 1.66080107\n",
      "Iteration 61417, loss = 1.43091374\n",
      "Iteration 61418, loss = 1.54355976\n",
      "Iteration 61419, loss = 1.47972733\n",
      "Iteration 61420, loss = 1.56401287\n",
      "Iteration 61421, loss = 1.47087232\n",
      "Iteration 61422, loss = 2.07266634\n",
      "Iteration 61423, loss = 1.54093184\n",
      "Iteration 61424, loss = 1.72663871\n",
      "Iteration 61425, loss = 1.64348981\n",
      "Iteration 61426, loss = 1.50409642\n",
      "Iteration 61427, loss = 1.56057313\n",
      "Iteration 61428, loss = 1.38433215\n",
      "Iteration 61429, loss = 1.38156980\n",
      "Iteration 61430, loss = 1.40327308\n",
      "Iteration 61431, loss = 1.43419072\n",
      "Iteration 61432, loss = 1.54138111\n",
      "Iteration 61433, loss = 1.29534166\n",
      "Iteration 61434, loss = 1.51406930\n",
      "Iteration 61435, loss = 1.41233996\n",
      "Iteration 61436, loss = 1.35630037\n",
      "Iteration 61437, loss = 1.43507693\n",
      "Iteration 61438, loss = 1.38100310\n",
      "Iteration 61439, loss = 1.29314779\n",
      "Iteration 61440, loss = 1.57930900\n",
      "Iteration 61441, loss = 1.45873656\n",
      "Iteration 61442, loss = 1.77180971\n",
      "Iteration 61443, loss = 1.70780811\n",
      "Iteration 61444, loss = 1.70480141\n",
      "Iteration 61445, loss = 1.49915973\n",
      "Iteration 61446, loss = 1.56267246\n",
      "Iteration 61447, loss = 1.60614364\n",
      "Iteration 61448, loss = 1.83304665\n",
      "Iteration 61449, loss = 1.61712655\n",
      "Iteration 61450, loss = 1.69488716\n",
      "Iteration 61451, loss = 1.52270170\n",
      "Iteration 61452, loss = 1.31427385\n",
      "Iteration 61453, loss = 1.32538567\n",
      "Iteration 61454, loss = 1.30178618\n",
      "Iteration 61455, loss = 1.50735347\n",
      "Iteration 61456, loss = 1.66362698\n",
      "Iteration 61457, loss = 1.50391216\n",
      "Iteration 61458, loss = 1.23244513\n",
      "Iteration 61459, loss = 1.35050170\n",
      "Iteration 61460, loss = 1.22241643\n",
      "Iteration 61461, loss = 1.16879993\n",
      "Iteration 61462, loss = 1.33473977\n",
      "Iteration 61463, loss = 1.32129248\n",
      "Iteration 61464, loss = 1.27906438\n",
      "Iteration 61465, loss = 1.51689717\n",
      "Iteration 61466, loss = 1.98661148\n",
      "Iteration 61467, loss = 1.61159574\n",
      "Iteration 61468, loss = 1.64926140\n",
      "Iteration 61469, loss = 1.82741389\n",
      "Iteration 61470, loss = 2.44183187\n",
      "Iteration 61471, loss = 2.01302199\n",
      "Iteration 61472, loss = 2.45199738\n",
      "Iteration 61473, loss = 1.87856378\n",
      "Iteration 61474, loss = 1.55671555\n",
      "Iteration 61475, loss = 1.39051457\n",
      "Iteration 61476, loss = 1.37670175\n",
      "Iteration 61477, loss = 1.31680863\n",
      "Iteration 61478, loss = 1.20203321\n",
      "Iteration 61479, loss = 1.24848361\n",
      "Iteration 61480, loss = 1.24139317\n",
      "Iteration 61481, loss = 1.24879792\n",
      "Iteration 61482, loss = 1.20794520\n",
      "Iteration 61483, loss = 1.32066013\n",
      "Iteration 61484, loss = 1.49415335\n",
      "Iteration 61485, loss = 1.61828760\n",
      "Iteration 61486, loss = 1.70871134\n",
      "Iteration 61487, loss = 1.82814131\n",
      "Iteration 61488, loss = 2.10139862\n",
      "Iteration 61489, loss = 2.79562336\n",
      "Iteration 61490, loss = 3.83741574\n",
      "Iteration 61491, loss = 2.87659228\n",
      "Iteration 61492, loss = 3.17889328\n",
      "Iteration 61493, loss = 4.00533796\n",
      "Iteration 61494, loss = 2.93223226\n",
      "Iteration 61495, loss = 1.56784793\n",
      "Iteration 61496, loss = 1.57015467\n",
      "Iteration 61497, loss = 1.80039528\n",
      "Iteration 61498, loss = 1.44859375\n",
      "Iteration 61499, loss = 1.51587330\n",
      "Iteration 61500, loss = 1.85210692\n",
      "Iteration 61501, loss = 1.43832357\n",
      "Iteration 61502, loss = 1.72191081\n",
      "Iteration 61503, loss = 1.36534621\n",
      "Iteration 61504, loss = 1.20709363\n",
      "Iteration 61505, loss = 1.25163701\n",
      "Iteration 61506, loss = 1.43627497\n",
      "Iteration 61507, loss = 1.54144382\n",
      "Iteration 61508, loss = 1.54918346\n",
      "Iteration 61509, loss = 1.58675203\n",
      "Iteration 61510, loss = 1.32777821\n",
      "Iteration 61511, loss = 1.48378588\n",
      "Iteration 61512, loss = 1.46704047\n",
      "Iteration 61513, loss = 1.52671010\n",
      "Iteration 61514, loss = 1.29147237\n",
      "Iteration 61515, loss = 1.42565671\n",
      "Iteration 61516, loss = 1.51589540\n",
      "Iteration 61517, loss = 1.66397648\n",
      "Iteration 61518, loss = 1.28653651\n",
      "Iteration 61519, loss = 1.48204049\n",
      "Iteration 61520, loss = 1.48537041\n",
      "Iteration 61521, loss = 1.41691067\n",
      "Iteration 61522, loss = 1.75978260\n",
      "Iteration 61523, loss = 1.65694583\n",
      "Iteration 61524, loss = 1.80879114\n",
      "Iteration 61525, loss = 2.13036758\n",
      "Iteration 61526, loss = 2.07850602\n",
      "Iteration 61527, loss = 1.73810165\n",
      "Iteration 61528, loss = 1.50311216\n",
      "Iteration 61529, loss = 1.66554635\n",
      "Iteration 61530, loss = 1.80262454\n",
      "Iteration 61531, loss = 1.50794337\n",
      "Iteration 61532, loss = 1.36241689\n",
      "Iteration 61533, loss = 1.46544565\n",
      "Iteration 61534, loss = 1.23989651\n",
      "Iteration 61535, loss = 1.16391239\n",
      "Iteration 61536, loss = 1.23957048\n",
      "Iteration 61537, loss = 1.48587581\n",
      "Iteration 61538, loss = 1.32897501\n",
      "Iteration 61539, loss = 1.22715555\n",
      "Iteration 61540, loss = 1.25378892\n",
      "Iteration 61541, loss = 1.16561007\n",
      "Iteration 61542, loss = 1.44561066\n",
      "Iteration 61543, loss = 1.36564820\n",
      "Iteration 61544, loss = 1.18670615\n",
      "Iteration 61545, loss = 1.25471556\n",
      "Iteration 61546, loss = 1.31592603\n",
      "Iteration 61547, loss = 1.50909273\n",
      "Iteration 61548, loss = 1.49789073\n",
      "Iteration 61549, loss = 1.43075375\n",
      "Iteration 61550, loss = 1.24167731\n",
      "Iteration 61551, loss = 1.37073251\n",
      "Iteration 61552, loss = 1.76673750\n",
      "Iteration 61553, loss = 1.35781160\n",
      "Iteration 61554, loss = 1.42733459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 61555, loss = 1.35569873\n",
      "Iteration 61556, loss = 1.41226899\n",
      "Iteration 61557, loss = 1.54626217\n",
      "Iteration 61558, loss = 1.24366271\n",
      "Iteration 61559, loss = 1.58362501\n",
      "Iteration 61560, loss = 1.35663556\n",
      "Iteration 61561, loss = 1.46520704\n",
      "Iteration 61562, loss = 1.22029612\n",
      "Iteration 61563, loss = 1.59652986\n",
      "Iteration 61564, loss = 1.44576862\n",
      "Iteration 61565, loss = 1.80218347\n",
      "Iteration 61566, loss = 1.53067956\n",
      "Iteration 61567, loss = 1.81524112\n",
      "Iteration 61568, loss = 1.87813418\n",
      "Iteration 61569, loss = 2.36023658\n",
      "Iteration 61570, loss = 2.29135296\n",
      "Iteration 61571, loss = 2.05634309\n",
      "Iteration 61572, loss = 1.63547656\n",
      "Iteration 61573, loss = 1.60397747\n",
      "Iteration 61574, loss = 1.38898632\n",
      "Iteration 61575, loss = 1.47844293\n",
      "Iteration 61576, loss = 1.40862737\n",
      "Iteration 61577, loss = 1.29968173\n",
      "Iteration 61578, loss = 1.23779054\n",
      "Iteration 61579, loss = 1.35780570\n",
      "Iteration 61580, loss = 1.47438971\n",
      "Iteration 61581, loss = 1.63717819\n",
      "Iteration 61582, loss = 1.49221838\n",
      "Iteration 61583, loss = 1.47837243\n",
      "Iteration 61584, loss = 1.70564019\n",
      "Iteration 61585, loss = 2.18703331\n",
      "Iteration 61586, loss = 1.67740241\n",
      "Iteration 61587, loss = 1.70970029\n",
      "Iteration 61588, loss = 1.50687844\n",
      "Iteration 61589, loss = 1.40824728\n",
      "Iteration 61590, loss = 1.44551964\n",
      "Iteration 61591, loss = 1.42658152\n",
      "Iteration 61592, loss = 1.49393792\n",
      "Iteration 61593, loss = 1.36308676\n",
      "Iteration 61594, loss = 1.34798965\n",
      "Iteration 61595, loss = 1.39159725\n",
      "Iteration 61596, loss = 1.32309450\n",
      "Iteration 61597, loss = 1.33663594\n",
      "Iteration 61598, loss = 1.57585451\n",
      "Iteration 61599, loss = 1.65751037\n",
      "Iteration 61600, loss = 1.62156864\n",
      "Iteration 61601, loss = 1.46712186\n",
      "Iteration 61602, loss = 1.24064144\n",
      "Iteration 61603, loss = 1.32919407\n",
      "Iteration 61604, loss = 1.20885228\n",
      "Iteration 61605, loss = 1.20379409\n",
      "Iteration 61606, loss = 1.39194944\n",
      "Iteration 61607, loss = 1.29251666\n",
      "Iteration 61608, loss = 1.59995767\n",
      "Iteration 61609, loss = 1.83060622\n",
      "Iteration 61610, loss = 2.07792623\n",
      "Iteration 61611, loss = 2.35706885\n",
      "Iteration 61612, loss = 1.62725583\n",
      "Iteration 61613, loss = 1.52461970\n",
      "Iteration 61614, loss = 1.54012641\n",
      "Iteration 61615, loss = 1.42174238\n",
      "Iteration 61616, loss = 3.26261075\n",
      "Iteration 61617, loss = 3.95772053\n",
      "Iteration 61618, loss = 4.99529113\n",
      "Iteration 61619, loss = 2.56427033\n",
      "Iteration 61620, loss = 2.82173896\n",
      "Iteration 61621, loss = 2.14384072\n",
      "Iteration 61622, loss = 2.32602637\n",
      "Iteration 61623, loss = 1.97155409\n",
      "Iteration 61624, loss = 1.97514497\n",
      "Iteration 61625, loss = 2.01491409\n",
      "Iteration 61626, loss = 1.54195581\n",
      "Iteration 61627, loss = 1.30078189\n",
      "Iteration 61628, loss = 1.24621046\n",
      "Iteration 61629, loss = 1.34320213\n",
      "Iteration 61630, loss = 1.21842968\n",
      "Iteration 61631, loss = 1.31751290\n",
      "Iteration 61632, loss = 1.34013175\n",
      "Iteration 61633, loss = 1.51325237\n",
      "Iteration 61634, loss = 1.62898226\n",
      "Iteration 61635, loss = 1.98753584\n",
      "Iteration 61636, loss = 2.05274936\n",
      "Iteration 61637, loss = 2.54830292\n",
      "Iteration 61638, loss = 1.70341343\n",
      "Iteration 61639, loss = 1.88153607\n",
      "Iteration 61640, loss = 1.39334460\n",
      "Iteration 61641, loss = 1.25435104\n",
      "Iteration 61642, loss = 1.30365818\n",
      "Iteration 61643, loss = 1.27599902\n",
      "Iteration 61644, loss = 1.50742956\n",
      "Iteration 61645, loss = 1.23248818\n",
      "Iteration 61646, loss = 1.23371511\n",
      "Iteration 61647, loss = 1.28995547\n",
      "Iteration 61648, loss = 1.26769278\n",
      "Iteration 61649, loss = 1.29507034\n",
      "Iteration 61650, loss = 1.61192550\n",
      "Iteration 61651, loss = 1.92959559\n",
      "Iteration 61652, loss = 1.51201251\n",
      "Iteration 61653, loss = 1.77716435\n",
      "Iteration 61654, loss = 1.99956822\n",
      "Iteration 61655, loss = 1.87258966\n",
      "Iteration 61656, loss = 1.44184210\n",
      "Iteration 61657, loss = 1.57185164\n",
      "Iteration 61658, loss = 1.46011667\n",
      "Iteration 61659, loss = 1.86435389\n",
      "Iteration 61660, loss = 1.81111836\n",
      "Iteration 61661, loss = 1.81545831\n",
      "Iteration 61662, loss = 1.91447144\n",
      "Iteration 61663, loss = 1.60130546\n",
      "Iteration 61664, loss = 1.26716130\n",
      "Iteration 61665, loss = 1.59756007\n",
      "Iteration 61666, loss = 1.70618652\n",
      "Iteration 61667, loss = 1.74611033\n",
      "Iteration 61668, loss = 1.55795824\n",
      "Iteration 61669, loss = 1.47752693\n",
      "Iteration 61670, loss = 1.73019605\n",
      "Iteration 61671, loss = 1.71553744\n",
      "Iteration 61672, loss = 1.74522486\n",
      "Iteration 61673, loss = 1.84288746\n",
      "Iteration 61674, loss = 1.72690837\n",
      "Iteration 61675, loss = 1.82423824\n",
      "Iteration 61676, loss = 1.64587785\n",
      "Iteration 61677, loss = 1.59407495\n",
      "Iteration 61678, loss = 1.65248909\n",
      "Iteration 61679, loss = 1.37654880\n",
      "Iteration 61680, loss = 1.37657819\n",
      "Iteration 61681, loss = 1.44578933\n",
      "Iteration 61682, loss = 1.36244349\n",
      "Iteration 61683, loss = 1.46508157\n",
      "Iteration 61684, loss = 1.56063317\n",
      "Iteration 61685, loss = 1.71440746\n",
      "Iteration 61686, loss = 1.55782138\n",
      "Iteration 61687, loss = 1.50543718\n",
      "Iteration 61688, loss = 1.42104278\n",
      "Iteration 61689, loss = 1.42616557\n",
      "Iteration 61690, loss = 1.42080934\n",
      "Iteration 61691, loss = 1.49027438\n",
      "Iteration 61692, loss = 1.58494237\n",
      "Iteration 61693, loss = 1.37948683\n",
      "Iteration 61694, loss = 1.32260674\n",
      "Iteration 61695, loss = 1.60549466\n",
      "Iteration 61696, loss = 1.70279224\n",
      "Iteration 61697, loss = 1.45517859\n",
      "Iteration 61698, loss = 1.51696811\n",
      "Iteration 61699, loss = 1.75432665\n",
      "Iteration 61700, loss = 1.57673503\n",
      "Iteration 61701, loss = 1.43662142\n",
      "Iteration 61702, loss = 1.37041599\n",
      "Iteration 61703, loss = 1.55924900\n",
      "Iteration 61704, loss = 1.29953707\n",
      "Iteration 61705, loss = 1.39365584\n",
      "Iteration 61706, loss = 1.31274137\n",
      "Iteration 61707, loss = 1.18163559\n",
      "Iteration 61708, loss = 1.20677100\n",
      "Iteration 61709, loss = 1.31965909\n",
      "Iteration 61710, loss = 1.23458902\n",
      "Iteration 61711, loss = 1.24010686\n",
      "Iteration 61712, loss = 1.23472968\n",
      "Iteration 61713, loss = 1.28871638\n",
      "Iteration 61714, loss = 1.29596589\n",
      "Iteration 61715, loss = 1.49634680\n",
      "Iteration 61716, loss = 1.61099000\n",
      "Iteration 61717, loss = 1.29503231\n",
      "Iteration 61718, loss = 2.11777373\n",
      "Iteration 61719, loss = 2.35833657\n",
      "Iteration 61720, loss = 2.59333133\n",
      "Iteration 61721, loss = 1.70097122\n",
      "Iteration 61722, loss = 2.21181639\n",
      "Iteration 61723, loss = 2.36180284\n",
      "Iteration 61724, loss = 2.27615457\n",
      "Iteration 61725, loss = 1.94905956\n",
      "Iteration 61726, loss = 2.52144837\n",
      "Iteration 61727, loss = 2.19125733\n",
      "Iteration 61728, loss = 2.07188280\n",
      "Iteration 61729, loss = 2.07256746\n",
      "Iteration 61730, loss = 1.71466923\n",
      "Iteration 61731, loss = 1.91016902\n",
      "Iteration 61732, loss = 1.62793505\n",
      "Iteration 61733, loss = 1.24793241\n",
      "Iteration 61734, loss = 1.19934495\n",
      "Iteration 61735, loss = 1.12878455\n",
      "Iteration 61736, loss = 1.13170247\n",
      "Iteration 61737, loss = 1.27554653\n",
      "Iteration 61738, loss = 1.50673035\n",
      "Iteration 61739, loss = 1.55478024\n",
      "Iteration 61740, loss = 1.73352864\n",
      "Iteration 61741, loss = 1.66600741\n",
      "Iteration 61742, loss = 1.90489191\n",
      "Iteration 61743, loss = 1.70505341\n",
      "Iteration 61744, loss = 1.87790756\n",
      "Iteration 61745, loss = 1.48164180\n",
      "Iteration 61746, loss = 1.43462469\n",
      "Iteration 61747, loss = 1.32473120\n",
      "Iteration 61748, loss = 1.67373479\n",
      "Iteration 61749, loss = 1.50510834\n",
      "Iteration 61750, loss = 1.64402890\n",
      "Iteration 61751, loss = 1.52409517\n",
      "Iteration 61752, loss = 2.05225118\n",
      "Iteration 61753, loss = 1.53667560\n",
      "Iteration 61754, loss = 1.49828010\n",
      "Iteration 61755, loss = 1.61503670\n",
      "Iteration 61756, loss = 1.97753328\n",
      "Iteration 61757, loss = 1.67961632\n",
      "Iteration 61758, loss = 1.48357675\n",
      "Iteration 61759, loss = 1.60653412\n",
      "Iteration 61760, loss = 1.33174736\n",
      "Iteration 61761, loss = 1.53320477\n",
      "Iteration 61762, loss = 1.60038109\n",
      "Iteration 61763, loss = 1.52165233\n",
      "Iteration 61764, loss = 1.46077373\n",
      "Iteration 61765, loss = 1.51190551\n",
      "Iteration 61766, loss = 1.45667962\n",
      "Iteration 61767, loss = 1.90440690\n",
      "Iteration 61768, loss = 1.73300476\n",
      "Iteration 61769, loss = 1.42239260\n",
      "Iteration 61770, loss = 1.21795218\n",
      "Iteration 61771, loss = 1.20109324\n",
      "Iteration 61772, loss = 1.32199441\n",
      "Iteration 61773, loss = 1.50922210\n",
      "Iteration 61774, loss = 1.77165597\n",
      "Iteration 61775, loss = 1.66547218\n",
      "Iteration 61776, loss = 1.50179281\n",
      "Iteration 61777, loss = 1.53141337\n",
      "Iteration 61778, loss = 1.74126313\n",
      "Iteration 61779, loss = 1.67495304\n",
      "Iteration 61780, loss = 1.53294482\n",
      "Iteration 61781, loss = 1.50963734\n",
      "Iteration 61782, loss = 1.48280279\n",
      "Iteration 61783, loss = 1.61029235\n",
      "Iteration 61784, loss = 1.82468356\n",
      "Iteration 61785, loss = 1.59807828\n",
      "Iteration 61786, loss = 1.62177967\n",
      "Iteration 61787, loss = 1.34145398\n",
      "Iteration 61788, loss = 1.34912442\n",
      "Iteration 61789, loss = 1.42017961\n",
      "Iteration 61790, loss = 1.29205469\n",
      "Iteration 61791, loss = 1.47265463\n",
      "Iteration 61792, loss = 1.55167728\n",
      "Iteration 61793, loss = 1.37084931\n",
      "Iteration 61794, loss = 1.28676580\n",
      "Iteration 61795, loss = 1.26857112\n",
      "Iteration 61796, loss = 1.35680986\n",
      "Iteration 61797, loss = 1.22892355\n",
      "Iteration 61798, loss = 1.34240842\n",
      "Iteration 61799, loss = 1.30524038\n",
      "Iteration 61800, loss = 1.52481058\n",
      "Iteration 61801, loss = 1.65222577\n",
      "Iteration 61802, loss = 1.60515165\n",
      "Iteration 61803, loss = 1.63288790\n",
      "Iteration 61804, loss = 1.58169458\n",
      "Iteration 61805, loss = 2.02310887\n",
      "Iteration 61806, loss = 1.72835718\n",
      "Iteration 61807, loss = 1.52469585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 61808, loss = 1.58966980\n",
      "Iteration 61809, loss = 1.63362705\n",
      "Iteration 61810, loss = 1.64326494\n",
      "Iteration 61811, loss = 2.18523227\n",
      "Iteration 61812, loss = 1.74733196\n",
      "Iteration 61813, loss = 1.67285237\n",
      "Iteration 61814, loss = 1.84786796\n",
      "Iteration 61815, loss = 2.68839299\n",
      "Iteration 61816, loss = 2.62545272\n",
      "Iteration 61817, loss = 1.56946238\n",
      "Iteration 61818, loss = 1.95469740\n",
      "Iteration 61819, loss = 1.95937835\n",
      "Iteration 61820, loss = 2.37690975\n",
      "Iteration 61821, loss = 1.94387082\n",
      "Iteration 61822, loss = 2.13630186\n",
      "Iteration 61823, loss = 1.96718424\n",
      "Iteration 61824, loss = 2.25366393\n",
      "Iteration 61825, loss = 2.02829284\n",
      "Iteration 61826, loss = 1.50585385\n",
      "Iteration 61827, loss = 1.91326674\n",
      "Iteration 61828, loss = 1.72090802\n",
      "Iteration 61829, loss = 2.05199004\n",
      "Iteration 61830, loss = 2.88337375\n",
      "Iteration 61831, loss = 2.88501649\n",
      "Iteration 61832, loss = 1.92322500\n",
      "Iteration 61833, loss = 3.02154518\n",
      "Iteration 61834, loss = 2.69904349\n",
      "Iteration 61835, loss = 2.11463522\n",
      "Iteration 61836, loss = 2.22720712\n",
      "Iteration 61837, loss = 1.83010563\n",
      "Iteration 61838, loss = 2.17098784\n",
      "Iteration 61839, loss = 2.05461880\n",
      "Iteration 61840, loss = 1.90992817\n",
      "Iteration 61841, loss = 1.67068116\n",
      "Iteration 61842, loss = 1.63662331\n",
      "Iteration 61843, loss = 1.75020336\n",
      "Iteration 61844, loss = 1.29998387\n",
      "Iteration 61845, loss = 1.35275714\n",
      "Iteration 61846, loss = 1.22318508\n",
      "Iteration 61847, loss = 1.28701356\n",
      "Iteration 61848, loss = 1.28679162\n",
      "Iteration 61849, loss = 1.48306172\n",
      "Iteration 61850, loss = 1.23986693\n",
      "Iteration 61851, loss = 1.19632483\n",
      "Iteration 61852, loss = 1.20904932\n",
      "Iteration 61853, loss = 1.21746779\n",
      "Iteration 61854, loss = 1.15824122\n",
      "Iteration 61855, loss = 1.19000678\n",
      "Iteration 61856, loss = 1.17736583\n",
      "Iteration 61857, loss = 1.17299305\n",
      "Iteration 61858, loss = 1.34627133\n",
      "Iteration 61859, loss = 1.23051909\n",
      "Iteration 61860, loss = 1.67978431\n",
      "Iteration 61861, loss = 1.81338170\n",
      "Iteration 61862, loss = 2.29250742\n",
      "Iteration 61863, loss = 1.69455590\n",
      "Iteration 61864, loss = 2.20060437\n",
      "Iteration 61865, loss = 1.59400489\n",
      "Iteration 61866, loss = 1.42169776\n",
      "Iteration 61867, loss = 1.83431608\n",
      "Iteration 61868, loss = 1.96384137\n",
      "Iteration 61869, loss = 1.78867494\n",
      "Iteration 61870, loss = 1.78541202\n",
      "Iteration 61871, loss = 1.69458327\n",
      "Iteration 61872, loss = 1.70756402\n",
      "Iteration 61873, loss = 1.64306362\n",
      "Iteration 61874, loss = 1.72807800\n",
      "Iteration 61875, loss = 1.54556945\n",
      "Iteration 61876, loss = 1.57323091\n",
      "Iteration 61877, loss = 1.37124864\n",
      "Iteration 61878, loss = 1.21043416\n",
      "Iteration 61879, loss = 1.29755498\n",
      "Iteration 61880, loss = 1.29528844\n",
      "Iteration 61881, loss = 1.45624457\n",
      "Iteration 61882, loss = 1.30412828\n",
      "Iteration 61883, loss = 1.33434517\n",
      "Iteration 61884, loss = 1.47534281\n",
      "Iteration 61885, loss = 1.57694843\n",
      "Iteration 61886, loss = 1.45620362\n",
      "Iteration 61887, loss = 1.25475843\n",
      "Iteration 61888, loss = 1.42720726\n",
      "Iteration 61889, loss = 1.50013187\n",
      "Iteration 61890, loss = 1.52792602\n",
      "Iteration 61891, loss = 1.59010637\n",
      "Iteration 61892, loss = 1.79347644\n",
      "Iteration 61893, loss = 2.03739647\n",
      "Iteration 61894, loss = 2.33493001\n",
      "Iteration 61895, loss = 2.23148670\n",
      "Iteration 61896, loss = 1.77246165\n",
      "Iteration 61897, loss = 1.48623716\n",
      "Iteration 61898, loss = 1.21924797\n",
      "Iteration 61899, loss = 1.44502350\n",
      "Iteration 61900, loss = 2.03610961\n",
      "Iteration 61901, loss = 2.26118927\n",
      "Iteration 61902, loss = 2.22208663\n",
      "Iteration 61903, loss = 2.12547056\n",
      "Iteration 61904, loss = 1.71526947\n",
      "Iteration 61905, loss = 1.90117331\n",
      "Iteration 61906, loss = 1.78482552\n",
      "Iteration 61907, loss = 1.52058741\n",
      "Iteration 61908, loss = 1.49132064\n",
      "Iteration 61909, loss = 1.34949251\n",
      "Iteration 61910, loss = 1.21133738\n",
      "Iteration 61911, loss = 1.17198045\n",
      "Iteration 61912, loss = 1.17109865\n",
      "Iteration 61913, loss = 1.13511700\n",
      "Iteration 61914, loss = 1.23859765\n",
      "Iteration 61915, loss = 1.20417778\n",
      "Iteration 61916, loss = 1.14840353\n",
      "Iteration 61917, loss = 1.26993422\n",
      "Iteration 61918, loss = 1.27382396\n",
      "Iteration 61919, loss = 1.63842267\n",
      "Iteration 61920, loss = 1.31917838\n",
      "Iteration 61921, loss = 1.23558408\n",
      "Iteration 61922, loss = 1.51300707\n",
      "Iteration 61923, loss = 1.47743971\n",
      "Iteration 61924, loss = 1.82311640\n",
      "Iteration 61925, loss = 1.88259182\n",
      "Iteration 61926, loss = 1.82709840\n",
      "Iteration 61927, loss = 1.59813144\n",
      "Iteration 61928, loss = 1.98351687\n",
      "Iteration 61929, loss = 1.85796459\n",
      "Iteration 61930, loss = 1.68406308\n",
      "Iteration 61931, loss = 1.71442346\n",
      "Iteration 61932, loss = 1.82429902\n",
      "Iteration 61933, loss = 1.55365419\n",
      "Iteration 61934, loss = 1.69337505\n",
      "Iteration 61935, loss = 1.47302123\n",
      "Iteration 61936, loss = 1.73612620\n",
      "Iteration 61937, loss = 1.66523668\n",
      "Iteration 61938, loss = 1.55324764\n",
      "Iteration 61939, loss = 1.65495772\n",
      "Iteration 61940, loss = 1.49764565\n",
      "Iteration 61941, loss = 1.37088171\n",
      "Iteration 61942, loss = 1.26937320\n",
      "Iteration 61943, loss = 1.30842596\n",
      "Iteration 61944, loss = 1.55559183\n",
      "Iteration 61945, loss = 1.52061566\n",
      "Iteration 61946, loss = 1.59821913\n",
      "Iteration 61947, loss = 1.26476074\n",
      "Iteration 61948, loss = 1.43002048\n",
      "Iteration 61949, loss = 1.74687460\n",
      "Iteration 61950, loss = 1.61460772\n",
      "Iteration 61951, loss = 1.27481937\n",
      "Iteration 61952, loss = 1.39109502\n",
      "Iteration 61953, loss = 1.40433140\n",
      "Iteration 61954, loss = 1.43880614\n",
      "Iteration 61955, loss = 1.45799147\n",
      "Iteration 61956, loss = 1.29188696\n",
      "Iteration 61957, loss = 1.28336982\n",
      "Iteration 61958, loss = 1.34163690\n",
      "Iteration 61959, loss = 1.21462337\n",
      "Iteration 61960, loss = 1.28076651\n",
      "Iteration 61961, loss = 1.27635563\n",
      "Iteration 61962, loss = 1.21542774\n",
      "Iteration 61963, loss = 1.22040883\n",
      "Iteration 61964, loss = 1.36673897\n",
      "Iteration 61965, loss = 1.31282453\n",
      "Iteration 61966, loss = 1.31118289\n",
      "Iteration 61967, loss = 1.59218597\n",
      "Iteration 61968, loss = 1.50446730\n",
      "Iteration 61969, loss = 1.39817198\n",
      "Iteration 61970, loss = 1.74369867\n",
      "Iteration 61971, loss = 1.91278145\n",
      "Iteration 61972, loss = 1.59275455\n",
      "Iteration 61973, loss = 1.54939624\n",
      "Iteration 61974, loss = 1.77276450\n",
      "Iteration 61975, loss = 1.63475314\n",
      "Iteration 61976, loss = 2.18777663\n",
      "Iteration 61977, loss = 1.89069628\n",
      "Iteration 61978, loss = 1.92149457\n",
      "Iteration 61979, loss = 1.80585803\n",
      "Iteration 61980, loss = 1.78474207\n",
      "Iteration 61981, loss = 1.48402727\n",
      "Iteration 61982, loss = 1.25743219\n",
      "Iteration 61983, loss = 1.19756048\n",
      "Iteration 61984, loss = 1.16174144\n",
      "Iteration 61985, loss = 1.22126899\n",
      "Iteration 61986, loss = 1.61990548\n",
      "Iteration 61987, loss = 1.44067134\n",
      "Iteration 61988, loss = 1.30956512\n",
      "Iteration 61989, loss = 1.36412362\n",
      "Iteration 61990, loss = 1.57435987\n",
      "Iteration 61991, loss = 1.85017112\n",
      "Iteration 61992, loss = 1.79846879\n",
      "Iteration 61993, loss = 1.61081401\n",
      "Iteration 61994, loss = 1.55805263\n",
      "Iteration 61995, loss = 1.59752768\n",
      "Iteration 61996, loss = 1.51422542\n",
      "Iteration 61997, loss = 2.67945357\n",
      "Iteration 61998, loss = 2.84622944\n",
      "Iteration 61999, loss = 2.71092275\n",
      "Iteration 62000, loss = 2.80050211\n",
      "Iteration 62001, loss = 3.26065099\n",
      "Iteration 62002, loss = 2.93848686\n",
      "Iteration 62003, loss = 2.14562371\n",
      "Iteration 62004, loss = 3.00937080\n",
      "Iteration 62005, loss = 1.90232536\n",
      "Iteration 62006, loss = 1.67689130\n",
      "Iteration 62007, loss = 1.62608297\n",
      "Iteration 62008, loss = 1.33416656\n",
      "Iteration 62009, loss = 1.51282221\n",
      "Iteration 62010, loss = 1.66585276\n",
      "Iteration 62011, loss = 1.43967459\n",
      "Iteration 62012, loss = 1.43959136\n",
      "Iteration 62013, loss = 1.36799061\n",
      "Iteration 62014, loss = 1.36076711\n",
      "Iteration 62015, loss = 1.56102725\n",
      "Iteration 62016, loss = 1.54970984\n",
      "Iteration 62017, loss = 1.44566999\n",
      "Iteration 62018, loss = 1.31700499\n",
      "Iteration 62019, loss = 1.27922452\n",
      "Iteration 62020, loss = 1.37483379\n",
      "Iteration 62021, loss = 1.44512234\n",
      "Iteration 62022, loss = 1.35072229\n",
      "Iteration 62023, loss = 1.42424425\n",
      "Iteration 62024, loss = 1.36943585\n",
      "Iteration 62025, loss = 1.28083450\n",
      "Iteration 62026, loss = 1.39758580\n",
      "Iteration 62027, loss = 1.39857664\n",
      "Iteration 62028, loss = 1.49061887\n",
      "Iteration 62029, loss = 1.47707344\n",
      "Iteration 62030, loss = 1.43004255\n",
      "Iteration 62031, loss = 1.40206658\n",
      "Iteration 62032, loss = 1.60069792\n",
      "Iteration 62033, loss = 1.50786207\n",
      "Iteration 62034, loss = 1.53424915\n",
      "Iteration 62035, loss = 1.49589623\n",
      "Iteration 62036, loss = 1.69937477\n",
      "Iteration 62037, loss = 1.39408504\n",
      "Iteration 62038, loss = 1.32554612\n",
      "Iteration 62039, loss = 1.37499632\n",
      "Iteration 62040, loss = 1.48758729\n",
      "Iteration 62041, loss = 1.48167908\n",
      "Iteration 62042, loss = 1.35671413\n",
      "Iteration 62043, loss = 1.60492896\n",
      "Iteration 62044, loss = 1.46293991\n",
      "Iteration 62045, loss = 1.46678836\n",
      "Iteration 62046, loss = 1.45208295\n",
      "Iteration 62047, loss = 1.81961980\n",
      "Iteration 62048, loss = 1.73978569\n",
      "Iteration 62049, loss = 1.55351383\n",
      "Iteration 62050, loss = 1.38428176\n",
      "Iteration 62051, loss = 1.47915002\n",
      "Iteration 62052, loss = 1.39013226\n",
      "Iteration 62053, loss = 1.41306457\n",
      "Iteration 62054, loss = 1.43702365\n",
      "Iteration 62055, loss = 1.31154534\n",
      "Iteration 62056, loss = 1.33255668\n",
      "Iteration 62057, loss = 1.16069532\n",
      "Iteration 62058, loss = 1.32914270\n",
      "Iteration 62059, loss = 1.14900378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 62060, loss = 1.46228954\n",
      "Iteration 62061, loss = 1.24991509\n",
      "Iteration 62062, loss = 1.21346115\n",
      "Iteration 62063, loss = 1.51832793\n",
      "Iteration 62064, loss = 1.56996371\n",
      "Iteration 62065, loss = 1.20375049\n",
      "Iteration 62066, loss = 1.29219496\n",
      "Iteration 62067, loss = 1.43301445\n",
      "Iteration 62068, loss = 1.38428996\n",
      "Iteration 62069, loss = 1.18587936\n",
      "Iteration 62070, loss = 1.19649054\n",
      "Iteration 62071, loss = 1.22287880\n",
      "Iteration 62072, loss = 1.25177456\n",
      "Iteration 62073, loss = 1.38214070\n",
      "Iteration 62074, loss = 1.39731933\n",
      "Iteration 62075, loss = 1.25548534\n",
      "Iteration 62076, loss = 1.17878290\n",
      "Iteration 62077, loss = 1.57905054\n",
      "Iteration 62078, loss = 1.33489118\n",
      "Iteration 62079, loss = 1.28702127\n",
      "Iteration 62080, loss = 1.44131422\n",
      "Iteration 62081, loss = 1.17316296\n",
      "Iteration 62082, loss = 1.26211173\n",
      "Iteration 62083, loss = 1.23207512\n",
      "Iteration 62084, loss = 1.26649385\n",
      "Iteration 62085, loss = 1.32657376\n",
      "Iteration 62086, loss = 1.49860552\n",
      "Iteration 62087, loss = 1.28715784\n",
      "Iteration 62088, loss = 1.21014031\n",
      "Iteration 62089, loss = 1.41968298\n",
      "Iteration 62090, loss = 1.38179440\n",
      "Iteration 62091, loss = 1.43807324\n",
      "Iteration 62092, loss = 1.29238124\n",
      "Iteration 62093, loss = 1.35637027\n",
      "Iteration 62094, loss = 1.53324901\n",
      "Iteration 62095, loss = 1.46743939\n",
      "Iteration 62096, loss = 1.42663792\n",
      "Iteration 62097, loss = 1.51285588\n",
      "Iteration 62098, loss = 1.42000338\n",
      "Iteration 62099, loss = 1.54167741\n",
      "Iteration 62100, loss = 1.62175823\n",
      "Iteration 62101, loss = 1.49613255\n",
      "Iteration 62102, loss = 1.40596313\n",
      "Iteration 62103, loss = 1.31571722\n",
      "Iteration 62104, loss = 1.70174238\n",
      "Iteration 62105, loss = 1.74265626\n",
      "Iteration 62106, loss = 1.79863260\n",
      "Iteration 62107, loss = 2.01239979\n",
      "Iteration 62108, loss = 4.00659196\n",
      "Iteration 62109, loss = 5.06009947\n",
      "Iteration 62110, loss = 6.05705218\n",
      "Iteration 62111, loss = 3.61665515\n",
      "Iteration 62112, loss = 4.58923858\n",
      "Iteration 62113, loss = 2.89523022\n",
      "Iteration 62114, loss = 1.96975186\n",
      "Iteration 62115, loss = 2.06332908\n",
      "Iteration 62116, loss = 1.70845329\n",
      "Iteration 62117, loss = 1.80330782\n",
      "Iteration 62118, loss = 1.64544389\n",
      "Iteration 62119, loss = 1.76600444\n",
      "Iteration 62120, loss = 1.62058611\n",
      "Iteration 62121, loss = 1.63791832\n",
      "Iteration 62122, loss = 1.39351406\n",
      "Iteration 62123, loss = 1.35451178\n",
      "Iteration 62124, loss = 1.36856221\n",
      "Iteration 62125, loss = 1.38908686\n",
      "Iteration 62126, loss = 1.54631348\n",
      "Iteration 62127, loss = 1.37863411\n",
      "Iteration 62128, loss = 1.28145162\n",
      "Iteration 62129, loss = 1.35168036\n",
      "Iteration 62130, loss = 1.30215574\n",
      "Iteration 62131, loss = 1.42339982\n",
      "Iteration 62132, loss = 1.32895536\n",
      "Iteration 62133, loss = 1.75283454\n",
      "Iteration 62134, loss = 2.31205971\n",
      "Iteration 62135, loss = 1.81610900\n",
      "Iteration 62136, loss = 1.91793088\n",
      "Iteration 62137, loss = 1.72262537\n",
      "Iteration 62138, loss = 1.58584673\n",
      "Iteration 62139, loss = 1.75197594\n",
      "Iteration 62140, loss = 1.79753339\n",
      "Iteration 62141, loss = 1.81962117\n",
      "Iteration 62142, loss = 1.48740968\n",
      "Iteration 62143, loss = 1.57570291\n",
      "Iteration 62144, loss = 1.40889287\n",
      "Iteration 62145, loss = 1.24270522\n",
      "Iteration 62146, loss = 1.19800054\n",
      "Iteration 62147, loss = 1.34617076\n",
      "Iteration 62148, loss = 1.70275045\n",
      "Iteration 62149, loss = 1.81944135\n",
      "Iteration 62150, loss = 1.37383007\n",
      "Iteration 62151, loss = 1.45617133\n",
      "Iteration 62152, loss = 1.33058908\n",
      "Iteration 62153, loss = 1.27078369\n",
      "Iteration 62154, loss = 1.20549234\n",
      "Iteration 62155, loss = 1.23197047\n",
      "Iteration 62156, loss = 1.44133790\n",
      "Iteration 62157, loss = 1.48406116\n",
      "Iteration 62158, loss = 1.30508263\n",
      "Iteration 62159, loss = 1.18360341\n",
      "Iteration 62160, loss = 1.30446248\n",
      "Iteration 62161, loss = 1.33590634\n",
      "Iteration 62162, loss = 1.42800210\n",
      "Iteration 62163, loss = 1.14443063\n",
      "Iteration 62164, loss = 1.36083227\n",
      "Iteration 62165, loss = 2.09434482\n",
      "Iteration 62166, loss = 2.13223682\n",
      "Iteration 62167, loss = 1.91562076\n",
      "Iteration 62168, loss = 1.95557179\n",
      "Iteration 62169, loss = 2.57785264\n",
      "Iteration 62170, loss = 2.33578726\n",
      "Iteration 62171, loss = 2.09392547\n",
      "Iteration 62172, loss = 1.80770782\n",
      "Iteration 62173, loss = 1.52541792\n",
      "Iteration 62174, loss = 1.47012818\n",
      "Iteration 62175, loss = 1.33520745\n",
      "Iteration 62176, loss = 1.61689488\n",
      "Iteration 62177, loss = 1.48911733\n",
      "Iteration 62178, loss = 1.61358782\n",
      "Iteration 62179, loss = 1.35874224\n",
      "Iteration 62180, loss = 1.27029993\n",
      "Iteration 62181, loss = 1.28192065\n",
      "Iteration 62182, loss = 1.30562298\n",
      "Iteration 62183, loss = 1.41078507\n",
      "Iteration 62184, loss = 1.45293674\n",
      "Iteration 62185, loss = 1.49879122\n",
      "Iteration 62186, loss = 1.44818481\n",
      "Iteration 62187, loss = 1.37870700\n",
      "Iteration 62188, loss = 1.22009515\n",
      "Iteration 62189, loss = 1.52466069\n",
      "Iteration 62190, loss = 1.40843625\n",
      "Iteration 62191, loss = 1.44620874\n",
      "Iteration 62192, loss = 1.32289366\n",
      "Iteration 62193, loss = 1.29144509\n",
      "Iteration 62194, loss = 1.36917973\n",
      "Iteration 62195, loss = 1.25675489\n",
      "Iteration 62196, loss = 1.31633096\n",
      "Iteration 62197, loss = 1.40582103\n",
      "Iteration 62198, loss = 1.64518670\n",
      "Iteration 62199, loss = 1.57097770\n",
      "Iteration 62200, loss = 1.80928390\n",
      "Iteration 62201, loss = 2.34542771\n",
      "Iteration 62202, loss = 1.77678054\n",
      "Iteration 62203, loss = 2.43844002\n",
      "Iteration 62204, loss = 1.96456966\n",
      "Iteration 62205, loss = 1.53077239\n",
      "Iteration 62206, loss = 1.56986296\n",
      "Iteration 62207, loss = 1.55625325\n",
      "Iteration 62208, loss = 1.34641434\n",
      "Iteration 62209, loss = 1.61985854\n",
      "Iteration 62210, loss = 1.66836553\n",
      "Iteration 62211, loss = 1.65575907\n",
      "Iteration 62212, loss = 1.91627874\n",
      "Iteration 62213, loss = 2.03714791\n",
      "Iteration 62214, loss = 1.89195027\n",
      "Iteration 62215, loss = 1.63541236\n",
      "Iteration 62216, loss = 1.69621333\n",
      "Iteration 62217, loss = 1.92801592\n",
      "Iteration 62218, loss = 1.87941909\n",
      "Iteration 62219, loss = 1.72663418\n",
      "Iteration 62220, loss = 1.46014509\n",
      "Iteration 62221, loss = 1.43429857\n",
      "Iteration 62222, loss = 1.45468586\n",
      "Iteration 62223, loss = 1.38274007\n",
      "Iteration 62224, loss = 1.44296298\n",
      "Iteration 62225, loss = 1.74203260\n",
      "Iteration 62226, loss = 1.71856023\n",
      "Iteration 62227, loss = 1.87301944\n",
      "Iteration 62228, loss = 1.96778923\n",
      "Iteration 62229, loss = 1.62418467\n",
      "Iteration 62230, loss = 1.79560329\n",
      "Iteration 62231, loss = 1.50657361\n",
      "Iteration 62232, loss = 1.47517613\n",
      "Iteration 62233, loss = 1.45146331\n",
      "Iteration 62234, loss = 1.61045924\n",
      "Iteration 62235, loss = 2.43911275\n",
      "Iteration 62236, loss = 2.12299672\n",
      "Iteration 62237, loss = 1.59601719\n",
      "Iteration 62238, loss = 1.37150142\n",
      "Iteration 62239, loss = 1.38094165\n",
      "Iteration 62240, loss = 1.48725553\n",
      "Iteration 62241, loss = 1.52498510\n",
      "Iteration 62242, loss = 1.73616845\n",
      "Iteration 62243, loss = 1.80344121\n",
      "Iteration 62244, loss = 2.16804646\n",
      "Iteration 62245, loss = 2.87881557\n",
      "Iteration 62246, loss = 2.58989899\n",
      "Iteration 62247, loss = 2.60294468\n",
      "Iteration 62248, loss = 1.73474291\n",
      "Iteration 62249, loss = 1.75083197\n",
      "Iteration 62250, loss = 1.77465159\n",
      "Iteration 62251, loss = 1.35188427\n",
      "Iteration 62252, loss = 1.70076161\n",
      "Iteration 62253, loss = 1.58274772\n",
      "Iteration 62254, loss = 1.25519742\n",
      "Iteration 62255, loss = 1.26556153\n",
      "Iteration 62256, loss = 1.34691271\n",
      "Iteration 62257, loss = 1.30199163\n",
      "Iteration 62258, loss = 1.37157520\n",
      "Iteration 62259, loss = 1.22453674\n",
      "Iteration 62260, loss = 1.38915028\n",
      "Iteration 62261, loss = 1.53765480\n",
      "Iteration 62262, loss = 1.37318893\n",
      "Iteration 62263, loss = 1.62229982\n",
      "Iteration 62264, loss = 1.60643811\n",
      "Iteration 62265, loss = 2.17402614\n",
      "Iteration 62266, loss = 1.70512673\n",
      "Iteration 62267, loss = 1.61989774\n",
      "Iteration 62268, loss = 1.53697844\n",
      "Iteration 62269, loss = 1.47936230\n",
      "Iteration 62270, loss = 1.49858168\n",
      "Iteration 62271, loss = 1.46883442\n",
      "Iteration 62272, loss = 1.69184269\n",
      "Iteration 62273, loss = 1.49374905\n",
      "Iteration 62274, loss = 1.49329382\n",
      "Iteration 62275, loss = 1.40382470\n",
      "Iteration 62276, loss = 1.89821866\n",
      "Iteration 62277, loss = 1.63084505\n",
      "Iteration 62278, loss = 1.65791590\n",
      "Iteration 62279, loss = 1.71589267\n",
      "Iteration 62280, loss = 1.82881485\n",
      "Iteration 62281, loss = 1.99074963\n",
      "Iteration 62282, loss = 2.14681964\n",
      "Iteration 62283, loss = 1.93041074\n",
      "Iteration 62284, loss = 1.54962848\n",
      "Iteration 62285, loss = 1.44960517\n",
      "Iteration 62286, loss = 1.19906235\n",
      "Iteration 62287, loss = 1.22821217\n",
      "Iteration 62288, loss = 1.34949646\n",
      "Iteration 62289, loss = 1.56391980\n",
      "Iteration 62290, loss = 1.34851806\n",
      "Iteration 62291, loss = 1.26679310\n",
      "Iteration 62292, loss = 1.46654001\n",
      "Iteration 62293, loss = 1.47731310\n",
      "Iteration 62294, loss = 2.03753033\n",
      "Iteration 62295, loss = 2.12560484\n",
      "Iteration 62296, loss = 1.56018719\n",
      "Iteration 62297, loss = 1.45890571\n",
      "Iteration 62298, loss = 1.49198536\n",
      "Iteration 62299, loss = 1.41712135\n",
      "Iteration 62300, loss = 1.25067755\n",
      "Iteration 62301, loss = 1.28104429\n",
      "Iteration 62302, loss = 1.53969876\n",
      "Iteration 62303, loss = 1.87987457\n",
      "Iteration 62304, loss = 1.58529020\n",
      "Iteration 62305, loss = 1.42166789\n",
      "Iteration 62306, loss = 1.30832880\n",
      "Iteration 62307, loss = 1.34702316\n",
      "Iteration 62308, loss = 1.29320186\n",
      "Iteration 62309, loss = 1.20393690\n",
      "Iteration 62310, loss = 1.28536259\n",
      "Iteration 62311, loss = 1.44891360\n",
      "Iteration 62312, loss = 1.41572731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 62313, loss = 1.58825236\n",
      "Iteration 62314, loss = 1.89789186\n",
      "Iteration 62315, loss = 2.13191959\n",
      "Iteration 62316, loss = 1.32051108\n",
      "Iteration 62317, loss = 1.47967672\n",
      "Iteration 62318, loss = 1.34185837\n",
      "Iteration 62319, loss = 1.16888431\n",
      "Iteration 62320, loss = 1.25869451\n",
      "Iteration 62321, loss = 1.36306293\n",
      "Iteration 62322, loss = 1.44162144\n",
      "Iteration 62323, loss = 1.65683331\n",
      "Iteration 62324, loss = 1.51375948\n",
      "Iteration 62325, loss = 1.52266752\n",
      "Iteration 62326, loss = 1.53271963\n",
      "Iteration 62327, loss = 1.68304815\n",
      "Iteration 62328, loss = 1.65186476\n",
      "Iteration 62329, loss = 1.64545716\n",
      "Iteration 62330, loss = 1.70649260\n",
      "Iteration 62331, loss = 2.08744527\n",
      "Iteration 62332, loss = 2.00182197\n",
      "Iteration 62333, loss = 1.64523655\n",
      "Iteration 62334, loss = 2.25545745\n",
      "Iteration 62335, loss = 1.60186302\n",
      "Iteration 62336, loss = 1.71939592\n",
      "Iteration 62337, loss = 1.78534709\n",
      "Iteration 62338, loss = 1.48172414\n",
      "Iteration 62339, loss = 1.42182495\n",
      "Iteration 62340, loss = 1.47279390\n",
      "Iteration 62341, loss = 1.44550561\n",
      "Iteration 62342, loss = 1.30942998\n",
      "Iteration 62343, loss = 1.97048599\n",
      "Iteration 62344, loss = 2.87386639\n",
      "Iteration 62345, loss = 2.97907889\n",
      "Iteration 62346, loss = 1.84643742\n",
      "Iteration 62347, loss = 1.78413562\n",
      "Iteration 62348, loss = 1.81624803\n",
      "Iteration 62349, loss = 1.64809787\n",
      "Iteration 62350, loss = 1.55790701\n",
      "Iteration 62351, loss = 1.41229595\n",
      "Iteration 62352, loss = 1.74813137\n",
      "Iteration 62353, loss = 1.50240841\n",
      "Iteration 62354, loss = 1.49303034\n",
      "Iteration 62355, loss = 1.33546217\n",
      "Iteration 62356, loss = 1.16936964\n",
      "Iteration 62357, loss = 1.20264493\n",
      "Iteration 62358, loss = 1.25000221\n",
      "Iteration 62359, loss = 1.37162700\n",
      "Iteration 62360, loss = 1.35600886\n",
      "Iteration 62361, loss = 1.52933200\n",
      "Iteration 62362, loss = 1.65320634\n",
      "Iteration 62363, loss = 1.59380470\n",
      "Iteration 62364, loss = 1.72993710\n",
      "Iteration 62365, loss = 1.87979623\n",
      "Iteration 62366, loss = 1.33591972\n",
      "Iteration 62367, loss = 1.29382505\n",
      "Iteration 62368, loss = 1.36877547\n",
      "Iteration 62369, loss = 1.41772119\n",
      "Iteration 62370, loss = 1.17325794\n",
      "Iteration 62371, loss = 1.20489282\n",
      "Iteration 62372, loss = 1.21255106\n",
      "Iteration 62373, loss = 1.34259300\n",
      "Iteration 62374, loss = 1.31516450\n",
      "Iteration 62375, loss = 1.19630779\n",
      "Iteration 62376, loss = 1.17430144\n",
      "Iteration 62377, loss = 1.21837662\n",
      "Iteration 62378, loss = 1.52722002\n",
      "Iteration 62379, loss = 1.29974894\n",
      "Iteration 62380, loss = 1.30987760\n",
      "Iteration 62381, loss = 1.69741549\n",
      "Iteration 62382, loss = 1.67190055\n",
      "Iteration 62383, loss = 1.42017365\n",
      "Iteration 62384, loss = 1.65326026\n",
      "Iteration 62385, loss = 1.73305720\n",
      "Iteration 62386, loss = 1.87596704\n",
      "Iteration 62387, loss = 1.94120950\n",
      "Iteration 62388, loss = 1.58433050\n",
      "Iteration 62389, loss = 2.94206746\n",
      "Iteration 62390, loss = 1.69253900\n",
      "Iteration 62391, loss = 1.83183228\n",
      "Iteration 62392, loss = 1.50592201\n",
      "Iteration 62393, loss = 1.59804635\n",
      "Iteration 62394, loss = 1.54935295\n",
      "Iteration 62395, loss = 1.71612523\n",
      "Iteration 62396, loss = 1.72832948\n",
      "Iteration 62397, loss = 1.61399446\n",
      "Iteration 62398, loss = 1.30611131\n",
      "Iteration 62399, loss = 1.48343262\n",
      "Iteration 62400, loss = 1.82071669\n",
      "Iteration 62401, loss = 1.98734232\n",
      "Iteration 62402, loss = 2.18123349\n",
      "Iteration 62403, loss = 1.78389276\n",
      "Iteration 62404, loss = 1.54614996\n",
      "Iteration 62405, loss = 1.28193351\n",
      "Iteration 62406, loss = 1.27766028\n",
      "Iteration 62407, loss = 1.29199748\n",
      "Iteration 62408, loss = 1.31999853\n",
      "Iteration 62409, loss = 1.53327270\n",
      "Iteration 62410, loss = 1.28216412\n",
      "Iteration 62411, loss = 1.24153484\n",
      "Iteration 62412, loss = 1.36101326\n",
      "Iteration 62413, loss = 1.28670809\n",
      "Iteration 62414, loss = 1.28740315\n",
      "Iteration 62415, loss = 1.15104666\n",
      "Iteration 62416, loss = 1.28628599\n",
      "Iteration 62417, loss = 1.54756588\n",
      "Iteration 62418, loss = 1.57294013\n",
      "Iteration 62419, loss = 1.73989504\n",
      "Iteration 62420, loss = 1.30285111\n",
      "Iteration 62421, loss = 1.29127246\n",
      "Iteration 62422, loss = 1.76417657\n",
      "Iteration 62423, loss = 1.36588115\n",
      "Iteration 62424, loss = 1.47599078\n",
      "Iteration 62425, loss = 1.52121970\n",
      "Iteration 62426, loss = 1.60354398\n",
      "Iteration 62427, loss = 1.54712883\n",
      "Iteration 62428, loss = 1.35935768\n",
      "Iteration 62429, loss = 1.29720504\n",
      "Iteration 62430, loss = 1.65953557\n",
      "Iteration 62431, loss = 2.22597209\n",
      "Iteration 62432, loss = 1.88905107\n",
      "Iteration 62433, loss = 2.12438362\n",
      "Iteration 62434, loss = 2.40102891\n",
      "Iteration 62435, loss = 2.04788954\n",
      "Iteration 62436, loss = 2.03044644\n",
      "Iteration 62437, loss = 1.83656244\n",
      "Iteration 62438, loss = 1.81678242\n",
      "Iteration 62439, loss = 1.46559066\n",
      "Iteration 62440, loss = 1.28070037\n",
      "Iteration 62441, loss = 1.65525483\n",
      "Iteration 62442, loss = 1.75874276\n",
      "Iteration 62443, loss = 1.96322766\n",
      "Iteration 62444, loss = 1.59412274\n",
      "Iteration 62445, loss = 1.60088404\n",
      "Iteration 62446, loss = 1.51236553\n",
      "Iteration 62447, loss = 1.35433390\n",
      "Iteration 62448, loss = 1.54902236\n",
      "Iteration 62449, loss = 1.81285423\n",
      "Iteration 62450, loss = 1.47467576\n",
      "Iteration 62451, loss = 1.42016919\n",
      "Iteration 62452, loss = 1.43227444\n",
      "Iteration 62453, loss = 1.24066144\n",
      "Iteration 62454, loss = 1.16742464\n",
      "Iteration 62455, loss = 1.28621242\n",
      "Iteration 62456, loss = 1.29878763\n",
      "Iteration 62457, loss = 1.28684365\n",
      "Iteration 62458, loss = 1.81598516\n",
      "Iteration 62459, loss = 1.36989048\n",
      "Iteration 62460, loss = 1.28403397\n",
      "Iteration 62461, loss = 1.37703945\n",
      "Iteration 62462, loss = 1.40589167\n",
      "Iteration 62463, loss = 1.21071765\n",
      "Iteration 62464, loss = 1.19427979\n",
      "Iteration 62465, loss = 1.18120724\n",
      "Iteration 62466, loss = 1.43980232\n",
      "Iteration 62467, loss = 1.32434388\n",
      "Iteration 62468, loss = 1.34884365\n",
      "Iteration 62469, loss = 1.25819990\n",
      "Iteration 62470, loss = 1.54909245\n",
      "Iteration 62471, loss = 1.41803711\n",
      "Iteration 62472, loss = 1.32902708\n",
      "Iteration 62473, loss = 1.52048266\n",
      "Iteration 62474, loss = 1.19531809\n",
      "Iteration 62475, loss = 1.14686712\n",
      "Iteration 62476, loss = 1.14879482\n",
      "Iteration 62477, loss = 1.27907174\n",
      "Iteration 62478, loss = 1.18784482\n",
      "Iteration 62479, loss = 1.22229053\n",
      "Iteration 62480, loss = 1.21329899\n",
      "Iteration 62481, loss = 1.30386047\n",
      "Iteration 62482, loss = 1.31507411\n",
      "Iteration 62483, loss = 1.33040618\n",
      "Iteration 62484, loss = 2.12363895\n",
      "Iteration 62485, loss = 1.97700725\n",
      "Iteration 62486, loss = 1.96960802\n",
      "Iteration 62487, loss = 1.41540734\n",
      "Iteration 62488, loss = 1.58952881\n",
      "Iteration 62489, loss = 1.98181673\n",
      "Iteration 62490, loss = 1.61962136\n",
      "Iteration 62491, loss = 2.71698980\n",
      "Iteration 62492, loss = 2.17170959\n",
      "Iteration 62493, loss = 2.15931041\n",
      "Iteration 62494, loss = 1.91880984\n",
      "Iteration 62495, loss = 1.66197973\n",
      "Iteration 62496, loss = 1.85523000\n",
      "Iteration 62497, loss = 2.02404536\n",
      "Iteration 62498, loss = 1.65821154\n",
      "Iteration 62499, loss = 1.46854113\n",
      "Iteration 62500, loss = 1.58566861\n",
      "Iteration 62501, loss = 1.69414910\n",
      "Iteration 62502, loss = 1.38262487\n",
      "Iteration 62503, loss = 1.35313297\n",
      "Iteration 62504, loss = 1.53742841\n",
      "Iteration 62505, loss = 1.52861123\n",
      "Iteration 62506, loss = 1.25964160\n",
      "Iteration 62507, loss = 1.17684063\n",
      "Iteration 62508, loss = 1.16226217\n",
      "Iteration 62509, loss = 1.19968747\n",
      "Iteration 62510, loss = 1.17077867\n",
      "Iteration 62511, loss = 1.18487955\n",
      "Iteration 62512, loss = 1.21088936\n",
      "Iteration 62513, loss = 1.32504143\n",
      "Iteration 62514, loss = 1.47785860\n",
      "Iteration 62515, loss = 1.68593058\n",
      "Iteration 62516, loss = 1.49792537\n",
      "Iteration 62517, loss = 1.63166303\n",
      "Iteration 62518, loss = 1.67566469\n",
      "Iteration 62519, loss = 1.47662151\n",
      "Iteration 62520, loss = 1.77481429\n",
      "Iteration 62521, loss = 1.72560731\n",
      "Iteration 62522, loss = 1.75959193\n",
      "Iteration 62523, loss = 1.92923138\n",
      "Iteration 62524, loss = 1.61197533\n",
      "Iteration 62525, loss = 1.50494875\n",
      "Iteration 62526, loss = 1.65549143\n",
      "Iteration 62527, loss = 1.32241051\n",
      "Iteration 62528, loss = 1.23542499\n",
      "Iteration 62529, loss = 1.24387451\n",
      "Iteration 62530, loss = 1.26173570\n",
      "Iteration 62531, loss = 1.27348823\n",
      "Iteration 62532, loss = 1.20789957\n",
      "Iteration 62533, loss = 1.27026038\n",
      "Iteration 62534, loss = 1.38163760\n",
      "Iteration 62535, loss = 1.27768455\n",
      "Iteration 62536, loss = 1.33497539\n",
      "Iteration 62537, loss = 1.27779209\n",
      "Iteration 62538, loss = 1.23059662\n",
      "Iteration 62539, loss = 1.26439793\n",
      "Iteration 62540, loss = 1.33577566\n",
      "Iteration 62541, loss = 1.27100568\n",
      "Iteration 62542, loss = 1.34911524\n",
      "Iteration 62543, loss = 1.46102295\n",
      "Iteration 62544, loss = 1.45123255\n",
      "Iteration 62545, loss = 1.82462231\n",
      "Iteration 62546, loss = 2.16874431\n",
      "Iteration 62547, loss = 1.63928226\n",
      "Iteration 62548, loss = 1.50553882\n",
      "Iteration 62549, loss = 1.94889705\n",
      "Iteration 62550, loss = 1.67250003\n",
      "Iteration 62551, loss = 1.49664936\n",
      "Iteration 62552, loss = 1.58363773\n",
      "Iteration 62553, loss = 2.16821862\n",
      "Iteration 62554, loss = 1.93942314\n",
      "Iteration 62555, loss = 2.11459750\n",
      "Iteration 62556, loss = 1.86416361\n",
      "Iteration 62557, loss = 1.50683249\n",
      "Iteration 62558, loss = 1.40771445\n",
      "Iteration 62559, loss = 1.27226826\n",
      "Iteration 62560, loss = 1.20869246\n",
      "Iteration 62561, loss = 1.29077394\n",
      "Iteration 62562, loss = 1.34152936\n",
      "Iteration 62563, loss = 1.46719133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 62564, loss = 1.30236269\n",
      "Iteration 62565, loss = 1.48160963\n",
      "Iteration 62566, loss = 2.01106174\n",
      "Iteration 62567, loss = 2.65368540\n",
      "Iteration 62568, loss = 2.07666289\n",
      "Iteration 62569, loss = 2.14136042\n",
      "Iteration 62570, loss = 1.92036793\n",
      "Iteration 62571, loss = 1.59130345\n",
      "Iteration 62572, loss = 1.30660419\n",
      "Iteration 62573, loss = 1.19936725\n",
      "Iteration 62574, loss = 1.34879238\n",
      "Iteration 62575, loss = 1.43541655\n",
      "Iteration 62576, loss = 1.65027914\n",
      "Iteration 62577, loss = 1.42919593\n",
      "Iteration 62578, loss = 1.48848027\n",
      "Iteration 62579, loss = 1.38713499\n",
      "Iteration 62580, loss = 1.34998294\n",
      "Iteration 62581, loss = 1.30134998\n",
      "Iteration 62582, loss = 1.54583707\n",
      "Iteration 62583, loss = 1.33774402\n",
      "Iteration 62584, loss = 1.47497242\n",
      "Iteration 62585, loss = 1.56070427\n",
      "Iteration 62586, loss = 1.27500821\n",
      "Iteration 62587, loss = 1.48249821\n",
      "Iteration 62588, loss = 1.46136584\n",
      "Iteration 62589, loss = 1.35486138\n",
      "Iteration 62590, loss = 1.39866758\n",
      "Iteration 62591, loss = 1.30965442\n",
      "Iteration 62592, loss = 1.43169543\n",
      "Iteration 62593, loss = 1.24223734\n",
      "Iteration 62594, loss = 1.33748495\n",
      "Iteration 62595, loss = 1.25648111\n",
      "Iteration 62596, loss = 1.53395469\n",
      "Iteration 62597, loss = 1.41509592\n",
      "Iteration 62598, loss = 1.57366125\n",
      "Iteration 62599, loss = 1.33277120\n",
      "Iteration 62600, loss = 1.48493277\n",
      "Iteration 62601, loss = 1.47635257\n",
      "Iteration 62602, loss = 2.15208206\n",
      "Iteration 62603, loss = 2.17735169\n",
      "Iteration 62604, loss = 1.63829976\n",
      "Iteration 62605, loss = 1.56290301\n",
      "Iteration 62606, loss = 1.61929558\n",
      "Iteration 62607, loss = 1.52388917\n",
      "Iteration 62608, loss = 1.27549406\n",
      "Iteration 62609, loss = 1.39844712\n",
      "Iteration 62610, loss = 1.27374512\n",
      "Iteration 62611, loss = 1.45740980\n",
      "Iteration 62612, loss = 1.49168286\n",
      "Iteration 62613, loss = 1.45688238\n",
      "Iteration 62614, loss = 1.61388288\n",
      "Iteration 62615, loss = 1.47932766\n",
      "Iteration 62616, loss = 1.56567553\n",
      "Iteration 62617, loss = 1.81514880\n",
      "Iteration 62618, loss = 1.67302111\n",
      "Iteration 62619, loss = 1.41232559\n",
      "Iteration 62620, loss = 1.27983202\n",
      "Iteration 62621, loss = 1.44377560\n",
      "Iteration 62622, loss = 1.29098836\n",
      "Iteration 62623, loss = 1.39716378\n",
      "Iteration 62624, loss = 1.66516383\n",
      "Iteration 62625, loss = 2.03666423\n",
      "Iteration 62626, loss = 1.71464788\n",
      "Iteration 62627, loss = 1.93974109\n",
      "Iteration 62628, loss = 1.93561901\n",
      "Iteration 62629, loss = 1.52032450\n",
      "Iteration 62630, loss = 1.30950595\n",
      "Iteration 62631, loss = 1.35705096\n",
      "Iteration 62632, loss = 1.45908396\n",
      "Iteration 62633, loss = 1.61499755\n",
      "Iteration 62634, loss = 1.38796005\n",
      "Iteration 62635, loss = 1.36742067\n",
      "Iteration 62636, loss = 1.30839346\n",
      "Iteration 62637, loss = 1.35380784\n",
      "Iteration 62638, loss = 1.23390722\n",
      "Iteration 62639, loss = 1.20618977\n",
      "Iteration 62640, loss = 1.24626891\n",
      "Iteration 62641, loss = 1.37700932\n",
      "Iteration 62642, loss = 1.53075681\n",
      "Iteration 62643, loss = 1.44935388\n",
      "Iteration 62644, loss = 1.47365237\n",
      "Iteration 62645, loss = 1.26564969\n",
      "Iteration 62646, loss = 1.34119439\n",
      "Iteration 62647, loss = 1.20367640\n",
      "Iteration 62648, loss = 1.45435509\n",
      "Iteration 62649, loss = 1.35924134\n",
      "Iteration 62650, loss = 1.43466591\n",
      "Iteration 62651, loss = 1.86460753\n",
      "Iteration 62652, loss = 1.64586147\n",
      "Iteration 62653, loss = 1.42961267\n",
      "Iteration 62654, loss = 1.52793521\n",
      "Iteration 62655, loss = 1.44484106\n",
      "Iteration 62656, loss = 1.61867155\n",
      "Iteration 62657, loss = 1.72646082\n",
      "Iteration 62658, loss = 2.42961298\n",
      "Iteration 62659, loss = 1.91928354\n",
      "Iteration 62660, loss = 2.64616667\n",
      "Iteration 62661, loss = 1.52189577\n",
      "Iteration 62662, loss = 2.42847402\n",
      "Iteration 62663, loss = 1.92470061\n",
      "Iteration 62664, loss = 1.74532700\n",
      "Iteration 62665, loss = 1.46034899\n",
      "Iteration 62666, loss = 1.36612938\n",
      "Iteration 62667, loss = 1.45668646\n",
      "Iteration 62668, loss = 1.63234359\n",
      "Iteration 62669, loss = 1.45029162\n",
      "Iteration 62670, loss = 1.33735351\n",
      "Iteration 62671, loss = 1.26885165\n",
      "Iteration 62672, loss = 1.25333495\n",
      "Iteration 62673, loss = 1.26065838\n",
      "Iteration 62674, loss = 1.27852642\n",
      "Iteration 62675, loss = 1.23703854\n",
      "Iteration 62676, loss = 1.44690252\n",
      "Iteration 62677, loss = 1.23403171\n",
      "Iteration 62678, loss = 1.48139128\n",
      "Iteration 62679, loss = 1.41412421\n",
      "Iteration 62680, loss = 1.55345956\n",
      "Iteration 62681, loss = 1.98399950\n",
      "Iteration 62682, loss = 1.70269910\n",
      "Iteration 62683, loss = 1.74747869\n",
      "Iteration 62684, loss = 1.54259700\n",
      "Iteration 62685, loss = 1.59587682\n",
      "Iteration 62686, loss = 1.84665868\n",
      "Iteration 62687, loss = 2.77003676\n",
      "Iteration 62688, loss = 2.98792448\n",
      "Iteration 62689, loss = 2.68100231\n",
      "Iteration 62690, loss = 2.59142069\n",
      "Iteration 62691, loss = 3.38932012\n",
      "Iteration 62692, loss = 3.20570128\n",
      "Iteration 62693, loss = 4.02754882\n",
      "Iteration 62694, loss = 2.35086006\n",
      "Iteration 62695, loss = 3.81347161\n",
      "Iteration 62696, loss = 2.90995432\n",
      "Iteration 62697, loss = 1.88343657\n",
      "Iteration 62698, loss = 1.86474692\n",
      "Iteration 62699, loss = 2.12596821\n",
      "Iteration 62700, loss = 2.16672603\n",
      "Iteration 62701, loss = 2.24275360\n",
      "Iteration 62702, loss = 1.67451001\n",
      "Iteration 62703, loss = 1.63006459\n",
      "Iteration 62704, loss = 2.08883197\n",
      "Iteration 62705, loss = 1.75346401\n",
      "Iteration 62706, loss = 1.54505419\n",
      "Iteration 62707, loss = 1.54273183\n",
      "Iteration 62708, loss = 1.49662490\n",
      "Iteration 62709, loss = 1.57590568\n",
      "Iteration 62710, loss = 1.43802830\n",
      "Iteration 62711, loss = 1.78094953\n",
      "Iteration 62712, loss = 1.54996217\n",
      "Iteration 62713, loss = 1.40711427\n",
      "Iteration 62714, loss = 1.46504593\n",
      "Iteration 62715, loss = 1.79354106\n",
      "Iteration 62716, loss = 1.70130582\n",
      "Iteration 62717, loss = 1.53258722\n",
      "Iteration 62718, loss = 1.74190882\n",
      "Iteration 62719, loss = 1.50259270\n",
      "Iteration 62720, loss = 1.71232372\n",
      "Iteration 62721, loss = 1.46606404\n",
      "Iteration 62722, loss = 1.42442068\n",
      "Iteration 62723, loss = 1.40200836\n",
      "Iteration 62724, loss = 1.42490955\n",
      "Iteration 62725, loss = 1.40975537\n",
      "Iteration 62726, loss = 1.44499135\n",
      "Iteration 62727, loss = 1.37792469\n",
      "Iteration 62728, loss = 1.59920352\n",
      "Iteration 62729, loss = 1.88311540\n",
      "Iteration 62730, loss = 1.74109622\n",
      "Iteration 62731, loss = 1.96440943\n",
      "Iteration 62732, loss = 1.60773903\n",
      "Iteration 62733, loss = 1.43124413\n",
      "Iteration 62734, loss = 1.31287601\n",
      "Iteration 62735, loss = 1.32364191\n",
      "Iteration 62736, loss = 1.25867195\n",
      "Iteration 62737, loss = 1.33276669\n",
      "Iteration 62738, loss = 1.49438294\n",
      "Iteration 62739, loss = 1.53966084\n",
      "Iteration 62740, loss = 1.33866505\n",
      "Iteration 62741, loss = 1.40145821\n",
      "Iteration 62742, loss = 1.20124401\n",
      "Iteration 62743, loss = 1.41946791\n",
      "Iteration 62744, loss = 1.30041504\n",
      "Iteration 62745, loss = 1.34815396\n",
      "Iteration 62746, loss = 1.23345326\n",
      "Iteration 62747, loss = 1.16879703\n",
      "Iteration 62748, loss = 1.31125305\n",
      "Iteration 62749, loss = 1.12746690\n",
      "Iteration 62750, loss = 1.36585778\n",
      "Iteration 62751, loss = 1.18754836\n",
      "Iteration 62752, loss = 1.20657020\n",
      "Iteration 62753, loss = 1.17371813\n",
      "Iteration 62754, loss = 1.15826955\n",
      "Iteration 62755, loss = 1.16825194\n",
      "Iteration 62756, loss = 1.18249077\n",
      "Iteration 62757, loss = 1.20303679\n",
      "Iteration 62758, loss = 1.41067086\n",
      "Iteration 62759, loss = 1.71752229\n",
      "Iteration 62760, loss = 1.41035733\n",
      "Iteration 62761, loss = 1.42381958\n",
      "Iteration 62762, loss = 1.41100045\n",
      "Iteration 62763, loss = 1.36005959\n",
      "Iteration 62764, loss = 1.64304943\n",
      "Iteration 62765, loss = 1.61824174\n",
      "Iteration 62766, loss = 1.40883444\n",
      "Iteration 62767, loss = 1.26772235\n",
      "Iteration 62768, loss = 1.25897275\n",
      "Iteration 62769, loss = 1.47080954\n",
      "Iteration 62770, loss = 1.61569671\n",
      "Iteration 62771, loss = 1.41226363\n",
      "Iteration 62772, loss = 1.52750560\n",
      "Iteration 62773, loss = 1.36182577\n",
      "Iteration 62774, loss = 1.55277412\n",
      "Iteration 62775, loss = 1.46736660\n",
      "Iteration 62776, loss = 1.37702741\n",
      "Iteration 62777, loss = 1.33765367\n",
      "Iteration 62778, loss = 1.29533269\n",
      "Iteration 62779, loss = 1.48593489\n",
      "Iteration 62780, loss = 1.46296913\n",
      "Iteration 62781, loss = 1.41581493\n",
      "Iteration 62782, loss = 1.36858479\n",
      "Iteration 62783, loss = 1.30113987\n",
      "Iteration 62784, loss = 1.12120919\n",
      "Iteration 62785, loss = 1.37464022\n",
      "Iteration 62786, loss = 1.41359443\n",
      "Iteration 62787, loss = 1.47275021\n",
      "Iteration 62788, loss = 1.33692237\n",
      "Iteration 62789, loss = 1.34925204\n",
      "Iteration 62790, loss = 1.26677027\n",
      "Iteration 62791, loss = 1.23773324\n",
      "Iteration 62792, loss = 1.58134328\n",
      "Iteration 62793, loss = 1.74026716\n",
      "Iteration 62794, loss = 1.73983961\n",
      "Iteration 62795, loss = 1.47369537\n",
      "Iteration 62796, loss = 1.51429375\n",
      "Iteration 62797, loss = 2.00451234\n",
      "Iteration 62798, loss = 1.77186026\n",
      "Iteration 62799, loss = 1.94318997\n",
      "Iteration 62800, loss = 1.80185712\n",
      "Iteration 62801, loss = 1.50151143\n",
      "Iteration 62802, loss = 1.65791882\n",
      "Iteration 62803, loss = 1.51241892\n",
      "Iteration 62804, loss = 1.23885896\n",
      "Iteration 62805, loss = 1.68548135\n",
      "Iteration 62806, loss = 1.80779011\n",
      "Iteration 62807, loss = 1.75687975\n",
      "Iteration 62808, loss = 1.74656209\n",
      "Iteration 62809, loss = 1.78781102\n",
      "Iteration 62810, loss = 1.63283081\n",
      "Iteration 62811, loss = 1.42199940\n",
      "Iteration 62812, loss = 1.59035249\n",
      "Iteration 62813, loss = 1.79970650\n",
      "Iteration 62814, loss = 1.90067144\n",
      "Iteration 62815, loss = 1.81692407\n",
      "Iteration 62816, loss = 1.82495443\n",
      "Iteration 62817, loss = 2.82081011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 62818, loss = 2.14149216\n",
      "Iteration 62819, loss = 1.97412649\n",
      "Iteration 62820, loss = 2.11339639\n",
      "Iteration 62821, loss = 1.97740236\n",
      "Iteration 62822, loss = 1.87318527\n",
      "Iteration 62823, loss = 1.73394645\n",
      "Iteration 62824, loss = 1.50121703\n",
      "Iteration 62825, loss = 1.70704730\n",
      "Iteration 62826, loss = 2.03128267\n",
      "Iteration 62827, loss = 1.79989680\n",
      "Iteration 62828, loss = 2.53550209\n",
      "Iteration 62829, loss = 1.90447649\n",
      "Iteration 62830, loss = 1.53255508\n",
      "Iteration 62831, loss = 1.33831954\n",
      "Iteration 62832, loss = 1.29971165\n",
      "Iteration 62833, loss = 1.64839803\n",
      "Iteration 62834, loss = 1.66697279\n",
      "Iteration 62835, loss = 1.57372968\n",
      "Iteration 62836, loss = 1.38370271\n",
      "Iteration 62837, loss = 1.27192832\n",
      "Iteration 62838, loss = 1.44156174\n",
      "Iteration 62839, loss = 1.66604481\n",
      "Iteration 62840, loss = 1.93764683\n",
      "Iteration 62841, loss = 2.21973265\n",
      "Iteration 62842, loss = 2.30706442\n",
      "Iteration 62843, loss = 1.87240269\n",
      "Iteration 62844, loss = 1.66045297\n",
      "Iteration 62845, loss = 1.29764314\n",
      "Iteration 62846, loss = 1.35103715\n",
      "Iteration 62847, loss = 1.65823743\n",
      "Iteration 62848, loss = 1.66074472\n",
      "Iteration 62849, loss = 1.48726818\n",
      "Iteration 62850, loss = 1.50480861\n",
      "Iteration 62851, loss = 1.42102729\n",
      "Iteration 62852, loss = 1.33995057\n",
      "Iteration 62853, loss = 1.32452065\n",
      "Iteration 62854, loss = 1.38762502\n",
      "Iteration 62855, loss = 1.48582393\n",
      "Iteration 62856, loss = 1.82886366\n",
      "Iteration 62857, loss = 1.55758642\n",
      "Iteration 62858, loss = 1.62720151\n",
      "Iteration 62859, loss = 1.71104679\n",
      "Iteration 62860, loss = 1.29714034\n",
      "Iteration 62861, loss = 1.26355898\n",
      "Iteration 62862, loss = 1.31554127\n",
      "Iteration 62863, loss = 1.40152703\n",
      "Iteration 62864, loss = 1.55270453\n",
      "Iteration 62865, loss = 1.51875245\n",
      "Iteration 62866, loss = 1.31622083\n",
      "Iteration 62867, loss = 1.66781579\n",
      "Iteration 62868, loss = 1.70355185\n",
      "Iteration 62869, loss = 1.85469360\n",
      "Iteration 62870, loss = 1.58434917\n",
      "Iteration 62871, loss = 1.79673738\n",
      "Iteration 62872, loss = 1.71036430\n",
      "Iteration 62873, loss = 1.31529043\n",
      "Iteration 62874, loss = 1.40088615\n",
      "Iteration 62875, loss = 1.35653706\n",
      "Iteration 62876, loss = 1.36560077\n",
      "Iteration 62877, loss = 1.24276315\n",
      "Iteration 62878, loss = 1.38689051\n",
      "Iteration 62879, loss = 1.37325937\n",
      "Iteration 62880, loss = 1.33384773\n",
      "Iteration 62881, loss = 1.37851351\n",
      "Iteration 62882, loss = 1.47296646\n",
      "Iteration 62883, loss = 1.34345979\n",
      "Iteration 62884, loss = 1.28603017\n",
      "Iteration 62885, loss = 1.29338606\n",
      "Iteration 62886, loss = 1.46932371\n",
      "Iteration 62887, loss = 1.33512540\n",
      "Iteration 62888, loss = 1.19976504\n",
      "Iteration 62889, loss = 1.15489126\n",
      "Iteration 62890, loss = 1.24436598\n",
      "Iteration 62891, loss = 1.37367798\n",
      "Iteration 62892, loss = 1.25453509\n",
      "Iteration 62893, loss = 1.27663518\n",
      "Iteration 62894, loss = 1.36735844\n",
      "Iteration 62895, loss = 1.27980715\n",
      "Iteration 62896, loss = 1.26050939\n",
      "Iteration 62897, loss = 1.33209104\n",
      "Iteration 62898, loss = 1.43046200\n",
      "Iteration 62899, loss = 1.38922794\n",
      "Iteration 62900, loss = 1.51455847\n",
      "Iteration 62901, loss = 1.61621841\n",
      "Iteration 62902, loss = 1.41528936\n",
      "Iteration 62903, loss = 1.53965960\n",
      "Iteration 62904, loss = 1.37554984\n",
      "Iteration 62905, loss = 1.31238311\n",
      "Iteration 62906, loss = 1.64660452\n",
      "Iteration 62907, loss = 2.45889492\n",
      "Iteration 62908, loss = 2.27844778\n",
      "Iteration 62909, loss = 1.84120335\n",
      "Iteration 62910, loss = 3.43518067\n",
      "Iteration 62911, loss = 5.01354807\n",
      "Iteration 62912, loss = 2.82519024\n",
      "Iteration 62913, loss = 2.67663447\n",
      "Iteration 62914, loss = 2.23582051\n",
      "Iteration 62915, loss = 1.81353696\n",
      "Iteration 62916, loss = 2.19340817\n",
      "Iteration 62917, loss = 1.68855616\n",
      "Iteration 62918, loss = 1.65251828\n",
      "Iteration 62919, loss = 1.39100656\n",
      "Iteration 62920, loss = 1.41143136\n",
      "Iteration 62921, loss = 1.35928602\n",
      "Iteration 62922, loss = 1.25209372\n",
      "Iteration 62923, loss = 1.22008580\n",
      "Iteration 62924, loss = 1.39542979\n",
      "Iteration 62925, loss = 1.67827990\n",
      "Iteration 62926, loss = 1.40810342\n",
      "Iteration 62927, loss = 1.29447572\n",
      "Iteration 62928, loss = 1.18680438\n",
      "Iteration 62929, loss = 1.20880561\n",
      "Iteration 62930, loss = 1.34790313\n",
      "Iteration 62931, loss = 1.24256733\n",
      "Iteration 62932, loss = 1.52173879\n",
      "Iteration 62933, loss = 1.24010175\n",
      "Iteration 62934, loss = 1.25939198\n",
      "Iteration 62935, loss = 1.20699263\n",
      "Iteration 62936, loss = 1.32961754\n",
      "Iteration 62937, loss = 1.41794194\n",
      "Iteration 62938, loss = 1.30904899\n",
      "Iteration 62939, loss = 1.16329625\n",
      "Iteration 62940, loss = 1.28090204\n",
      "Iteration 62941, loss = 1.43133657\n",
      "Iteration 62942, loss = 1.52375456\n",
      "Iteration 62943, loss = 1.38408997\n",
      "Iteration 62944, loss = 1.31429547\n",
      "Iteration 62945, loss = 1.25198121\n",
      "Iteration 62946, loss = 1.23238189\n",
      "Iteration 62947, loss = 1.32436615\n",
      "Iteration 62948, loss = 1.20740391\n",
      "Iteration 62949, loss = 1.44062474\n",
      "Iteration 62950, loss = 1.31178702\n",
      "Iteration 62951, loss = 1.28652518\n",
      "Iteration 62952, loss = 1.59148111\n",
      "Iteration 62953, loss = 1.24433912\n",
      "Iteration 62954, loss = 1.29575108\n",
      "Iteration 62955, loss = 1.25813569\n",
      "Iteration 62956, loss = 1.29970457\n",
      "Iteration 62957, loss = 1.41281367\n",
      "Iteration 62958, loss = 2.09058693\n",
      "Iteration 62959, loss = 2.43092907\n",
      "Iteration 62960, loss = 2.40709152\n",
      "Iteration 62961, loss = 3.37216178\n",
      "Iteration 62962, loss = 3.68058796\n",
      "Iteration 62963, loss = 2.75393679\n",
      "Iteration 62964, loss = 2.68415657\n",
      "Iteration 62965, loss = 2.50796854\n",
      "Iteration 62966, loss = 1.86843949\n",
      "Iteration 62967, loss = 1.74882481\n",
      "Iteration 62968, loss = 1.61089657\n",
      "Iteration 62969, loss = 1.42405406\n",
      "Iteration 62970, loss = 1.64902201\n",
      "Iteration 62971, loss = 1.76132189\n",
      "Iteration 62972, loss = 1.86087348\n",
      "Iteration 62973, loss = 1.70433029\n",
      "Iteration 62974, loss = 1.94016963\n",
      "Iteration 62975, loss = 1.81124661\n",
      "Iteration 62976, loss = 1.48533306\n",
      "Iteration 62977, loss = 1.28781021\n",
      "Iteration 62978, loss = 1.35383099\n",
      "Iteration 62979, loss = 1.89194763\n",
      "Iteration 62980, loss = 1.64305906\n",
      "Iteration 62981, loss = 1.46218720\n",
      "Iteration 62982, loss = 1.34603401\n",
      "Iteration 62983, loss = 1.24979090\n",
      "Iteration 62984, loss = 1.34157907\n",
      "Iteration 62985, loss = 1.27745294\n",
      "Iteration 62986, loss = 1.50525450\n",
      "Iteration 62987, loss = 1.35006623\n",
      "Iteration 62988, loss = 1.25706252\n",
      "Iteration 62989, loss = 1.27233005\n",
      "Iteration 62990, loss = 1.28560204\n",
      "Iteration 62991, loss = 1.19717148\n",
      "Iteration 62992, loss = 1.27263628\n",
      "Iteration 62993, loss = 1.61695174\n",
      "Iteration 62994, loss = 1.38818252\n",
      "Iteration 62995, loss = 1.29019676\n",
      "Iteration 62996, loss = 1.48124076\n",
      "Iteration 62997, loss = 1.24259302\n",
      "Iteration 62998, loss = 1.64280201\n",
      "Iteration 62999, loss = 1.48543892\n",
      "Iteration 63000, loss = 1.73653464\n",
      "Iteration 63001, loss = 1.35551747\n",
      "Iteration 63002, loss = 1.41013256\n",
      "Iteration 63003, loss = 1.51353494\n",
      "Iteration 63004, loss = 1.10687223\n",
      "Iteration 63005, loss = 1.31669816\n",
      "Iteration 63006, loss = 1.25298796\n",
      "Iteration 63007, loss = 1.23797789\n",
      "Iteration 63008, loss = 1.20157112\n",
      "Iteration 63009, loss = 1.39461317\n",
      "Iteration 63010, loss = 1.44483264\n",
      "Iteration 63011, loss = 1.32792123\n",
      "Iteration 63012, loss = 1.33746936\n",
      "Iteration 63013, loss = 1.36745327\n",
      "Iteration 63014, loss = 1.32650873\n",
      "Iteration 63015, loss = 1.38667015\n",
      "Iteration 63016, loss = 1.26956763\n",
      "Iteration 63017, loss = 1.21719224\n",
      "Iteration 63018, loss = 1.25753186\n",
      "Iteration 63019, loss = 1.27924068\n",
      "Iteration 63020, loss = 1.37970519\n",
      "Iteration 63021, loss = 1.31541988\n",
      "Iteration 63022, loss = 1.16823347\n",
      "Iteration 63023, loss = 1.12818803\n",
      "Iteration 63024, loss = 1.17846846\n",
      "Iteration 63025, loss = 1.20830625\n",
      "Iteration 63026, loss = 1.14570917\n",
      "Iteration 63027, loss = 1.18868432\n",
      "Iteration 63028, loss = 1.52550190\n",
      "Iteration 63029, loss = 1.61302133\n",
      "Iteration 63030, loss = 1.58794615\n",
      "Iteration 63031, loss = 1.36007626\n",
      "Iteration 63032, loss = 1.79067480\n",
      "Iteration 63033, loss = 1.30506046\n",
      "Iteration 63034, loss = 1.16838898\n",
      "Iteration 63035, loss = 1.36321521\n",
      "Iteration 63036, loss = 1.42176107\n",
      "Iteration 63037, loss = 1.52981599\n",
      "Iteration 63038, loss = 1.47628963\n",
      "Iteration 63039, loss = 1.39140198\n",
      "Iteration 63040, loss = 1.41853225\n",
      "Iteration 63041, loss = 1.49185185\n",
      "Iteration 63042, loss = 1.28406975\n",
      "Iteration 63043, loss = 1.21845009\n",
      "Iteration 63044, loss = 1.20829563\n",
      "Iteration 63045, loss = 1.21574091\n",
      "Iteration 63046, loss = 1.33241800\n",
      "Iteration 63047, loss = 1.29166527\n",
      "Iteration 63048, loss = 1.29744372\n",
      "Iteration 63049, loss = 1.34212050\n",
      "Iteration 63050, loss = 1.77434441\n",
      "Iteration 63051, loss = 1.81096046\n",
      "Iteration 63052, loss = 1.82266796\n",
      "Iteration 63053, loss = 1.61662085\n",
      "Iteration 63054, loss = 1.72912741\n",
      "Iteration 63055, loss = 1.31078530\n",
      "Iteration 63056, loss = 1.48599911\n",
      "Iteration 63057, loss = 1.43680255\n",
      "Iteration 63058, loss = 1.46188598\n",
      "Iteration 63059, loss = 1.35351819\n",
      "Iteration 63060, loss = 1.69911883\n",
      "Iteration 63061, loss = 1.42147303\n",
      "Iteration 63062, loss = 1.43642765\n",
      "Iteration 63063, loss = 1.59721829\n",
      "Iteration 63064, loss = 1.37575941\n",
      "Iteration 63065, loss = 1.41152964\n",
      "Iteration 63066, loss = 1.46681177\n",
      "Iteration 63067, loss = 1.42044438\n",
      "Iteration 63068, loss = 1.40676675\n",
      "Iteration 63069, loss = 1.39354414\n",
      "Iteration 63070, loss = 1.32638654\n",
      "Iteration 63071, loss = 1.22829120\n",
      "Iteration 63072, loss = 1.26312407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 63073, loss = 1.33463061\n",
      "Iteration 63074, loss = 1.70010692\n",
      "Iteration 63075, loss = 1.59152894\n",
      "Iteration 63076, loss = 1.42024160\n",
      "Iteration 63077, loss = 1.40323919\n",
      "Iteration 63078, loss = 1.36646602\n",
      "Iteration 63079, loss = 1.20560147\n",
      "Iteration 63080, loss = 1.45348980\n",
      "Iteration 63081, loss = 1.35198587\n",
      "Iteration 63082, loss = 1.18804458\n",
      "Iteration 63083, loss = 1.13314147\n",
      "Iteration 63084, loss = 1.34592315\n",
      "Iteration 63085, loss = 1.43351572\n",
      "Iteration 63086, loss = 1.26446516\n",
      "Iteration 63087, loss = 1.22999670\n",
      "Iteration 63088, loss = 1.71571244\n",
      "Iteration 63089, loss = 1.74355781\n",
      "Iteration 63090, loss = 1.79494768\n",
      "Iteration 63091, loss = 1.51407429\n",
      "Iteration 63092, loss = 1.74601910\n",
      "Iteration 63093, loss = 1.43064131\n",
      "Iteration 63094, loss = 1.87755450\n",
      "Iteration 63095, loss = 1.45860923\n",
      "Iteration 63096, loss = 1.76260948\n",
      "Iteration 63097, loss = 1.50664132\n",
      "Iteration 63098, loss = 1.36299397\n",
      "Iteration 63099, loss = 1.30235095\n",
      "Iteration 63100, loss = 1.18989578\n",
      "Iteration 63101, loss = 1.35532606\n",
      "Iteration 63102, loss = 1.27103878\n",
      "Iteration 63103, loss = 1.63953459\n",
      "Iteration 63104, loss = 1.40606054\n",
      "Iteration 63105, loss = 1.32873956\n",
      "Iteration 63106, loss = 1.75419939\n",
      "Iteration 63107, loss = 1.52992876\n",
      "Iteration 63108, loss = 1.40250397\n",
      "Iteration 63109, loss = 1.35633002\n",
      "Iteration 63110, loss = 1.41554708\n",
      "Iteration 63111, loss = 1.64809897\n",
      "Iteration 63112, loss = 1.85379532\n",
      "Iteration 63113, loss = 2.33331545\n",
      "Iteration 63114, loss = 2.02241353\n",
      "Iteration 63115, loss = 1.53258445\n",
      "Iteration 63116, loss = 1.76628001\n",
      "Iteration 63117, loss = 2.11146882\n",
      "Iteration 63118, loss = 1.94972170\n",
      "Iteration 63119, loss = 2.52166364\n",
      "Iteration 63120, loss = 3.10918898\n",
      "Iteration 63121, loss = 2.62981274\n",
      "Iteration 63122, loss = 2.17778633\n",
      "Iteration 63123, loss = 1.80002600\n",
      "Iteration 63124, loss = 2.05443054\n",
      "Iteration 63125, loss = 1.89410773\n",
      "Iteration 63126, loss = 2.25935842\n",
      "Iteration 63127, loss = 1.63169457\n",
      "Iteration 63128, loss = 2.31698128\n",
      "Iteration 63129, loss = 2.37416658\n",
      "Iteration 63130, loss = 1.99822031\n",
      "Iteration 63131, loss = 1.71590865\n",
      "Iteration 63132, loss = 2.43820475\n",
      "Iteration 63133, loss = 3.68281291\n",
      "Iteration 63134, loss = 3.99810665\n",
      "Iteration 63135, loss = 3.67384002\n",
      "Iteration 63136, loss = 2.32262201\n",
      "Iteration 63137, loss = 2.01891125\n",
      "Iteration 63138, loss = 2.14667539\n",
      "Iteration 63139, loss = 2.13655363\n",
      "Iteration 63140, loss = 1.56610784\n",
      "Iteration 63141, loss = 2.12427318\n",
      "Iteration 63142, loss = 1.96356010\n",
      "Iteration 63143, loss = 1.63647370\n",
      "Iteration 63144, loss = 1.61315135\n",
      "Iteration 63145, loss = 1.40697438\n",
      "Iteration 63146, loss = 1.20913218\n",
      "Iteration 63147, loss = 1.25434464\n",
      "Iteration 63148, loss = 1.27027393\n",
      "Iteration 63149, loss = 1.19781506\n",
      "Iteration 63150, loss = 1.27215354\n",
      "Iteration 63151, loss = 1.19720229\n",
      "Iteration 63152, loss = 1.15092118\n",
      "Iteration 63153, loss = 1.31442475\n",
      "Iteration 63154, loss = 1.16931824\n",
      "Iteration 63155, loss = 1.45124375\n",
      "Iteration 63156, loss = 1.66993929\n",
      "Iteration 63157, loss = 1.96428483\n",
      "Iteration 63158, loss = 1.83319133\n",
      "Iteration 63159, loss = 1.86824117\n",
      "Iteration 63160, loss = 1.57720294\n",
      "Iteration 63161, loss = 1.38591443\n",
      "Iteration 63162, loss = 1.46252052\n",
      "Iteration 63163, loss = 1.73674217\n",
      "Iteration 63164, loss = 1.51712438\n",
      "Iteration 63165, loss = 1.47873103\n",
      "Iteration 63166, loss = 1.34967061\n",
      "Iteration 63167, loss = 1.24205958\n",
      "Iteration 63168, loss = 1.25480707\n",
      "Iteration 63169, loss = 1.38805259\n",
      "Iteration 63170, loss = 1.70482247\n",
      "Iteration 63171, loss = 1.60684338\n",
      "Iteration 63172, loss = 1.49824559\n",
      "Iteration 63173, loss = 1.40879652\n",
      "Iteration 63174, loss = 1.55386844\n",
      "Iteration 63175, loss = 1.80000613\n",
      "Iteration 63176, loss = 2.07894753\n",
      "Iteration 63177, loss = 2.18014035\n",
      "Iteration 63178, loss = 1.83880147\n",
      "Iteration 63179, loss = 1.82584584\n",
      "Iteration 63180, loss = 1.66577753\n",
      "Iteration 63181, loss = 1.29153098\n",
      "Iteration 63182, loss = 1.43818410\n",
      "Iteration 63183, loss = 1.52138675\n",
      "Iteration 63184, loss = 1.44038380\n",
      "Iteration 63185, loss = 1.71955833\n",
      "Iteration 63186, loss = 2.41968077\n",
      "Iteration 63187, loss = 2.34603290\n",
      "Iteration 63188, loss = 2.02082072\n",
      "Iteration 63189, loss = 1.78482646\n",
      "Iteration 63190, loss = 1.61352371\n",
      "Iteration 63191, loss = 1.32817023\n",
      "Iteration 63192, loss = 1.45547892\n",
      "Iteration 63193, loss = 1.41044788\n",
      "Iteration 63194, loss = 1.25308936\n",
      "Iteration 63195, loss = 1.25206649\n",
      "Iteration 63196, loss = 1.32658488\n",
      "Iteration 63197, loss = 1.22101994\n",
      "Iteration 63198, loss = 1.21557410\n",
      "Iteration 63199, loss = 1.41503815\n",
      "Iteration 63200, loss = 1.73940745\n",
      "Iteration 63201, loss = 1.71236072\n",
      "Iteration 63202, loss = 1.54886472\n",
      "Iteration 63203, loss = 1.49946979\n",
      "Iteration 63204, loss = 1.66005479\n",
      "Iteration 63205, loss = 1.56961875\n",
      "Iteration 63206, loss = 1.91708732\n",
      "Iteration 63207, loss = 1.70232696\n",
      "Iteration 63208, loss = 1.59299609\n",
      "Iteration 63209, loss = 1.43796543\n",
      "Iteration 63210, loss = 1.34436420\n",
      "Iteration 63211, loss = 1.31001543\n",
      "Iteration 63212, loss = 1.29196072\n",
      "Iteration 63213, loss = 1.29175832\n",
      "Iteration 63214, loss = 1.25390394\n",
      "Iteration 63215, loss = 1.22655123\n",
      "Iteration 63216, loss = 1.21720633\n",
      "Iteration 63217, loss = 1.26190145\n",
      "Iteration 63218, loss = 1.61089460\n",
      "Iteration 63219, loss = 1.28793707\n",
      "Iteration 63220, loss = 1.22373364\n",
      "Iteration 63221, loss = 1.27887043\n",
      "Iteration 63222, loss = 1.39296394\n",
      "Iteration 63223, loss = 1.79990489\n",
      "Iteration 63224, loss = 1.97482888\n",
      "Iteration 63225, loss = 1.75378203\n",
      "Iteration 63226, loss = 1.80279699\n",
      "Iteration 63227, loss = 1.99543395\n",
      "Iteration 63228, loss = 1.81009184\n",
      "Iteration 63229, loss = 2.15926385\n",
      "Iteration 63230, loss = 2.22392987\n",
      "Iteration 63231, loss = 2.48638536\n",
      "Iteration 63232, loss = 2.14949023\n",
      "Iteration 63233, loss = 1.48314032\n",
      "Iteration 63234, loss = 1.58839442\n",
      "Iteration 63235, loss = 1.66027531\n",
      "Iteration 63236, loss = 1.57510603\n",
      "Iteration 63237, loss = 1.34612395\n",
      "Iteration 63238, loss = 1.46637834\n",
      "Iteration 63239, loss = 1.57818454\n",
      "Iteration 63240, loss = 1.46722465\n",
      "Iteration 63241, loss = 1.58605160\n",
      "Iteration 63242, loss = 1.68527971\n",
      "Iteration 63243, loss = 2.02765167\n",
      "Iteration 63244, loss = 1.72210049\n",
      "Iteration 63245, loss = 1.22309528\n",
      "Iteration 63246, loss = 1.27983409\n",
      "Iteration 63247, loss = 1.56018265\n",
      "Iteration 63248, loss = 1.35782165\n",
      "Iteration 63249, loss = 1.44855021\n",
      "Iteration 63250, loss = 1.41680029\n",
      "Iteration 63251, loss = 1.78396744\n",
      "Iteration 63252, loss = 2.26205533\n",
      "Iteration 63253, loss = 2.58347825\n",
      "Iteration 63254, loss = 3.56185384\n",
      "Iteration 63255, loss = 3.37316859\n",
      "Iteration 63256, loss = 2.11834370\n",
      "Iteration 63257, loss = 1.91767846\n",
      "Iteration 63258, loss = 1.64749762\n",
      "Iteration 63259, loss = 1.53167578\n",
      "Iteration 63260, loss = 1.39899570\n",
      "Iteration 63261, loss = 1.31855006\n",
      "Iteration 63262, loss = 1.22491303\n",
      "Iteration 63263, loss = 1.43709789\n",
      "Iteration 63264, loss = 1.29593196\n",
      "Iteration 63265, loss = 1.33959870\n",
      "Iteration 63266, loss = 1.29431246\n",
      "Iteration 63267, loss = 1.63234598\n",
      "Iteration 63268, loss = 1.60680950\n",
      "Iteration 63269, loss = 1.49068281\n",
      "Iteration 63270, loss = 1.48461741\n",
      "Iteration 63271, loss = 2.08669880\n",
      "Iteration 63272, loss = 2.44478395\n",
      "Iteration 63273, loss = 2.44591359\n",
      "Iteration 63274, loss = 1.94464843\n",
      "Iteration 63275, loss = 2.11794525\n",
      "Iteration 63276, loss = 2.12304396\n",
      "Iteration 63277, loss = 2.19135422\n",
      "Iteration 63278, loss = 2.17964805\n",
      "Iteration 63279, loss = 1.79110390\n",
      "Iteration 63280, loss = 1.55597294\n",
      "Iteration 63281, loss = 1.28542107\n",
      "Iteration 63282, loss = 1.27694428\n",
      "Iteration 63283, loss = 1.22844341\n",
      "Iteration 63284, loss = 1.16940806\n",
      "Iteration 63285, loss = 1.26787622\n",
      "Iteration 63286, loss = 1.37992608\n",
      "Iteration 63287, loss = 1.40797034\n",
      "Iteration 63288, loss = 1.36211300\n",
      "Iteration 63289, loss = 1.21434979\n",
      "Iteration 63290, loss = 1.34077105\n",
      "Iteration 63291, loss = 1.34175946\n",
      "Iteration 63292, loss = 1.50349960\n",
      "Iteration 63293, loss = 1.57193434\n",
      "Iteration 63294, loss = 1.33729475\n",
      "Iteration 63295, loss = 1.37632794\n",
      "Iteration 63296, loss = 1.31294350\n",
      "Iteration 63297, loss = 1.55064509\n",
      "Iteration 63298, loss = 2.05051547\n",
      "Iteration 63299, loss = 1.58798775\n",
      "Iteration 63300, loss = 2.00924356\n",
      "Iteration 63301, loss = 2.01542140\n",
      "Iteration 63302, loss = 1.99492822\n",
      "Iteration 63303, loss = 1.79147658\n",
      "Iteration 63304, loss = 1.92020326\n",
      "Iteration 63305, loss = 1.74787672\n",
      "Iteration 63306, loss = 2.03000491\n",
      "Iteration 63307, loss = 2.74787920\n",
      "Iteration 63308, loss = 2.48866202\n",
      "Iteration 63309, loss = 2.73928644\n",
      "Iteration 63310, loss = 3.00909918\n",
      "Iteration 63311, loss = 2.81826235\n",
      "Iteration 63312, loss = 2.09654924\n",
      "Iteration 63313, loss = 1.68091217\n",
      "Iteration 63314, loss = 1.72009192\n",
      "Iteration 63315, loss = 1.70096501\n",
      "Iteration 63316, loss = 1.82094415\n",
      "Iteration 63317, loss = 2.44608659\n",
      "Iteration 63318, loss = 2.05788411\n",
      "Iteration 63319, loss = 1.80305159\n",
      "Iteration 63320, loss = 1.47169550\n",
      "Iteration 63321, loss = 1.65544504\n",
      "Iteration 63322, loss = 1.44493709\n",
      "Iteration 63323, loss = 1.46156987\n",
      "Iteration 63324, loss = 1.58616473\n",
      "Iteration 63325, loss = 1.78531732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 63326, loss = 1.82299906\n",
      "Iteration 63327, loss = 1.29019330\n",
      "Iteration 63328, loss = 1.40397040\n",
      "Iteration 63329, loss = 1.33704172\n",
      "Iteration 63330, loss = 1.41474965\n",
      "Iteration 63331, loss = 1.31143777\n",
      "Iteration 63332, loss = 1.39112230\n",
      "Iteration 63333, loss = 1.78963633\n",
      "Iteration 63334, loss = 1.91727305\n",
      "Iteration 63335, loss = 1.88486782\n",
      "Iteration 63336, loss = 1.60379557\n",
      "Iteration 63337, loss = 1.62359110\n",
      "Iteration 63338, loss = 1.67979801\n",
      "Iteration 63339, loss = 1.72414803\n",
      "Iteration 63340, loss = 1.53725603\n",
      "Iteration 63341, loss = 1.92896298\n",
      "Iteration 63342, loss = 1.95143477\n",
      "Iteration 63343, loss = 1.42965473\n",
      "Iteration 63344, loss = 1.63567889\n",
      "Iteration 63345, loss = 1.48172281\n",
      "Iteration 63346, loss = 1.48353028\n",
      "Iteration 63347, loss = 1.47410312\n",
      "Iteration 63348, loss = 2.01830258\n",
      "Iteration 63349, loss = 1.60894920\n",
      "Iteration 63350, loss = 1.33593235\n",
      "Iteration 63351, loss = 1.31389708\n",
      "Iteration 63352, loss = 1.27908359\n",
      "Iteration 63353, loss = 1.54688861\n",
      "Iteration 63354, loss = 1.32561160\n",
      "Iteration 63355, loss = 1.40259297\n",
      "Iteration 63356, loss = 1.27746115\n",
      "Iteration 63357, loss = 1.25632779\n",
      "Iteration 63358, loss = 1.27044603\n",
      "Iteration 63359, loss = 1.27345278\n",
      "Iteration 63360, loss = 1.25659729\n",
      "Iteration 63361, loss = 1.36195999\n",
      "Iteration 63362, loss = 1.47556528\n",
      "Iteration 63363, loss = 1.58885112\n",
      "Iteration 63364, loss = 1.69431347\n",
      "Iteration 63365, loss = 1.47599912\n",
      "Iteration 63366, loss = 1.25452585\n",
      "Iteration 63367, loss = 1.17506529\n",
      "Iteration 63368, loss = 1.20749561\n",
      "Iteration 63369, loss = 1.38199764\n",
      "Iteration 63370, loss = 1.15923642\n",
      "Iteration 63371, loss = 1.19143186\n",
      "Iteration 63372, loss = 1.25246190\n",
      "Iteration 63373, loss = 1.62979318\n",
      "Iteration 63374, loss = 1.30038672\n",
      "Iteration 63375, loss = 1.24691456\n",
      "Iteration 63376, loss = 1.22918930\n",
      "Iteration 63377, loss = 1.20377548\n",
      "Iteration 63378, loss = 1.14616368\n",
      "Iteration 63379, loss = 1.14394602\n",
      "Iteration 63380, loss = 1.22213882\n",
      "Iteration 63381, loss = 1.27454771\n",
      "Iteration 63382, loss = 1.34654685\n",
      "Iteration 63383, loss = 1.33977899\n",
      "Iteration 63384, loss = 1.28862959\n",
      "Iteration 63385, loss = 1.32411481\n",
      "Iteration 63386, loss = 1.43320468\n",
      "Iteration 63387, loss = 1.64269972\n",
      "Iteration 63388, loss = 1.66835107\n",
      "Iteration 63389, loss = 1.65557071\n",
      "Iteration 63390, loss = 1.47319756\n",
      "Iteration 63391, loss = 1.67899777\n",
      "Iteration 63392, loss = 1.67467340\n",
      "Iteration 63393, loss = 1.64331057\n",
      "Iteration 63394, loss = 1.37233319\n",
      "Iteration 63395, loss = 1.39150753\n",
      "Iteration 63396, loss = 1.53950037\n",
      "Iteration 63397, loss = 1.61552671\n",
      "Iteration 63398, loss = 1.86333738\n",
      "Iteration 63399, loss = 1.68049433\n",
      "Iteration 63400, loss = 1.53952457\n",
      "Iteration 63401, loss = 1.99285860\n",
      "Iteration 63402, loss = 2.17709227\n",
      "Iteration 63403, loss = 2.05461573\n",
      "Iteration 63404, loss = 1.93381818\n",
      "Iteration 63405, loss = 2.52068950\n",
      "Iteration 63406, loss = 2.10984766\n",
      "Iteration 63407, loss = 1.55473838\n",
      "Iteration 63408, loss = 2.04414295\n",
      "Iteration 63409, loss = 2.53482755\n",
      "Iteration 63410, loss = 1.92788956\n",
      "Iteration 63411, loss = 1.98615124\n",
      "Iteration 63412, loss = 2.31752736\n",
      "Iteration 63413, loss = 1.75206198\n",
      "Iteration 63414, loss = 2.57873190\n",
      "Iteration 63415, loss = 2.15373284\n",
      "Iteration 63416, loss = 1.79804878\n",
      "Iteration 63417, loss = 1.87171412\n",
      "Iteration 63418, loss = 1.41761248\n",
      "Iteration 63419, loss = 1.50494318\n",
      "Iteration 63420, loss = 1.20766275\n",
      "Iteration 63421, loss = 1.42518190\n",
      "Iteration 63422, loss = 1.35934825\n",
      "Iteration 63423, loss = 1.31885523\n",
      "Iteration 63424, loss = 1.64790751\n",
      "Iteration 63425, loss = 1.99866289\n",
      "Iteration 63426, loss = 1.71657617\n",
      "Iteration 63427, loss = 1.26308966\n",
      "Iteration 63428, loss = 1.30746566\n",
      "Iteration 63429, loss = 1.26300800\n",
      "Iteration 63430, loss = 1.21392385\n",
      "Iteration 63431, loss = 1.33617111\n",
      "Iteration 63432, loss = 1.40995023\n",
      "Iteration 63433, loss = 1.28532705\n",
      "Iteration 63434, loss = 1.66085048\n",
      "Iteration 63435, loss = 1.57619248\n",
      "Iteration 63436, loss = 1.70656164\n",
      "Iteration 63437, loss = 1.82722596\n",
      "Iteration 63438, loss = 1.64489004\n",
      "Iteration 63439, loss = 1.56756996\n",
      "Iteration 63440, loss = 1.56557598\n",
      "Iteration 63441, loss = 1.27379165\n",
      "Iteration 63442, loss = 1.29181743\n",
      "Iteration 63443, loss = 1.42325137\n",
      "Iteration 63444, loss = 1.49289023\n",
      "Iteration 63445, loss = 1.40151056\n",
      "Iteration 63446, loss = 1.56633709\n",
      "Iteration 63447, loss = 1.48317863\n",
      "Iteration 63448, loss = 1.38582291\n",
      "Iteration 63449, loss = 1.42789931\n",
      "Iteration 63450, loss = 1.24802760\n",
      "Iteration 63451, loss = 1.37177852\n",
      "Iteration 63452, loss = 1.89793492\n",
      "Iteration 63453, loss = 1.80107496\n",
      "Iteration 63454, loss = 1.41739487\n",
      "Iteration 63455, loss = 1.19035330\n",
      "Iteration 63456, loss = 1.17443806\n",
      "Iteration 63457, loss = 1.21540872\n",
      "Iteration 63458, loss = 1.22096237\n",
      "Iteration 63459, loss = 1.39305739\n",
      "Iteration 63460, loss = 1.45734371\n",
      "Iteration 63461, loss = 1.31628501\n",
      "Iteration 63462, loss = 1.32528641\n",
      "Iteration 63463, loss = 1.26445880\n",
      "Iteration 63464, loss = 1.47438646\n",
      "Iteration 63465, loss = 2.50568096\n",
      "Iteration 63466, loss = 1.64594347\n",
      "Iteration 63467, loss = 1.55864682\n",
      "Iteration 63468, loss = 1.65876012\n",
      "Iteration 63469, loss = 1.38796257\n",
      "Iteration 63470, loss = 1.32196922\n",
      "Iteration 63471, loss = 1.28271179\n",
      "Iteration 63472, loss = 1.68150460\n",
      "Iteration 63473, loss = 1.48591441\n",
      "Iteration 63474, loss = 1.40777213\n",
      "Iteration 63475, loss = 1.26003748\n",
      "Iteration 63476, loss = 1.16848190\n",
      "Iteration 63477, loss = 1.33797065\n",
      "Iteration 63478, loss = 1.21815559\n",
      "Iteration 63479, loss = 1.16902406\n",
      "Iteration 63480, loss = 1.22911865\n",
      "Iteration 63481, loss = 1.38671650\n",
      "Iteration 63482, loss = 1.56921547\n",
      "Iteration 63483, loss = 1.62836321\n",
      "Iteration 63484, loss = 1.95038991\n",
      "Iteration 63485, loss = 1.79749359\n",
      "Iteration 63486, loss = 1.52459826\n",
      "Iteration 63487, loss = 1.81556933\n",
      "Iteration 63488, loss = 1.62468366\n",
      "Iteration 63489, loss = 2.50592709\n",
      "Iteration 63490, loss = 1.95509329\n",
      "Iteration 63491, loss = 2.54665899\n",
      "Iteration 63492, loss = 6.11916175\n",
      "Iteration 63493, loss = 4.03526429\n",
      "Iteration 63494, loss = 4.61314096\n",
      "Iteration 63495, loss = 2.58798341\n",
      "Iteration 63496, loss = 2.02302162\n",
      "Iteration 63497, loss = 2.20095955\n",
      "Iteration 63498, loss = 2.10056704\n",
      "Iteration 63499, loss = 2.43761901\n",
      "Iteration 63500, loss = 2.13516656\n",
      "Iteration 63501, loss = 1.76469532\n",
      "Iteration 63502, loss = 1.66471392\n",
      "Iteration 63503, loss = 1.62107562\n",
      "Iteration 63504, loss = 1.48411877\n",
      "Iteration 63505, loss = 1.48737466\n",
      "Iteration 63506, loss = 1.38105111\n",
      "Iteration 63507, loss = 1.53968505\n",
      "Iteration 63508, loss = 1.47242736\n",
      "Iteration 63509, loss = 1.59117339\n",
      "Iteration 63510, loss = 1.52491632\n",
      "Iteration 63511, loss = 1.49109447\n",
      "Iteration 63512, loss = 1.60092941\n",
      "Iteration 63513, loss = 1.34737958\n",
      "Iteration 63514, loss = 1.27110959\n",
      "Iteration 63515, loss = 1.43552230\n",
      "Iteration 63516, loss = 1.41126314\n",
      "Iteration 63517, loss = 1.81700159\n",
      "Iteration 63518, loss = 1.88828153\n",
      "Iteration 63519, loss = 1.50512796\n",
      "Iteration 63520, loss = 1.84433562\n",
      "Iteration 63521, loss = 1.96572278\n",
      "Iteration 63522, loss = 1.70994642\n",
      "Iteration 63523, loss = 1.67624629\n",
      "Iteration 63524, loss = 1.53348437\n",
      "Iteration 63525, loss = 1.28220002\n",
      "Iteration 63526, loss = 1.19317656\n",
      "Iteration 63527, loss = 1.44304960\n",
      "Iteration 63528, loss = 1.55018820\n",
      "Iteration 63529, loss = 1.32529230\n",
      "Iteration 63530, loss = 1.26199794\n",
      "Iteration 63531, loss = 1.15283406\n",
      "Iteration 63532, loss = 1.19020942\n",
      "Iteration 63533, loss = 1.19778829\n",
      "Iteration 63534, loss = 1.25034374\n",
      "Iteration 63535, loss = 1.42746416\n",
      "Iteration 63536, loss = 1.17516234\n",
      "Iteration 63537, loss = 1.29199543\n",
      "Iteration 63538, loss = 1.35600016\n",
      "Iteration 63539, loss = 1.28588428\n",
      "Iteration 63540, loss = 1.23125049\n",
      "Iteration 63541, loss = 1.30833689\n",
      "Iteration 63542, loss = 1.12458369\n",
      "Iteration 63543, loss = 1.36843190\n",
      "Iteration 63544, loss = 1.31629572\n",
      "Iteration 63545, loss = 1.48396175\n",
      "Iteration 63546, loss = 1.44875029\n",
      "Iteration 63547, loss = 1.59987126\n",
      "Iteration 63548, loss = 1.58350148\n",
      "Iteration 63549, loss = 1.56393818\n",
      "Iteration 63550, loss = 1.47056738\n",
      "Iteration 63551, loss = 1.33005399\n",
      "Iteration 63552, loss = 1.35281490\n",
      "Iteration 63553, loss = 1.38835722\n",
      "Iteration 63554, loss = 1.24998386\n",
      "Iteration 63555, loss = 1.38637536\n",
      "Iteration 63556, loss = 1.47141942\n",
      "Iteration 63557, loss = 1.56999385\n",
      "Iteration 63558, loss = 1.46876516\n",
      "Iteration 63559, loss = 1.25695368\n",
      "Iteration 63560, loss = 1.25808762\n",
      "Iteration 63561, loss = 1.32501284\n",
      "Iteration 63562, loss = 1.32863738\n",
      "Iteration 63563, loss = 2.03698134\n",
      "Iteration 63564, loss = 1.58284944\n",
      "Iteration 63565, loss = 1.90875217\n",
      "Iteration 63566, loss = 1.75640470\n",
      "Iteration 63567, loss = 1.86884899\n",
      "Iteration 63568, loss = 1.77330621\n",
      "Iteration 63569, loss = 1.21260873\n",
      "Iteration 63570, loss = 1.39225742\n",
      "Iteration 63571, loss = 1.48371921\n",
      "Iteration 63572, loss = 1.30169486\n",
      "Iteration 63573, loss = 1.41252079\n",
      "Iteration 63574, loss = 1.25893547\n",
      "Iteration 63575, loss = 1.63232541\n",
      "Iteration 63576, loss = 1.56105860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 63577, loss = 1.45813794\n",
      "Iteration 63578, loss = 1.23578110\n",
      "Iteration 63579, loss = 1.58225500\n",
      "Iteration 63580, loss = 1.44936438\n",
      "Iteration 63581, loss = 1.35444840\n",
      "Iteration 63582, loss = 1.38659070\n",
      "Iteration 63583, loss = 1.37144465\n",
      "Iteration 63584, loss = 1.25423909\n",
      "Iteration 63585, loss = 1.33015717\n",
      "Iteration 63586, loss = 1.59716659\n",
      "Iteration 63587, loss = 1.88848866\n",
      "Iteration 63588, loss = 1.42250421\n",
      "Iteration 63589, loss = 1.38097477\n",
      "Iteration 63590, loss = 1.34184828\n",
      "Iteration 63591, loss = 1.19494495\n",
      "Iteration 63592, loss = 1.52827397\n",
      "Iteration 63593, loss = 1.29983482\n",
      "Iteration 63594, loss = 1.45162857\n",
      "Iteration 63595, loss = 1.30218630\n",
      "Iteration 63596, loss = 1.28287530\n",
      "Iteration 63597, loss = 1.19508920\n",
      "Iteration 63598, loss = 1.21981573\n",
      "Iteration 63599, loss = 1.21835372\n",
      "Iteration 63600, loss = 1.22298067\n",
      "Iteration 63601, loss = 1.22404500\n",
      "Iteration 63602, loss = 1.27444883\n",
      "Iteration 63603, loss = 1.25100263\n",
      "Iteration 63604, loss = 1.20526942\n",
      "Iteration 63605, loss = 1.37124855\n",
      "Iteration 63606, loss = 1.18680534\n",
      "Iteration 63607, loss = 1.33149785\n",
      "Iteration 63608, loss = 1.16222804\n",
      "Iteration 63609, loss = 1.16120721\n",
      "Iteration 63610, loss = 1.34988156\n",
      "Iteration 63611, loss = 1.59053283\n",
      "Iteration 63612, loss = 1.57145077\n",
      "Iteration 63613, loss = 1.45231065\n",
      "Iteration 63614, loss = 1.59464702\n",
      "Iteration 63615, loss = 1.57814999\n",
      "Iteration 63616, loss = 2.11903346\n",
      "Iteration 63617, loss = 1.71430744\n",
      "Iteration 63618, loss = 1.52197804\n",
      "Iteration 63619, loss = 1.29772405\n",
      "Iteration 63620, loss = 1.17357352\n",
      "Iteration 63621, loss = 1.32053057\n",
      "Iteration 63622, loss = 1.19229330\n",
      "Iteration 63623, loss = 1.27099179\n",
      "Iteration 63624, loss = 1.36044097\n",
      "Iteration 63625, loss = 1.20168375\n",
      "Iteration 63626, loss = 1.53216879\n",
      "Iteration 63627, loss = 1.55439343\n",
      "Iteration 63628, loss = 1.59435391\n",
      "Iteration 63629, loss = 1.81454089\n",
      "Iteration 63630, loss = 1.72230849\n",
      "Iteration 63631, loss = 1.40510285\n",
      "Iteration 63632, loss = 1.25745680\n",
      "Iteration 63633, loss = 1.26907484\n",
      "Iteration 63634, loss = 1.20347967\n",
      "Iteration 63635, loss = 1.26342328\n",
      "Iteration 63636, loss = 1.41162916\n",
      "Iteration 63637, loss = 1.47331410\n",
      "Iteration 63638, loss = 1.84432954\n",
      "Iteration 63639, loss = 1.61900863\n",
      "Iteration 63640, loss = 1.46616286\n",
      "Iteration 63641, loss = 1.94517112\n",
      "Iteration 63642, loss = 1.52714054\n",
      "Iteration 63643, loss = 1.49466020\n",
      "Iteration 63644, loss = 1.60265429\n",
      "Iteration 63645, loss = 1.73490638\n",
      "Iteration 63646, loss = 1.52496084\n",
      "Iteration 63647, loss = 1.80890554\n",
      "Iteration 63648, loss = 2.23485874\n",
      "Iteration 63649, loss = 1.82895205\n",
      "Iteration 63650, loss = 1.39501922\n",
      "Iteration 63651, loss = 1.39132742\n",
      "Iteration 63652, loss = 1.60574037\n",
      "Iteration 63653, loss = 1.88414672\n",
      "Iteration 63654, loss = 2.00960253\n",
      "Iteration 63655, loss = 1.67830315\n",
      "Iteration 63656, loss = 1.68851552\n",
      "Iteration 63657, loss = 1.53947284\n",
      "Iteration 63658, loss = 1.50245096\n",
      "Iteration 63659, loss = 1.38449491\n",
      "Iteration 63660, loss = 1.50578070\n",
      "Iteration 63661, loss = 1.46829747\n",
      "Iteration 63662, loss = 1.75612017\n",
      "Iteration 63663, loss = 1.55100748\n",
      "Iteration 63664, loss = 2.34312836\n",
      "Iteration 63665, loss = 1.68899142\n",
      "Iteration 63666, loss = 1.98865981\n",
      "Iteration 63667, loss = 1.51655206\n",
      "Iteration 63668, loss = 1.34262799\n",
      "Iteration 63669, loss = 1.55004002\n",
      "Iteration 63670, loss = 1.56522900\n",
      "Iteration 63671, loss = 1.72197719\n",
      "Iteration 63672, loss = 1.50886094\n",
      "Iteration 63673, loss = 1.88569317\n",
      "Iteration 63674, loss = 1.75836364\n",
      "Iteration 63675, loss = 1.46883509\n",
      "Iteration 63676, loss = 1.48643700\n",
      "Iteration 63677, loss = 1.20928546\n",
      "Iteration 63678, loss = 1.13698555\n",
      "Iteration 63679, loss = 1.30999405\n",
      "Iteration 63680, loss = 1.34086157\n",
      "Iteration 63681, loss = 1.39040769\n",
      "Iteration 63682, loss = 1.28279186\n",
      "Iteration 63683, loss = 1.26989128\n",
      "Iteration 63684, loss = 1.60296549\n",
      "Iteration 63685, loss = 1.75197608\n",
      "Iteration 63686, loss = 1.50155879\n",
      "Iteration 63687, loss = 2.18710047\n",
      "Iteration 63688, loss = 1.65004669\n",
      "Iteration 63689, loss = 1.46965560\n",
      "Iteration 63690, loss = 1.14170370\n",
      "Iteration 63691, loss = 1.24657962\n",
      "Iteration 63692, loss = 1.41367481\n",
      "Iteration 63693, loss = 1.61543934\n",
      "Iteration 63694, loss = 1.50336168\n",
      "Iteration 63695, loss = 1.64675753\n",
      "Iteration 63696, loss = 1.86964949\n",
      "Iteration 63697, loss = 1.88891000\n",
      "Iteration 63698, loss = 1.42337441\n",
      "Iteration 63699, loss = 1.52584608\n",
      "Iteration 63700, loss = 1.68768000\n",
      "Iteration 63701, loss = 1.46744378\n",
      "Iteration 63702, loss = 1.55296509\n",
      "Iteration 63703, loss = 1.35854978\n",
      "Iteration 63704, loss = 1.45737655\n",
      "Iteration 63705, loss = 1.37491264\n",
      "Iteration 63706, loss = 1.23357403\n",
      "Iteration 63707, loss = 1.27228055\n",
      "Iteration 63708, loss = 1.25790790\n",
      "Iteration 63709, loss = 1.18132578\n",
      "Iteration 63710, loss = 1.23692738\n",
      "Iteration 63711, loss = 1.15214227\n",
      "Iteration 63712, loss = 1.27806330\n",
      "Iteration 63713, loss = 1.31404505\n",
      "Iteration 63714, loss = 1.32924253\n",
      "Iteration 63715, loss = 1.17748220\n",
      "Iteration 63716, loss = 1.25647316\n",
      "Iteration 63717, loss = 1.29964535\n",
      "Iteration 63718, loss = 1.45387479\n",
      "Iteration 63719, loss = 1.77742496\n",
      "Iteration 63720, loss = 1.72708572\n",
      "Iteration 63721, loss = 1.62700613\n",
      "Iteration 63722, loss = 1.52521526\n",
      "Iteration 63723, loss = 1.66353925\n",
      "Iteration 63724, loss = 2.30303319\n",
      "Iteration 63725, loss = 2.36810202\n",
      "Iteration 63726, loss = 2.38045157\n",
      "Iteration 63727, loss = 2.38155941\n",
      "Iteration 63728, loss = 2.33746174\n",
      "Iteration 63729, loss = 1.61966669\n",
      "Iteration 63730, loss = 1.98702013\n",
      "Iteration 63731, loss = 1.62308037\n",
      "Iteration 63732, loss = 1.48839783\n",
      "Iteration 63733, loss = 1.46318606\n",
      "Iteration 63734, loss = 1.31933152\n",
      "Iteration 63735, loss = 1.43172546\n",
      "Iteration 63736, loss = 1.19134803\n",
      "Iteration 63737, loss = 1.24966049\n",
      "Iteration 63738, loss = 1.13763052\n",
      "Iteration 63739, loss = 1.12694840\n",
      "Iteration 63740, loss = 1.18767608\n",
      "Iteration 63741, loss = 1.41265714\n",
      "Iteration 63742, loss = 1.44258435\n",
      "Iteration 63743, loss = 1.36389576\n",
      "Iteration 63744, loss = 1.54656959\n",
      "Iteration 63745, loss = 1.66461501\n",
      "Iteration 63746, loss = 1.78068318\n",
      "Iteration 63747, loss = 1.61940264\n",
      "Iteration 63748, loss = 1.54617513\n",
      "Iteration 63749, loss = 1.32011739\n",
      "Iteration 63750, loss = 1.42467745\n",
      "Iteration 63751, loss = 1.22625623\n",
      "Iteration 63752, loss = 1.31394835\n",
      "Iteration 63753, loss = 1.97574690\n",
      "Iteration 63754, loss = 2.20813004\n",
      "Iteration 63755, loss = 2.97007225\n",
      "Iteration 63756, loss = 2.02966783\n",
      "Iteration 63757, loss = 2.32456421\n",
      "Iteration 63758, loss = 1.72734007\n",
      "Iteration 63759, loss = 2.19095729\n",
      "Iteration 63760, loss = 1.82972684\n",
      "Iteration 63761, loss = 1.98687602\n",
      "Iteration 63762, loss = 1.87997866\n",
      "Iteration 63763, loss = 1.85075969\n",
      "Iteration 63764, loss = 1.41493187\n",
      "Iteration 63765, loss = 1.78717046\n",
      "Iteration 63766, loss = 1.72326558\n",
      "Iteration 63767, loss = 1.90622620\n",
      "Iteration 63768, loss = 1.57825269\n",
      "Iteration 63769, loss = 1.45497051\n",
      "Iteration 63770, loss = 1.27592911\n",
      "Iteration 63771, loss = 1.64729545\n",
      "Iteration 63772, loss = 1.51174602\n",
      "Iteration 63773, loss = 1.25837524\n",
      "Iteration 63774, loss = 1.41341470\n",
      "Iteration 63775, loss = 1.24564308\n",
      "Iteration 63776, loss = 1.06103616\n",
      "Iteration 63777, loss = 1.24862207\n",
      "Iteration 63778, loss = 1.18648635\n",
      "Iteration 63779, loss = 1.17193506\n",
      "Iteration 63780, loss = 1.19522459\n",
      "Iteration 63781, loss = 1.24610589\n",
      "Iteration 63782, loss = 1.29109728\n",
      "Iteration 63783, loss = 1.18514026\n",
      "Iteration 63784, loss = 1.51543250\n",
      "Iteration 63785, loss = 1.28415355\n",
      "Iteration 63786, loss = 1.20425847\n",
      "Iteration 63787, loss = 1.29978050\n",
      "Iteration 63788, loss = 1.47332055\n",
      "Iteration 63789, loss = 1.53121768\n",
      "Iteration 63790, loss = 1.53293682\n",
      "Iteration 63791, loss = 1.67949754\n",
      "Iteration 63792, loss = 1.77679781\n",
      "Iteration 63793, loss = 1.62803797\n",
      "Iteration 63794, loss = 1.39705947\n",
      "Iteration 63795, loss = 1.31728875\n",
      "Iteration 63796, loss = 1.20998939\n",
      "Iteration 63797, loss = 1.15927347\n",
      "Iteration 63798, loss = 1.15515614\n",
      "Iteration 63799, loss = 1.15581805\n",
      "Iteration 63800, loss = 1.26950553\n",
      "Iteration 63801, loss = 1.42372150\n",
      "Iteration 63802, loss = 1.50139974\n",
      "Iteration 63803, loss = 1.72810394\n",
      "Iteration 63804, loss = 1.32694769\n",
      "Iteration 63805, loss = 1.33016473\n",
      "Iteration 63806, loss = 1.40112550\n",
      "Iteration 63807, loss = 1.40137778\n",
      "Iteration 63808, loss = 1.64710469\n",
      "Iteration 63809, loss = 1.71544481\n",
      "Iteration 63810, loss = 2.10410770\n",
      "Iteration 63811, loss = 1.67338871\n",
      "Iteration 63812, loss = 1.32708527\n",
      "Iteration 63813, loss = 1.27916160\n",
      "Iteration 63814, loss = 1.49750836\n",
      "Iteration 63815, loss = 1.62868912\n",
      "Iteration 63816, loss = 1.30114539\n",
      "Iteration 63817, loss = 1.28502406\n",
      "Iteration 63818, loss = 1.57196083\n",
      "Iteration 63819, loss = 1.20805727\n",
      "Iteration 63820, loss = 1.47196683\n",
      "Iteration 63821, loss = 1.37653128\n",
      "Iteration 63822, loss = 1.22462398\n",
      "Iteration 63823, loss = 1.45940773\n",
      "Iteration 63824, loss = 1.51094494\n",
      "Iteration 63825, loss = 1.38512624\n",
      "Iteration 63826, loss = 1.29914524\n",
      "Iteration 63827, loss = 1.26395786\n",
      "Iteration 63828, loss = 1.18776184\n",
      "Iteration 63829, loss = 1.30178367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 63830, loss = 1.32056247\n",
      "Iteration 63831, loss = 1.14852269\n",
      "Iteration 63832, loss = 1.10487180\n",
      "Iteration 63833, loss = 1.17036311\n",
      "Iteration 63834, loss = 1.17605385\n",
      "Iteration 63835, loss = 1.41943949\n",
      "Iteration 63836, loss = 1.35079497\n",
      "Iteration 63837, loss = 1.34104486\n",
      "Iteration 63838, loss = 1.30016008\n",
      "Iteration 63839, loss = 1.38210534\n",
      "Iteration 63840, loss = 1.34417194\n",
      "Iteration 63841, loss = 1.39003997\n",
      "Iteration 63842, loss = 1.89731332\n",
      "Iteration 63843, loss = 1.52909682\n",
      "Iteration 63844, loss = 1.61688529\n",
      "Iteration 63845, loss = 1.71120659\n",
      "Iteration 63846, loss = 1.44272079\n",
      "Iteration 63847, loss = 1.38764450\n",
      "Iteration 63848, loss = 1.22513036\n",
      "Iteration 63849, loss = 1.41643129\n",
      "Iteration 63850, loss = 1.53182806\n",
      "Iteration 63851, loss = 1.71817964\n",
      "Iteration 63852, loss = 1.85181446\n",
      "Iteration 63853, loss = 1.86873250\n",
      "Iteration 63854, loss = 1.84037632\n",
      "Iteration 63855, loss = 1.87186379\n",
      "Iteration 63856, loss = 2.10569843\n",
      "Iteration 63857, loss = 1.39757591\n",
      "Iteration 63858, loss = 1.77577213\n",
      "Iteration 63859, loss = 1.65623213\n",
      "Iteration 63860, loss = 1.25632744\n",
      "Iteration 63861, loss = 1.41860925\n",
      "Iteration 63862, loss = 1.37627497\n",
      "Iteration 63863, loss = 1.28710046\n",
      "Iteration 63864, loss = 1.21243750\n",
      "Iteration 63865, loss = 1.23713195\n",
      "Iteration 63866, loss = 1.43967598\n",
      "Iteration 63867, loss = 1.35164542\n",
      "Iteration 63868, loss = 1.15887258\n",
      "Iteration 63869, loss = 1.22779563\n",
      "Iteration 63870, loss = 1.38177888\n",
      "Iteration 63871, loss = 1.29630986\n",
      "Iteration 63872, loss = 1.15726094\n",
      "Iteration 63873, loss = 1.19374757\n",
      "Iteration 63874, loss = 1.11860436\n",
      "Iteration 63875, loss = 1.25841634\n",
      "Iteration 63876, loss = 1.75013352\n",
      "Iteration 63877, loss = 1.86959982\n",
      "Iteration 63878, loss = 1.77830292\n",
      "Iteration 63879, loss = 1.51189035\n",
      "Iteration 63880, loss = 1.38790108\n",
      "Iteration 63881, loss = 1.26324545\n",
      "Iteration 63882, loss = 1.40619440\n",
      "Iteration 63883, loss = 1.24383597\n",
      "Iteration 63884, loss = 1.48853908\n",
      "Iteration 63885, loss = 1.30900507\n",
      "Iteration 63886, loss = 1.46200231\n",
      "Iteration 63887, loss = 1.39371502\n",
      "Iteration 63888, loss = 1.22501941\n",
      "Iteration 63889, loss = 1.37127178\n",
      "Iteration 63890, loss = 1.45453904\n",
      "Iteration 63891, loss = 1.51522685\n",
      "Iteration 63892, loss = 1.54978628\n",
      "Iteration 63893, loss = 1.51067409\n",
      "Iteration 63894, loss = 1.35366848\n",
      "Iteration 63895, loss = 1.34475138\n",
      "Iteration 63896, loss = 1.53521446\n",
      "Iteration 63897, loss = 1.80977304\n",
      "Iteration 63898, loss = 1.70874626\n",
      "Iteration 63899, loss = 1.38641236\n",
      "Iteration 63900, loss = 1.32211552\n",
      "Iteration 63901, loss = 1.43474434\n",
      "Iteration 63902, loss = 1.25520382\n",
      "Iteration 63903, loss = 1.33962808\n",
      "Iteration 63904, loss = 1.48630872\n",
      "Iteration 63905, loss = 1.38333511\n",
      "Iteration 63906, loss = 1.27395101\n",
      "Iteration 63907, loss = 1.35695797\n",
      "Iteration 63908, loss = 1.60823521\n",
      "Iteration 63909, loss = 1.72736415\n",
      "Iteration 63910, loss = 1.72027211\n",
      "Iteration 63911, loss = 2.37768610\n",
      "Iteration 63912, loss = 3.13347308\n",
      "Iteration 63913, loss = 2.89496382\n",
      "Iteration 63914, loss = 2.02070405\n",
      "Iteration 63915, loss = 1.69906827\n",
      "Iteration 63916, loss = 1.48713151\n",
      "Iteration 63917, loss = 1.86019722\n",
      "Iteration 63918, loss = 1.65567351\n",
      "Iteration 63919, loss = 1.96789861\n",
      "Iteration 63920, loss = 1.38212577\n",
      "Iteration 63921, loss = 1.27980922\n",
      "Iteration 63922, loss = 1.25880514\n",
      "Iteration 63923, loss = 1.20646747\n",
      "Iteration 63924, loss = 1.20555703\n",
      "Iteration 63925, loss = 1.20018898\n",
      "Iteration 63926, loss = 1.32653921\n",
      "Iteration 63927, loss = 1.79504772\n",
      "Iteration 63928, loss = 1.29752559\n",
      "Iteration 63929, loss = 1.46549794\n",
      "Iteration 63930, loss = 1.36206918\n",
      "Iteration 63931, loss = 1.41559838\n",
      "Iteration 63932, loss = 1.34734629\n",
      "Iteration 63933, loss = 1.30102670\n",
      "Iteration 63934, loss = 1.40351609\n",
      "Iteration 63935, loss = 1.48333728\n",
      "Iteration 63936, loss = 1.31090441\n",
      "Iteration 63937, loss = 1.68284656\n",
      "Iteration 63938, loss = 1.48918218\n",
      "Iteration 63939, loss = 1.88552698\n",
      "Iteration 63940, loss = 2.11112812\n",
      "Iteration 63941, loss = 1.94273225\n",
      "Iteration 63942, loss = 1.78040150\n",
      "Iteration 63943, loss = 1.45901269\n",
      "Iteration 63944, loss = 1.68436871\n",
      "Iteration 63945, loss = 1.79073906\n",
      "Iteration 63946, loss = 1.92368874\n",
      "Iteration 63947, loss = 1.85979711\n",
      "Iteration 63948, loss = 1.66640048\n",
      "Iteration 63949, loss = 1.82712311\n",
      "Iteration 63950, loss = 1.56337323\n",
      "Iteration 63951, loss = 1.53951283\n",
      "Iteration 63952, loss = 1.64970792\n",
      "Iteration 63953, loss = 1.62496249\n",
      "Iteration 63954, loss = 1.44658503\n",
      "Iteration 63955, loss = 1.18181136\n",
      "Iteration 63956, loss = 1.32843094\n",
      "Iteration 63957, loss = 1.42967588\n",
      "Iteration 63958, loss = 1.51333772\n",
      "Iteration 63959, loss = 2.61750786\n",
      "Iteration 63960, loss = 2.76573612\n",
      "Iteration 63961, loss = 3.00671601\n",
      "Iteration 63962, loss = 2.15070569\n",
      "Iteration 63963, loss = 2.00918713\n",
      "Iteration 63964, loss = 1.85618032\n",
      "Iteration 63965, loss = 1.53799889\n",
      "Iteration 63966, loss = 1.57736629\n",
      "Iteration 63967, loss = 1.83973135\n",
      "Iteration 63968, loss = 1.49823089\n",
      "Iteration 63969, loss = 1.27063488\n",
      "Iteration 63970, loss = 1.38983111\n",
      "Iteration 63971, loss = 1.37633125\n",
      "Iteration 63972, loss = 1.62996074\n",
      "Iteration 63973, loss = 1.76933619\n",
      "Iteration 63974, loss = 1.38332183\n",
      "Iteration 63975, loss = 1.44198853\n",
      "Iteration 63976, loss = 1.40540440\n",
      "Iteration 63977, loss = 1.59987068\n",
      "Iteration 63978, loss = 1.32313404\n",
      "Iteration 63979, loss = 1.46083134\n",
      "Iteration 63980, loss = 1.96285678\n",
      "Iteration 63981, loss = 1.55586839\n",
      "Iteration 63982, loss = 1.60639048\n",
      "Iteration 63983, loss = 2.61804008\n",
      "Iteration 63984, loss = 2.31064490\n",
      "Iteration 63985, loss = 1.86079227\n",
      "Iteration 63986, loss = 1.50939588\n",
      "Iteration 63987, loss = 1.18585867\n",
      "Iteration 63988, loss = 1.30738767\n",
      "Iteration 63989, loss = 1.28394925\n",
      "Iteration 63990, loss = 1.49458410\n",
      "Iteration 63991, loss = 1.29737482\n",
      "Iteration 63992, loss = 1.41469273\n",
      "Iteration 63993, loss = 1.75221022\n",
      "Iteration 63994, loss = 1.48345282\n",
      "Iteration 63995, loss = 2.00004567\n",
      "Iteration 63996, loss = 2.30910128\n",
      "Iteration 63997, loss = 2.82080975\n",
      "Iteration 63998, loss = 2.01745311\n",
      "Iteration 63999, loss = 1.74455516\n",
      "Iteration 64000, loss = 1.61513221\n",
      "Iteration 64001, loss = 1.57384920\n",
      "Iteration 64002, loss = 1.59717345\n",
      "Iteration 64003, loss = 1.39344613\n",
      "Iteration 64004, loss = 1.45682348\n",
      "Iteration 64005, loss = 1.45892975\n",
      "Iteration 64006, loss = 1.46823415\n",
      "Iteration 64007, loss = 1.33921983\n",
      "Iteration 64008, loss = 1.45982010\n",
      "Iteration 64009, loss = 1.99244360\n",
      "Iteration 64010, loss = 1.33036955\n",
      "Iteration 64011, loss = 1.70668069\n",
      "Iteration 64012, loss = 2.13791021\n",
      "Iteration 64013, loss = 1.91328215\n",
      "Iteration 64014, loss = 1.45180331\n",
      "Iteration 64015, loss = 1.83783288\n",
      "Iteration 64016, loss = 1.58093352\n",
      "Iteration 64017, loss = 1.65123397\n",
      "Iteration 64018, loss = 1.61589571\n",
      "Iteration 64019, loss = 1.55465445\n",
      "Iteration 64020, loss = 1.81125516\n",
      "Iteration 64021, loss = 1.81653148\n",
      "Iteration 64022, loss = 2.63512564\n",
      "Iteration 64023, loss = 2.67723045\n",
      "Iteration 64024, loss = 2.34992082\n",
      "Iteration 64025, loss = 3.04756829\n",
      "Iteration 64026, loss = 2.48699986\n",
      "Iteration 64027, loss = 2.40342988\n",
      "Iteration 64028, loss = 1.77185389\n",
      "Iteration 64029, loss = 1.67885959\n",
      "Iteration 64030, loss = 1.30511833\n",
      "Iteration 64031, loss = 2.19468298\n",
      "Iteration 64032, loss = 2.30809081\n",
      "Iteration 64033, loss = 1.60130886\n",
      "Iteration 64034, loss = 1.80052601\n",
      "Iteration 64035, loss = 1.42047925\n",
      "Iteration 64036, loss = 1.53906946\n",
      "Iteration 64037, loss = 1.32049590\n",
      "Iteration 64038, loss = 1.21644815\n",
      "Iteration 64039, loss = 1.38541509\n",
      "Iteration 64040, loss = 1.24428500\n",
      "Iteration 64041, loss = 1.76649929\n",
      "Iteration 64042, loss = 1.67740544\n",
      "Iteration 64043, loss = 1.53990185\n",
      "Iteration 64044, loss = 1.47095663\n",
      "Iteration 64045, loss = 1.39494109\n",
      "Iteration 64046, loss = 1.63048098\n",
      "Iteration 64047, loss = 1.43911853\n",
      "Iteration 64048, loss = 1.33580226\n",
      "Iteration 64049, loss = 1.29625939\n",
      "Iteration 64050, loss = 1.28193004\n",
      "Iteration 64051, loss = 1.36194457\n",
      "Iteration 64052, loss = 1.35369841\n",
      "Iteration 64053, loss = 1.33901654\n",
      "Iteration 64054, loss = 1.62010221\n",
      "Iteration 64055, loss = 1.68285134\n",
      "Iteration 64056, loss = 1.41982132\n",
      "Iteration 64057, loss = 1.49364148\n",
      "Iteration 64058, loss = 1.65692855\n",
      "Iteration 64059, loss = 1.57212339\n",
      "Iteration 64060, loss = 1.87632077\n",
      "Iteration 64061, loss = 2.16835934\n",
      "Iteration 64062, loss = 2.42388817\n",
      "Iteration 64063, loss = 2.69475147\n",
      "Iteration 64064, loss = 2.17449377\n",
      "Iteration 64065, loss = 3.47401336\n",
      "Iteration 64066, loss = 3.10423777\n",
      "Iteration 64067, loss = 2.90005512\n",
      "Iteration 64068, loss = 1.97978754\n",
      "Iteration 64069, loss = 1.69702197\n",
      "Iteration 64070, loss = 2.37983974\n",
      "Iteration 64071, loss = 2.22876048\n",
      "Iteration 64072, loss = 1.80022848\n",
      "Iteration 64073, loss = 1.52796020\n",
      "Iteration 64074, loss = 1.30343021\n",
      "Iteration 64075, loss = 1.25137829\n",
      "Iteration 64076, loss = 1.44695759\n",
      "Iteration 64077, loss = 1.58005230\n",
      "Iteration 64078, loss = 1.57648936\n",
      "Iteration 64079, loss = 1.71095342\n",
      "Iteration 64080, loss = 1.71160188\n",
      "Iteration 64081, loss = 1.76978726\n",
      "Iteration 64082, loss = 1.92540259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 64083, loss = 1.98948330\n",
      "Iteration 64084, loss = 1.64598962\n",
      "Iteration 64085, loss = 1.34384215\n",
      "Iteration 64086, loss = 1.56223722\n",
      "Iteration 64087, loss = 1.76810719\n",
      "Iteration 64088, loss = 1.51620588\n",
      "Iteration 64089, loss = 1.48034910\n",
      "Iteration 64090, loss = 1.33670134\n",
      "Iteration 64091, loss = 1.30927474\n",
      "Iteration 64092, loss = 1.22309235\n",
      "Iteration 64093, loss = 1.26734811\n",
      "Iteration 64094, loss = 1.59056043\n",
      "Iteration 64095, loss = 1.36352456\n",
      "Iteration 64096, loss = 1.68042120\n",
      "Iteration 64097, loss = 1.42731498\n",
      "Iteration 64098, loss = 1.71305593\n",
      "Iteration 64099, loss = 1.68494513\n",
      "Iteration 64100, loss = 1.56340127\n",
      "Iteration 64101, loss = 1.85081811\n",
      "Iteration 64102, loss = 1.67979095\n",
      "Iteration 64103, loss = 1.59949878\n",
      "Iteration 64104, loss = 2.11959715\n",
      "Iteration 64105, loss = 1.60598143\n",
      "Iteration 64106, loss = 1.83769964\n",
      "Iteration 64107, loss = 1.76726394\n",
      "Iteration 64108, loss = 2.17994536\n",
      "Iteration 64109, loss = 2.02051338\n",
      "Iteration 64110, loss = 2.04715180\n",
      "Iteration 64111, loss = 1.69308468\n",
      "Iteration 64112, loss = 1.97423636\n",
      "Iteration 64113, loss = 1.46779629\n",
      "Iteration 64114, loss = 1.57177913\n",
      "Iteration 64115, loss = 1.88598533\n",
      "Iteration 64116, loss = 2.12650508\n",
      "Iteration 64117, loss = 2.19232205\n",
      "Iteration 64118, loss = 1.88866733\n",
      "Iteration 64119, loss = 1.63901455\n",
      "Iteration 64120, loss = 1.42257932\n",
      "Iteration 64121, loss = 1.23918472\n",
      "Iteration 64122, loss = 1.18879333\n",
      "Iteration 64123, loss = 1.25975259\n",
      "Iteration 64124, loss = 1.23852896\n",
      "Iteration 64125, loss = 1.71337941\n",
      "Iteration 64126, loss = 2.06654522\n",
      "Iteration 64127, loss = 1.61579303\n",
      "Iteration 64128, loss = 1.57958664\n",
      "Iteration 64129, loss = 1.73521167\n",
      "Iteration 64130, loss = 1.68398877\n",
      "Iteration 64131, loss = 1.60881165\n",
      "Iteration 64132, loss = 1.22371119\n",
      "Iteration 64133, loss = 1.19943254\n",
      "Iteration 64134, loss = 1.40813588\n",
      "Iteration 64135, loss = 1.27982155\n",
      "Iteration 64136, loss = 1.25900741\n",
      "Iteration 64137, loss = 1.29039891\n",
      "Iteration 64138, loss = 1.24705301\n",
      "Iteration 64139, loss = 1.33972131\n",
      "Iteration 64140, loss = 1.39593271\n",
      "Iteration 64141, loss = 1.58483359\n",
      "Iteration 64142, loss = 1.43000777\n",
      "Iteration 64143, loss = 1.28632244\n",
      "Iteration 64144, loss = 1.61636625\n",
      "Iteration 64145, loss = 1.73279089\n",
      "Iteration 64146, loss = 1.58635029\n",
      "Iteration 64147, loss = 1.65473306\n",
      "Iteration 64148, loss = 1.47466570\n",
      "Iteration 64149, loss = 1.27576379\n",
      "Iteration 64150, loss = 1.20866666\n",
      "Iteration 64151, loss = 1.37121911\n",
      "Iteration 64152, loss = 1.19351950\n",
      "Iteration 64153, loss = 1.23420275\n",
      "Iteration 64154, loss = 1.30353419\n",
      "Iteration 64155, loss = 1.43488382\n",
      "Iteration 64156, loss = 1.37980647\n",
      "Iteration 64157, loss = 1.82277976\n",
      "Iteration 64158, loss = 1.82971937\n",
      "Iteration 64159, loss = 1.46656660\n",
      "Iteration 64160, loss = 1.19073225\n",
      "Iteration 64161, loss = 1.23527038\n",
      "Iteration 64162, loss = 1.29049295\n",
      "Iteration 64163, loss = 1.23018824\n",
      "Iteration 64164, loss = 1.12704237\n",
      "Iteration 64165, loss = 1.50958920\n",
      "Iteration 64166, loss = 1.74924645\n",
      "Iteration 64167, loss = 1.43876506\n",
      "Iteration 64168, loss = 1.45057094\n",
      "Iteration 64169, loss = 1.27305670\n",
      "Iteration 64170, loss = 1.38119960\n",
      "Iteration 64171, loss = 1.55803788\n",
      "Iteration 64172, loss = 1.43715949\n",
      "Iteration 64173, loss = 1.35903880\n",
      "Iteration 64174, loss = 1.62635481\n",
      "Iteration 64175, loss = 1.76397968\n",
      "Iteration 64176, loss = 2.06865158\n",
      "Iteration 64177, loss = 2.08739121\n",
      "Iteration 64178, loss = 1.79702650\n",
      "Iteration 64179, loss = 1.60944683\n",
      "Iteration 64180, loss = 1.71445954\n",
      "Iteration 64181, loss = 1.63773351\n",
      "Iteration 64182, loss = 1.40382686\n",
      "Iteration 64183, loss = 1.35966562\n",
      "Iteration 64184, loss = 1.40001101\n",
      "Iteration 64185, loss = 1.24914431\n",
      "Iteration 64186, loss = 1.13294930\n",
      "Iteration 64187, loss = 1.26017506\n",
      "Iteration 64188, loss = 1.32111869\n",
      "Iteration 64189, loss = 1.31270194\n",
      "Iteration 64190, loss = 1.26472744\n",
      "Iteration 64191, loss = 1.20887088\n",
      "Iteration 64192, loss = 1.13802694\n",
      "Iteration 64193, loss = 1.21981899\n",
      "Iteration 64194, loss = 1.40363180\n",
      "Iteration 64195, loss = 1.31379228\n",
      "Iteration 64196, loss = 1.16551318\n",
      "Iteration 64197, loss = 1.51343090\n",
      "Iteration 64198, loss = 1.66691098\n",
      "Iteration 64199, loss = 1.88723067\n",
      "Iteration 64200, loss = 1.97032256\n",
      "Iteration 64201, loss = 1.77542883\n",
      "Iteration 64202, loss = 1.65376776\n",
      "Iteration 64203, loss = 1.31673620\n",
      "Iteration 64204, loss = 1.47692435\n",
      "Iteration 64205, loss = 1.30346145\n",
      "Iteration 64206, loss = 1.32178308\n",
      "Iteration 64207, loss = 1.70921070\n",
      "Iteration 64208, loss = 1.46415661\n",
      "Iteration 64209, loss = 1.70434649\n",
      "Iteration 64210, loss = 1.54567667\n",
      "Iteration 64211, loss = 1.82732977\n",
      "Iteration 64212, loss = 2.38033855\n",
      "Iteration 64213, loss = 2.16024757\n",
      "Iteration 64214, loss = 1.65420146\n",
      "Iteration 64215, loss = 1.38554791\n",
      "Iteration 64216, loss = 1.35838166\n",
      "Iteration 64217, loss = 1.20790850\n",
      "Iteration 64218, loss = 1.30221859\n",
      "Iteration 64219, loss = 1.25740439\n",
      "Iteration 64220, loss = 1.62644117\n",
      "Iteration 64221, loss = 1.63114206\n",
      "Iteration 64222, loss = 1.40022031\n",
      "Iteration 64223, loss = 1.46669188\n",
      "Iteration 64224, loss = 1.48123857\n",
      "Iteration 64225, loss = 1.31604750\n",
      "Iteration 64226, loss = 1.34459438\n",
      "Iteration 64227, loss = 1.49226656\n",
      "Iteration 64228, loss = 1.16386500\n",
      "Iteration 64229, loss = 1.18154915\n",
      "Iteration 64230, loss = 1.20965477\n",
      "Iteration 64231, loss = 1.28261340\n",
      "Iteration 64232, loss = 1.40513285\n",
      "Iteration 64233, loss = 1.78017342\n",
      "Iteration 64234, loss = 1.53998771\n",
      "Iteration 64235, loss = 1.25271237\n",
      "Iteration 64236, loss = 1.67823717\n",
      "Iteration 64237, loss = 2.92744067\n",
      "Iteration 64238, loss = 3.05041262\n",
      "Iteration 64239, loss = 2.61518328\n",
      "Iteration 64240, loss = 1.78438989\n",
      "Iteration 64241, loss = 2.15436779\n",
      "Iteration 64242, loss = 1.84807208\n",
      "Iteration 64243, loss = 1.38900591\n",
      "Iteration 64244, loss = 1.31797379\n",
      "Iteration 64245, loss = 1.46682379\n",
      "Iteration 64246, loss = 1.33343619\n",
      "Iteration 64247, loss = 1.60589243\n",
      "Iteration 64248, loss = 1.69732330\n",
      "Iteration 64249, loss = 1.76200660\n",
      "Iteration 64250, loss = 1.30901193\n",
      "Iteration 64251, loss = 1.27553608\n",
      "Iteration 64252, loss = 1.28664051\n",
      "Iteration 64253, loss = 1.46079755\n",
      "Iteration 64254, loss = 1.45125718\n",
      "Iteration 64255, loss = 1.51886043\n",
      "Iteration 64256, loss = 1.34853434\n",
      "Iteration 64257, loss = 1.36023635\n",
      "Iteration 64258, loss = 1.66248335\n",
      "Iteration 64259, loss = 1.40551352\n",
      "Iteration 64260, loss = 1.29034260\n",
      "Iteration 64261, loss = 1.35192894\n",
      "Iteration 64262, loss = 1.55092176\n",
      "Iteration 64263, loss = 1.43211676\n",
      "Iteration 64264, loss = 1.45250324\n",
      "Iteration 64265, loss = 1.21882076\n",
      "Iteration 64266, loss = 1.48108284\n",
      "Iteration 64267, loss = 1.57412560\n",
      "Iteration 64268, loss = 1.44906070\n",
      "Iteration 64269, loss = 1.26821655\n",
      "Iteration 64270, loss = 1.26300406\n",
      "Iteration 64271, loss = 1.45237363\n",
      "Iteration 64272, loss = 1.36939673\n",
      "Iteration 64273, loss = 1.59848728\n",
      "Iteration 64274, loss = 1.35476356\n",
      "Iteration 64275, loss = 1.45881182\n",
      "Iteration 64276, loss = 1.34735804\n",
      "Iteration 64277, loss = 1.58630487\n",
      "Iteration 64278, loss = 1.93918439\n",
      "Iteration 64279, loss = 1.53708964\n",
      "Iteration 64280, loss = 1.75754113\n",
      "Iteration 64281, loss = 1.89223095\n",
      "Iteration 64282, loss = 1.52241244\n",
      "Iteration 64283, loss = 1.23476496\n",
      "Iteration 64284, loss = 1.18076438\n",
      "Iteration 64285, loss = 1.41999876\n",
      "Iteration 64286, loss = 1.48469616\n",
      "Iteration 64287, loss = 1.57453847\n",
      "Iteration 64288, loss = 1.96333429\n",
      "Iteration 64289, loss = 1.58289900\n",
      "Iteration 64290, loss = 1.49885938\n",
      "Iteration 64291, loss = 1.32511248\n",
      "Iteration 64292, loss = 1.20651676\n",
      "Iteration 64293, loss = 1.45450385\n",
      "Iteration 64294, loss = 1.41978276\n",
      "Iteration 64295, loss = 1.33278187\n",
      "Iteration 64296, loss = 1.30089251\n",
      "Iteration 64297, loss = 1.51847764\n",
      "Iteration 64298, loss = 1.35818109\n",
      "Iteration 64299, loss = 1.55288716\n",
      "Iteration 64300, loss = 1.35736862\n",
      "Iteration 64301, loss = 1.20288227\n",
      "Iteration 64302, loss = 1.19176069\n",
      "Iteration 64303, loss = 1.26122902\n",
      "Iteration 64304, loss = 1.54524946\n",
      "Iteration 64305, loss = 1.32515356\n",
      "Iteration 64306, loss = 1.53159028\n",
      "Iteration 64307, loss = 1.54982792\n",
      "Iteration 64308, loss = 1.33089000\n",
      "Iteration 64309, loss = 1.35939054\n",
      "Iteration 64310, loss = 1.40219399\n",
      "Iteration 64311, loss = 1.80522318\n",
      "Iteration 64312, loss = 2.05450781\n",
      "Iteration 64313, loss = 1.67684107\n",
      "Iteration 64314, loss = 1.97664216\n",
      "Iteration 64315, loss = 2.44028062\n",
      "Iteration 64316, loss = 2.81427952\n",
      "Iteration 64317, loss = 2.68352522\n",
      "Iteration 64318, loss = 3.28380437\n",
      "Iteration 64319, loss = 2.08046281\n",
      "Iteration 64320, loss = 1.82265176\n",
      "Iteration 64321, loss = 1.80021905\n",
      "Iteration 64322, loss = 2.31790329\n",
      "Iteration 64323, loss = 2.51223260\n",
      "Iteration 64324, loss = 2.91854771\n",
      "Iteration 64325, loss = 3.59224210\n",
      "Iteration 64326, loss = 2.34960578\n",
      "Iteration 64327, loss = 2.36173277\n",
      "Iteration 64328, loss = 2.10627575\n",
      "Iteration 64329, loss = 1.61567557\n",
      "Iteration 64330, loss = 1.45289359\n",
      "Iteration 64331, loss = 1.34743824\n",
      "Iteration 64332, loss = 1.37339123\n",
      "Iteration 64333, loss = 1.28515686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 64334, loss = 1.18622044\n",
      "Iteration 64335, loss = 1.18920311\n",
      "Iteration 64336, loss = 1.16982867\n",
      "Iteration 64337, loss = 1.44345596\n",
      "Iteration 64338, loss = 1.49720166\n",
      "Iteration 64339, loss = 1.26933910\n",
      "Iteration 64340, loss = 1.44120059\n",
      "Iteration 64341, loss = 1.59606515\n",
      "Iteration 64342, loss = 1.23117218\n",
      "Iteration 64343, loss = 1.19387923\n",
      "Iteration 64344, loss = 1.23627722\n",
      "Iteration 64345, loss = 1.53624296\n",
      "Iteration 64346, loss = 1.43464905\n",
      "Iteration 64347, loss = 1.61398710\n",
      "Iteration 64348, loss = 1.41119127\n",
      "Iteration 64349, loss = 1.44980398\n",
      "Iteration 64350, loss = 1.48879845\n",
      "Iteration 64351, loss = 1.83056946\n",
      "Iteration 64352, loss = 1.62032766\n",
      "Iteration 64353, loss = 1.52155954\n",
      "Iteration 64354, loss = 1.51977030\n",
      "Iteration 64355, loss = 1.76326465\n",
      "Iteration 64356, loss = 1.74056842\n",
      "Iteration 64357, loss = 1.86024957\n",
      "Iteration 64358, loss = 1.91543775\n",
      "Iteration 64359, loss = 1.85929806\n",
      "Iteration 64360, loss = 1.66969797\n",
      "Iteration 64361, loss = 1.34361767\n",
      "Iteration 64362, loss = 1.62394781\n",
      "Iteration 64363, loss = 1.56954647\n",
      "Iteration 64364, loss = 1.39801362\n",
      "Iteration 64365, loss = 1.42320648\n",
      "Iteration 64366, loss = 1.58015637\n",
      "Iteration 64367, loss = 2.54035156\n",
      "Iteration 64368, loss = 1.69671248\n",
      "Iteration 64369, loss = 1.30965919\n",
      "Iteration 64370, loss = 1.40276493\n",
      "Iteration 64371, loss = 1.33496942\n",
      "Iteration 64372, loss = 1.27083926\n",
      "Iteration 64373, loss = 1.16208817\n",
      "Iteration 64374, loss = 1.25038683\n",
      "Iteration 64375, loss = 1.26029197\n",
      "Iteration 64376, loss = 1.41178541\n",
      "Iteration 64377, loss = 1.41000606\n",
      "Iteration 64378, loss = 1.14072230\n",
      "Iteration 64379, loss = 1.26526111\n",
      "Iteration 64380, loss = 1.53816025\n",
      "Iteration 64381, loss = 1.47746136\n",
      "Iteration 64382, loss = 1.52432942\n",
      "Iteration 64383, loss = 1.27905001\n",
      "Iteration 64384, loss = 1.25232646\n",
      "Iteration 64385, loss = 1.20955581\n",
      "Iteration 64386, loss = 1.21564884\n",
      "Iteration 64387, loss = 1.30400216\n",
      "Iteration 64388, loss = 1.34719006\n",
      "Iteration 64389, loss = 1.76310946\n",
      "Iteration 64390, loss = 1.62260668\n",
      "Iteration 64391, loss = 1.82507844\n",
      "Iteration 64392, loss = 1.81794276\n",
      "Iteration 64393, loss = 2.46954618\n",
      "Iteration 64394, loss = 1.77662703\n",
      "Iteration 64395, loss = 1.46822851\n",
      "Iteration 64396, loss = 1.69781737\n",
      "Iteration 64397, loss = 1.73369691\n",
      "Iteration 64398, loss = 1.72652109\n",
      "Iteration 64399, loss = 1.42419655\n",
      "Iteration 64400, loss = 1.62989562\n",
      "Iteration 64401, loss = 1.90314146\n",
      "Iteration 64402, loss = 1.92788210\n",
      "Iteration 64403, loss = 1.64274650\n",
      "Iteration 64404, loss = 1.86366956\n",
      "Iteration 64405, loss = 1.58055371\n",
      "Iteration 64406, loss = 1.72065154\n",
      "Iteration 64407, loss = 1.32865792\n",
      "Iteration 64408, loss = 1.26969407\n",
      "Iteration 64409, loss = 1.45508972\n",
      "Iteration 64410, loss = 1.65698500\n",
      "Iteration 64411, loss = 1.49991087\n",
      "Iteration 64412, loss = 1.60701726\n",
      "Iteration 64413, loss = 1.42087164\n",
      "Iteration 64414, loss = 1.28490571\n",
      "Iteration 64415, loss = 1.51936319\n",
      "Iteration 64416, loss = 1.31947112\n",
      "Iteration 64417, loss = 1.31577594\n",
      "Iteration 64418, loss = 1.38510916\n",
      "Iteration 64419, loss = 1.27978567\n",
      "Iteration 64420, loss = 1.20717569\n",
      "Iteration 64421, loss = 1.15265672\n",
      "Iteration 64422, loss = 1.17646065\n",
      "Iteration 64423, loss = 1.27200059\n",
      "Iteration 64424, loss = 1.26442669\n",
      "Iteration 64425, loss = 1.19471165\n",
      "Iteration 64426, loss = 1.09208251\n",
      "Iteration 64427, loss = 1.12742585\n",
      "Iteration 64428, loss = 1.27170871\n",
      "Iteration 64429, loss = 1.70733660\n",
      "Iteration 64430, loss = 1.49038824\n",
      "Iteration 64431, loss = 1.56304379\n",
      "Iteration 64432, loss = 1.94366775\n",
      "Iteration 64433, loss = 1.73996181\n",
      "Iteration 64434, loss = 1.58548823\n",
      "Iteration 64435, loss = 1.52120699\n",
      "Iteration 64436, loss = 1.84449800\n",
      "Iteration 64437, loss = 2.67274247\n",
      "Iteration 64438, loss = 2.69818810\n",
      "Iteration 64439, loss = 2.03783451\n",
      "Iteration 64440, loss = 1.98387525\n",
      "Iteration 64441, loss = 2.70635110\n",
      "Iteration 64442, loss = 2.23775257\n",
      "Iteration 64443, loss = 1.53343507\n",
      "Iteration 64444, loss = 1.77407986\n",
      "Iteration 64445, loss = 2.25901599\n",
      "Iteration 64446, loss = 2.10772743\n",
      "Iteration 64447, loss = 1.35968332\n",
      "Iteration 64448, loss = 2.41139597\n",
      "Iteration 64449, loss = 1.80466034\n",
      "Iteration 64450, loss = 1.89419260\n",
      "Iteration 64451, loss = 1.68769080\n",
      "Iteration 64452, loss = 1.87640089\n",
      "Iteration 64453, loss = 1.62465444\n",
      "Iteration 64454, loss = 1.75729013\n",
      "Iteration 64455, loss = 1.65540039\n",
      "Iteration 64456, loss = 1.48170740\n",
      "Iteration 64457, loss = 1.38930089\n",
      "Iteration 64458, loss = 1.25545662\n",
      "Iteration 64459, loss = 1.38989187\n",
      "Iteration 64460, loss = 1.31781882\n",
      "Iteration 64461, loss = 1.21448327\n",
      "Iteration 64462, loss = 1.16438651\n",
      "Iteration 64463, loss = 1.16325164\n",
      "Iteration 64464, loss = 1.15939613\n",
      "Iteration 64465, loss = 1.15999159\n",
      "Iteration 64466, loss = 1.33890105\n",
      "Iteration 64467, loss = 1.27411528\n",
      "Iteration 64468, loss = 1.33709865\n",
      "Iteration 64469, loss = 1.22074356\n",
      "Iteration 64470, loss = 1.30455191\n",
      "Iteration 64471, loss = 1.23198898\n",
      "Iteration 64472, loss = 1.28997130\n",
      "Iteration 64473, loss = 1.19172738\n",
      "Iteration 64474, loss = 1.11288317\n",
      "Iteration 64475, loss = 1.12620317\n",
      "Iteration 64476, loss = 1.12468998\n",
      "Iteration 64477, loss = 1.21447170\n",
      "Iteration 64478, loss = 1.23955401\n",
      "Iteration 64479, loss = 1.32705969\n",
      "Iteration 64480, loss = 1.32928775\n",
      "Iteration 64481, loss = 1.27346541\n",
      "Iteration 64482, loss = 1.23765253\n",
      "Iteration 64483, loss = 1.43926659\n",
      "Iteration 64484, loss = 1.35672858\n",
      "Iteration 64485, loss = 1.41298366\n",
      "Iteration 64486, loss = 1.40040670\n",
      "Iteration 64487, loss = 1.09646100\n",
      "Iteration 64488, loss = 1.13685854\n",
      "Iteration 64489, loss = 1.16587404\n",
      "Iteration 64490, loss = 1.12479857\n",
      "Iteration 64491, loss = 1.12797913\n",
      "Iteration 64492, loss = 1.12136819\n",
      "Iteration 64493, loss = 1.32666174\n",
      "Iteration 64494, loss = 1.33013596\n",
      "Iteration 64495, loss = 1.49292679\n",
      "Iteration 64496, loss = 1.73526689\n",
      "Iteration 64497, loss = 1.94740858\n",
      "Iteration 64498, loss = 1.52488031\n",
      "Iteration 64499, loss = 2.00225354\n",
      "Iteration 64500, loss = 1.92594020\n",
      "Iteration 64501, loss = 1.63009401\n",
      "Iteration 64502, loss = 1.55271923\n",
      "Iteration 64503, loss = 1.18936750\n",
      "Iteration 64504, loss = 1.11647419\n",
      "Iteration 64505, loss = 1.15076147\n",
      "Iteration 64506, loss = 1.19072837\n",
      "Iteration 64507, loss = 1.54962402\n",
      "Iteration 64508, loss = 2.12808597\n",
      "Iteration 64509, loss = 2.49996924\n",
      "Iteration 64510, loss = 2.87215813\n",
      "Iteration 64511, loss = 1.98630293\n",
      "Iteration 64512, loss = 1.58888955\n",
      "Iteration 64513, loss = 1.33713298\n",
      "Iteration 64514, loss = 1.44646590\n",
      "Iteration 64515, loss = 1.46930817\n",
      "Iteration 64516, loss = 1.60514641\n",
      "Iteration 64517, loss = 1.54543353\n",
      "Iteration 64518, loss = 1.79656730\n",
      "Iteration 64519, loss = 2.42951664\n",
      "Iteration 64520, loss = 2.02773116\n",
      "Iteration 64521, loss = 1.65026623\n",
      "Iteration 64522, loss = 1.85990131\n",
      "Iteration 64523, loss = 1.75261535\n",
      "Iteration 64524, loss = 1.71439772\n",
      "Iteration 64525, loss = 1.39473877\n",
      "Iteration 64526, loss = 1.26968402\n",
      "Iteration 64527, loss = 1.38613853\n",
      "Iteration 64528, loss = 1.36643199\n",
      "Iteration 64529, loss = 1.48364278\n",
      "Iteration 64530, loss = 1.72385673\n",
      "Iteration 64531, loss = 1.67274689\n",
      "Iteration 64532, loss = 1.99965812\n",
      "Iteration 64533, loss = 2.37344016\n",
      "Iteration 64534, loss = 2.35281956\n",
      "Iteration 64535, loss = 1.78077961\n",
      "Iteration 64536, loss = 1.53762920\n",
      "Iteration 64537, loss = 1.61488109\n",
      "Iteration 64538, loss = 1.38543832\n",
      "Iteration 64539, loss = 1.31166302\n",
      "Iteration 64540, loss = 1.43601408\n",
      "Iteration 64541, loss = 1.78898648\n",
      "Iteration 64542, loss = 1.78656503\n",
      "Iteration 64543, loss = 1.69741921\n",
      "Iteration 64544, loss = 1.54974025\n",
      "Iteration 64545, loss = 1.41248699\n",
      "Iteration 64546, loss = 1.38311717\n",
      "Iteration 64547, loss = 1.63264116\n",
      "Iteration 64548, loss = 1.58373691\n",
      "Iteration 64549, loss = 1.37200510\n",
      "Iteration 64550, loss = 1.32606005\n",
      "Iteration 64551, loss = 1.35445864\n",
      "Iteration 64552, loss = 1.37168698\n",
      "Iteration 64553, loss = 1.48133872\n",
      "Iteration 64554, loss = 1.57458100\n",
      "Iteration 64555, loss = 1.21532817\n",
      "Iteration 64556, loss = 1.38689982\n",
      "Iteration 64557, loss = 1.27703799\n",
      "Iteration 64558, loss = 1.38209403\n",
      "Iteration 64559, loss = 1.23072313\n",
      "Iteration 64560, loss = 1.48453584\n",
      "Iteration 64561, loss = 1.50702841\n",
      "Iteration 64562, loss = 1.36799955\n",
      "Iteration 64563, loss = 1.30606935\n",
      "Iteration 64564, loss = 1.31529308\n",
      "Iteration 64565, loss = 1.37987061\n",
      "Iteration 64566, loss = 1.33836343\n",
      "Iteration 64567, loss = 1.46169511\n",
      "Iteration 64568, loss = 1.59976490\n",
      "Iteration 64569, loss = 1.23513115\n",
      "Iteration 64570, loss = 1.21596621\n",
      "Iteration 64571, loss = 1.20936546\n",
      "Iteration 64572, loss = 1.30820206\n",
      "Iteration 64573, loss = 1.37148769\n",
      "Iteration 64574, loss = 1.32342358\n",
      "Iteration 64575, loss = 1.32444957\n",
      "Iteration 64576, loss = 1.40556072\n",
      "Iteration 64577, loss = 1.45929099\n",
      "Iteration 64578, loss = 1.44155438\n",
      "Iteration 64579, loss = 2.08055469\n",
      "Iteration 64580, loss = 1.79703325\n",
      "Iteration 64581, loss = 1.56544908\n",
      "Iteration 64582, loss = 1.77232257\n",
      "Iteration 64583, loss = 1.52330268\n",
      "Iteration 64584, loss = 1.37542495\n",
      "Iteration 64585, loss = 1.23273102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 64586, loss = 1.22239369\n",
      "Iteration 64587, loss = 1.43426714\n",
      "Iteration 64588, loss = 1.41393261\n",
      "Iteration 64589, loss = 1.35026295\n",
      "Iteration 64590, loss = 1.36651167\n",
      "Iteration 64591, loss = 1.38498595\n",
      "Iteration 64592, loss = 1.27961651\n",
      "Iteration 64593, loss = 1.21160723\n",
      "Iteration 64594, loss = 1.22883710\n",
      "Iteration 64595, loss = 1.11854094\n",
      "Iteration 64596, loss = 1.40218185\n",
      "Iteration 64597, loss = 1.31578298\n",
      "Iteration 64598, loss = 1.46583655\n",
      "Iteration 64599, loss = 1.46571826\n",
      "Iteration 64600, loss = 1.49977972\n",
      "Iteration 64601, loss = 1.68582184\n",
      "Iteration 64602, loss = 1.37700053\n",
      "Iteration 64603, loss = 1.17714047\n",
      "Iteration 64604, loss = 1.37636644\n",
      "Iteration 64605, loss = 1.68910646\n",
      "Iteration 64606, loss = 1.19359426\n",
      "Iteration 64607, loss = 1.52801922\n",
      "Iteration 64608, loss = 1.47436687\n",
      "Iteration 64609, loss = 1.35114029\n",
      "Iteration 64610, loss = 1.20264913\n",
      "Iteration 64611, loss = 1.10086174\n",
      "Iteration 64612, loss = 1.19138844\n",
      "Iteration 64613, loss = 1.18842238\n",
      "Iteration 64614, loss = 1.16332525\n",
      "Iteration 64615, loss = 1.09334404\n",
      "Iteration 64616, loss = 1.17134465\n",
      "Iteration 64617, loss = 1.18686094\n",
      "Iteration 64618, loss = 1.17972276\n",
      "Iteration 64619, loss = 1.43138316\n",
      "Iteration 64620, loss = 1.61003093\n",
      "Iteration 64621, loss = 1.43679798\n",
      "Iteration 64622, loss = 1.46783811\n",
      "Iteration 64623, loss = 1.48071526\n",
      "Iteration 64624, loss = 1.64537068\n",
      "Iteration 64625, loss = 1.79177036\n",
      "Iteration 64626, loss = 2.69583403\n",
      "Iteration 64627, loss = 2.37983948\n",
      "Iteration 64628, loss = 3.13521423\n",
      "Iteration 64629, loss = 5.58039540\n",
      "Iteration 64630, loss = 5.32582097\n",
      "Iteration 64631, loss = 2.31122836\n",
      "Iteration 64632, loss = 2.64413687\n",
      "Iteration 64633, loss = 2.02934663\n",
      "Iteration 64634, loss = 3.11608518\n",
      "Iteration 64635, loss = 2.06534816\n",
      "Iteration 64636, loss = 2.31831942\n",
      "Iteration 64637, loss = 1.81614916\n",
      "Iteration 64638, loss = 1.82475061\n",
      "Iteration 64639, loss = 1.82622437\n",
      "Iteration 64640, loss = 1.66630655\n",
      "Iteration 64641, loss = 1.48946903\n",
      "Iteration 64642, loss = 1.47017327\n",
      "Iteration 64643, loss = 1.78730158\n",
      "Iteration 64644, loss = 1.90453460\n",
      "Iteration 64645, loss = 1.72520892\n",
      "Iteration 64646, loss = 1.51632817\n",
      "Iteration 64647, loss = 1.97781143\n",
      "Iteration 64648, loss = 2.53825698\n",
      "Iteration 64649, loss = 3.33298712\n",
      "Iteration 64650, loss = 2.97580249\n",
      "Iteration 64651, loss = 1.48575195\n",
      "Iteration 64652, loss = 1.92831243\n",
      "Iteration 64653, loss = 1.80470281\n",
      "Iteration 64654, loss = 2.31183517\n",
      "Iteration 64655, loss = 2.01010442\n",
      "Iteration 64656, loss = 1.94495015\n",
      "Iteration 64657, loss = 1.89852224\n",
      "Iteration 64658, loss = 1.37562025\n",
      "Iteration 64659, loss = 1.67784254\n",
      "Iteration 64660, loss = 1.29704713\n",
      "Iteration 64661, loss = 1.30295855\n",
      "Iteration 64662, loss = 1.85789935\n",
      "Iteration 64663, loss = 2.26493228\n",
      "Iteration 64664, loss = 2.04402267\n",
      "Iteration 64665, loss = 1.72945998\n",
      "Iteration 64666, loss = 1.89491581\n",
      "Iteration 64667, loss = 1.51884140\n",
      "Iteration 64668, loss = 1.56032426\n",
      "Iteration 64669, loss = 1.27160625\n",
      "Iteration 64670, loss = 1.44773133\n",
      "Iteration 64671, loss = 1.53990728\n",
      "Iteration 64672, loss = 1.31817308\n",
      "Iteration 64673, loss = 1.24022355\n",
      "Iteration 64674, loss = 1.36015616\n",
      "Iteration 64675, loss = 1.64140257\n",
      "Iteration 64676, loss = 1.68106281\n",
      "Iteration 64677, loss = 1.66249856\n",
      "Iteration 64678, loss = 1.59099123\n",
      "Iteration 64679, loss = 1.77522131\n",
      "Iteration 64680, loss = 1.85975383\n",
      "Iteration 64681, loss = 1.50222088\n",
      "Iteration 64682, loss = 1.54677621\n",
      "Iteration 64683, loss = 1.31324821\n",
      "Iteration 64684, loss = 1.32189382\n",
      "Iteration 64685, loss = 1.42694067\n",
      "Iteration 64686, loss = 2.05694521\n",
      "Iteration 64687, loss = 1.78439351\n",
      "Iteration 64688, loss = 1.73456253\n",
      "Iteration 64689, loss = 1.31543654\n",
      "Iteration 64690, loss = 1.41832132\n",
      "Iteration 64691, loss = 1.30435987\n",
      "Iteration 64692, loss = 1.24501196\n",
      "Iteration 64693, loss = 1.18833832\n",
      "Iteration 64694, loss = 1.28286456\n",
      "Iteration 64695, loss = 1.23065657\n",
      "Iteration 64696, loss = 1.23494104\n",
      "Iteration 64697, loss = 1.17055877\n",
      "Iteration 64698, loss = 1.19690986\n",
      "Iteration 64699, loss = 1.21748988\n",
      "Iteration 64700, loss = 1.21886817\n",
      "Iteration 64701, loss = 1.46270741\n",
      "Iteration 64702, loss = 1.34761152\n",
      "Iteration 64703, loss = 1.15044073\n",
      "Iteration 64704, loss = 1.30522747\n",
      "Iteration 64705, loss = 1.40997908\n",
      "Iteration 64706, loss = 1.95855479\n",
      "Iteration 64707, loss = 1.69011372\n",
      "Iteration 64708, loss = 1.50792779\n",
      "Iteration 64709, loss = 1.44084482\n",
      "Iteration 64710, loss = 1.30364129\n",
      "Iteration 64711, loss = 1.27802476\n",
      "Iteration 64712, loss = 1.56072428\n",
      "Iteration 64713, loss = 2.11722846\n",
      "Iteration 64714, loss = 2.20756451\n",
      "Iteration 64715, loss = 2.41477723\n",
      "Iteration 64716, loss = 2.13239622\n",
      "Iteration 64717, loss = 1.68483198\n",
      "Iteration 64718, loss = 1.82876839\n",
      "Iteration 64719, loss = 1.41667585\n",
      "Iteration 64720, loss = 1.59299848\n",
      "Iteration 64721, loss = 1.62535858\n",
      "Iteration 64722, loss = 1.37434602\n",
      "Iteration 64723, loss = 1.17072818\n",
      "Iteration 64724, loss = 1.27928360\n",
      "Iteration 64725, loss = 1.53367215\n",
      "Iteration 64726, loss = 1.42740412\n",
      "Iteration 64727, loss = 1.41072444\n",
      "Iteration 64728, loss = 1.18854543\n",
      "Iteration 64729, loss = 1.35923549\n",
      "Iteration 64730, loss = 1.36922070\n",
      "Iteration 64731, loss = 1.22214445\n",
      "Iteration 64732, loss = 1.18672721\n",
      "Iteration 64733, loss = 1.34022631\n",
      "Iteration 64734, loss = 1.49856214\n",
      "Iteration 64735, loss = 1.51630459\n",
      "Iteration 64736, loss = 1.92869111\n",
      "Iteration 64737, loss = 2.26149296\n",
      "Iteration 64738, loss = 1.71868169\n",
      "Iteration 64739, loss = 1.78585567\n",
      "Iteration 64740, loss = 3.31360153\n",
      "Iteration 64741, loss = 2.98305534\n",
      "Iteration 64742, loss = 2.88567161\n",
      "Iteration 64743, loss = 2.50755962\n",
      "Iteration 64744, loss = 2.05196893\n",
      "Iteration 64745, loss = 2.93005307\n",
      "Iteration 64746, loss = 1.55545028\n",
      "Iteration 64747, loss = 2.46790626\n",
      "Iteration 64748, loss = 1.89369746\n",
      "Iteration 64749, loss = 1.81111254\n",
      "Iteration 64750, loss = 1.62409434\n",
      "Iteration 64751, loss = 1.59707371\n",
      "Iteration 64752, loss = 1.45529908\n",
      "Iteration 64753, loss = 1.40292913\n",
      "Iteration 64754, loss = 1.23493080\n",
      "Iteration 64755, loss = 1.15756591\n",
      "Iteration 64756, loss = 1.29176330\n",
      "Iteration 64757, loss = 1.42404775\n",
      "Iteration 64758, loss = 1.52987018\n",
      "Iteration 64759, loss = 1.91986130\n",
      "Iteration 64760, loss = 1.50862743\n",
      "Iteration 64761, loss = 1.66121844\n",
      "Iteration 64762, loss = 1.47418104\n",
      "Iteration 64763, loss = 1.37974641\n",
      "Iteration 64764, loss = 1.31929723\n",
      "Iteration 64765, loss = 1.21641597\n",
      "Iteration 64766, loss = 1.80323502\n",
      "Iteration 64767, loss = 1.68180408\n",
      "Iteration 64768, loss = 1.54387286\n",
      "Iteration 64769, loss = 1.70852330\n",
      "Iteration 64770, loss = 1.53880063\n",
      "Iteration 64771, loss = 1.35774679\n",
      "Iteration 64772, loss = 1.36963174\n",
      "Iteration 64773, loss = 1.23234840\n",
      "Iteration 64774, loss = 1.30066532\n",
      "Iteration 64775, loss = 1.36395919\n",
      "Iteration 64776, loss = 1.56854449\n",
      "Iteration 64777, loss = 1.43138207\n",
      "Iteration 64778, loss = 1.70030055\n",
      "Iteration 64779, loss = 1.81896496\n",
      "Iteration 64780, loss = 1.50376886\n",
      "Iteration 64781, loss = 1.67212252\n",
      "Iteration 64782, loss = 1.55771442\n",
      "Iteration 64783, loss = 1.62698923\n",
      "Iteration 64784, loss = 1.42815176\n",
      "Iteration 64785, loss = 1.79188883\n",
      "Iteration 64786, loss = 2.13478113\n",
      "Iteration 64787, loss = 1.60039134\n",
      "Iteration 64788, loss = 2.02179966\n",
      "Iteration 64789, loss = 1.73340575\n",
      "Iteration 64790, loss = 2.37293966\n",
      "Iteration 64791, loss = 1.72520053\n",
      "Iteration 64792, loss = 1.78733985\n",
      "Iteration 64793, loss = 1.71132695\n",
      "Iteration 64794, loss = 1.64979331\n",
      "Iteration 64795, loss = 1.67125912\n",
      "Iteration 64796, loss = 1.82646747\n",
      "Iteration 64797, loss = 1.67274194\n",
      "Iteration 64798, loss = 2.05167761\n",
      "Iteration 64799, loss = 2.25077314\n",
      "Iteration 64800, loss = 1.72239760\n",
      "Iteration 64801, loss = 1.64676848\n",
      "Iteration 64802, loss = 1.64244606\n",
      "Iteration 64803, loss = 1.51217181\n",
      "Iteration 64804, loss = 1.51437756\n",
      "Iteration 64805, loss = 1.41920821\n",
      "Iteration 64806, loss = 1.58172835\n",
      "Iteration 64807, loss = 1.55318186\n",
      "Iteration 64808, loss = 1.33363917\n",
      "Iteration 64809, loss = 1.17741918\n",
      "Iteration 64810, loss = 1.20519752\n",
      "Iteration 64811, loss = 1.37982625\n",
      "Iteration 64812, loss = 1.56972195\n",
      "Iteration 64813, loss = 1.16430225\n",
      "Iteration 64814, loss = 1.30646923\n",
      "Iteration 64815, loss = 1.25369189\n",
      "Iteration 64816, loss = 1.15614573\n",
      "Iteration 64817, loss = 1.25488203\n",
      "Iteration 64818, loss = 1.22236304\n",
      "Iteration 64819, loss = 1.38454827\n",
      "Iteration 64820, loss = 1.77589774\n",
      "Iteration 64821, loss = 1.59334343\n",
      "Iteration 64822, loss = 1.84324042\n",
      "Iteration 64823, loss = 1.81965751\n",
      "Iteration 64824, loss = 1.75936281\n",
      "Iteration 64825, loss = 2.10638216\n",
      "Iteration 64826, loss = 2.03524439\n",
      "Iteration 64827, loss = 1.73439719\n",
      "Iteration 64828, loss = 1.43807841\n",
      "Iteration 64829, loss = 1.37189173\n",
      "Iteration 64830, loss = 1.25830055\n",
      "Iteration 64831, loss = 1.18953021\n",
      "Iteration 64832, loss = 1.23934387\n",
      "Iteration 64833, loss = 1.09590997\n",
      "Iteration 64834, loss = 1.19164512\n",
      "Iteration 64835, loss = 1.22760181\n",
      "Iteration 64836, loss = 1.20198795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 64837, loss = 1.41796283\n",
      "Iteration 64838, loss = 1.53883817\n",
      "Iteration 64839, loss = 1.59559446\n",
      "Iteration 64840, loss = 1.37678603\n",
      "Iteration 64841, loss = 1.39502199\n",
      "Iteration 64842, loss = 1.55589853\n",
      "Iteration 64843, loss = 1.49905145\n",
      "Iteration 64844, loss = 1.22542804\n",
      "Iteration 64845, loss = 1.30373031\n",
      "Iteration 64846, loss = 1.32324357\n",
      "Iteration 64847, loss = 1.42948010\n",
      "Iteration 64848, loss = 1.83539280\n",
      "Iteration 64849, loss = 1.95096508\n",
      "Iteration 64850, loss = 1.47701050\n",
      "Iteration 64851, loss = 1.93862096\n",
      "Iteration 64852, loss = 1.40207796\n",
      "Iteration 64853, loss = 1.39181161\n",
      "Iteration 64854, loss = 1.40642608\n",
      "Iteration 64855, loss = 1.35227286\n",
      "Iteration 64856, loss = 2.03664767\n",
      "Iteration 64857, loss = 1.86812519\n",
      "Iteration 64858, loss = 1.74077045\n",
      "Iteration 64859, loss = 1.35775045\n",
      "Iteration 64860, loss = 1.27440769\n",
      "Iteration 64861, loss = 1.27599903\n",
      "Iteration 64862, loss = 1.35219575\n",
      "Iteration 64863, loss = 1.23620039\n",
      "Iteration 64864, loss = 1.23100774\n",
      "Iteration 64865, loss = 1.23496828\n",
      "Iteration 64866, loss = 1.33051408\n",
      "Iteration 64867, loss = 1.42307927\n",
      "Iteration 64868, loss = 1.27847120\n",
      "Iteration 64869, loss = 1.28568690\n",
      "Iteration 64870, loss = 1.37726821\n",
      "Iteration 64871, loss = 1.32728257\n",
      "Iteration 64872, loss = 1.23971393\n",
      "Iteration 64873, loss = 1.32835553\n",
      "Iteration 64874, loss = 1.20417492\n",
      "Iteration 64875, loss = 1.28520674\n",
      "Iteration 64876, loss = 1.25069232\n",
      "Iteration 64877, loss = 1.21386664\n",
      "Iteration 64878, loss = 1.20796631\n",
      "Iteration 64879, loss = 1.20301103\n",
      "Iteration 64880, loss = 1.34123217\n",
      "Iteration 64881, loss = 1.40213392\n",
      "Iteration 64882, loss = 1.21417243\n",
      "Iteration 64883, loss = 1.32972203\n",
      "Iteration 64884, loss = 1.37336905\n",
      "Iteration 64885, loss = 1.25877765\n",
      "Iteration 64886, loss = 1.53500814\n",
      "Iteration 64887, loss = 1.57467264\n",
      "Iteration 64888, loss = 1.60032886\n",
      "Iteration 64889, loss = 1.40494515\n",
      "Iteration 64890, loss = 1.40118557\n",
      "Iteration 64891, loss = 1.41774235\n",
      "Iteration 64892, loss = 1.50644685\n",
      "Iteration 64893, loss = 1.80092305\n",
      "Iteration 64894, loss = 1.57067014\n",
      "Iteration 64895, loss = 1.45776398\n",
      "Iteration 64896, loss = 1.78154553\n",
      "Iteration 64897, loss = 1.31652284\n",
      "Iteration 64898, loss = 1.29165913\n",
      "Iteration 64899, loss = 1.36417193\n",
      "Iteration 64900, loss = 1.19956928\n",
      "Iteration 64901, loss = 1.16448142\n",
      "Iteration 64902, loss = 1.26879445\n",
      "Iteration 64903, loss = 1.15756655\n",
      "Iteration 64904, loss = 1.17316968\n",
      "Iteration 64905, loss = 1.11369815\n",
      "Iteration 64906, loss = 1.30965643\n",
      "Iteration 64907, loss = 1.51515029\n",
      "Iteration 64908, loss = 1.78802972\n",
      "Iteration 64909, loss = 1.58804767\n",
      "Iteration 64910, loss = 1.37184666\n",
      "Iteration 64911, loss = 1.54553065\n",
      "Iteration 64912, loss = 1.67116787\n",
      "Iteration 64913, loss = 1.38503594\n",
      "Iteration 64914, loss = 1.18033749\n",
      "Iteration 64915, loss = 1.28497788\n",
      "Iteration 64916, loss = 1.18310868\n",
      "Iteration 64917, loss = 1.16469654\n",
      "Iteration 64918, loss = 1.54136767\n",
      "Iteration 64919, loss = 1.50537819\n",
      "Iteration 64920, loss = 1.41165856\n",
      "Iteration 64921, loss = 1.25201909\n",
      "Iteration 64922, loss = 1.23554313\n",
      "Iteration 64923, loss = 1.75359474\n",
      "Iteration 64924, loss = 2.07373937\n",
      "Iteration 64925, loss = 1.40116543\n",
      "Iteration 64926, loss = 2.09337647\n",
      "Iteration 64927, loss = 4.07432070\n",
      "Iteration 64928, loss = 5.95509221\n",
      "Iteration 64929, loss = 4.06828179\n",
      "Iteration 64930, loss = 3.58491242\n",
      "Iteration 64931, loss = 3.36419297\n",
      "Iteration 64932, loss = 2.45938548\n",
      "Iteration 64933, loss = 2.31647326\n",
      "Iteration 64934, loss = 1.71942645\n",
      "Iteration 64935, loss = 1.51354357\n",
      "Iteration 64936, loss = 1.25833080\n",
      "Iteration 64937, loss = 1.39050776\n",
      "Iteration 64938, loss = 1.57718842\n",
      "Iteration 64939, loss = 1.54873920\n",
      "Iteration 64940, loss = 1.77085597\n",
      "Iteration 64941, loss = 1.84935604\n",
      "Iteration 64942, loss = 1.49521581\n",
      "Iteration 64943, loss = 1.59909831\n",
      "Iteration 64944, loss = 1.79514003\n",
      "Iteration 64945, loss = 1.91168212\n",
      "Iteration 64946, loss = 1.75920444\n",
      "Iteration 64947, loss = 1.61401091\n",
      "Iteration 64948, loss = 1.58846053\n",
      "Iteration 64949, loss = 1.45079161\n",
      "Iteration 64950, loss = 1.52039007\n",
      "Iteration 64951, loss = 1.44550779\n",
      "Iteration 64952, loss = 1.47222344\n",
      "Iteration 64953, loss = 1.37405981\n",
      "Iteration 64954, loss = 1.52906665\n",
      "Iteration 64955, loss = 1.20605363\n",
      "Iteration 64956, loss = 1.23298137\n",
      "Iteration 64957, loss = 1.39790760\n",
      "Iteration 64958, loss = 1.40823171\n",
      "Iteration 64959, loss = 1.35048707\n",
      "Iteration 64960, loss = 1.17465018\n",
      "Iteration 64961, loss = 1.21663309\n",
      "Iteration 64962, loss = 1.37038342\n",
      "Iteration 64963, loss = 1.48145714\n",
      "Iteration 64964, loss = 1.36743655\n",
      "Iteration 64965, loss = 1.10872112\n",
      "Iteration 64966, loss = 1.09003018\n",
      "Iteration 64967, loss = 1.17585972\n",
      "Iteration 64968, loss = 1.28873759\n",
      "Iteration 64969, loss = 1.17988139\n",
      "Iteration 64970, loss = 1.13028125\n",
      "Iteration 64971, loss = 1.18236580\n",
      "Iteration 64972, loss = 1.30552845\n",
      "Iteration 64973, loss = 1.49455678\n",
      "Iteration 64974, loss = 1.48318666\n",
      "Iteration 64975, loss = 1.70910991\n",
      "Iteration 64976, loss = 1.51846946\n",
      "Iteration 64977, loss = 1.78510757\n",
      "Iteration 64978, loss = 1.38626809\n",
      "Iteration 64979, loss = 1.44285112\n",
      "Iteration 64980, loss = 1.39179297\n",
      "Iteration 64981, loss = 1.25249949\n",
      "Iteration 64982, loss = 1.08556973\n",
      "Iteration 64983, loss = 1.16680989\n",
      "Iteration 64984, loss = 1.11114050\n",
      "Iteration 64985, loss = 1.10414095\n",
      "Iteration 64986, loss = 1.37112769\n",
      "Iteration 64987, loss = 1.51637492\n",
      "Iteration 64988, loss = 1.66309335\n",
      "Iteration 64989, loss = 1.77607342\n",
      "Iteration 64990, loss = 1.39930999\n",
      "Iteration 64991, loss = 1.28277573\n",
      "Iteration 64992, loss = 1.19554293\n",
      "Iteration 64993, loss = 1.20217143\n",
      "Iteration 64994, loss = 1.42836127\n",
      "Iteration 64995, loss = 1.38200204\n",
      "Iteration 64996, loss = 1.34457534\n",
      "Iteration 64997, loss = 1.30569841\n",
      "Iteration 64998, loss = 1.60250050\n",
      "Iteration 64999, loss = 1.82218647\n",
      "Iteration 65000, loss = 1.98783816\n",
      "Iteration 65001, loss = 2.97017410\n",
      "Iteration 65002, loss = 2.98425175\n",
      "Iteration 65003, loss = 2.27685885\n",
      "Iteration 65004, loss = 2.20634820\n",
      "Iteration 65005, loss = 2.14856510\n",
      "Iteration 65006, loss = 1.37699979\n",
      "Iteration 65007, loss = 1.79130362\n",
      "Iteration 65008, loss = 1.67651172\n",
      "Iteration 65009, loss = 1.55189546\n",
      "Iteration 65010, loss = 1.37230862\n",
      "Iteration 65011, loss = 1.33243515\n",
      "Iteration 65012, loss = 1.34712549\n",
      "Iteration 65013, loss = 1.36374722\n",
      "Iteration 65014, loss = 1.31875527\n",
      "Iteration 65015, loss = 1.14969513\n",
      "Iteration 65016, loss = 1.27931160\n",
      "Iteration 65017, loss = 1.20271707\n",
      "Iteration 65018, loss = 1.24418256\n",
      "Iteration 65019, loss = 1.13886467\n",
      "Iteration 65020, loss = 1.28145263\n",
      "Iteration 65021, loss = 1.22468777\n",
      "Iteration 65022, loss = 1.31747794\n",
      "Iteration 65023, loss = 1.42402731\n",
      "Iteration 65024, loss = 1.61063239\n",
      "Iteration 65025, loss = 2.59794961\n",
      "Iteration 65026, loss = 1.87593135\n",
      "Iteration 65027, loss = 1.67432040\n",
      "Iteration 65028, loss = 1.29320239\n",
      "Iteration 65029, loss = 1.28490093\n",
      "Iteration 65030, loss = 1.29696197\n",
      "Iteration 65031, loss = 1.25712397\n",
      "Iteration 65032, loss = 1.17269870\n",
      "Iteration 65033, loss = 1.13875849\n",
      "Iteration 65034, loss = 1.18371348\n",
      "Iteration 65035, loss = 1.23948115\n",
      "Iteration 65036, loss = 1.20523302\n",
      "Iteration 65037, loss = 1.34594342\n",
      "Iteration 65038, loss = 1.44873257\n",
      "Iteration 65039, loss = 1.40209665\n",
      "Iteration 65040, loss = 1.27534439\n",
      "Iteration 65041, loss = 1.67730415\n",
      "Iteration 65042, loss = 1.87481404\n",
      "Iteration 65043, loss = 2.12713900\n",
      "Iteration 65044, loss = 2.22283398\n",
      "Iteration 65045, loss = 1.66153874\n",
      "Iteration 65046, loss = 1.43739381\n",
      "Iteration 65047, loss = 1.31117147\n",
      "Iteration 65048, loss = 1.68073179\n",
      "Iteration 65049, loss = 1.53998338\n",
      "Iteration 65050, loss = 1.68424096\n",
      "Iteration 65051, loss = 1.73288397\n",
      "Iteration 65052, loss = 1.83108511\n",
      "Iteration 65053, loss = 1.33372414\n",
      "Iteration 65054, loss = 1.23483449\n",
      "Iteration 65055, loss = 1.25894507\n",
      "Iteration 65056, loss = 1.25450974\n",
      "Iteration 65057, loss = 1.20465695\n",
      "Iteration 65058, loss = 1.31216174\n",
      "Iteration 65059, loss = 1.25759636\n",
      "Iteration 65060, loss = 1.27294288\n",
      "Iteration 65061, loss = 1.45354272\n",
      "Iteration 65062, loss = 1.30946358\n",
      "Iteration 65063, loss = 1.26765272\n",
      "Iteration 65064, loss = 1.35899864\n",
      "Iteration 65065, loss = 1.24615700\n",
      "Iteration 65066, loss = 1.24511510\n",
      "Iteration 65067, loss = 1.22337364\n",
      "Iteration 65068, loss = 1.26588209\n",
      "Iteration 65069, loss = 1.53703048\n",
      "Iteration 65070, loss = 1.27089492\n",
      "Iteration 65071, loss = 1.05229575\n",
      "Iteration 65072, loss = 1.10253173\n",
      "Iteration 65073, loss = 1.09055999\n",
      "Iteration 65074, loss = 1.29917704\n",
      "Iteration 65075, loss = 1.42747462\n",
      "Iteration 65076, loss = 1.14037762\n",
      "Iteration 65077, loss = 1.18718777\n",
      "Iteration 65078, loss = 1.11674130\n",
      "Iteration 65079, loss = 1.16992532\n",
      "Iteration 65080, loss = 1.40350905\n",
      "Iteration 65081, loss = 1.46636833\n",
      "Iteration 65082, loss = 1.95527415\n",
      "Iteration 65083, loss = 2.29767073\n",
      "Iteration 65084, loss = 2.13953564\n",
      "Iteration 65085, loss = 1.52143578\n",
      "Iteration 65086, loss = 1.58540864\n",
      "Iteration 65087, loss = 1.48854159\n",
      "Iteration 65088, loss = 2.22335607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 65089, loss = 1.73324472\n",
      "Iteration 65090, loss = 2.27074143\n",
      "Iteration 65091, loss = 1.92793152\n",
      "Iteration 65092, loss = 1.79756589\n",
      "Iteration 65093, loss = 2.14801437\n",
      "Iteration 65094, loss = 1.86018924\n",
      "Iteration 65095, loss = 2.08283975\n",
      "Iteration 65096, loss = 1.55350003\n",
      "Iteration 65097, loss = 1.72369376\n",
      "Iteration 65098, loss = 2.05357947\n",
      "Iteration 65099, loss = 1.51825805\n",
      "Iteration 65100, loss = 1.52826052\n",
      "Iteration 65101, loss = 1.33243802\n",
      "Iteration 65102, loss = 1.31796010\n",
      "Iteration 65103, loss = 1.33173980\n",
      "Iteration 65104, loss = 1.27213049\n",
      "Iteration 65105, loss = 1.30222547\n",
      "Iteration 65106, loss = 1.27891936\n",
      "Iteration 65107, loss = 1.25493316\n",
      "Iteration 65108, loss = 1.17140630\n",
      "Iteration 65109, loss = 1.28985867\n",
      "Iteration 65110, loss = 1.31539008\n",
      "Iteration 65111, loss = 1.15104594\n",
      "Iteration 65112, loss = 1.31902708\n",
      "Iteration 65113, loss = 1.19403935\n",
      "Iteration 65114, loss = 1.51538370\n",
      "Iteration 65115, loss = 1.65028238\n",
      "Iteration 65116, loss = 1.83419767\n",
      "Iteration 65117, loss = 1.35387812\n",
      "Iteration 65118, loss = 1.42321558\n",
      "Iteration 65119, loss = 1.40169544\n",
      "Iteration 65120, loss = 1.62287790\n",
      "Iteration 65121, loss = 1.51931320\n",
      "Iteration 65122, loss = 1.39814303\n",
      "Iteration 65123, loss = 1.39006258\n",
      "Iteration 65124, loss = 1.74395455\n",
      "Iteration 65125, loss = 1.67648277\n",
      "Iteration 65126, loss = 1.56520635\n",
      "Iteration 65127, loss = 1.38756477\n",
      "Iteration 65128, loss = 1.48525034\n",
      "Iteration 65129, loss = 1.65172795\n",
      "Iteration 65130, loss = 1.61918031\n",
      "Iteration 65131, loss = 1.79757031\n",
      "Iteration 65132, loss = 1.74816333\n",
      "Iteration 65133, loss = 1.67352362\n",
      "Iteration 65134, loss = 1.38611613\n",
      "Iteration 65135, loss = 1.27441953\n",
      "Iteration 65136, loss = 1.25007544\n",
      "Iteration 65137, loss = 1.13578409\n",
      "Iteration 65138, loss = 1.19641294\n",
      "Iteration 65139, loss = 1.15707772\n",
      "Iteration 65140, loss = 1.16737049\n",
      "Iteration 65141, loss = 1.38377953\n",
      "Iteration 65142, loss = 1.19379223\n",
      "Iteration 65143, loss = 1.19298737\n",
      "Iteration 65144, loss = 1.13889667\n",
      "Iteration 65145, loss = 1.18638350\n",
      "Iteration 65146, loss = 1.39275091\n",
      "Iteration 65147, loss = 1.39956105\n",
      "Iteration 65148, loss = 1.35011296\n",
      "Iteration 65149, loss = 1.41810799\n",
      "Iteration 65150, loss = 1.61080449\n",
      "Iteration 65151, loss = 1.11318204\n",
      "Iteration 65152, loss = 1.08979594\n",
      "Iteration 65153, loss = 1.15566946\n",
      "Iteration 65154, loss = 1.11280212\n",
      "Iteration 65155, loss = 1.23565923\n",
      "Iteration 65156, loss = 1.19502896\n",
      "Iteration 65157, loss = 1.26782247\n",
      "Iteration 65158, loss = 1.76494318\n",
      "Iteration 65159, loss = 1.79892735\n",
      "Iteration 65160, loss = 2.01619855\n",
      "Iteration 65161, loss = 1.61505736\n",
      "Iteration 65162, loss = 1.42705737\n",
      "Iteration 65163, loss = 1.28388275\n",
      "Iteration 65164, loss = 1.23540099\n",
      "Iteration 65165, loss = 1.15440906\n",
      "Iteration 65166, loss = 1.65734298\n",
      "Iteration 65167, loss = 1.60650957\n",
      "Iteration 65168, loss = 1.38164798\n",
      "Iteration 65169, loss = 1.23293836\n",
      "Iteration 65170, loss = 1.29314708\n",
      "Iteration 65171, loss = 1.41030480\n",
      "Iteration 65172, loss = 1.20478425\n",
      "Iteration 65173, loss = 1.58740595\n",
      "Iteration 65174, loss = 1.92380512\n",
      "Iteration 65175, loss = 1.93883995\n",
      "Iteration 65176, loss = 1.51981464\n",
      "Iteration 65177, loss = 1.40203720\n",
      "Iteration 65178, loss = 1.37970211\n",
      "Iteration 65179, loss = 1.80141453\n",
      "Iteration 65180, loss = 2.61200260\n",
      "Iteration 65181, loss = 2.28080049\n",
      "Iteration 65182, loss = 1.80042059\n",
      "Iteration 65183, loss = 1.44277423\n",
      "Iteration 65184, loss = 1.21334617\n",
      "Iteration 65185, loss = 1.16931850\n",
      "Iteration 65186, loss = 1.20250993\n",
      "Iteration 65187, loss = 1.61265865\n",
      "Iteration 65188, loss = 1.64487809\n",
      "Iteration 65189, loss = 1.57857314\n",
      "Iteration 65190, loss = 1.57809897\n",
      "Iteration 65191, loss = 1.63565332\n",
      "Iteration 65192, loss = 1.33372932\n",
      "Iteration 65193, loss = 1.25534472\n",
      "Iteration 65194, loss = 1.39037598\n",
      "Iteration 65195, loss = 1.26377480\n",
      "Iteration 65196, loss = 1.14211053\n",
      "Iteration 65197, loss = 1.11350979\n",
      "Iteration 65198, loss = 1.14837358\n",
      "Iteration 65199, loss = 1.17767452\n",
      "Iteration 65200, loss = 1.30531581\n",
      "Iteration 65201, loss = 1.40245310\n",
      "Iteration 65202, loss = 1.27618571\n",
      "Iteration 65203, loss = 1.46661222\n",
      "Iteration 65204, loss = 1.33572331\n",
      "Iteration 65205, loss = 1.46306795\n",
      "Iteration 65206, loss = 1.43936675\n",
      "Iteration 65207, loss = 1.50114477\n",
      "Iteration 65208, loss = 1.69315254\n",
      "Iteration 65209, loss = 2.00882840\n",
      "Iteration 65210, loss = 2.26912203\n",
      "Iteration 65211, loss = 2.37682867\n",
      "Iteration 65212, loss = 1.90499649\n",
      "Iteration 65213, loss = 1.93180697\n",
      "Iteration 65214, loss = 2.16361197\n",
      "Iteration 65215, loss = 1.70791378\n",
      "Iteration 65216, loss = 1.51732527\n",
      "Iteration 65217, loss = 1.30042337\n",
      "Iteration 65218, loss = 1.25971893\n",
      "Iteration 65219, loss = 1.17644591\n",
      "Iteration 65220, loss = 1.39314383\n",
      "Iteration 65221, loss = 1.52966486\n",
      "Iteration 65222, loss = 1.17442380\n",
      "Iteration 65223, loss = 1.17929675\n",
      "Iteration 65224, loss = 1.31671423\n",
      "Iteration 65225, loss = 1.65670526\n",
      "Iteration 65226, loss = 1.52533680\n",
      "Iteration 65227, loss = 1.26159779\n",
      "Iteration 65228, loss = 1.49884716\n",
      "Iteration 65229, loss = 1.32374414\n",
      "Iteration 65230, loss = 1.21977015\n",
      "Iteration 65231, loss = 1.29955397\n",
      "Iteration 65232, loss = 1.29618773\n",
      "Iteration 65233, loss = 1.21312009\n",
      "Iteration 65234, loss = 1.13447830\n",
      "Iteration 65235, loss = 1.06749128\n",
      "Iteration 65236, loss = 1.10854474\n",
      "Iteration 65237, loss = 1.13277825\n",
      "Iteration 65238, loss = 1.10488292\n",
      "Iteration 65239, loss = 1.12985386\n",
      "Iteration 65240, loss = 1.24786502\n",
      "Iteration 65241, loss = 1.26325482\n",
      "Iteration 65242, loss = 1.30237582\n",
      "Iteration 65243, loss = 1.17029517\n",
      "Iteration 65244, loss = 1.38495290\n",
      "Iteration 65245, loss = 1.07516403\n",
      "Iteration 65246, loss = 1.14742702\n",
      "Iteration 65247, loss = 1.11358855\n",
      "Iteration 65248, loss = 1.44783097\n",
      "Iteration 65249, loss = 1.18693622\n",
      "Iteration 65250, loss = 1.32830523\n",
      "Iteration 65251, loss = 1.33168438\n",
      "Iteration 65252, loss = 1.44878138\n",
      "Iteration 65253, loss = 1.83537068\n",
      "Iteration 65254, loss = 2.04735766\n",
      "Iteration 65255, loss = 1.44554983\n",
      "Iteration 65256, loss = 1.57227134\n",
      "Iteration 65257, loss = 1.51315057\n",
      "Iteration 65258, loss = 1.66826941\n",
      "Iteration 65259, loss = 1.57745003\n",
      "Iteration 65260, loss = 1.39337371\n",
      "Iteration 65261, loss = 1.24431435\n",
      "Iteration 65262, loss = 1.48684782\n",
      "Iteration 65263, loss = 1.92749814\n",
      "Iteration 65264, loss = 2.50961687\n",
      "Iteration 65265, loss = 2.44263126\n",
      "Iteration 65266, loss = 1.47230009\n",
      "Iteration 65267, loss = 1.75672283\n",
      "Iteration 65268, loss = 1.46474296\n",
      "Iteration 65269, loss = 1.68742489\n",
      "Iteration 65270, loss = 1.54535567\n",
      "Iteration 65271, loss = 1.22724373\n",
      "Iteration 65272, loss = 1.22317812\n",
      "Iteration 65273, loss = 1.18021055\n",
      "Iteration 65274, loss = 1.12953303\n",
      "Iteration 65275, loss = 1.22728936\n",
      "Iteration 65276, loss = 1.56433106\n",
      "Iteration 65277, loss = 1.38218972\n",
      "Iteration 65278, loss = 1.34965020\n",
      "Iteration 65279, loss = 1.57530884\n",
      "Iteration 65280, loss = 1.63743653\n",
      "Iteration 65281, loss = 1.74141313\n",
      "Iteration 65282, loss = 1.62793618\n",
      "Iteration 65283, loss = 1.27360040\n",
      "Iteration 65284, loss = 1.40768832\n",
      "Iteration 65285, loss = 1.51554774\n",
      "Iteration 65286, loss = 1.26917009\n",
      "Iteration 65287, loss = 1.19848782\n",
      "Iteration 65288, loss = 1.62814689\n",
      "Iteration 65289, loss = 1.19814348\n",
      "Iteration 65290, loss = 1.36023208\n",
      "Iteration 65291, loss = 1.59487904\n",
      "Iteration 65292, loss = 1.33089447\n",
      "Iteration 65293, loss = 1.54479920\n",
      "Iteration 65294, loss = 1.98684162\n",
      "Iteration 65295, loss = 2.03301251\n",
      "Iteration 65296, loss = 1.74846493\n",
      "Iteration 65297, loss = 1.31802169\n",
      "Iteration 65298, loss = 1.29594802\n",
      "Iteration 65299, loss = 1.49693388\n",
      "Iteration 65300, loss = 1.40814169\n",
      "Iteration 65301, loss = 1.39249417\n",
      "Iteration 65302, loss = 1.65708787\n",
      "Iteration 65303, loss = 1.63941807\n",
      "Iteration 65304, loss = 1.55417698\n",
      "Iteration 65305, loss = 1.33201100\n",
      "Iteration 65306, loss = 1.44367924\n",
      "Iteration 65307, loss = 1.30026372\n",
      "Iteration 65308, loss = 1.32949638\n",
      "Iteration 65309, loss = 1.47674547\n",
      "Iteration 65310, loss = 1.18018030\n",
      "Iteration 65311, loss = 1.08034256\n",
      "Iteration 65312, loss = 1.09551367\n",
      "Iteration 65313, loss = 1.08215852\n",
      "Iteration 65314, loss = 1.08411323\n",
      "Iteration 65315, loss = 1.33513964\n",
      "Iteration 65316, loss = 1.23371793\n",
      "Iteration 65317, loss = 1.23565898\n",
      "Iteration 65318, loss = 1.22886219\n",
      "Iteration 65319, loss = 1.21126582\n",
      "Iteration 65320, loss = 1.20070773\n",
      "Iteration 65321, loss = 1.45172821\n",
      "Iteration 65322, loss = 1.57289003\n",
      "Iteration 65323, loss = 1.25027374\n",
      "Iteration 65324, loss = 1.26571588\n",
      "Iteration 65325, loss = 1.41019902\n",
      "Iteration 65326, loss = 1.34878847\n",
      "Iteration 65327, loss = 1.42968096\n",
      "Iteration 65328, loss = 1.47876591\n",
      "Iteration 65329, loss = 1.20664695\n",
      "Iteration 65330, loss = 1.16188968\n",
      "Iteration 65331, loss = 1.12223676\n",
      "Iteration 65332, loss = 1.16864716\n",
      "Iteration 65333, loss = 1.41875191\n",
      "Iteration 65334, loss = 1.31504492\n",
      "Iteration 65335, loss = 1.44852339\n",
      "Iteration 65336, loss = 1.59519259\n",
      "Iteration 65337, loss = 1.23730425\n",
      "Iteration 65338, loss = 1.29663681\n",
      "Iteration 65339, loss = 1.14956478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 65340, loss = 1.19554546\n",
      "Iteration 65341, loss = 1.10770906\n",
      "Iteration 65342, loss = 1.15600520\n",
      "Iteration 65343, loss = 1.26706131\n",
      "Iteration 65344, loss = 1.21848326\n",
      "Iteration 65345, loss = 1.16157860\n",
      "Iteration 65346, loss = 1.19890902\n",
      "Iteration 65347, loss = 1.26791443\n",
      "Iteration 65348, loss = 1.26400335\n",
      "Iteration 65349, loss = 1.51033053\n",
      "Iteration 65350, loss = 1.34599499\n",
      "Iteration 65351, loss = 2.13379643\n",
      "Iteration 65352, loss = 2.00945700\n",
      "Iteration 65353, loss = 1.45843619\n",
      "Iteration 65354, loss = 1.34036662\n",
      "Iteration 65355, loss = 1.28687276\n",
      "Iteration 65356, loss = 1.32403837\n",
      "Iteration 65357, loss = 1.30746074\n",
      "Iteration 65358, loss = 1.43232859\n",
      "Iteration 65359, loss = 1.81993612\n",
      "Iteration 65360, loss = 1.47903594\n",
      "Iteration 65361, loss = 1.38953877\n",
      "Iteration 65362, loss = 1.19904849\n",
      "Iteration 65363, loss = 1.35041252\n",
      "Iteration 65364, loss = 1.83458345\n",
      "Iteration 65365, loss = 1.60578178\n",
      "Iteration 65366, loss = 1.37305615\n",
      "Iteration 65367, loss = 1.27480615\n",
      "Iteration 65368, loss = 1.34952416\n",
      "Iteration 65369, loss = 1.22957500\n",
      "Iteration 65370, loss = 1.13653103\n",
      "Iteration 65371, loss = 1.56035915\n",
      "Iteration 65372, loss = 1.42282951\n",
      "Iteration 65373, loss = 1.26919319\n",
      "Iteration 65374, loss = 1.28224996\n",
      "Iteration 65375, loss = 1.15430294\n",
      "Iteration 65376, loss = 1.23387559\n",
      "Iteration 65377, loss = 1.35082316\n",
      "Iteration 65378, loss = 1.28695138\n",
      "Iteration 65379, loss = 1.63419160\n",
      "Iteration 65380, loss = 1.72359713\n",
      "Iteration 65381, loss = 1.85238891\n",
      "Iteration 65382, loss = 2.00449572\n",
      "Iteration 65383, loss = 1.91722535\n",
      "Iteration 65384, loss = 2.14180349\n",
      "Iteration 65385, loss = 1.97204738\n",
      "Iteration 65386, loss = 1.36583748\n",
      "Iteration 65387, loss = 1.39386842\n",
      "Iteration 65388, loss = 1.37633237\n",
      "Iteration 65389, loss = 1.67108697\n",
      "Iteration 65390, loss = 1.70987454\n",
      "Iteration 65391, loss = 1.57113930\n",
      "Iteration 65392, loss = 1.50371606\n",
      "Iteration 65393, loss = 1.29865645\n",
      "Iteration 65394, loss = 1.13251076\n",
      "Iteration 65395, loss = 1.26655847\n",
      "Iteration 65396, loss = 1.36205722\n",
      "Iteration 65397, loss = 1.16057656\n",
      "Iteration 65398, loss = 1.28127857\n",
      "Iteration 65399, loss = 1.25259276\n",
      "Iteration 65400, loss = 1.11824462\n",
      "Iteration 65401, loss = 1.12134516\n",
      "Iteration 65402, loss = 1.26804054\n",
      "Iteration 65403, loss = 1.23366117\n",
      "Iteration 65404, loss = 1.24622487\n",
      "Iteration 65405, loss = 1.30837278\n",
      "Iteration 65406, loss = 1.30845612\n",
      "Iteration 65407, loss = 1.76419497\n",
      "Iteration 65408, loss = 1.74114615\n",
      "Iteration 65409, loss = 1.62918722\n",
      "Iteration 65410, loss = 1.63325251\n",
      "Iteration 65411, loss = 2.36424740\n",
      "Iteration 65412, loss = 2.73072473\n",
      "Iteration 65413, loss = 1.80436811\n",
      "Iteration 65414, loss = 2.15799214\n",
      "Iteration 65415, loss = 2.42649596\n",
      "Iteration 65416, loss = 2.87522226\n",
      "Iteration 65417, loss = 2.42873976\n",
      "Iteration 65418, loss = 1.47777517\n",
      "Iteration 65419, loss = 1.26727091\n",
      "Iteration 65420, loss = 1.31166106\n",
      "Iteration 65421, loss = 1.15437465\n",
      "Iteration 65422, loss = 1.19624839\n",
      "Iteration 65423, loss = 1.22005332\n",
      "Iteration 65424, loss = 1.25211677\n",
      "Iteration 65425, loss = 1.24053193\n",
      "Iteration 65426, loss = 1.36944570\n",
      "Iteration 65427, loss = 1.58631332\n",
      "Iteration 65428, loss = 1.47278273\n",
      "Iteration 65429, loss = 1.33058920\n",
      "Iteration 65430, loss = 1.26971349\n",
      "Iteration 65431, loss = 1.45799894\n",
      "Iteration 65432, loss = 1.10596300\n",
      "Iteration 65433, loss = 1.35613639\n",
      "Iteration 65434, loss = 1.29639546\n",
      "Iteration 65435, loss = 1.54532829\n",
      "Iteration 65436, loss = 1.72399740\n",
      "Iteration 65437, loss = 1.86971235\n",
      "Iteration 65438, loss = 2.13624977\n",
      "Iteration 65439, loss = 2.19879782\n",
      "Iteration 65440, loss = 1.84818820\n",
      "Iteration 65441, loss = 1.56675549\n",
      "Iteration 65442, loss = 1.35608156\n",
      "Iteration 65443, loss = 1.39905196\n",
      "Iteration 65444, loss = 1.26921280\n",
      "Iteration 65445, loss = 1.29134593\n",
      "Iteration 65446, loss = 1.20848862\n",
      "Iteration 65447, loss = 1.19108891\n",
      "Iteration 65448, loss = 1.20936860\n",
      "Iteration 65449, loss = 1.15382333\n",
      "Iteration 65450, loss = 1.16360191\n",
      "Iteration 65451, loss = 1.27719344\n",
      "Iteration 65452, loss = 1.58562867\n",
      "Iteration 65453, loss = 1.39681879\n",
      "Iteration 65454, loss = 1.43215913\n",
      "Iteration 65455, loss = 1.24395572\n",
      "Iteration 65456, loss = 1.46375127\n",
      "Iteration 65457, loss = 1.25733710\n",
      "Iteration 65458, loss = 1.62801719\n",
      "Iteration 65459, loss = 1.87495607\n",
      "Iteration 65460, loss = 1.89028273\n",
      "Iteration 65461, loss = 1.53634292\n",
      "Iteration 65462, loss = 1.47888206\n",
      "Iteration 65463, loss = 1.81802669\n",
      "Iteration 65464, loss = 2.13353671\n",
      "Iteration 65465, loss = 1.55954706\n",
      "Iteration 65466, loss = 1.37666893\n",
      "Iteration 65467, loss = 1.27789077\n",
      "Iteration 65468, loss = 1.47059751\n",
      "Iteration 65469, loss = 1.52924077\n",
      "Iteration 65470, loss = 1.59725097\n",
      "Iteration 65471, loss = 1.45437284\n",
      "Iteration 65472, loss = 1.41398252\n",
      "Iteration 65473, loss = 1.26792937\n",
      "Iteration 65474, loss = 1.17678710\n",
      "Iteration 65475, loss = 1.35151007\n",
      "Iteration 65476, loss = 1.20329118\n",
      "Iteration 65477, loss = 1.29322753\n",
      "Iteration 65478, loss = 1.35737815\n",
      "Iteration 65479, loss = 1.17812298\n",
      "Iteration 65480, loss = 1.18258694\n",
      "Iteration 65481, loss = 1.53435242\n",
      "Iteration 65482, loss = 1.63408778\n",
      "Iteration 65483, loss = 1.51420331\n",
      "Iteration 65484, loss = 1.50049531\n",
      "Iteration 65485, loss = 1.68381372\n",
      "Iteration 65486, loss = 1.44037082\n",
      "Iteration 65487, loss = 1.32362872\n",
      "Iteration 65488, loss = 1.96203606\n",
      "Iteration 65489, loss = 1.67725225\n",
      "Iteration 65490, loss = 1.38098569\n",
      "Iteration 65491, loss = 1.25650242\n",
      "Iteration 65492, loss = 1.03662776\n",
      "Iteration 65493, loss = 1.43158795\n",
      "Iteration 65494, loss = 1.74947468\n",
      "Iteration 65495, loss = 1.73170581\n",
      "Iteration 65496, loss = 1.41499525\n",
      "Iteration 65497, loss = 1.47741845\n",
      "Iteration 65498, loss = 1.18814654\n",
      "Iteration 65499, loss = 1.15277199\n",
      "Iteration 65500, loss = 1.26225705\n",
      "Iteration 65501, loss = 1.24867088\n",
      "Iteration 65502, loss = 1.22452000\n",
      "Iteration 65503, loss = 1.30191276\n",
      "Iteration 65504, loss = 1.33494092\n",
      "Iteration 65505, loss = 1.39788039\n",
      "Iteration 65506, loss = 1.56350849\n",
      "Iteration 65507, loss = 1.52573434\n",
      "Iteration 65508, loss = 1.36580377\n",
      "Iteration 65509, loss = 1.28680461\n",
      "Iteration 65510, loss = 1.36315974\n",
      "Iteration 65511, loss = 1.20304140\n",
      "Iteration 65512, loss = 1.24030443\n",
      "Iteration 65513, loss = 1.35613083\n",
      "Iteration 65514, loss = 1.35516191\n",
      "Iteration 65515, loss = 1.60775893\n",
      "Iteration 65516, loss = 1.21849139\n",
      "Iteration 65517, loss = 1.22388523\n",
      "Iteration 65518, loss = 1.37139340\n",
      "Iteration 65519, loss = 1.77526050\n",
      "Iteration 65520, loss = 1.32970199\n",
      "Iteration 65521, loss = 1.50237037\n",
      "Iteration 65522, loss = 1.52764368\n",
      "Iteration 65523, loss = 1.25707649\n",
      "Iteration 65524, loss = 1.26508258\n",
      "Iteration 65525, loss = 1.19455949\n",
      "Iteration 65526, loss = 1.18779798\n",
      "Iteration 65527, loss = 1.56323508\n",
      "Iteration 65528, loss = 1.79861487\n",
      "Iteration 65529, loss = 1.69407153\n",
      "Iteration 65530, loss = 1.59072262\n",
      "Iteration 65531, loss = 1.79491035\n",
      "Iteration 65532, loss = 1.84470128\n",
      "Iteration 65533, loss = 1.45467905\n",
      "Iteration 65534, loss = 1.23911391\n",
      "Iteration 65535, loss = 1.32770576\n",
      "Iteration 65536, loss = 1.37311180\n",
      "Iteration 65537, loss = 1.25810383\n",
      "Iteration 65538, loss = 1.55925227\n",
      "Iteration 65539, loss = 1.79805305\n",
      "Iteration 65540, loss = 1.52730862\n",
      "Iteration 65541, loss = 1.29646470\n",
      "Iteration 65542, loss = 1.24613035\n",
      "Iteration 65543, loss = 1.22591996\n",
      "Iteration 65544, loss = 1.26271910\n",
      "Iteration 65545, loss = 1.15265700\n",
      "Iteration 65546, loss = 1.17249278\n",
      "Iteration 65547, loss = 1.13806730\n",
      "Iteration 65548, loss = 1.14442024\n",
      "Iteration 65549, loss = 1.22040560\n",
      "Iteration 65550, loss = 1.25153686\n",
      "Iteration 65551, loss = 1.29635593\n",
      "Iteration 65552, loss = 1.20748292\n",
      "Iteration 65553, loss = 1.13150672\n",
      "Iteration 65554, loss = 1.08271250\n",
      "Iteration 65555, loss = 1.11209526\n",
      "Iteration 65556, loss = 1.20373875\n",
      "Iteration 65557, loss = 1.17530479\n",
      "Iteration 65558, loss = 1.17407252\n",
      "Iteration 65559, loss = 1.23074951\n",
      "Iteration 65560, loss = 1.68575588\n",
      "Iteration 65561, loss = 1.74631755\n",
      "Iteration 65562, loss = 1.70850715\n",
      "Iteration 65563, loss = 2.25233708\n",
      "Iteration 65564, loss = 2.41280941\n",
      "Iteration 65565, loss = 1.89887218\n",
      "Iteration 65566, loss = 1.60808306\n",
      "Iteration 65567, loss = 1.39038180\n",
      "Iteration 65568, loss = 1.39031512\n",
      "Iteration 65569, loss = 1.44606979\n",
      "Iteration 65570, loss = 1.43025795\n",
      "Iteration 65571, loss = 1.59505248\n",
      "Iteration 65572, loss = 1.23105909\n",
      "Iteration 65573, loss = 1.25410807\n",
      "Iteration 65574, loss = 1.22387929\n",
      "Iteration 65575, loss = 1.24720622\n",
      "Iteration 65576, loss = 1.24989759\n",
      "Iteration 65577, loss = 1.36115124\n",
      "Iteration 65578, loss = 1.42928551\n",
      "Iteration 65579, loss = 1.35496311\n",
      "Iteration 65580, loss = 1.21196919\n",
      "Iteration 65581, loss = 1.33950369\n",
      "Iteration 65582, loss = 1.32134771\n",
      "Iteration 65583, loss = 1.57956770\n",
      "Iteration 65584, loss = 1.41343911\n",
      "Iteration 65585, loss = 1.66583994\n",
      "Iteration 65586, loss = 1.64281003\n",
      "Iteration 65587, loss = 1.84448218\n",
      "Iteration 65588, loss = 1.91183552\n",
      "Iteration 65589, loss = 1.48402098\n",
      "Iteration 65590, loss = 1.83586918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 65591, loss = 2.00572013\n",
      "Iteration 65592, loss = 2.42099456\n",
      "Iteration 65593, loss = 2.49177072\n",
      "Iteration 65594, loss = 1.80625652\n",
      "Iteration 65595, loss = 1.47434601\n",
      "Iteration 65596, loss = 1.51034592\n",
      "Iteration 65597, loss = 1.47709903\n",
      "Iteration 65598, loss = 1.98776562\n",
      "Iteration 65599, loss = 2.58318608\n",
      "Iteration 65600, loss = 2.48028918\n",
      "Iteration 65601, loss = 2.32713525\n",
      "Iteration 65602, loss = 2.00669618\n",
      "Iteration 65603, loss = 1.56881425\n",
      "Iteration 65604, loss = 1.08006049\n",
      "Iteration 65605, loss = 1.22921643\n",
      "Iteration 65606, loss = 1.28897719\n",
      "Iteration 65607, loss = 1.52960461\n",
      "Iteration 65608, loss = 1.58077849\n",
      "Iteration 65609, loss = 1.60442032\n",
      "Iteration 65610, loss = 1.53095037\n",
      "Iteration 65611, loss = 1.44826590\n",
      "Iteration 65612, loss = 1.70131864\n",
      "Iteration 65613, loss = 1.46151231\n",
      "Iteration 65614, loss = 1.41537965\n",
      "Iteration 65615, loss = 1.63347858\n",
      "Iteration 65616, loss = 1.70955120\n",
      "Iteration 65617, loss = 1.60370808\n",
      "Iteration 65618, loss = 1.29329090\n",
      "Iteration 65619, loss = 1.35017827\n",
      "Iteration 65620, loss = 1.35741281\n",
      "Iteration 65621, loss = 1.40691160\n",
      "Iteration 65622, loss = 1.31336412\n",
      "Iteration 65623, loss = 1.35483412\n",
      "Iteration 65624, loss = 1.20783190\n",
      "Iteration 65625, loss = 1.09251080\n",
      "Iteration 65626, loss = 1.12837915\n",
      "Iteration 65627, loss = 1.15365143\n",
      "Iteration 65628, loss = 1.18831957\n",
      "Iteration 65629, loss = 1.23472244\n",
      "Iteration 65630, loss = 1.45250771\n",
      "Iteration 65631, loss = 1.53548798\n",
      "Iteration 65632, loss = 1.44977016\n",
      "Iteration 65633, loss = 1.56557557\n",
      "Iteration 65634, loss = 1.28638896\n",
      "Iteration 65635, loss = 1.32159846\n",
      "Iteration 65636, loss = 1.67223795\n",
      "Iteration 65637, loss = 1.82126195\n",
      "Iteration 65638, loss = 1.46798948\n",
      "Iteration 65639, loss = 1.39155649\n",
      "Iteration 65640, loss = 1.16527645\n",
      "Iteration 65641, loss = 1.32202403\n",
      "Iteration 65642, loss = 1.47948876\n",
      "Iteration 65643, loss = 1.49326424\n",
      "Iteration 65644, loss = 1.33886820\n",
      "Iteration 65645, loss = 1.53918519\n",
      "Iteration 65646, loss = 1.23899280\n",
      "Iteration 65647, loss = 1.33286881\n",
      "Iteration 65648, loss = 1.08866650\n",
      "Iteration 65649, loss = 1.26634006\n",
      "Iteration 65650, loss = 1.21581522\n",
      "Iteration 65651, loss = 1.17567301\n",
      "Iteration 65652, loss = 1.27804197\n",
      "Iteration 65653, loss = 1.24921777\n",
      "Iteration 65654, loss = 1.29357555\n",
      "Iteration 65655, loss = 1.33078502\n",
      "Iteration 65656, loss = 1.41603059\n",
      "Iteration 65657, loss = 1.37947925\n",
      "Iteration 65658, loss = 2.04045753\n",
      "Iteration 65659, loss = 2.06252954\n",
      "Iteration 65660, loss = 2.39926473\n",
      "Iteration 65661, loss = 2.13487052\n",
      "Iteration 65662, loss = 1.67750918\n",
      "Iteration 65663, loss = 1.79706572\n",
      "Iteration 65664, loss = 1.69853110\n",
      "Iteration 65665, loss = 1.58451686\n",
      "Iteration 65666, loss = 1.24953394\n",
      "Iteration 65667, loss = 1.29234587\n",
      "Iteration 65668, loss = 1.47753822\n",
      "Iteration 65669, loss = 1.27558009\n",
      "Iteration 65670, loss = 1.23292152\n",
      "Iteration 65671, loss = 1.24624997\n",
      "Iteration 65672, loss = 1.24682357\n",
      "Iteration 65673, loss = 1.25346202\n",
      "Iteration 65674, loss = 1.68961499\n",
      "Iteration 65675, loss = 1.49734040\n",
      "Iteration 65676, loss = 1.21341639\n",
      "Iteration 65677, loss = 1.26806057\n",
      "Iteration 65678, loss = 1.39485960\n",
      "Iteration 65679, loss = 1.36118792\n",
      "Iteration 65680, loss = 1.35244583\n",
      "Iteration 65681, loss = 1.97658173\n",
      "Iteration 65682, loss = 1.70343810\n",
      "Iteration 65683, loss = 1.72991346\n",
      "Iteration 65684, loss = 3.01782146\n",
      "Iteration 65685, loss = 2.60895118\n",
      "Iteration 65686, loss = 2.25772609\n",
      "Iteration 65687, loss = 1.32700308\n",
      "Iteration 65688, loss = 1.27464385\n",
      "Iteration 65689, loss = 1.19408090\n",
      "Iteration 65690, loss = 1.23093031\n",
      "Iteration 65691, loss = 1.25655121\n",
      "Iteration 65692, loss = 1.11042777\n",
      "Iteration 65693, loss = 1.23836731\n",
      "Iteration 65694, loss = 1.23199870\n",
      "Iteration 65695, loss = 1.41127003\n",
      "Iteration 65696, loss = 1.30249489\n",
      "Iteration 65697, loss = 1.42014035\n",
      "Iteration 65698, loss = 1.50560411\n",
      "Iteration 65699, loss = 1.52682367\n",
      "Iteration 65700, loss = 1.72725565\n",
      "Iteration 65701, loss = 1.48984920\n",
      "Iteration 65702, loss = 1.16054765\n",
      "Iteration 65703, loss = 1.10909854\n",
      "Iteration 65704, loss = 1.10912055\n",
      "Iteration 65705, loss = 1.19366866\n",
      "Iteration 65706, loss = 1.10670285\n",
      "Iteration 65707, loss = 1.21715906\n",
      "Iteration 65708, loss = 1.43095757\n",
      "Iteration 65709, loss = 1.48804996\n",
      "Iteration 65710, loss = 1.62471740\n",
      "Iteration 65711, loss = 1.60348635\n",
      "Iteration 65712, loss = 2.16665615\n",
      "Iteration 65713, loss = 2.06603857\n",
      "Iteration 65714, loss = 2.45144952\n",
      "Iteration 65715, loss = 3.16593375\n",
      "Iteration 65716, loss = 2.14775780\n",
      "Iteration 65717, loss = 2.05922320\n",
      "Iteration 65718, loss = 1.89574479\n",
      "Iteration 65719, loss = 1.81769414\n",
      "Iteration 65720, loss = 2.02875350\n",
      "Iteration 65721, loss = 1.49330467\n",
      "Iteration 65722, loss = 1.52308814\n",
      "Iteration 65723, loss = 1.67437769\n",
      "Iteration 65724, loss = 1.57067552\n",
      "Iteration 65725, loss = 1.38845071\n",
      "Iteration 65726, loss = 1.18354051\n",
      "Iteration 65727, loss = 1.16920420\n",
      "Iteration 65728, loss = 1.31205990\n",
      "Iteration 65729, loss = 1.33958833\n",
      "Iteration 65730, loss = 1.35121055\n",
      "Iteration 65731, loss = 1.35383474\n",
      "Iteration 65732, loss = 1.29208251\n",
      "Iteration 65733, loss = 1.25462530\n",
      "Iteration 65734, loss = 1.22408837\n",
      "Iteration 65735, loss = 1.30409028\n",
      "Iteration 65736, loss = 1.29283098\n",
      "Iteration 65737, loss = 1.26959929\n",
      "Iteration 65738, loss = 1.30331714\n",
      "Iteration 65739, loss = 1.43696234\n",
      "Iteration 65740, loss = 1.34662614\n",
      "Iteration 65741, loss = 1.29618630\n",
      "Iteration 65742, loss = 1.24218976\n",
      "Iteration 65743, loss = 1.19174250\n",
      "Iteration 65744, loss = 1.15296607\n",
      "Iteration 65745, loss = 1.16728021\n",
      "Iteration 65746, loss = 1.15922375\n",
      "Iteration 65747, loss = 1.24683908\n",
      "Iteration 65748, loss = 1.12806388\n",
      "Iteration 65749, loss = 1.06246289\n",
      "Iteration 65750, loss = 1.14659627\n",
      "Iteration 65751, loss = 1.28089308\n",
      "Iteration 65752, loss = 1.37062792\n",
      "Iteration 65753, loss = 1.37873368\n",
      "Iteration 65754, loss = 1.72709317\n",
      "Iteration 65755, loss = 1.36066550\n",
      "Iteration 65756, loss = 1.71251437\n",
      "Iteration 65757, loss = 1.48722832\n",
      "Iteration 65758, loss = 1.38209707\n",
      "Iteration 65759, loss = 1.45234596\n",
      "Iteration 65760, loss = 1.20539209\n",
      "Iteration 65761, loss = 1.42606324\n",
      "Iteration 65762, loss = 1.40171266\n",
      "Iteration 65763, loss = 1.34932777\n",
      "Iteration 65764, loss = 1.26659335\n",
      "Iteration 65765, loss = 1.20762727\n",
      "Iteration 65766, loss = 1.19195246\n",
      "Iteration 65767, loss = 1.09701284\n",
      "Iteration 65768, loss = 1.13254706\n",
      "Iteration 65769, loss = 1.21249503\n",
      "Iteration 65770, loss = 1.27610380\n",
      "Iteration 65771, loss = 1.22335246\n",
      "Iteration 65772, loss = 1.38931064\n",
      "Iteration 65773, loss = 1.21541470\n",
      "Iteration 65774, loss = 1.29566968\n",
      "Iteration 65775, loss = 1.35766033\n",
      "Iteration 65776, loss = 1.21636192\n",
      "Iteration 65777, loss = 1.44722952\n",
      "Iteration 65778, loss = 1.53333954\n",
      "Iteration 65779, loss = 1.40521245\n",
      "Iteration 65780, loss = 1.31238677\n",
      "Iteration 65781, loss = 1.24525145\n",
      "Iteration 65782, loss = 1.26523057\n",
      "Iteration 65783, loss = 1.26775440\n",
      "Iteration 65784, loss = 1.35157299\n",
      "Iteration 65785, loss = 1.86644635\n",
      "Iteration 65786, loss = 1.82531750\n",
      "Iteration 65787, loss = 1.90348786\n",
      "Iteration 65788, loss = 2.27129963\n",
      "Iteration 65789, loss = 1.89136872\n",
      "Iteration 65790, loss = 1.66599191\n",
      "Iteration 65791, loss = 1.32938212\n",
      "Iteration 65792, loss = 1.34863059\n",
      "Iteration 65793, loss = 1.51977545\n",
      "Iteration 65794, loss = 1.81489483\n",
      "Iteration 65795, loss = 1.68684911\n",
      "Iteration 65796, loss = 1.65778537\n",
      "Iteration 65797, loss = 2.04427629\n",
      "Iteration 65798, loss = 2.44181817\n",
      "Iteration 65799, loss = 2.02926831\n",
      "Iteration 65800, loss = 2.31978285\n",
      "Iteration 65801, loss = 3.32955234\n",
      "Iteration 65802, loss = 3.11614650\n",
      "Iteration 65803, loss = 2.33236750\n",
      "Iteration 65804, loss = 2.50785528\n",
      "Iteration 65805, loss = 3.78570445\n",
      "Iteration 65806, loss = 4.20528876\n",
      "Iteration 65807, loss = 3.21065125\n",
      "Iteration 65808, loss = 3.53095576\n",
      "Iteration 65809, loss = 2.94268310\n",
      "Iteration 65810, loss = 2.08460024\n",
      "Iteration 65811, loss = 2.46510092\n",
      "Iteration 65812, loss = 2.44589103\n",
      "Iteration 65813, loss = 2.00789986\n",
      "Iteration 65814, loss = 2.19935508\n",
      "Iteration 65815, loss = 2.11552660\n",
      "Iteration 65816, loss = 1.50583422\n",
      "Iteration 65817, loss = 1.48684412\n",
      "Iteration 65818, loss = 1.79802929\n",
      "Iteration 65819, loss = 1.59318604\n",
      "Iteration 65820, loss = 1.63091935\n",
      "Iteration 65821, loss = 1.67184607\n",
      "Iteration 65822, loss = 1.63226000\n",
      "Iteration 65823, loss = 1.72890254\n",
      "Iteration 65824, loss = 1.33656193\n",
      "Iteration 65825, loss = 1.56359185\n",
      "Iteration 65826, loss = 1.40715092\n",
      "Iteration 65827, loss = 1.65140343\n",
      "Iteration 65828, loss = 1.29665365\n",
      "Iteration 65829, loss = 1.20240487\n",
      "Iteration 65830, loss = 1.31437009\n",
      "Iteration 65831, loss = 1.12554278\n",
      "Iteration 65832, loss = 1.28962753\n",
      "Iteration 65833, loss = 1.10563785\n",
      "Iteration 65834, loss = 1.36614792\n",
      "Iteration 65835, loss = 1.45231987\n",
      "Iteration 65836, loss = 1.33911909\n",
      "Iteration 65837, loss = 1.39329325\n",
      "Iteration 65838, loss = 1.30756591\n",
      "Iteration 65839, loss = 1.58791357\n",
      "Iteration 65840, loss = 1.46535788\n",
      "Iteration 65841, loss = 1.29960799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 65842, loss = 1.17012141\n",
      "Iteration 65843, loss = 1.15750077\n",
      "Iteration 65844, loss = 1.37370208\n",
      "Iteration 65845, loss = 1.77306692\n",
      "Iteration 65846, loss = 1.76300668\n",
      "Iteration 65847, loss = 1.40777271\n",
      "Iteration 65848, loss = 1.56743187\n",
      "Iteration 65849, loss = 1.77144678\n",
      "Iteration 65850, loss = 2.10771723\n",
      "Iteration 65851, loss = 1.55460154\n",
      "Iteration 65852, loss = 1.59461243\n",
      "Iteration 65853, loss = 1.65118835\n",
      "Iteration 65854, loss = 1.40286385\n",
      "Iteration 65855, loss = 1.65200717\n",
      "Iteration 65856, loss = 1.83060384\n",
      "Iteration 65857, loss = 3.28612542\n",
      "Iteration 65858, loss = 2.68371381\n",
      "Iteration 65859, loss = 2.63361636\n",
      "Iteration 65860, loss = 2.53544698\n",
      "Iteration 65861, loss = 2.32059548\n",
      "Iteration 65862, loss = 2.13638238\n",
      "Iteration 65863, loss = 1.38875266\n",
      "Iteration 65864, loss = 1.46028262\n",
      "Iteration 65865, loss = 1.59196075\n",
      "Iteration 65866, loss = 1.60854389\n",
      "Iteration 65867, loss = 1.36047277\n",
      "Iteration 65868, loss = 1.33315434\n",
      "Iteration 65869, loss = 1.27854846\n",
      "Iteration 65870, loss = 1.14680239\n",
      "Iteration 65871, loss = 1.14560659\n",
      "Iteration 65872, loss = 1.12803533\n",
      "Iteration 65873, loss = 1.10649232\n",
      "Iteration 65874, loss = 1.27162965\n",
      "Iteration 65875, loss = 1.16946681\n",
      "Iteration 65876, loss = 1.12461749\n",
      "Iteration 65877, loss = 1.13322119\n",
      "Iteration 65878, loss = 1.13103530\n",
      "Iteration 65879, loss = 1.45615591\n",
      "Iteration 65880, loss = 2.32643198\n",
      "Iteration 65881, loss = 2.21177453\n",
      "Iteration 65882, loss = 1.93809054\n",
      "Iteration 65883, loss = 1.54090038\n",
      "Iteration 65884, loss = 1.29370537\n",
      "Iteration 65885, loss = 1.30761792\n",
      "Iteration 65886, loss = 1.21442614\n",
      "Iteration 65887, loss = 1.25090982\n",
      "Iteration 65888, loss = 1.29065409\n",
      "Iteration 65889, loss = 1.15316915\n",
      "Iteration 65890, loss = 1.16427971\n",
      "Iteration 65891, loss = 1.17513511\n",
      "Iteration 65892, loss = 1.70581786\n",
      "Iteration 65893, loss = 1.40573499\n",
      "Iteration 65894, loss = 1.25548874\n",
      "Iteration 65895, loss = 1.26238460\n",
      "Iteration 65896, loss = 1.52465930\n",
      "Iteration 65897, loss = 1.72680583\n",
      "Iteration 65898, loss = 2.67073813\n",
      "Iteration 65899, loss = 2.59889638\n",
      "Iteration 65900, loss = 2.24340539\n",
      "Iteration 65901, loss = 1.39399483\n",
      "Iteration 65902, loss = 1.67101182\n",
      "Iteration 65903, loss = 1.40835426\n",
      "Iteration 65904, loss = 1.47380418\n",
      "Iteration 65905, loss = 1.33374859\n",
      "Iteration 65906, loss = 1.26591032\n",
      "Iteration 65907, loss = 1.26886421\n",
      "Iteration 65908, loss = 1.17712049\n",
      "Iteration 65909, loss = 1.37513607\n",
      "Iteration 65910, loss = 1.44173991\n",
      "Iteration 65911, loss = 1.39710774\n",
      "Iteration 65912, loss = 1.33420988\n",
      "Iteration 65913, loss = 1.16570290\n",
      "Iteration 65914, loss = 1.20954446\n",
      "Iteration 65915, loss = 1.31507688\n",
      "Iteration 65916, loss = 1.20457253\n",
      "Iteration 65917, loss = 1.45212704\n",
      "Iteration 65918, loss = 1.34269831\n",
      "Iteration 65919, loss = 1.43954960\n",
      "Iteration 65920, loss = 1.43151098\n",
      "Iteration 65921, loss = 1.80468145\n",
      "Iteration 65922, loss = 1.47445190\n",
      "Iteration 65923, loss = 1.40731717\n",
      "Iteration 65924, loss = 1.29242905\n",
      "Iteration 65925, loss = 1.37088658\n",
      "Iteration 65926, loss = 1.52164698\n",
      "Iteration 65927, loss = 1.22386591\n",
      "Iteration 65928, loss = 1.22244889\n",
      "Iteration 65929, loss = 1.17537702\n",
      "Iteration 65930, loss = 1.44547229\n",
      "Iteration 65931, loss = 1.54258670\n",
      "Iteration 65932, loss = 1.69678629\n",
      "Iteration 65933, loss = 1.42459856\n",
      "Iteration 65934, loss = 1.19522552\n",
      "Iteration 65935, loss = 1.15818417\n",
      "Iteration 65936, loss = 1.10111786\n",
      "Iteration 65937, loss = 1.18292054\n",
      "Iteration 65938, loss = 1.35312271\n",
      "Iteration 65939, loss = 1.43034381\n",
      "Iteration 65940, loss = 1.58658550\n",
      "Iteration 65941, loss = 1.67674881\n",
      "Iteration 65942, loss = 1.59796600\n",
      "Iteration 65943, loss = 2.36083595\n",
      "Iteration 65944, loss = 2.39186842\n",
      "Iteration 65945, loss = 2.28096201\n",
      "Iteration 65946, loss = 1.98902030\n",
      "Iteration 65947, loss = 1.82782368\n",
      "Iteration 65948, loss = 1.52603522\n",
      "Iteration 65949, loss = 1.40874381\n",
      "Iteration 65950, loss = 1.45224118\n",
      "Iteration 65951, loss = 1.57644213\n",
      "Iteration 65952, loss = 1.70806749\n",
      "Iteration 65953, loss = 1.21330232\n",
      "Iteration 65954, loss = 1.25953890\n",
      "Iteration 65955, loss = 1.18481648\n",
      "Iteration 65956, loss = 1.12813525\n",
      "Iteration 65957, loss = 1.50558648\n",
      "Iteration 65958, loss = 1.66209094\n",
      "Iteration 65959, loss = 1.93767453\n",
      "Iteration 65960, loss = 3.14491841\n",
      "Iteration 65961, loss = 3.06097935\n",
      "Iteration 65962, loss = 2.15702228\n",
      "Iteration 65963, loss = 1.95872122\n",
      "Iteration 65964, loss = 1.74847092\n",
      "Iteration 65965, loss = 1.93959278\n",
      "Iteration 65966, loss = 1.94489521\n",
      "Iteration 65967, loss = 1.51443406\n",
      "Iteration 65968, loss = 1.29996216\n",
      "Iteration 65969, loss = 1.27993447\n",
      "Iteration 65970, loss = 1.71959019\n",
      "Iteration 65971, loss = 1.76066280\n",
      "Iteration 65972, loss = 1.54487488\n",
      "Iteration 65973, loss = 1.31274163\n",
      "Iteration 65974, loss = 1.51602332\n",
      "Iteration 65975, loss = 1.97821383\n",
      "Iteration 65976, loss = 1.63832254\n",
      "Iteration 65977, loss = 1.55609837\n",
      "Iteration 65978, loss = 1.33110615\n",
      "Iteration 65979, loss = 1.84718894\n",
      "Iteration 65980, loss = 1.80408502\n",
      "Iteration 65981, loss = 2.03720079\n",
      "Iteration 65982, loss = 1.71871857\n",
      "Iteration 65983, loss = 1.43825678\n",
      "Iteration 65984, loss = 1.28630698\n",
      "Iteration 65985, loss = 1.46621404\n",
      "Iteration 65986, loss = 1.41358816\n",
      "Iteration 65987, loss = 1.38373204\n",
      "Iteration 65988, loss = 1.45102161\n",
      "Iteration 65989, loss = 1.28803207\n",
      "Iteration 65990, loss = 1.37270699\n",
      "Iteration 65991, loss = 1.40529600\n",
      "Iteration 65992, loss = 1.92051365\n",
      "Iteration 65993, loss = 1.63688035\n",
      "Iteration 65994, loss = 1.77819597\n",
      "Iteration 65995, loss = 1.51674195\n",
      "Iteration 65996, loss = 1.52437074\n",
      "Iteration 65997, loss = 1.39394795\n",
      "Iteration 65998, loss = 1.29082114\n",
      "Iteration 65999, loss = 1.43091090\n",
      "Iteration 66000, loss = 1.34041867\n",
      "Iteration 66001, loss = 1.25532842\n",
      "Iteration 66002, loss = 1.02840670\n",
      "Iteration 66003, loss = 1.14949420\n",
      "Iteration 66004, loss = 1.08708683\n",
      "Iteration 66005, loss = 1.37299664\n",
      "Iteration 66006, loss = 2.08006269\n",
      "Iteration 66007, loss = 2.09450134\n",
      "Iteration 66008, loss = 2.19393805\n",
      "Iteration 66009, loss = 1.62385410\n",
      "Iteration 66010, loss = 1.63961022\n",
      "Iteration 66011, loss = 1.52875589\n",
      "Iteration 66012, loss = 1.61576582\n",
      "Iteration 66013, loss = 1.55959007\n",
      "Iteration 66014, loss = 1.45069087\n",
      "Iteration 66015, loss = 1.38058887\n",
      "Iteration 66016, loss = 1.29765341\n",
      "Iteration 66017, loss = 1.51584604\n",
      "Iteration 66018, loss = 1.28937937\n",
      "Iteration 66019, loss = 1.15853535\n",
      "Iteration 66020, loss = 1.16121096\n",
      "Iteration 66021, loss = 1.34048132\n",
      "Iteration 66022, loss = 1.56174403\n",
      "Iteration 66023, loss = 1.38166880\n",
      "Iteration 66024, loss = 1.41883532\n",
      "Iteration 66025, loss = 1.35656492\n",
      "Iteration 66026, loss = 1.37875379\n",
      "Iteration 66027, loss = 1.18238499\n",
      "Iteration 66028, loss = 1.26630444\n",
      "Iteration 66029, loss = 1.24123394\n",
      "Iteration 66030, loss = 1.18882794\n",
      "Iteration 66031, loss = 1.48368775\n",
      "Iteration 66032, loss = 1.67569429\n",
      "Iteration 66033, loss = 1.36438124\n",
      "Iteration 66034, loss = 1.30229741\n",
      "Iteration 66035, loss = 1.27219888\n",
      "Iteration 66036, loss = 1.22665159\n",
      "Iteration 66037, loss = 1.33153141\n",
      "Iteration 66038, loss = 1.53080195\n",
      "Iteration 66039, loss = 1.52379855\n",
      "Iteration 66040, loss = 1.47440017\n",
      "Iteration 66041, loss = 1.40299161\n",
      "Iteration 66042, loss = 1.41867204\n",
      "Iteration 66043, loss = 1.29438270\n",
      "Iteration 66044, loss = 1.56395474\n",
      "Iteration 66045, loss = 1.49152755\n",
      "Iteration 66046, loss = 1.66928624\n",
      "Iteration 66047, loss = 1.59741428\n",
      "Iteration 66048, loss = 1.46193756\n",
      "Iteration 66049, loss = 1.69842731\n",
      "Iteration 66050, loss = 1.59892203\n",
      "Iteration 66051, loss = 2.09695431\n",
      "Iteration 66052, loss = 1.68474417\n",
      "Iteration 66053, loss = 1.48002707\n",
      "Iteration 66054, loss = 1.65991793\n",
      "Iteration 66055, loss = 1.63464865\n",
      "Iteration 66056, loss = 1.80471851\n",
      "Iteration 66057, loss = 1.51420506\n",
      "Iteration 66058, loss = 1.47310197\n",
      "Iteration 66059, loss = 1.46103071\n",
      "Iteration 66060, loss = 1.49058851\n",
      "Iteration 66061, loss = 1.47784147\n",
      "Iteration 66062, loss = 1.65008368\n",
      "Iteration 66063, loss = 1.55961774\n",
      "Iteration 66064, loss = 1.39477175\n",
      "Iteration 66065, loss = 1.29852732\n",
      "Iteration 66066, loss = 1.35520008\n",
      "Iteration 66067, loss = 1.37154471\n",
      "Iteration 66068, loss = 1.75540547\n",
      "Iteration 66069, loss = 1.76843435\n",
      "Iteration 66070, loss = 1.53389936\n",
      "Iteration 66071, loss = 1.18904077\n",
      "Iteration 66072, loss = 1.28161764\n",
      "Iteration 66073, loss = 1.40422629\n",
      "Iteration 66074, loss = 1.56047225\n",
      "Iteration 66075, loss = 1.56926184\n",
      "Iteration 66076, loss = 1.53731732\n",
      "Iteration 66077, loss = 1.80377935\n",
      "Iteration 66078, loss = 1.50843079\n",
      "Iteration 66079, loss = 1.21145268\n",
      "Iteration 66080, loss = 1.21025229\n",
      "Iteration 66081, loss = 1.29566459\n",
      "Iteration 66082, loss = 1.36260314\n",
      "Iteration 66083, loss = 1.36744913\n",
      "Iteration 66084, loss = 1.65165568\n",
      "Iteration 66085, loss = 1.66722645\n",
      "Iteration 66086, loss = 1.19978545\n",
      "Iteration 66087, loss = 1.20017457\n",
      "Iteration 66088, loss = 1.27734865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 66089, loss = 1.23063647\n",
      "Iteration 66090, loss = 1.21230457\n",
      "Iteration 66091, loss = 1.11362290\n",
      "Iteration 66092, loss = 1.17607959\n",
      "Iteration 66093, loss = 1.40089244\n",
      "Iteration 66094, loss = 1.55771881\n",
      "Iteration 66095, loss = 1.60623539\n",
      "Iteration 66096, loss = 1.49816687\n",
      "Iteration 66097, loss = 1.13729164\n",
      "Iteration 66098, loss = 1.11734528\n",
      "Iteration 66099, loss = 1.19961956\n",
      "Iteration 66100, loss = 1.17257636\n",
      "Iteration 66101, loss = 1.22283735\n",
      "Iteration 66102, loss = 1.22284353\n",
      "Iteration 66103, loss = 1.08409051\n",
      "Iteration 66104, loss = 1.16505866\n",
      "Iteration 66105, loss = 1.20580112\n",
      "Iteration 66106, loss = 1.24864613\n",
      "Iteration 66107, loss = 1.08313285\n",
      "Iteration 66108, loss = 1.40874848\n",
      "Iteration 66109, loss = 1.52145429\n",
      "Iteration 66110, loss = 1.72431762\n",
      "Iteration 66111, loss = 1.55848695\n",
      "Iteration 66112, loss = 1.94653043\n",
      "Iteration 66113, loss = 1.40906868\n",
      "Iteration 66114, loss = 1.49582370\n",
      "Iteration 66115, loss = 1.90049595\n",
      "Iteration 66116, loss = 1.72412344\n",
      "Iteration 66117, loss = 1.27739860\n",
      "Iteration 66118, loss = 1.20298045\n",
      "Iteration 66119, loss = 1.34533136\n",
      "Iteration 66120, loss = 1.19414912\n",
      "Iteration 66121, loss = 1.13288403\n",
      "Iteration 66122, loss = 1.13441602\n",
      "Iteration 66123, loss = 1.12700281\n",
      "Iteration 66124, loss = 1.16618913\n",
      "Iteration 66125, loss = 1.29541021\n",
      "Iteration 66126, loss = 1.34204212\n",
      "Iteration 66127, loss = 1.28212745\n",
      "Iteration 66128, loss = 1.33107628\n",
      "Iteration 66129, loss = 1.43647514\n",
      "Iteration 66130, loss = 1.36696900\n",
      "Iteration 66131, loss = 1.53302027\n",
      "Iteration 66132, loss = 1.42775883\n",
      "Iteration 66133, loss = 1.34417162\n",
      "Iteration 66134, loss = 1.34456170\n",
      "Iteration 66135, loss = 1.46275516\n",
      "Iteration 66136, loss = 1.18142024\n",
      "Iteration 66137, loss = 1.15358567\n",
      "Iteration 66138, loss = 1.20315956\n",
      "Iteration 66139, loss = 1.26560661\n",
      "Iteration 66140, loss = 1.20072225\n",
      "Iteration 66141, loss = 1.28618212\n",
      "Iteration 66142, loss = 1.25482576\n",
      "Iteration 66143, loss = 1.48297398\n",
      "Iteration 66144, loss = 1.82491336\n",
      "Iteration 66145, loss = 2.20218831\n",
      "Iteration 66146, loss = 1.67951079\n",
      "Iteration 66147, loss = 2.10547477\n",
      "Iteration 66148, loss = 1.74805240\n",
      "Iteration 66149, loss = 2.11631065\n",
      "Iteration 66150, loss = 1.39105324\n",
      "Iteration 66151, loss = 1.25726451\n",
      "Iteration 66152, loss = 1.55792338\n",
      "Iteration 66153, loss = 1.33576010\n",
      "Iteration 66154, loss = 1.52874236\n",
      "Iteration 66155, loss = 1.40237729\n",
      "Iteration 66156, loss = 1.39356533\n",
      "Iteration 66157, loss = 2.00919966\n",
      "Iteration 66158, loss = 2.53379438\n",
      "Iteration 66159, loss = 1.87687080\n",
      "Iteration 66160, loss = 1.51757208\n",
      "Iteration 66161, loss = 1.72860418\n",
      "Iteration 66162, loss = 1.74147849\n",
      "Iteration 66163, loss = 1.48196021\n",
      "Iteration 66164, loss = 1.92253122\n",
      "Iteration 66165, loss = 2.59210889\n",
      "Iteration 66166, loss = 2.99601846\n",
      "Iteration 66167, loss = 2.21160602\n",
      "Iteration 66168, loss = 1.74223048\n",
      "Iteration 66169, loss = 1.40868716\n",
      "Iteration 66170, loss = 1.39849651\n",
      "Iteration 66171, loss = 1.39057923\n",
      "Iteration 66172, loss = 1.36314747\n",
      "Iteration 66173, loss = 1.44027983\n",
      "Iteration 66174, loss = 1.56894519\n",
      "Iteration 66175, loss = 1.26050332\n",
      "Iteration 66176, loss = 1.25665134\n",
      "Iteration 66177, loss = 1.16226210\n",
      "Iteration 66178, loss = 1.26297617\n",
      "Iteration 66179, loss = 1.31142386\n",
      "Iteration 66180, loss = 1.23935146\n",
      "Iteration 66181, loss = 1.21088872\n",
      "Iteration 66182, loss = 1.65398214\n",
      "Iteration 66183, loss = 1.44537945\n",
      "Iteration 66184, loss = 1.34683543\n",
      "Iteration 66185, loss = 1.50952125\n",
      "Iteration 66186, loss = 1.14653252\n",
      "Iteration 66187, loss = 1.20494327\n",
      "Iteration 66188, loss = 1.19102679\n",
      "Iteration 66189, loss = 1.07568600\n",
      "Iteration 66190, loss = 1.19367530\n",
      "Iteration 66191, loss = 1.20840115\n",
      "Iteration 66192, loss = 1.28969483\n",
      "Iteration 66193, loss = 1.24128827\n",
      "Iteration 66194, loss = 1.52070745\n",
      "Iteration 66195, loss = 1.34876586\n",
      "Iteration 66196, loss = 1.31617372\n",
      "Iteration 66197, loss = 1.42172179\n",
      "Iteration 66198, loss = 1.17353132\n",
      "Iteration 66199, loss = 1.39358880\n",
      "Iteration 66200, loss = 1.52171868\n",
      "Iteration 66201, loss = 1.40812866\n",
      "Iteration 66202, loss = 1.36376390\n",
      "Iteration 66203, loss = 1.17181120\n",
      "Iteration 66204, loss = 1.38413786\n",
      "Iteration 66205, loss = 1.42876815\n",
      "Iteration 66206, loss = 1.61982059\n",
      "Iteration 66207, loss = 1.65287859\n",
      "Iteration 66208, loss = 1.61809560\n",
      "Iteration 66209, loss = 1.52928063\n",
      "Iteration 66210, loss = 1.78346883\n",
      "Iteration 66211, loss = 1.50650510\n",
      "Iteration 66212, loss = 1.12213970\n",
      "Iteration 66213, loss = 1.25150565\n",
      "Iteration 66214, loss = 1.36153821\n",
      "Iteration 66215, loss = 1.52856643\n",
      "Iteration 66216, loss = 1.55881679\n",
      "Iteration 66217, loss = 1.37248707\n",
      "Iteration 66218, loss = 1.30039818\n",
      "Iteration 66219, loss = 1.27169318\n",
      "Iteration 66220, loss = 1.38357931\n",
      "Iteration 66221, loss = 1.46979250\n",
      "Iteration 66222, loss = 1.42752784\n",
      "Iteration 66223, loss = 1.48790191\n",
      "Iteration 66224, loss = 1.57371951\n",
      "Iteration 66225, loss = 1.25004927\n",
      "Iteration 66226, loss = 1.22333504\n",
      "Iteration 66227, loss = 1.15247254\n",
      "Iteration 66228, loss = 1.27241045\n",
      "Iteration 66229, loss = 1.36871192\n",
      "Iteration 66230, loss = 1.21537888\n",
      "Iteration 66231, loss = 1.23207690\n",
      "Iteration 66232, loss = 1.18698622\n",
      "Iteration 66233, loss = 1.32130031\n",
      "Iteration 66234, loss = 1.14697704\n",
      "Iteration 66235, loss = 1.78701578\n",
      "Iteration 66236, loss = 2.04073871\n",
      "Iteration 66237, loss = 1.57964156\n",
      "Iteration 66238, loss = 1.53512156\n",
      "Iteration 66239, loss = 1.55774916\n",
      "Iteration 66240, loss = 1.23427237\n",
      "Iteration 66241, loss = 1.20812285\n",
      "Iteration 66242, loss = 1.33710435\n",
      "Iteration 66243, loss = 1.44287455\n",
      "Iteration 66244, loss = 1.68624586\n",
      "Iteration 66245, loss = 1.82871990\n",
      "Iteration 66246, loss = 1.84241425\n",
      "Iteration 66247, loss = 1.52869414\n",
      "Iteration 66248, loss = 1.23830056\n",
      "Iteration 66249, loss = 1.39737877\n",
      "Iteration 66250, loss = 1.64012880\n",
      "Iteration 66251, loss = 1.82393073\n",
      "Iteration 66252, loss = 1.84275120\n",
      "Iteration 66253, loss = 1.63586318\n",
      "Iteration 66254, loss = 1.97185683\n",
      "Iteration 66255, loss = 1.73710132\n",
      "Iteration 66256, loss = 1.73967735\n",
      "Iteration 66257, loss = 1.32211293\n",
      "Iteration 66258, loss = 1.74505941\n",
      "Iteration 66259, loss = 1.90833089\n",
      "Iteration 66260, loss = 1.48124749\n",
      "Iteration 66261, loss = 1.27251499\n",
      "Iteration 66262, loss = 1.25103731\n",
      "Iteration 66263, loss = 1.30107141\n",
      "Iteration 66264, loss = 1.68371397\n",
      "Iteration 66265, loss = 1.70032644\n",
      "Iteration 66266, loss = 1.62338303\n",
      "Iteration 66267, loss = 1.43162049\n",
      "Iteration 66268, loss = 1.30042357\n",
      "Iteration 66269, loss = 1.60732678\n",
      "Iteration 66270, loss = 1.41196753\n",
      "Iteration 66271, loss = 1.76339697\n",
      "Iteration 66272, loss = 1.62228941\n",
      "Iteration 66273, loss = 1.47217790\n",
      "Iteration 66274, loss = 1.19308903\n",
      "Iteration 66275, loss = 1.25052471\n",
      "Iteration 66276, loss = 1.22723525\n",
      "Iteration 66277, loss = 1.35172510\n",
      "Iteration 66278, loss = 1.35020355\n",
      "Iteration 66279, loss = 1.63266315\n",
      "Iteration 66280, loss = 1.45760584\n",
      "Iteration 66281, loss = 1.34565242\n",
      "Iteration 66282, loss = 1.17487043\n",
      "Iteration 66283, loss = 1.61713549\n",
      "Iteration 66284, loss = 1.54479742\n",
      "Iteration 66285, loss = 1.39043485\n",
      "Iteration 66286, loss = 1.16464549\n",
      "Iteration 66287, loss = 1.26195134\n",
      "Iteration 66288, loss = 1.47505724\n",
      "Iteration 66289, loss = 1.82339956\n",
      "Iteration 66290, loss = 1.82266627\n",
      "Iteration 66291, loss = 1.84366380\n",
      "Iteration 66292, loss = 1.88089327\n",
      "Iteration 66293, loss = 2.30092750\n",
      "Iteration 66294, loss = 1.83946697\n",
      "Iteration 66295, loss = 1.36352769\n",
      "Iteration 66296, loss = 1.43575190\n",
      "Iteration 66297, loss = 1.35116841\n",
      "Iteration 66298, loss = 1.25206130\n",
      "Iteration 66299, loss = 1.39392178\n",
      "Iteration 66300, loss = 1.54033243\n",
      "Iteration 66301, loss = 1.51203990\n",
      "Iteration 66302, loss = 1.34041608\n",
      "Iteration 66303, loss = 1.33729979\n",
      "Iteration 66304, loss = 1.50374190\n",
      "Iteration 66305, loss = 1.34379088\n",
      "Iteration 66306, loss = 1.33341163\n",
      "Iteration 66307, loss = 1.39114260\n",
      "Iteration 66308, loss = 1.21091969\n",
      "Iteration 66309, loss = 1.29368198\n",
      "Iteration 66310, loss = 1.07191316\n",
      "Iteration 66311, loss = 1.03847598\n",
      "Iteration 66312, loss = 1.10660055\n",
      "Iteration 66313, loss = 1.18243706\n",
      "Iteration 66314, loss = 1.13182056\n",
      "Iteration 66315, loss = 1.13759059\n",
      "Iteration 66316, loss = 1.25192921\n",
      "Iteration 66317, loss = 1.18886867\n",
      "Iteration 66318, loss = 1.45867285\n",
      "Iteration 66319, loss = 2.35454244\n",
      "Iteration 66320, loss = 2.02506520\n",
      "Iteration 66321, loss = 1.98567777\n",
      "Iteration 66322, loss = 1.45512347\n",
      "Iteration 66323, loss = 1.20748797\n",
      "Iteration 66324, loss = 1.16954952\n",
      "Iteration 66325, loss = 1.25377929\n",
      "Iteration 66326, loss = 1.06612019\n",
      "Iteration 66327, loss = 1.54195529\n",
      "Iteration 66328, loss = 1.45516484\n",
      "Iteration 66329, loss = 1.46012589\n",
      "Iteration 66330, loss = 1.32363035\n",
      "Iteration 66331, loss = 1.53486418\n",
      "Iteration 66332, loss = 1.38361077\n",
      "Iteration 66333, loss = 1.27351887\n",
      "Iteration 66334, loss = 1.22455200\n",
      "Iteration 66335, loss = 1.20881171\n",
      "Iteration 66336, loss = 1.20562027\n",
      "Iteration 66337, loss = 1.18075902\n",
      "Iteration 66338, loss = 1.10683808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 66339, loss = 1.17215772\n",
      "Iteration 66340, loss = 1.47232384\n",
      "Iteration 66341, loss = 1.34974604\n",
      "Iteration 66342, loss = 1.91204101\n",
      "Iteration 66343, loss = 1.40745196\n",
      "Iteration 66344, loss = 1.38927054\n",
      "Iteration 66345, loss = 1.21813093\n",
      "Iteration 66346, loss = 1.12433512\n",
      "Iteration 66347, loss = 1.22371722\n",
      "Iteration 66348, loss = 1.35727470\n",
      "Iteration 66349, loss = 1.52534288\n",
      "Iteration 66350, loss = 1.60749093\n",
      "Iteration 66351, loss = 1.41565643\n",
      "Iteration 66352, loss = 1.46425374\n",
      "Iteration 66353, loss = 1.48043111\n",
      "Iteration 66354, loss = 1.26232584\n",
      "Iteration 66355, loss = 1.32146233\n",
      "Iteration 66356, loss = 1.27943814\n",
      "Iteration 66357, loss = 1.44493635\n",
      "Iteration 66358, loss = 1.24509782\n",
      "Iteration 66359, loss = 1.59103284\n",
      "Iteration 66360, loss = 1.36205527\n",
      "Iteration 66361, loss = 1.51917886\n",
      "Iteration 66362, loss = 1.59465197\n",
      "Iteration 66363, loss = 1.52952881\n",
      "Iteration 66364, loss = 1.64857542\n",
      "Iteration 66365, loss = 1.34676589\n",
      "Iteration 66366, loss = 1.27516373\n",
      "Iteration 66367, loss = 1.30263326\n",
      "Iteration 66368, loss = 1.25359810\n",
      "Iteration 66369, loss = 1.27019250\n",
      "Iteration 66370, loss = 1.28367315\n",
      "Iteration 66371, loss = 1.40214177\n",
      "Iteration 66372, loss = 1.33426002\n",
      "Iteration 66373, loss = 1.12618303\n",
      "Iteration 66374, loss = 1.24503083\n",
      "Iteration 66375, loss = 1.50245192\n",
      "Iteration 66376, loss = 1.48807425\n",
      "Iteration 66377, loss = 1.37245817\n",
      "Iteration 66378, loss = 1.70774405\n",
      "Iteration 66379, loss = 1.49306058\n",
      "Iteration 66380, loss = 1.45760490\n",
      "Iteration 66381, loss = 1.36114328\n",
      "Iteration 66382, loss = 1.41091773\n",
      "Iteration 66383, loss = 1.30948888\n",
      "Iteration 66384, loss = 1.35631119\n",
      "Iteration 66385, loss = 1.44264952\n",
      "Iteration 66386, loss = 1.69193901\n",
      "Iteration 66387, loss = 1.47013333\n",
      "Iteration 66388, loss = 1.33375009\n",
      "Iteration 66389, loss = 1.37800543\n",
      "Iteration 66390, loss = 1.35452849\n",
      "Iteration 66391, loss = 1.52982359\n",
      "Iteration 66392, loss = 1.21185043\n",
      "Iteration 66393, loss = 1.58011610\n",
      "Iteration 66394, loss = 1.51492939\n",
      "Iteration 66395, loss = 1.56808337\n",
      "Iteration 66396, loss = 1.84870405\n",
      "Iteration 66397, loss = 1.82889924\n",
      "Iteration 66398, loss = 1.79786959\n",
      "Iteration 66399, loss = 1.57378045\n",
      "Iteration 66400, loss = 1.57740684\n",
      "Iteration 66401, loss = 1.73364156\n",
      "Iteration 66402, loss = 2.02980379\n",
      "Iteration 66403, loss = 1.45389952\n",
      "Iteration 66404, loss = 1.48007033\n",
      "Iteration 66405, loss = 1.28007268\n",
      "Iteration 66406, loss = 1.14442214\n",
      "Iteration 66407, loss = 1.21054665\n",
      "Iteration 66408, loss = 1.30272751\n",
      "Iteration 66409, loss = 1.25468995\n",
      "Iteration 66410, loss = 1.59312228\n",
      "Iteration 66411, loss = 1.65003860\n",
      "Iteration 66412, loss = 1.60758872\n",
      "Iteration 66413, loss = 1.32769047\n",
      "Iteration 66414, loss = 1.86712088\n",
      "Iteration 66415, loss = 1.51818822\n",
      "Iteration 66416, loss = 1.61409305\n",
      "Iteration 66417, loss = 1.73255921\n",
      "Iteration 66418, loss = 1.70252881\n",
      "Iteration 66419, loss = 2.14471779\n",
      "Iteration 66420, loss = 2.86272370\n",
      "Iteration 66421, loss = 2.97270599\n",
      "Iteration 66422, loss = 3.07054361\n",
      "Iteration 66423, loss = 2.39224338\n",
      "Iteration 66424, loss = 1.92839097\n",
      "Iteration 66425, loss = 2.32292771\n",
      "Iteration 66426, loss = 1.98886844\n",
      "Iteration 66427, loss = 1.37707487\n",
      "Iteration 66428, loss = 1.21658109\n",
      "Iteration 66429, loss = 1.21141159\n",
      "Iteration 66430, loss = 1.20016444\n",
      "Iteration 66431, loss = 1.27592074\n",
      "Iteration 66432, loss = 1.37826543\n",
      "Iteration 66433, loss = 1.44037238\n",
      "Iteration 66434, loss = 1.64738168\n",
      "Iteration 66435, loss = 1.48432155\n",
      "Iteration 66436, loss = 1.34043468\n",
      "Iteration 66437, loss = 1.28574549\n",
      "Iteration 66438, loss = 1.30030137\n",
      "Iteration 66439, loss = 1.47506522\n",
      "Iteration 66440, loss = 1.23361819\n",
      "Iteration 66441, loss = 1.22303506\n",
      "Iteration 66442, loss = 1.12892295\n",
      "Iteration 66443, loss = 1.12737755\n",
      "Iteration 66444, loss = 1.15916413\n",
      "Iteration 66445, loss = 1.24675287\n",
      "Iteration 66446, loss = 1.20383411\n",
      "Iteration 66447, loss = 1.16268343\n",
      "Iteration 66448, loss = 1.09163091\n",
      "Iteration 66449, loss = 1.20165357\n",
      "Iteration 66450, loss = 1.32128946\n",
      "Iteration 66451, loss = 1.20964998\n",
      "Iteration 66452, loss = 1.07364752\n",
      "Iteration 66453, loss = 1.12209271\n",
      "Iteration 66454, loss = 1.24313788\n",
      "Iteration 66455, loss = 1.41175320\n",
      "Iteration 66456, loss = 1.51356429\n",
      "Iteration 66457, loss = 1.67063123\n",
      "Iteration 66458, loss = 1.39767711\n",
      "Iteration 66459, loss = 1.36567660\n",
      "Iteration 66460, loss = 1.63718412\n",
      "Iteration 66461, loss = 1.96203756\n",
      "Iteration 66462, loss = 1.75791259\n",
      "Iteration 66463, loss = 1.58449163\n",
      "Iteration 66464, loss = 2.17227797\n",
      "Iteration 66465, loss = 3.32002316\n",
      "Iteration 66466, loss = 3.62193343\n",
      "Iteration 66467, loss = 3.76203863\n",
      "Iteration 66468, loss = 3.90363853\n",
      "Iteration 66469, loss = 3.20693006\n",
      "Iteration 66470, loss = 2.33804606\n",
      "Iteration 66471, loss = 1.69653339\n",
      "Iteration 66472, loss = 1.71287484\n",
      "Iteration 66473, loss = 1.63206498\n",
      "Iteration 66474, loss = 1.71504099\n",
      "Iteration 66475, loss = 1.35877899\n",
      "Iteration 66476, loss = 1.62511245\n",
      "Iteration 66477, loss = 1.43396139\n",
      "Iteration 66478, loss = 1.26463229\n",
      "Iteration 66479, loss = 1.21450710\n",
      "Iteration 66480, loss = 1.14420244\n",
      "Iteration 66481, loss = 1.15573672\n",
      "Iteration 66482, loss = 1.28419986\n",
      "Iteration 66483, loss = 1.08788906\n",
      "Iteration 66484, loss = 1.08964235\n",
      "Iteration 66485, loss = 1.07198843\n",
      "Iteration 66486, loss = 1.15288639\n",
      "Iteration 66487, loss = 1.15582186\n",
      "Iteration 66488, loss = 1.34386721\n",
      "Iteration 66489, loss = 1.21479985\n",
      "Iteration 66490, loss = 1.63236372\n",
      "Iteration 66491, loss = 1.42105859\n",
      "Iteration 66492, loss = 1.56080160\n",
      "Iteration 66493, loss = 1.35373998\n",
      "Iteration 66494, loss = 1.76726387\n",
      "Iteration 66495, loss = 1.86331722\n",
      "Iteration 66496, loss = 1.71264751\n",
      "Iteration 66497, loss = 2.53998146\n",
      "Iteration 66498, loss = 2.98072937\n",
      "Iteration 66499, loss = 1.88536902\n",
      "Iteration 66500, loss = 1.72824872\n",
      "Iteration 66501, loss = 1.27818462\n",
      "Iteration 66502, loss = 1.15132574\n",
      "Iteration 66503, loss = 1.23889202\n",
      "Iteration 66504, loss = 1.21641268\n",
      "Iteration 66505, loss = 1.19365157\n",
      "Iteration 66506, loss = 1.62578042\n",
      "Iteration 66507, loss = 1.45546142\n",
      "Iteration 66508, loss = 1.32145310\n",
      "Iteration 66509, loss = 1.99278743\n",
      "Iteration 66510, loss = 2.05510696\n",
      "Iteration 66511, loss = 1.68781183\n",
      "Iteration 66512, loss = 1.54996657\n",
      "Iteration 66513, loss = 1.34028788\n",
      "Iteration 66514, loss = 1.53281812\n",
      "Iteration 66515, loss = 1.38653387\n",
      "Iteration 66516, loss = 1.39334342\n",
      "Iteration 66517, loss = 1.36288962\n",
      "Iteration 66518, loss = 1.33567902\n",
      "Iteration 66519, loss = 1.36501483\n",
      "Iteration 66520, loss = 1.24807632\n",
      "Iteration 66521, loss = 1.27804644\n",
      "Iteration 66522, loss = 1.19048508\n",
      "Iteration 66523, loss = 1.44913590\n",
      "Iteration 66524, loss = 1.67277714\n",
      "Iteration 66525, loss = 1.59993603\n",
      "Iteration 66526, loss = 1.61398672\n",
      "Iteration 66527, loss = 1.87521546\n",
      "Iteration 66528, loss = 1.61478602\n",
      "Iteration 66529, loss = 1.53425249\n",
      "Iteration 66530, loss = 1.26729288\n",
      "Iteration 66531, loss = 1.16590547\n",
      "Iteration 66532, loss = 1.26098103\n",
      "Iteration 66533, loss = 1.24052994\n",
      "Iteration 66534, loss = 1.20200450\n",
      "Iteration 66535, loss = 1.62670913\n",
      "Iteration 66536, loss = 2.26775629\n",
      "Iteration 66537, loss = 1.74853750\n",
      "Iteration 66538, loss = 1.60247346\n",
      "Iteration 66539, loss = 1.39690230\n",
      "Iteration 66540, loss = 1.66611414\n",
      "Iteration 66541, loss = 1.69861665\n",
      "Iteration 66542, loss = 2.17529324\n",
      "Iteration 66543, loss = 1.91098094\n",
      "Iteration 66544, loss = 1.59162838\n",
      "Iteration 66545, loss = 1.38645025\n",
      "Iteration 66546, loss = 1.28306712\n",
      "Iteration 66547, loss = 1.23285598\n",
      "Iteration 66548, loss = 1.10226608\n",
      "Iteration 66549, loss = 1.13631034\n",
      "Iteration 66550, loss = 1.13318445\n",
      "Iteration 66551, loss = 1.27244846\n",
      "Iteration 66552, loss = 1.29418427\n",
      "Iteration 66553, loss = 1.12413749\n",
      "Iteration 66554, loss = 1.11623666\n",
      "Iteration 66555, loss = 1.18380520\n",
      "Iteration 66556, loss = 1.11581829\n",
      "Iteration 66557, loss = 1.24756396\n",
      "Iteration 66558, loss = 1.37534294\n",
      "Iteration 66559, loss = 1.83880981\n",
      "Iteration 66560, loss = 1.74494348\n",
      "Iteration 66561, loss = 1.63907683\n",
      "Iteration 66562, loss = 1.45126457\n",
      "Iteration 66563, loss = 1.40754542\n",
      "Iteration 66564, loss = 1.67542495\n",
      "Iteration 66565, loss = 1.97617693\n",
      "Iteration 66566, loss = 1.85045978\n",
      "Iteration 66567, loss = 1.55526219\n",
      "Iteration 66568, loss = 1.20847268\n",
      "Iteration 66569, loss = 1.66125748\n",
      "Iteration 66570, loss = 1.74020036\n",
      "Iteration 66571, loss = 1.87270771\n",
      "Iteration 66572, loss = 1.37496482\n",
      "Iteration 66573, loss = 1.40305317\n",
      "Iteration 66574, loss = 1.31402828\n",
      "Iteration 66575, loss = 1.39061745\n",
      "Iteration 66576, loss = 1.54843947\n",
      "Iteration 66577, loss = 1.45463685\n",
      "Iteration 66578, loss = 1.45028147\n",
      "Iteration 66579, loss = 1.71714357\n",
      "Iteration 66580, loss = 2.59756878\n",
      "Iteration 66581, loss = 2.98437287\n",
      "Iteration 66582, loss = 3.53470946\n",
      "Iteration 66583, loss = 3.49516194\n",
      "Iteration 66584, loss = 3.60805565\n",
      "Iteration 66585, loss = 4.20917028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 66586, loss = 3.34053268\n",
      "Iteration 66587, loss = 3.71397608\n",
      "Iteration 66588, loss = 2.50901137\n",
      "Iteration 66589, loss = 2.33944488\n",
      "Iteration 66590, loss = 1.63064483\n",
      "Iteration 66591, loss = 1.56376694\n",
      "Iteration 66592, loss = 1.56829986\n",
      "Iteration 66593, loss = 1.47139028\n",
      "Iteration 66594, loss = 1.54058991\n",
      "Iteration 66595, loss = 2.03169652\n",
      "Iteration 66596, loss = 1.64628066\n",
      "Iteration 66597, loss = 1.54290348\n",
      "Iteration 66598, loss = 1.90369106\n",
      "Iteration 66599, loss = 1.53240891\n",
      "Iteration 66600, loss = 1.15265701\n",
      "Iteration 66601, loss = 1.55629439\n",
      "Iteration 66602, loss = 1.67581483\n",
      "Iteration 66603, loss = 1.48047566\n",
      "Iteration 66604, loss = 1.47134942\n",
      "Iteration 66605, loss = 1.24339201\n",
      "Iteration 66606, loss = 1.69788664\n",
      "Iteration 66607, loss = 1.75061301\n",
      "Iteration 66608, loss = 2.42517372\n",
      "Iteration 66609, loss = 1.40693262\n",
      "Iteration 66610, loss = 1.38917456\n",
      "Iteration 66611, loss = 1.33064464\n",
      "Iteration 66612, loss = 1.19836320\n",
      "Iteration 66613, loss = 1.20910361\n",
      "Iteration 66614, loss = 1.18267678\n",
      "Iteration 66615, loss = 1.25419516\n",
      "Iteration 66616, loss = 1.16309403\n",
      "Iteration 66617, loss = 1.21310594\n",
      "Iteration 66618, loss = 1.33726829\n",
      "Iteration 66619, loss = 1.33213078\n",
      "Iteration 66620, loss = 1.21980057\n",
      "Iteration 66621, loss = 1.06689733\n",
      "Iteration 66622, loss = 1.07836747\n",
      "Iteration 66623, loss = 1.11674086\n",
      "Iteration 66624, loss = 1.20888612\n",
      "Iteration 66625, loss = 1.23308305\n",
      "Iteration 66626, loss = 1.32686259\n",
      "Iteration 66627, loss = 1.36568537\n",
      "Iteration 66628, loss = 1.18066705\n",
      "Iteration 66629, loss = 1.28000932\n",
      "Iteration 66630, loss = 1.18655747\n",
      "Iteration 66631, loss = 1.14594723\n",
      "Iteration 66632, loss = 1.24771471\n",
      "Iteration 66633, loss = 1.26165702\n",
      "Iteration 66634, loss = 1.08525924\n",
      "Iteration 66635, loss = 1.12015807\n",
      "Iteration 66636, loss = 1.26918698\n",
      "Iteration 66637, loss = 1.39957087\n",
      "Iteration 66638, loss = 1.32056772\n",
      "Iteration 66639, loss = 1.22154187\n",
      "Iteration 66640, loss = 1.11325781\n",
      "Iteration 66641, loss = 1.24656300\n",
      "Iteration 66642, loss = 1.21329903\n",
      "Iteration 66643, loss = 1.22894046\n",
      "Iteration 66644, loss = 1.16502560\n",
      "Iteration 66645, loss = 1.06334924\n",
      "Iteration 66646, loss = 1.17608696\n",
      "Iteration 66647, loss = 1.30589141\n",
      "Iteration 66648, loss = 1.66354975\n",
      "Iteration 66649, loss = 1.57433543\n",
      "Iteration 66650, loss = 1.43325498\n",
      "Iteration 66651, loss = 1.33375933\n",
      "Iteration 66652, loss = 1.42483137\n",
      "Iteration 66653, loss = 1.30722542\n",
      "Iteration 66654, loss = 1.16431665\n",
      "Iteration 66655, loss = 1.15025592\n",
      "Iteration 66656, loss = 1.04113454\n",
      "Iteration 66657, loss = 1.16026301\n",
      "Iteration 66658, loss = 1.22362900\n",
      "Iteration 66659, loss = 1.06871258\n",
      "Iteration 66660, loss = 1.14883692\n",
      "Iteration 66661, loss = 1.25227416\n",
      "Iteration 66662, loss = 1.58649282\n",
      "Iteration 66663, loss = 1.62360584\n",
      "Iteration 66664, loss = 1.29191312\n",
      "Iteration 66665, loss = 1.37479139\n",
      "Iteration 66666, loss = 1.14694576\n",
      "Iteration 66667, loss = 1.15261687\n",
      "Iteration 66668, loss = 1.18828388\n",
      "Iteration 66669, loss = 1.51824271\n",
      "Iteration 66670, loss = 1.75187749\n",
      "Iteration 66671, loss = 1.49490330\n",
      "Iteration 66672, loss = 1.34045954\n",
      "Iteration 66673, loss = 1.58688658\n",
      "Iteration 66674, loss = 1.27274401\n",
      "Iteration 66675, loss = 1.28000643\n",
      "Iteration 66676, loss = 1.39552024\n",
      "Iteration 66677, loss = 1.21821541\n",
      "Iteration 66678, loss = 1.23775771\n",
      "Iteration 66679, loss = 1.36829132\n",
      "Iteration 66680, loss = 1.28194571\n",
      "Iteration 66681, loss = 1.53034398\n",
      "Iteration 66682, loss = 1.41558807\n",
      "Iteration 66683, loss = 1.42785731\n",
      "Iteration 66684, loss = 1.49553746\n",
      "Iteration 66685, loss = 1.95195083\n",
      "Iteration 66686, loss = 2.55264293\n",
      "Iteration 66687, loss = 1.65408653\n",
      "Iteration 66688, loss = 1.58391817\n",
      "Iteration 66689, loss = 1.66462619\n",
      "Iteration 66690, loss = 1.41337511\n",
      "Iteration 66691, loss = 1.52704433\n",
      "Iteration 66692, loss = 1.37871654\n",
      "Iteration 66693, loss = 1.24628636\n",
      "Iteration 66694, loss = 1.26256516\n",
      "Iteration 66695, loss = 1.20197880\n",
      "Iteration 66696, loss = 1.15658503\n",
      "Iteration 66697, loss = 1.17282250\n",
      "Iteration 66698, loss = 1.30129870\n",
      "Iteration 66699, loss = 1.52461567\n",
      "Iteration 66700, loss = 1.39917365\n",
      "Iteration 66701, loss = 1.73585038\n",
      "Iteration 66702, loss = 2.40600237\n",
      "Iteration 66703, loss = 1.64315095\n",
      "Iteration 66704, loss = 1.61861525\n",
      "Iteration 66705, loss = 1.60807682\n",
      "Iteration 66706, loss = 1.33127941\n",
      "Iteration 66707, loss = 1.74023759\n",
      "Iteration 66708, loss = 1.49557186\n",
      "Iteration 66709, loss = 1.61778091\n",
      "Iteration 66710, loss = 1.80549662\n",
      "Iteration 66711, loss = 1.69306708\n",
      "Iteration 66712, loss = 1.84571936\n",
      "Iteration 66713, loss = 1.38029330\n",
      "Iteration 66714, loss = 1.51318369\n",
      "Iteration 66715, loss = 1.43497398\n",
      "Iteration 66716, loss = 1.41992767\n",
      "Iteration 66717, loss = 1.31714141\n",
      "Iteration 66718, loss = 1.15173119\n",
      "Iteration 66719, loss = 1.21901826\n",
      "Iteration 66720, loss = 1.30414400\n",
      "Iteration 66721, loss = 1.51392892\n",
      "Iteration 66722, loss = 1.53837479\n",
      "Iteration 66723, loss = 1.58032878\n",
      "Iteration 66724, loss = 1.59007184\n",
      "Iteration 66725, loss = 1.58231993\n",
      "Iteration 66726, loss = 1.49090721\n",
      "Iteration 66727, loss = 1.35972571\n",
      "Iteration 66728, loss = 1.43487237\n",
      "Iteration 66729, loss = 1.35358038\n",
      "Iteration 66730, loss = 1.27040012\n",
      "Iteration 66731, loss = 1.24447649\n",
      "Iteration 66732, loss = 1.20509255\n",
      "Iteration 66733, loss = 1.04923454\n",
      "Iteration 66734, loss = 1.06484609\n",
      "Iteration 66735, loss = 1.15894051\n",
      "Iteration 66736, loss = 1.24200292\n",
      "Iteration 66737, loss = 1.17715135\n",
      "Iteration 66738, loss = 1.12920530\n",
      "Iteration 66739, loss = 1.29899983\n",
      "Iteration 66740, loss = 1.32567445\n",
      "Iteration 66741, loss = 1.33636550\n",
      "Iteration 66742, loss = 1.40788751\n",
      "Iteration 66743, loss = 1.29628276\n",
      "Iteration 66744, loss = 1.41300540\n",
      "Iteration 66745, loss = 1.24122273\n",
      "Iteration 66746, loss = 1.20033787\n",
      "Iteration 66747, loss = 1.29784116\n",
      "Iteration 66748, loss = 1.38038076\n",
      "Iteration 66749, loss = 1.12272959\n",
      "Iteration 66750, loss = 1.32445139\n",
      "Iteration 66751, loss = 1.32671420\n",
      "Iteration 66752, loss = 1.21809507\n",
      "Iteration 66753, loss = 1.35425186\n",
      "Iteration 66754, loss = 1.31299384\n",
      "Iteration 66755, loss = 2.00283311\n",
      "Iteration 66756, loss = 1.75725333\n",
      "Iteration 66757, loss = 1.83898216\n",
      "Iteration 66758, loss = 1.74076560\n",
      "Iteration 66759, loss = 1.91143869\n",
      "Iteration 66760, loss = 2.33304220\n",
      "Iteration 66761, loss = 2.11062156\n",
      "Iteration 66762, loss = 3.29616605\n",
      "Iteration 66763, loss = 3.63031779\n",
      "Iteration 66764, loss = 2.92159511\n",
      "Iteration 66765, loss = 2.95356102\n",
      "Iteration 66766, loss = 2.22327549\n",
      "Iteration 66767, loss = 2.15406865\n",
      "Iteration 66768, loss = 1.72857164\n",
      "Iteration 66769, loss = 1.55921193\n",
      "Iteration 66770, loss = 1.88667345\n",
      "Iteration 66771, loss = 1.80020991\n",
      "Iteration 66772, loss = 2.19785724\n",
      "Iteration 66773, loss = 1.55467690\n",
      "Iteration 66774, loss = 1.29496530\n",
      "Iteration 66775, loss = 1.30377798\n",
      "Iteration 66776, loss = 1.25286441\n",
      "Iteration 66777, loss = 1.24208853\n",
      "Iteration 66778, loss = 1.33500963\n",
      "Iteration 66779, loss = 1.18760510\n",
      "Iteration 66780, loss = 1.12751086\n",
      "Iteration 66781, loss = 1.04762945\n",
      "Iteration 66782, loss = 1.09475030\n",
      "Iteration 66783, loss = 1.35771710\n",
      "Iteration 66784, loss = 1.52693962\n",
      "Iteration 66785, loss = 1.32305720\n",
      "Iteration 66786, loss = 1.22310405\n",
      "Iteration 66787, loss = 1.22182053\n",
      "Iteration 66788, loss = 1.20443149\n",
      "Iteration 66789, loss = 1.29206175\n",
      "Iteration 66790, loss = 1.41683988\n",
      "Iteration 66791, loss = 1.47931132\n",
      "Iteration 66792, loss = 1.24282971\n",
      "Iteration 66793, loss = 1.18039244\n",
      "Iteration 66794, loss = 1.14492915\n",
      "Iteration 66795, loss = 1.05710152\n",
      "Iteration 66796, loss = 1.23415097\n",
      "Iteration 66797, loss = 1.31484479\n",
      "Iteration 66798, loss = 1.17124528\n",
      "Iteration 66799, loss = 1.15626276\n",
      "Iteration 66800, loss = 1.14293198\n",
      "Iteration 66801, loss = 1.26093116\n",
      "Iteration 66802, loss = 1.60134071\n",
      "Iteration 66803, loss = 1.58777397\n",
      "Iteration 66804, loss = 1.83832206\n",
      "Iteration 66805, loss = 1.62434497\n",
      "Iteration 66806, loss = 1.44175389\n",
      "Iteration 66807, loss = 1.56827730\n",
      "Iteration 66808, loss = 1.34817686\n",
      "Iteration 66809, loss = 1.37464657\n",
      "Iteration 66810, loss = 1.54051238\n",
      "Iteration 66811, loss = 1.31401129\n",
      "Iteration 66812, loss = 1.30006591\n",
      "Iteration 66813, loss = 1.34107645\n",
      "Iteration 66814, loss = 1.54391552\n",
      "Iteration 66815, loss = 1.74351009\n",
      "Iteration 66816, loss = 1.75937496\n",
      "Iteration 66817, loss = 2.09882839\n",
      "Iteration 66818, loss = 1.97665730\n",
      "Iteration 66819, loss = 1.78255317\n",
      "Iteration 66820, loss = 1.67589911\n",
      "Iteration 66821, loss = 1.25428309\n",
      "Iteration 66822, loss = 1.37299379\n",
      "Iteration 66823, loss = 1.22573421\n",
      "Iteration 66824, loss = 1.82539287\n",
      "Iteration 66825, loss = 2.03587117\n",
      "Iteration 66826, loss = 1.78212218\n",
      "Iteration 66827, loss = 1.84355355\n",
      "Iteration 66828, loss = 1.49031160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 66829, loss = 1.42363046\n",
      "Iteration 66830, loss = 1.71856575\n",
      "Iteration 66831, loss = 1.45229528\n",
      "Iteration 66832, loss = 1.45331645\n",
      "Iteration 66833, loss = 1.82093121\n",
      "Iteration 66834, loss = 2.10322472\n",
      "Iteration 66835, loss = 1.75407524\n",
      "Iteration 66836, loss = 1.65756873\n",
      "Iteration 66837, loss = 2.03116348\n",
      "Iteration 66838, loss = 2.04522660\n",
      "Iteration 66839, loss = 2.04832992\n",
      "Iteration 66840, loss = 2.26611620\n",
      "Iteration 66841, loss = 1.98698339\n",
      "Iteration 66842, loss = 1.87378035\n",
      "Iteration 66843, loss = 1.92312573\n",
      "Iteration 66844, loss = 1.34055314\n",
      "Iteration 66845, loss = 1.29955828\n",
      "Iteration 66846, loss = 1.31827040\n",
      "Iteration 66847, loss = 1.38088137\n",
      "Iteration 66848, loss = 1.23779386\n",
      "Iteration 66849, loss = 1.26633555\n",
      "Iteration 66850, loss = 1.28257957\n",
      "Iteration 66851, loss = 1.39262833\n",
      "Iteration 66852, loss = 1.40125115\n",
      "Iteration 66853, loss = 1.32122540\n",
      "Iteration 66854, loss = 1.10863330\n",
      "Iteration 66855, loss = 1.21877828\n",
      "Iteration 66856, loss = 1.21794277\n",
      "Iteration 66857, loss = 1.25798562\n",
      "Iteration 66858, loss = 1.48675583\n",
      "Iteration 66859, loss = 1.46426239\n",
      "Iteration 66860, loss = 1.30681024\n",
      "Iteration 66861, loss = 1.38366710\n",
      "Iteration 66862, loss = 1.32350588\n",
      "Iteration 66863, loss = 1.35075137\n",
      "Iteration 66864, loss = 1.31668065\n",
      "Iteration 66865, loss = 1.36055133\n",
      "Iteration 66866, loss = 1.29782492\n",
      "Iteration 66867, loss = 1.27961009\n",
      "Iteration 66868, loss = 1.59359445\n",
      "Iteration 66869, loss = 1.64666785\n",
      "Iteration 66870, loss = 1.18981874\n",
      "Iteration 66871, loss = 1.28144642\n",
      "Iteration 66872, loss = 1.14416666\n",
      "Iteration 66873, loss = 1.26910231\n",
      "Iteration 66874, loss = 1.33944496\n",
      "Iteration 66875, loss = 1.41012163\n",
      "Iteration 66876, loss = 1.36080664\n",
      "Iteration 66877, loss = 1.41122217\n",
      "Iteration 66878, loss = 1.54975025\n",
      "Iteration 66879, loss = 1.62895547\n",
      "Iteration 66880, loss = 1.30770066\n",
      "Iteration 66881, loss = 1.23658802\n",
      "Iteration 66882, loss = 1.47512052\n",
      "Iteration 66883, loss = 1.24662004\n",
      "Iteration 66884, loss = 1.39600229\n",
      "Iteration 66885, loss = 1.41787994\n",
      "Iteration 66886, loss = 1.34102674\n",
      "Iteration 66887, loss = 1.21061585\n",
      "Iteration 66888, loss = 1.15723735\n",
      "Iteration 66889, loss = 1.15300298\n",
      "Iteration 66890, loss = 1.18483616\n",
      "Iteration 66891, loss = 1.16739766\n",
      "Iteration 66892, loss = 1.14580578\n",
      "Iteration 66893, loss = 1.03157095\n",
      "Iteration 66894, loss = 1.02859331\n",
      "Iteration 66895, loss = 1.13800669\n",
      "Iteration 66896, loss = 1.37163116\n",
      "Iteration 66897, loss = 1.66055231\n",
      "Iteration 66898, loss = 1.40837587\n",
      "Iteration 66899, loss = 1.29578140\n",
      "Iteration 66900, loss = 1.42636381\n",
      "Iteration 66901, loss = 1.39614306\n",
      "Iteration 66902, loss = 1.33000984\n",
      "Iteration 66903, loss = 1.22084561\n",
      "Iteration 66904, loss = 1.50343320\n",
      "Iteration 66905, loss = 1.38795661\n",
      "Iteration 66906, loss = 1.27100126\n",
      "Iteration 66907, loss = 1.70132574\n",
      "Iteration 66908, loss = 1.49379184\n",
      "Iteration 66909, loss = 1.56292231\n",
      "Iteration 66910, loss = 1.55673105\n",
      "Iteration 66911, loss = 1.18292449\n",
      "Iteration 66912, loss = 1.39749211\n",
      "Iteration 66913, loss = 1.21678357\n",
      "Iteration 66914, loss = 1.34810086\n",
      "Iteration 66915, loss = 1.31695681\n",
      "Iteration 66916, loss = 1.29937766\n",
      "Iteration 66917, loss = 1.25523085\n",
      "Iteration 66918, loss = 1.45244662\n",
      "Iteration 66919, loss = 1.33244105\n",
      "Iteration 66920, loss = 1.68928182\n",
      "Iteration 66921, loss = 1.95255495\n",
      "Iteration 66922, loss = 2.54835314\n",
      "Iteration 66923, loss = 2.00579194\n",
      "Iteration 66924, loss = 1.56826659\n",
      "Iteration 66925, loss = 1.59166361\n",
      "Iteration 66926, loss = 2.01819499\n",
      "Iteration 66927, loss = 2.05678235\n",
      "Iteration 66928, loss = 1.77800604\n",
      "Iteration 66929, loss = 1.52093090\n",
      "Iteration 66930, loss = 1.25192491\n",
      "Iteration 66931, loss = 1.29446955\n",
      "Iteration 66932, loss = 1.39415588\n",
      "Iteration 66933, loss = 1.46470488\n",
      "Iteration 66934, loss = 1.23999078\n",
      "Iteration 66935, loss = 1.40323959\n",
      "Iteration 66936, loss = 1.24730113\n",
      "Iteration 66937, loss = 1.38376948\n",
      "Iteration 66938, loss = 1.55212002\n",
      "Iteration 66939, loss = 1.41649503\n",
      "Iteration 66940, loss = 1.35474976\n",
      "Iteration 66941, loss = 1.35647884\n",
      "Iteration 66942, loss = 1.49492375\n",
      "Iteration 66943, loss = 1.96735904\n",
      "Iteration 66944, loss = 2.52106768\n",
      "Iteration 66945, loss = 2.68681952\n",
      "Iteration 66946, loss = 2.28106684\n",
      "Iteration 66947, loss = 2.25178575\n",
      "Iteration 66948, loss = 1.41666608\n",
      "Iteration 66949, loss = 1.65679665\n",
      "Iteration 66950, loss = 1.51454827\n",
      "Iteration 66951, loss = 1.39953872\n",
      "Iteration 66952, loss = 1.36813969\n",
      "Iteration 66953, loss = 1.44480717\n",
      "Iteration 66954, loss = 1.25613821\n",
      "Iteration 66955, loss = 1.27000686\n",
      "Iteration 66956, loss = 1.11849135\n",
      "Iteration 66957, loss = 1.51270796\n",
      "Iteration 66958, loss = 1.38991120\n",
      "Iteration 66959, loss = 1.64737792\n",
      "Iteration 66960, loss = 2.61157746\n",
      "Iteration 66961, loss = 2.45308948\n",
      "Iteration 66962, loss = 2.63402568\n",
      "Iteration 66963, loss = 2.81485333\n",
      "Iteration 66964, loss = 3.13114115\n",
      "Iteration 66965, loss = 2.90115557\n",
      "Iteration 66966, loss = 3.31790004\n",
      "Iteration 66967, loss = 3.19168254\n",
      "Iteration 66968, loss = 3.31294361\n",
      "Iteration 66969, loss = 2.37304654\n",
      "Iteration 66970, loss = 1.79894785\n",
      "Iteration 66971, loss = 1.67911042\n",
      "Iteration 66972, loss = 1.31648138\n",
      "Iteration 66973, loss = 1.35148564\n",
      "Iteration 66974, loss = 1.45061582\n",
      "Iteration 66975, loss = 1.45252320\n",
      "Iteration 66976, loss = 1.39422599\n",
      "Iteration 66977, loss = 1.42565316\n",
      "Iteration 66978, loss = 1.55128009\n",
      "Iteration 66979, loss = 1.15832310\n",
      "Iteration 66980, loss = 1.55707957\n",
      "Iteration 66981, loss = 1.45614333\n",
      "Iteration 66982, loss = 1.27303575\n",
      "Iteration 66983, loss = 1.31807322\n",
      "Iteration 66984, loss = 1.10116544\n",
      "Iteration 66985, loss = 1.13925952\n",
      "Iteration 66986, loss = 1.16591271\n",
      "Iteration 66987, loss = 1.29500819\n",
      "Iteration 66988, loss = 1.36850343\n",
      "Iteration 66989, loss = 1.18687370\n",
      "Iteration 66990, loss = 1.18235026\n",
      "Iteration 66991, loss = 1.33704791\n",
      "Iteration 66992, loss = 1.23613373\n",
      "Iteration 66993, loss = 1.29327518\n",
      "Iteration 66994, loss = 1.14517990\n",
      "Iteration 66995, loss = 1.16823802\n",
      "Iteration 66996, loss = 1.20419485\n",
      "Iteration 66997, loss = 1.16237684\n",
      "Iteration 66998, loss = 1.20155186\n",
      "Iteration 66999, loss = 1.18999506\n",
      "Iteration 67000, loss = 1.23777154\n",
      "Iteration 67001, loss = 1.53991635\n",
      "Iteration 67002, loss = 2.16504583\n",
      "Iteration 67003, loss = 1.57616825\n",
      "Iteration 67004, loss = 1.29134931\n",
      "Iteration 67005, loss = 1.28202895\n",
      "Iteration 67006, loss = 1.34622089\n",
      "Iteration 67007, loss = 1.17639381\n",
      "Iteration 67008, loss = 1.13240115\n",
      "Iteration 67009, loss = 1.06627182\n",
      "Iteration 67010, loss = 1.03050183\n",
      "Iteration 67011, loss = 1.12880513\n",
      "Iteration 67012, loss = 1.21536221\n",
      "Iteration 67013, loss = 1.66901342\n",
      "Iteration 67014, loss = 1.91416971\n",
      "Iteration 67015, loss = 1.74642252\n",
      "Iteration 67016, loss = 1.93193459\n",
      "Iteration 67017, loss = 1.69042652\n",
      "Iteration 67018, loss = 2.14323676\n",
      "Iteration 67019, loss = 2.25427405\n",
      "Iteration 67020, loss = 1.96224611\n",
      "Iteration 67021, loss = 1.44540220\n",
      "Iteration 67022, loss = 1.55016914\n",
      "Iteration 67023, loss = 1.42207793\n",
      "Iteration 67024, loss = 1.68200518\n",
      "Iteration 67025, loss = 2.05674747\n",
      "Iteration 67026, loss = 2.00923347\n",
      "Iteration 67027, loss = 2.10821986\n",
      "Iteration 67028, loss = 1.66250397\n",
      "Iteration 67029, loss = 1.62754054\n",
      "Iteration 67030, loss = 1.35984351\n",
      "Iteration 67031, loss = 1.33090330\n",
      "Iteration 67032, loss = 1.16897715\n",
      "Iteration 67033, loss = 1.26832750\n",
      "Iteration 67034, loss = 1.25690377\n",
      "Iteration 67035, loss = 1.21303306\n",
      "Iteration 67036, loss = 1.20400922\n",
      "Iteration 67037, loss = 1.22911511\n",
      "Iteration 67038, loss = 1.09623757\n",
      "Iteration 67039, loss = 1.05693455\n",
      "Iteration 67040, loss = 1.17647206\n",
      "Iteration 67041, loss = 1.05266390\n",
      "Iteration 67042, loss = 1.27619681\n",
      "Iteration 67043, loss = 1.23015610\n",
      "Iteration 67044, loss = 1.23454222\n",
      "Iteration 67045, loss = 1.05436549\n",
      "Iteration 67046, loss = 1.21614363\n",
      "Iteration 67047, loss = 1.14125907\n",
      "Iteration 67048, loss = 1.21271299\n",
      "Iteration 67049, loss = 1.26221911\n",
      "Iteration 67050, loss = 1.34491994\n",
      "Iteration 67051, loss = 1.26552337\n",
      "Iteration 67052, loss = 1.33099480\n",
      "Iteration 67053, loss = 1.13619040\n",
      "Iteration 67054, loss = 1.21351523\n",
      "Iteration 67055, loss = 1.76982015\n",
      "Iteration 67056, loss = 1.81541195\n",
      "Iteration 67057, loss = 1.56431428\n",
      "Iteration 67058, loss = 1.33984467\n",
      "Iteration 67059, loss = 1.34705324\n",
      "Iteration 67060, loss = 1.36668865\n",
      "Iteration 67061, loss = 1.71939179\n",
      "Iteration 67062, loss = 1.58002712\n",
      "Iteration 67063, loss = 1.88425035\n",
      "Iteration 67064, loss = 1.95442884\n",
      "Iteration 67065, loss = 2.23503185\n",
      "Iteration 67066, loss = 2.62173019\n",
      "Iteration 67067, loss = 2.48883425\n",
      "Iteration 67068, loss = 2.25354593\n",
      "Iteration 67069, loss = 1.87815752\n",
      "Iteration 67070, loss = 1.66745116\n",
      "Iteration 67071, loss = 1.80434573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 67072, loss = 1.75983359\n",
      "Iteration 67073, loss = 1.79343275\n",
      "Iteration 67074, loss = 2.19573919\n",
      "Iteration 67075, loss = 1.82349394\n",
      "Iteration 67076, loss = 1.47890874\n",
      "Iteration 67077, loss = 1.76757609\n",
      "Iteration 67078, loss = 1.53060680\n",
      "Iteration 67079, loss = 1.45176735\n",
      "Iteration 67080, loss = 1.29204968\n",
      "Iteration 67081, loss = 1.23494783\n",
      "Iteration 67082, loss = 1.32873763\n",
      "Iteration 67083, loss = 1.32175709\n",
      "Iteration 67084, loss = 1.40064661\n",
      "Iteration 67085, loss = 1.10540431\n",
      "Iteration 67086, loss = 1.10987344\n",
      "Iteration 67087, loss = 1.18028305\n",
      "Iteration 67088, loss = 1.10991397\n",
      "Iteration 67089, loss = 1.29888396\n",
      "Iteration 67090, loss = 1.37167819\n",
      "Iteration 67091, loss = 1.45710578\n",
      "Iteration 67092, loss = 1.56551027\n",
      "Iteration 67093, loss = 1.36700758\n",
      "Iteration 67094, loss = 1.33825995\n",
      "Iteration 67095, loss = 1.24353792\n",
      "Iteration 67096, loss = 1.35085926\n",
      "Iteration 67097, loss = 1.21747928\n",
      "Iteration 67098, loss = 1.31856963\n",
      "Iteration 67099, loss = 1.22276231\n",
      "Iteration 67100, loss = 1.07762987\n",
      "Iteration 67101, loss = 1.14118377\n",
      "Iteration 67102, loss = 1.12333996\n",
      "Iteration 67103, loss = 1.18934145\n",
      "Iteration 67104, loss = 1.05433374\n",
      "Iteration 67105, loss = 1.51050621\n",
      "Iteration 67106, loss = 1.69011860\n",
      "Iteration 67107, loss = 1.79193600\n",
      "Iteration 67108, loss = 1.55697923\n",
      "Iteration 67109, loss = 1.33838030\n",
      "Iteration 67110, loss = 1.11430359\n",
      "Iteration 67111, loss = 1.23584412\n",
      "Iteration 67112, loss = 1.34796181\n",
      "Iteration 67113, loss = 1.23522398\n",
      "Iteration 67114, loss = 1.29643708\n",
      "Iteration 67115, loss = 1.14554074\n",
      "Iteration 67116, loss = 1.24148322\n",
      "Iteration 67117, loss = 1.32367887\n",
      "Iteration 67118, loss = 1.29658227\n",
      "Iteration 67119, loss = 1.20792564\n",
      "Iteration 67120, loss = 1.39878682\n",
      "Iteration 67121, loss = 1.46907681\n",
      "Iteration 67122, loss = 1.20924178\n",
      "Iteration 67123, loss = 1.22175198\n",
      "Iteration 67124, loss = 1.10282020\n",
      "Iteration 67125, loss = 1.17069587\n",
      "Iteration 67126, loss = 1.17082935\n",
      "Iteration 67127, loss = 1.09795952\n",
      "Iteration 67128, loss = 1.17202361\n",
      "Iteration 67129, loss = 1.44824707\n",
      "Iteration 67130, loss = 1.30715843\n",
      "Iteration 67131, loss = 1.40235586\n",
      "Iteration 67132, loss = 1.31619551\n",
      "Iteration 67133, loss = 1.29130213\n",
      "Iteration 67134, loss = 1.32590446\n",
      "Iteration 67135, loss = 1.56083690\n",
      "Iteration 67136, loss = 1.68046718\n",
      "Iteration 67137, loss = 1.78829799\n",
      "Iteration 67138, loss = 1.41725970\n",
      "Iteration 67139, loss = 1.58642798\n",
      "Iteration 67140, loss = 1.65399919\n",
      "Iteration 67141, loss = 1.81217010\n",
      "Iteration 67142, loss = 1.90298752\n",
      "Iteration 67143, loss = 2.06712995\n",
      "Iteration 67144, loss = 1.89089060\n",
      "Iteration 67145, loss = 1.73582804\n",
      "Iteration 67146, loss = 1.28221299\n",
      "Iteration 67147, loss = 1.65676452\n",
      "Iteration 67148, loss = 1.50771186\n",
      "Iteration 67149, loss = 1.33996529\n",
      "Iteration 67150, loss = 1.25272993\n",
      "Iteration 67151, loss = 1.57799464\n",
      "Iteration 67152, loss = 1.30307232\n",
      "Iteration 67153, loss = 1.50010244\n",
      "Iteration 67154, loss = 1.33644253\n",
      "Iteration 67155, loss = 1.13429723\n",
      "Iteration 67156, loss = 1.12098710\n",
      "Iteration 67157, loss = 1.10404416\n",
      "Iteration 67158, loss = 1.24201620\n",
      "Iteration 67159, loss = 1.45838042\n",
      "Iteration 67160, loss = 1.20556509\n",
      "Iteration 67161, loss = 1.16194682\n",
      "Iteration 67162, loss = 1.36494907\n",
      "Iteration 67163, loss = 1.58241951\n",
      "Iteration 67164, loss = 1.94030796\n",
      "Iteration 67165, loss = 2.66301753\n",
      "Iteration 67166, loss = 2.04523479\n",
      "Iteration 67167, loss = 1.60240496\n",
      "Iteration 67168, loss = 1.66256149\n",
      "Iteration 67169, loss = 2.00555197\n",
      "Iteration 67170, loss = 1.54500349\n",
      "Iteration 67171, loss = 1.18951865\n",
      "Iteration 67172, loss = 1.24389886\n",
      "Iteration 67173, loss = 1.43089774\n",
      "Iteration 67174, loss = 1.30508577\n",
      "Iteration 67175, loss = 1.42268477\n",
      "Iteration 67176, loss = 1.25645584\n",
      "Iteration 67177, loss = 1.45171492\n",
      "Iteration 67178, loss = 1.34407352\n",
      "Iteration 67179, loss = 1.24087927\n",
      "Iteration 67180, loss = 1.44489226\n",
      "Iteration 67181, loss = 1.45983581\n",
      "Iteration 67182, loss = 1.80465364\n",
      "Iteration 67183, loss = 1.51145159\n",
      "Iteration 67184, loss = 1.40667048\n",
      "Iteration 67185, loss = 1.16359284\n",
      "Iteration 67186, loss = 1.13437777\n",
      "Iteration 67187, loss = 1.18214770\n",
      "Iteration 67188, loss = 1.38357616\n",
      "Iteration 67189, loss = 1.50521390\n",
      "Iteration 67190, loss = 1.31182247\n",
      "Iteration 67191, loss = 1.21194106\n",
      "Iteration 67192, loss = 1.10965009\n",
      "Iteration 67193, loss = 1.14999103\n",
      "Iteration 67194, loss = 1.16140163\n",
      "Iteration 67195, loss = 1.40991048\n",
      "Iteration 67196, loss = 1.45237868\n",
      "Iteration 67197, loss = 1.90900711\n",
      "Iteration 67198, loss = 2.34023972\n",
      "Iteration 67199, loss = 2.01616621\n",
      "Iteration 67200, loss = 1.44452855\n",
      "Iteration 67201, loss = 1.73548858\n",
      "Iteration 67202, loss = 1.86776959\n",
      "Iteration 67203, loss = 1.75484878\n",
      "Iteration 67204, loss = 1.45136453\n",
      "Iteration 67205, loss = 1.37791132\n",
      "Iteration 67206, loss = 1.29227549\n",
      "Iteration 67207, loss = 1.18366276\n",
      "Iteration 67208, loss = 1.10592016\n",
      "Iteration 67209, loss = 1.32692566\n",
      "Iteration 67210, loss = 1.57336854\n",
      "Iteration 67211, loss = 1.77236246\n",
      "Iteration 67212, loss = 1.61546132\n",
      "Iteration 67213, loss = 1.53733359\n",
      "Iteration 67214, loss = 1.51316998\n",
      "Iteration 67215, loss = 1.40653801\n",
      "Iteration 67216, loss = 1.27995700\n",
      "Iteration 67217, loss = 1.35571196\n",
      "Iteration 67218, loss = 1.61244406\n",
      "Iteration 67219, loss = 1.18183511\n",
      "Iteration 67220, loss = 1.29068218\n",
      "Iteration 67221, loss = 1.17524575\n",
      "Iteration 67222, loss = 1.25104461\n",
      "Iteration 67223, loss = 1.18337373\n",
      "Iteration 67224, loss = 1.27467280\n",
      "Iteration 67225, loss = 1.71376106\n",
      "Iteration 67226, loss = 1.46405507\n",
      "Iteration 67227, loss = 1.54008804\n",
      "Iteration 67228, loss = 1.50923922\n",
      "Iteration 67229, loss = 1.78289044\n",
      "Iteration 67230, loss = 1.90596437\n",
      "Iteration 67231, loss = 1.58462080\n",
      "Iteration 67232, loss = 1.38809508\n",
      "Iteration 67233, loss = 1.35440690\n",
      "Iteration 67234, loss = 1.15310088\n",
      "Iteration 67235, loss = 1.16393688\n",
      "Iteration 67236, loss = 1.28350312\n",
      "Iteration 67237, loss = 1.22537789\n",
      "Iteration 67238, loss = 1.27255157\n",
      "Iteration 67239, loss = 1.32817730\n",
      "Iteration 67240, loss = 1.27060928\n",
      "Iteration 67241, loss = 1.08067455\n",
      "Iteration 67242, loss = 1.05772723\n",
      "Iteration 67243, loss = 1.07870064\n",
      "Iteration 67244, loss = 1.23039596\n",
      "Iteration 67245, loss = 1.28815959\n",
      "Iteration 67246, loss = 1.26314011\n",
      "Iteration 67247, loss = 1.22012606\n",
      "Iteration 67248, loss = 1.32027479\n",
      "Iteration 67249, loss = 1.34013158\n",
      "Iteration 67250, loss = 1.40104114\n",
      "Iteration 67251, loss = 1.23578859\n",
      "Iteration 67252, loss = 1.49320594\n",
      "Iteration 67253, loss = 2.04869154\n",
      "Iteration 67254, loss = 1.38512141\n",
      "Iteration 67255, loss = 1.09540572\n",
      "Iteration 67256, loss = 1.19951775\n",
      "Iteration 67257, loss = 1.10159595\n",
      "Iteration 67258, loss = 1.15052404\n",
      "Iteration 67259, loss = 1.19644794\n",
      "Iteration 67260, loss = 1.26822078\n",
      "Iteration 67261, loss = 1.30058472\n",
      "Iteration 67262, loss = 1.25623126\n",
      "Iteration 67263, loss = 1.11687343\n",
      "Iteration 67264, loss = 1.16953185\n",
      "Iteration 67265, loss = 1.33752662\n",
      "Iteration 67266, loss = 1.38347675\n",
      "Iteration 67267, loss = 1.38518906\n",
      "Iteration 67268, loss = 1.39664182\n",
      "Iteration 67269, loss = 1.30833813\n",
      "Iteration 67270, loss = 1.40514672\n",
      "Iteration 67271, loss = 1.43843101\n",
      "Iteration 67272, loss = 1.39736267\n",
      "Iteration 67273, loss = 1.23463589\n",
      "Iteration 67274, loss = 1.19258837\n",
      "Iteration 67275, loss = 1.35224289\n",
      "Iteration 67276, loss = 1.25830040\n",
      "Iteration 67277, loss = 1.14055668\n",
      "Iteration 67278, loss = 1.32919809\n",
      "Iteration 67279, loss = 1.18214556\n",
      "Iteration 67280, loss = 1.37578242\n",
      "Iteration 67281, loss = 1.34566688\n",
      "Iteration 67282, loss = 1.28500262\n",
      "Iteration 67283, loss = 1.37561331\n",
      "Iteration 67284, loss = 1.51621461\n",
      "Iteration 67285, loss = 2.51634627\n",
      "Iteration 67286, loss = 2.09842317\n",
      "Iteration 67287, loss = 2.38825848\n",
      "Iteration 67288, loss = 3.15016402\n",
      "Iteration 67289, loss = 1.93507277\n",
      "Iteration 67290, loss = 2.25647669\n",
      "Iteration 67291, loss = 1.56326142\n",
      "Iteration 67292, loss = 2.12045749\n",
      "Iteration 67293, loss = 2.03444615\n",
      "Iteration 67294, loss = 1.37865540\n",
      "Iteration 67295, loss = 1.09288999\n",
      "Iteration 67296, loss = 1.08654395\n",
      "Iteration 67297, loss = 1.10794660\n",
      "Iteration 67298, loss = 1.17654624\n",
      "Iteration 67299, loss = 1.17059841\n",
      "Iteration 67300, loss = 1.15262824\n",
      "Iteration 67301, loss = 1.24254753\n",
      "Iteration 67302, loss = 1.54519846\n",
      "Iteration 67303, loss = 1.78720108\n",
      "Iteration 67304, loss = 1.61481943\n",
      "Iteration 67305, loss = 1.60625699\n",
      "Iteration 67306, loss = 1.40273382\n",
      "Iteration 67307, loss = 1.87786179\n",
      "Iteration 67308, loss = 1.47944488\n",
      "Iteration 67309, loss = 1.65750097\n",
      "Iteration 67310, loss = 1.42399887\n",
      "Iteration 67311, loss = 1.32064231\n",
      "Iteration 67312, loss = 1.24814423\n",
      "Iteration 67313, loss = 1.17843237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 67314, loss = 1.27029169\n",
      "Iteration 67315, loss = 1.49160635\n",
      "Iteration 67316, loss = 1.72737411\n",
      "Iteration 67317, loss = 1.24668297\n",
      "Iteration 67318, loss = 1.48797180\n",
      "Iteration 67319, loss = 1.22209342\n",
      "Iteration 67320, loss = 1.17502638\n",
      "Iteration 67321, loss = 1.16194730\n",
      "Iteration 67322, loss = 1.91552503\n",
      "Iteration 67323, loss = 1.82327592\n",
      "Iteration 67324, loss = 1.72748197\n",
      "Iteration 67325, loss = 1.65533007\n",
      "Iteration 67326, loss = 1.54010542\n",
      "Iteration 67327, loss = 1.42257228\n",
      "Iteration 67328, loss = 1.43927272\n",
      "Iteration 67329, loss = 1.22372293\n",
      "Iteration 67330, loss = 1.34417280\n",
      "Iteration 67331, loss = 1.58050299\n",
      "Iteration 67332, loss = 2.17540954\n",
      "Iteration 67333, loss = 1.52717793\n",
      "Iteration 67334, loss = 1.45996761\n",
      "Iteration 67335, loss = 1.57835382\n",
      "Iteration 67336, loss = 1.46004428\n",
      "Iteration 67337, loss = 1.65356967\n",
      "Iteration 67338, loss = 1.61742486\n",
      "Iteration 67339, loss = 1.62045165\n",
      "Iteration 67340, loss = 1.56950292\n",
      "Iteration 67341, loss = 1.74397357\n",
      "Iteration 67342, loss = 1.64284265\n",
      "Iteration 67343, loss = 1.63801864\n",
      "Iteration 67344, loss = 1.76895615\n",
      "Iteration 67345, loss = 1.34474106\n",
      "Iteration 67346, loss = 1.34701929\n",
      "Iteration 67347, loss = 1.41947714\n",
      "Iteration 67348, loss = 1.29972935\n",
      "Iteration 67349, loss = 1.25654901\n",
      "Iteration 67350, loss = 1.18139730\n",
      "Iteration 67351, loss = 1.07788012\n",
      "Iteration 67352, loss = 1.04488423\n",
      "Iteration 67353, loss = 1.08532632\n",
      "Iteration 67354, loss = 1.10269439\n",
      "Iteration 67355, loss = 1.03834194\n",
      "Iteration 67356, loss = 1.13392143\n",
      "Iteration 67357, loss = 1.24557912\n",
      "Iteration 67358, loss = 1.33805320\n",
      "Iteration 67359, loss = 1.47684415\n",
      "Iteration 67360, loss = 1.50608267\n",
      "Iteration 67361, loss = 1.78714062\n",
      "Iteration 67362, loss = 1.81665548\n",
      "Iteration 67363, loss = 1.37105150\n",
      "Iteration 67364, loss = 1.42835359\n",
      "Iteration 67365, loss = 1.55331221\n",
      "Iteration 67366, loss = 1.27877226\n",
      "Iteration 67367, loss = 1.29844857\n",
      "Iteration 67368, loss = 1.23809299\n",
      "Iteration 67369, loss = 1.21418672\n",
      "Iteration 67370, loss = 1.20805438\n",
      "Iteration 67371, loss = 1.17476134\n",
      "Iteration 67372, loss = 1.18596200\n",
      "Iteration 67373, loss = 1.34310316\n",
      "Iteration 67374, loss = 1.91171973\n",
      "Iteration 67375, loss = 2.32770071\n",
      "Iteration 67376, loss = 2.54121344\n",
      "Iteration 67377, loss = 1.89361277\n",
      "Iteration 67378, loss = 1.71812163\n",
      "Iteration 67379, loss = 2.32924530\n",
      "Iteration 67380, loss = 2.28367208\n",
      "Iteration 67381, loss = 1.91595263\n",
      "Iteration 67382, loss = 1.67712404\n",
      "Iteration 67383, loss = 2.00738126\n",
      "Iteration 67384, loss = 1.47779430\n",
      "Iteration 67385, loss = 1.44969025\n",
      "Iteration 67386, loss = 1.76529450\n",
      "Iteration 67387, loss = 1.85664305\n",
      "Iteration 67388, loss = 1.65786381\n",
      "Iteration 67389, loss = 1.33473691\n",
      "Iteration 67390, loss = 1.49876415\n",
      "Iteration 67391, loss = 1.18073486\n",
      "Iteration 67392, loss = 1.35788592\n",
      "Iteration 67393, loss = 1.29061577\n",
      "Iteration 67394, loss = 1.23502679\n",
      "Iteration 67395, loss = 1.26536253\n",
      "Iteration 67396, loss = 1.13219426\n",
      "Iteration 67397, loss = 1.37912367\n",
      "Iteration 67398, loss = 1.34325478\n",
      "Iteration 67399, loss = 1.30620167\n",
      "Iteration 67400, loss = 1.17061104\n",
      "Iteration 67401, loss = 1.23554030\n",
      "Iteration 67402, loss = 1.08292600\n",
      "Iteration 67403, loss = 1.24364042\n",
      "Iteration 67404, loss = 1.48296556\n",
      "Iteration 67405, loss = 1.81522123\n",
      "Iteration 67406, loss = 1.47513532\n",
      "Iteration 67407, loss = 1.69877987\n",
      "Iteration 67408, loss = 1.44231126\n",
      "Iteration 67409, loss = 1.33530303\n",
      "Iteration 67410, loss = 1.36661895\n",
      "Iteration 67411, loss = 1.19724529\n",
      "Iteration 67412, loss = 1.29542782\n",
      "Iteration 67413, loss = 1.50865788\n",
      "Iteration 67414, loss = 1.32338567\n",
      "Iteration 67415, loss = 1.36690512\n",
      "Iteration 67416, loss = 1.29671026\n",
      "Iteration 67417, loss = 1.59404084\n",
      "Iteration 67418, loss = 1.59239568\n",
      "Iteration 67419, loss = 1.45152771\n",
      "Iteration 67420, loss = 2.01072821\n",
      "Iteration 67421, loss = 1.83126089\n",
      "Iteration 67422, loss = 2.46453454\n",
      "Iteration 67423, loss = 3.69803435\n",
      "Iteration 67424, loss = 3.46734853\n",
      "Iteration 67425, loss = 2.82247788\n",
      "Iteration 67426, loss = 2.42232932\n",
      "Iteration 67427, loss = 1.79854833\n",
      "Iteration 67428, loss = 2.41964099\n",
      "Iteration 67429, loss = 2.02407273\n",
      "Iteration 67430, loss = 1.70728139\n",
      "Iteration 67431, loss = 1.26799899\n",
      "Iteration 67432, loss = 1.09899453\n",
      "Iteration 67433, loss = 1.32130509\n",
      "Iteration 67434, loss = 1.26026296\n",
      "Iteration 67435, loss = 1.21856189\n",
      "Iteration 67436, loss = 1.23634785\n",
      "Iteration 67437, loss = 1.35219230\n",
      "Iteration 67438, loss = 2.06410882\n",
      "Iteration 67439, loss = 2.02002390\n",
      "Iteration 67440, loss = 2.16755464\n",
      "Iteration 67441, loss = 1.83258411\n",
      "Iteration 67442, loss = 1.86443097\n",
      "Iteration 67443, loss = 1.71567016\n",
      "Iteration 67444, loss = 1.32206122\n",
      "Iteration 67445, loss = 1.59804509\n",
      "Iteration 67446, loss = 1.31547157\n",
      "Iteration 67447, loss = 1.11053154\n",
      "Iteration 67448, loss = 1.17215800\n",
      "Iteration 67449, loss = 1.18226699\n",
      "Iteration 67450, loss = 1.41186981\n",
      "Iteration 67451, loss = 1.32773093\n",
      "Iteration 67452, loss = 1.25557029\n",
      "Iteration 67453, loss = 1.26736152\n",
      "Iteration 67454, loss = 1.28290650\n",
      "Iteration 67455, loss = 1.47465773\n",
      "Iteration 67456, loss = 1.22544411\n",
      "Iteration 67457, loss = 1.20671813\n",
      "Iteration 67458, loss = 1.26842361\n",
      "Iteration 67459, loss = 1.27866022\n",
      "Iteration 67460, loss = 1.13371860\n",
      "Iteration 67461, loss = 1.42761344\n",
      "Iteration 67462, loss = 1.27655416\n",
      "Iteration 67463, loss = 1.36065309\n",
      "Iteration 67464, loss = 1.38425836\n",
      "Iteration 67465, loss = 1.17406110\n",
      "Iteration 67466, loss = 1.16935090\n",
      "Iteration 67467, loss = 1.12754400\n",
      "Iteration 67468, loss = 1.13557814\n",
      "Iteration 67469, loss = 1.40396148\n",
      "Iteration 67470, loss = 1.31715393\n",
      "Iteration 67471, loss = 1.33216067\n",
      "Iteration 67472, loss = 1.30242916\n",
      "Iteration 67473, loss = 1.39996310\n",
      "Iteration 67474, loss = 1.58105324\n",
      "Iteration 67475, loss = 2.78304386\n",
      "Iteration 67476, loss = 2.72694953\n",
      "Iteration 67477, loss = 2.20973782\n",
      "Iteration 67478, loss = 1.71951331\n",
      "Iteration 67479, loss = 1.46988010\n",
      "Iteration 67480, loss = 1.28537639\n",
      "Iteration 67481, loss = 1.30995614\n",
      "Iteration 67482, loss = 1.28517600\n",
      "Iteration 67483, loss = 1.51023008\n",
      "Iteration 67484, loss = 1.26693942\n",
      "Iteration 67485, loss = 1.28837865\n",
      "Iteration 67486, loss = 1.30278200\n",
      "Iteration 67487, loss = 1.32640423\n",
      "Iteration 67488, loss = 1.58326590\n",
      "Iteration 67489, loss = 1.29580198\n",
      "Iteration 67490, loss = 1.61445143\n",
      "Iteration 67491, loss = 1.97569929\n",
      "Iteration 67492, loss = 1.66502721\n",
      "Iteration 67493, loss = 1.85103916\n",
      "Iteration 67494, loss = 3.11758069\n",
      "Iteration 67495, loss = 3.22301297\n",
      "Iteration 67496, loss = 1.98056538\n",
      "Iteration 67497, loss = 2.17114463\n",
      "Iteration 67498, loss = 2.38687583\n",
      "Iteration 67499, loss = 2.16725785\n",
      "Iteration 67500, loss = 2.00044687\n",
      "Iteration 67501, loss = 1.98767513\n",
      "Iteration 67502, loss = 1.20498675\n",
      "Iteration 67503, loss = 1.50654344\n",
      "Iteration 67504, loss = 1.42930338\n",
      "Iteration 67505, loss = 1.63688303\n",
      "Iteration 67506, loss = 1.41633958\n",
      "Iteration 67507, loss = 1.36028505\n",
      "Iteration 67508, loss = 1.53003711\n",
      "Iteration 67509, loss = 1.57205747\n",
      "Iteration 67510, loss = 1.19916248\n",
      "Iteration 67511, loss = 1.20428889\n",
      "Iteration 67512, loss = 1.23050699\n",
      "Iteration 67513, loss = 1.24419428\n",
      "Iteration 67514, loss = 1.68583088\n",
      "Iteration 67515, loss = 1.34840290\n",
      "Iteration 67516, loss = 1.23557201\n",
      "Iteration 67517, loss = 1.30768251\n",
      "Iteration 67518, loss = 1.52946682\n",
      "Iteration 67519, loss = 1.36005086\n",
      "Iteration 67520, loss = 1.31313546\n",
      "Iteration 67521, loss = 1.28155867\n",
      "Iteration 67522, loss = 1.18084518\n",
      "Iteration 67523, loss = 1.20013588\n",
      "Iteration 67524, loss = 1.39667188\n",
      "Iteration 67525, loss = 1.31588054\n",
      "Iteration 67526, loss = 1.52779157\n",
      "Iteration 67527, loss = 1.37481149\n",
      "Iteration 67528, loss = 1.23502355\n",
      "Iteration 67529, loss = 1.16724064\n",
      "Iteration 67530, loss = 1.13761875\n",
      "Iteration 67531, loss = 1.16207656\n",
      "Iteration 67532, loss = 1.23278051\n",
      "Iteration 67533, loss = 1.17100616\n",
      "Iteration 67534, loss = 1.12288894\n",
      "Iteration 67535, loss = 1.05867730\n",
      "Iteration 67536, loss = 1.10680745\n",
      "Iteration 67537, loss = 1.11653182\n",
      "Iteration 67538, loss = 1.12176301\n",
      "Iteration 67539, loss = 1.27409629\n",
      "Iteration 67540, loss = 1.31849869\n",
      "Iteration 67541, loss = 1.67183355\n",
      "Iteration 67542, loss = 2.09392522\n",
      "Iteration 67543, loss = 1.80174840\n",
      "Iteration 67544, loss = 1.37025754\n",
      "Iteration 67545, loss = 1.37849808\n",
      "Iteration 67546, loss = 1.18869909\n",
      "Iteration 67547, loss = 1.23195246\n",
      "Iteration 67548, loss = 1.20180243\n",
      "Iteration 67549, loss = 1.40924001\n",
      "Iteration 67550, loss = 1.40357840\n",
      "Iteration 67551, loss = 1.40544443\n",
      "Iteration 67552, loss = 1.57410079\n",
      "Iteration 67553, loss = 1.83582803\n",
      "Iteration 67554, loss = 1.62050891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 67555, loss = 1.44255244\n",
      "Iteration 67556, loss = 1.26978672\n",
      "Iteration 67557, loss = 1.50635656\n",
      "Iteration 67558, loss = 1.46488079\n",
      "Iteration 67559, loss = 1.35108587\n",
      "Iteration 67560, loss = 1.48675069\n",
      "Iteration 67561, loss = 1.39059605\n",
      "Iteration 67562, loss = 1.53632204\n",
      "Iteration 67563, loss = 1.23293042\n",
      "Iteration 67564, loss = 1.56841433\n",
      "Iteration 67565, loss = 1.68877492\n",
      "Iteration 67566, loss = 1.67717651\n",
      "Iteration 67567, loss = 1.46477769\n",
      "Iteration 67568, loss = 1.41133630\n",
      "Iteration 67569, loss = 1.17300594\n",
      "Iteration 67570, loss = 1.40419526\n",
      "Iteration 67571, loss = 1.49691747\n",
      "Iteration 67572, loss = 1.52892886\n",
      "Iteration 67573, loss = 1.70974521\n",
      "Iteration 67574, loss = 1.25068644\n",
      "Iteration 67575, loss = 1.22285858\n",
      "Iteration 67576, loss = 1.31065990\n",
      "Iteration 67577, loss = 1.53536826\n",
      "Iteration 67578, loss = 1.54824222\n",
      "Iteration 67579, loss = 1.48879905\n",
      "Iteration 67580, loss = 1.51693237\n",
      "Iteration 67581, loss = 1.12424740\n",
      "Iteration 67582, loss = 1.21354670\n",
      "Iteration 67583, loss = 1.25187227\n",
      "Iteration 67584, loss = 1.34173163\n",
      "Iteration 67585, loss = 1.49290618\n",
      "Iteration 67586, loss = 1.40928351\n",
      "Iteration 67587, loss = 1.56125322\n",
      "Iteration 67588, loss = 1.27798399\n",
      "Iteration 67589, loss = 2.02705672\n",
      "Iteration 67590, loss = 1.81761079\n",
      "Iteration 67591, loss = 2.22782376\n",
      "Iteration 67592, loss = 2.90354702\n",
      "Iteration 67593, loss = 2.29851387\n",
      "Iteration 67594, loss = 2.22732285\n",
      "Iteration 67595, loss = 2.83044739\n",
      "Iteration 67596, loss = 2.88532380\n",
      "Iteration 67597, loss = 1.52549646\n",
      "Iteration 67598, loss = 2.13477822\n",
      "Iteration 67599, loss = 1.68685005\n",
      "Iteration 67600, loss = 2.16529656\n",
      "Iteration 67601, loss = 2.03477964\n",
      "Iteration 67602, loss = 1.42749215\n",
      "Iteration 67603, loss = 1.39650724\n",
      "Iteration 67604, loss = 1.41359592\n",
      "Iteration 67605, loss = 1.35149107\n",
      "Iteration 67606, loss = 1.48888819\n",
      "Iteration 67607, loss = 1.33071733\n",
      "Iteration 67608, loss = 1.44306304\n",
      "Iteration 67609, loss = 1.18633761\n",
      "Iteration 67610, loss = 1.18949646\n",
      "Iteration 67611, loss = 1.10755143\n",
      "Iteration 67612, loss = 1.25000752\n",
      "Iteration 67613, loss = 1.42363355\n",
      "Iteration 67614, loss = 1.24034638\n",
      "Iteration 67615, loss = 1.22010599\n",
      "Iteration 67616, loss = 1.34223584\n",
      "Iteration 67617, loss = 1.32817434\n",
      "Iteration 67618, loss = 1.38694965\n",
      "Iteration 67619, loss = 1.25297265\n",
      "Iteration 67620, loss = 1.42974219\n",
      "Iteration 67621, loss = 1.17878928\n",
      "Iteration 67622, loss = 1.12311392\n",
      "Iteration 67623, loss = 1.17950114\n",
      "Iteration 67624, loss = 1.25554571\n",
      "Iteration 67625, loss = 1.44310021\n",
      "Iteration 67626, loss = 1.19650705\n",
      "Iteration 67627, loss = 1.10850968\n",
      "Iteration 67628, loss = 1.18249697\n",
      "Iteration 67629, loss = 1.12862198\n",
      "Iteration 67630, loss = 1.12214393\n",
      "Iteration 67631, loss = 1.11978291\n",
      "Iteration 67632, loss = 1.30581884\n",
      "Iteration 67633, loss = 1.14543413\n",
      "Iteration 67634, loss = 1.21152913\n",
      "Iteration 67635, loss = 1.24038240\n",
      "Iteration 67636, loss = 1.49217737\n",
      "Iteration 67637, loss = 1.25057454\n",
      "Iteration 67638, loss = 1.18689718\n",
      "Iteration 67639, loss = 1.14130087\n",
      "Iteration 67640, loss = 1.06203643\n",
      "Iteration 67641, loss = 1.17333273\n",
      "Iteration 67642, loss = 1.21496202\n",
      "Iteration 67643, loss = 1.22696093\n",
      "Iteration 67644, loss = 1.14235799\n",
      "Iteration 67645, loss = 1.26341927\n",
      "Iteration 67646, loss = 1.33621975\n",
      "Iteration 67647, loss = 1.15504816\n",
      "Iteration 67648, loss = 1.35275958\n",
      "Iteration 67649, loss = 1.17805352\n",
      "Iteration 67650, loss = 1.15110555\n",
      "Iteration 67651, loss = 1.25301530\n",
      "Iteration 67652, loss = 1.19388362\n",
      "Iteration 67653, loss = 1.02128722\n",
      "Iteration 67654, loss = 1.09058660\n",
      "Iteration 67655, loss = 1.10917879\n",
      "Iteration 67656, loss = 1.15725216\n",
      "Iteration 67657, loss = 1.46327646\n",
      "Iteration 67658, loss = 1.92987948\n",
      "Iteration 67659, loss = 2.04692016\n",
      "Iteration 67660, loss = 2.14374447\n",
      "Iteration 67661, loss = 2.14125057\n",
      "Iteration 67662, loss = 2.98296470\n",
      "Iteration 67663, loss = 3.74546472\n",
      "Iteration 67664, loss = 3.37317488\n",
      "Iteration 67665, loss = 3.83437419\n",
      "Iteration 67666, loss = 2.23098397\n",
      "Iteration 67667, loss = 2.15772993\n",
      "Iteration 67668, loss = 1.71935456\n",
      "Iteration 67669, loss = 1.62186652\n",
      "Iteration 67670, loss = 1.27701179\n",
      "Iteration 67671, loss = 1.19325207\n",
      "Iteration 67672, loss = 1.29557188\n",
      "Iteration 67673, loss = 1.31699794\n",
      "Iteration 67674, loss = 1.31311611\n",
      "Iteration 67675, loss = 1.25116289\n",
      "Iteration 67676, loss = 1.60952097\n",
      "Iteration 67677, loss = 1.62883664\n",
      "Iteration 67678, loss = 1.58918336\n",
      "Iteration 67679, loss = 1.68191110\n",
      "Iteration 67680, loss = 1.42333902\n",
      "Iteration 67681, loss = 1.36205613\n",
      "Iteration 67682, loss = 1.44083786\n",
      "Iteration 67683, loss = 1.91419318\n",
      "Iteration 67684, loss = 1.79383611\n",
      "Iteration 67685, loss = 1.76364304\n",
      "Iteration 67686, loss = 1.52554220\n",
      "Iteration 67687, loss = 1.68695237\n",
      "Iteration 67688, loss = 1.75758005\n",
      "Iteration 67689, loss = 1.38509795\n",
      "Iteration 67690, loss = 1.34564569\n",
      "Iteration 67691, loss = 1.34474327\n",
      "Iteration 67692, loss = 1.40608947\n",
      "Iteration 67693, loss = 1.12060378\n",
      "Iteration 67694, loss = 1.02374951\n",
      "Iteration 67695, loss = 1.12172535\n",
      "Iteration 67696, loss = 1.32760693\n",
      "Iteration 67697, loss = 1.13972355\n",
      "Iteration 67698, loss = 1.24585490\n",
      "Iteration 67699, loss = 1.18230758\n",
      "Iteration 67700, loss = 1.25386076\n",
      "Iteration 67701, loss = 1.53389263\n",
      "Iteration 67702, loss = 1.51224946\n",
      "Iteration 67703, loss = 1.67416923\n",
      "Iteration 67704, loss = 1.58246403\n",
      "Iteration 67705, loss = 1.50289655\n",
      "Iteration 67706, loss = 1.65873682\n",
      "Iteration 67707, loss = 1.33627026\n",
      "Iteration 67708, loss = 1.27314457\n",
      "Iteration 67709, loss = 1.62505855\n",
      "Iteration 67710, loss = 1.26575310\n",
      "Iteration 67711, loss = 1.34812150\n",
      "Iteration 67712, loss = 1.46280235\n",
      "Iteration 67713, loss = 1.34271290\n",
      "Iteration 67714, loss = 1.44053104\n",
      "Iteration 67715, loss = 1.33486551\n",
      "Iteration 67716, loss = 1.29355616\n",
      "Iteration 67717, loss = 1.75286632\n",
      "Iteration 67718, loss = 1.68658653\n",
      "Iteration 67719, loss = 2.08026782\n",
      "Iteration 67720, loss = 1.79307476\n",
      "Iteration 67721, loss = 1.50107415\n",
      "Iteration 67722, loss = 1.47386172\n",
      "Iteration 67723, loss = 1.49947212\n",
      "Iteration 67724, loss = 1.72565449\n",
      "Iteration 67725, loss = 1.69874515\n",
      "Iteration 67726, loss = 1.66634917\n",
      "Iteration 67727, loss = 1.58244523\n",
      "Iteration 67728, loss = 1.10611203\n",
      "Iteration 67729, loss = 1.21282289\n",
      "Iteration 67730, loss = 1.49997777\n",
      "Iteration 67731, loss = 1.74274083\n",
      "Iteration 67732, loss = 2.23543679\n",
      "Iteration 67733, loss = 2.09991513\n",
      "Iteration 67734, loss = 1.98323209\n",
      "Iteration 67735, loss = 2.22038353\n",
      "Iteration 67736, loss = 2.13529593\n",
      "Iteration 67737, loss = 2.67991656\n",
      "Iteration 67738, loss = 2.40015112\n",
      "Iteration 67739, loss = 2.56567566\n",
      "Iteration 67740, loss = 1.75546138\n",
      "Iteration 67741, loss = 1.41397754\n",
      "Iteration 67742, loss = 1.49369793\n",
      "Iteration 67743, loss = 1.32494110\n",
      "Iteration 67744, loss = 1.17285734\n",
      "Iteration 67745, loss = 1.17187687\n",
      "Iteration 67746, loss = 1.42443234\n",
      "Iteration 67747, loss = 1.39386640\n",
      "Iteration 67748, loss = 1.32458103\n",
      "Iteration 67749, loss = 1.36446989\n",
      "Iteration 67750, loss = 1.31783533\n",
      "Iteration 67751, loss = 1.28260709\n",
      "Iteration 67752, loss = 1.15204342\n",
      "Iteration 67753, loss = 1.13085956\n",
      "Iteration 67754, loss = 1.21868473\n",
      "Iteration 67755, loss = 1.27804274\n",
      "Iteration 67756, loss = 1.38186275\n",
      "Iteration 67757, loss = 1.16275960\n",
      "Iteration 67758, loss = 1.25877902\n",
      "Iteration 67759, loss = 1.23017786\n",
      "Iteration 67760, loss = 1.35068753\n",
      "Iteration 67761, loss = 1.29925026\n",
      "Iteration 67762, loss = 1.45812080\n",
      "Iteration 67763, loss = 1.10059463\n",
      "Iteration 67764, loss = 1.17946986\n",
      "Iteration 67765, loss = 1.33645176\n",
      "Iteration 67766, loss = 1.25388722\n",
      "Iteration 67767, loss = 1.25340683\n",
      "Iteration 67768, loss = 1.23780709\n",
      "Iteration 67769, loss = 1.25712169\n",
      "Iteration 67770, loss = 1.56293287\n",
      "Iteration 67771, loss = 1.94972924\n",
      "Iteration 67772, loss = 2.21343102\n",
      "Iteration 67773, loss = 1.78638351\n",
      "Iteration 67774, loss = 2.07988430\n",
      "Iteration 67775, loss = 1.70925598\n",
      "Iteration 67776, loss = 1.47485695\n",
      "Iteration 67777, loss = 1.18327715\n",
      "Iteration 67778, loss = 1.17426412\n",
      "Iteration 67779, loss = 1.18270017\n",
      "Iteration 67780, loss = 1.09571728\n",
      "Iteration 67781, loss = 1.11668403\n",
      "Iteration 67782, loss = 1.08820902\n",
      "Iteration 67783, loss = 1.18509562\n",
      "Iteration 67784, loss = 1.03388867\n",
      "Iteration 67785, loss = 1.08420217\n",
      "Iteration 67786, loss = 1.20585986\n",
      "Iteration 67787, loss = 1.22639652\n",
      "Iteration 67788, loss = 1.80067051\n",
      "Iteration 67789, loss = 1.94538718\n",
      "Iteration 67790, loss = 1.83982800\n",
      "Iteration 67791, loss = 1.35026410\n",
      "Iteration 67792, loss = 1.47040773\n",
      "Iteration 67793, loss = 1.26932070\n",
      "Iteration 67794, loss = 1.48486234\n",
      "Iteration 67795, loss = 1.36637655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 67796, loss = 1.21697991\n",
      "Iteration 67797, loss = 1.34133708\n",
      "Iteration 67798, loss = 1.32936870\n",
      "Iteration 67799, loss = 1.69615244\n",
      "Iteration 67800, loss = 1.74533021\n",
      "Iteration 67801, loss = 1.57681539\n",
      "Iteration 67802, loss = 1.51949492\n",
      "Iteration 67803, loss = 1.39275074\n",
      "Iteration 67804, loss = 1.29946804\n",
      "Iteration 67805, loss = 1.35017583\n",
      "Iteration 67806, loss = 1.31688481\n",
      "Iteration 67807, loss = 1.54635517\n",
      "Iteration 67808, loss = 1.84754564\n",
      "Iteration 67809, loss = 1.82556006\n",
      "Iteration 67810, loss = 1.48367336\n",
      "Iteration 67811, loss = 1.28545673\n",
      "Iteration 67812, loss = 1.13457358\n",
      "Iteration 67813, loss = 1.22656405\n",
      "Iteration 67814, loss = 1.08280947\n",
      "Iteration 67815, loss = 1.39762356\n",
      "Iteration 67816, loss = 1.28256184\n",
      "Iteration 67817, loss = 1.57822025\n",
      "Iteration 67818, loss = 1.63232347\n",
      "Iteration 67819, loss = 1.93070054\n",
      "Iteration 67820, loss = 1.72938605\n",
      "Iteration 67821, loss = 2.26987300\n",
      "Iteration 67822, loss = 2.16072571\n",
      "Iteration 67823, loss = 1.93764483\n",
      "Iteration 67824, loss = 1.50922753\n",
      "Iteration 67825, loss = 1.34195728\n",
      "Iteration 67826, loss = 1.99817984\n",
      "Iteration 67827, loss = 1.38448956\n",
      "Iteration 67828, loss = 1.31568094\n",
      "Iteration 67829, loss = 1.49002530\n",
      "Iteration 67830, loss = 1.28908547\n",
      "Iteration 67831, loss = 1.28235206\n",
      "Iteration 67832, loss = 1.32194170\n",
      "Iteration 67833, loss = 1.51093831\n",
      "Iteration 67834, loss = 1.82941056\n",
      "Iteration 67835, loss = 1.36129769\n",
      "Iteration 67836, loss = 1.38806477\n",
      "Iteration 67837, loss = 1.19483405\n",
      "Iteration 67838, loss = 1.24827759\n",
      "Iteration 67839, loss = 1.19314192\n",
      "Iteration 67840, loss = 1.32896706\n",
      "Iteration 67841, loss = 1.30991146\n",
      "Iteration 67842, loss = 1.24790882\n",
      "Iteration 67843, loss = 1.41276775\n",
      "Iteration 67844, loss = 1.43107422\n",
      "Iteration 67845, loss = 1.12805327\n",
      "Iteration 67846, loss = 1.17352297\n",
      "Iteration 67847, loss = 1.11386942\n",
      "Iteration 67848, loss = 1.24424607\n",
      "Iteration 67849, loss = 1.31171071\n",
      "Iteration 67850, loss = 1.37935968\n",
      "Iteration 67851, loss = 1.17755151\n",
      "Iteration 67852, loss = 1.05720418\n",
      "Iteration 67853, loss = 1.39682700\n",
      "Iteration 67854, loss = 1.45593846\n",
      "Iteration 67855, loss = 1.71154358\n",
      "Iteration 67856, loss = 2.15848346\n",
      "Iteration 67857, loss = 1.55317440\n",
      "Iteration 67858, loss = 1.56951982\n",
      "Iteration 67859, loss = 1.39057247\n",
      "Iteration 67860, loss = 1.29146036\n",
      "Iteration 67861, loss = 1.28062151\n",
      "Iteration 67862, loss = 1.07689633\n",
      "Iteration 67863, loss = 1.07266522\n",
      "Iteration 67864, loss = 1.14195323\n",
      "Iteration 67865, loss = 1.13770233\n",
      "Iteration 67866, loss = 1.45587443\n",
      "Iteration 67867, loss = 1.70022203\n",
      "Iteration 67868, loss = 1.71695452\n",
      "Iteration 67869, loss = 1.30084141\n",
      "Iteration 67870, loss = 1.31013959\n",
      "Iteration 67871, loss = 1.49885849\n",
      "Iteration 67872, loss = 1.36099440\n",
      "Iteration 67873, loss = 1.22259726\n",
      "Iteration 67874, loss = 1.39376650\n",
      "Iteration 67875, loss = 1.31527374\n",
      "Iteration 67876, loss = 1.10002413\n",
      "Iteration 67877, loss = 1.15252066\n",
      "Iteration 67878, loss = 1.40993842\n",
      "Iteration 67879, loss = 1.31460806\n",
      "Iteration 67880, loss = 1.39165267\n",
      "Iteration 67881, loss = 1.62218453\n",
      "Iteration 67882, loss = 1.44934615\n",
      "Iteration 67883, loss = 1.13904772\n",
      "Iteration 67884, loss = 1.10618837\n",
      "Iteration 67885, loss = 1.11158660\n",
      "Iteration 67886, loss = 1.20519089\n",
      "Iteration 67887, loss = 1.12143108\n",
      "Iteration 67888, loss = 1.15115329\n",
      "Iteration 67889, loss = 1.21746942\n",
      "Iteration 67890, loss = 1.16336579\n",
      "Iteration 67891, loss = 1.23853594\n",
      "Iteration 67892, loss = 1.46166249\n",
      "Iteration 67893, loss = 1.48221951\n",
      "Iteration 67894, loss = 1.50692244\n",
      "Iteration 67895, loss = 1.26152119\n",
      "Iteration 67896, loss = 1.47634406\n",
      "Iteration 67897, loss = 1.49448823\n",
      "Iteration 67898, loss = 1.59871220\n",
      "Iteration 67899, loss = 2.22785939\n",
      "Iteration 67900, loss = 2.18539888\n",
      "Iteration 67901, loss = 2.83725195\n",
      "Iteration 67902, loss = 2.55806201\n",
      "Iteration 67903, loss = 1.95008361\n",
      "Iteration 67904, loss = 1.94408517\n",
      "Iteration 67905, loss = 1.88307180\n",
      "Iteration 67906, loss = 2.18551080\n",
      "Iteration 67907, loss = 2.17800813\n",
      "Iteration 67908, loss = 1.65744979\n",
      "Iteration 67909, loss = 1.33535996\n",
      "Iteration 67910, loss = 1.38140085\n",
      "Iteration 67911, loss = 1.21724803\n",
      "Iteration 67912, loss = 1.26473620\n",
      "Iteration 67913, loss = 1.26351963\n",
      "Iteration 67914, loss = 1.31957365\n",
      "Iteration 67915, loss = 1.30121564\n",
      "Iteration 67916, loss = 1.31162643\n",
      "Iteration 67917, loss = 1.30430993\n",
      "Iteration 67918, loss = 1.21620547\n",
      "Iteration 67919, loss = 1.60404776\n",
      "Iteration 67920, loss = 1.53695435\n",
      "Iteration 67921, loss = 2.15878629\n",
      "Iteration 67922, loss = 1.78384371\n",
      "Iteration 67923, loss = 1.99686575\n",
      "Iteration 67924, loss = 1.71455232\n",
      "Iteration 67925, loss = 1.34968512\n",
      "Iteration 67926, loss = 1.60119143\n",
      "Iteration 67927, loss = 1.28679000\n",
      "Iteration 67928, loss = 1.27366119\n",
      "Iteration 67929, loss = 1.60503359\n",
      "Iteration 67930, loss = 1.47723998\n",
      "Iteration 67931, loss = 1.68890223\n",
      "Iteration 67932, loss = 1.44561872\n",
      "Iteration 67933, loss = 1.58280918\n",
      "Iteration 67934, loss = 1.31502681\n",
      "Iteration 67935, loss = 1.39326224\n",
      "Iteration 67936, loss = 1.25454995\n",
      "Iteration 67937, loss = 1.37585194\n",
      "Iteration 67938, loss = 1.87020183\n",
      "Iteration 67939, loss = 2.10286202\n",
      "Iteration 67940, loss = 2.07076080\n",
      "Iteration 67941, loss = 2.84470180\n",
      "Iteration 67942, loss = 4.90265284\n",
      "Iteration 67943, loss = 4.88166092\n",
      "Iteration 67944, loss = 4.04657614\n",
      "Iteration 67945, loss = 3.35447171\n",
      "Iteration 67946, loss = 2.34349527\n",
      "Iteration 67947, loss = 1.98956147\n",
      "Iteration 67948, loss = 2.03015119\n",
      "Iteration 67949, loss = 1.77506621\n",
      "Iteration 67950, loss = 1.43191112\n",
      "Iteration 67951, loss = 1.36384148\n",
      "Iteration 67952, loss = 1.16960786\n",
      "Iteration 67953, loss = 1.23996002\n",
      "Iteration 67954, loss = 1.39263331\n",
      "Iteration 67955, loss = 1.46459321\n",
      "Iteration 67956, loss = 1.31273186\n",
      "Iteration 67957, loss = 1.33832408\n",
      "Iteration 67958, loss = 1.25621693\n",
      "Iteration 67959, loss = 1.29080842\n",
      "Iteration 67960, loss = 1.31259531\n",
      "Iteration 67961, loss = 1.32882968\n",
      "Iteration 67962, loss = 1.73772213\n",
      "Iteration 67963, loss = 1.57235816\n",
      "Iteration 67964, loss = 1.53563927\n",
      "Iteration 67965, loss = 1.33473327\n",
      "Iteration 67966, loss = 1.34810058\n",
      "Iteration 67967, loss = 1.20627018\n",
      "Iteration 67968, loss = 1.11342073\n",
      "Iteration 67969, loss = 1.28418982\n",
      "Iteration 67970, loss = 1.33443419\n",
      "Iteration 67971, loss = 1.25841548\n",
      "Iteration 67972, loss = 1.24590753\n",
      "Iteration 67973, loss = 1.27853531\n",
      "Iteration 67974, loss = 1.18306088\n",
      "Iteration 67975, loss = 1.15433298\n",
      "Iteration 67976, loss = 1.06840698\n",
      "Iteration 67977, loss = 1.19376556\n",
      "Iteration 67978, loss = 1.30855164\n",
      "Iteration 67979, loss = 1.31986185\n",
      "Iteration 67980, loss = 1.23509243\n",
      "Iteration 67981, loss = 1.36246505\n",
      "Iteration 67982, loss = 1.14603004\n",
      "Iteration 67983, loss = 1.05787117\n",
      "Iteration 67984, loss = 1.06885824\n",
      "Iteration 67985, loss = 1.07832268\n",
      "Iteration 67986, loss = 1.06385193\n",
      "Iteration 67987, loss = 1.30843979\n",
      "Iteration 67988, loss = 1.23969002\n",
      "Iteration 67989, loss = 1.09308650\n",
      "Iteration 67990, loss = 1.23901088\n",
      "Iteration 67991, loss = 1.18202182\n",
      "Iteration 67992, loss = 1.38849630\n",
      "Iteration 67993, loss = 1.29246516\n",
      "Iteration 67994, loss = 1.55572729\n",
      "Iteration 67995, loss = 1.37700971\n",
      "Iteration 67996, loss = 1.22226469\n",
      "Iteration 67997, loss = 1.50252737\n",
      "Iteration 67998, loss = 1.68601215\n",
      "Iteration 67999, loss = 1.25199161\n",
      "Iteration 68000, loss = 1.37284863\n",
      "Iteration 68001, loss = 1.64417779\n",
      "Iteration 68002, loss = 1.44292522\n",
      "Iteration 68003, loss = 1.30665465\n",
      "Iteration 68004, loss = 1.36101507\n",
      "Iteration 68005, loss = 1.50306886\n",
      "Iteration 68006, loss = 1.67745715\n",
      "Iteration 68007, loss = 1.25174805\n",
      "Iteration 68008, loss = 1.37704358\n",
      "Iteration 68009, loss = 1.42024267\n",
      "Iteration 68010, loss = 1.53403558\n",
      "Iteration 68011, loss = 1.36051667\n",
      "Iteration 68012, loss = 1.49903836\n",
      "Iteration 68013, loss = 1.49776721\n",
      "Iteration 68014, loss = 1.51770724\n",
      "Iteration 68015, loss = 1.28006678\n",
      "Iteration 68016, loss = 1.25014771\n",
      "Iteration 68017, loss = 1.22462150\n",
      "Iteration 68018, loss = 1.27289049\n",
      "Iteration 68019, loss = 1.49966595\n",
      "Iteration 68020, loss = 1.31176827\n",
      "Iteration 68021, loss = 1.24771180\n",
      "Iteration 68022, loss = 1.24127640\n",
      "Iteration 68023, loss = 1.23973003\n",
      "Iteration 68024, loss = 1.33685639\n",
      "Iteration 68025, loss = 1.61774993\n",
      "Iteration 68026, loss = 1.94585407\n",
      "Iteration 68027, loss = 1.99132059\n",
      "Iteration 68028, loss = 1.44705239\n",
      "Iteration 68029, loss = 1.29634405\n",
      "Iteration 68030, loss = 1.40098554\n",
      "Iteration 68031, loss = 1.15097183\n",
      "Iteration 68032, loss = 1.20773145\n",
      "Iteration 68033, loss = 1.16094061\n",
      "Iteration 68034, loss = 1.17568976\n",
      "Iteration 68035, loss = 1.19872361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 68036, loss = 1.15118480\n",
      "Iteration 68037, loss = 1.16415512\n",
      "Iteration 68038, loss = 1.15825683\n",
      "Iteration 68039, loss = 1.21288424\n",
      "Iteration 68040, loss = 1.28464947\n",
      "Iteration 68041, loss = 1.37180186\n",
      "Iteration 68042, loss = 1.51777992\n",
      "Iteration 68043, loss = 1.31177066\n",
      "Iteration 68044, loss = 1.26076941\n",
      "Iteration 68045, loss = 1.17631960\n",
      "Iteration 68046, loss = 1.20230506\n",
      "Iteration 68047, loss = 1.29820640\n",
      "Iteration 68048, loss = 1.20256323\n",
      "Iteration 68049, loss = 1.25500344\n",
      "Iteration 68050, loss = 1.12803949\n",
      "Iteration 68051, loss = 1.10163262\n",
      "Iteration 68052, loss = 1.12998120\n",
      "Iteration 68053, loss = 1.16200982\n",
      "Iteration 68054, loss = 1.15608892\n",
      "Iteration 68055, loss = 1.39164409\n",
      "Iteration 68056, loss = 1.24503396\n",
      "Iteration 68057, loss = 1.58517588\n",
      "Iteration 68058, loss = 1.43029282\n",
      "Iteration 68059, loss = 1.34307531\n",
      "Iteration 68060, loss = 1.33770439\n",
      "Iteration 68061, loss = 1.53308676\n",
      "Iteration 68062, loss = 1.27088726\n",
      "Iteration 68063, loss = 1.29668540\n",
      "Iteration 68064, loss = 1.30604196\n",
      "Iteration 68065, loss = 1.55621652\n",
      "Iteration 68066, loss = 1.15265994\n",
      "Iteration 68067, loss = 1.45191759\n",
      "Iteration 68068, loss = 1.77929141\n",
      "Iteration 68069, loss = 1.90464643\n",
      "Iteration 68070, loss = 1.55128154\n",
      "Iteration 68071, loss = 1.37896618\n",
      "Iteration 68072, loss = 1.17795314\n",
      "Iteration 68073, loss = 1.29441030\n",
      "Iteration 68074, loss = 1.25309479\n",
      "Iteration 68075, loss = 1.16947408\n",
      "Iteration 68076, loss = 1.26865462\n",
      "Iteration 68077, loss = 1.45051830\n",
      "Iteration 68078, loss = 1.94324922\n",
      "Iteration 68079, loss = 1.75381832\n",
      "Iteration 68080, loss = 2.05645654\n",
      "Iteration 68081, loss = 1.88135001\n",
      "Iteration 68082, loss = 1.66371915\n",
      "Iteration 68083, loss = 1.40823171\n",
      "Iteration 68084, loss = 1.51900126\n",
      "Iteration 68085, loss = 1.34290750\n",
      "Iteration 68086, loss = 1.38593633\n",
      "Iteration 68087, loss = 1.22679891\n",
      "Iteration 68088, loss = 1.32751030\n",
      "Iteration 68089, loss = 1.30026308\n",
      "Iteration 68090, loss = 1.30961950\n",
      "Iteration 68091, loss = 1.66327803\n",
      "Iteration 68092, loss = 2.06707422\n",
      "Iteration 68093, loss = 1.98371031\n",
      "Iteration 68094, loss = 2.25986858\n",
      "Iteration 68095, loss = 2.24798259\n",
      "Iteration 68096, loss = 2.46169178\n",
      "Iteration 68097, loss = 1.90277099\n",
      "Iteration 68098, loss = 2.07378037\n",
      "Iteration 68099, loss = 1.75677292\n",
      "Iteration 68100, loss = 2.02276772\n",
      "Iteration 68101, loss = 2.63723446\n",
      "Iteration 68102, loss = 2.48556495\n",
      "Iteration 68103, loss = 1.88157959\n",
      "Iteration 68104, loss = 2.01606628\n",
      "Iteration 68105, loss = 1.46082156\n",
      "Iteration 68106, loss = 2.37179331\n",
      "Iteration 68107, loss = 2.00560563\n",
      "Iteration 68108, loss = 1.28729640\n",
      "Iteration 68109, loss = 1.37743105\n",
      "Iteration 68110, loss = 1.26756144\n",
      "Iteration 68111, loss = 1.45934328\n",
      "Iteration 68112, loss = 1.28981415\n",
      "Iteration 68113, loss = 1.29997191\n",
      "Iteration 68114, loss = 1.12211036\n",
      "Iteration 68115, loss = 1.18878778\n",
      "Iteration 68116, loss = 1.14830340\n",
      "Iteration 68117, loss = 1.10914448\n",
      "Iteration 68118, loss = 1.44887302\n",
      "Iteration 68119, loss = 1.24805853\n",
      "Iteration 68120, loss = 1.44570498\n",
      "Iteration 68121, loss = 1.89076501\n",
      "Iteration 68122, loss = 1.66338553\n",
      "Iteration 68123, loss = 1.37798145\n",
      "Iteration 68124, loss = 1.33234760\n",
      "Iteration 68125, loss = 1.72489012\n",
      "Iteration 68126, loss = 1.64754484\n",
      "Iteration 68127, loss = 1.83580688\n",
      "Iteration 68128, loss = 1.23804145\n",
      "Iteration 68129, loss = 1.23440515\n",
      "Iteration 68130, loss = 1.17704623\n",
      "Iteration 68131, loss = 1.14356011\n",
      "Iteration 68132, loss = 1.06659184\n",
      "Iteration 68133, loss = 1.17128390\n",
      "Iteration 68134, loss = 1.16846257\n",
      "Iteration 68135, loss = 1.12648068\n",
      "Iteration 68136, loss = 1.24435805\n",
      "Iteration 68137, loss = 1.85224783\n",
      "Iteration 68138, loss = 1.85562401\n",
      "Iteration 68139, loss = 1.58195243\n",
      "Iteration 68140, loss = 1.31697205\n",
      "Iteration 68141, loss = 1.27179150\n",
      "Iteration 68142, loss = 1.33459156\n",
      "Iteration 68143, loss = 1.37421195\n",
      "Iteration 68144, loss = 1.52681210\n",
      "Iteration 68145, loss = 1.43759493\n",
      "Iteration 68146, loss = 1.37939627\n",
      "Iteration 68147, loss = 1.41256397\n",
      "Iteration 68148, loss = 1.18542347\n",
      "Iteration 68149, loss = 1.16442874\n",
      "Iteration 68150, loss = 1.21365279\n",
      "Iteration 68151, loss = 1.25013005\n",
      "Iteration 68152, loss = 1.40117798\n",
      "Iteration 68153, loss = 1.57644300\n",
      "Iteration 68154, loss = 1.44116735\n",
      "Iteration 68155, loss = 1.23163909\n",
      "Iteration 68156, loss = 1.38642589\n",
      "Iteration 68157, loss = 1.21652444\n",
      "Iteration 68158, loss = 1.12836612\n",
      "Iteration 68159, loss = 1.25485822\n",
      "Iteration 68160, loss = 1.28421203\n",
      "Iteration 68161, loss = 1.63241197\n",
      "Iteration 68162, loss = 1.47844830\n",
      "Iteration 68163, loss = 1.82437241\n",
      "Iteration 68164, loss = 1.69092976\n",
      "Iteration 68165, loss = 1.41702838\n",
      "Iteration 68166, loss = 2.58843830\n",
      "Iteration 68167, loss = 2.66138084\n",
      "Iteration 68168, loss = 2.83157310\n",
      "Iteration 68169, loss = 2.32482238\n",
      "Iteration 68170, loss = 2.55706861\n",
      "Iteration 68171, loss = 2.11129250\n",
      "Iteration 68172, loss = 2.14168469\n",
      "Iteration 68173, loss = 2.05218466\n",
      "Iteration 68174, loss = 1.96618864\n",
      "Iteration 68175, loss = 1.65197550\n",
      "Iteration 68176, loss = 1.67026287\n",
      "Iteration 68177, loss = 1.60699247\n",
      "Iteration 68178, loss = 1.43575136\n",
      "Iteration 68179, loss = 1.40529733\n",
      "Iteration 68180, loss = 1.11822900\n",
      "Iteration 68181, loss = 1.72417806\n",
      "Iteration 68182, loss = 1.98724072\n",
      "Iteration 68183, loss = 1.46965666\n",
      "Iteration 68184, loss = 1.28850997\n",
      "Iteration 68185, loss = 1.20428295\n",
      "Iteration 68186, loss = 1.13084031\n",
      "Iteration 68187, loss = 1.08226703\n",
      "Iteration 68188, loss = 1.16939912\n",
      "Iteration 68189, loss = 1.10493049\n",
      "Iteration 68190, loss = 1.31624928\n",
      "Iteration 68191, loss = 1.27243518\n",
      "Iteration 68192, loss = 1.17630272\n",
      "Iteration 68193, loss = 1.44269913\n",
      "Iteration 68194, loss = 1.33731870\n",
      "Iteration 68195, loss = 1.32103528\n",
      "Iteration 68196, loss = 1.82023503\n",
      "Iteration 68197, loss = 1.85374309\n",
      "Iteration 68198, loss = 1.75953836\n",
      "Iteration 68199, loss = 1.38091694\n",
      "Iteration 68200, loss = 1.36881146\n",
      "Iteration 68201, loss = 1.47926120\n",
      "Iteration 68202, loss = 1.67257078\n",
      "Iteration 68203, loss = 1.48060301\n",
      "Iteration 68204, loss = 1.22760653\n",
      "Iteration 68205, loss = 1.22693879\n",
      "Iteration 68206, loss = 1.24202955\n",
      "Iteration 68207, loss = 1.22076520\n",
      "Iteration 68208, loss = 1.10533947\n",
      "Iteration 68209, loss = 1.12830052\n",
      "Iteration 68210, loss = 1.18694592\n",
      "Iteration 68211, loss = 1.19551720\n",
      "Iteration 68212, loss = 1.54762760\n",
      "Iteration 68213, loss = 1.51289744\n",
      "Iteration 68214, loss = 1.75725452\n",
      "Iteration 68215, loss = 1.65051060\n",
      "Iteration 68216, loss = 1.16380434\n",
      "Iteration 68217, loss = 1.49917974\n",
      "Iteration 68218, loss = 1.60903560\n",
      "Iteration 68219, loss = 1.56046791\n",
      "Iteration 68220, loss = 1.40685854\n",
      "Iteration 68221, loss = 1.38518556\n",
      "Iteration 68222, loss = 1.25035041\n",
      "Iteration 68223, loss = 1.28421948\n",
      "Iteration 68224, loss = 1.47132119\n",
      "Iteration 68225, loss = 1.38606314\n",
      "Iteration 68226, loss = 1.33135021\n",
      "Iteration 68227, loss = 1.08910585\n",
      "Iteration 68228, loss = 1.16375352\n",
      "Iteration 68229, loss = 1.11574490\n",
      "Iteration 68230, loss = 1.14716195\n",
      "Iteration 68231, loss = 1.12512869\n",
      "Iteration 68232, loss = 1.59795140\n",
      "Iteration 68233, loss = 1.30368152\n",
      "Iteration 68234, loss = 1.50062268\n",
      "Iteration 68235, loss = 1.27510186\n",
      "Iteration 68236, loss = 1.08252762\n",
      "Iteration 68237, loss = 1.12622339\n",
      "Iteration 68238, loss = 1.23054913\n",
      "Iteration 68239, loss = 1.13132193\n",
      "Iteration 68240, loss = 1.47528090\n",
      "Iteration 68241, loss = 1.54570157\n",
      "Iteration 68242, loss = 1.38726466\n",
      "Iteration 68243, loss = 1.48201582\n",
      "Iteration 68244, loss = 1.30936938\n",
      "Iteration 68245, loss = 1.21701865\n",
      "Iteration 68246, loss = 1.30218420\n",
      "Iteration 68247, loss = 1.24604358\n",
      "Iteration 68248, loss = 1.14222741\n",
      "Iteration 68249, loss = 1.13507411\n",
      "Iteration 68250, loss = 1.11230326\n",
      "Iteration 68251, loss = 1.19470175\n",
      "Iteration 68252, loss = 1.26713149\n",
      "Iteration 68253, loss = 1.21458275\n",
      "Iteration 68254, loss = 1.14148690\n",
      "Iteration 68255, loss = 1.03835036\n",
      "Iteration 68256, loss = 1.18363471\n",
      "Iteration 68257, loss = 1.40082650\n",
      "Iteration 68258, loss = 1.50984923\n",
      "Iteration 68259, loss = 1.92295425\n",
      "Iteration 68260, loss = 1.75439214\n",
      "Iteration 68261, loss = 1.33180545\n",
      "Iteration 68262, loss = 1.33320760\n",
      "Iteration 68263, loss = 1.49617677\n",
      "Iteration 68264, loss = 1.49339759\n",
      "Iteration 68265, loss = 1.54757762\n",
      "Iteration 68266, loss = 1.53829587\n",
      "Iteration 68267, loss = 1.17800879\n",
      "Iteration 68268, loss = 1.24100613\n",
      "Iteration 68269, loss = 1.21530485\n",
      "Iteration 68270, loss = 1.35404070\n",
      "Iteration 68271, loss = 1.12614197\n",
      "Iteration 68272, loss = 1.17992875\n",
      "Iteration 68273, loss = 1.15529881\n",
      "Iteration 68274, loss = 1.31841911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 68275, loss = 1.30361022\n",
      "Iteration 68276, loss = 1.29501704\n",
      "Iteration 68277, loss = 1.60854132\n",
      "Iteration 68278, loss = 1.48900692\n",
      "Iteration 68279, loss = 1.30564030\n",
      "Iteration 68280, loss = 1.21691287\n",
      "Iteration 68281, loss = 1.48901352\n",
      "Iteration 68282, loss = 1.47732569\n",
      "Iteration 68283, loss = 1.27994233\n",
      "Iteration 68284, loss = 1.45733516\n",
      "Iteration 68285, loss = 1.43933488\n",
      "Iteration 68286, loss = 1.13202424\n",
      "Iteration 68287, loss = 1.19279761\n",
      "Iteration 68288, loss = 1.35196277\n",
      "Iteration 68289, loss = 1.73454381\n",
      "Iteration 68290, loss = 2.20629714\n",
      "Iteration 68291, loss = 2.25259714\n",
      "Iteration 68292, loss = 2.21500280\n",
      "Iteration 68293, loss = 2.05023093\n",
      "Iteration 68294, loss = 2.23341580\n",
      "Iteration 68295, loss = 1.64789793\n",
      "Iteration 68296, loss = 1.46034056\n",
      "Iteration 68297, loss = 1.42407561\n",
      "Iteration 68298, loss = 1.28666481\n",
      "Iteration 68299, loss = 1.39566283\n",
      "Iteration 68300, loss = 1.57164339\n",
      "Iteration 68301, loss = 1.33392078\n",
      "Iteration 68302, loss = 1.42153288\n",
      "Iteration 68303, loss = 1.15219799\n",
      "Iteration 68304, loss = 1.12918627\n",
      "Iteration 68305, loss = 1.33344360\n",
      "Iteration 68306, loss = 1.26857065\n",
      "Iteration 68307, loss = 1.27784340\n",
      "Iteration 68308, loss = 1.32511799\n",
      "Iteration 68309, loss = 1.16144407\n",
      "Iteration 68310, loss = 1.07612041\n",
      "Iteration 68311, loss = 1.36691640\n",
      "Iteration 68312, loss = 1.34808803\n",
      "Iteration 68313, loss = 1.60739255\n",
      "Iteration 68314, loss = 1.49694813\n",
      "Iteration 68315, loss = 1.67234518\n",
      "Iteration 68316, loss = 1.89042032\n",
      "Iteration 68317, loss = 1.78771341\n",
      "Iteration 68318, loss = 1.40731193\n",
      "Iteration 68319, loss = 1.42286446\n",
      "Iteration 68320, loss = 1.58438703\n",
      "Iteration 68321, loss = 1.62260774\n",
      "Iteration 68322, loss = 1.31373978\n",
      "Iteration 68323, loss = 1.16722345\n",
      "Iteration 68324, loss = 1.23482242\n",
      "Iteration 68325, loss = 1.25039118\n",
      "Iteration 68326, loss = 1.39478939\n",
      "Iteration 68327, loss = 1.53955178\n",
      "Iteration 68328, loss = 1.81513332\n",
      "Iteration 68329, loss = 1.66381880\n",
      "Iteration 68330, loss = 1.48968129\n",
      "Iteration 68331, loss = 1.25905626\n",
      "Iteration 68332, loss = 1.10127597\n",
      "Iteration 68333, loss = 1.16440657\n",
      "Iteration 68334, loss = 1.04934428\n",
      "Iteration 68335, loss = 1.24132293\n",
      "Iteration 68336, loss = 1.14045830\n",
      "Iteration 68337, loss = 1.31667702\n",
      "Iteration 68338, loss = 1.30800082\n",
      "Iteration 68339, loss = 1.09597184\n",
      "Iteration 68340, loss = 1.09253374\n",
      "Iteration 68341, loss = 1.25614272\n",
      "Iteration 68342, loss = 1.28333030\n",
      "Iteration 68343, loss = 1.35330067\n",
      "Iteration 68344, loss = 1.43872653\n",
      "Iteration 68345, loss = 1.50745815\n",
      "Iteration 68346, loss = 1.42961542\n",
      "Iteration 68347, loss = 1.44554216\n",
      "Iteration 68348, loss = 1.54172967\n",
      "Iteration 68349, loss = 1.43205511\n",
      "Iteration 68350, loss = 1.64332951\n",
      "Iteration 68351, loss = 1.72102782\n",
      "Iteration 68352, loss = 1.31491418\n",
      "Iteration 68353, loss = 1.22544860\n",
      "Iteration 68354, loss = 1.26351841\n",
      "Iteration 68355, loss = 1.48322191\n",
      "Iteration 68356, loss = 1.68731863\n",
      "Iteration 68357, loss = 1.54836036\n",
      "Iteration 68358, loss = 1.69000080\n",
      "Iteration 68359, loss = 1.48790293\n",
      "Iteration 68360, loss = 1.43650460\n",
      "Iteration 68361, loss = 1.17284975\n",
      "Iteration 68362, loss = 1.19319599\n",
      "Iteration 68363, loss = 1.08209410\n",
      "Iteration 68364, loss = 1.23507331\n",
      "Iteration 68365, loss = 1.28870476\n",
      "Iteration 68366, loss = 1.30185357\n",
      "Iteration 68367, loss = 1.14245432\n",
      "Iteration 68368, loss = 1.16793498\n",
      "Iteration 68369, loss = 1.44375057\n",
      "Iteration 68370, loss = 1.30349679\n",
      "Iteration 68371, loss = 1.36991727\n",
      "Iteration 68372, loss = 1.43215969\n",
      "Iteration 68373, loss = 1.44227972\n",
      "Iteration 68374, loss = 1.49117412\n",
      "Iteration 68375, loss = 1.89441354\n",
      "Iteration 68376, loss = 1.54362605\n",
      "Iteration 68377, loss = 2.59679200\n",
      "Iteration 68378, loss = 1.91663470\n",
      "Iteration 68379, loss = 1.75043034\n",
      "Iteration 68380, loss = 1.42138282\n",
      "Iteration 68381, loss = 1.71349815\n",
      "Iteration 68382, loss = 1.98090729\n",
      "Iteration 68383, loss = 1.43385600\n",
      "Iteration 68384, loss = 1.40325108\n",
      "Iteration 68385, loss = 1.58791684\n",
      "Iteration 68386, loss = 1.79326303\n",
      "Iteration 68387, loss = 1.29468411\n",
      "Iteration 68388, loss = 1.21712118\n",
      "Iteration 68389, loss = 1.28492841\n",
      "Iteration 68390, loss = 1.27403334\n",
      "Iteration 68391, loss = 1.14276666\n",
      "Iteration 68392, loss = 1.15097803\n",
      "Iteration 68393, loss = 1.39407995\n",
      "Iteration 68394, loss = 1.44209313\n",
      "Iteration 68395, loss = 1.67507432\n",
      "Iteration 68396, loss = 1.53166191\n",
      "Iteration 68397, loss = 1.28801152\n",
      "Iteration 68398, loss = 1.33649836\n",
      "Iteration 68399, loss = 1.27809815\n",
      "Iteration 68400, loss = 1.59940213\n",
      "Iteration 68401, loss = 1.44225592\n",
      "Iteration 68402, loss = 1.22328353\n",
      "Iteration 68403, loss = 1.61741099\n",
      "Iteration 68404, loss = 1.60670007\n",
      "Iteration 68405, loss = 1.85776734\n",
      "Iteration 68406, loss = 1.96237045\n",
      "Iteration 68407, loss = 1.70994636\n",
      "Iteration 68408, loss = 1.48512783\n",
      "Iteration 68409, loss = 1.53602579\n",
      "Iteration 68410, loss = 1.95677074\n",
      "Iteration 68411, loss = 1.87100027\n",
      "Iteration 68412, loss = 1.93051172\n",
      "Iteration 68413, loss = 2.43859170\n",
      "Iteration 68414, loss = 1.92502059\n",
      "Iteration 68415, loss = 1.61012952\n",
      "Iteration 68416, loss = 1.35085262\n",
      "Iteration 68417, loss = 1.31944662\n",
      "Iteration 68418, loss = 1.25736415\n",
      "Iteration 68419, loss = 1.35786413\n",
      "Iteration 68420, loss = 1.35691370\n",
      "Iteration 68421, loss = 1.36742945\n",
      "Iteration 68422, loss = 1.23476469\n",
      "Iteration 68423, loss = 1.27850266\n",
      "Iteration 68424, loss = 1.31129258\n",
      "Iteration 68425, loss = 1.25677746\n",
      "Iteration 68426, loss = 1.14824347\n",
      "Iteration 68427, loss = 1.15402682\n",
      "Iteration 68428, loss = 1.23020307\n",
      "Iteration 68429, loss = 1.06850749\n",
      "Iteration 68430, loss = 1.08563268\n",
      "Iteration 68431, loss = 1.03425809\n",
      "Iteration 68432, loss = 1.13087433\n",
      "Iteration 68433, loss = 1.26144321\n",
      "Iteration 68434, loss = 1.13850451\n",
      "Iteration 68435, loss = 1.35448934\n",
      "Iteration 68436, loss = 1.34606552\n",
      "Iteration 68437, loss = 1.44574280\n",
      "Iteration 68438, loss = 1.16248978\n",
      "Iteration 68439, loss = 1.22844679\n",
      "Iteration 68440, loss = 1.31756084\n",
      "Iteration 68441, loss = 1.28883365\n",
      "Iteration 68442, loss = 1.71346079\n",
      "Iteration 68443, loss = 1.56454624\n",
      "Iteration 68444, loss = 1.94673486\n",
      "Iteration 68445, loss = 1.62421325\n",
      "Iteration 68446, loss = 1.45032907\n",
      "Iteration 68447, loss = 1.13352479\n",
      "Iteration 68448, loss = 1.17368776\n",
      "Iteration 68449, loss = 1.09754812\n",
      "Iteration 68450, loss = 1.15639755\n",
      "Iteration 68451, loss = 1.73933743\n",
      "Iteration 68452, loss = 1.80926929\n",
      "Iteration 68453, loss = 1.87197304\n",
      "Iteration 68454, loss = 1.55375487\n",
      "Iteration 68455, loss = 1.68174763\n",
      "Iteration 68456, loss = 1.30722447\n",
      "Iteration 68457, loss = 1.46519047\n",
      "Iteration 68458, loss = 1.55337369\n",
      "Iteration 68459, loss = 1.81789704\n",
      "Iteration 68460, loss = 1.67452608\n",
      "Iteration 68461, loss = 1.99907953\n",
      "Iteration 68462, loss = 1.88389064\n",
      "Iteration 68463, loss = 1.68923797\n",
      "Iteration 68464, loss = 1.52356590\n",
      "Iteration 68465, loss = 1.33629152\n",
      "Iteration 68466, loss = 1.33099686\n",
      "Iteration 68467, loss = 1.47242364\n",
      "Iteration 68468, loss = 1.81975454\n",
      "Iteration 68469, loss = 1.72621778\n",
      "Iteration 68470, loss = 1.39458470\n",
      "Iteration 68471, loss = 1.33043792\n",
      "Iteration 68472, loss = 1.32825327\n",
      "Iteration 68473, loss = 1.33575299\n",
      "Iteration 68474, loss = 1.20045198\n",
      "Iteration 68475, loss = 1.49531920\n",
      "Iteration 68476, loss = 1.69784092\n",
      "Iteration 68477, loss = 1.44567576\n",
      "Iteration 68478, loss = 1.72540185\n",
      "Iteration 68479, loss = 1.40512026\n",
      "Iteration 68480, loss = 1.35950077\n",
      "Iteration 68481, loss = 1.08510469\n",
      "Iteration 68482, loss = 1.14968664\n",
      "Iteration 68483, loss = 1.56242097\n",
      "Iteration 68484, loss = 1.33637208\n",
      "Iteration 68485, loss = 1.40904562\n",
      "Iteration 68486, loss = 1.52206776\n",
      "Iteration 68487, loss = 2.05828805\n",
      "Iteration 68488, loss = 1.65531165\n",
      "Iteration 68489, loss = 1.30136825\n",
      "Iteration 68490, loss = 1.36910680\n",
      "Iteration 68491, loss = 1.19104305\n",
      "Iteration 68492, loss = 1.39239451\n",
      "Iteration 68493, loss = 1.21121455\n",
      "Iteration 68494, loss = 1.07450490\n",
      "Iteration 68495, loss = 1.08193192\n",
      "Iteration 68496, loss = 1.08412435\n",
      "Iteration 68497, loss = 1.17839400\n",
      "Iteration 68498, loss = 1.11697304\n",
      "Iteration 68499, loss = 1.14071941\n",
      "Iteration 68500, loss = 1.14587667\n",
      "Iteration 68501, loss = 1.12784671\n",
      "Iteration 68502, loss = 1.16859120\n",
      "Iteration 68503, loss = 1.36804766\n",
      "Iteration 68504, loss = 1.22458755\n",
      "Iteration 68505, loss = 1.13240206\n",
      "Iteration 68506, loss = 1.28191701\n",
      "Iteration 68507, loss = 1.29639418\n",
      "Iteration 68508, loss = 1.30349664\n",
      "Iteration 68509, loss = 1.44228157\n",
      "Iteration 68510, loss = 1.33402925\n",
      "Iteration 68511, loss = 1.28896112\n",
      "Iteration 68512, loss = 1.45299901\n",
      "Iteration 68513, loss = 1.64439216\n",
      "Iteration 68514, loss = 1.63552672\n",
      "Iteration 68515, loss = 1.49087528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 68516, loss = 1.36948609\n",
      "Iteration 68517, loss = 1.41025750\n",
      "Iteration 68518, loss = 1.23501866\n",
      "Iteration 68519, loss = 1.05458843\n",
      "Iteration 68520, loss = 1.20982227\n",
      "Iteration 68521, loss = 1.28120848\n",
      "Iteration 68522, loss = 1.29674945\n",
      "Iteration 68523, loss = 1.28677602\n",
      "Iteration 68524, loss = 1.24208828\n",
      "Iteration 68525, loss = 1.42996769\n",
      "Iteration 68526, loss = 1.62940184\n",
      "Iteration 68527, loss = 1.52198422\n",
      "Iteration 68528, loss = 1.71984149\n",
      "Iteration 68529, loss = 1.50035258\n",
      "Iteration 68530, loss = 1.57932203\n",
      "Iteration 68531, loss = 1.67439533\n",
      "Iteration 68532, loss = 1.39500452\n",
      "Iteration 68533, loss = 1.33397708\n",
      "Iteration 68534, loss = 1.22699409\n",
      "Iteration 68535, loss = 1.35324641\n",
      "Iteration 68536, loss = 1.20265229\n",
      "Iteration 68537, loss = 1.08883830\n",
      "Iteration 68538, loss = 1.31974611\n",
      "Iteration 68539, loss = 1.16241315\n",
      "Iteration 68540, loss = 1.56346067\n",
      "Iteration 68541, loss = 2.16482785\n",
      "Iteration 68542, loss = 1.86278447\n",
      "Iteration 68543, loss = 1.90777721\n",
      "Iteration 68544, loss = 1.45516744\n",
      "Iteration 68545, loss = 1.50322227\n",
      "Iteration 68546, loss = 1.53303275\n",
      "Iteration 68547, loss = 1.38153007\n",
      "Iteration 68548, loss = 1.58403139\n",
      "Iteration 68549, loss = 1.97861252\n",
      "Iteration 68550, loss = 2.74219787\n",
      "Iteration 68551, loss = 2.47560996\n",
      "Iteration 68552, loss = 1.94736139\n",
      "Iteration 68553, loss = 1.95568086\n",
      "Iteration 68554, loss = 2.23987782\n",
      "Iteration 68555, loss = 2.90916825\n",
      "Iteration 68556, loss = 2.05286238\n",
      "Iteration 68557, loss = 1.78095779\n",
      "Iteration 68558, loss = 1.80478253\n",
      "Iteration 68559, loss = 1.73212994\n",
      "Iteration 68560, loss = 1.60395277\n",
      "Iteration 68561, loss = 1.24036324\n",
      "Iteration 68562, loss = 1.32852132\n",
      "Iteration 68563, loss = 1.40223145\n",
      "Iteration 68564, loss = 2.02886229\n",
      "Iteration 68565, loss = 1.80244573\n",
      "Iteration 68566, loss = 1.41140406\n",
      "Iteration 68567, loss = 1.32242721\n",
      "Iteration 68568, loss = 1.20377976\n",
      "Iteration 68569, loss = 1.08458224\n",
      "Iteration 68570, loss = 1.09916021\n",
      "Iteration 68571, loss = 1.09954123\n",
      "Iteration 68572, loss = 1.05999173\n",
      "Iteration 68573, loss = 1.05160138\n",
      "Iteration 68574, loss = 1.21956070\n",
      "Iteration 68575, loss = 1.37955322\n",
      "Iteration 68576, loss = 1.51035867\n",
      "Iteration 68577, loss = 1.54343309\n",
      "Iteration 68578, loss = 1.32628100\n",
      "Iteration 68579, loss = 1.36957417\n",
      "Iteration 68580, loss = 1.22736909\n",
      "Iteration 68581, loss = 1.18318342\n",
      "Iteration 68582, loss = 1.04274596\n",
      "Iteration 68583, loss = 1.00656232\n",
      "Iteration 68584, loss = 1.34376142\n",
      "Iteration 68585, loss = 1.34450099\n",
      "Iteration 68586, loss = 1.41382392\n",
      "Iteration 68587, loss = 1.34224142\n",
      "Iteration 68588, loss = 1.44088275\n",
      "Iteration 68589, loss = 1.57665835\n",
      "Iteration 68590, loss = 1.46155246\n",
      "Iteration 68591, loss = 1.26753823\n",
      "Iteration 68592, loss = 1.26312382\n",
      "Iteration 68593, loss = 1.16389605\n",
      "Iteration 68594, loss = 1.10793231\n",
      "Iteration 68595, loss = 1.16023341\n",
      "Iteration 68596, loss = 1.15997732\n",
      "Iteration 68597, loss = 1.27860541\n",
      "Iteration 68598, loss = 1.53609971\n",
      "Iteration 68599, loss = 1.23247719\n",
      "Iteration 68600, loss = 1.16095515\n",
      "Iteration 68601, loss = 1.29608739\n",
      "Iteration 68602, loss = 1.30700092\n",
      "Iteration 68603, loss = 1.18087811\n",
      "Iteration 68604, loss = 1.25301080\n",
      "Iteration 68605, loss = 1.42564043\n",
      "Iteration 68606, loss = 1.49395827\n",
      "Iteration 68607, loss = 1.49903775\n",
      "Iteration 68608, loss = 1.77480956\n",
      "Iteration 68609, loss = 1.72153605\n",
      "Iteration 68610, loss = 2.03855632\n",
      "Iteration 68611, loss = 1.91104276\n",
      "Iteration 68612, loss = 1.81363066\n",
      "Iteration 68613, loss = 2.05218530\n",
      "Iteration 68614, loss = 1.55897131\n",
      "Iteration 68615, loss = 1.49070925\n",
      "Iteration 68616, loss = 1.35853570\n",
      "Iteration 68617, loss = 1.29677116\n",
      "Iteration 68618, loss = 1.43192989\n",
      "Iteration 68619, loss = 1.23710000\n",
      "Iteration 68620, loss = 1.41870607\n",
      "Iteration 68621, loss = 1.72896892\n",
      "Iteration 68622, loss = 1.67089458\n",
      "Iteration 68623, loss = 1.78223542\n",
      "Iteration 68624, loss = 1.42032887\n",
      "Iteration 68625, loss = 1.70314761\n",
      "Iteration 68626, loss = 2.00661768\n",
      "Iteration 68627, loss = 1.84852815\n",
      "Iteration 68628, loss = 1.74327657\n",
      "Iteration 68629, loss = 1.73544235\n",
      "Iteration 68630, loss = 1.37986416\n",
      "Iteration 68631, loss = 1.61450767\n",
      "Iteration 68632, loss = 1.38902789\n",
      "Iteration 68633, loss = 1.19785341\n",
      "Iteration 68634, loss = 1.72314195\n",
      "Iteration 68635, loss = 1.71513888\n",
      "Iteration 68636, loss = 1.31256592\n",
      "Iteration 68637, loss = 1.42919186\n",
      "Iteration 68638, loss = 2.24982329\n",
      "Iteration 68639, loss = 1.60527337\n",
      "Iteration 68640, loss = 1.75305801\n",
      "Iteration 68641, loss = 1.71171896\n",
      "Iteration 68642, loss = 1.51545682\n",
      "Iteration 68643, loss = 1.42360450\n",
      "Iteration 68644, loss = 1.44335804\n",
      "Iteration 68645, loss = 1.68313354\n",
      "Iteration 68646, loss = 1.47173581\n",
      "Iteration 68647, loss = 1.85020578\n",
      "Iteration 68648, loss = 1.64475407\n",
      "Iteration 68649, loss = 1.48114562\n",
      "Iteration 68650, loss = 1.31646958\n",
      "Iteration 68651, loss = 1.54629135\n",
      "Iteration 68652, loss = 1.69616903\n",
      "Iteration 68653, loss = 2.46771325\n",
      "Iteration 68654, loss = 1.72032217\n",
      "Iteration 68655, loss = 2.09975737\n",
      "Iteration 68656, loss = 2.16080846\n",
      "Iteration 68657, loss = 2.19587538\n",
      "Iteration 68658, loss = 2.17615636\n",
      "Iteration 68659, loss = 1.83709498\n",
      "Iteration 68660, loss = 1.52369023\n",
      "Iteration 68661, loss = 1.95022504\n",
      "Iteration 68662, loss = 1.50740778\n",
      "Iteration 68663, loss = 1.35321037\n",
      "Iteration 68664, loss = 1.26518768\n",
      "Iteration 68665, loss = 1.10389895\n",
      "Iteration 68666, loss = 1.37697320\n",
      "Iteration 68667, loss = 1.18972962\n",
      "Iteration 68668, loss = 1.17451604\n",
      "Iteration 68669, loss = 1.15954607\n",
      "Iteration 68670, loss = 1.31900026\n",
      "Iteration 68671, loss = 1.62160499\n",
      "Iteration 68672, loss = 1.50431825\n",
      "Iteration 68673, loss = 1.24157266\n",
      "Iteration 68674, loss = 1.37011162\n",
      "Iteration 68675, loss = 1.41694824\n",
      "Iteration 68676, loss = 1.54667038\n",
      "Iteration 68677, loss = 1.68731663\n",
      "Iteration 68678, loss = 1.42362494\n",
      "Iteration 68679, loss = 1.61187413\n",
      "Iteration 68680, loss = 1.54202952\n",
      "Iteration 68681, loss = 1.60520410\n",
      "Iteration 68682, loss = 1.39803561\n",
      "Iteration 68683, loss = 1.40243918\n",
      "Iteration 68684, loss = 1.16451247\n",
      "Iteration 68685, loss = 1.05163999\n",
      "Iteration 68686, loss = 1.12861549\n",
      "Iteration 68687, loss = 1.15020712\n",
      "Iteration 68688, loss = 1.19686865\n",
      "Iteration 68689, loss = 1.45718607\n",
      "Iteration 68690, loss = 1.45458467\n",
      "Iteration 68691, loss = 1.67058441\n",
      "Iteration 68692, loss = 1.58477712\n",
      "Iteration 68693, loss = 1.55374631\n",
      "Iteration 68694, loss = 1.29371200\n",
      "Iteration 68695, loss = 1.35668338\n",
      "Iteration 68696, loss = 1.49279808\n",
      "Iteration 68697, loss = 1.75449848\n",
      "Iteration 68698, loss = 1.30779452\n",
      "Iteration 68699, loss = 1.32042840\n",
      "Iteration 68700, loss = 1.24065903\n",
      "Iteration 68701, loss = 1.27098900\n",
      "Iteration 68702, loss = 1.31532020\n",
      "Iteration 68703, loss = 1.23220086\n",
      "Iteration 68704, loss = 1.34933571\n",
      "Iteration 68705, loss = 1.43994338\n",
      "Iteration 68706, loss = 1.20312372\n",
      "Iteration 68707, loss = 1.20648802\n",
      "Iteration 68708, loss = 1.19149856\n",
      "Iteration 68709, loss = 1.11065911\n",
      "Iteration 68710, loss = 1.15597488\n",
      "Iteration 68711, loss = 1.12044170\n",
      "Iteration 68712, loss = 1.06312101\n",
      "Iteration 68713, loss = 1.07453327\n",
      "Iteration 68714, loss = 1.14007798\n",
      "Iteration 68715, loss = 1.11220185\n",
      "Iteration 68716, loss = 1.06290150\n",
      "Iteration 68717, loss = 1.04762434\n",
      "Iteration 68718, loss = 1.12338414\n",
      "Iteration 68719, loss = 1.16721182\n",
      "Iteration 68720, loss = 1.21189021\n",
      "Iteration 68721, loss = 1.34940198\n",
      "Iteration 68722, loss = 1.26204010\n",
      "Iteration 68723, loss = 1.31102449\n",
      "Iteration 68724, loss = 1.22453946\n",
      "Iteration 68725, loss = 1.24148455\n",
      "Iteration 68726, loss = 1.10736952\n",
      "Iteration 68727, loss = 1.17831751\n",
      "Iteration 68728, loss = 1.16738542\n",
      "Iteration 68729, loss = 1.09676499\n",
      "Iteration 68730, loss = 1.37572862\n",
      "Iteration 68731, loss = 1.69588331\n",
      "Iteration 68732, loss = 1.26090205\n",
      "Iteration 68733, loss = 1.37589658\n",
      "Iteration 68734, loss = 1.79405490\n",
      "Iteration 68735, loss = 1.59556873\n",
      "Iteration 68736, loss = 1.27820374\n",
      "Iteration 68737, loss = 1.21954498\n",
      "Iteration 68738, loss = 1.38234810\n",
      "Iteration 68739, loss = 1.12239845\n",
      "Iteration 68740, loss = 1.10071158\n",
      "Iteration 68741, loss = 1.17520952\n",
      "Iteration 68742, loss = 1.20972116\n",
      "Iteration 68743, loss = 1.18480628\n",
      "Iteration 68744, loss = 1.57143419\n",
      "Iteration 68745, loss = 1.58109776\n",
      "Iteration 68746, loss = 1.45425266\n",
      "Iteration 68747, loss = 1.63663720\n",
      "Iteration 68748, loss = 1.39557056\n",
      "Iteration 68749, loss = 1.43432202\n",
      "Iteration 68750, loss = 1.44934258\n",
      "Iteration 68751, loss = 1.36614631\n",
      "Iteration 68752, loss = 1.16763526\n",
      "Iteration 68753, loss = 1.20738565\n",
      "Iteration 68754, loss = 1.31164651\n",
      "Iteration 68755, loss = 1.38697063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 68756, loss = 1.38542608\n",
      "Iteration 68757, loss = 2.06267387\n",
      "Iteration 68758, loss = 1.93670971\n",
      "Iteration 68759, loss = 1.78292017\n",
      "Iteration 68760, loss = 1.51273638\n",
      "Iteration 68761, loss = 1.94236303\n",
      "Iteration 68762, loss = 1.33643989\n",
      "Iteration 68763, loss = 1.23523454\n",
      "Iteration 68764, loss = 1.20352845\n",
      "Iteration 68765, loss = 1.28281025\n",
      "Iteration 68766, loss = 1.41511781\n",
      "Iteration 68767, loss = 2.25958932\n",
      "Iteration 68768, loss = 2.81811846\n",
      "Iteration 68769, loss = 2.48607690\n",
      "Iteration 68770, loss = 3.65086104\n",
      "Iteration 68771, loss = 2.63928808\n",
      "Iteration 68772, loss = 2.36577420\n",
      "Iteration 68773, loss = 1.90712677\n",
      "Iteration 68774, loss = 1.80093749\n",
      "Iteration 68775, loss = 1.38144919\n",
      "Iteration 68776, loss = 1.33847573\n",
      "Iteration 68777, loss = 1.26105656\n",
      "Iteration 68778, loss = 1.24641367\n",
      "Iteration 68779, loss = 1.38530230\n",
      "Iteration 68780, loss = 1.29748268\n",
      "Iteration 68781, loss = 1.60434368\n",
      "Iteration 68782, loss = 1.16901030\n",
      "Iteration 68783, loss = 1.23539789\n",
      "Iteration 68784, loss = 1.19157124\n",
      "Iteration 68785, loss = 1.29970559\n",
      "Iteration 68786, loss = 1.24313907\n",
      "Iteration 68787, loss = 1.14879963\n",
      "Iteration 68788, loss = 1.18291742\n",
      "Iteration 68789, loss = 1.24775316\n",
      "Iteration 68790, loss = 1.31506740\n",
      "Iteration 68791, loss = 1.34380110\n",
      "Iteration 68792, loss = 1.53674181\n",
      "Iteration 68793, loss = 1.26659154\n",
      "Iteration 68794, loss = 1.09327083\n",
      "Iteration 68795, loss = 1.07533964\n",
      "Iteration 68796, loss = 1.03883926\n",
      "Iteration 68797, loss = 1.08843530\n",
      "Iteration 68798, loss = 1.16129057\n",
      "Iteration 68799, loss = 1.12892189\n",
      "Iteration 68800, loss = 1.08496434\n",
      "Iteration 68801, loss = 1.09343267\n",
      "Iteration 68802, loss = 1.19089306\n",
      "Iteration 68803, loss = 1.34263211\n",
      "Iteration 68804, loss = 1.81781209\n",
      "Iteration 68805, loss = 1.23867381\n",
      "Iteration 68806, loss = 1.18895615\n",
      "Iteration 68807, loss = 1.08845171\n",
      "Iteration 68808, loss = 1.11872538\n",
      "Iteration 68809, loss = 1.28653435\n",
      "Iteration 68810, loss = 1.13364017\n",
      "Iteration 68811, loss = 1.43751968\n",
      "Iteration 68812, loss = 1.65673616\n",
      "Iteration 68813, loss = 1.67871929\n",
      "Iteration 68814, loss = 1.59553677\n",
      "Iteration 68815, loss = 1.66515103\n",
      "Iteration 68816, loss = 1.46347610\n",
      "Iteration 68817, loss = 1.70094590\n",
      "Iteration 68818, loss = 1.56405469\n",
      "Iteration 68819, loss = 2.00647442\n",
      "Iteration 68820, loss = 1.91209955\n",
      "Iteration 68821, loss = 2.15754119\n",
      "Iteration 68822, loss = 1.73733088\n",
      "Iteration 68823, loss = 1.48340529\n",
      "Iteration 68824, loss = 1.42505586\n",
      "Iteration 68825, loss = 1.27643814\n",
      "Iteration 68826, loss = 1.41360084\n",
      "Iteration 68827, loss = 1.56409314\n",
      "Iteration 68828, loss = 1.20781969\n",
      "Iteration 68829, loss = 1.22518914\n",
      "Iteration 68830, loss = 1.28856785\n",
      "Iteration 68831, loss = 1.34912546\n",
      "Iteration 68832, loss = 1.38375072\n",
      "Iteration 68833, loss = 1.05737938\n",
      "Iteration 68834, loss = 1.25070310\n",
      "Iteration 68835, loss = 1.17868380\n",
      "Iteration 68836, loss = 1.33489437\n",
      "Iteration 68837, loss = 1.27156508\n",
      "Iteration 68838, loss = 1.18726927\n",
      "Iteration 68839, loss = 1.01094631\n",
      "Iteration 68840, loss = 1.07309134\n",
      "Iteration 68841, loss = 1.16846265\n",
      "Iteration 68842, loss = 1.11403083\n",
      "Iteration 68843, loss = 1.23170700\n",
      "Iteration 68844, loss = 1.31591547\n",
      "Iteration 68845, loss = 1.52406019\n",
      "Iteration 68846, loss = 1.29942249\n",
      "Iteration 68847, loss = 1.34397636\n",
      "Iteration 68848, loss = 1.38339625\n",
      "Iteration 68849, loss = 1.59988585\n",
      "Iteration 68850, loss = 1.39544881\n",
      "Iteration 68851, loss = 1.25186344\n",
      "Iteration 68852, loss = 1.21469342\n",
      "Iteration 68853, loss = 1.18931714\n",
      "Iteration 68854, loss = 1.37797236\n",
      "Iteration 68855, loss = 1.21075409\n",
      "Iteration 68856, loss = 1.47815344\n",
      "Iteration 68857, loss = 1.76642730\n",
      "Iteration 68858, loss = 2.03536217\n",
      "Iteration 68859, loss = 2.18401088\n",
      "Iteration 68860, loss = 1.71422688\n",
      "Iteration 68861, loss = 1.71964426\n",
      "Iteration 68862, loss = 1.33269580\n",
      "Iteration 68863, loss = 1.44768555\n",
      "Iteration 68864, loss = 1.16087903\n",
      "Iteration 68865, loss = 0.98416194\n",
      "Iteration 68866, loss = 1.07964447\n",
      "Iteration 68867, loss = 1.10384415\n",
      "Iteration 68868, loss = 1.25761934\n",
      "Iteration 68869, loss = 1.30808788\n",
      "Iteration 68870, loss = 1.10771474\n",
      "Iteration 68871, loss = 1.25569995\n",
      "Iteration 68872, loss = 1.54488469\n",
      "Iteration 68873, loss = 1.75724520\n",
      "Iteration 68874, loss = 1.56979357\n",
      "Iteration 68875, loss = 1.27571786\n",
      "Iteration 68876, loss = 1.43569620\n",
      "Iteration 68877, loss = 1.55939248\n",
      "Iteration 68878, loss = 1.46895688\n",
      "Iteration 68879, loss = 1.49589855\n",
      "Iteration 68880, loss = 1.56033524\n",
      "Iteration 68881, loss = 1.37221648\n",
      "Iteration 68882, loss = 1.48247258\n",
      "Iteration 68883, loss = 1.29724562\n",
      "Iteration 68884, loss = 1.60990391\n",
      "Iteration 68885, loss = 1.58495618\n",
      "Iteration 68886, loss = 1.84701062\n",
      "Iteration 68887, loss = 1.73820299\n",
      "Iteration 68888, loss = 1.76701859\n",
      "Iteration 68889, loss = 1.61639068\n",
      "Iteration 68890, loss = 1.55725035\n",
      "Iteration 68891, loss = 1.36056883\n",
      "Iteration 68892, loss = 1.25297468\n",
      "Iteration 68893, loss = 1.35048804\n",
      "Iteration 68894, loss = 1.69500990\n",
      "Iteration 68895, loss = 1.54289153\n",
      "Iteration 68896, loss = 1.49683087\n",
      "Iteration 68897, loss = 1.16026136\n",
      "Iteration 68898, loss = 1.29241314\n",
      "Iteration 68899, loss = 1.16492611\n",
      "Iteration 68900, loss = 1.14235226\n",
      "Iteration 68901, loss = 1.03712600\n",
      "Iteration 68902, loss = 1.21020827\n",
      "Iteration 68903, loss = 1.10475105\n",
      "Iteration 68904, loss = 1.05652690\n",
      "Iteration 68905, loss = 1.04742745\n",
      "Iteration 68906, loss = 1.11793563\n",
      "Iteration 68907, loss = 1.12382581\n",
      "Iteration 68908, loss = 1.23785577\n",
      "Iteration 68909, loss = 1.07628166\n",
      "Iteration 68910, loss = 1.18493045\n",
      "Iteration 68911, loss = 1.19083354\n",
      "Iteration 68912, loss = 1.35203179\n",
      "Iteration 68913, loss = 1.33315462\n",
      "Iteration 68914, loss = 1.87668534\n",
      "Iteration 68915, loss = 1.71397628\n",
      "Iteration 68916, loss = 1.27703925\n",
      "Iteration 68917, loss = 1.12449222\n",
      "Iteration 68918, loss = 1.08492901\n",
      "Iteration 68919, loss = 0.99554252\n",
      "Iteration 68920, loss = 1.13353340\n",
      "Iteration 68921, loss = 1.23153636\n",
      "Iteration 68922, loss = 1.10980705\n",
      "Iteration 68923, loss = 1.18002150\n",
      "Iteration 68924, loss = 1.19673318\n",
      "Iteration 68925, loss = 1.84778691\n",
      "Iteration 68926, loss = 1.38208651\n",
      "Iteration 68927, loss = 1.37503655\n",
      "Iteration 68928, loss = 1.59119764\n",
      "Iteration 68929, loss = 1.76313436\n",
      "Iteration 68930, loss = 1.52441206\n",
      "Iteration 68931, loss = 1.37905793\n",
      "Iteration 68932, loss = 1.59520892\n",
      "Iteration 68933, loss = 1.86554218\n",
      "Iteration 68934, loss = 1.53987373\n",
      "Iteration 68935, loss = 2.15900267\n",
      "Iteration 68936, loss = 1.80279060\n",
      "Iteration 68937, loss = 1.62443707\n",
      "Iteration 68938, loss = 1.72658115\n",
      "Iteration 68939, loss = 1.44865455\n",
      "Iteration 68940, loss = 1.40416626\n",
      "Iteration 68941, loss = 1.48183081\n",
      "Iteration 68942, loss = 1.24765896\n",
      "Iteration 68943, loss = 1.29098785\n",
      "Iteration 68944, loss = 1.40362086\n",
      "Iteration 68945, loss = 1.49236348\n",
      "Iteration 68946, loss = 2.12303766\n",
      "Iteration 68947, loss = 2.64278819\n",
      "Iteration 68948, loss = 2.05295683\n",
      "Iteration 68949, loss = 1.37609743\n",
      "Iteration 68950, loss = 1.31404311\n",
      "Iteration 68951, loss = 1.31421646\n",
      "Iteration 68952, loss = 1.23002660\n",
      "Iteration 68953, loss = 1.34436289\n",
      "Iteration 68954, loss = 1.27800442\n",
      "Iteration 68955, loss = 1.32555958\n",
      "Iteration 68956, loss = 1.13762400\n",
      "Iteration 68957, loss = 1.12762462\n",
      "Iteration 68958, loss = 1.52592641\n",
      "Iteration 68959, loss = 1.39127710\n",
      "Iteration 68960, loss = 1.26667883\n",
      "Iteration 68961, loss = 1.22048787\n",
      "Iteration 68962, loss = 1.28456424\n",
      "Iteration 68963, loss = 1.58966456\n",
      "Iteration 68964, loss = 1.46674087\n",
      "Iteration 68965, loss = 1.65621748\n",
      "Iteration 68966, loss = 1.48306108\n",
      "Iteration 68967, loss = 1.54245922\n",
      "Iteration 68968, loss = 1.36253694\n",
      "Iteration 68969, loss = 1.21486911\n",
      "Iteration 68970, loss = 1.64569116\n",
      "Iteration 68971, loss = 1.47209222\n",
      "Iteration 68972, loss = 1.42898863\n",
      "Iteration 68973, loss = 1.53410024\n",
      "Iteration 68974, loss = 1.63720059\n",
      "Iteration 68975, loss = 3.29785820\n",
      "Iteration 68976, loss = 2.86521058\n",
      "Iteration 68977, loss = 2.87297857\n",
      "Iteration 68978, loss = 1.74871693\n",
      "Iteration 68979, loss = 1.40259351\n",
      "Iteration 68980, loss = 1.45454237\n",
      "Iteration 68981, loss = 1.45220461\n",
      "Iteration 68982, loss = 1.37269157\n",
      "Iteration 68983, loss = 1.22280729\n",
      "Iteration 68984, loss = 1.47985084\n",
      "Iteration 68985, loss = 1.62949830\n",
      "Iteration 68986, loss = 1.73962276\n",
      "Iteration 68987, loss = 1.73713722\n",
      "Iteration 68988, loss = 1.47632540\n",
      "Iteration 68989, loss = 1.58442342\n",
      "Iteration 68990, loss = 1.53125661\n",
      "Iteration 68991, loss = 1.34517392\n",
      "Iteration 68992, loss = 1.30756283\n",
      "Iteration 68993, loss = 1.44812353\n",
      "Iteration 68994, loss = 1.43432559\n",
      "Iteration 68995, loss = 1.14260159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 68996, loss = 1.08428084\n",
      "Iteration 68997, loss = 1.29508303\n",
      "Iteration 68998, loss = 1.50625076\n",
      "Iteration 68999, loss = 1.93106488\n",
      "Iteration 69000, loss = 1.62004837\n",
      "Iteration 69001, loss = 1.23553553\n",
      "Iteration 69002, loss = 1.36433238\n",
      "Iteration 69003, loss = 1.60883386\n",
      "Iteration 69004, loss = 1.34107810\n",
      "Iteration 69005, loss = 1.45649922\n",
      "Iteration 69006, loss = 1.09585050\n",
      "Iteration 69007, loss = 1.25030053\n",
      "Iteration 69008, loss = 1.41477034\n",
      "Iteration 69009, loss = 1.42663085\n",
      "Iteration 69010, loss = 1.30723695\n",
      "Iteration 69011, loss = 1.34500238\n",
      "Iteration 69012, loss = 1.12631968\n",
      "Iteration 69013, loss = 1.08309324\n",
      "Iteration 69014, loss = 1.22244833\n",
      "Iteration 69015, loss = 1.11894912\n",
      "Iteration 69016, loss = 1.16507216\n",
      "Iteration 69017, loss = 1.13124140\n",
      "Iteration 69018, loss = 1.12228795\n",
      "Iteration 69019, loss = 1.20532742\n",
      "Iteration 69020, loss = 1.43191313\n",
      "Iteration 69021, loss = 1.61270770\n",
      "Iteration 69022, loss = 1.65922901\n",
      "Iteration 69023, loss = 1.60946416\n",
      "Iteration 69024, loss = 1.78645744\n",
      "Iteration 69025, loss = 1.50413012\n",
      "Iteration 69026, loss = 1.27172183\n",
      "Iteration 69027, loss = 1.56677041\n",
      "Iteration 69028, loss = 1.38648195\n",
      "Iteration 69029, loss = 1.85716139\n",
      "Iteration 69030, loss = 1.73522232\n",
      "Iteration 69031, loss = 1.77414105\n",
      "Iteration 69032, loss = 1.72267665\n",
      "Iteration 69033, loss = 2.01896398\n",
      "Iteration 69034, loss = 1.76810323\n",
      "Iteration 69035, loss = 1.56874997\n",
      "Iteration 69036, loss = 1.77591617\n",
      "Iteration 69037, loss = 1.65514998\n",
      "Iteration 69038, loss = 1.21443358\n",
      "Iteration 69039, loss = 1.84630575\n",
      "Iteration 69040, loss = 1.63762930\n",
      "Iteration 69041, loss = 1.40663383\n",
      "Iteration 69042, loss = 1.57893600\n",
      "Iteration 69043, loss = 2.12983404\n",
      "Iteration 69044, loss = 1.43781088\n",
      "Iteration 69045, loss = 1.32419201\n",
      "Iteration 69046, loss = 1.40022571\n",
      "Iteration 69047, loss = 1.12201565\n",
      "Iteration 69048, loss = 1.22428975\n",
      "Iteration 69049, loss = 1.23867230\n",
      "Iteration 69050, loss = 1.41774585\n",
      "Iteration 69051, loss = 1.48796920\n",
      "Iteration 69052, loss = 1.49424768\n",
      "Iteration 69053, loss = 1.76866485\n",
      "Iteration 69054, loss = 1.95358830\n",
      "Iteration 69055, loss = 1.86137068\n",
      "Iteration 69056, loss = 3.00427241\n",
      "Iteration 69057, loss = 2.53311227\n",
      "Iteration 69058, loss = 1.85161040\n",
      "Iteration 69059, loss = 1.90309799\n",
      "Iteration 69060, loss = 1.43896303\n",
      "Iteration 69061, loss = 1.37872679\n",
      "Iteration 69062, loss = 1.38574193\n",
      "Iteration 69063, loss = 1.51961684\n",
      "Iteration 69064, loss = 1.83376919\n",
      "Iteration 69065, loss = 1.57418252\n",
      "Iteration 69066, loss = 1.47288864\n",
      "Iteration 69067, loss = 1.27215491\n",
      "Iteration 69068, loss = 1.26490415\n",
      "Iteration 69069, loss = 1.16350338\n",
      "Iteration 69070, loss = 1.21173337\n",
      "Iteration 69071, loss = 1.15137888\n",
      "Iteration 69072, loss = 1.21020282\n",
      "Iteration 69073, loss = 1.09512262\n",
      "Iteration 69074, loss = 1.21879118\n",
      "Iteration 69075, loss = 1.04275825\n",
      "Iteration 69076, loss = 1.37091597\n",
      "Iteration 69077, loss = 1.37703057\n",
      "Iteration 69078, loss = 1.50915259\n",
      "Iteration 69079, loss = 1.49643788\n",
      "Iteration 69080, loss = 1.36296282\n",
      "Iteration 69081, loss = 1.22683353\n",
      "Iteration 69082, loss = 1.11212667\n",
      "Iteration 69083, loss = 1.44898430\n",
      "Iteration 69084, loss = 1.40847932\n",
      "Iteration 69085, loss = 1.24033220\n",
      "Iteration 69086, loss = 1.07085273\n",
      "Iteration 69087, loss = 1.10221188\n",
      "Iteration 69088, loss = 1.15704829\n",
      "Iteration 69089, loss = 1.08865620\n",
      "Iteration 69090, loss = 1.03110214\n",
      "Iteration 69091, loss = 1.13476181\n",
      "Iteration 69092, loss = 1.18852246\n",
      "Iteration 69093, loss = 1.28292382\n",
      "Iteration 69094, loss = 1.26027674\n",
      "Iteration 69095, loss = 1.13903925\n",
      "Iteration 69096, loss = 1.11609637\n",
      "Iteration 69097, loss = 1.23411249\n",
      "Iteration 69098, loss = 1.24555260\n",
      "Iteration 69099, loss = 1.33858950\n",
      "Iteration 69100, loss = 1.22720161\n",
      "Iteration 69101, loss = 1.15606553\n",
      "Iteration 69102, loss = 1.11150142\n",
      "Iteration 69103, loss = 1.30902706\n",
      "Iteration 69104, loss = 1.55303187\n",
      "Iteration 69105, loss = 1.43047593\n",
      "Iteration 69106, loss = 1.40102860\n",
      "Iteration 69107, loss = 1.73150939\n",
      "Iteration 69108, loss = 1.70441494\n",
      "Iteration 69109, loss = 1.85061052\n",
      "Iteration 69110, loss = 1.84640483\n",
      "Iteration 69111, loss = 1.47554795\n",
      "Iteration 69112, loss = 1.48862935\n",
      "Iteration 69113, loss = 1.42160022\n",
      "Iteration 69114, loss = 1.13238437\n",
      "Iteration 69115, loss = 1.17684024\n",
      "Iteration 69116, loss = 1.24252546\n",
      "Iteration 69117, loss = 1.32119746\n",
      "Iteration 69118, loss = 2.30897207\n",
      "Iteration 69119, loss = 3.20320717\n",
      "Iteration 69120, loss = 2.37428785\n",
      "Iteration 69121, loss = 1.77053962\n",
      "Iteration 69122, loss = 1.50804724\n",
      "Iteration 69123, loss = 1.36838373\n",
      "Iteration 69124, loss = 1.59060776\n",
      "Iteration 69125, loss = 1.28321216\n",
      "Iteration 69126, loss = 1.21071047\n",
      "Iteration 69127, loss = 1.11404415\n",
      "Iteration 69128, loss = 1.40113705\n",
      "Iteration 69129, loss = 1.29448499\n",
      "Iteration 69130, loss = 1.21972783\n",
      "Iteration 69131, loss = 1.23762240\n",
      "Iteration 69132, loss = 1.40270833\n",
      "Iteration 69133, loss = 1.53108277\n",
      "Iteration 69134, loss = 1.34526468\n",
      "Iteration 69135, loss = 1.21844043\n",
      "Iteration 69136, loss = 1.16366725\n",
      "Iteration 69137, loss = 1.06193350\n",
      "Iteration 69138, loss = 1.10631147\n",
      "Iteration 69139, loss = 1.12430394\n",
      "Iteration 69140, loss = 1.07048036\n",
      "Iteration 69141, loss = 1.20328822\n",
      "Iteration 69142, loss = 1.35191162\n",
      "Iteration 69143, loss = 1.18797240\n",
      "Iteration 69144, loss = 1.26397543\n",
      "Iteration 69145, loss = 1.66545027\n",
      "Iteration 69146, loss = 1.68968133\n",
      "Iteration 69147, loss = 1.50765056\n",
      "Iteration 69148, loss = 1.30045403\n",
      "Iteration 69149, loss = 1.18883743\n",
      "Iteration 69150, loss = 1.17072002\n",
      "Iteration 69151, loss = 1.18364060\n",
      "Iteration 69152, loss = 1.19482786\n",
      "Iteration 69153, loss = 1.09427432\n",
      "Iteration 69154, loss = 1.16740257\n",
      "Iteration 69155, loss = 1.24203850\n",
      "Iteration 69156, loss = 1.12990286\n",
      "Iteration 69157, loss = 1.12588837\n",
      "Iteration 69158, loss = 1.45135014\n",
      "Iteration 69159, loss = 1.28599212\n",
      "Iteration 69160, loss = 1.18510435\n",
      "Iteration 69161, loss = 1.25869370\n",
      "Iteration 69162, loss = 1.09116621\n",
      "Iteration 69163, loss = 1.22158385\n",
      "Iteration 69164, loss = 1.55913012\n",
      "Iteration 69165, loss = 1.59703964\n",
      "Iteration 69166, loss = 1.78435352\n",
      "Iteration 69167, loss = 2.04118703\n",
      "Iteration 69168, loss = 1.37013988\n",
      "Iteration 69169, loss = 1.34145023\n",
      "Iteration 69170, loss = 1.44102930\n",
      "Iteration 69171, loss = 1.30741520\n",
      "Iteration 69172, loss = 1.36984081\n",
      "Iteration 69173, loss = 1.67559187\n",
      "Iteration 69174, loss = 1.44844613\n",
      "Iteration 69175, loss = 1.49956707\n",
      "Iteration 69176, loss = 2.57127459\n",
      "Iteration 69177, loss = 1.90008815\n",
      "Iteration 69178, loss = 1.58860159\n",
      "Iteration 69179, loss = 1.33427305\n",
      "Iteration 69180, loss = 1.59114575\n",
      "Iteration 69181, loss = 1.33588134\n",
      "Iteration 69182, loss = 1.49170889\n",
      "Iteration 69183, loss = 1.59891538\n",
      "Iteration 69184, loss = 1.35151799\n",
      "Iteration 69185, loss = 1.10071936\n",
      "Iteration 69186, loss = 1.19516268\n",
      "Iteration 69187, loss = 1.14320864\n",
      "Iteration 69188, loss = 1.15139966\n",
      "Iteration 69189, loss = 1.03028607\n",
      "Iteration 69190, loss = 1.05245531\n",
      "Iteration 69191, loss = 1.28917349\n",
      "Iteration 69192, loss = 1.33872020\n",
      "Iteration 69193, loss = 1.43307778\n",
      "Iteration 69194, loss = 1.42097400\n",
      "Iteration 69195, loss = 1.27270915\n",
      "Iteration 69196, loss = 1.14594075\n",
      "Iteration 69197, loss = 1.23986039\n",
      "Iteration 69198, loss = 1.27334622\n",
      "Iteration 69199, loss = 1.17027905\n",
      "Iteration 69200, loss = 1.41790430\n",
      "Iteration 69201, loss = 1.51139377\n",
      "Iteration 69202, loss = 1.31588442\n",
      "Iteration 69203, loss = 1.32716917\n",
      "Iteration 69204, loss = 1.06752907\n",
      "Iteration 69205, loss = 1.15325151\n",
      "Iteration 69206, loss = 1.14738249\n",
      "Iteration 69207, loss = 1.51046894\n",
      "Iteration 69208, loss = 1.19158759\n",
      "Iteration 69209, loss = 1.09115858\n",
      "Iteration 69210, loss = 1.10257475\n",
      "Iteration 69211, loss = 1.17882955\n",
      "Iteration 69212, loss = 1.06832839\n",
      "Iteration 69213, loss = 1.06595374\n",
      "Iteration 69214, loss = 1.10077002\n",
      "Iteration 69215, loss = 1.17571762\n",
      "Iteration 69216, loss = 1.24164484\n",
      "Iteration 69217, loss = 1.47652862\n",
      "Iteration 69218, loss = 1.26823812\n",
      "Iteration 69219, loss = 1.58723869\n",
      "Iteration 69220, loss = 1.35439215\n",
      "Iteration 69221, loss = 1.41782944\n",
      "Iteration 69222, loss = 1.21894293\n",
      "Iteration 69223, loss = 1.07767665\n",
      "Iteration 69224, loss = 1.03162198\n",
      "Iteration 69225, loss = 1.13237703\n",
      "Iteration 69226, loss = 1.16061805\n",
      "Iteration 69227, loss = 1.22638213\n",
      "Iteration 69228, loss = 1.21794562\n",
      "Iteration 69229, loss = 1.08095468\n",
      "Iteration 69230, loss = 1.09192069\n",
      "Iteration 69231, loss = 1.19565597\n",
      "Iteration 69232, loss = 1.02395261\n",
      "Iteration 69233, loss = 1.07725224\n",
      "Iteration 69234, loss = 1.22622764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 69235, loss = 1.22824581\n",
      "Iteration 69236, loss = 1.22473729\n",
      "Iteration 69237, loss = 1.47468737\n",
      "Iteration 69238, loss = 1.58667233\n",
      "Iteration 69239, loss = 1.70204212\n",
      "Iteration 69240, loss = 1.48470350\n",
      "Iteration 69241, loss = 1.72696201\n",
      "Iteration 69242, loss = 1.75095091\n",
      "Iteration 69243, loss = 1.66889769\n",
      "Iteration 69244, loss = 1.29838433\n",
      "Iteration 69245, loss = 1.54335878\n",
      "Iteration 69246, loss = 1.12013601\n",
      "Iteration 69247, loss = 1.04674357\n",
      "Iteration 69248, loss = 1.06983291\n",
      "Iteration 69249, loss = 1.15934863\n",
      "Iteration 69250, loss = 1.14647562\n",
      "Iteration 69251, loss = 1.11801332\n",
      "Iteration 69252, loss = 1.09027133\n",
      "Iteration 69253, loss = 1.14072298\n",
      "Iteration 69254, loss = 1.05477030\n",
      "Iteration 69255, loss = 1.33786597\n",
      "Iteration 69256, loss = 1.56950914\n",
      "Iteration 69257, loss = 1.47858456\n",
      "Iteration 69258, loss = 1.27616542\n",
      "Iteration 69259, loss = 2.04447436\n",
      "Iteration 69260, loss = 2.22883503\n",
      "Iteration 69261, loss = 1.96913766\n",
      "Iteration 69262, loss = 1.72741489\n",
      "Iteration 69263, loss = 1.59558479\n",
      "Iteration 69264, loss = 1.47268787\n",
      "Iteration 69265, loss = 1.56803803\n",
      "Iteration 69266, loss = 1.74507205\n",
      "Iteration 69267, loss = 1.31916129\n",
      "Iteration 69268, loss = 1.34310991\n",
      "Iteration 69269, loss = 1.65382826\n",
      "Iteration 69270, loss = 1.81125662\n",
      "Iteration 69271, loss = 1.53861662\n",
      "Iteration 69272, loss = 1.33457753\n",
      "Iteration 69273, loss = 1.17339318\n",
      "Iteration 69274, loss = 1.16517051\n",
      "Iteration 69275, loss = 1.08984293\n",
      "Iteration 69276, loss = 1.15138198\n",
      "Iteration 69277, loss = 1.42754494\n",
      "Iteration 69278, loss = 1.37265543\n",
      "Iteration 69279, loss = 1.28889151\n",
      "Iteration 69280, loss = 1.35469843\n",
      "Iteration 69281, loss = 1.31135946\n",
      "Iteration 69282, loss = 1.20759420\n",
      "Iteration 69283, loss = 1.47568393\n",
      "Iteration 69284, loss = 1.26435982\n",
      "Iteration 69285, loss = 1.44315282\n",
      "Iteration 69286, loss = 1.69501369\n",
      "Iteration 69287, loss = 2.08240561\n",
      "Iteration 69288, loss = 1.47958221\n",
      "Iteration 69289, loss = 1.58685955\n",
      "Iteration 69290, loss = 1.25575785\n",
      "Iteration 69291, loss = 1.30721177\n",
      "Iteration 69292, loss = 1.19238268\n",
      "Iteration 69293, loss = 1.24894976\n",
      "Iteration 69294, loss = 1.29514372\n",
      "Iteration 69295, loss = 1.15755497\n",
      "Iteration 69296, loss = 1.14159054\n",
      "Iteration 69297, loss = 1.11158938\n",
      "Iteration 69298, loss = 1.05325920\n",
      "Iteration 69299, loss = 1.07591575\n",
      "Iteration 69300, loss = 1.04496260\n",
      "Iteration 69301, loss = 1.08957007\n",
      "Iteration 69302, loss = 1.20419944\n",
      "Iteration 69303, loss = 1.42573367\n",
      "Iteration 69304, loss = 1.11266328\n",
      "Iteration 69305, loss = 1.20088496\n",
      "Iteration 69306, loss = 1.23034831\n",
      "Iteration 69307, loss = 1.41441597\n",
      "Iteration 69308, loss = 1.32598205\n",
      "Iteration 69309, loss = 1.27434131\n",
      "Iteration 69310, loss = 1.16899245\n",
      "Iteration 69311, loss = 1.27661265\n",
      "Iteration 69312, loss = 1.55192092\n",
      "Iteration 69313, loss = 2.18998742\n",
      "Iteration 69314, loss = 2.31344555\n",
      "Iteration 69315, loss = 2.19291943\n",
      "Iteration 69316, loss = 1.50870276\n",
      "Iteration 69317, loss = 1.59218118\n",
      "Iteration 69318, loss = 1.74273964\n",
      "Iteration 69319, loss = 1.47046195\n",
      "Iteration 69320, loss = 1.43977010\n",
      "Iteration 69321, loss = 1.14285875\n",
      "Iteration 69322, loss = 1.33138116\n",
      "Iteration 69323, loss = 1.30220804\n",
      "Iteration 69324, loss = 1.23593031\n",
      "Iteration 69325, loss = 1.38602400\n",
      "Iteration 69326, loss = 1.57083202\n",
      "Iteration 69327, loss = 1.45721867\n",
      "Iteration 69328, loss = 1.20787319\n",
      "Iteration 69329, loss = 1.20619794\n",
      "Iteration 69330, loss = 1.14904011\n",
      "Iteration 69331, loss = 1.22106574\n",
      "Iteration 69332, loss = 1.14990974\n",
      "Iteration 69333, loss = 1.15758682\n",
      "Iteration 69334, loss = 1.30339840\n",
      "Iteration 69335, loss = 1.22447791\n",
      "Iteration 69336, loss = 1.00273587\n",
      "Iteration 69337, loss = 1.03427483\n",
      "Iteration 69338, loss = 1.13492887\n",
      "Iteration 69339, loss = 1.20893406\n",
      "Iteration 69340, loss = 1.36087602\n",
      "Iteration 69341, loss = 1.97827685\n",
      "Iteration 69342, loss = 1.57946153\n",
      "Iteration 69343, loss = 3.10723481\n",
      "Iteration 69344, loss = 5.73663588\n",
      "Iteration 69345, loss = 5.37283172\n",
      "Iteration 69346, loss = 3.51180076\n",
      "Iteration 69347, loss = 4.36028741\n",
      "Iteration 69348, loss = 3.22310094\n",
      "Iteration 69349, loss = 2.68128620\n",
      "Iteration 69350, loss = 2.34223221\n",
      "Iteration 69351, loss = 2.43095653\n",
      "Iteration 69352, loss = 2.13302057\n",
      "Iteration 69353, loss = 1.86062591\n",
      "Iteration 69354, loss = 1.64899253\n",
      "Iteration 69355, loss = 1.61964654\n",
      "Iteration 69356, loss = 1.34251590\n",
      "Iteration 69357, loss = 1.38860584\n",
      "Iteration 69358, loss = 1.43588380\n",
      "Iteration 69359, loss = 1.47768609\n",
      "Iteration 69360, loss = 1.32000173\n",
      "Iteration 69361, loss = 1.41788344\n",
      "Iteration 69362, loss = 1.29803741\n",
      "Iteration 69363, loss = 1.11142662\n",
      "Iteration 69364, loss = 1.20709014\n",
      "Iteration 69365, loss = 1.30424408\n",
      "Iteration 69366, loss = 1.18725843\n",
      "Iteration 69367, loss = 1.09939906\n",
      "Iteration 69368, loss = 1.06860699\n",
      "Iteration 69369, loss = 1.11198805\n",
      "Iteration 69370, loss = 1.17612212\n",
      "Iteration 69371, loss = 1.18831907\n",
      "Iteration 69372, loss = 1.21300078\n",
      "Iteration 69373, loss = 1.38539717\n",
      "Iteration 69374, loss = 1.13392482\n",
      "Iteration 69375, loss = 1.29074924\n",
      "Iteration 69376, loss = 1.21370526\n",
      "Iteration 69377, loss = 1.14923224\n",
      "Iteration 69378, loss = 1.14719688\n",
      "Iteration 69379, loss = 1.15877257\n",
      "Iteration 69380, loss = 1.17773179\n",
      "Iteration 69381, loss = 1.11323186\n",
      "Iteration 69382, loss = 1.14902819\n",
      "Iteration 69383, loss = 1.13695703\n",
      "Iteration 69384, loss = 1.24297400\n",
      "Iteration 69385, loss = 1.03094936\n",
      "Iteration 69386, loss = 1.05073658\n",
      "Iteration 69387, loss = 1.02681668\n",
      "Iteration 69388, loss = 1.17529535\n",
      "Iteration 69389, loss = 1.01445341\n",
      "Iteration 69390, loss = 1.28751213\n",
      "Iteration 69391, loss = 1.56432306\n",
      "Iteration 69392, loss = 2.04333067\n",
      "Iteration 69393, loss = 1.83186537\n",
      "Iteration 69394, loss = 1.43136671\n",
      "Iteration 69395, loss = 1.59725552\n",
      "Iteration 69396, loss = 1.23357076\n",
      "Iteration 69397, loss = 1.58362494\n",
      "Iteration 69398, loss = 1.53929976\n",
      "Iteration 69399, loss = 1.55379082\n",
      "Iteration 69400, loss = 1.28041411\n",
      "Iteration 69401, loss = 1.24612603\n",
      "Iteration 69402, loss = 1.61096366\n",
      "Iteration 69403, loss = 1.68064232\n",
      "Iteration 69404, loss = 1.83650550\n",
      "Iteration 69405, loss = 1.52181006\n",
      "Iteration 69406, loss = 1.23107343\n",
      "Iteration 69407, loss = 1.38739929\n",
      "Iteration 69408, loss = 1.41461133\n",
      "Iteration 69409, loss = 1.21701940\n",
      "Iteration 69410, loss = 1.09111656\n",
      "Iteration 69411, loss = 1.08758793\n",
      "Iteration 69412, loss = 1.87568189\n",
      "Iteration 69413, loss = 1.50013715\n",
      "Iteration 69414, loss = 1.52753722\n",
      "Iteration 69415, loss = 1.86547817\n",
      "Iteration 69416, loss = 1.72063940\n",
      "Iteration 69417, loss = 1.19599179\n",
      "Iteration 69418, loss = 1.24942667\n",
      "Iteration 69419, loss = 1.35038892\n",
      "Iteration 69420, loss = 1.09840650\n",
      "Iteration 69421, loss = 1.24390545\n",
      "Iteration 69422, loss = 1.23419821\n",
      "Iteration 69423, loss = 1.23197881\n",
      "Iteration 69424, loss = 1.40462966\n",
      "Iteration 69425, loss = 1.19119447\n",
      "Iteration 69426, loss = 1.09825993\n",
      "Iteration 69427, loss = 1.34341336\n",
      "Iteration 69428, loss = 1.43164592\n",
      "Iteration 69429, loss = 1.56536665\n",
      "Iteration 69430, loss = 1.08953370\n",
      "Iteration 69431, loss = 1.13632399\n",
      "Iteration 69432, loss = 1.08064770\n",
      "Iteration 69433, loss = 1.04862383\n",
      "Iteration 69434, loss = 1.12374387\n",
      "Iteration 69435, loss = 0.97184078\n",
      "Iteration 69436, loss = 1.11194444\n",
      "Iteration 69437, loss = 1.27425962\n",
      "Iteration 69438, loss = 1.25178017\n",
      "Iteration 69439, loss = 1.27532031\n",
      "Iteration 69440, loss = 1.33819076\n",
      "Iteration 69441, loss = 1.61066982\n",
      "Iteration 69442, loss = 1.51766056\n",
      "Iteration 69443, loss = 1.31328137\n",
      "Iteration 69444, loss = 1.36654838\n",
      "Iteration 69445, loss = 1.22364630\n",
      "Iteration 69446, loss = 1.03734020\n",
      "Iteration 69447, loss = 1.21298962\n",
      "Iteration 69448, loss = 1.21571914\n",
      "Iteration 69449, loss = 1.24479454\n",
      "Iteration 69450, loss = 1.24981340\n",
      "Iteration 69451, loss = 1.15853996\n",
      "Iteration 69452, loss = 1.04943416\n",
      "Iteration 69453, loss = 1.02948940\n",
      "Iteration 69454, loss = 1.01710834\n",
      "Iteration 69455, loss = 1.12437784\n",
      "Iteration 69456, loss = 1.20636523\n",
      "Iteration 69457, loss = 1.07841402\n",
      "Iteration 69458, loss = 1.15665470\n",
      "Iteration 69459, loss = 1.04847830\n",
      "Iteration 69460, loss = 1.03940237\n",
      "Iteration 69461, loss = 1.08209052\n",
      "Iteration 69462, loss = 1.25629376\n",
      "Iteration 69463, loss = 1.83091358\n",
      "Iteration 69464, loss = 2.01050582\n",
      "Iteration 69465, loss = 1.58127688\n",
      "Iteration 69466, loss = 1.54320073\n",
      "Iteration 69467, loss = 1.77529814\n",
      "Iteration 69468, loss = 1.45123742\n",
      "Iteration 69469, loss = 1.91181682\n",
      "Iteration 69470, loss = 1.32425445\n",
      "Iteration 69471, loss = 1.38757342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 69472, loss = 1.30300210\n",
      "Iteration 69473, loss = 1.26497829\n",
      "Iteration 69474, loss = 1.77152948\n",
      "Iteration 69475, loss = 1.58583583\n",
      "Iteration 69476, loss = 2.01309016\n",
      "Iteration 69477, loss = 2.20833932\n",
      "Iteration 69478, loss = 2.10497373\n",
      "Iteration 69479, loss = 1.78999765\n",
      "Iteration 69480, loss = 1.93438001\n",
      "Iteration 69481, loss = 2.21772641\n",
      "Iteration 69482, loss = 1.53574012\n",
      "Iteration 69483, loss = 1.25797033\n",
      "Iteration 69484, loss = 1.48118620\n",
      "Iteration 69485, loss = 1.73948759\n",
      "Iteration 69486, loss = 1.93696860\n",
      "Iteration 69487, loss = 1.48542682\n",
      "Iteration 69488, loss = 1.28607003\n",
      "Iteration 69489, loss = 1.21685209\n",
      "Iteration 69490, loss = 1.22891710\n",
      "Iteration 69491, loss = 1.20652505\n",
      "Iteration 69492, loss = 1.27516654\n",
      "Iteration 69493, loss = 1.65531102\n",
      "Iteration 69494, loss = 1.65612604\n",
      "Iteration 69495, loss = 1.62662312\n",
      "Iteration 69496, loss = 1.48034137\n",
      "Iteration 69497, loss = 1.17143385\n",
      "Iteration 69498, loss = 1.24745676\n",
      "Iteration 69499, loss = 1.33193995\n",
      "Iteration 69500, loss = 1.26522875\n",
      "Iteration 69501, loss = 1.11368822\n",
      "Iteration 69502, loss = 1.41910054\n",
      "Iteration 69503, loss = 1.34887874\n",
      "Iteration 69504, loss = 1.48049074\n",
      "Iteration 69505, loss = 1.50220843\n",
      "Iteration 69506, loss = 2.39364645\n",
      "Iteration 69507, loss = 1.96668377\n",
      "Iteration 69508, loss = 1.84371970\n",
      "Iteration 69509, loss = 1.42746853\n",
      "Iteration 69510, loss = 1.42453918\n",
      "Iteration 69511, loss = 1.31185504\n",
      "Iteration 69512, loss = 1.40254172\n",
      "Iteration 69513, loss = 1.34948922\n",
      "Iteration 69514, loss = 1.33571751\n",
      "Iteration 69515, loss = 1.50097487\n",
      "Iteration 69516, loss = 1.48438410\n",
      "Iteration 69517, loss = 1.50763892\n",
      "Iteration 69518, loss = 2.00579421\n",
      "Iteration 69519, loss = 1.57074214\n",
      "Iteration 69520, loss = 1.58792802\n",
      "Iteration 69521, loss = 1.38684018\n",
      "Iteration 69522, loss = 1.17300624\n",
      "Iteration 69523, loss = 1.34415733\n",
      "Iteration 69524, loss = 1.27437213\n",
      "Iteration 69525, loss = 1.32403548\n",
      "Iteration 69526, loss = 1.27987586\n",
      "Iteration 69527, loss = 1.32071875\n",
      "Iteration 69528, loss = 1.62545258\n",
      "Iteration 69529, loss = 1.24496266\n",
      "Iteration 69530, loss = 1.55234213\n",
      "Iteration 69531, loss = 1.39655505\n",
      "Iteration 69532, loss = 1.09210289\n",
      "Iteration 69533, loss = 1.14118160\n",
      "Iteration 69534, loss = 1.09699860\n",
      "Iteration 69535, loss = 1.10662259\n",
      "Iteration 69536, loss = 1.65037146\n",
      "Iteration 69537, loss = 1.43646563\n",
      "Iteration 69538, loss = 1.52580610\n",
      "Iteration 69539, loss = 1.55798636\n",
      "Iteration 69540, loss = 1.45640935\n",
      "Iteration 69541, loss = 1.30792999\n",
      "Iteration 69542, loss = 1.11747138\n",
      "Iteration 69543, loss = 1.15158472\n",
      "Iteration 69544, loss = 1.11467607\n",
      "Iteration 69545, loss = 1.17104644\n",
      "Iteration 69546, loss = 1.22972079\n",
      "Iteration 69547, loss = 1.06202013\n",
      "Iteration 69548, loss = 1.00817275\n",
      "Iteration 69549, loss = 1.08008958\n",
      "Iteration 69550, loss = 1.07110319\n",
      "Iteration 69551, loss = 1.20894270\n",
      "Iteration 69552, loss = 1.19363078\n",
      "Iteration 69553, loss = 1.10761803\n",
      "Iteration 69554, loss = 1.41647925\n",
      "Iteration 69555, loss = 1.28278665\n",
      "Iteration 69556, loss = 1.28049107\n",
      "Iteration 69557, loss = 1.38295074\n",
      "Iteration 69558, loss = 1.38962053\n",
      "Iteration 69559, loss = 1.26366351\n",
      "Iteration 69560, loss = 1.21560495\n",
      "Iteration 69561, loss = 1.17578745\n",
      "Iteration 69562, loss = 1.12931804\n",
      "Iteration 69563, loss = 1.22196605\n",
      "Iteration 69564, loss = 1.04681265\n",
      "Iteration 69565, loss = 1.03400650\n",
      "Iteration 69566, loss = 1.33158969\n",
      "Iteration 69567, loss = 1.53338598\n",
      "Iteration 69568, loss = 1.27837873\n",
      "Iteration 69569, loss = 1.25329273\n",
      "Iteration 69570, loss = 1.39048098\n",
      "Iteration 69571, loss = 1.19778585\n",
      "Iteration 69572, loss = 1.21592277\n",
      "Iteration 69573, loss = 1.58284478\n",
      "Iteration 69574, loss = 1.42456416\n",
      "Iteration 69575, loss = 1.45211534\n",
      "Iteration 69576, loss = 1.35505624\n",
      "Iteration 69577, loss = 1.41590007\n",
      "Iteration 69578, loss = 1.52338295\n",
      "Iteration 69579, loss = 1.36970695\n",
      "Iteration 69580, loss = 1.26970618\n",
      "Iteration 69581, loss = 1.00730848\n",
      "Iteration 69582, loss = 1.20961825\n",
      "Iteration 69583, loss = 1.38398054\n",
      "Iteration 69584, loss = 1.39283078\n",
      "Iteration 69585, loss = 1.37548084\n",
      "Iteration 69586, loss = 1.49902070\n",
      "Iteration 69587, loss = 1.34773198\n",
      "Iteration 69588, loss = 1.26287253\n",
      "Iteration 69589, loss = 1.09468175\n",
      "Iteration 69590, loss = 1.27084465\n",
      "Iteration 69591, loss = 1.13532212\n",
      "Iteration 69592, loss = 1.53213161\n",
      "Iteration 69593, loss = 1.26957536\n",
      "Iteration 69594, loss = 1.53726797\n",
      "Iteration 69595, loss = 2.25974187\n",
      "Iteration 69596, loss = 1.62213048\n",
      "Iteration 69597, loss = 1.26273327\n",
      "Iteration 69598, loss = 1.27549538\n",
      "Iteration 69599, loss = 1.21929860\n",
      "Iteration 69600, loss = 1.41389708\n",
      "Iteration 69601, loss = 1.26825552\n",
      "Iteration 69602, loss = 1.25351728\n",
      "Iteration 69603, loss = 1.09423431\n",
      "Iteration 69604, loss = 1.06108994\n",
      "Iteration 69605, loss = 1.20953197\n",
      "Iteration 69606, loss = 1.28059923\n",
      "Iteration 69607, loss = 1.12559619\n",
      "Iteration 69608, loss = 1.11151986\n",
      "Iteration 69609, loss = 1.07867263\n",
      "Iteration 69610, loss = 1.05390759\n",
      "Iteration 69611, loss = 1.08016856\n",
      "Iteration 69612, loss = 1.38767626\n",
      "Iteration 69613, loss = 1.41097452\n",
      "Iteration 69614, loss = 1.45833863\n",
      "Iteration 69615, loss = 1.48994972\n",
      "Iteration 69616, loss = 1.39220746\n",
      "Iteration 69617, loss = 1.47264067\n",
      "Iteration 69618, loss = 1.85356547\n",
      "Iteration 69619, loss = 1.52332607\n",
      "Iteration 69620, loss = 1.41587280\n",
      "Iteration 69621, loss = 1.51137819\n",
      "Iteration 69622, loss = 1.58039002\n",
      "Iteration 69623, loss = 1.32591768\n",
      "Iteration 69624, loss = 1.55868292\n",
      "Iteration 69625, loss = 1.23304638\n",
      "Iteration 69626, loss = 1.17428752\n",
      "Iteration 69627, loss = 1.15712991\n",
      "Iteration 69628, loss = 1.06469192\n",
      "Iteration 69629, loss = 1.17162606\n",
      "Iteration 69630, loss = 1.22599068\n",
      "Iteration 69631, loss = 1.32251195\n",
      "Iteration 69632, loss = 1.27000428\n",
      "Iteration 69633, loss = 1.23718845\n",
      "Iteration 69634, loss = 1.19751315\n",
      "Iteration 69635, loss = 1.07431151\n",
      "Iteration 69636, loss = 1.10704647\n",
      "Iteration 69637, loss = 1.14411699\n",
      "Iteration 69638, loss = 1.10999262\n",
      "Iteration 69639, loss = 1.25964435\n",
      "Iteration 69640, loss = 1.41149830\n",
      "Iteration 69641, loss = 1.60491962\n",
      "Iteration 69642, loss = 1.62223130\n",
      "Iteration 69643, loss = 1.61059720\n",
      "Iteration 69644, loss = 1.56146741\n",
      "Iteration 69645, loss = 1.75983774\n",
      "Iteration 69646, loss = 2.12523617\n",
      "Iteration 69647, loss = 2.24756622\n",
      "Iteration 69648, loss = 2.45097720\n",
      "Iteration 69649, loss = 2.08258904\n",
      "Iteration 69650, loss = 2.18610236\n",
      "Iteration 69651, loss = 2.50091180\n",
      "Iteration 69652, loss = 2.74529643\n",
      "Iteration 69653, loss = 1.84806679\n",
      "Iteration 69654, loss = 1.88101280\n",
      "Iteration 69655, loss = 1.51724833\n",
      "Iteration 69656, loss = 1.43356874\n",
      "Iteration 69657, loss = 1.20694028\n",
      "Iteration 69658, loss = 1.25863064\n",
      "Iteration 69659, loss = 1.35723996\n",
      "Iteration 69660, loss = 1.54314881\n",
      "Iteration 69661, loss = 1.32423746\n",
      "Iteration 69662, loss = 1.28302184\n",
      "Iteration 69663, loss = 1.18729785\n",
      "Iteration 69664, loss = 1.09121340\n",
      "Iteration 69665, loss = 1.07225108\n",
      "Iteration 69666, loss = 1.08305968\n",
      "Iteration 69667, loss = 1.10532497\n",
      "Iteration 69668, loss = 1.10268386\n",
      "Iteration 69669, loss = 1.15664014\n",
      "Iteration 69670, loss = 1.26540482\n",
      "Iteration 69671, loss = 1.19684878\n",
      "Iteration 69672, loss = 1.38722279\n",
      "Iteration 69673, loss = 1.45700347\n",
      "Iteration 69674, loss = 1.67884587\n",
      "Iteration 69675, loss = 1.65795516\n",
      "Iteration 69676, loss = 1.66252857\n",
      "Iteration 69677, loss = 1.80810278\n",
      "Iteration 69678, loss = 1.44884290\n",
      "Iteration 69679, loss = 1.14856587\n",
      "Iteration 69680, loss = 1.13518464\n",
      "Iteration 69681, loss = 1.11914584\n",
      "Iteration 69682, loss = 1.08515132\n",
      "Iteration 69683, loss = 1.12441485\n",
      "Iteration 69684, loss = 1.06325849\n",
      "Iteration 69685, loss = 1.20725345\n",
      "Iteration 69686, loss = 1.00233806\n",
      "Iteration 69687, loss = 1.07066881\n",
      "Iteration 69688, loss = 1.11659561\n",
      "Iteration 69689, loss = 1.06791528\n",
      "Iteration 69690, loss = 1.23952674\n",
      "Iteration 69691, loss = 1.44576461\n",
      "Iteration 69692, loss = 1.65734260\n",
      "Iteration 69693, loss = 1.33656132\n",
      "Iteration 69694, loss = 1.20097184\n",
      "Iteration 69695, loss = 1.53358318\n",
      "Iteration 69696, loss = 1.40805711\n",
      "Iteration 69697, loss = 1.11201452\n",
      "Iteration 69698, loss = 1.29589121\n",
      "Iteration 69699, loss = 1.16639725\n",
      "Iteration 69700, loss = 1.32003051\n",
      "Iteration 69701, loss = 1.52731462\n",
      "Iteration 69702, loss = 1.53125428\n",
      "Iteration 69703, loss = 1.45147741\n",
      "Iteration 69704, loss = 1.19739604\n",
      "Iteration 69705, loss = 1.06790687\n",
      "Iteration 69706, loss = 1.36216542\n",
      "Iteration 69707, loss = 1.22262030\n",
      "Iteration 69708, loss = 1.15176349\n",
      "Iteration 69709, loss = 1.30663016\n",
      "Iteration 69710, loss = 1.29632273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 69711, loss = 1.50770557\n",
      "Iteration 69712, loss = 1.48335222\n",
      "Iteration 69713, loss = 1.10095609\n",
      "Iteration 69714, loss = 1.20678924\n",
      "Iteration 69715, loss = 1.20970142\n",
      "Iteration 69716, loss = 1.29118465\n",
      "Iteration 69717, loss = 1.19997405\n",
      "Iteration 69718, loss = 1.15937131\n",
      "Iteration 69719, loss = 1.17282487\n",
      "Iteration 69720, loss = 1.21619869\n",
      "Iteration 69721, loss = 1.39981961\n",
      "Iteration 69722, loss = 1.03482364\n",
      "Iteration 69723, loss = 1.09588605\n",
      "Iteration 69724, loss = 1.27519356\n",
      "Iteration 69725, loss = 1.28178469\n",
      "Iteration 69726, loss = 1.32005636\n",
      "Iteration 69727, loss = 1.04198329\n",
      "Iteration 69728, loss = 1.22298215\n",
      "Iteration 69729, loss = 1.13183957\n",
      "Iteration 69730, loss = 1.17553476\n",
      "Iteration 69731, loss = 1.17105333\n",
      "Iteration 69732, loss = 1.18111725\n",
      "Iteration 69733, loss = 1.09171573\n",
      "Iteration 69734, loss = 1.16794602\n",
      "Iteration 69735, loss = 1.23172370\n",
      "Iteration 69736, loss = 1.29074646\n",
      "Iteration 69737, loss = 1.46938617\n",
      "Iteration 69738, loss = 1.52808877\n",
      "Iteration 69739, loss = 1.42973751\n",
      "Iteration 69740, loss = 1.97792150\n",
      "Iteration 69741, loss = 1.56519103\n",
      "Iteration 69742, loss = 1.64848706\n",
      "Iteration 69743, loss = 2.02707102\n",
      "Iteration 69744, loss = 2.23391577\n",
      "Iteration 69745, loss = 1.87302301\n",
      "Iteration 69746, loss = 1.84800455\n",
      "Iteration 69747, loss = 1.53981183\n",
      "Iteration 69748, loss = 1.37691947\n",
      "Iteration 69749, loss = 1.85533781\n",
      "Iteration 69750, loss = 1.58359951\n",
      "Iteration 69751, loss = 1.93058583\n",
      "Iteration 69752, loss = 1.47603823\n",
      "Iteration 69753, loss = 1.22953153\n",
      "Iteration 69754, loss = 1.25312718\n",
      "Iteration 69755, loss = 1.28600997\n",
      "Iteration 69756, loss = 1.26055700\n",
      "Iteration 69757, loss = 1.20292525\n",
      "Iteration 69758, loss = 1.26496113\n",
      "Iteration 69759, loss = 1.15206577\n",
      "Iteration 69760, loss = 1.19529301\n",
      "Iteration 69761, loss = 1.11316820\n",
      "Iteration 69762, loss = 1.24232780\n",
      "Iteration 69763, loss = 1.37884911\n",
      "Iteration 69764, loss = 1.26491030\n",
      "Iteration 69765, loss = 1.63244434\n",
      "Iteration 69766, loss = 1.55737405\n",
      "Iteration 69767, loss = 1.40486843\n",
      "Iteration 69768, loss = 1.26002985\n",
      "Iteration 69769, loss = 1.40930577\n",
      "Iteration 69770, loss = 1.79305056\n",
      "Iteration 69771, loss = 1.54800570\n",
      "Iteration 69772, loss = 1.62027560\n",
      "Iteration 69773, loss = 2.09840266\n",
      "Iteration 69774, loss = 1.63170373\n",
      "Iteration 69775, loss = 1.42388544\n",
      "Iteration 69776, loss = 1.54404713\n",
      "Iteration 69777, loss = 1.39171490\n",
      "Iteration 69778, loss = 1.31417920\n",
      "Iteration 69779, loss = 1.26007897\n",
      "Iteration 69780, loss = 1.41341537\n",
      "Iteration 69781, loss = 1.31572266\n",
      "Iteration 69782, loss = 1.43525712\n",
      "Iteration 69783, loss = 1.26290878\n",
      "Iteration 69784, loss = 1.27710036\n",
      "Iteration 69785, loss = 1.48493984\n",
      "Iteration 69786, loss = 1.54423903\n",
      "Iteration 69787, loss = 1.25440787\n",
      "Iteration 69788, loss = 1.41527776\n",
      "Iteration 69789, loss = 1.44885350\n",
      "Iteration 69790, loss = 1.28643819\n",
      "Iteration 69791, loss = 1.28243098\n",
      "Iteration 69792, loss = 1.42932377\n",
      "Iteration 69793, loss = 1.30268465\n",
      "Iteration 69794, loss = 1.36743587\n",
      "Iteration 69795, loss = 1.51014501\n",
      "Iteration 69796, loss = 1.34566250\n",
      "Iteration 69797, loss = 1.68029236\n",
      "Iteration 69798, loss = 1.57682917\n",
      "Iteration 69799, loss = 1.36655394\n",
      "Iteration 69800, loss = 1.39693026\n",
      "Iteration 69801, loss = 1.49470593\n",
      "Iteration 69802, loss = 1.48220804\n",
      "Iteration 69803, loss = 1.80031197\n",
      "Iteration 69804, loss = 1.37044378\n",
      "Iteration 69805, loss = 1.18558551\n",
      "Iteration 69806, loss = 1.55895125\n",
      "Iteration 69807, loss = 1.50183207\n",
      "Iteration 69808, loss = 2.18695735\n",
      "Iteration 69809, loss = 3.88294838\n",
      "Iteration 69810, loss = 5.12141872\n",
      "Iteration 69811, loss = 5.23622543\n",
      "Iteration 69812, loss = 3.13198814\n",
      "Iteration 69813, loss = 3.35001048\n",
      "Iteration 69814, loss = 3.88273987\n",
      "Iteration 69815, loss = 3.34942031\n",
      "Iteration 69816, loss = 2.65803783\n",
      "Iteration 69817, loss = 2.66930086\n",
      "Iteration 69818, loss = 1.77638613\n",
      "Iteration 69819, loss = 1.65130876\n",
      "Iteration 69820, loss = 1.69364470\n",
      "Iteration 69821, loss = 1.94792696\n",
      "Iteration 69822, loss = 1.41192030\n",
      "Iteration 69823, loss = 1.43060313\n",
      "Iteration 69824, loss = 1.18145601\n",
      "Iteration 69825, loss = 1.29681533\n",
      "Iteration 69826, loss = 1.23857074\n",
      "Iteration 69827, loss = 1.19975976\n",
      "Iteration 69828, loss = 1.64565009\n",
      "Iteration 69829, loss = 1.46747646\n",
      "Iteration 69830, loss = 1.53277543\n",
      "Iteration 69831, loss = 1.35353357\n",
      "Iteration 69832, loss = 2.42435363\n",
      "Iteration 69833, loss = 2.57219067\n",
      "Iteration 69834, loss = 2.39851139\n",
      "Iteration 69835, loss = 1.83820200\n",
      "Iteration 69836, loss = 2.08531875\n",
      "Iteration 69837, loss = 2.18454480\n",
      "Iteration 69838, loss = 1.90618974\n",
      "Iteration 69839, loss = 2.06772215\n",
      "Iteration 69840, loss = 1.86363277\n",
      "Iteration 69841, loss = 1.67710515\n",
      "Iteration 69842, loss = 1.57478741\n",
      "Iteration 69843, loss = 1.70265219\n",
      "Iteration 69844, loss = 1.41415694\n",
      "Iteration 69845, loss = 1.68768422\n",
      "Iteration 69846, loss = 1.42307807\n",
      "Iteration 69847, loss = 1.25873359\n",
      "Iteration 69848, loss = 1.42469399\n",
      "Iteration 69849, loss = 1.55715644\n",
      "Iteration 69850, loss = 1.44607517\n",
      "Iteration 69851, loss = 1.26844609\n",
      "Iteration 69852, loss = 1.02426202\n",
      "Iteration 69853, loss = 0.99068719\n",
      "Iteration 69854, loss = 1.01725901\n",
      "Iteration 69855, loss = 1.00206956\n",
      "Iteration 69856, loss = 0.99773882\n",
      "Iteration 69857, loss = 1.06263637\n",
      "Iteration 69858, loss = 1.21895733\n",
      "Iteration 69859, loss = 1.31867423\n",
      "Iteration 69860, loss = 1.26717823\n",
      "Iteration 69861, loss = 1.30603009\n",
      "Iteration 69862, loss = 1.31685667\n",
      "Iteration 69863, loss = 1.30947509\n",
      "Iteration 69864, loss = 1.12485122\n",
      "Iteration 69865, loss = 1.37952121\n",
      "Iteration 69866, loss = 1.43232145\n",
      "Iteration 69867, loss = 1.50923790\n",
      "Iteration 69868, loss = 1.42146465\n",
      "Iteration 69869, loss = 1.42536604\n",
      "Iteration 69870, loss = 1.63517666\n",
      "Iteration 69871, loss = 2.31409559\n",
      "Iteration 69872, loss = 3.70125541\n",
      "Iteration 69873, loss = 3.05532357\n",
      "Iteration 69874, loss = 2.58649846\n",
      "Iteration 69875, loss = 2.20651743\n",
      "Iteration 69876, loss = 2.02040490\n",
      "Iteration 69877, loss = 1.65651599\n",
      "Iteration 69878, loss = 1.45477499\n",
      "Iteration 69879, loss = 1.42235588\n",
      "Iteration 69880, loss = 1.21260760\n",
      "Iteration 69881, loss = 1.22454068\n",
      "Iteration 69882, loss = 1.14950087\n",
      "Iteration 69883, loss = 1.01412182\n",
      "Iteration 69884, loss = 1.04081559\n",
      "Iteration 69885, loss = 1.09119379\n",
      "Iteration 69886, loss = 1.07651732\n",
      "Iteration 69887, loss = 1.35849909\n",
      "Iteration 69888, loss = 1.24639928\n",
      "Iteration 69889, loss = 1.19585568\n",
      "Iteration 69890, loss = 1.43376124\n",
      "Iteration 69891, loss = 1.30210938\n",
      "Iteration 69892, loss = 1.12658939\n",
      "Iteration 69893, loss = 1.03716437\n",
      "Iteration 69894, loss = 1.22384730\n",
      "Iteration 69895, loss = 1.16038663\n",
      "Iteration 69896, loss = 1.20671823\n",
      "Iteration 69897, loss = 1.20268089\n",
      "Iteration 69898, loss = 1.04760958\n",
      "Iteration 69899, loss = 1.08501068\n",
      "Iteration 69900, loss = 1.11930264\n",
      "Iteration 69901, loss = 1.13378901\n",
      "Iteration 69902, loss = 1.16619669\n",
      "Iteration 69903, loss = 1.13552230\n",
      "Iteration 69904, loss = 1.04053460\n",
      "Iteration 69905, loss = 1.45786629\n",
      "Iteration 69906, loss = 1.68522418\n",
      "Iteration 69907, loss = 1.58570345\n",
      "Iteration 69908, loss = 1.73536607\n",
      "Iteration 69909, loss = 1.41332980\n",
      "Iteration 69910, loss = 1.60224945\n",
      "Iteration 69911, loss = 1.28578827\n",
      "Iteration 69912, loss = 1.30583846\n",
      "Iteration 69913, loss = 1.22969388\n",
      "Iteration 69914, loss = 1.60208854\n",
      "Iteration 69915, loss = 1.70793473\n",
      "Iteration 69916, loss = 1.80999038\n",
      "Iteration 69917, loss = 1.66354505\n",
      "Iteration 69918, loss = 1.28417316\n",
      "Iteration 69919, loss = 1.28644637\n",
      "Iteration 69920, loss = 1.42039478\n",
      "Iteration 69921, loss = 1.37997306\n",
      "Iteration 69922, loss = 1.20394114\n",
      "Iteration 69923, loss = 1.31880194\n",
      "Iteration 69924, loss = 1.07896259\n",
      "Iteration 69925, loss = 1.19489105\n",
      "Iteration 69926, loss = 1.26211594\n",
      "Iteration 69927, loss = 1.23026769\n",
      "Iteration 69928, loss = 1.82371895\n",
      "Iteration 69929, loss = 1.73030653\n",
      "Iteration 69930, loss = 2.06944569\n",
      "Iteration 69931, loss = 1.71658615\n",
      "Iteration 69932, loss = 1.67089285\n",
      "Iteration 69933, loss = 1.59856002\n",
      "Iteration 69934, loss = 1.79470472\n",
      "Iteration 69935, loss = 1.34733122\n",
      "Iteration 69936, loss = 1.16204427\n",
      "Iteration 69937, loss = 1.12905514\n",
      "Iteration 69938, loss = 1.09438883\n",
      "Iteration 69939, loss = 1.08829089\n",
      "Iteration 69940, loss = 1.22933756\n",
      "Iteration 69941, loss = 1.08791277\n",
      "Iteration 69942, loss = 1.12150862\n",
      "Iteration 69943, loss = 1.10640744\n",
      "Iteration 69944, loss = 1.06760069\n",
      "Iteration 69945, loss = 1.22960953\n",
      "Iteration 69946, loss = 1.52163339\n",
      "Iteration 69947, loss = 1.25432673\n",
      "Iteration 69948, loss = 1.19636665\n",
      "Iteration 69949, loss = 1.12491682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 69950, loss = 1.34939640\n",
      "Iteration 69951, loss = 1.48923068\n",
      "Iteration 69952, loss = 1.94878972\n",
      "Iteration 69953, loss = 1.51171849\n",
      "Iteration 69954, loss = 1.37560201\n",
      "Iteration 69955, loss = 1.44915867\n",
      "Iteration 69956, loss = 1.36534726\n",
      "Iteration 69957, loss = 1.28906405\n",
      "Iteration 69958, loss = 1.54315667\n",
      "Iteration 69959, loss = 1.17542695\n",
      "Iteration 69960, loss = 1.27658415\n",
      "Iteration 69961, loss = 1.56385953\n",
      "Iteration 69962, loss = 1.28802257\n",
      "Iteration 69963, loss = 1.21887953\n",
      "Iteration 69964, loss = 1.20598603\n",
      "Iteration 69965, loss = 1.12599293\n",
      "Iteration 69966, loss = 1.17325384\n",
      "Iteration 69967, loss = 1.30870213\n",
      "Iteration 69968, loss = 1.36760010\n",
      "Iteration 69969, loss = 1.47696826\n",
      "Iteration 69970, loss = 1.21412171\n",
      "Iteration 69971, loss = 1.40795244\n",
      "Iteration 69972, loss = 1.19541463\n",
      "Iteration 69973, loss = 1.32343972\n",
      "Iteration 69974, loss = 1.46644676\n",
      "Iteration 69975, loss = 1.15706167\n",
      "Iteration 69976, loss = 1.50988117\n",
      "Iteration 69977, loss = 1.44696424\n",
      "Iteration 69978, loss = 1.45096000\n",
      "Iteration 69979, loss = 1.23591643\n",
      "Iteration 69980, loss = 1.26135378\n",
      "Iteration 69981, loss = 1.19203281\n",
      "Iteration 69982, loss = 1.27704151\n",
      "Iteration 69983, loss = 1.07991049\n",
      "Iteration 69984, loss = 1.15873990\n",
      "Iteration 69985, loss = 1.26284151\n",
      "Iteration 69986, loss = 1.35584258\n",
      "Iteration 69987, loss = 1.07995594\n",
      "Iteration 69988, loss = 1.05737462\n",
      "Iteration 69989, loss = 1.08498922\n",
      "Iteration 69990, loss = 1.23920182\n",
      "Iteration 69991, loss = 1.18622005\n",
      "Iteration 69992, loss = 1.28565899\n",
      "Iteration 69993, loss = 1.51310145\n",
      "Iteration 69994, loss = 1.86212864\n",
      "Iteration 69995, loss = 1.73467927\n",
      "Iteration 69996, loss = 1.52262276\n",
      "Iteration 69997, loss = 2.12291402\n",
      "Iteration 69998, loss = 1.38785586\n",
      "Iteration 69999, loss = 1.25595380\n",
      "Iteration 70000, loss = 1.24365635\n",
      "Iteration 70001, loss = 1.17516016\n",
      "Iteration 70002, loss = 1.16931216\n",
      "Iteration 70003, loss = 1.15594594\n",
      "Iteration 70004, loss = 1.13870991\n",
      "Iteration 70005, loss = 1.01040312\n",
      "Iteration 70006, loss = 1.24008057\n",
      "Iteration 70007, loss = 1.08636954\n",
      "Iteration 70008, loss = 1.39787182\n",
      "Iteration 70009, loss = 1.50891086\n",
      "Iteration 70010, loss = 2.00464894\n",
      "Iteration 70011, loss = 1.42036063\n",
      "Iteration 70012, loss = 1.35195228\n",
      "Iteration 70013, loss = 1.32269271\n",
      "Iteration 70014, loss = 1.10444491\n",
      "Iteration 70015, loss = 1.16346199\n",
      "Iteration 70016, loss = 1.04077677\n",
      "Iteration 70017, loss = 1.08539371\n",
      "Iteration 70018, loss = 1.08012430\n",
      "Iteration 70019, loss = 1.12495478\n",
      "Iteration 70020, loss = 1.20407978\n",
      "Iteration 70021, loss = 1.36107016\n",
      "Iteration 70022, loss = 1.10776163\n",
      "Iteration 70023, loss = 1.15219262\n",
      "Iteration 70024, loss = 1.19230431\n",
      "Iteration 70025, loss = 1.15385729\n",
      "Iteration 70026, loss = 1.12225389\n",
      "Iteration 70027, loss = 1.12018723\n",
      "Iteration 70028, loss = 1.42271611\n",
      "Iteration 70029, loss = 1.25699503\n",
      "Iteration 70030, loss = 1.20976362\n",
      "Iteration 70031, loss = 1.09664876\n",
      "Iteration 70032, loss = 1.09013509\n",
      "Iteration 70033, loss = 1.12515427\n",
      "Iteration 70034, loss = 1.13334933\n",
      "Iteration 70035, loss = 1.65054789\n",
      "Iteration 70036, loss = 1.50606444\n",
      "Iteration 70037, loss = 1.37813747\n",
      "Iteration 70038, loss = 1.40230654\n",
      "Iteration 70039, loss = 2.06416490\n",
      "Iteration 70040, loss = 1.38677864\n",
      "Iteration 70041, loss = 1.61753404\n",
      "Iteration 70042, loss = 2.22558287\n",
      "Iteration 70043, loss = 1.85969837\n",
      "Iteration 70044, loss = 1.89245698\n",
      "Iteration 70045, loss = 1.98612166\n",
      "Iteration 70046, loss = 1.73371648\n",
      "Iteration 70047, loss = 1.53228780\n",
      "Iteration 70048, loss = 1.31057005\n",
      "Iteration 70049, loss = 1.38225530\n",
      "Iteration 70050, loss = 1.72805918\n",
      "Iteration 70051, loss = 2.23860241\n",
      "Iteration 70052, loss = 1.88247339\n",
      "Iteration 70053, loss = 2.19737448\n",
      "Iteration 70054, loss = 1.49672239\n",
      "Iteration 70055, loss = 1.84440207\n",
      "Iteration 70056, loss = 2.64279676\n",
      "Iteration 70057, loss = 3.04013517\n",
      "Iteration 70058, loss = 3.12597106\n",
      "Iteration 70059, loss = 2.39250878\n",
      "Iteration 70060, loss = 1.68897052\n",
      "Iteration 70061, loss = 1.36046593\n",
      "Iteration 70062, loss = 1.46480047\n",
      "Iteration 70063, loss = 1.19547180\n",
      "Iteration 70064, loss = 1.13214651\n",
      "Iteration 70065, loss = 1.08850651\n",
      "Iteration 70066, loss = 1.03294749\n",
      "Iteration 70067, loss = 1.11444149\n",
      "Iteration 70068, loss = 1.06125727\n",
      "Iteration 70069, loss = 1.06490382\n",
      "Iteration 70070, loss = 1.21516771\n",
      "Iteration 70071, loss = 1.19423788\n",
      "Iteration 70072, loss = 1.15767699\n",
      "Iteration 70073, loss = 1.09309222\n",
      "Iteration 70074, loss = 1.16745147\n",
      "Iteration 70075, loss = 1.12273420\n",
      "Iteration 70076, loss = 1.21376706\n",
      "Iteration 70077, loss = 1.39155619\n",
      "Iteration 70078, loss = 1.77173753\n",
      "Iteration 70079, loss = 1.60338907\n",
      "Iteration 70080, loss = 1.98840106\n",
      "Iteration 70081, loss = 1.98524429\n",
      "Iteration 70082, loss = 1.38647677\n",
      "Iteration 70083, loss = 1.40495738\n",
      "Iteration 70084, loss = 1.18266948\n",
      "Iteration 70085, loss = 1.30915246\n",
      "Iteration 70086, loss = 1.12789920\n",
      "Iteration 70087, loss = 1.42454702\n",
      "Iteration 70088, loss = 1.13497038\n",
      "Iteration 70089, loss = 1.14008346\n",
      "Iteration 70090, loss = 1.17756160\n",
      "Iteration 70091, loss = 1.17447972\n",
      "Iteration 70092, loss = 1.29588665\n",
      "Iteration 70093, loss = 1.43812949\n",
      "Iteration 70094, loss = 1.17331180\n",
      "Iteration 70095, loss = 1.22972763\n",
      "Iteration 70096, loss = 1.24957917\n",
      "Iteration 70097, loss = 1.58536913\n",
      "Iteration 70098, loss = 1.28195535\n",
      "Iteration 70099, loss = 1.14008819\n",
      "Iteration 70100, loss = 1.55592024\n",
      "Iteration 70101, loss = 1.30971865\n",
      "Iteration 70102, loss = 1.60989166\n",
      "Iteration 70103, loss = 1.99312089\n",
      "Iteration 70104, loss = 2.15108248\n",
      "Iteration 70105, loss = 1.22710006\n",
      "Iteration 70106, loss = 1.26444270\n",
      "Iteration 70107, loss = 1.10144130\n",
      "Iteration 70108, loss = 1.05710805\n",
      "Iteration 70109, loss = 1.02922888\n",
      "Iteration 70110, loss = 1.02298327\n",
      "Iteration 70111, loss = 1.06949718\n",
      "Iteration 70112, loss = 1.09609098\n",
      "Iteration 70113, loss = 1.13606744\n",
      "Iteration 70114, loss = 1.61739455\n",
      "Iteration 70115, loss = 1.89090945\n",
      "Iteration 70116, loss = 1.70107578\n",
      "Iteration 70117, loss = 1.33236049\n",
      "Iteration 70118, loss = 1.16401160\n",
      "Iteration 70119, loss = 1.10989134\n",
      "Iteration 70120, loss = 1.19545656\n",
      "Iteration 70121, loss = 1.05899038\n",
      "Iteration 70122, loss = 1.03265459\n",
      "Iteration 70123, loss = 1.06284971\n",
      "Iteration 70124, loss = 1.17076889\n",
      "Iteration 70125, loss = 1.16544999\n",
      "Iteration 70126, loss = 1.13222963\n",
      "Iteration 70127, loss = 1.22784529\n",
      "Iteration 70128, loss = 1.10639785\n",
      "Iteration 70129, loss = 1.20479730\n",
      "Iteration 70130, loss = 1.35189733\n",
      "Iteration 70131, loss = 1.14908892\n",
      "Iteration 70132, loss = 1.20861724\n",
      "Iteration 70133, loss = 1.14770740\n",
      "Iteration 70134, loss = 1.10806902\n",
      "Iteration 70135, loss = 1.20844404\n",
      "Iteration 70136, loss = 1.54093766\n",
      "Iteration 70137, loss = 1.74450449\n",
      "Iteration 70138, loss = 1.76482025\n",
      "Iteration 70139, loss = 1.95412562\n",
      "Iteration 70140, loss = 2.91557919\n",
      "Iteration 70141, loss = 2.41031879\n",
      "Iteration 70142, loss = 2.49490355\n",
      "Iteration 70143, loss = 2.09690547\n",
      "Iteration 70144, loss = 1.74733509\n",
      "Iteration 70145, loss = 1.86108444\n",
      "Iteration 70146, loss = 1.35650923\n",
      "Iteration 70147, loss = 1.45652760\n",
      "Iteration 70148, loss = 1.08161193\n",
      "Iteration 70149, loss = 1.15556358\n",
      "Iteration 70150, loss = 1.29436403\n",
      "Iteration 70151, loss = 1.26575537\n",
      "Iteration 70152, loss = 1.24738486\n",
      "Iteration 70153, loss = 1.28339894\n",
      "Iteration 70154, loss = 1.31567753\n",
      "Iteration 70155, loss = 1.76746262\n",
      "Iteration 70156, loss = 1.69082180\n",
      "Iteration 70157, loss = 1.51864438\n",
      "Iteration 70158, loss = 1.22367621\n",
      "Iteration 70159, loss = 1.58031197\n",
      "Iteration 70160, loss = 1.18728579\n",
      "Iteration 70161, loss = 1.22840996\n",
      "Iteration 70162, loss = 1.32489926\n",
      "Iteration 70163, loss = 1.17796954\n",
      "Iteration 70164, loss = 1.60213104\n",
      "Iteration 70165, loss = 1.22413011\n",
      "Iteration 70166, loss = 1.12280710\n",
      "Iteration 70167, loss = 1.09460374\n",
      "Iteration 70168, loss = 1.06389756\n",
      "Iteration 70169, loss = 1.18438628\n",
      "Iteration 70170, loss = 1.13154082\n",
      "Iteration 70171, loss = 1.20859600\n",
      "Iteration 70172, loss = 1.31560077\n",
      "Iteration 70173, loss = 1.46560024\n",
      "Iteration 70174, loss = 1.69728827\n",
      "Iteration 70175, loss = 1.59676885\n",
      "Iteration 70176, loss = 1.47202369\n",
      "Iteration 70177, loss = 1.52392746\n",
      "Iteration 70178, loss = 1.60237744\n",
      "Iteration 70179, loss = 1.42365656\n",
      "Iteration 70180, loss = 1.53136725\n",
      "Iteration 70181, loss = 1.54933063\n",
      "Iteration 70182, loss = 1.50822943\n",
      "Iteration 70183, loss = 1.44636237\n",
      "Iteration 70184, loss = 1.54976665\n",
      "Iteration 70185, loss = 1.52456395\n",
      "Iteration 70186, loss = 1.44709047\n",
      "Iteration 70187, loss = 1.41585880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 70188, loss = 1.21499271\n",
      "Iteration 70189, loss = 1.47319865\n",
      "Iteration 70190, loss = 1.74622215\n",
      "Iteration 70191, loss = 1.82006352\n",
      "Iteration 70192, loss = 1.78735851\n",
      "Iteration 70193, loss = 1.29463107\n",
      "Iteration 70194, loss = 1.15330427\n",
      "Iteration 70195, loss = 1.18956386\n",
      "Iteration 70196, loss = 1.15405985\n",
      "Iteration 70197, loss = 1.16562050\n",
      "Iteration 70198, loss = 1.08816778\n",
      "Iteration 70199, loss = 1.14581935\n",
      "Iteration 70200, loss = 1.10979655\n",
      "Iteration 70201, loss = 1.21966507\n",
      "Iteration 70202, loss = 1.28153936\n",
      "Iteration 70203, loss = 1.33686759\n",
      "Iteration 70204, loss = 1.39097500\n",
      "Iteration 70205, loss = 1.34295310\n",
      "Iteration 70206, loss = 1.16507122\n",
      "Iteration 70207, loss = 1.11482755\n",
      "Iteration 70208, loss = 1.55098993\n",
      "Iteration 70209, loss = 1.41915435\n",
      "Iteration 70210, loss = 1.24341563\n",
      "Iteration 70211, loss = 1.25350185\n",
      "Iteration 70212, loss = 1.26017713\n",
      "Iteration 70213, loss = 1.09344875\n",
      "Iteration 70214, loss = 1.20694226\n",
      "Iteration 70215, loss = 1.29762559\n",
      "Iteration 70216, loss = 1.25828854\n",
      "Iteration 70217, loss = 1.22342305\n",
      "Iteration 70218, loss = 1.38639313\n",
      "Iteration 70219, loss = 1.69123051\n",
      "Iteration 70220, loss = 1.74282222\n",
      "Iteration 70221, loss = 1.83027074\n",
      "Iteration 70222, loss = 2.50133413\n",
      "Iteration 70223, loss = 2.43686371\n",
      "Iteration 70224, loss = 2.33129895\n",
      "Iteration 70225, loss = 2.34992292\n",
      "Iteration 70226, loss = 1.86865961\n",
      "Iteration 70227, loss = 1.55152900\n",
      "Iteration 70228, loss = 1.49366656\n",
      "Iteration 70229, loss = 1.44763152\n",
      "Iteration 70230, loss = 1.80275083\n",
      "Iteration 70231, loss = 1.39643957\n",
      "Iteration 70232, loss = 1.21983141\n",
      "Iteration 70233, loss = 1.18550712\n",
      "Iteration 70234, loss = 1.25579327\n",
      "Iteration 70235, loss = 1.06380006\n",
      "Iteration 70236, loss = 1.09388355\n",
      "Iteration 70237, loss = 1.09453110\n",
      "Iteration 70238, loss = 1.16980602\n",
      "Iteration 70239, loss = 1.21139703\n",
      "Iteration 70240, loss = 1.08556183\n",
      "Iteration 70241, loss = 1.04914974\n",
      "Iteration 70242, loss = 1.34398671\n",
      "Iteration 70243, loss = 1.18131150\n",
      "Iteration 70244, loss = 1.17395762\n",
      "Iteration 70245, loss = 1.21307925\n",
      "Iteration 70246, loss = 1.20081512\n",
      "Iteration 70247, loss = 1.16032241\n",
      "Iteration 70248, loss = 1.32829964\n",
      "Iteration 70249, loss = 1.23644485\n",
      "Iteration 70250, loss = 1.48256480\n",
      "Iteration 70251, loss = 1.38920776\n",
      "Iteration 70252, loss = 1.40362238\n",
      "Iteration 70253, loss = 1.43868923\n",
      "Iteration 70254, loss = 1.53894031\n",
      "Iteration 70255, loss = 1.64898521\n",
      "Iteration 70256, loss = 2.53381579\n",
      "Iteration 70257, loss = 2.40019766\n",
      "Iteration 70258, loss = 3.14385173\n",
      "Iteration 70259, loss = 2.98638283\n",
      "Iteration 70260, loss = 4.09125907\n",
      "Iteration 70261, loss = 3.67019289\n",
      "Iteration 70262, loss = 2.29511221\n",
      "Iteration 70263, loss = 3.65233181\n",
      "Iteration 70264, loss = 4.48444736\n",
      "Iteration 70265, loss = 4.18818511\n",
      "Iteration 70266, loss = 3.10676535\n",
      "Iteration 70267, loss = 1.77385797\n",
      "Iteration 70268, loss = 1.49742993\n",
      "Iteration 70269, loss = 1.34178973\n",
      "Iteration 70270, loss = 1.31894904\n",
      "Iteration 70271, loss = 1.38803182\n",
      "Iteration 70272, loss = 1.34641063\n",
      "Iteration 70273, loss = 1.59240071\n",
      "Iteration 70274, loss = 1.61906276\n",
      "Iteration 70275, loss = 1.31560455\n",
      "Iteration 70276, loss = 1.41547423\n",
      "Iteration 70277, loss = 1.50964958\n",
      "Iteration 70278, loss = 1.47074651\n",
      "Iteration 70279, loss = 1.41754620\n",
      "Iteration 70280, loss = 1.41012799\n",
      "Iteration 70281, loss = 1.61014588\n",
      "Iteration 70282, loss = 1.43633980\n",
      "Iteration 70283, loss = 1.26692840\n",
      "Iteration 70284, loss = 1.20944289\n",
      "Iteration 70285, loss = 1.05156456\n",
      "Iteration 70286, loss = 1.00310761\n",
      "Iteration 70287, loss = 1.15526306\n",
      "Iteration 70288, loss = 1.14469178\n",
      "Iteration 70289, loss = 1.17175942\n",
      "Iteration 70290, loss = 1.07570588\n",
      "Iteration 70291, loss = 1.12409501\n",
      "Iteration 70292, loss = 1.21321050\n",
      "Iteration 70293, loss = 1.17749889\n",
      "Iteration 70294, loss = 1.09065211\n",
      "Iteration 70295, loss = 1.14761771\n",
      "Iteration 70296, loss = 1.22890658\n",
      "Iteration 70297, loss = 1.43841404\n",
      "Iteration 70298, loss = 1.54279018\n",
      "Iteration 70299, loss = 1.36025681\n",
      "Iteration 70300, loss = 1.46034066\n",
      "Iteration 70301, loss = 1.25216430\n",
      "Iteration 70302, loss = 1.27124752\n",
      "Iteration 70303, loss = 1.15473794\n",
      "Iteration 70304, loss = 1.39666738\n",
      "Iteration 70305, loss = 1.31599735\n",
      "Iteration 70306, loss = 1.52932394\n",
      "Iteration 70307, loss = 1.36892518\n",
      "Iteration 70308, loss = 1.10436106\n",
      "Iteration 70309, loss = 1.28402529\n",
      "Iteration 70310, loss = 1.14958590\n",
      "Iteration 70311, loss = 1.05378777\n",
      "Iteration 70312, loss = 1.12377624\n",
      "Iteration 70313, loss = 1.04116269\n",
      "Iteration 70314, loss = 1.09621835\n",
      "Iteration 70315, loss = 1.04581833\n",
      "Iteration 70316, loss = 1.04839613\n",
      "Iteration 70317, loss = 1.11332149\n",
      "Iteration 70318, loss = 1.12413263\n",
      "Iteration 70319, loss = 0.98657791\n",
      "Iteration 70320, loss = 1.09215960\n",
      "Iteration 70321, loss = 1.20282480\n",
      "Iteration 70322, loss = 1.36533573\n",
      "Iteration 70323, loss = 1.18726103\n",
      "Iteration 70324, loss = 1.02365279\n",
      "Iteration 70325, loss = 1.00570454\n",
      "Iteration 70326, loss = 1.18714242\n",
      "Iteration 70327, loss = 1.26397724\n",
      "Iteration 70328, loss = 1.36296828\n",
      "Iteration 70329, loss = 1.28503556\n",
      "Iteration 70330, loss = 1.20018540\n",
      "Iteration 70331, loss = 1.25721875\n",
      "Iteration 70332, loss = 1.31661257\n",
      "Iteration 70333, loss = 1.20150861\n",
      "Iteration 70334, loss = 1.27577339\n",
      "Iteration 70335, loss = 1.12247374\n",
      "Iteration 70336, loss = 1.08849398\n",
      "Iteration 70337, loss = 1.09817250\n",
      "Iteration 70338, loss = 1.16301510\n",
      "Iteration 70339, loss = 1.05215306\n",
      "Iteration 70340, loss = 1.01180009\n",
      "Iteration 70341, loss = 1.10900928\n",
      "Iteration 70342, loss = 1.24020258\n",
      "Iteration 70343, loss = 1.33454420\n",
      "Iteration 70344, loss = 1.28124778\n",
      "Iteration 70345, loss = 1.07162255\n",
      "Iteration 70346, loss = 1.28099165\n",
      "Iteration 70347, loss = 1.11644309\n",
      "Iteration 70348, loss = 1.11201877\n",
      "Iteration 70349, loss = 1.01308143\n",
      "Iteration 70350, loss = 1.13060327\n",
      "Iteration 70351, loss = 1.09517754\n",
      "Iteration 70352, loss = 1.41620356\n",
      "Iteration 70353, loss = 1.51978801\n",
      "Iteration 70354, loss = 1.68387708\n",
      "Iteration 70355, loss = 3.37740260\n",
      "Iteration 70356, loss = 3.58842469\n",
      "Iteration 70357, loss = 3.06986949\n",
      "Iteration 70358, loss = 2.05865702\n",
      "Iteration 70359, loss = 2.40342676\n",
      "Iteration 70360, loss = 2.57235647\n",
      "Iteration 70361, loss = 2.99544575\n",
      "Iteration 70362, loss = 2.07579530\n",
      "Iteration 70363, loss = 1.61224543\n",
      "Iteration 70364, loss = 2.05355386\n",
      "Iteration 70365, loss = 1.95811407\n",
      "Iteration 70366, loss = 2.19242380\n",
      "Iteration 70367, loss = 1.89350362\n",
      "Iteration 70368, loss = 1.87901976\n",
      "Iteration 70369, loss = 1.65211670\n",
      "Iteration 70370, loss = 1.69128606\n",
      "Iteration 70371, loss = 1.82957526\n",
      "Iteration 70372, loss = 1.63289937\n",
      "Iteration 70373, loss = 2.21887711\n",
      "Iteration 70374, loss = 1.85386181\n",
      "Iteration 70375, loss = 2.35559710\n",
      "Iteration 70376, loss = 1.71540942\n",
      "Iteration 70377, loss = 2.20756485\n",
      "Iteration 70378, loss = 2.21505750\n",
      "Iteration 70379, loss = 1.84222201\n",
      "Iteration 70380, loss = 1.62226019\n",
      "Iteration 70381, loss = 1.68214510\n",
      "Iteration 70382, loss = 1.33880243\n",
      "Iteration 70383, loss = 1.24038945\n",
      "Iteration 70384, loss = 1.39298653\n",
      "Iteration 70385, loss = 1.97639332\n",
      "Iteration 70386, loss = 1.64183058\n",
      "Iteration 70387, loss = 2.07302367\n",
      "Iteration 70388, loss = 2.12937999\n",
      "Iteration 70389, loss = 2.19440451\n",
      "Iteration 70390, loss = 1.52244379\n",
      "Iteration 70391, loss = 1.86507310\n",
      "Iteration 70392, loss = 2.11490694\n",
      "Iteration 70393, loss = 1.33707620\n",
      "Iteration 70394, loss = 1.36737398\n",
      "Iteration 70395, loss = 1.38025791\n",
      "Iteration 70396, loss = 1.11586848\n",
      "Iteration 70397, loss = 1.06401719\n",
      "Iteration 70398, loss = 1.14418548\n",
      "Iteration 70399, loss = 1.05968425\n",
      "Iteration 70400, loss = 1.06326008\n",
      "Iteration 70401, loss = 1.16651652\n",
      "Iteration 70402, loss = 1.07718078\n",
      "Iteration 70403, loss = 1.30333447\n",
      "Iteration 70404, loss = 1.12907078\n",
      "Iteration 70405, loss = 1.04400784\n",
      "Iteration 70406, loss = 1.28841014\n",
      "Iteration 70407, loss = 1.11092108\n",
      "Iteration 70408, loss = 1.33425159\n",
      "Iteration 70409, loss = 1.60969883\n",
      "Iteration 70410, loss = 1.67469108\n",
      "Iteration 70411, loss = 1.48651480\n",
      "Iteration 70412, loss = 1.38779144\n",
      "Iteration 70413, loss = 1.42528437\n",
      "Iteration 70414, loss = 1.29583842\n",
      "Iteration 70415, loss = 1.20442383\n",
      "Iteration 70416, loss = 1.34506770\n",
      "Iteration 70417, loss = 1.08773533\n",
      "Iteration 70418, loss = 1.04621227\n",
      "Iteration 70419, loss = 1.15800173\n",
      "Iteration 70420, loss = 1.32529808\n",
      "Iteration 70421, loss = 1.18009597\n",
      "Iteration 70422, loss = 1.20000529\n",
      "Iteration 70423, loss = 1.36194709\n",
      "Iteration 70424, loss = 1.16187151\n",
      "Iteration 70425, loss = 1.33525437\n",
      "Iteration 70426, loss = 1.12184541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 70427, loss = 1.12481785\n",
      "Iteration 70428, loss = 1.17402466\n",
      "Iteration 70429, loss = 1.04003012\n",
      "Iteration 70430, loss = 1.04113986\n",
      "Iteration 70431, loss = 1.16699911\n",
      "Iteration 70432, loss = 1.05389041\n",
      "Iteration 70433, loss = 1.12113013\n",
      "Iteration 70434, loss = 1.09527186\n",
      "Iteration 70435, loss = 1.05261451\n",
      "Iteration 70436, loss = 1.18898032\n",
      "Iteration 70437, loss = 1.40566925\n",
      "Iteration 70438, loss = 1.23215712\n",
      "Iteration 70439, loss = 1.68670240\n",
      "Iteration 70440, loss = 1.23318251\n",
      "Iteration 70441, loss = 1.46374744\n",
      "Iteration 70442, loss = 1.60361701\n",
      "Iteration 70443, loss = 1.29035058\n",
      "Iteration 70444, loss = 1.32809532\n",
      "Iteration 70445, loss = 1.56578853\n",
      "Iteration 70446, loss = 1.57303231\n",
      "Iteration 70447, loss = 1.37819838\n",
      "Iteration 70448, loss = 2.05137071\n",
      "Iteration 70449, loss = 1.46978760\n",
      "Iteration 70450, loss = 1.38470470\n",
      "Iteration 70451, loss = 1.20808953\n",
      "Iteration 70452, loss = 1.30599433\n",
      "Iteration 70453, loss = 1.04489357\n",
      "Iteration 70454, loss = 1.27018416\n",
      "Iteration 70455, loss = 1.72641836\n",
      "Iteration 70456, loss = 1.99337785\n",
      "Iteration 70457, loss = 1.92617477\n",
      "Iteration 70458, loss = 2.08213413\n",
      "Iteration 70459, loss = 2.02636421\n",
      "Iteration 70460, loss = 1.87017478\n",
      "Iteration 70461, loss = 1.89143663\n",
      "Iteration 70462, loss = 1.77123120\n",
      "Iteration 70463, loss = 1.35158137\n",
      "Iteration 70464, loss = 1.34696553\n",
      "Iteration 70465, loss = 1.79824163\n",
      "Iteration 70466, loss = 1.81536884\n",
      "Iteration 70467, loss = 1.33431121\n",
      "Iteration 70468, loss = 1.23433072\n",
      "Iteration 70469, loss = 1.28200341\n",
      "Iteration 70470, loss = 1.67349239\n",
      "Iteration 70471, loss = 1.59975384\n",
      "Iteration 70472, loss = 1.68928892\n",
      "Iteration 70473, loss = 1.74422413\n",
      "Iteration 70474, loss = 1.44593369\n",
      "Iteration 70475, loss = 1.34226674\n",
      "Iteration 70476, loss = 1.14885767\n",
      "Iteration 70477, loss = 1.12925088\n",
      "Iteration 70478, loss = 1.20319003\n",
      "Iteration 70479, loss = 1.45732767\n",
      "Iteration 70480, loss = 1.39276915\n",
      "Iteration 70481, loss = 1.32124247\n",
      "Iteration 70482, loss = 1.13908885\n",
      "Iteration 70483, loss = 1.10170292\n",
      "Iteration 70484, loss = 1.15483179\n",
      "Iteration 70485, loss = 0.96252317\n",
      "Iteration 70486, loss = 1.02492214\n",
      "Iteration 70487, loss = 1.09044223\n",
      "Iteration 70488, loss = 1.09876951\n",
      "Iteration 70489, loss = 1.18467868\n",
      "Iteration 70490, loss = 1.37520164\n",
      "Iteration 70491, loss = 1.09374556\n",
      "Iteration 70492, loss = 1.15016849\n",
      "Iteration 70493, loss = 1.10835527\n",
      "Iteration 70494, loss = 1.02797095\n",
      "Iteration 70495, loss = 0.98166207\n",
      "Iteration 70496, loss = 1.03662258\n",
      "Iteration 70497, loss = 1.06430802\n",
      "Iteration 70498, loss = 1.08323473\n",
      "Iteration 70499, loss = 1.03688567\n",
      "Iteration 70500, loss = 1.07163380\n",
      "Iteration 70501, loss = 1.10544559\n",
      "Iteration 70502, loss = 1.01526993\n",
      "Iteration 70503, loss = 1.13240383\n",
      "Iteration 70504, loss = 1.07136662\n",
      "Iteration 70505, loss = 1.07124127\n",
      "Iteration 70506, loss = 1.13293968\n",
      "Iteration 70507, loss = 1.36466628\n",
      "Iteration 70508, loss = 1.30578203\n",
      "Iteration 70509, loss = 1.19255358\n",
      "Iteration 70510, loss = 1.81658850\n",
      "Iteration 70511, loss = 1.51610284\n",
      "Iteration 70512, loss = 1.50812547\n",
      "Iteration 70513, loss = 1.47121379\n",
      "Iteration 70514, loss = 1.38391365\n",
      "Iteration 70515, loss = 1.48785771\n",
      "Iteration 70516, loss = 1.20642441\n",
      "Iteration 70517, loss = 1.12818016\n",
      "Iteration 70518, loss = 1.10116655\n",
      "Iteration 70519, loss = 1.24778486\n",
      "Iteration 70520, loss = 1.32026975\n",
      "Iteration 70521, loss = 1.53848518\n",
      "Iteration 70522, loss = 1.60404721\n",
      "Iteration 70523, loss = 1.39345025\n",
      "Iteration 70524, loss = 1.11279666\n",
      "Iteration 70525, loss = 1.23052350\n",
      "Iteration 70526, loss = 1.34827837\n",
      "Iteration 70527, loss = 1.23139329\n",
      "Iteration 70528, loss = 1.28291923\n",
      "Iteration 70529, loss = 1.31485493\n",
      "Iteration 70530, loss = 1.50152891\n",
      "Iteration 70531, loss = 1.34707880\n",
      "Iteration 70532, loss = 1.31183271\n",
      "Iteration 70533, loss = 1.36845238\n",
      "Iteration 70534, loss = 1.48936687\n",
      "Iteration 70535, loss = 1.54480692\n",
      "Iteration 70536, loss = 1.37007561\n",
      "Iteration 70537, loss = 1.10260667\n",
      "Iteration 70538, loss = 1.20543222\n",
      "Iteration 70539, loss = 1.18423662\n",
      "Iteration 70540, loss = 1.15137317\n",
      "Iteration 70541, loss = 1.15592289\n",
      "Iteration 70542, loss = 1.47870084\n",
      "Iteration 70543, loss = 1.39437032\n",
      "Iteration 70544, loss = 1.49816145\n",
      "Iteration 70545, loss = 1.18275485\n",
      "Iteration 70546, loss = 1.07465132\n",
      "Iteration 70547, loss = 1.15208048\n",
      "Iteration 70548, loss = 1.24923398\n",
      "Iteration 70549, loss = 1.20113952\n",
      "Iteration 70550, loss = 1.12128462\n",
      "Iteration 70551, loss = 1.26991005\n",
      "Iteration 70552, loss = 1.19077294\n",
      "Iteration 70553, loss = 1.38079700\n",
      "Iteration 70554, loss = 1.36039792\n",
      "Iteration 70555, loss = 1.23338890\n",
      "Iteration 70556, loss = 1.41885141\n",
      "Iteration 70557, loss = 1.31525357\n",
      "Iteration 70558, loss = 1.20927738\n",
      "Iteration 70559, loss = 1.22782847\n",
      "Iteration 70560, loss = 1.37818128\n",
      "Iteration 70561, loss = 1.16120753\n",
      "Iteration 70562, loss = 1.26180671\n",
      "Iteration 70563, loss = 1.08396333\n",
      "Iteration 70564, loss = 1.24997151\n",
      "Iteration 70565, loss = 1.28791527\n",
      "Iteration 70566, loss = 1.22119122\n",
      "Iteration 70567, loss = 1.23136040\n",
      "Iteration 70568, loss = 1.50776777\n",
      "Iteration 70569, loss = 1.53250141\n",
      "Iteration 70570, loss = 1.57226189\n",
      "Iteration 70571, loss = 1.37428096\n",
      "Iteration 70572, loss = 1.51367634\n",
      "Iteration 70573, loss = 1.58039479\n",
      "Iteration 70574, loss = 1.54265567\n",
      "Iteration 70575, loss = 1.47715385\n",
      "Iteration 70576, loss = 1.24641553\n",
      "Iteration 70577, loss = 1.26231154\n",
      "Iteration 70578, loss = 1.38906139\n",
      "Iteration 70579, loss = 1.26218078\n",
      "Iteration 70580, loss = 1.20841684\n",
      "Iteration 70581, loss = 1.19710577\n",
      "Iteration 70582, loss = 1.16301149\n",
      "Iteration 70583, loss = 1.14779781\n",
      "Iteration 70584, loss = 1.06207323\n",
      "Iteration 70585, loss = 1.05702904\n",
      "Iteration 70586, loss = 1.07066940\n",
      "Iteration 70587, loss = 1.14603535\n",
      "Iteration 70588, loss = 1.29394246\n",
      "Iteration 70589, loss = 1.52933259\n",
      "Iteration 70590, loss = 1.40208837\n",
      "Iteration 70591, loss = 1.32067200\n",
      "Iteration 70592, loss = 1.59420346\n",
      "Iteration 70593, loss = 1.40575766\n",
      "Iteration 70594, loss = 1.24789156\n",
      "Iteration 70595, loss = 1.11048892\n",
      "Iteration 70596, loss = 1.15782911\n",
      "Iteration 70597, loss = 1.15210777\n",
      "Iteration 70598, loss = 1.21714993\n",
      "Iteration 70599, loss = 1.20939168\n",
      "Iteration 70600, loss = 1.23840124\n",
      "Iteration 70601, loss = 1.15591951\n",
      "Iteration 70602, loss = 1.52072531\n",
      "Iteration 70603, loss = 1.53668870\n",
      "Iteration 70604, loss = 1.81800212\n",
      "Iteration 70605, loss = 1.41509148\n",
      "Iteration 70606, loss = 1.26334272\n",
      "Iteration 70607, loss = 1.12997353\n",
      "Iteration 70608, loss = 1.44392324\n",
      "Iteration 70609, loss = 1.59247747\n",
      "Iteration 70610, loss = 1.22720755\n",
      "Iteration 70611, loss = 1.57457980\n",
      "Iteration 70612, loss = 1.32327692\n",
      "Iteration 70613, loss = 1.92787306\n",
      "Iteration 70614, loss = 1.70490770\n",
      "Iteration 70615, loss = 1.84179531\n",
      "Iteration 70616, loss = 1.95451673\n",
      "Iteration 70617, loss = 2.23433445\n",
      "Iteration 70618, loss = 2.30701819\n",
      "Iteration 70619, loss = 1.76115635\n",
      "Iteration 70620, loss = 2.02624021\n",
      "Iteration 70621, loss = 1.94313222\n",
      "Iteration 70622, loss = 1.80872113\n",
      "Iteration 70623, loss = 1.40376392\n",
      "Iteration 70624, loss = 1.22110657\n",
      "Iteration 70625, loss = 1.37164376\n",
      "Iteration 70626, loss = 1.56380564\n",
      "Iteration 70627, loss = 1.41336423\n",
      "Iteration 70628, loss = 1.60321096\n",
      "Iteration 70629, loss = 1.67290564\n",
      "Iteration 70630, loss = 1.26530876\n",
      "Iteration 70631, loss = 1.04620089\n",
      "Iteration 70632, loss = 1.01403394\n",
      "Iteration 70633, loss = 1.24364916\n",
      "Iteration 70634, loss = 1.24219015\n",
      "Iteration 70635, loss = 1.27009979\n",
      "Iteration 70636, loss = 1.27462982\n",
      "Iteration 70637, loss = 1.14259116\n",
      "Iteration 70638, loss = 1.13324062\n",
      "Iteration 70639, loss = 1.38758228\n",
      "Iteration 70640, loss = 1.24897639\n",
      "Iteration 70641, loss = 1.36164586\n",
      "Iteration 70642, loss = 1.09065291\n",
      "Iteration 70643, loss = 1.48394708\n",
      "Iteration 70644, loss = 1.41772537\n",
      "Iteration 70645, loss = 1.20913132\n",
      "Iteration 70646, loss = 1.21663276\n",
      "Iteration 70647, loss = 1.23451236\n",
      "Iteration 70648, loss = 1.08927648\n",
      "Iteration 70649, loss = 1.14139039\n",
      "Iteration 70650, loss = 1.30575585\n",
      "Iteration 70651, loss = 1.65953399\n",
      "Iteration 70652, loss = 1.24990335\n",
      "Iteration 70653, loss = 1.33209918\n",
      "Iteration 70654, loss = 1.35592910\n",
      "Iteration 70655, loss = 1.45194419\n",
      "Iteration 70656, loss = 1.26567302\n",
      "Iteration 70657, loss = 1.45852042\n",
      "Iteration 70658, loss = 1.38954405\n",
      "Iteration 70659, loss = 1.27087328\n",
      "Iteration 70660, loss = 1.14844086\n",
      "Iteration 70661, loss = 1.16307196\n",
      "Iteration 70662, loss = 1.32831951\n",
      "Iteration 70663, loss = 1.41655600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 70664, loss = 1.34149853\n",
      "Iteration 70665, loss = 1.48453006\n",
      "Iteration 70666, loss = 1.48868083\n",
      "Iteration 70667, loss = 1.48038313\n",
      "Iteration 70668, loss = 2.00068345\n",
      "Iteration 70669, loss = 1.83375149\n",
      "Iteration 70670, loss = 1.56692490\n",
      "Iteration 70671, loss = 1.30576364\n",
      "Iteration 70672, loss = 1.15233155\n",
      "Iteration 70673, loss = 1.52567662\n",
      "Iteration 70674, loss = 1.26550860\n",
      "Iteration 70675, loss = 1.12044129\n",
      "Iteration 70676, loss = 1.15574033\n",
      "Iteration 70677, loss = 1.18498259\n",
      "Iteration 70678, loss = 1.05792328\n",
      "Iteration 70679, loss = 1.12074028\n",
      "Iteration 70680, loss = 1.07290217\n",
      "Iteration 70681, loss = 1.23955169\n",
      "Iteration 70682, loss = 1.17979100\n",
      "Iteration 70683, loss = 1.53256716\n",
      "Iteration 70684, loss = 1.26688162\n",
      "Iteration 70685, loss = 1.31653227\n",
      "Iteration 70686, loss = 1.34401372\n",
      "Iteration 70687, loss = 1.15991595\n",
      "Iteration 70688, loss = 1.52193101\n",
      "Iteration 70689, loss = 1.57083918\n",
      "Iteration 70690, loss = 1.19397058\n",
      "Iteration 70691, loss = 1.30347907\n",
      "Iteration 70692, loss = 1.13012957\n",
      "Iteration 70693, loss = 1.14423524\n",
      "Iteration 70694, loss = 1.19425692\n",
      "Iteration 70695, loss = 1.18453886\n",
      "Iteration 70696, loss = 1.03449738\n",
      "Iteration 70697, loss = 1.13181814\n",
      "Iteration 70698, loss = 1.26422625\n",
      "Iteration 70699, loss = 1.38231463\n",
      "Iteration 70700, loss = 1.36585676\n",
      "Iteration 70701, loss = 1.21772753\n",
      "Iteration 70702, loss = 1.16843564\n",
      "Iteration 70703, loss = 1.15736332\n",
      "Iteration 70704, loss = 1.26597541\n",
      "Iteration 70705, loss = 1.00979332\n",
      "Iteration 70706, loss = 1.09883897\n",
      "Iteration 70707, loss = 1.01339354\n",
      "Iteration 70708, loss = 1.07308250\n",
      "Iteration 70709, loss = 1.14884226\n",
      "Iteration 70710, loss = 1.11430108\n",
      "Iteration 70711, loss = 1.37745496\n",
      "Iteration 70712, loss = 1.39693510\n",
      "Iteration 70713, loss = 1.47019802\n",
      "Iteration 70714, loss = 1.31480097\n",
      "Iteration 70715, loss = 1.29701352\n",
      "Iteration 70716, loss = 1.59720657\n",
      "Iteration 70717, loss = 1.60936147\n",
      "Iteration 70718, loss = 1.41707735\n",
      "Iteration 70719, loss = 1.45206314\n",
      "Iteration 70720, loss = 1.32677363\n",
      "Iteration 70721, loss = 1.31194571\n",
      "Iteration 70722, loss = 1.05834872\n",
      "Iteration 70723, loss = 1.16874912\n",
      "Iteration 70724, loss = 1.07152816\n",
      "Iteration 70725, loss = 1.14258748\n",
      "Iteration 70726, loss = 1.15013008\n",
      "Iteration 70727, loss = 1.13599543\n",
      "Iteration 70728, loss = 1.27496670\n",
      "Iteration 70729, loss = 1.29217952\n",
      "Iteration 70730, loss = 1.39456371\n",
      "Iteration 70731, loss = 1.41970568\n",
      "Iteration 70732, loss = 1.46664212\n",
      "Iteration 70733, loss = 1.24821619\n",
      "Iteration 70734, loss = 1.09844208\n",
      "Iteration 70735, loss = 1.60696134\n",
      "Iteration 70736, loss = 1.31606869\n",
      "Iteration 70737, loss = 1.45366685\n",
      "Iteration 70738, loss = 1.20387500\n",
      "Iteration 70739, loss = 1.07767389\n",
      "Iteration 70740, loss = 1.22833439\n",
      "Iteration 70741, loss = 1.05729397\n",
      "Iteration 70742, loss = 1.19648124\n",
      "Iteration 70743, loss = 1.53086471\n",
      "Iteration 70744, loss = 1.30864368\n",
      "Iteration 70745, loss = 1.22367840\n",
      "Iteration 70746, loss = 1.05986549\n",
      "Iteration 70747, loss = 1.04287163\n",
      "Iteration 70748, loss = 1.01671744\n",
      "Iteration 70749, loss = 1.15786531\n",
      "Iteration 70750, loss = 1.08476218\n",
      "Iteration 70751, loss = 1.22987090\n",
      "Iteration 70752, loss = 1.11414832\n",
      "Iteration 70753, loss = 1.39692890\n",
      "Iteration 70754, loss = 1.68263054\n",
      "Iteration 70755, loss = 1.36837103\n",
      "Iteration 70756, loss = 1.40700528\n",
      "Iteration 70757, loss = 1.49279245\n",
      "Iteration 70758, loss = 1.40169064\n",
      "Iteration 70759, loss = 1.27115493\n",
      "Iteration 70760, loss = 1.22358633\n",
      "Iteration 70761, loss = 1.22016555\n",
      "Iteration 70762, loss = 1.17136715\n",
      "Iteration 70763, loss = 1.09537692\n",
      "Iteration 70764, loss = 0.97939360\n",
      "Iteration 70765, loss = 1.07601273\n",
      "Iteration 70766, loss = 1.17983565\n",
      "Iteration 70767, loss = 1.20041977\n",
      "Iteration 70768, loss = 1.10929154\n",
      "Iteration 70769, loss = 1.35147656\n",
      "Iteration 70770, loss = 1.38967390\n",
      "Iteration 70771, loss = 1.57503885\n",
      "Iteration 70772, loss = 1.40762769\n",
      "Iteration 70773, loss = 1.61041761\n",
      "Iteration 70774, loss = 1.23304243\n",
      "Iteration 70775, loss = 1.54612438\n",
      "Iteration 70776, loss = 1.30091878\n",
      "Iteration 70777, loss = 1.40592171\n",
      "Iteration 70778, loss = 1.18943493\n",
      "Iteration 70779, loss = 1.07174976\n",
      "Iteration 70780, loss = 1.15963099\n",
      "Iteration 70781, loss = 1.43049627\n",
      "Iteration 70782, loss = 1.32110029\n",
      "Iteration 70783, loss = 1.09230161\n",
      "Iteration 70784, loss = 1.15096166\n",
      "Iteration 70785, loss = 1.18381996\n",
      "Iteration 70786, loss = 1.17556161\n",
      "Iteration 70787, loss = 1.33262814\n",
      "Iteration 70788, loss = 1.53279711\n",
      "Iteration 70789, loss = 2.31378554\n",
      "Iteration 70790, loss = 1.88590369\n",
      "Iteration 70791, loss = 5.41062534\n",
      "Iteration 70792, loss = 8.96622699\n",
      "Iteration 70793, loss = 7.33625847\n",
      "Iteration 70794, loss = 4.21956182\n",
      "Iteration 70795, loss = 3.17389861\n",
      "Iteration 70796, loss = 3.39182773\n",
      "Iteration 70797, loss = 3.16617622\n",
      "Iteration 70798, loss = 2.83249097\n",
      "Iteration 70799, loss = 2.61265080\n",
      "Iteration 70800, loss = 3.03285468\n",
      "Iteration 70801, loss = 2.25127484\n",
      "Iteration 70802, loss = 1.86458045\n",
      "Iteration 70803, loss = 1.82718329\n",
      "Iteration 70804, loss = 1.62825958\n",
      "Iteration 70805, loss = 1.58283202\n",
      "Iteration 70806, loss = 1.38561522\n",
      "Iteration 70807, loss = 1.18300352\n",
      "Iteration 70808, loss = 1.19698910\n",
      "Iteration 70809, loss = 1.10613696\n",
      "Iteration 70810, loss = 1.10863665\n",
      "Iteration 70811, loss = 1.20390597\n",
      "Iteration 70812, loss = 1.26392878\n",
      "Iteration 70813, loss = 1.23860708\n",
      "Iteration 70814, loss = 1.42886851\n",
      "Iteration 70815, loss = 1.71337714\n",
      "Iteration 70816, loss = 2.11945341\n",
      "Iteration 70817, loss = 1.78365442\n",
      "Iteration 70818, loss = 1.52581998\n",
      "Iteration 70819, loss = 1.40214793\n",
      "Iteration 70820, loss = 1.42539272\n",
      "Iteration 70821, loss = 1.34901706\n",
      "Iteration 70822, loss = 1.42202821\n",
      "Iteration 70823, loss = 1.17456678\n",
      "Iteration 70824, loss = 1.39690171\n",
      "Iteration 70825, loss = 1.65805390\n",
      "Iteration 70826, loss = 1.62346587\n",
      "Iteration 70827, loss = 1.52132487\n",
      "Iteration 70828, loss = 1.41759376\n",
      "Iteration 70829, loss = 1.31504199\n",
      "Iteration 70830, loss = 1.27811445\n",
      "Iteration 70831, loss = 1.37090247\n",
      "Iteration 70832, loss = 1.30929842\n",
      "Iteration 70833, loss = 1.15563264\n",
      "Iteration 70834, loss = 1.46106833\n",
      "Iteration 70835, loss = 2.73638571\n",
      "Iteration 70836, loss = 3.65009734\n",
      "Iteration 70837, loss = 2.50338767\n",
      "Iteration 70838, loss = 1.73934750\n",
      "Iteration 70839, loss = 1.64214466\n",
      "Iteration 70840, loss = 1.63989059\n",
      "Iteration 70841, loss = 1.70126916\n",
      "Iteration 70842, loss = 1.42041987\n",
      "Iteration 70843, loss = 1.26263543\n",
      "Iteration 70844, loss = 1.05296757\n",
      "Iteration 70845, loss = 1.46727383\n",
      "Iteration 70846, loss = 1.60022251\n",
      "Iteration 70847, loss = 1.96768971\n",
      "Iteration 70848, loss = 2.08898624\n",
      "Iteration 70849, loss = 1.65714702\n",
      "Iteration 70850, loss = 1.63588022\n",
      "Iteration 70851, loss = 1.27919901\n",
      "Iteration 70852, loss = 1.28871269\n",
      "Iteration 70853, loss = 1.28062518\n",
      "Iteration 70854, loss = 1.27260709\n",
      "Iteration 70855, loss = 1.31180360\n",
      "Iteration 70856, loss = 1.54360434\n",
      "Iteration 70857, loss = 1.33505457\n",
      "Iteration 70858, loss = 1.34814862\n",
      "Iteration 70859, loss = 1.74202695\n",
      "Iteration 70860, loss = 1.66036121\n",
      "Iteration 70861, loss = 1.36633334\n",
      "Iteration 70862, loss = 1.29946966\n",
      "Iteration 70863, loss = 1.38583322\n",
      "Iteration 70864, loss = 1.38812779\n",
      "Iteration 70865, loss = 1.63091545\n",
      "Iteration 70866, loss = 1.30509282\n",
      "Iteration 70867, loss = 1.24525009\n",
      "Iteration 70868, loss = 1.05833638\n",
      "Iteration 70869, loss = 1.17713745\n",
      "Iteration 70870, loss = 1.11515118\n",
      "Iteration 70871, loss = 1.07392789\n",
      "Iteration 70872, loss = 1.04745250\n",
      "Iteration 70873, loss = 1.26955630\n",
      "Iteration 70874, loss = 1.22946358\n",
      "Iteration 70875, loss = 1.11060140\n",
      "Iteration 70876, loss = 1.12082563\n",
      "Iteration 70877, loss = 1.06620359\n",
      "Iteration 70878, loss = 1.03322406\n",
      "Iteration 70879, loss = 1.22212084\n",
      "Iteration 70880, loss = 1.29147884\n",
      "Iteration 70881, loss = 1.30582436\n",
      "Iteration 70882, loss = 1.34830834\n",
      "Iteration 70883, loss = 1.36464310\n",
      "Iteration 70884, loss = 1.48813721\n",
      "Iteration 70885, loss = 1.50866905\n",
      "Iteration 70886, loss = 1.42227250\n",
      "Iteration 70887, loss = 1.29328475\n",
      "Iteration 70888, loss = 1.24019358\n",
      "Iteration 70889, loss = 1.06295747\n",
      "Iteration 70890, loss = 1.16041692\n",
      "Iteration 70891, loss = 1.21620152\n",
      "Iteration 70892, loss = 1.21368525\n",
      "Iteration 70893, loss = 1.15342751\n",
      "Iteration 70894, loss = 1.11540530\n",
      "Iteration 70895, loss = 1.24504180\n",
      "Iteration 70896, loss = 1.15231462\n",
      "Iteration 70897, loss = 1.30842158\n",
      "Iteration 70898, loss = 1.03776161\n",
      "Iteration 70899, loss = 1.76573684\n",
      "Iteration 70900, loss = 1.62331862\n",
      "Iteration 70901, loss = 1.67737064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 70902, loss = 1.60009637\n",
      "Iteration 70903, loss = 1.48325243\n",
      "Iteration 70904, loss = 1.46888444\n",
      "Iteration 70905, loss = 1.27297844\n",
      "Iteration 70906, loss = 1.27486214\n",
      "Iteration 70907, loss = 1.32149121\n",
      "Iteration 70908, loss = 1.59774845\n",
      "Iteration 70909, loss = 1.95971335\n",
      "Iteration 70910, loss = 1.50012431\n",
      "Iteration 70911, loss = 1.56503424\n",
      "Iteration 70912, loss = 1.14811651\n",
      "Iteration 70913, loss = 1.30692415\n",
      "Iteration 70914, loss = 1.25709778\n",
      "Iteration 70915, loss = 1.69712800\n",
      "Iteration 70916, loss = 1.80156402\n",
      "Iteration 70917, loss = 1.71787880\n",
      "Iteration 70918, loss = 1.35279967\n",
      "Iteration 70919, loss = 1.23976351\n",
      "Iteration 70920, loss = 1.41759503\n",
      "Iteration 70921, loss = 1.42186456\n",
      "Iteration 70922, loss = 1.31451196\n",
      "Iteration 70923, loss = 1.64156075\n",
      "Iteration 70924, loss = 1.71774197\n",
      "Iteration 70925, loss = 1.57709330\n",
      "Iteration 70926, loss = 1.53094451\n",
      "Iteration 70927, loss = 1.57124789\n",
      "Iteration 70928, loss = 1.72699517\n",
      "Iteration 70929, loss = 1.18890668\n",
      "Iteration 70930, loss = 1.19952684\n",
      "Iteration 70931, loss = 1.29536010\n",
      "Iteration 70932, loss = 1.14725010\n",
      "Iteration 70933, loss = 1.19024566\n",
      "Iteration 70934, loss = 1.09862226\n",
      "Iteration 70935, loss = 1.12267514\n",
      "Iteration 70936, loss = 1.13189862\n",
      "Iteration 70937, loss = 1.32727925\n",
      "Iteration 70938, loss = 1.37130808\n",
      "Iteration 70939, loss = 1.70951784\n",
      "Iteration 70940, loss = 1.47123031\n",
      "Iteration 70941, loss = 1.17052623\n",
      "Iteration 70942, loss = 1.31773123\n",
      "Iteration 70943, loss = 1.46657855\n",
      "Iteration 70944, loss = 1.06337980\n",
      "Iteration 70945, loss = 1.13214996\n",
      "Iteration 70946, loss = 1.23021684\n",
      "Iteration 70947, loss = 1.04170651\n",
      "Iteration 70948, loss = 1.31910827\n",
      "Iteration 70949, loss = 1.16804433\n",
      "Iteration 70950, loss = 1.09705203\n",
      "Iteration 70951, loss = 1.16400403\n",
      "Iteration 70952, loss = 1.13558042\n",
      "Iteration 70953, loss = 1.26485979\n",
      "Iteration 70954, loss = 1.25412389\n",
      "Iteration 70955, loss = 1.24083599\n",
      "Iteration 70956, loss = 1.21335996\n",
      "Iteration 70957, loss = 1.29343015\n",
      "Iteration 70958, loss = 1.16640109\n",
      "Iteration 70959, loss = 1.33321713\n",
      "Iteration 70960, loss = 1.88105065\n",
      "Iteration 70961, loss = 1.64390075\n",
      "Iteration 70962, loss = 1.41919545\n",
      "Iteration 70963, loss = 1.25321256\n",
      "Iteration 70964, loss = 1.09349505\n",
      "Iteration 70965, loss = 1.22539733\n",
      "Iteration 70966, loss = 1.22369500\n",
      "Iteration 70967, loss = 1.22697996\n",
      "Iteration 70968, loss = 1.32876382\n",
      "Iteration 70969, loss = 1.14227320\n",
      "Iteration 70970, loss = 1.07639463\n",
      "Iteration 70971, loss = 1.12988145\n",
      "Iteration 70972, loss = 1.13181382\n",
      "Iteration 70973, loss = 1.19402145\n",
      "Iteration 70974, loss = 1.20763079\n",
      "Iteration 70975, loss = 1.29486783\n",
      "Iteration 70976, loss = 1.25802960\n",
      "Iteration 70977, loss = 1.41313283\n",
      "Iteration 70978, loss = 1.48790234\n",
      "Iteration 70979, loss = 1.41894713\n",
      "Iteration 70980, loss = 1.64046308\n",
      "Iteration 70981, loss = 1.75901127\n",
      "Iteration 70982, loss = 1.28361837\n",
      "Iteration 70983, loss = 1.11750790\n",
      "Iteration 70984, loss = 1.20106812\n",
      "Iteration 70985, loss = 1.35783129\n",
      "Iteration 70986, loss = 1.26202132\n",
      "Iteration 70987, loss = 1.40538900\n",
      "Iteration 70988, loss = 1.04002175\n",
      "Iteration 70989, loss = 1.35315006\n",
      "Iteration 70990, loss = 1.20029501\n",
      "Iteration 70991, loss = 1.12412927\n",
      "Iteration 70992, loss = 1.01288503\n",
      "Iteration 70993, loss = 1.05818775\n",
      "Iteration 70994, loss = 1.10845203\n",
      "Iteration 70995, loss = 1.17280989\n",
      "Iteration 70996, loss = 1.14285004\n",
      "Iteration 70997, loss = 1.55645607\n",
      "Iteration 70998, loss = 1.30641286\n",
      "Iteration 70999, loss = 1.11974700\n",
      "Iteration 71000, loss = 1.40474737\n",
      "Iteration 71001, loss = 1.20831102\n",
      "Iteration 71002, loss = 1.79081733\n",
      "Iteration 71003, loss = 1.79214383\n",
      "Iteration 71004, loss = 1.83531371\n",
      "Iteration 71005, loss = 1.77764353\n",
      "Iteration 71006, loss = 1.66995106\n",
      "Iteration 71007, loss = 1.32424382\n",
      "Iteration 71008, loss = 1.11459585\n",
      "Iteration 71009, loss = 1.09079041\n",
      "Iteration 71010, loss = 1.14948573\n",
      "Iteration 71011, loss = 1.11252370\n",
      "Iteration 71012, loss = 1.10756025\n",
      "Iteration 71013, loss = 1.11224386\n",
      "Iteration 71014, loss = 1.46444255\n",
      "Iteration 71015, loss = 1.22469832\n",
      "Iteration 71016, loss = 1.42458832\n",
      "Iteration 71017, loss = 1.46750051\n",
      "Iteration 71018, loss = 1.46700409\n",
      "Iteration 71019, loss = 1.35464987\n",
      "Iteration 71020, loss = 1.22329834\n",
      "Iteration 71021, loss = 1.30860537\n",
      "Iteration 71022, loss = 1.15683046\n",
      "Iteration 71023, loss = 1.33900257\n",
      "Iteration 71024, loss = 1.08919756\n",
      "Iteration 71025, loss = 1.08632130\n",
      "Iteration 71026, loss = 1.29889125\n",
      "Iteration 71027, loss = 1.23542698\n",
      "Iteration 71028, loss = 1.19950362\n",
      "Iteration 71029, loss = 1.20191544\n",
      "Iteration 71030, loss = 1.05130489\n",
      "Iteration 71031, loss = 1.20809491\n",
      "Iteration 71032, loss = 1.17657139\n",
      "Iteration 71033, loss = 1.19691566\n",
      "Iteration 71034, loss = 1.26394897\n",
      "Iteration 71035, loss = 1.30292154\n",
      "Iteration 71036, loss = 1.17735379\n",
      "Iteration 71037, loss = 1.34632642\n",
      "Iteration 71038, loss = 1.50013999\n",
      "Iteration 71039, loss = 1.38691450\n",
      "Iteration 71040, loss = 1.35069988\n",
      "Iteration 71041, loss = 1.80439146\n",
      "Iteration 71042, loss = 1.71983103\n",
      "Iteration 71043, loss = 1.50371731\n",
      "Iteration 71044, loss = 1.18962436\n",
      "Iteration 71045, loss = 1.16214712\n",
      "Iteration 71046, loss = 1.12105703\n",
      "Iteration 71047, loss = 1.41046566\n",
      "Iteration 71048, loss = 1.40906842\n",
      "Iteration 71049, loss = 1.35852240\n",
      "Iteration 71050, loss = 1.24613848\n",
      "Iteration 71051, loss = 1.37097969\n",
      "Iteration 71052, loss = 1.36110637\n",
      "Iteration 71053, loss = 1.31448312\n",
      "Iteration 71054, loss = 1.35302261\n",
      "Iteration 71055, loss = 1.20208987\n",
      "Iteration 71056, loss = 1.09694694\n",
      "Iteration 71057, loss = 1.19409486\n",
      "Iteration 71058, loss = 1.21898610\n",
      "Iteration 71059, loss = 1.21021538\n",
      "Iteration 71060, loss = 1.57294419\n",
      "Iteration 71061, loss = 1.35969131\n",
      "Iteration 71062, loss = 1.57266246\n",
      "Iteration 71063, loss = 1.82384912\n",
      "Iteration 71064, loss = 1.71177701\n",
      "Iteration 71065, loss = 1.50052437\n",
      "Iteration 71066, loss = 1.54022990\n",
      "Iteration 71067, loss = 1.54228530\n",
      "Iteration 71068, loss = 1.41250106\n",
      "Iteration 71069, loss = 1.11288540\n",
      "Iteration 71070, loss = 1.24910685\n",
      "Iteration 71071, loss = 1.94646871\n",
      "Iteration 71072, loss = 2.19870605\n",
      "Iteration 71073, loss = 2.79684376\n",
      "Iteration 71074, loss = 2.87731790\n",
      "Iteration 71075, loss = 1.76656269\n",
      "Iteration 71076, loss = 1.71244376\n",
      "Iteration 71077, loss = 1.96827409\n",
      "Iteration 71078, loss = 1.74017601\n",
      "Iteration 71079, loss = 1.45234168\n",
      "Iteration 71080, loss = 1.43251460\n",
      "Iteration 71081, loss = 1.35820438\n",
      "Iteration 71082, loss = 1.51151564\n",
      "Iteration 71083, loss = 1.45131813\n",
      "Iteration 71084, loss = 1.80139363\n",
      "Iteration 71085, loss = 1.64698530\n",
      "Iteration 71086, loss = 2.12445012\n",
      "Iteration 71087, loss = 1.84429638\n",
      "Iteration 71088, loss = 1.52712662\n",
      "Iteration 71089, loss = 1.64544427\n",
      "Iteration 71090, loss = 1.49720303\n",
      "Iteration 71091, loss = 1.38675362\n",
      "Iteration 71092, loss = 1.24002569\n",
      "Iteration 71093, loss = 1.12862751\n",
      "Iteration 71094, loss = 1.23844499\n",
      "Iteration 71095, loss = 1.53335486\n",
      "Iteration 71096, loss = 1.44877601\n",
      "Iteration 71097, loss = 1.13606579\n",
      "Iteration 71098, loss = 1.08214213\n",
      "Iteration 71099, loss = 1.09398540\n",
      "Iteration 71100, loss = 1.02449094\n",
      "Iteration 71101, loss = 0.99387193\n",
      "Iteration 71102, loss = 1.00110231\n",
      "Iteration 71103, loss = 0.96576457\n",
      "Iteration 71104, loss = 1.04803115\n",
      "Iteration 71105, loss = 1.07242249\n",
      "Iteration 71106, loss = 1.01143661\n",
      "Iteration 71107, loss = 1.18445465\n",
      "Iteration 71108, loss = 1.17708887\n",
      "Iteration 71109, loss = 1.16680421\n",
      "Iteration 71110, loss = 1.13916575\n",
      "Iteration 71111, loss = 1.27216999\n",
      "Iteration 71112, loss = 1.36390873\n",
      "Iteration 71113, loss = 1.30337584\n",
      "Iteration 71114, loss = 1.28737703\n",
      "Iteration 71115, loss = 1.17712815\n",
      "Iteration 71116, loss = 1.28229370\n",
      "Iteration 71117, loss = 1.22673572\n",
      "Iteration 71118, loss = 1.11171689\n",
      "Iteration 71119, loss = 1.14708257\n",
      "Iteration 71120, loss = 1.22116939\n",
      "Iteration 71121, loss = 1.25609899\n",
      "Iteration 71122, loss = 1.17524422\n",
      "Iteration 71123, loss = 1.27023536\n",
      "Iteration 71124, loss = 1.59938262\n",
      "Iteration 71125, loss = 1.52585812\n",
      "Iteration 71126, loss = 1.97424124\n",
      "Iteration 71127, loss = 2.83308533\n",
      "Iteration 71128, loss = 3.36627379\n",
      "Iteration 71129, loss = 2.96933157\n",
      "Iteration 71130, loss = 2.21236381\n",
      "Iteration 71131, loss = 1.90576768\n",
      "Iteration 71132, loss = 1.65466674\n",
      "Iteration 71133, loss = 2.15417077\n",
      "Iteration 71134, loss = 2.03998618\n",
      "Iteration 71135, loss = 1.64980996\n",
      "Iteration 71136, loss = 1.79450051\n",
      "Iteration 71137, loss = 1.13144651\n",
      "Iteration 71138, loss = 1.15582912\n",
      "Iteration 71139, loss = 1.28548210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 71140, loss = 1.31705817\n",
      "Iteration 71141, loss = 1.12700937\n",
      "Iteration 71142, loss = 1.06848243\n",
      "Iteration 71143, loss = 1.09114322\n",
      "Iteration 71144, loss = 1.49114913\n",
      "Iteration 71145, loss = 1.22089571\n",
      "Iteration 71146, loss = 1.33979640\n",
      "Iteration 71147, loss = 1.28367782\n",
      "Iteration 71148, loss = 1.09841777\n",
      "Iteration 71149, loss = 1.08343392\n",
      "Iteration 71150, loss = 1.00307365\n",
      "Iteration 71151, loss = 1.09645498\n",
      "Iteration 71152, loss = 1.07122769\n",
      "Iteration 71153, loss = 1.14023673\n",
      "Iteration 71154, loss = 1.19738544\n",
      "Iteration 71155, loss = 1.17841594\n",
      "Iteration 71156, loss = 1.22162841\n",
      "Iteration 71157, loss = 1.20055796\n",
      "Iteration 71158, loss = 1.32446054\n",
      "Iteration 71159, loss = 1.24203267\n",
      "Iteration 71160, loss = 1.18930394\n",
      "Iteration 71161, loss = 1.15840284\n",
      "Iteration 71162, loss = 1.10883383\n",
      "Iteration 71163, loss = 1.06971640\n",
      "Iteration 71164, loss = 1.01817481\n",
      "Iteration 71165, loss = 1.09018614\n",
      "Iteration 71166, loss = 1.18855959\n",
      "Iteration 71167, loss = 1.23145198\n",
      "Iteration 71168, loss = 1.24824741\n",
      "Iteration 71169, loss = 1.07897266\n",
      "Iteration 71170, loss = 1.07556808\n",
      "Iteration 71171, loss = 1.21295719\n",
      "Iteration 71172, loss = 1.22927275\n",
      "Iteration 71173, loss = 1.19446173\n",
      "Iteration 71174, loss = 1.07128186\n",
      "Iteration 71175, loss = 1.14040416\n",
      "Iteration 71176, loss = 1.25728864\n",
      "Iteration 71177, loss = 1.72750651\n",
      "Iteration 71178, loss = 1.50792886\n",
      "Iteration 71179, loss = 1.47102675\n",
      "Iteration 71180, loss = 1.57886065\n",
      "Iteration 71181, loss = 2.39244734\n",
      "Iteration 71182, loss = 2.13388296\n",
      "Iteration 71183, loss = 1.69593082\n",
      "Iteration 71184, loss = 1.42478372\n",
      "Iteration 71185, loss = 1.41782579\n",
      "Iteration 71186, loss = 1.48339347\n",
      "Iteration 71187, loss = 1.33398632\n",
      "Iteration 71188, loss = 1.45500551\n",
      "Iteration 71189, loss = 1.47965709\n",
      "Iteration 71190, loss = 1.60908999\n",
      "Iteration 71191, loss = 1.54239539\n",
      "Iteration 71192, loss = 1.33545719\n",
      "Iteration 71193, loss = 1.27351638\n",
      "Iteration 71194, loss = 1.24153647\n",
      "Iteration 71195, loss = 1.27010384\n",
      "Iteration 71196, loss = 1.39952088\n",
      "Iteration 71197, loss = 1.04817811\n",
      "Iteration 71198, loss = 1.72308562\n",
      "Iteration 71199, loss = 1.29606453\n",
      "Iteration 71200, loss = 1.29019597\n",
      "Iteration 71201, loss = 1.82458275\n",
      "Iteration 71202, loss = 1.66333549\n",
      "Iteration 71203, loss = 1.60688677\n",
      "Iteration 71204, loss = 1.63484712\n",
      "Iteration 71205, loss = 1.06635862\n",
      "Iteration 71206, loss = 1.09470019\n",
      "Iteration 71207, loss = 1.07713183\n",
      "Iteration 71208, loss = 1.03609976\n",
      "Iteration 71209, loss = 1.04892659\n",
      "Iteration 71210, loss = 1.06390185\n",
      "Iteration 71211, loss = 1.17555682\n",
      "Iteration 71212, loss = 1.15445531\n",
      "Iteration 71213, loss = 1.09297765\n",
      "Iteration 71214, loss = 1.23841955\n",
      "Iteration 71215, loss = 1.13329921\n",
      "Iteration 71216, loss = 1.13011507\n",
      "Iteration 71217, loss = 1.40099506\n",
      "Iteration 71218, loss = 1.73384479\n",
      "Iteration 71219, loss = 1.48130790\n",
      "Iteration 71220, loss = 1.46520126\n",
      "Iteration 71221, loss = 1.34330251\n",
      "Iteration 71222, loss = 1.43257438\n",
      "Iteration 71223, loss = 1.48203647\n",
      "Iteration 71224, loss = 1.22746263\n",
      "Iteration 71225, loss = 1.52428693\n",
      "Iteration 71226, loss = 1.60441822\n",
      "Iteration 71227, loss = 1.52156735\n",
      "Iteration 71228, loss = 1.29840191\n",
      "Iteration 71229, loss = 1.21333165\n",
      "Iteration 71230, loss = 1.19896632\n",
      "Iteration 71231, loss = 1.25427384\n",
      "Iteration 71232, loss = 1.12309846\n",
      "Iteration 71233, loss = 1.16454713\n",
      "Iteration 71234, loss = 1.10294212\n",
      "Iteration 71235, loss = 1.08732092\n",
      "Iteration 71236, loss = 1.19824156\n",
      "Iteration 71237, loss = 1.26073511\n",
      "Iteration 71238, loss = 1.08740038\n",
      "Iteration 71239, loss = 1.18519338\n",
      "Iteration 71240, loss = 1.16022976\n",
      "Iteration 71241, loss = 1.19613372\n",
      "Iteration 71242, loss = 1.13238344\n",
      "Iteration 71243, loss = 1.09266035\n",
      "Iteration 71244, loss = 1.12442883\n",
      "Iteration 71245, loss = 1.12263594\n",
      "Iteration 71246, loss = 1.01803401\n",
      "Iteration 71247, loss = 1.18863284\n",
      "Iteration 71248, loss = 1.11780760\n",
      "Iteration 71249, loss = 1.16625054\n",
      "Iteration 71250, loss = 1.09968409\n",
      "Iteration 71251, loss = 1.42420850\n",
      "Iteration 71252, loss = 1.60733129\n",
      "Iteration 71253, loss = 1.35332697\n",
      "Iteration 71254, loss = 1.14628305\n",
      "Iteration 71255, loss = 1.30843381\n",
      "Iteration 71256, loss = 1.04969443\n",
      "Iteration 71257, loss = 1.06613345\n",
      "Iteration 71258, loss = 1.10193221\n",
      "Iteration 71259, loss = 1.18210648\n",
      "Iteration 71260, loss = 1.23274359\n",
      "Iteration 71261, loss = 1.12581063\n",
      "Iteration 71262, loss = 1.12141971\n",
      "Iteration 71263, loss = 1.50414355\n",
      "Iteration 71264, loss = 1.31219986\n",
      "Iteration 71265, loss = 1.23476793\n",
      "Iteration 71266, loss = 1.29910748\n",
      "Iteration 71267, loss = 1.31180862\n",
      "Iteration 71268, loss = 1.31395924\n",
      "Iteration 71269, loss = 1.27311936\n",
      "Iteration 71270, loss = 1.39087339\n",
      "Iteration 71271, loss = 1.39231493\n",
      "Iteration 71272, loss = 1.31523949\n",
      "Iteration 71273, loss = 1.60385115\n",
      "Iteration 71274, loss = 1.34030336\n",
      "Iteration 71275, loss = 1.14886153\n",
      "Iteration 71276, loss = 1.17913600\n",
      "Iteration 71277, loss = 1.46155762\n",
      "Iteration 71278, loss = 1.44381171\n",
      "Iteration 71279, loss = 1.10999812\n",
      "Iteration 71280, loss = 1.45347451\n",
      "Iteration 71281, loss = 1.47814069\n",
      "Iteration 71282, loss = 1.26483956\n",
      "Iteration 71283, loss = 1.19948564\n",
      "Iteration 71284, loss = 1.25949966\n",
      "Iteration 71285, loss = 1.35702218\n",
      "Iteration 71286, loss = 1.17229212\n",
      "Iteration 71287, loss = 1.18556078\n",
      "Iteration 71288, loss = 1.24442508\n",
      "Iteration 71289, loss = 1.15498056\n",
      "Iteration 71290, loss = 1.13146665\n",
      "Iteration 71291, loss = 0.96173125\n",
      "Iteration 71292, loss = 1.12446258\n",
      "Iteration 71293, loss = 1.10410700\n",
      "Iteration 71294, loss = 1.37529452\n",
      "Iteration 71295, loss = 1.54145410\n",
      "Iteration 71296, loss = 2.23141692\n",
      "Iteration 71297, loss = 2.18379961\n",
      "Iteration 71298, loss = 2.03402985\n",
      "Iteration 71299, loss = 2.48169919\n",
      "Iteration 71300, loss = 2.30043981\n",
      "Iteration 71301, loss = 2.04817231\n",
      "Iteration 71302, loss = 1.35608253\n",
      "Iteration 71303, loss = 1.76036980\n",
      "Iteration 71304, loss = 1.65841138\n",
      "Iteration 71305, loss = 1.51426728\n",
      "Iteration 71306, loss = 1.94428301\n",
      "Iteration 71307, loss = 1.81264229\n",
      "Iteration 71308, loss = 1.59919741\n",
      "Iteration 71309, loss = 1.53984329\n",
      "Iteration 71310, loss = 1.49128058\n",
      "Iteration 71311, loss = 1.32886222\n",
      "Iteration 71312, loss = 1.52379442\n",
      "Iteration 71313, loss = 1.32744256\n",
      "Iteration 71314, loss = 1.08173096\n",
      "Iteration 71315, loss = 1.04559785\n",
      "Iteration 71316, loss = 1.17363218\n",
      "Iteration 71317, loss = 1.29288225\n",
      "Iteration 71318, loss = 1.32803136\n",
      "Iteration 71319, loss = 1.17958015\n",
      "Iteration 71320, loss = 1.12223099\n",
      "Iteration 71321, loss = 1.16161273\n",
      "Iteration 71322, loss = 1.25706112\n",
      "Iteration 71323, loss = 1.15102798\n",
      "Iteration 71324, loss = 1.05690712\n",
      "Iteration 71325, loss = 1.04106615\n",
      "Iteration 71326, loss = 1.04780104\n",
      "Iteration 71327, loss = 1.16412930\n",
      "Iteration 71328, loss = 1.24755315\n",
      "Iteration 71329, loss = 1.11918377\n",
      "Iteration 71330, loss = 1.24501604\n",
      "Iteration 71331, loss = 1.09894721\n",
      "Iteration 71332, loss = 1.16175684\n",
      "Iteration 71333, loss = 1.05660507\n",
      "Iteration 71334, loss = 1.13984833\n",
      "Iteration 71335, loss = 1.08966789\n",
      "Iteration 71336, loss = 1.25033316\n",
      "Iteration 71337, loss = 1.28100691\n",
      "Iteration 71338, loss = 1.16950843\n",
      "Iteration 71339, loss = 1.07038342\n",
      "Iteration 71340, loss = 1.23829947\n",
      "Iteration 71341, loss = 1.19903800\n",
      "Iteration 71342, loss = 0.99174955\n",
      "Iteration 71343, loss = 1.01941018\n",
      "Iteration 71344, loss = 1.06404002\n",
      "Iteration 71345, loss = 1.18285422\n",
      "Iteration 71346, loss = 1.23305974\n",
      "Iteration 71347, loss = 1.38192498\n",
      "Iteration 71348, loss = 1.58041481\n",
      "Iteration 71349, loss = 1.48221937\n",
      "Iteration 71350, loss = 1.59246618\n",
      "Iteration 71351, loss = 1.64925118\n",
      "Iteration 71352, loss = 2.20827389\n",
      "Iteration 71353, loss = 2.55066135\n",
      "Iteration 71354, loss = 3.03423773\n",
      "Iteration 71355, loss = 2.58006800\n",
      "Iteration 71356, loss = 2.78482005\n",
      "Iteration 71357, loss = 2.12563689\n",
      "Iteration 71358, loss = 1.57863158\n",
      "Iteration 71359, loss = 1.47644970\n",
      "Iteration 71360, loss = 1.20600221\n",
      "Iteration 71361, loss = 1.25117536\n",
      "Iteration 71362, loss = 1.18453635\n",
      "Iteration 71363, loss = 1.06557614\n",
      "Iteration 71364, loss = 1.07458876\n",
      "Iteration 71365, loss = 1.24421141\n",
      "Iteration 71366, loss = 1.13681837\n",
      "Iteration 71367, loss = 1.15207344\n",
      "Iteration 71368, loss = 1.01329677\n",
      "Iteration 71369, loss = 1.13173027\n",
      "Iteration 71370, loss = 1.11487054\n",
      "Iteration 71371, loss = 1.12401335\n",
      "Iteration 71372, loss = 0.99537989\n",
      "Iteration 71373, loss = 1.01195457\n",
      "Iteration 71374, loss = 1.10170980\n",
      "Iteration 71375, loss = 1.16612349\n",
      "Iteration 71376, loss = 1.01319270\n",
      "Iteration 71377, loss = 1.11877065\n",
      "Iteration 71378, loss = 1.03502708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 71379, loss = 1.04940999\n",
      "Iteration 71380, loss = 1.08814459\n",
      "Iteration 71381, loss = 1.03291235\n",
      "Iteration 71382, loss = 1.16363545\n",
      "Iteration 71383, loss = 1.17109242\n",
      "Iteration 71384, loss = 1.23016837\n",
      "Iteration 71385, loss = 1.15579496\n",
      "Iteration 71386, loss = 1.28767878\n",
      "Iteration 71387, loss = 1.12345706\n",
      "Iteration 71388, loss = 1.04162972\n",
      "Iteration 71389, loss = 1.10846260\n",
      "Iteration 71390, loss = 1.01448956\n",
      "Iteration 71391, loss = 1.11713569\n",
      "Iteration 71392, loss = 1.04106634\n",
      "Iteration 71393, loss = 1.09787137\n",
      "Iteration 71394, loss = 1.00770655\n",
      "Iteration 71395, loss = 1.12814418\n",
      "Iteration 71396, loss = 1.25663126\n",
      "Iteration 71397, loss = 1.36648886\n",
      "Iteration 71398, loss = 1.30894060\n",
      "Iteration 71399, loss = 1.38313577\n",
      "Iteration 71400, loss = 1.40832367\n",
      "Iteration 71401, loss = 1.48994211\n",
      "Iteration 71402, loss = 1.37423825\n",
      "Iteration 71403, loss = 1.35374552\n",
      "Iteration 71404, loss = 1.31934034\n",
      "Iteration 71405, loss = 1.25495829\n",
      "Iteration 71406, loss = 1.12058872\n",
      "Iteration 71407, loss = 1.40036353\n",
      "Iteration 71408, loss = 1.15807879\n",
      "Iteration 71409, loss = 1.34932194\n",
      "Iteration 71410, loss = 1.35096212\n",
      "Iteration 71411, loss = 1.33903285\n",
      "Iteration 71412, loss = 1.28984320\n",
      "Iteration 71413, loss = 1.41867203\n",
      "Iteration 71414, loss = 2.16503914\n",
      "Iteration 71415, loss = 2.04998878\n",
      "Iteration 71416, loss = 1.86039894\n",
      "Iteration 71417, loss = 1.58030768\n",
      "Iteration 71418, loss = 2.54973901\n",
      "Iteration 71419, loss = 1.61611934\n",
      "Iteration 71420, loss = 2.14967443\n",
      "Iteration 71421, loss = 2.45051459\n",
      "Iteration 71422, loss = 3.05624455\n",
      "Iteration 71423, loss = 2.71575222\n",
      "Iteration 71424, loss = 1.81685831\n",
      "Iteration 71425, loss = 1.44735872\n",
      "Iteration 71426, loss = 1.43503991\n",
      "Iteration 71427, loss = 1.27393260\n",
      "Iteration 71428, loss = 1.28119927\n",
      "Iteration 71429, loss = 1.24521795\n",
      "Iteration 71430, loss = 1.05330257\n",
      "Iteration 71431, loss = 1.12168206\n",
      "Iteration 71432, loss = 1.19107375\n",
      "Iteration 71433, loss = 1.34917252\n",
      "Iteration 71434, loss = 1.18487696\n",
      "Iteration 71435, loss = 1.34609548\n",
      "Iteration 71436, loss = 1.23749153\n",
      "Iteration 71437, loss = 1.28494309\n",
      "Iteration 71438, loss = 1.10856554\n",
      "Iteration 71439, loss = 1.08917139\n",
      "Iteration 71440, loss = 1.14345376\n",
      "Iteration 71441, loss = 1.12091177\n",
      "Iteration 71442, loss = 1.44359619\n",
      "Iteration 71443, loss = 1.34435712\n",
      "Iteration 71444, loss = 1.31929501\n",
      "Iteration 71445, loss = 1.20279214\n",
      "Iteration 71446, loss = 1.11331517\n",
      "Iteration 71447, loss = 1.21646706\n",
      "Iteration 71448, loss = 1.16717364\n",
      "Iteration 71449, loss = 1.22020269\n",
      "Iteration 71450, loss = 1.18013243\n",
      "Iteration 71451, loss = 1.36365022\n",
      "Iteration 71452, loss = 1.74293914\n",
      "Iteration 71453, loss = 1.98524566\n",
      "Iteration 71454, loss = 2.04565164\n",
      "Iteration 71455, loss = 2.21331168\n",
      "Iteration 71456, loss = 1.72439663\n",
      "Iteration 71457, loss = 1.42448800\n",
      "Iteration 71458, loss = 1.13796362\n",
      "Iteration 71459, loss = 1.13539945\n",
      "Iteration 71460, loss = 1.35061009\n",
      "Iteration 71461, loss = 1.39617374\n",
      "Iteration 71462, loss = 1.22240122\n",
      "Iteration 71463, loss = 1.08580801\n",
      "Iteration 71464, loss = 1.45887913\n",
      "Iteration 71465, loss = 1.90466926\n",
      "Iteration 71466, loss = 1.56524437\n",
      "Iteration 71467, loss = 2.00496321\n",
      "Iteration 71468, loss = 1.91894536\n",
      "Iteration 71469, loss = 2.02471734\n",
      "Iteration 71470, loss = 2.26002101\n",
      "Iteration 71471, loss = 2.08391469\n",
      "Iteration 71472, loss = 1.46220930\n",
      "Iteration 71473, loss = 1.32861602\n",
      "Iteration 71474, loss = 1.45620350\n",
      "Iteration 71475, loss = 1.44554884\n",
      "Iteration 71476, loss = 1.23801254\n",
      "Iteration 71477, loss = 1.41621035\n",
      "Iteration 71478, loss = 1.33369949\n",
      "Iteration 71479, loss = 1.11082102\n",
      "Iteration 71480, loss = 1.08858140\n",
      "Iteration 71481, loss = 1.23452355\n",
      "Iteration 71482, loss = 1.06308702\n",
      "Iteration 71483, loss = 1.08856682\n",
      "Iteration 71484, loss = 1.15922578\n",
      "Iteration 71485, loss = 1.06240168\n",
      "Iteration 71486, loss = 1.13996302\n",
      "Iteration 71487, loss = 1.14552374\n",
      "Iteration 71488, loss = 0.98298108\n",
      "Iteration 71489, loss = 1.03004965\n",
      "Iteration 71490, loss = 1.07240121\n",
      "Iteration 71491, loss = 1.14112766\n",
      "Iteration 71492, loss = 1.08597184\n",
      "Iteration 71493, loss = 1.20506571\n",
      "Iteration 71494, loss = 1.48934221\n",
      "Iteration 71495, loss = 1.31126490\n",
      "Iteration 71496, loss = 1.42962688\n",
      "Iteration 71497, loss = 1.36970538\n",
      "Iteration 71498, loss = 1.98179910\n",
      "Iteration 71499, loss = 3.70281803\n",
      "Iteration 71500, loss = 4.56354481\n",
      "Iteration 71501, loss = 3.89943285\n",
      "Iteration 71502, loss = 3.56129997\n",
      "Iteration 71503, loss = 3.02700688\n",
      "Iteration 71504, loss = 3.31772344\n",
      "Iteration 71505, loss = 2.65988440\n",
      "Iteration 71506, loss = 2.03037821\n",
      "Iteration 71507, loss = 2.32451486\n",
      "Iteration 71508, loss = 2.30951844\n",
      "Iteration 71509, loss = 3.61907250\n",
      "Iteration 71510, loss = 3.86286860\n",
      "Iteration 71511, loss = 3.74710299\n",
      "Iteration 71512, loss = 2.89951191\n",
      "Iteration 71513, loss = 3.39776622\n",
      "Iteration 71514, loss = 3.04025583\n",
      "Iteration 71515, loss = 1.94596275\n",
      "Iteration 71516, loss = 1.46477841\n",
      "Iteration 71517, loss = 1.71726678\n",
      "Iteration 71518, loss = 1.82963108\n",
      "Iteration 71519, loss = 1.51710462\n",
      "Iteration 71520, loss = 1.44947094\n",
      "Iteration 71521, loss = 1.69164585\n",
      "Iteration 71522, loss = 1.96009636\n",
      "Iteration 71523, loss = 1.45832064\n",
      "Iteration 71524, loss = 1.37989937\n",
      "Iteration 71525, loss = 1.20383445\n",
      "Iteration 71526, loss = 1.18853682\n",
      "Iteration 71527, loss = 1.30587491\n",
      "Iteration 71528, loss = 1.27305664\n",
      "Iteration 71529, loss = 1.26147255\n",
      "Iteration 71530, loss = 1.22809240\n",
      "Iteration 71531, loss = 1.20056018\n",
      "Iteration 71532, loss = 1.15255344\n",
      "Iteration 71533, loss = 1.09918060\n",
      "Iteration 71534, loss = 1.31016445\n",
      "Iteration 71535, loss = 1.22402842\n",
      "Iteration 71536, loss = 1.15228375\n",
      "Iteration 71537, loss = 1.17096553\n",
      "Iteration 71538, loss = 1.15379974\n",
      "Iteration 71539, loss = 1.08373936\n",
      "Iteration 71540, loss = 1.09554020\n",
      "Iteration 71541, loss = 1.03312768\n",
      "Iteration 71542, loss = 1.14993923\n",
      "Iteration 71543, loss = 1.01272602\n",
      "Iteration 71544, loss = 1.01586492\n",
      "Iteration 71545, loss = 1.21520117\n",
      "Iteration 71546, loss = 1.08883291\n",
      "Iteration 71547, loss = 1.15959933\n",
      "Iteration 71548, loss = 1.04072799\n",
      "Iteration 71549, loss = 1.00010710\n",
      "Iteration 71550, loss = 1.00885698\n",
      "Iteration 71551, loss = 1.01844742\n",
      "Iteration 71552, loss = 1.02372863\n",
      "Iteration 71553, loss = 1.13241334\n",
      "Iteration 71554, loss = 1.30651084\n",
      "Iteration 71555, loss = 1.04903765\n",
      "Iteration 71556, loss = 1.04328155\n",
      "Iteration 71557, loss = 1.13461711\n",
      "Iteration 71558, loss = 1.32146609\n",
      "Iteration 71559, loss = 1.51258041\n",
      "Iteration 71560, loss = 1.22644465\n",
      "Iteration 71561, loss = 1.29655361\n",
      "Iteration 71562, loss = 1.50974437\n",
      "Iteration 71563, loss = 1.38091901\n",
      "Iteration 71564, loss = 1.27049854\n",
      "Iteration 71565, loss = 1.31751132\n",
      "Iteration 71566, loss = 1.09774418\n",
      "Iteration 71567, loss = 1.13771093\n",
      "Iteration 71568, loss = 1.27967770\n",
      "Iteration 71569, loss = 1.45940337\n",
      "Iteration 71570, loss = 1.29427363\n",
      "Iteration 71571, loss = 1.22201874\n",
      "Iteration 71572, loss = 1.12212107\n",
      "Iteration 71573, loss = 1.16446565\n",
      "Iteration 71574, loss = 1.12357962\n",
      "Iteration 71575, loss = 1.05610967\n",
      "Iteration 71576, loss = 1.00300786\n",
      "Iteration 71577, loss = 1.17810166\n",
      "Iteration 71578, loss = 1.02828739\n",
      "Iteration 71579, loss = 1.27794353\n",
      "Iteration 71580, loss = 1.08844867\n",
      "Iteration 71581, loss = 0.97270271\n",
      "Iteration 71582, loss = 1.15814728\n",
      "Iteration 71583, loss = 1.18511160\n",
      "Iteration 71584, loss = 1.30959816\n",
      "Iteration 71585, loss = 1.49737286\n",
      "Iteration 71586, loss = 1.42374361\n",
      "Iteration 71587, loss = 1.76983920\n",
      "Iteration 71588, loss = 1.42424690\n",
      "Iteration 71589, loss = 1.53063711\n",
      "Iteration 71590, loss = 1.71094453\n",
      "Iteration 71591, loss = 1.52121554\n",
      "Iteration 71592, loss = 1.55705598\n",
      "Iteration 71593, loss = 1.32330233\n",
      "Iteration 71594, loss = 1.21200885\n",
      "Iteration 71595, loss = 1.01390333\n",
      "Iteration 71596, loss = 1.06600873\n",
      "Iteration 71597, loss = 1.06309396\n",
      "Iteration 71598, loss = 1.27391533\n",
      "Iteration 71599, loss = 1.10699190\n",
      "Iteration 71600, loss = 1.19610463\n",
      "Iteration 71601, loss = 1.35993681\n",
      "Iteration 71602, loss = 1.20382571\n",
      "Iteration 71603, loss = 1.11710940\n",
      "Iteration 71604, loss = 1.08904512\n",
      "Iteration 71605, loss = 1.33479393\n",
      "Iteration 71606, loss = 1.40336062\n",
      "Iteration 71607, loss = 1.54949246\n",
      "Iteration 71608, loss = 1.58520474\n",
      "Iteration 71609, loss = 1.20556680\n",
      "Iteration 71610, loss = 1.48545605\n",
      "Iteration 71611, loss = 1.39881865\n",
      "Iteration 71612, loss = 1.46310064\n",
      "Iteration 71613, loss = 1.45345604\n",
      "Iteration 71614, loss = 1.50648982\n",
      "Iteration 71615, loss = 1.49664045\n",
      "Iteration 71616, loss = 1.47619427\n",
      "Iteration 71617, loss = 1.51083350\n",
      "Iteration 71618, loss = 1.19911179\n",
      "Iteration 71619, loss = 1.06959488\n",
      "Iteration 71620, loss = 1.00960403\n",
      "Iteration 71621, loss = 1.16676135\n",
      "Iteration 71622, loss = 1.15172112\n",
      "Iteration 71623, loss = 1.12636882\n",
      "Iteration 71624, loss = 1.39663380\n",
      "Iteration 71625, loss = 1.48141048\n",
      "Iteration 71626, loss = 1.42162937\n",
      "Iteration 71627, loss = 1.30040425\n",
      "Iteration 71628, loss = 1.69769597\n",
      "Iteration 71629, loss = 1.50106553\n",
      "Iteration 71630, loss = 1.41457242\n",
      "Iteration 71631, loss = 1.65011347\n",
      "Iteration 71632, loss = 1.24683328\n",
      "Iteration 71633, loss = 1.26513091\n",
      "Iteration 71634, loss = 1.63954189\n",
      "Iteration 71635, loss = 2.12792874\n",
      "Iteration 71636, loss = 1.89732575\n",
      "Iteration 71637, loss = 1.66196002\n",
      "Iteration 71638, loss = 1.85857238\n",
      "Iteration 71639, loss = 1.88018151\n",
      "Iteration 71640, loss = 1.60183693\n",
      "Iteration 71641, loss = 1.39573095\n",
      "Iteration 71642, loss = 1.22181965\n",
      "Iteration 71643, loss = 1.14354021\n",
      "Iteration 71644, loss = 1.13664283\n",
      "Iteration 71645, loss = 0.98373201\n",
      "Iteration 71646, loss = 1.04945506\n",
      "Iteration 71647, loss = 1.06601865\n",
      "Iteration 71648, loss = 1.08936810\n",
      "Iteration 71649, loss = 1.05329277\n",
      "Iteration 71650, loss = 1.23131494\n",
      "Iteration 71651, loss = 1.21855579\n",
      "Iteration 71652, loss = 1.15763936\n",
      "Iteration 71653, loss = 1.04927158\n",
      "Iteration 71654, loss = 1.08878624\n",
      "Iteration 71655, loss = 1.14043935\n",
      "Iteration 71656, loss = 0.98512029\n",
      "Iteration 71657, loss = 1.02560873\n",
      "Iteration 71658, loss = 1.05335387\n",
      "Iteration 71659, loss = 0.97120288\n",
      "Iteration 71660, loss = 1.05295096\n",
      "Iteration 71661, loss = 1.00950846\n",
      "Iteration 71662, loss = 1.16442892\n",
      "Iteration 71663, loss = 1.02154042\n",
      "Iteration 71664, loss = 1.08744450\n",
      "Iteration 71665, loss = 1.06854899\n",
      "Iteration 71666, loss = 1.07432875\n",
      "Iteration 71667, loss = 1.08040847\n",
      "Iteration 71668, loss = 1.16218417\n",
      "Iteration 71669, loss = 1.57994343\n",
      "Iteration 71670, loss = 1.80198125\n",
      "Iteration 71671, loss = 1.67514280\n",
      "Iteration 71672, loss = 1.45202608\n",
      "Iteration 71673, loss = 1.59774270\n",
      "Iteration 71674, loss = 1.59249501\n",
      "Iteration 71675, loss = 1.29432347\n",
      "Iteration 71676, loss = 1.22096383\n",
      "Iteration 71677, loss = 1.39610214\n",
      "Iteration 71678, loss = 1.91414273\n",
      "Iteration 71679, loss = 1.93554648\n",
      "Iteration 71680, loss = 1.76282210\n",
      "Iteration 71681, loss = 2.04158646\n",
      "Iteration 71682, loss = 1.72288153\n",
      "Iteration 71683, loss = 1.26675511\n",
      "Iteration 71684, loss = 1.36997834\n",
      "Iteration 71685, loss = 1.17862687\n",
      "Iteration 71686, loss = 1.12658545\n",
      "Iteration 71687, loss = 1.17662467\n",
      "Iteration 71688, loss = 1.15445174\n",
      "Iteration 71689, loss = 1.05096387\n",
      "Iteration 71690, loss = 1.17571802\n",
      "Iteration 71691, loss = 1.19542118\n",
      "Iteration 71692, loss = 1.14791854\n",
      "Iteration 71693, loss = 1.20486214\n",
      "Iteration 71694, loss = 1.01066066\n",
      "Iteration 71695, loss = 1.06871349\n",
      "Iteration 71696, loss = 1.23868243\n",
      "Iteration 71697, loss = 1.54484208\n",
      "Iteration 71698, loss = 1.35809714\n",
      "Iteration 71699, loss = 1.18315503\n",
      "Iteration 71700, loss = 1.07604818\n",
      "Iteration 71701, loss = 1.07810328\n",
      "Iteration 71702, loss = 1.23711478\n",
      "Iteration 71703, loss = 1.30699886\n",
      "Iteration 71704, loss = 1.36754816\n",
      "Iteration 71705, loss = 1.18754225\n",
      "Iteration 71706, loss = 1.42698931\n",
      "Iteration 71707, loss = 1.42216491\n",
      "Iteration 71708, loss = 1.40457040\n",
      "Iteration 71709, loss = 1.30977437\n",
      "Iteration 71710, loss = 1.10872071\n",
      "Iteration 71711, loss = 1.04770310\n",
      "Iteration 71712, loss = 1.03331089\n",
      "Iteration 71713, loss = 1.05848938\n",
      "Iteration 71714, loss = 1.05418290\n",
      "Iteration 71715, loss = 1.27477834\n",
      "Iteration 71716, loss = 1.13061037\n",
      "Iteration 71717, loss = 1.12923742\n",
      "Iteration 71718, loss = 1.12772492\n",
      "Iteration 71719, loss = 1.49324458\n",
      "Iteration 71720, loss = 1.66985351\n",
      "Iteration 71721, loss = 1.53724328\n",
      "Iteration 71722, loss = 1.15742129\n",
      "Iteration 71723, loss = 1.06902594\n",
      "Iteration 71724, loss = 1.07721185\n",
      "Iteration 71725, loss = 0.98653265\n",
      "Iteration 71726, loss = 1.08478890\n",
      "Iteration 71727, loss = 1.22258570\n",
      "Iteration 71728, loss = 1.18216638\n",
      "Iteration 71729, loss = 1.31269532\n",
      "Iteration 71730, loss = 1.24651253\n",
      "Iteration 71731, loss = 1.14999276\n",
      "Iteration 71732, loss = 1.39773731\n",
      "Iteration 71733, loss = 1.30946561\n",
      "Iteration 71734, loss = 1.17292030\n",
      "Iteration 71735, loss = 1.23698939\n",
      "Iteration 71736, loss = 1.31537506\n",
      "Iteration 71737, loss = 1.21768279\n",
      "Iteration 71738, loss = 1.12600468\n",
      "Iteration 71739, loss = 1.04496793\n",
      "Iteration 71740, loss = 1.29771435\n",
      "Iteration 71741, loss = 1.64198602\n",
      "Iteration 71742, loss = 1.65245877\n",
      "Iteration 71743, loss = 1.38842763\n",
      "Iteration 71744, loss = 1.27441519\n",
      "Iteration 71745, loss = 1.22869902\n",
      "Iteration 71746, loss = 1.27621939\n",
      "Iteration 71747, loss = 1.18049183\n",
      "Iteration 71748, loss = 1.32537939\n",
      "Iteration 71749, loss = 1.17354613\n",
      "Iteration 71750, loss = 1.08124475\n",
      "Iteration 71751, loss = 1.01264893\n",
      "Iteration 71752, loss = 1.06716124\n",
      "Iteration 71753, loss = 0.99733753\n",
      "Iteration 71754, loss = 1.04799817\n",
      "Iteration 71755, loss = 1.04170282\n",
      "Iteration 71756, loss = 1.28010181\n",
      "Iteration 71757, loss = 1.25889881\n",
      "Iteration 71758, loss = 1.05517380\n",
      "Iteration 71759, loss = 1.29270424\n",
      "Iteration 71760, loss = 1.19592488\n",
      "Iteration 71761, loss = 1.15456851\n",
      "Iteration 71762, loss = 1.41361274\n",
      "Iteration 71763, loss = 1.27340846\n",
      "Iteration 71764, loss = 1.04204371\n",
      "Iteration 71765, loss = 1.16139277\n",
      "Iteration 71766, loss = 1.14015533\n",
      "Iteration 71767, loss = 0.97705124\n",
      "Iteration 71768, loss = 1.13703278\n",
      "Iteration 71769, loss = 1.12979378\n",
      "Iteration 71770, loss = 1.43105582\n",
      "Iteration 71771, loss = 1.29939900\n",
      "Iteration 71772, loss = 1.45765026\n",
      "Iteration 71773, loss = 1.51058589\n",
      "Iteration 71774, loss = 1.37002256\n",
      "Iteration 71775, loss = 1.70771226\n",
      "Iteration 71776, loss = 1.83853626\n",
      "Iteration 71777, loss = 1.39615875\n",
      "Iteration 71778, loss = 1.52942170\n",
      "Iteration 71779, loss = 1.37799634\n",
      "Iteration 71780, loss = 1.09211719\n",
      "Iteration 71781, loss = 1.20935708\n",
      "Iteration 71782, loss = 1.13154314\n",
      "Iteration 71783, loss = 1.03588326\n",
      "Iteration 71784, loss = 0.98770095\n",
      "Iteration 71785, loss = 1.01299827\n",
      "Iteration 71786, loss = 1.01218487\n",
      "Iteration 71787, loss = 1.04286895\n",
      "Iteration 71788, loss = 1.03959137\n",
      "Iteration 71789, loss = 1.14271873\n",
      "Iteration 71790, loss = 1.03527748\n",
      "Iteration 71791, loss = 1.00755990\n",
      "Iteration 71792, loss = 1.00725082\n",
      "Iteration 71793, loss = 1.06883497\n",
      "Iteration 71794, loss = 1.12692170\n",
      "Iteration 71795, loss = 1.07487421\n",
      "Iteration 71796, loss = 1.12725681\n",
      "Iteration 71797, loss = 1.10748420\n",
      "Iteration 71798, loss = 1.02428555\n",
      "Iteration 71799, loss = 1.20846115\n",
      "Iteration 71800, loss = 1.11199344\n",
      "Iteration 71801, loss = 1.15014242\n",
      "Iteration 71802, loss = 1.48949212\n",
      "Iteration 71803, loss = 1.23868532\n",
      "Iteration 71804, loss = 1.19860745\n",
      "Iteration 71805, loss = 1.16819788\n",
      "Iteration 71806, loss = 1.25464538\n",
      "Iteration 71807, loss = 1.21637634\n",
      "Iteration 71808, loss = 1.14450834\n",
      "Iteration 71809, loss = 1.38718789\n",
      "Iteration 71810, loss = 1.51130112\n",
      "Iteration 71811, loss = 1.20846421\n",
      "Iteration 71812, loss = 1.16468060\n",
      "Iteration 71813, loss = 1.16283702\n",
      "Iteration 71814, loss = 1.11295478\n",
      "Iteration 71815, loss = 1.05527602\n",
      "Iteration 71816, loss = 1.11056166\n",
      "Iteration 71817, loss = 1.09429016\n",
      "Iteration 71818, loss = 1.12413218\n",
      "Iteration 71819, loss = 1.02558916\n",
      "Iteration 71820, loss = 1.09916149\n",
      "Iteration 71821, loss = 1.14485002\n",
      "Iteration 71822, loss = 1.02759089\n",
      "Iteration 71823, loss = 1.03493045\n",
      "Iteration 71824, loss = 1.01814421\n",
      "Iteration 71825, loss = 1.10023649\n",
      "Iteration 71826, loss = 1.17724625\n",
      "Iteration 71827, loss = 1.71758094\n",
      "Iteration 71828, loss = 1.49210085\n",
      "Iteration 71829, loss = 1.42346201\n",
      "Iteration 71830, loss = 1.48871791\n",
      "Iteration 71831, loss = 1.90109167\n",
      "Iteration 71832, loss = 1.74556081\n",
      "Iteration 71833, loss = 2.39664145\n",
      "Iteration 71834, loss = 1.95763073\n",
      "Iteration 71835, loss = 2.16589383\n",
      "Iteration 71836, loss = 2.06137419\n",
      "Iteration 71837, loss = 1.29633433\n",
      "Iteration 71838, loss = 1.21214768\n",
      "Iteration 71839, loss = 1.22853556\n",
      "Iteration 71840, loss = 1.06850134\n",
      "Iteration 71841, loss = 1.00951643\n",
      "Iteration 71842, loss = 1.11285851\n",
      "Iteration 71843, loss = 1.05918112\n",
      "Iteration 71844, loss = 1.12623371\n",
      "Iteration 71845, loss = 1.22336371\n",
      "Iteration 71846, loss = 1.26515653\n",
      "Iteration 71847, loss = 1.22124031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 71848, loss = 1.09770640\n",
      "Iteration 71849, loss = 1.01843647\n",
      "Iteration 71850, loss = 1.04696024\n",
      "Iteration 71851, loss = 1.15785959\n",
      "Iteration 71852, loss = 1.13132688\n",
      "Iteration 71853, loss = 1.14839707\n",
      "Iteration 71854, loss = 1.15733916\n",
      "Iteration 71855, loss = 1.22962752\n",
      "Iteration 71856, loss = 1.56105918\n",
      "Iteration 71857, loss = 1.66477566\n",
      "Iteration 71858, loss = 2.22693243\n",
      "Iteration 71859, loss = 2.32186877\n",
      "Iteration 71860, loss = 2.00917326\n",
      "Iteration 71861, loss = 2.41934761\n",
      "Iteration 71862, loss = 2.78654150\n",
      "Iteration 71863, loss = 1.98998230\n",
      "Iteration 71864, loss = 1.30053884\n",
      "Iteration 71865, loss = 1.25694017\n",
      "Iteration 71866, loss = 1.19376251\n",
      "Iteration 71867, loss = 1.42941141\n",
      "Iteration 71868, loss = 1.25939303\n",
      "Iteration 71869, loss = 1.44444786\n",
      "Iteration 71870, loss = 1.27023065\n",
      "Iteration 71871, loss = 1.08026537\n",
      "Iteration 71872, loss = 1.26144383\n",
      "Iteration 71873, loss = 1.27701363\n",
      "Iteration 71874, loss = 1.23421731\n",
      "Iteration 71875, loss = 1.06977322\n",
      "Iteration 71876, loss = 1.19132224\n",
      "Iteration 71877, loss = 1.40351378\n",
      "Iteration 71878, loss = 1.12156914\n",
      "Iteration 71879, loss = 1.05650548\n",
      "Iteration 71880, loss = 1.04156702\n",
      "Iteration 71881, loss = 1.09040012\n",
      "Iteration 71882, loss = 1.09591977\n",
      "Iteration 71883, loss = 1.19664781\n",
      "Iteration 71884, loss = 1.32926129\n",
      "Iteration 71885, loss = 1.34055845\n",
      "Iteration 71886, loss = 1.20210308\n",
      "Iteration 71887, loss = 1.15221741\n",
      "Iteration 71888, loss = 1.16247152\n",
      "Iteration 71889, loss = 1.21172220\n",
      "Iteration 71890, loss = 1.39353994\n",
      "Iteration 71891, loss = 1.21879856\n",
      "Iteration 71892, loss = 1.14286629\n",
      "Iteration 71893, loss = 1.14551129\n",
      "Iteration 71894, loss = 1.59817454\n",
      "Iteration 71895, loss = 1.40219777\n",
      "Iteration 71896, loss = 1.28126528\n",
      "Iteration 71897, loss = 1.45836567\n",
      "Iteration 71898, loss = 1.62354099\n",
      "Iteration 71899, loss = 1.28222394\n",
      "Iteration 71900, loss = 1.24335643\n",
      "Iteration 71901, loss = 1.11309433\n",
      "Iteration 71902, loss = 1.24353978\n",
      "Iteration 71903, loss = 1.55148238\n",
      "Iteration 71904, loss = 1.81933717\n",
      "Iteration 71905, loss = 1.48360976\n",
      "Iteration 71906, loss = 1.27963246\n",
      "Iteration 71907, loss = 1.30526625\n",
      "Iteration 71908, loss = 1.21795616\n",
      "Iteration 71909, loss = 1.11970926\n",
      "Iteration 71910, loss = 1.15117155\n",
      "Iteration 71911, loss = 1.16843219\n",
      "Iteration 71912, loss = 1.20346720\n",
      "Iteration 71913, loss = 1.38972363\n",
      "Iteration 71914, loss = 1.79023921\n",
      "Iteration 71915, loss = 2.11220108\n",
      "Iteration 71916, loss = 2.82695948\n",
      "Iteration 71917, loss = 2.48981800\n",
      "Iteration 71918, loss = 1.96993859\n",
      "Iteration 71919, loss = 1.36116366\n",
      "Iteration 71920, loss = 1.38548988\n",
      "Iteration 71921, loss = 1.32472984\n",
      "Iteration 71922, loss = 1.19094877\n",
      "Iteration 71923, loss = 1.47711538\n",
      "Iteration 71924, loss = 1.65007461\n",
      "Iteration 71925, loss = 1.44986116\n",
      "Iteration 71926, loss = 1.13984852\n",
      "Iteration 71927, loss = 1.35780655\n",
      "Iteration 71928, loss = 1.65756572\n",
      "Iteration 71929, loss = 1.55212899\n",
      "Iteration 71930, loss = 1.31857206\n",
      "Iteration 71931, loss = 1.00712633\n",
      "Iteration 71932, loss = 1.16655454\n",
      "Iteration 71933, loss = 1.05722504\n",
      "Iteration 71934, loss = 1.08880427\n",
      "Iteration 71935, loss = 1.03930170\n",
      "Iteration 71936, loss = 1.15124927\n",
      "Iteration 71937, loss = 1.48141445\n",
      "Iteration 71938, loss = 1.37499308\n",
      "Iteration 71939, loss = 1.50183723\n",
      "Iteration 71940, loss = 1.66291540\n",
      "Iteration 71941, loss = 1.87810016\n",
      "Iteration 71942, loss = 1.34130716\n",
      "Iteration 71943, loss = 1.18190228\n",
      "Iteration 71944, loss = 1.17913309\n",
      "Iteration 71945, loss = 1.22590795\n",
      "Iteration 71946, loss = 1.18012938\n",
      "Iteration 71947, loss = 1.13308348\n",
      "Iteration 71948, loss = 1.08824842\n",
      "Iteration 71949, loss = 1.06323044\n",
      "Iteration 71950, loss = 1.17547335\n",
      "Iteration 71951, loss = 1.04461325\n",
      "Iteration 71952, loss = 1.00299748\n",
      "Iteration 71953, loss = 1.15724648\n",
      "Iteration 71954, loss = 1.14414719\n",
      "Iteration 71955, loss = 1.11548802\n",
      "Iteration 71956, loss = 1.06381099\n",
      "Iteration 71957, loss = 1.17744610\n",
      "Iteration 71958, loss = 1.13123027\n",
      "Iteration 71959, loss = 1.10623735\n",
      "Iteration 71960, loss = 0.97827345\n",
      "Iteration 71961, loss = 1.04038933\n",
      "Iteration 71962, loss = 1.23703665\n",
      "Iteration 71963, loss = 1.22788276\n",
      "Iteration 71964, loss = 1.33443453\n",
      "Iteration 71965, loss = 1.23190468\n",
      "Iteration 71966, loss = 1.09068777\n",
      "Iteration 71967, loss = 1.03683062\n",
      "Iteration 71968, loss = 1.14604282\n",
      "Iteration 71969, loss = 1.19894122\n",
      "Iteration 71970, loss = 1.17931442\n",
      "Iteration 71971, loss = 1.18272192\n",
      "Iteration 71972, loss = 1.35257434\n",
      "Iteration 71973, loss = 1.34265877\n",
      "Iteration 71974, loss = 1.32815563\n",
      "Iteration 71975, loss = 1.55654611\n",
      "Iteration 71976, loss = 1.45620851\n",
      "Iteration 71977, loss = 1.55481290\n",
      "Iteration 71978, loss = 1.33962800\n",
      "Iteration 71979, loss = 1.22720066\n",
      "Iteration 71980, loss = 1.35015584\n",
      "Iteration 71981, loss = 1.18789672\n",
      "Iteration 71982, loss = 1.07428065\n",
      "Iteration 71983, loss = 1.16360161\n",
      "Iteration 71984, loss = 1.04768543\n",
      "Iteration 71985, loss = 1.23859382\n",
      "Iteration 71986, loss = 1.13282322\n",
      "Iteration 71987, loss = 1.17030707\n",
      "Iteration 71988, loss = 1.12538501\n",
      "Iteration 71989, loss = 1.02347336\n",
      "Iteration 71990, loss = 1.05325992\n",
      "Iteration 71991, loss = 1.09301856\n",
      "Iteration 71992, loss = 1.36207826\n",
      "Iteration 71993, loss = 1.57301859\n",
      "Iteration 71994, loss = 2.06776591\n",
      "Iteration 71995, loss = 1.66301075\n",
      "Iteration 71996, loss = 1.25163556\n",
      "Iteration 71997, loss = 1.22028543\n",
      "Iteration 71998, loss = 1.15225602\n",
      "Iteration 71999, loss = 1.02545484\n",
      "Iteration 72000, loss = 1.16598211\n",
      "Iteration 72001, loss = 1.19727371\n",
      "Iteration 72002, loss = 1.46252127\n",
      "Iteration 72003, loss = 1.23118133\n",
      "Iteration 72004, loss = 1.12479741\n",
      "Iteration 72005, loss = 1.22867812\n",
      "Iteration 72006, loss = 1.12355851\n",
      "Iteration 72007, loss = 1.05097482\n",
      "Iteration 72008, loss = 1.03139264\n",
      "Iteration 72009, loss = 1.10138852\n",
      "Iteration 72010, loss = 1.05761388\n",
      "Iteration 72011, loss = 1.14907216\n",
      "Iteration 72012, loss = 1.05708879\n",
      "Iteration 72013, loss = 1.04662259\n",
      "Iteration 72014, loss = 1.04983713\n",
      "Iteration 72015, loss = 1.08020700\n",
      "Iteration 72016, loss = 1.06031212\n",
      "Iteration 72017, loss = 1.11119010\n",
      "Iteration 72018, loss = 1.01598041\n",
      "Iteration 72019, loss = 1.01636409\n",
      "Iteration 72020, loss = 1.12255358\n",
      "Iteration 72021, loss = 1.62845502\n",
      "Iteration 72022, loss = 1.36331597\n",
      "Iteration 72023, loss = 1.30454786\n",
      "Iteration 72024, loss = 1.50246925\n",
      "Iteration 72025, loss = 1.42580931\n",
      "Iteration 72026, loss = 1.44480843\n",
      "Iteration 72027, loss = 1.70331781\n",
      "Iteration 72028, loss = 1.84868935\n",
      "Iteration 72029, loss = 2.01876401\n",
      "Iteration 72030, loss = 2.53489308\n",
      "Iteration 72031, loss = 2.01719582\n",
      "Iteration 72032, loss = 2.33936364\n",
      "Iteration 72033, loss = 2.01939484\n",
      "Iteration 72034, loss = 1.76078861\n",
      "Iteration 72035, loss = 2.15305240\n",
      "Iteration 72036, loss = 2.10241407\n",
      "Iteration 72037, loss = 1.44187675\n",
      "Iteration 72038, loss = 1.56184428\n",
      "Iteration 72039, loss = 1.60492202\n",
      "Iteration 72040, loss = 1.41849731\n",
      "Iteration 72041, loss = 2.07947349\n",
      "Iteration 72042, loss = 1.73392846\n",
      "Iteration 72043, loss = 1.12932873\n",
      "Iteration 72044, loss = 1.23561693\n",
      "Iteration 72045, loss = 1.07577944\n",
      "Iteration 72046, loss = 1.07664022\n",
      "Iteration 72047, loss = 1.15107711\n",
      "Iteration 72048, loss = 1.25574445\n",
      "Iteration 72049, loss = 1.46190071\n",
      "Iteration 72050, loss = 1.29651164\n",
      "Iteration 72051, loss = 1.26872458\n",
      "Iteration 72052, loss = 1.67686986\n",
      "Iteration 72053, loss = 1.50257801\n",
      "Iteration 72054, loss = 1.65203407\n",
      "Iteration 72055, loss = 1.76946812\n",
      "Iteration 72056, loss = 1.72526890\n",
      "Iteration 72057, loss = 1.62828198\n",
      "Iteration 72058, loss = 1.35652164\n",
      "Iteration 72059, loss = 1.08244068\n",
      "Iteration 72060, loss = 1.19103783\n",
      "Iteration 72061, loss = 1.07527137\n",
      "Iteration 72062, loss = 1.14589767\n",
      "Iteration 72063, loss = 1.27521248\n",
      "Iteration 72064, loss = 1.31776759\n",
      "Iteration 72065, loss = 1.18340036\n",
      "Iteration 72066, loss = 1.20603807\n",
      "Iteration 72067, loss = 1.14821716\n",
      "Iteration 72068, loss = 1.54262531\n",
      "Iteration 72069, loss = 1.42117826\n",
      "Iteration 72070, loss = 1.41825628\n",
      "Iteration 72071, loss = 1.46329825\n",
      "Iteration 72072, loss = 1.77431876\n",
      "Iteration 72073, loss = 1.72821836\n",
      "Iteration 72074, loss = 2.71366075\n",
      "Iteration 72075, loss = 1.56958881\n",
      "Iteration 72076, loss = 1.58118193\n",
      "Iteration 72077, loss = 1.40570759\n",
      "Iteration 72078, loss = 1.54646792\n",
      "Iteration 72079, loss = 1.33521272\n",
      "Iteration 72080, loss = 1.72392854\n",
      "Iteration 72081, loss = 1.29564737\n",
      "Iteration 72082, loss = 1.45676854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 72083, loss = 1.19156650\n",
      "Iteration 72084, loss = 1.17619441\n",
      "Iteration 72085, loss = 1.13881794\n",
      "Iteration 72086, loss = 1.10989689\n",
      "Iteration 72087, loss = 1.05000962\n",
      "Iteration 72088, loss = 1.21350665\n",
      "Iteration 72089, loss = 1.22077436\n",
      "Iteration 72090, loss = 1.31153897\n",
      "Iteration 72091, loss = 1.70395027\n",
      "Iteration 72092, loss = 1.45138771\n",
      "Iteration 72093, loss = 1.58279685\n",
      "Iteration 72094, loss = 1.77145561\n",
      "Iteration 72095, loss = 1.36504528\n",
      "Iteration 72096, loss = 1.71368760\n",
      "Iteration 72097, loss = 1.36023751\n",
      "Iteration 72098, loss = 1.22929643\n",
      "Iteration 72099, loss = 1.05174324\n",
      "Iteration 72100, loss = 0.99835003\n",
      "Iteration 72101, loss = 1.07276882\n",
      "Iteration 72102, loss = 1.14834072\n",
      "Iteration 72103, loss = 1.18622958\n",
      "Iteration 72104, loss = 1.50240270\n",
      "Iteration 72105, loss = 1.85774096\n",
      "Iteration 72106, loss = 2.05169905\n",
      "Iteration 72107, loss = 1.81166489\n",
      "Iteration 72108, loss = 1.37040191\n",
      "Iteration 72109, loss = 1.55894014\n",
      "Iteration 72110, loss = 1.17167859\n",
      "Iteration 72111, loss = 1.18664926\n",
      "Iteration 72112, loss = 1.41549746\n",
      "Iteration 72113, loss = 1.41204784\n",
      "Iteration 72114, loss = 1.20084233\n",
      "Iteration 72115, loss = 1.24607060\n",
      "Iteration 72116, loss = 1.09715273\n",
      "Iteration 72117, loss = 1.39509312\n",
      "Iteration 72118, loss = 1.43759134\n",
      "Iteration 72119, loss = 1.21998653\n",
      "Iteration 72120, loss = 1.09537552\n",
      "Iteration 72121, loss = 1.19554094\n",
      "Iteration 72122, loss = 1.16283146\n",
      "Iteration 72123, loss = 1.02622283\n",
      "Iteration 72124, loss = 1.20589936\n",
      "Iteration 72125, loss = 1.27543144\n",
      "Iteration 72126, loss = 1.31566009\n",
      "Iteration 72127, loss = 1.21340363\n",
      "Iteration 72128, loss = 1.42900809\n",
      "Iteration 72129, loss = 1.20600140\n",
      "Iteration 72130, loss = 1.04243278\n",
      "Iteration 72131, loss = 1.10948856\n",
      "Iteration 72132, loss = 1.27523007\n",
      "Iteration 72133, loss = 1.51259863\n",
      "Iteration 72134, loss = 1.17011588\n",
      "Iteration 72135, loss = 1.34104848\n",
      "Iteration 72136, loss = 1.19939304\n",
      "Iteration 72137, loss = 1.01326933\n",
      "Iteration 72138, loss = 1.16360876\n",
      "Iteration 72139, loss = 1.17454032\n",
      "Iteration 72140, loss = 1.40449503\n",
      "Iteration 72141, loss = 1.63159250\n",
      "Iteration 72142, loss = 1.59876651\n",
      "Iteration 72143, loss = 1.28581358\n",
      "Iteration 72144, loss = 1.14793515\n",
      "Iteration 72145, loss = 1.27326471\n",
      "Iteration 72146, loss = 1.35153064\n",
      "Iteration 72147, loss = 1.42421400\n",
      "Iteration 72148, loss = 1.63484855\n",
      "Iteration 72149, loss = 1.71275031\n",
      "Iteration 72150, loss = 1.49525295\n",
      "Iteration 72151, loss = 1.82981933\n",
      "Iteration 72152, loss = 1.92926782\n",
      "Iteration 72153, loss = 1.46074330\n",
      "Iteration 72154, loss = 1.29156555\n",
      "Iteration 72155, loss = 1.59256784\n",
      "Iteration 72156, loss = 1.80233329\n",
      "Iteration 72157, loss = 1.56553065\n",
      "Iteration 72158, loss = 1.34400363\n",
      "Iteration 72159, loss = 1.29854922\n",
      "Iteration 72160, loss = 1.14315473\n",
      "Iteration 72161, loss = 1.01665992\n",
      "Iteration 72162, loss = 1.03380366\n",
      "Iteration 72163, loss = 0.98023324\n",
      "Iteration 72164, loss = 1.37035992\n",
      "Iteration 72165, loss = 1.45669407\n",
      "Iteration 72166, loss = 1.66280482\n",
      "Iteration 72167, loss = 1.59747324\n",
      "Iteration 72168, loss = 1.69336109\n",
      "Iteration 72169, loss = 1.65841426\n",
      "Iteration 72170, loss = 1.47076521\n",
      "Iteration 72171, loss = 1.47784351\n",
      "Iteration 72172, loss = 1.84611588\n",
      "Iteration 72173, loss = 1.23277572\n",
      "Iteration 72174, loss = 1.51586192\n",
      "Iteration 72175, loss = 1.67640358\n",
      "Iteration 72176, loss = 1.41032579\n",
      "Iteration 72177, loss = 1.70577735\n",
      "Iteration 72178, loss = 1.34429304\n",
      "Iteration 72179, loss = 1.54044366\n",
      "Iteration 72180, loss = 1.75879825\n",
      "Iteration 72181, loss = 1.38632772\n",
      "Iteration 72182, loss = 1.26523695\n",
      "Iteration 72183, loss = 1.22380512\n",
      "Iteration 72184, loss = 1.24149136\n",
      "Iteration 72185, loss = 1.09683464\n",
      "Iteration 72186, loss = 1.14026157\n",
      "Iteration 72187, loss = 1.14404343\n",
      "Iteration 72188, loss = 1.26490456\n",
      "Iteration 72189, loss = 1.09717378\n",
      "Iteration 72190, loss = 1.11299088\n",
      "Iteration 72191, loss = 1.32409485\n",
      "Iteration 72192, loss = 1.28713092\n",
      "Iteration 72193, loss = 1.14300797\n",
      "Iteration 72194, loss = 1.06616816\n",
      "Iteration 72195, loss = 1.04246694\n",
      "Iteration 72196, loss = 1.02549607\n",
      "Iteration 72197, loss = 1.16133031\n",
      "Iteration 72198, loss = 1.51211433\n",
      "Iteration 72199, loss = 1.24036935\n",
      "Iteration 72200, loss = 1.15642294\n",
      "Iteration 72201, loss = 1.08340380\n",
      "Iteration 72202, loss = 1.07752922\n",
      "Iteration 72203, loss = 1.00301868\n",
      "Iteration 72204, loss = 1.09079429\n",
      "Iteration 72205, loss = 1.14522821\n",
      "Iteration 72206, loss = 1.06684102\n",
      "Iteration 72207, loss = 1.05116637\n",
      "Iteration 72208, loss = 1.06445921\n",
      "Iteration 72209, loss = 1.19770194\n",
      "Iteration 72210, loss = 1.13674736\n",
      "Iteration 72211, loss = 1.32925107\n",
      "Iteration 72212, loss = 1.30994737\n",
      "Iteration 72213, loss = 2.10561840\n",
      "Iteration 72214, loss = 2.19463389\n",
      "Iteration 72215, loss = 2.36150684\n",
      "Iteration 72216, loss = 2.15772501\n",
      "Iteration 72217, loss = 2.55044915\n",
      "Iteration 72218, loss = 2.58582344\n",
      "Iteration 72219, loss = 3.40091854\n",
      "Iteration 72220, loss = 2.48934943\n",
      "Iteration 72221, loss = 1.81266233\n",
      "Iteration 72222, loss = 1.51166536\n",
      "Iteration 72223, loss = 1.31256170\n",
      "Iteration 72224, loss = 1.72695128\n",
      "Iteration 72225, loss = 1.64144606\n",
      "Iteration 72226, loss = 1.36883394\n",
      "Iteration 72227, loss = 1.21662652\n",
      "Iteration 72228, loss = 1.34215454\n",
      "Iteration 72229, loss = 1.27351346\n",
      "Iteration 72230, loss = 1.43354687\n",
      "Iteration 72231, loss = 1.12650767\n",
      "Iteration 72232, loss = 1.27784540\n",
      "Iteration 72233, loss = 1.24535904\n",
      "Iteration 72234, loss = 1.25003804\n",
      "Iteration 72235, loss = 1.26956339\n",
      "Iteration 72236, loss = 1.33576389\n",
      "Iteration 72237, loss = 1.27732321\n",
      "Iteration 72238, loss = 1.36175410\n",
      "Iteration 72239, loss = 1.46879676\n",
      "Iteration 72240, loss = 1.54020170\n",
      "Iteration 72241, loss = 1.52577254\n",
      "Iteration 72242, loss = 1.72365548\n",
      "Iteration 72243, loss = 1.30754920\n",
      "Iteration 72244, loss = 1.36491510\n",
      "Iteration 72245, loss = 1.30871988\n",
      "Iteration 72246, loss = 1.11900497\n",
      "Iteration 72247, loss = 1.15314309\n",
      "Iteration 72248, loss = 1.23815774\n",
      "Iteration 72249, loss = 1.38364635\n",
      "Iteration 72250, loss = 1.36059011\n",
      "Iteration 72251, loss = 1.35680049\n",
      "Iteration 72252, loss = 1.13306622\n",
      "Iteration 72253, loss = 1.00293757\n",
      "Iteration 72254, loss = 1.10236843\n",
      "Iteration 72255, loss = 1.04346674\n",
      "Iteration 72256, loss = 1.13169185\n",
      "Iteration 72257, loss = 1.27303223\n",
      "Iteration 72258, loss = 1.52607430\n",
      "Iteration 72259, loss = 1.20174847\n",
      "Iteration 72260, loss = 1.07443023\n",
      "Iteration 72261, loss = 1.04786475\n",
      "Iteration 72262, loss = 1.09775200\n",
      "Iteration 72263, loss = 1.03325987\n",
      "Iteration 72264, loss = 1.04020780\n",
      "Iteration 72265, loss = 1.13170224\n",
      "Iteration 72266, loss = 1.05849968\n",
      "Iteration 72267, loss = 1.07895214\n",
      "Iteration 72268, loss = 0.99143892\n",
      "Iteration 72269, loss = 1.05922856\n",
      "Iteration 72270, loss = 1.19193772\n",
      "Iteration 72271, loss = 1.19691680\n",
      "Iteration 72272, loss = 1.09754377\n",
      "Iteration 72273, loss = 1.07181389\n",
      "Iteration 72274, loss = 1.11465981\n",
      "Iteration 72275, loss = 1.28954812\n",
      "Iteration 72276, loss = 1.55291461\n",
      "Iteration 72277, loss = 1.68150423\n",
      "Iteration 72278, loss = 1.74103734\n",
      "Iteration 72279, loss = 1.67814266\n",
      "Iteration 72280, loss = 1.16359351\n",
      "Iteration 72281, loss = 1.27720983\n",
      "Iteration 72282, loss = 1.46463928\n",
      "Iteration 72283, loss = 1.94186776\n",
      "Iteration 72284, loss = 1.34694901\n",
      "Iteration 72285, loss = 1.62059368\n",
      "Iteration 72286, loss = 1.60908110\n",
      "Iteration 72287, loss = 1.36487713\n",
      "Iteration 72288, loss = 1.62464134\n",
      "Iteration 72289, loss = 1.94829602\n",
      "Iteration 72290, loss = 2.93196799\n",
      "Iteration 72291, loss = 2.91006874\n",
      "Iteration 72292, loss = 2.13651219\n",
      "Iteration 72293, loss = 2.36963366\n",
      "Iteration 72294, loss = 1.90540751\n",
      "Iteration 72295, loss = 1.86831085\n",
      "Iteration 72296, loss = 1.67272907\n",
      "Iteration 72297, loss = 1.33797440\n",
      "Iteration 72298, loss = 1.06324137\n",
      "Iteration 72299, loss = 1.15674087\n",
      "Iteration 72300, loss = 1.12174749\n",
      "Iteration 72301, loss = 1.12636451\n",
      "Iteration 72302, loss = 1.30331985\n",
      "Iteration 72303, loss = 1.37102870\n",
      "Iteration 72304, loss = 1.00707972\n",
      "Iteration 72305, loss = 1.08013075\n",
      "Iteration 72306, loss = 1.10151741\n",
      "Iteration 72307, loss = 1.01479788\n",
      "Iteration 72308, loss = 1.32505758\n",
      "Iteration 72309, loss = 1.41670293\n",
      "Iteration 72310, loss = 1.20023258\n",
      "Iteration 72311, loss = 1.09977878\n",
      "Iteration 72312, loss = 1.10252064\n",
      "Iteration 72313, loss = 1.29648427\n",
      "Iteration 72314, loss = 1.36855085\n",
      "Iteration 72315, loss = 1.33482516\n",
      "Iteration 72316, loss = 1.37072220\n",
      "Iteration 72317, loss = 1.41152608\n",
      "Iteration 72318, loss = 1.14340149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 72319, loss = 1.12359704\n",
      "Iteration 72320, loss = 1.21055472\n",
      "Iteration 72321, loss = 1.11894364\n",
      "Iteration 72322, loss = 0.99160780\n",
      "Iteration 72323, loss = 0.99560328\n",
      "Iteration 72324, loss = 1.13932569\n",
      "Iteration 72325, loss = 1.08208056\n",
      "Iteration 72326, loss = 1.01658460\n",
      "Iteration 72327, loss = 1.17404856\n",
      "Iteration 72328, loss = 1.18728558\n",
      "Iteration 72329, loss = 1.19920095\n",
      "Iteration 72330, loss = 1.11910307\n",
      "Iteration 72331, loss = 1.25123526\n",
      "Iteration 72332, loss = 1.27401357\n",
      "Iteration 72333, loss = 1.06448398\n",
      "Iteration 72334, loss = 1.01463642\n",
      "Iteration 72335, loss = 0.96170318\n",
      "Iteration 72336, loss = 1.01650636\n",
      "Iteration 72337, loss = 0.99897926\n",
      "Iteration 72338, loss = 1.16078356\n",
      "Iteration 72339, loss = 1.14006346\n",
      "Iteration 72340, loss = 1.22921388\n",
      "Iteration 72341, loss = 1.17475440\n",
      "Iteration 72342, loss = 1.41783322\n",
      "Iteration 72343, loss = 1.38805433\n",
      "Iteration 72344, loss = 1.17842398\n",
      "Iteration 72345, loss = 1.13173026\n",
      "Iteration 72346, loss = 1.10018571\n",
      "Iteration 72347, loss = 1.20577618\n",
      "Iteration 72348, loss = 1.19793917\n",
      "Iteration 72349, loss = 1.31580448\n",
      "Iteration 72350, loss = 1.22378088\n",
      "Iteration 72351, loss = 1.22171631\n",
      "Iteration 72352, loss = 1.15468279\n",
      "Iteration 72353, loss = 1.05911088\n",
      "Iteration 72354, loss = 1.18959970\n",
      "Iteration 72355, loss = 1.43987903\n",
      "Iteration 72356, loss = 1.46242148\n",
      "Iteration 72357, loss = 1.44441709\n",
      "Iteration 72358, loss = 1.71996702\n",
      "Iteration 72359, loss = 1.53919335\n",
      "Iteration 72360, loss = 1.28348532\n",
      "Iteration 72361, loss = 1.12453632\n",
      "Iteration 72362, loss = 1.29448211\n",
      "Iteration 72363, loss = 1.09321848\n",
      "Iteration 72364, loss = 1.25117935\n",
      "Iteration 72365, loss = 1.28835466\n",
      "Iteration 72366, loss = 1.23295417\n",
      "Iteration 72367, loss = 1.31113724\n",
      "Iteration 72368, loss = 1.20078868\n",
      "Iteration 72369, loss = 1.48965692\n",
      "Iteration 72370, loss = 1.28117797\n",
      "Iteration 72371, loss = 1.14020117\n",
      "Iteration 72372, loss = 1.11539633\n",
      "Iteration 72373, loss = 1.12780484\n",
      "Iteration 72374, loss = 1.44652503\n",
      "Iteration 72375, loss = 1.43757100\n",
      "Iteration 72376, loss = 1.78420757\n",
      "Iteration 72377, loss = 2.10426755\n",
      "Iteration 72378, loss = 2.01453633\n",
      "Iteration 72379, loss = 1.68853638\n",
      "Iteration 72380, loss = 1.51101523\n",
      "Iteration 72381, loss = 1.51243559\n",
      "Iteration 72382, loss = 1.62212547\n",
      "Iteration 72383, loss = 1.40171559\n",
      "Iteration 72384, loss = 1.01425277\n",
      "Iteration 72385, loss = 1.00634092\n",
      "Iteration 72386, loss = 1.10420120\n",
      "Iteration 72387, loss = 1.05237017\n",
      "Iteration 72388, loss = 0.97270046\n",
      "Iteration 72389, loss = 1.16074902\n",
      "Iteration 72390, loss = 1.27980461\n",
      "Iteration 72391, loss = 1.54055248\n",
      "Iteration 72392, loss = 1.71452714\n",
      "Iteration 72393, loss = 1.07643621\n",
      "Iteration 72394, loss = 1.07332988\n",
      "Iteration 72395, loss = 1.15475627\n",
      "Iteration 72396, loss = 1.14238961\n",
      "Iteration 72397, loss = 1.35029409\n",
      "Iteration 72398, loss = 1.65655043\n",
      "Iteration 72399, loss = 1.64521397\n",
      "Iteration 72400, loss = 1.96936683\n",
      "Iteration 72401, loss = 2.24979951\n",
      "Iteration 72402, loss = 1.76995895\n",
      "Iteration 72403, loss = 1.97611203\n",
      "Iteration 72404, loss = 1.65325886\n",
      "Iteration 72405, loss = 1.16731535\n",
      "Iteration 72406, loss = 1.12507605\n",
      "Iteration 72407, loss = 1.01866743\n",
      "Iteration 72408, loss = 1.05353664\n",
      "Iteration 72409, loss = 1.16822621\n",
      "Iteration 72410, loss = 1.12858102\n",
      "Iteration 72411, loss = 1.07969901\n",
      "Iteration 72412, loss = 1.03451123\n",
      "Iteration 72413, loss = 1.11632197\n",
      "Iteration 72414, loss = 1.03506802\n",
      "Iteration 72415, loss = 1.00262805\n",
      "Iteration 72416, loss = 1.01850901\n",
      "Iteration 72417, loss = 1.05209076\n",
      "Iteration 72418, loss = 1.12760097\n",
      "Iteration 72419, loss = 1.23917356\n",
      "Iteration 72420, loss = 1.06967033\n",
      "Iteration 72421, loss = 1.02949976\n",
      "Iteration 72422, loss = 1.03705297\n",
      "Iteration 72423, loss = 1.20676943\n",
      "Iteration 72424, loss = 1.15020699\n",
      "Iteration 72425, loss = 1.10219204\n",
      "Iteration 72426, loss = 0.94928807\n",
      "Iteration 72427, loss = 1.17767293\n",
      "Iteration 72428, loss = 1.34023534\n",
      "Iteration 72429, loss = 1.29482840\n",
      "Iteration 72430, loss = 1.20012498\n",
      "Iteration 72431, loss = 1.38568455\n",
      "Iteration 72432, loss = 1.31471654\n",
      "Iteration 72433, loss = 1.34899861\n",
      "Iteration 72434, loss = 1.18521044\n",
      "Iteration 72435, loss = 1.06057125\n",
      "Iteration 72436, loss = 1.06101578\n",
      "Iteration 72437, loss = 0.97907066\n",
      "Iteration 72438, loss = 1.07284461\n",
      "Iteration 72439, loss = 1.03617460\n",
      "Iteration 72440, loss = 1.00349152\n",
      "Iteration 72441, loss = 1.02734887\n",
      "Iteration 72442, loss = 1.03093103\n",
      "Iteration 72443, loss = 0.98477132\n",
      "Iteration 72444, loss = 1.14182033\n",
      "Iteration 72445, loss = 1.08504310\n",
      "Iteration 72446, loss = 1.29050166\n",
      "Iteration 72447, loss = 1.50055795\n",
      "Iteration 72448, loss = 1.44505758\n",
      "Iteration 72449, loss = 1.13547998\n",
      "Iteration 72450, loss = 1.38978424\n",
      "Iteration 72451, loss = 1.62631706\n",
      "Iteration 72452, loss = 1.43852831\n",
      "Iteration 72453, loss = 1.33942593\n",
      "Iteration 72454, loss = 2.00051696\n",
      "Iteration 72455, loss = 2.46831861\n",
      "Iteration 72456, loss = 1.96804115\n",
      "Iteration 72457, loss = 1.97339788\n",
      "Iteration 72458, loss = 1.77187769\n",
      "Iteration 72459, loss = 2.13399062\n",
      "Iteration 72460, loss = 1.79452494\n",
      "Iteration 72461, loss = 1.82014561\n",
      "Iteration 72462, loss = 1.92909683\n",
      "Iteration 72463, loss = 2.28231645\n",
      "Iteration 72464, loss = 1.69503746\n",
      "Iteration 72465, loss = 1.74991819\n",
      "Iteration 72466, loss = 2.70918819\n",
      "Iteration 72467, loss = 2.84231328\n",
      "Iteration 72468, loss = 2.36230344\n",
      "Iteration 72469, loss = 2.45721223\n",
      "Iteration 72470, loss = 2.62637773\n",
      "Iteration 72471, loss = 1.54302606\n",
      "Iteration 72472, loss = 1.65365777\n",
      "Iteration 72473, loss = 1.36977592\n",
      "Iteration 72474, loss = 1.35451431\n",
      "Iteration 72475, loss = 1.41929862\n",
      "Iteration 72476, loss = 1.91478597\n",
      "Iteration 72477, loss = 1.50228224\n",
      "Iteration 72478, loss = 1.56058218\n",
      "Iteration 72479, loss = 1.46334996\n",
      "Iteration 72480, loss = 1.09088862\n",
      "Iteration 72481, loss = 1.21821825\n",
      "Iteration 72482, loss = 1.07451840\n",
      "Iteration 72483, loss = 1.01751724\n",
      "Iteration 72484, loss = 1.13346946\n",
      "Iteration 72485, loss = 0.98514246\n",
      "Iteration 72486, loss = 1.24547843\n",
      "Iteration 72487, loss = 1.16482125\n",
      "Iteration 72488, loss = 1.08053697\n",
      "Iteration 72489, loss = 1.12500458\n",
      "Iteration 72490, loss = 1.24703425\n",
      "Iteration 72491, loss = 1.26848640\n",
      "Iteration 72492, loss = 1.26224631\n",
      "Iteration 72493, loss = 1.59223653\n",
      "Iteration 72494, loss = 1.42313610\n",
      "Iteration 72495, loss = 1.27573633\n",
      "Iteration 72496, loss = 1.42438479\n",
      "Iteration 72497, loss = 1.62368240\n",
      "Iteration 72498, loss = 1.88835593\n",
      "Iteration 72499, loss = 1.95389136\n",
      "Iteration 72500, loss = 1.61745843\n",
      "Iteration 72501, loss = 1.55930030\n",
      "Iteration 72502, loss = 1.49077877\n",
      "Iteration 72503, loss = 1.20455173\n",
      "Iteration 72504, loss = 1.28850698\n",
      "Iteration 72505, loss = 1.14182617\n",
      "Iteration 72506, loss = 1.00344153\n",
      "Iteration 72507, loss = 1.21166493\n",
      "Iteration 72508, loss = 1.23213192\n",
      "Iteration 72509, loss = 1.18531202\n",
      "Iteration 72510, loss = 1.08157199\n",
      "Iteration 72511, loss = 1.35108270\n",
      "Iteration 72512, loss = 1.48575018\n",
      "Iteration 72513, loss = 1.49508726\n",
      "Iteration 72514, loss = 1.73665135\n",
      "Iteration 72515, loss = 1.42538374\n",
      "Iteration 72516, loss = 1.27425930\n",
      "Iteration 72517, loss = 0.99097415\n",
      "Iteration 72518, loss = 0.97962893\n",
      "Iteration 72519, loss = 1.03688494\n",
      "Iteration 72520, loss = 1.04953023\n",
      "Iteration 72521, loss = 0.94862045\n",
      "Iteration 72522, loss = 1.09149535\n",
      "Iteration 72523, loss = 1.16646652\n",
      "Iteration 72524, loss = 1.19822816\n",
      "Iteration 72525, loss = 1.17916155\n",
      "Iteration 72526, loss = 1.33450448\n",
      "Iteration 72527, loss = 1.63670837\n",
      "Iteration 72528, loss = 1.80686709\n",
      "Iteration 72529, loss = 1.96173721\n",
      "Iteration 72530, loss = 1.41525445\n",
      "Iteration 72531, loss = 1.40554016\n",
      "Iteration 72532, loss = 1.31248484\n",
      "Iteration 72533, loss = 1.39322898\n",
      "Iteration 72534, loss = 1.26232828\n",
      "Iteration 72535, loss = 1.12478482\n",
      "Iteration 72536, loss = 1.06521432\n",
      "Iteration 72537, loss = 1.12264501\n",
      "Iteration 72538, loss = 1.01647685\n",
      "Iteration 72539, loss = 1.04023394\n",
      "Iteration 72540, loss = 1.05384588\n",
      "Iteration 72541, loss = 0.97577089\n",
      "Iteration 72542, loss = 1.02765421\n",
      "Iteration 72543, loss = 0.99499651\n",
      "Iteration 72544, loss = 1.09649071\n",
      "Iteration 72545, loss = 1.37912729\n",
      "Iteration 72546, loss = 1.33283203\n",
      "Iteration 72547, loss = 1.11977511\n",
      "Iteration 72548, loss = 0.99664132\n",
      "Iteration 72549, loss = 0.97541716\n",
      "Iteration 72550, loss = 1.14965655\n",
      "Iteration 72551, loss = 1.29991740\n",
      "Iteration 72552, loss = 1.31312803\n",
      "Iteration 72553, loss = 1.22678182\n",
      "Iteration 72554, loss = 1.03626929\n",
      "Iteration 72555, loss = 1.05351959\n",
      "Iteration 72556, loss = 1.04866542\n",
      "Iteration 72557, loss = 1.07676877\n",
      "Iteration 72558, loss = 1.20770707\n",
      "Iteration 72559, loss = 1.03846994\n",
      "Iteration 72560, loss = 1.08159800\n",
      "Iteration 72561, loss = 1.04292261\n",
      "Iteration 72562, loss = 1.20266040\n",
      "Iteration 72563, loss = 1.26354531\n",
      "Iteration 72564, loss = 1.25135256\n",
      "Iteration 72565, loss = 1.35438035\n",
      "Iteration 72566, loss = 1.68220763\n",
      "Iteration 72567, loss = 1.65986080\n",
      "Iteration 72568, loss = 1.52441365\n",
      "Iteration 72569, loss = 1.44982628\n",
      "Iteration 72570, loss = 1.43404343\n",
      "Iteration 72571, loss = 1.69681855\n",
      "Iteration 72572, loss = 1.82084412\n",
      "Iteration 72573, loss = 1.84100403\n",
      "Iteration 72574, loss = 1.48922415\n",
      "Iteration 72575, loss = 1.27703950\n",
      "Iteration 72576, loss = 1.16039509\n",
      "Iteration 72577, loss = 1.17936819\n",
      "Iteration 72578, loss = 1.17497247\n",
      "Iteration 72579, loss = 1.07693996\n",
      "Iteration 72580, loss = 1.07492026\n",
      "Iteration 72581, loss = 1.08180701\n",
      "Iteration 72582, loss = 1.07851467\n",
      "Iteration 72583, loss = 1.25105007\n",
      "Iteration 72584, loss = 1.16232395\n",
      "Iteration 72585, loss = 1.05562113\n",
      "Iteration 72586, loss = 1.18262844\n",
      "Iteration 72587, loss = 1.12291522\n",
      "Iteration 72588, loss = 1.04292005\n",
      "Iteration 72589, loss = 1.30104021\n",
      "Iteration 72590, loss = 1.34981412\n",
      "Iteration 72591, loss = 1.12247738\n",
      "Iteration 72592, loss = 1.03446945\n",
      "Iteration 72593, loss = 1.06842675\n",
      "Iteration 72594, loss = 1.23647771\n",
      "Iteration 72595, loss = 1.12227095\n",
      "Iteration 72596, loss = 1.22756750\n",
      "Iteration 72597, loss = 1.23508782\n",
      "Iteration 72598, loss = 1.49220135\n",
      "Iteration 72599, loss = 1.59512364\n",
      "Iteration 72600, loss = 1.45032322\n",
      "Iteration 72601, loss = 1.36403755\n",
      "Iteration 72602, loss = 1.59410920\n",
      "Iteration 72603, loss = 1.89156396\n",
      "Iteration 72604, loss = 1.61913123\n",
      "Iteration 72605, loss = 1.38261586\n",
      "Iteration 72606, loss = 1.40465799\n",
      "Iteration 72607, loss = 1.29697689\n",
      "Iteration 72608, loss = 1.29460837\n",
      "Iteration 72609, loss = 1.24845232\n",
      "Iteration 72610, loss = 1.18953376\n",
      "Iteration 72611, loss = 1.48631281\n",
      "Iteration 72612, loss = 1.74318965\n",
      "Iteration 72613, loss = 1.30719169\n",
      "Iteration 72614, loss = 1.97320809\n",
      "Iteration 72615, loss = 1.80642634\n",
      "Iteration 72616, loss = 1.60619692\n",
      "Iteration 72617, loss = 1.29655600\n",
      "Iteration 72618, loss = 1.17287887\n",
      "Iteration 72619, loss = 1.14338543\n",
      "Iteration 72620, loss = 1.15981283\n",
      "Iteration 72621, loss = 1.14299328\n",
      "Iteration 72622, loss = 1.20591615\n",
      "Iteration 72623, loss = 1.23806725\n",
      "Iteration 72624, loss = 1.15295802\n",
      "Iteration 72625, loss = 1.08376406\n",
      "Iteration 72626, loss = 1.09004892\n",
      "Iteration 72627, loss = 1.12852586\n",
      "Iteration 72628, loss = 1.03186527\n",
      "Iteration 72629, loss = 0.98890619\n",
      "Iteration 72630, loss = 1.13390312\n",
      "Iteration 72631, loss = 1.13399912\n",
      "Iteration 72632, loss = 1.04937169\n",
      "Iteration 72633, loss = 1.25903860\n",
      "Iteration 72634, loss = 1.21944984\n",
      "Iteration 72635, loss = 1.11646440\n",
      "Iteration 72636, loss = 1.07914647\n",
      "Iteration 72637, loss = 1.06870073\n",
      "Iteration 72638, loss = 1.09894224\n",
      "Iteration 72639, loss = 1.26935818\n",
      "Iteration 72640, loss = 1.29456192\n",
      "Iteration 72641, loss = 1.32975805\n",
      "Iteration 72642, loss = 1.20375011\n",
      "Iteration 72643, loss = 1.18709103\n",
      "Iteration 72644, loss = 1.36038337\n",
      "Iteration 72645, loss = 1.61667821\n",
      "Iteration 72646, loss = 1.45016775\n",
      "Iteration 72647, loss = 1.26694471\n",
      "Iteration 72648, loss = 1.26729486\n",
      "Iteration 72649, loss = 1.20222560\n",
      "Iteration 72650, loss = 1.59644804\n",
      "Iteration 72651, loss = 1.22229157\n",
      "Iteration 72652, loss = 1.30052237\n",
      "Iteration 72653, loss = 1.24586436\n",
      "Iteration 72654, loss = 1.46542003\n",
      "Iteration 72655, loss = 1.37885962\n",
      "Iteration 72656, loss = 1.24843011\n",
      "Iteration 72657, loss = 1.21256603\n",
      "Iteration 72658, loss = 2.24669749\n",
      "Iteration 72659, loss = 3.95513898\n",
      "Iteration 72660, loss = 4.52309244\n",
      "Iteration 72661, loss = 3.66596076\n",
      "Iteration 72662, loss = 2.10634930\n",
      "Iteration 72663, loss = 2.40489405\n",
      "Iteration 72664, loss = 2.67046594\n",
      "Iteration 72665, loss = 1.99374150\n",
      "Iteration 72666, loss = 2.57825348\n",
      "Iteration 72667, loss = 2.56194047\n",
      "Iteration 72668, loss = 2.67780018\n",
      "Iteration 72669, loss = 1.38472310\n",
      "Iteration 72670, loss = 1.21014497\n",
      "Iteration 72671, loss = 1.13805026\n",
      "Iteration 72672, loss = 1.42262775\n",
      "Iteration 72673, loss = 1.44589225\n",
      "Iteration 72674, loss = 1.47166831\n",
      "Iteration 72675, loss = 1.88558507\n",
      "Iteration 72676, loss = 1.76404187\n",
      "Iteration 72677, loss = 1.66076228\n",
      "Iteration 72678, loss = 1.93213812\n",
      "Iteration 72679, loss = 1.74963616\n",
      "Iteration 72680, loss = 1.56570603\n",
      "Iteration 72681, loss = 1.38632936\n",
      "Iteration 72682, loss = 1.52464269\n",
      "Iteration 72683, loss = 1.39746363\n",
      "Iteration 72684, loss = 1.67674769\n",
      "Iteration 72685, loss = 1.38162750\n",
      "Iteration 72686, loss = 1.30117691\n",
      "Iteration 72687, loss = 1.14466072\n",
      "Iteration 72688, loss = 1.23176671\n",
      "Iteration 72689, loss = 1.25614499\n",
      "Iteration 72690, loss = 1.18389458\n",
      "Iteration 72691, loss = 1.42790375\n",
      "Iteration 72692, loss = 1.18390364\n",
      "Iteration 72693, loss = 1.24877287\n",
      "Iteration 72694, loss = 1.24817798\n",
      "Iteration 72695, loss = 1.06204706\n",
      "Iteration 72696, loss = 1.00967754\n",
      "Iteration 72697, loss = 0.98204312\n",
      "Iteration 72698, loss = 1.21021158\n",
      "Iteration 72699, loss = 1.05867229\n",
      "Iteration 72700, loss = 1.00157280\n",
      "Iteration 72701, loss = 1.04258992\n",
      "Iteration 72702, loss = 1.07095887\n",
      "Iteration 72703, loss = 1.02905382\n",
      "Iteration 72704, loss = 1.05094164\n",
      "Iteration 72705, loss = 1.27660718\n",
      "Iteration 72706, loss = 2.22050397\n",
      "Iteration 72707, loss = 2.08724303\n",
      "Iteration 72708, loss = 2.79274733\n",
      "Iteration 72709, loss = 2.21081089\n",
      "Iteration 72710, loss = 2.24185971\n",
      "Iteration 72711, loss = 2.32967716\n",
      "Iteration 72712, loss = 2.19566977\n",
      "Iteration 72713, loss = 1.67260672\n",
      "Iteration 72714, loss = 2.12754281\n",
      "Iteration 72715, loss = 1.56549042\n",
      "Iteration 72716, loss = 1.23465691\n",
      "Iteration 72717, loss = 1.24655645\n",
      "Iteration 72718, loss = 1.50250001\n",
      "Iteration 72719, loss = 1.42606256\n",
      "Iteration 72720, loss = 0.99148934\n",
      "Iteration 72721, loss = 1.07024523\n",
      "Iteration 72722, loss = 0.99727662\n",
      "Iteration 72723, loss = 0.99864444\n",
      "Iteration 72724, loss = 1.01721090\n",
      "Iteration 72725, loss = 0.97486259\n",
      "Iteration 72726, loss = 1.13775802\n",
      "Iteration 72727, loss = 1.15094346\n",
      "Iteration 72728, loss = 1.10012342\n",
      "Iteration 72729, loss = 1.04888390\n",
      "Iteration 72730, loss = 1.04564968\n",
      "Iteration 72731, loss = 1.08627459\n",
      "Iteration 72732, loss = 1.15359559\n",
      "Iteration 72733, loss = 1.14079067\n",
      "Iteration 72734, loss = 1.09311262\n",
      "Iteration 72735, loss = 1.54950202\n",
      "Iteration 72736, loss = 1.17918774\n",
      "Iteration 72737, loss = 1.24691048\n",
      "Iteration 72738, loss = 1.18185770\n",
      "Iteration 72739, loss = 1.23584039\n",
      "Iteration 72740, loss = 1.10089178\n",
      "Iteration 72741, loss = 1.16430415\n",
      "Iteration 72742, loss = 1.06361207\n",
      "Iteration 72743, loss = 0.98201855\n",
      "Iteration 72744, loss = 0.95103144\n",
      "Iteration 72745, loss = 0.96422262\n",
      "Iteration 72746, loss = 0.97759002\n",
      "Iteration 72747, loss = 0.95525656\n",
      "Iteration 72748, loss = 0.99395472\n",
      "Iteration 72749, loss = 0.97705008\n",
      "Iteration 72750, loss = 1.03697376\n",
      "Iteration 72751, loss = 1.08860925\n",
      "Iteration 72752, loss = 1.12303266\n",
      "Iteration 72753, loss = 1.18352197\n",
      "Iteration 72754, loss = 1.09321698\n",
      "Iteration 72755, loss = 1.30777898\n",
      "Iteration 72756, loss = 1.22683394\n",
      "Iteration 72757, loss = 1.28370410\n",
      "Iteration 72758, loss = 1.24917427\n",
      "Iteration 72759, loss = 1.22561744\n",
      "Iteration 72760, loss = 1.27821491\n",
      "Iteration 72761, loss = 1.33039947\n",
      "Iteration 72762, loss = 1.51239265\n",
      "Iteration 72763, loss = 1.42594923\n",
      "Iteration 72764, loss = 1.35805727\n",
      "Iteration 72765, loss = 1.48749546\n",
      "Iteration 72766, loss = 1.26542917\n",
      "Iteration 72767, loss = 1.34125811\n",
      "Iteration 72768, loss = 1.21076248\n",
      "Iteration 72769, loss = 1.17102657\n",
      "Iteration 72770, loss = 1.25556753\n",
      "Iteration 72771, loss = 1.19096567\n",
      "Iteration 72772, loss = 1.14893301\n",
      "Iteration 72773, loss = 1.20001691\n",
      "Iteration 72774, loss = 1.12980320\n",
      "Iteration 72775, loss = 1.09818945\n",
      "Iteration 72776, loss = 1.12363392\n",
      "Iteration 72777, loss = 1.14315189\n",
      "Iteration 72778, loss = 1.08962774\n",
      "Iteration 72779, loss = 1.11655267\n",
      "Iteration 72780, loss = 1.04256481\n",
      "Iteration 72781, loss = 1.09920913\n",
      "Iteration 72782, loss = 1.13254695\n",
      "Iteration 72783, loss = 1.12723663\n",
      "Iteration 72784, loss = 1.35152994\n",
      "Iteration 72785, loss = 1.27609227\n",
      "Iteration 72786, loss = 1.69660937\n",
      "Iteration 72787, loss = 1.30279500\n",
      "Iteration 72788, loss = 1.21124430\n",
      "Iteration 72789, loss = 1.28671713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 72790, loss = 1.21919875\n",
      "Iteration 72791, loss = 1.19275009\n",
      "Iteration 72792, loss = 1.37019503\n",
      "Iteration 72793, loss = 1.35149833\n",
      "Iteration 72794, loss = 1.71594763\n",
      "Iteration 72795, loss = 1.36260151\n",
      "Iteration 72796, loss = 1.34959275\n",
      "Iteration 72797, loss = 1.48995723\n",
      "Iteration 72798, loss = 1.33148070\n",
      "Iteration 72799, loss = 1.15271289\n",
      "Iteration 72800, loss = 1.18939888\n",
      "Iteration 72801, loss = 1.28915351\n",
      "Iteration 72802, loss = 1.28356643\n",
      "Iteration 72803, loss = 1.48803554\n",
      "Iteration 72804, loss = 1.39518456\n",
      "Iteration 72805, loss = 1.33173303\n",
      "Iteration 72806, loss = 1.49235157\n",
      "Iteration 72807, loss = 1.37113200\n",
      "Iteration 72808, loss = 1.23837879\n",
      "Iteration 72809, loss = 1.20634292\n",
      "Iteration 72810, loss = 1.17623633\n",
      "Iteration 72811, loss = 1.40896082\n",
      "Iteration 72812, loss = 1.48861536\n",
      "Iteration 72813, loss = 1.36523020\n",
      "Iteration 72814, loss = 1.29728567\n",
      "Iteration 72815, loss = 1.23164373\n",
      "Iteration 72816, loss = 1.25867535\n",
      "Iteration 72817, loss = 1.12522504\n",
      "Iteration 72818, loss = 1.53179172\n",
      "Iteration 72819, loss = 1.32179124\n",
      "Iteration 72820, loss = 1.96580887\n",
      "Iteration 72821, loss = 1.74885568\n",
      "Iteration 72822, loss = 1.66580735\n",
      "Iteration 72823, loss = 2.27016020\n",
      "Iteration 72824, loss = 1.75663952\n",
      "Iteration 72825, loss = 1.65769241\n",
      "Iteration 72826, loss = 1.28049383\n",
      "Iteration 72827, loss = 1.44966244\n",
      "Iteration 72828, loss = 1.60929713\n",
      "Iteration 72829, loss = 1.59870576\n",
      "Iteration 72830, loss = 1.37049347\n",
      "Iteration 72831, loss = 1.06670837\n",
      "Iteration 72832, loss = 1.20557025\n",
      "Iteration 72833, loss = 1.19029386\n",
      "Iteration 72834, loss = 1.68098134\n",
      "Iteration 72835, loss = 1.17986094\n",
      "Iteration 72836, loss = 1.14090818\n",
      "Iteration 72837, loss = 1.19101291\n",
      "Iteration 72838, loss = 1.21273273\n",
      "Iteration 72839, loss = 1.38711392\n",
      "Iteration 72840, loss = 1.30598850\n",
      "Iteration 72841, loss = 1.29015403\n",
      "Iteration 72842, loss = 1.42927008\n",
      "Iteration 72843, loss = 1.49414829\n",
      "Iteration 72844, loss = 1.57715993\n",
      "Iteration 72845, loss = 1.26775733\n",
      "Iteration 72846, loss = 1.63577314\n",
      "Iteration 72847, loss = 1.34640508\n",
      "Iteration 72848, loss = 1.33459890\n",
      "Iteration 72849, loss = 1.33797696\n",
      "Iteration 72850, loss = 1.19901289\n",
      "Iteration 72851, loss = 1.35119085\n",
      "Iteration 72852, loss = 1.30979139\n",
      "Iteration 72853, loss = 1.54935417\n",
      "Iteration 72854, loss = 1.75807530\n",
      "Iteration 72855, loss = 1.48525668\n",
      "Iteration 72856, loss = 1.36147836\n",
      "Iteration 72857, loss = 1.20492059\n",
      "Iteration 72858, loss = 1.23853366\n",
      "Iteration 72859, loss = 1.68707290\n",
      "Iteration 72860, loss = 1.79000101\n",
      "Iteration 72861, loss = 2.20913912\n",
      "Iteration 72862, loss = 2.38334075\n",
      "Iteration 72863, loss = 1.90190821\n",
      "Iteration 72864, loss = 1.61063017\n",
      "Iteration 72865, loss = 1.46524551\n",
      "Iteration 72866, loss = 1.76345748\n",
      "Iteration 72867, loss = 2.01680500\n",
      "Iteration 72868, loss = 1.52358905\n",
      "Iteration 72869, loss = 1.70134855\n",
      "Iteration 72870, loss = 1.53756271\n",
      "Iteration 72871, loss = 1.19290714\n",
      "Iteration 72872, loss = 1.28489450\n",
      "Iteration 72873, loss = 1.21345443\n",
      "Iteration 72874, loss = 1.13085353\n",
      "Iteration 72875, loss = 1.12229870\n",
      "Iteration 72876, loss = 1.01185972\n",
      "Iteration 72877, loss = 0.98734673\n",
      "Iteration 72878, loss = 0.98013483\n",
      "Iteration 72879, loss = 1.04359085\n",
      "Iteration 72880, loss = 1.01886120\n",
      "Iteration 72881, loss = 1.03333289\n",
      "Iteration 72882, loss = 1.02979294\n",
      "Iteration 72883, loss = 1.03445257\n",
      "Iteration 72884, loss = 1.05356710\n",
      "Iteration 72885, loss = 1.13569954\n",
      "Iteration 72886, loss = 1.10102659\n",
      "Iteration 72887, loss = 1.33383002\n",
      "Iteration 72888, loss = 1.22769290\n",
      "Iteration 72889, loss = 1.49799977\n",
      "Iteration 72890, loss = 1.77185717\n",
      "Iteration 72891, loss = 2.23850170\n",
      "Iteration 72892, loss = 2.74655976\n",
      "Iteration 72893, loss = 2.27349176\n",
      "Iteration 72894, loss = 2.01339949\n",
      "Iteration 72895, loss = 2.09468736\n",
      "Iteration 72896, loss = 1.82766830\n",
      "Iteration 72897, loss = 1.61601618\n",
      "Iteration 72898, loss = 1.53620203\n",
      "Iteration 72899, loss = 1.44223995\n",
      "Iteration 72900, loss = 1.47427710\n",
      "Iteration 72901, loss = 1.29046230\n",
      "Iteration 72902, loss = 1.12655476\n",
      "Iteration 72903, loss = 1.10214682\n",
      "Iteration 72904, loss = 1.24664944\n",
      "Iteration 72905, loss = 1.02768018\n",
      "Iteration 72906, loss = 1.03962375\n",
      "Iteration 72907, loss = 1.02094775\n",
      "Iteration 72908, loss = 1.22528976\n",
      "Iteration 72909, loss = 1.05396552\n",
      "Iteration 72910, loss = 1.13554889\n",
      "Iteration 72911, loss = 1.10299087\n",
      "Iteration 72912, loss = 1.23432575\n",
      "Iteration 72913, loss = 1.41027699\n",
      "Iteration 72914, loss = 1.59439422\n",
      "Iteration 72915, loss = 1.57168659\n",
      "Iteration 72916, loss = 1.63861956\n",
      "Iteration 72917, loss = 1.57714162\n",
      "Iteration 72918, loss = 1.33767670\n",
      "Iteration 72919, loss = 1.44051939\n",
      "Iteration 72920, loss = 1.23546064\n",
      "Iteration 72921, loss = 1.36223403\n",
      "Iteration 72922, loss = 1.29963538\n",
      "Iteration 72923, loss = 1.64010462\n",
      "Iteration 72924, loss = 2.32084784\n",
      "Iteration 72925, loss = 2.23592770\n",
      "Iteration 72926, loss = 1.66059762\n",
      "Iteration 72927, loss = 1.41278583\n",
      "Iteration 72928, loss = 1.34964609\n",
      "Iteration 72929, loss = 1.36910543\n",
      "Iteration 72930, loss = 1.49446665\n",
      "Iteration 72931, loss = 1.47615465\n",
      "Iteration 72932, loss = 1.81436870\n",
      "Iteration 72933, loss = 1.65753750\n",
      "Iteration 72934, loss = 1.24044635\n",
      "Iteration 72935, loss = 1.05634109\n",
      "Iteration 72936, loss = 1.06758607\n",
      "Iteration 72937, loss = 1.02816746\n",
      "Iteration 72938, loss = 1.14566054\n",
      "Iteration 72939, loss = 1.43576356\n",
      "Iteration 72940, loss = 1.05781069\n",
      "Iteration 72941, loss = 1.12191868\n",
      "Iteration 72942, loss = 1.11189214\n",
      "Iteration 72943, loss = 1.12192155\n",
      "Iteration 72944, loss = 1.18330827\n",
      "Iteration 72945, loss = 1.23571236\n",
      "Iteration 72946, loss = 1.34173828\n",
      "Iteration 72947, loss = 1.32573669\n",
      "Iteration 72948, loss = 1.12960983\n",
      "Iteration 72949, loss = 1.19112176\n",
      "Iteration 72950, loss = 1.12137379\n",
      "Iteration 72951, loss = 1.28032372\n",
      "Iteration 72952, loss = 1.48475353\n",
      "Iteration 72953, loss = 1.19852586\n",
      "Iteration 72954, loss = 1.56145277\n",
      "Iteration 72955, loss = 1.81589063\n",
      "Iteration 72956, loss = 1.30055562\n",
      "Iteration 72957, loss = 1.33431007\n",
      "Iteration 72958, loss = 1.19656003\n",
      "Iteration 72959, loss = 1.30805864\n",
      "Iteration 72960, loss = 0.99466907\n",
      "Iteration 72961, loss = 1.10224802\n",
      "Iteration 72962, loss = 1.10560687\n",
      "Iteration 72963, loss = 1.20160156\n",
      "Iteration 72964, loss = 1.16686038\n",
      "Iteration 72965, loss = 1.19284446\n",
      "Iteration 72966, loss = 1.26555804\n",
      "Iteration 72967, loss = 1.17081477\n",
      "Iteration 72968, loss = 1.18513489\n",
      "Iteration 72969, loss = 1.28796186\n",
      "Iteration 72970, loss = 1.29193688\n",
      "Iteration 72971, loss = 1.31157316\n",
      "Iteration 72972, loss = 1.14604279\n",
      "Iteration 72973, loss = 1.33029393\n",
      "Iteration 72974, loss = 1.35918735\n",
      "Iteration 72975, loss = 1.63146738\n",
      "Iteration 72976, loss = 1.26392191\n",
      "Iteration 72977, loss = 1.70498714\n",
      "Iteration 72978, loss = 1.57388910\n",
      "Iteration 72979, loss = 1.65368277\n",
      "Iteration 72980, loss = 1.59262708\n",
      "Iteration 72981, loss = 1.54768895\n",
      "Iteration 72982, loss = 1.58146202\n",
      "Iteration 72983, loss = 1.33503897\n",
      "Iteration 72984, loss = 1.36447513\n",
      "Iteration 72985, loss = 1.20392041\n",
      "Iteration 72986, loss = 1.44506260\n",
      "Iteration 72987, loss = 1.47986892\n",
      "Iteration 72988, loss = 1.58048623\n",
      "Iteration 72989, loss = 1.27103146\n",
      "Iteration 72990, loss = 1.24091404\n",
      "Iteration 72991, loss = 1.18291454\n",
      "Iteration 72992, loss = 1.12742769\n",
      "Iteration 72993, loss = 1.13409576\n",
      "Iteration 72994, loss = 1.20264162\n",
      "Iteration 72995, loss = 1.21834323\n",
      "Iteration 72996, loss = 1.06677658\n",
      "Iteration 72997, loss = 1.18413827\n",
      "Iteration 72998, loss = 1.45879813\n",
      "Iteration 72999, loss = 1.30425154\n",
      "Iteration 73000, loss = 1.26786528\n",
      "Iteration 73001, loss = 1.33553518\n",
      "Iteration 73002, loss = 1.20163595\n",
      "Iteration 73003, loss = 1.86919883\n",
      "Iteration 73004, loss = 1.93773860\n",
      "Iteration 73005, loss = 1.41901825\n",
      "Iteration 73006, loss = 1.10392428\n",
      "Iteration 73007, loss = 1.17574817\n",
      "Iteration 73008, loss = 1.15376407\n",
      "Iteration 73009, loss = 1.17637283\n",
      "Iteration 73010, loss = 1.24787957\n",
      "Iteration 73011, loss = 1.38667855\n",
      "Iteration 73012, loss = 1.31361489\n",
      "Iteration 73013, loss = 1.28394470\n",
      "Iteration 73014, loss = 1.64683341\n",
      "Iteration 73015, loss = 1.39491903\n",
      "Iteration 73016, loss = 1.12267836\n",
      "Iteration 73017, loss = 1.32378138\n",
      "Iteration 73018, loss = 1.19352941\n",
      "Iteration 73019, loss = 1.10855415\n",
      "Iteration 73020, loss = 1.33716775\n",
      "Iteration 73021, loss = 1.57901167\n",
      "Iteration 73022, loss = 1.69563625\n",
      "Iteration 73023, loss = 2.35861051\n",
      "Iteration 73024, loss = 2.10597369\n",
      "Iteration 73025, loss = 1.75349677\n",
      "Iteration 73026, loss = 2.09903987\n",
      "Iteration 73027, loss = 1.99467903\n",
      "Iteration 73028, loss = 1.67326068\n",
      "Iteration 73029, loss = 1.78921985\n",
      "Iteration 73030, loss = 1.51865740\n",
      "Iteration 73031, loss = 1.26820992\n",
      "Iteration 73032, loss = 1.19532143\n",
      "Iteration 73033, loss = 1.10453769\n",
      "Iteration 73034, loss = 1.10432289\n",
      "Iteration 73035, loss = 1.16241491\n",
      "Iteration 73036, loss = 1.11660859\n",
      "Iteration 73037, loss = 1.27600684\n",
      "Iteration 73038, loss = 1.04766196\n",
      "Iteration 73039, loss = 1.13597468\n",
      "Iteration 73040, loss = 0.95552197\n",
      "Iteration 73041, loss = 0.98971797\n",
      "Iteration 73042, loss = 1.06094387\n",
      "Iteration 73043, loss = 1.22042003\n",
      "Iteration 73044, loss = 1.12677338\n",
      "Iteration 73045, loss = 1.25540342\n",
      "Iteration 73046, loss = 1.05702046\n",
      "Iteration 73047, loss = 1.28487676\n",
      "Iteration 73048, loss = 1.20180848\n",
      "Iteration 73049, loss = 1.27379936\n",
      "Iteration 73050, loss = 1.17573929\n",
      "Iteration 73051, loss = 1.14158895\n",
      "Iteration 73052, loss = 1.01134914\n",
      "Iteration 73053, loss = 1.29049101\n",
      "Iteration 73054, loss = 1.45821448\n",
      "Iteration 73055, loss = 1.39136234\n",
      "Iteration 73056, loss = 1.24790207\n",
      "Iteration 73057, loss = 1.58575189\n",
      "Iteration 73058, loss = 1.63922277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 73059, loss = 1.80838756\n",
      "Iteration 73060, loss = 1.55635852\n",
      "Iteration 73061, loss = 1.29322320\n",
      "Iteration 73062, loss = 1.17150526\n",
      "Iteration 73063, loss = 1.11804088\n",
      "Iteration 73064, loss = 1.31229647\n",
      "Iteration 73065, loss = 1.05182180\n",
      "Iteration 73066, loss = 1.18852536\n",
      "Iteration 73067, loss = 1.66713977\n",
      "Iteration 73068, loss = 1.88140018\n",
      "Iteration 73069, loss = 1.51619941\n",
      "Iteration 73070, loss = 1.43453469\n",
      "Iteration 73071, loss = 1.41331291\n",
      "Iteration 73072, loss = 1.32047438\n",
      "Iteration 73073, loss = 1.30402476\n",
      "Iteration 73074, loss = 1.30335898\n",
      "Iteration 73075, loss = 1.23203843\n",
      "Iteration 73076, loss = 1.13815564\n",
      "Iteration 73077, loss = 1.26919807\n",
      "Iteration 73078, loss = 1.17014214\n",
      "Iteration 73079, loss = 1.13803333\n",
      "Iteration 73080, loss = 1.17549501\n",
      "Iteration 73081, loss = 1.10660800\n",
      "Iteration 73082, loss = 1.01567702\n",
      "Iteration 73083, loss = 1.17623314\n",
      "Iteration 73084, loss = 1.20101906\n",
      "Iteration 73085, loss = 1.19361754\n",
      "Iteration 73086, loss = 1.06966333\n",
      "Iteration 73087, loss = 1.19821408\n",
      "Iteration 73088, loss = 1.25086543\n",
      "Iteration 73089, loss = 1.16091910\n",
      "Iteration 73090, loss = 1.53314185\n",
      "Iteration 73091, loss = 1.73162093\n",
      "Iteration 73092, loss = 1.66761206\n",
      "Iteration 73093, loss = 1.57385943\n",
      "Iteration 73094, loss = 1.30742692\n",
      "Iteration 73095, loss = 1.20854631\n",
      "Iteration 73096, loss = 1.15564312\n",
      "Iteration 73097, loss = 1.18036684\n",
      "Iteration 73098, loss = 1.19718950\n",
      "Iteration 73099, loss = 1.17525186\n",
      "Iteration 73100, loss = 1.15929956\n",
      "Iteration 73101, loss = 1.52586385\n",
      "Iteration 73102, loss = 1.79467132\n",
      "Iteration 73103, loss = 1.44585295\n",
      "Iteration 73104, loss = 1.83608426\n",
      "Iteration 73105, loss = 1.80146920\n",
      "Iteration 73106, loss = 2.66983961\n",
      "Iteration 73107, loss = 1.93773884\n",
      "Iteration 73108, loss = 1.63381964\n",
      "Iteration 73109, loss = 1.29710357\n",
      "Iteration 73110, loss = 1.27121567\n",
      "Iteration 73111, loss = 1.16906077\n",
      "Iteration 73112, loss = 1.20684694\n",
      "Iteration 73113, loss = 1.23335326\n",
      "Iteration 73114, loss = 1.23887727\n",
      "Iteration 73115, loss = 1.15954092\n",
      "Iteration 73116, loss = 1.13199008\n",
      "Iteration 73117, loss = 1.21257745\n",
      "Iteration 73118, loss = 1.31494945\n",
      "Iteration 73119, loss = 1.15216168\n",
      "Iteration 73120, loss = 1.09843268\n",
      "Iteration 73121, loss = 1.00220306\n",
      "Iteration 73122, loss = 0.99126096\n",
      "Iteration 73123, loss = 1.05640096\n",
      "Iteration 73124, loss = 1.05779254\n",
      "Iteration 73125, loss = 1.07609225\n",
      "Iteration 73126, loss = 1.06633032\n",
      "Iteration 73127, loss = 1.14747601\n",
      "Iteration 73128, loss = 1.27350742\n",
      "Iteration 73129, loss = 1.43691324\n",
      "Iteration 73130, loss = 1.41970323\n",
      "Iteration 73131, loss = 1.41974014\n",
      "Iteration 73132, loss = 1.15974659\n",
      "Iteration 73133, loss = 1.18584916\n",
      "Iteration 73134, loss = 1.33011030\n",
      "Iteration 73135, loss = 1.77581832\n",
      "Iteration 73136, loss = 1.60979918\n",
      "Iteration 73137, loss = 1.45056849\n",
      "Iteration 73138, loss = 1.35057867\n",
      "Iteration 73139, loss = 1.38085451\n",
      "Iteration 73140, loss = 1.23753335\n",
      "Iteration 73141, loss = 1.21278140\n",
      "Iteration 73142, loss = 1.08036735\n",
      "Iteration 73143, loss = 1.03504809\n",
      "Iteration 73144, loss = 1.01285556\n",
      "Iteration 73145, loss = 1.05431128\n",
      "Iteration 73146, loss = 1.03201850\n",
      "Iteration 73147, loss = 1.08043304\n",
      "Iteration 73148, loss = 1.37609329\n",
      "Iteration 73149, loss = 1.36514237\n",
      "Iteration 73150, loss = 1.39090051\n",
      "Iteration 73151, loss = 1.20072932\n",
      "Iteration 73152, loss = 1.04975297\n",
      "Iteration 73153, loss = 1.12244310\n",
      "Iteration 73154, loss = 1.27840347\n",
      "Iteration 73155, loss = 1.20541986\n",
      "Iteration 73156, loss = 1.22919273\n",
      "Iteration 73157, loss = 1.45216361\n",
      "Iteration 73158, loss = 1.39226469\n",
      "Iteration 73159, loss = 1.59309623\n",
      "Iteration 73160, loss = 1.34543756\n",
      "Iteration 73161, loss = 2.16140834\n",
      "Iteration 73162, loss = 2.03349261\n",
      "Iteration 73163, loss = 1.80691826\n",
      "Iteration 73164, loss = 1.72729323\n",
      "Iteration 73165, loss = 1.86075413\n",
      "Iteration 73166, loss = 1.95820300\n",
      "Iteration 73167, loss = 1.90364755\n",
      "Iteration 73168, loss = 1.55101017\n",
      "Iteration 73169, loss = 1.25662312\n",
      "Iteration 73170, loss = 1.39548350\n",
      "Iteration 73171, loss = 1.33171057\n",
      "Iteration 73172, loss = 1.22780036\n",
      "Iteration 73173, loss = 1.16956525\n",
      "Iteration 73174, loss = 1.16972017\n",
      "Iteration 73175, loss = 1.14751363\n",
      "Iteration 73176, loss = 1.17713440\n",
      "Iteration 73177, loss = 1.17306555\n",
      "Iteration 73178, loss = 1.10964818\n",
      "Iteration 73179, loss = 1.13867448\n",
      "Iteration 73180, loss = 1.30338337\n",
      "Iteration 73181, loss = 1.31254041\n",
      "Iteration 73182, loss = 1.42932451\n",
      "Iteration 73183, loss = 1.37989321\n",
      "Iteration 73184, loss = 1.48176427\n",
      "Iteration 73185, loss = 2.05338928\n",
      "Iteration 73186, loss = 1.76796063\n",
      "Iteration 73187, loss = 1.85084092\n",
      "Iteration 73188, loss = 1.45996086\n",
      "Iteration 73189, loss = 1.38248270\n",
      "Iteration 73190, loss = 1.51404686\n",
      "Iteration 73191, loss = 2.22536851\n",
      "Iteration 73192, loss = 2.40300944\n",
      "Iteration 73193, loss = 1.29122543\n",
      "Iteration 73194, loss = 1.19206427\n",
      "Iteration 73195, loss = 1.31146879\n",
      "Iteration 73196, loss = 1.24433431\n",
      "Iteration 73197, loss = 1.25968892\n",
      "Iteration 73198, loss = 1.13174277\n",
      "Iteration 73199, loss = 1.00536571\n",
      "Iteration 73200, loss = 1.01343753\n",
      "Iteration 73201, loss = 1.01181485\n",
      "Iteration 73202, loss = 1.06735530\n",
      "Iteration 73203, loss = 1.17506180\n",
      "Iteration 73204, loss = 1.22148999\n",
      "Iteration 73205, loss = 1.22178284\n",
      "Iteration 73206, loss = 1.39025064\n",
      "Iteration 73207, loss = 1.60049973\n",
      "Iteration 73208, loss = 1.18759985\n",
      "Iteration 73209, loss = 1.08471658\n",
      "Iteration 73210, loss = 1.38050223\n",
      "Iteration 73211, loss = 1.67757662\n",
      "Iteration 73212, loss = 1.18467653\n",
      "Iteration 73213, loss = 1.04048710\n",
      "Iteration 73214, loss = 1.21133100\n",
      "Iteration 73215, loss = 1.22587577\n",
      "Iteration 73216, loss = 1.07176495\n",
      "Iteration 73217, loss = 1.26138973\n",
      "Iteration 73218, loss = 1.18978428\n",
      "Iteration 73219, loss = 1.43034484\n",
      "Iteration 73220, loss = 1.42644057\n",
      "Iteration 73221, loss = 1.27782686\n",
      "Iteration 73222, loss = 1.17980577\n",
      "Iteration 73223, loss = 1.30485256\n",
      "Iteration 73224, loss = 1.08606317\n",
      "Iteration 73225, loss = 1.16332835\n",
      "Iteration 73226, loss = 1.10274397\n",
      "Iteration 73227, loss = 1.01134643\n",
      "Iteration 73228, loss = 1.06436985\n",
      "Iteration 73229, loss = 1.03331381\n",
      "Iteration 73230, loss = 1.10174784\n",
      "Iteration 73231, loss = 1.13956811\n",
      "Iteration 73232, loss = 1.09392898\n",
      "Iteration 73233, loss = 1.23578549\n",
      "Iteration 73234, loss = 1.04833250\n",
      "Iteration 73235, loss = 1.00550991\n",
      "Iteration 73236, loss = 0.96905278\n",
      "Iteration 73237, loss = 1.10390036\n",
      "Iteration 73238, loss = 1.19526584\n",
      "Iteration 73239, loss = 1.08723111\n",
      "Iteration 73240, loss = 1.29179447\n",
      "Iteration 73241, loss = 1.23363444\n",
      "Iteration 73242, loss = 1.52760433\n",
      "Iteration 73243, loss = 1.82470763\n",
      "Iteration 73244, loss = 1.70610265\n",
      "Iteration 73245, loss = 1.60169246\n",
      "Iteration 73246, loss = 2.69875251\n",
      "Iteration 73247, loss = 2.18727199\n",
      "Iteration 73248, loss = 1.84973486\n",
      "Iteration 73249, loss = 1.23096994\n",
      "Iteration 73250, loss = 1.42019512\n",
      "Iteration 73251, loss = 1.74874223\n",
      "Iteration 73252, loss = 1.50132898\n",
      "Iteration 73253, loss = 1.23775022\n",
      "Iteration 73254, loss = 1.16061770\n",
      "Iteration 73255, loss = 1.43630520\n",
      "Iteration 73256, loss = 1.43775922\n",
      "Iteration 73257, loss = 1.19775766\n",
      "Iteration 73258, loss = 1.12166768\n",
      "Iteration 73259, loss = 1.17879809\n",
      "Iteration 73260, loss = 1.18402707\n",
      "Iteration 73261, loss = 1.12152735\n",
      "Iteration 73262, loss = 1.07646106\n",
      "Iteration 73263, loss = 1.13032630\n",
      "Iteration 73264, loss = 1.09194132\n",
      "Iteration 73265, loss = 1.26613800\n",
      "Iteration 73266, loss = 1.16442055\n",
      "Iteration 73267, loss = 1.12688259\n",
      "Iteration 73268, loss = 1.62217365\n",
      "Iteration 73269, loss = 2.36975603\n",
      "Iteration 73270, loss = 1.87271121\n",
      "Iteration 73271, loss = 1.75172470\n",
      "Iteration 73272, loss = 1.22717900\n",
      "Iteration 73273, loss = 1.38822808\n",
      "Iteration 73274, loss = 1.27651079\n",
      "Iteration 73275, loss = 1.41541095\n",
      "Iteration 73276, loss = 1.08930340\n",
      "Iteration 73277, loss = 1.18684029\n",
      "Iteration 73278, loss = 1.19520519\n",
      "Iteration 73279, loss = 1.14070833\n",
      "Iteration 73280, loss = 1.14272293\n",
      "Iteration 73281, loss = 1.08769050\n",
      "Iteration 73282, loss = 1.17226174\n",
      "Iteration 73283, loss = 1.08241659\n",
      "Iteration 73284, loss = 1.12317229\n",
      "Iteration 73285, loss = 0.98424197\n",
      "Iteration 73286, loss = 0.99600421\n",
      "Iteration 73287, loss = 1.02450605\n",
      "Iteration 73288, loss = 1.08921235\n",
      "Iteration 73289, loss = 1.02639193\n",
      "Iteration 73290, loss = 0.94719773\n",
      "Iteration 73291, loss = 0.99648564\n",
      "Iteration 73292, loss = 0.96176660\n",
      "Iteration 73293, loss = 0.99815926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 73294, loss = 1.28356395\n",
      "Iteration 73295, loss = 1.06652165\n",
      "Iteration 73296, loss = 1.09222837\n",
      "Iteration 73297, loss = 1.09435962\n",
      "Iteration 73298, loss = 1.07735677\n",
      "Iteration 73299, loss = 1.09978551\n",
      "Iteration 73300, loss = 1.10738369\n",
      "Iteration 73301, loss = 1.23648606\n",
      "Iteration 73302, loss = 1.41950172\n",
      "Iteration 73303, loss = 2.21980372\n",
      "Iteration 73304, loss = 2.21839632\n",
      "Iteration 73305, loss = 2.03213793\n",
      "Iteration 73306, loss = 1.98280545\n",
      "Iteration 73307, loss = 2.73169208\n",
      "Iteration 73308, loss = 3.87097967\n",
      "Iteration 73309, loss = 5.78312794\n",
      "Iteration 73310, loss = 2.36792938\n",
      "Iteration 73311, loss = 3.18709597\n",
      "Iteration 73312, loss = 2.16690568\n",
      "Iteration 73313, loss = 1.73837167\n",
      "Iteration 73314, loss = 1.86713791\n",
      "Iteration 73315, loss = 1.86962706\n",
      "Iteration 73316, loss = 1.74815585\n",
      "Iteration 73317, loss = 1.45747292\n",
      "Iteration 73318, loss = 1.52805775\n",
      "Iteration 73319, loss = 1.13433476\n",
      "Iteration 73320, loss = 1.15712230\n",
      "Iteration 73321, loss = 1.18603845\n",
      "Iteration 73322, loss = 1.13123291\n",
      "Iteration 73323, loss = 1.08247336\n",
      "Iteration 73324, loss = 1.26896692\n",
      "Iteration 73325, loss = 1.18214818\n",
      "Iteration 73326, loss = 1.38268993\n",
      "Iteration 73327, loss = 1.26611297\n",
      "Iteration 73328, loss = 1.10286189\n",
      "Iteration 73329, loss = 1.32001949\n",
      "Iteration 73330, loss = 1.14185493\n",
      "Iteration 73331, loss = 1.11486462\n",
      "Iteration 73332, loss = 1.40987986\n",
      "Iteration 73333, loss = 1.24636262\n",
      "Iteration 73334, loss = 1.15508104\n",
      "Iteration 73335, loss = 1.09806793\n",
      "Iteration 73336, loss = 1.07692174\n",
      "Iteration 73337, loss = 1.22770386\n",
      "Iteration 73338, loss = 1.35704648\n",
      "Iteration 73339, loss = 1.16314060\n",
      "Iteration 73340, loss = 1.13399542\n",
      "Iteration 73341, loss = 1.03871137\n",
      "Iteration 73342, loss = 1.07147279\n",
      "Iteration 73343, loss = 1.04609673\n",
      "Iteration 73344, loss = 1.15981777\n",
      "Iteration 73345, loss = 1.04173095\n",
      "Iteration 73346, loss = 1.24181494\n",
      "Iteration 73347, loss = 1.44983060\n",
      "Iteration 73348, loss = 1.35332505\n",
      "Iteration 73349, loss = 1.03329458\n",
      "Iteration 73350, loss = 1.10705702\n",
      "Iteration 73351, loss = 1.05647016\n",
      "Iteration 73352, loss = 1.14524255\n",
      "Iteration 73353, loss = 1.03616760\n",
      "Iteration 73354, loss = 1.07357185\n",
      "Iteration 73355, loss = 1.30809549\n",
      "Iteration 73356, loss = 1.12604325\n",
      "Iteration 73357, loss = 1.22536581\n",
      "Iteration 73358, loss = 1.14700783\n",
      "Iteration 73359, loss = 1.01746573\n",
      "Iteration 73360, loss = 1.12153436\n",
      "Iteration 73361, loss = 1.09885844\n",
      "Iteration 73362, loss = 1.01687481\n",
      "Iteration 73363, loss = 1.05250827\n",
      "Iteration 73364, loss = 1.02843646\n",
      "Iteration 73365, loss = 1.28476263\n",
      "Iteration 73366, loss = 1.15908471\n",
      "Iteration 73367, loss = 1.16750555\n",
      "Iteration 73368, loss = 1.09740054\n",
      "Iteration 73369, loss = 1.24599686\n",
      "Iteration 73370, loss = 1.32857357\n",
      "Iteration 73371, loss = 1.43565740\n",
      "Iteration 73372, loss = 1.35907442\n",
      "Iteration 73373, loss = 1.18537462\n",
      "Iteration 73374, loss = 1.17507488\n",
      "Iteration 73375, loss = 1.21544275\n",
      "Iteration 73376, loss = 1.11545529\n",
      "Iteration 73377, loss = 1.52703287\n",
      "Iteration 73378, loss = 1.47614763\n",
      "Iteration 73379, loss = 1.44468163\n",
      "Iteration 73380, loss = 1.72863869\n",
      "Iteration 73381, loss = 1.35454155\n",
      "Iteration 73382, loss = 1.23651503\n",
      "Iteration 73383, loss = 1.12233995\n",
      "Iteration 73384, loss = 1.01245991\n",
      "Iteration 73385, loss = 1.06554124\n",
      "Iteration 73386, loss = 1.06580168\n",
      "Iteration 73387, loss = 1.05810146\n",
      "Iteration 73388, loss = 1.03999313\n",
      "Iteration 73389, loss = 1.05103645\n",
      "Iteration 73390, loss = 1.16199745\n",
      "Iteration 73391, loss = 1.09071124\n",
      "Iteration 73392, loss = 1.06168373\n",
      "Iteration 73393, loss = 0.99311151\n",
      "Iteration 73394, loss = 1.12205290\n",
      "Iteration 73395, loss = 1.27910806\n",
      "Iteration 73396, loss = 1.23039115\n",
      "Iteration 73397, loss = 1.03974596\n",
      "Iteration 73398, loss = 1.06901166\n",
      "Iteration 73399, loss = 1.01350047\n",
      "Iteration 73400, loss = 1.38357461\n",
      "Iteration 73401, loss = 1.35833006\n",
      "Iteration 73402, loss = 1.54633731\n",
      "Iteration 73403, loss = 1.46626044\n",
      "Iteration 73404, loss = 1.30754478\n",
      "Iteration 73405, loss = 1.24976139\n",
      "Iteration 73406, loss = 1.20465284\n",
      "Iteration 73407, loss = 1.72658166\n",
      "Iteration 73408, loss = 1.61836053\n",
      "Iteration 73409, loss = 1.96022739\n",
      "Iteration 73410, loss = 1.93049873\n",
      "Iteration 73411, loss = 1.81071890\n",
      "Iteration 73412, loss = 1.65982163\n",
      "Iteration 73413, loss = 1.41749932\n",
      "Iteration 73414, loss = 1.31841202\n",
      "Iteration 73415, loss = 1.19813847\n",
      "Iteration 73416, loss = 1.20497529\n",
      "Iteration 73417, loss = 1.05690645\n",
      "Iteration 73418, loss = 1.02830934\n",
      "Iteration 73419, loss = 1.26713589\n",
      "Iteration 73420, loss = 1.21589915\n",
      "Iteration 73421, loss = 1.07303773\n",
      "Iteration 73422, loss = 0.98423749\n",
      "Iteration 73423, loss = 1.09628865\n",
      "Iteration 73424, loss = 1.45791356\n",
      "Iteration 73425, loss = 1.19810051\n",
      "Iteration 73426, loss = 1.22168414\n",
      "Iteration 73427, loss = 1.38526891\n",
      "Iteration 73428, loss = 1.90337184\n",
      "Iteration 73429, loss = 1.46427236\n",
      "Iteration 73430, loss = 1.30434693\n",
      "Iteration 73431, loss = 1.33279370\n",
      "Iteration 73432, loss = 1.51322677\n",
      "Iteration 73433, loss = 1.26023467\n",
      "Iteration 73434, loss = 1.19215805\n",
      "Iteration 73435, loss = 1.34939233\n",
      "Iteration 73436, loss = 1.09030618\n",
      "Iteration 73437, loss = 1.12025016\n",
      "Iteration 73438, loss = 1.28704466\n",
      "Iteration 73439, loss = 1.14055785\n",
      "Iteration 73440, loss = 1.20310189\n",
      "Iteration 73441, loss = 1.22028216\n",
      "Iteration 73442, loss = 1.18752319\n",
      "Iteration 73443, loss = 1.19442081\n",
      "Iteration 73444, loss = 1.12573463\n",
      "Iteration 73445, loss = 1.09685595\n",
      "Iteration 73446, loss = 1.15798704\n",
      "Iteration 73447, loss = 1.08257268\n",
      "Iteration 73448, loss = 1.15755198\n",
      "Iteration 73449, loss = 1.14887258\n",
      "Iteration 73450, loss = 1.01056364\n",
      "Iteration 73451, loss = 0.96228614\n",
      "Iteration 73452, loss = 1.34607344\n",
      "Iteration 73453, loss = 1.29018959\n",
      "Iteration 73454, loss = 1.01869749\n",
      "Iteration 73455, loss = 1.00531546\n",
      "Iteration 73456, loss = 1.05402722\n",
      "Iteration 73457, loss = 0.96274225\n",
      "Iteration 73458, loss = 1.02760688\n",
      "Iteration 73459, loss = 1.04954019\n",
      "Iteration 73460, loss = 1.03548688\n",
      "Iteration 73461, loss = 1.03224327\n",
      "Iteration 73462, loss = 1.25742605\n",
      "Iteration 73463, loss = 1.48449675\n",
      "Iteration 73464, loss = 1.07064028\n",
      "Iteration 73465, loss = 1.17349471\n",
      "Iteration 73466, loss = 1.20067318\n",
      "Iteration 73467, loss = 1.31124266\n",
      "Iteration 73468, loss = 1.23626797\n",
      "Iteration 73469, loss = 1.24331068\n",
      "Iteration 73470, loss = 1.60595022\n",
      "Iteration 73471, loss = 1.44480577\n",
      "Iteration 73472, loss = 1.43313682\n",
      "Iteration 73473, loss = 1.73089668\n",
      "Iteration 73474, loss = 1.75386396\n",
      "Iteration 73475, loss = 1.81912937\n",
      "Iteration 73476, loss = 1.51645997\n",
      "Iteration 73477, loss = 1.38092500\n",
      "Iteration 73478, loss = 1.78392652\n",
      "Iteration 73479, loss = 1.72485932\n",
      "Iteration 73480, loss = 1.19239687\n",
      "Iteration 73481, loss = 1.10029654\n",
      "Iteration 73482, loss = 1.09738156\n",
      "Iteration 73483, loss = 1.12439787\n",
      "Iteration 73484, loss = 1.26345058\n",
      "Iteration 73485, loss = 1.14128937\n",
      "Iteration 73486, loss = 1.31509748\n",
      "Iteration 73487, loss = 1.45408208\n",
      "Iteration 73488, loss = 1.76829590\n",
      "Iteration 73489, loss = 1.99322008\n",
      "Iteration 73490, loss = 2.14877714\n",
      "Iteration 73491, loss = 1.73188421\n",
      "Iteration 73492, loss = 1.28809025\n",
      "Iteration 73493, loss = 1.33784800\n",
      "Iteration 73494, loss = 1.32998349\n",
      "Iteration 73495, loss = 1.02438893\n",
      "Iteration 73496, loss = 1.13711725\n",
      "Iteration 73497, loss = 1.19526574\n",
      "Iteration 73498, loss = 1.15092074\n",
      "Iteration 73499, loss = 0.98890225\n",
      "Iteration 73500, loss = 1.00059857\n",
      "Iteration 73501, loss = 1.10037004\n",
      "Iteration 73502, loss = 1.13338138\n",
      "Iteration 73503, loss = 1.04788963\n",
      "Iteration 73504, loss = 1.00640875\n",
      "Iteration 73505, loss = 1.26232284\n",
      "Iteration 73506, loss = 1.33090742\n",
      "Iteration 73507, loss = 1.28825434\n",
      "Iteration 73508, loss = 1.32196582\n",
      "Iteration 73509, loss = 1.31727085\n",
      "Iteration 73510, loss = 1.15538236\n",
      "Iteration 73511, loss = 1.12818586\n",
      "Iteration 73512, loss = 1.08090582\n",
      "Iteration 73513, loss = 1.07096897\n",
      "Iteration 73514, loss = 1.22182572\n",
      "Iteration 73515, loss = 1.34767984\n",
      "Iteration 73516, loss = 1.31207187\n",
      "Iteration 73517, loss = 1.31371839\n",
      "Iteration 73518, loss = 1.53352681\n",
      "Iteration 73519, loss = 1.17189325\n",
      "Iteration 73520, loss = 1.24955315\n",
      "Iteration 73521, loss = 1.19059250\n",
      "Iteration 73522, loss = 1.36746732\n",
      "Iteration 73523, loss = 1.19890664\n",
      "Iteration 73524, loss = 1.07906014\n",
      "Iteration 73525, loss = 1.13209818\n",
      "Iteration 73526, loss = 1.22234826\n",
      "Iteration 73527, loss = 1.54004873\n",
      "Iteration 73528, loss = 1.35406160\n",
      "Iteration 73529, loss = 1.36891423\n",
      "Iteration 73530, loss = 1.26940157\n",
      "Iteration 73531, loss = 1.12497304\n",
      "Iteration 73532, loss = 1.10371568\n",
      "Iteration 73533, loss = 1.13508389\n",
      "Iteration 73534, loss = 0.97961732\n",
      "Iteration 73535, loss = 1.06918898\n",
      "Iteration 73536, loss = 0.97959308\n",
      "Iteration 73537, loss = 1.13572616\n",
      "Iteration 73538, loss = 1.00094559\n",
      "Iteration 73539, loss = 0.97204441\n",
      "Iteration 73540, loss = 1.11046494\n",
      "Iteration 73541, loss = 1.05364718\n",
      "Iteration 73542, loss = 1.04065939\n",
      "Iteration 73543, loss = 1.12711097\n",
      "Iteration 73544, loss = 1.22064596\n",
      "Iteration 73545, loss = 1.20346208\n",
      "Iteration 73546, loss = 1.06237446\n",
      "Iteration 73547, loss = 1.17698792\n",
      "Iteration 73548, loss = 1.22359047\n",
      "Iteration 73549, loss = 1.48467909\n",
      "Iteration 73550, loss = 1.80346082\n",
      "Iteration 73551, loss = 2.57483663\n",
      "Iteration 73552, loss = 2.12642983\n",
      "Iteration 73553, loss = 1.95305376\n",
      "Iteration 73554, loss = 1.40260340\n",
      "Iteration 73555, loss = 1.26891943\n",
      "Iteration 73556, loss = 1.65913850\n",
      "Iteration 73557, loss = 1.87300284\n",
      "Iteration 73558, loss = 1.74752127\n",
      "Iteration 73559, loss = 2.15409247\n",
      "Iteration 73560, loss = 1.97122594\n",
      "Iteration 73561, loss = 3.00809789\n",
      "Iteration 73562, loss = 2.11807307\n",
      "Iteration 73563, loss = 1.53746228\n",
      "Iteration 73564, loss = 1.31093275\n",
      "Iteration 73565, loss = 1.37641245\n",
      "Iteration 73566, loss = 1.23397778\n",
      "Iteration 73567, loss = 1.17604005\n",
      "Iteration 73568, loss = 1.14330975\n",
      "Iteration 73569, loss = 1.09789766\n",
      "Iteration 73570, loss = 1.11171267\n",
      "Iteration 73571, loss = 1.06621685\n",
      "Iteration 73572, loss = 1.14778148\n",
      "Iteration 73573, loss = 1.28430326\n",
      "Iteration 73574, loss = 1.37693315\n",
      "Iteration 73575, loss = 1.26882879\n",
      "Iteration 73576, loss = 1.47150173\n",
      "Iteration 73577, loss = 1.58927356\n",
      "Iteration 73578, loss = 1.26937200\n",
      "Iteration 73579, loss = 1.88584511\n",
      "Iteration 73580, loss = 2.98928860\n",
      "Iteration 73581, loss = 2.44025624\n",
      "Iteration 73582, loss = 2.87226480\n",
      "Iteration 73583, loss = 2.81086415\n",
      "Iteration 73584, loss = 2.73437814\n",
      "Iteration 73585, loss = 3.21954604\n",
      "Iteration 73586, loss = 2.92080605\n",
      "Iteration 73587, loss = 1.48306150\n",
      "Iteration 73588, loss = 1.51889763\n",
      "Iteration 73589, loss = 1.57980529\n",
      "Iteration 73590, loss = 1.39025080\n",
      "Iteration 73591, loss = 1.07911486\n",
      "Iteration 73592, loss = 1.03682881\n",
      "Iteration 73593, loss = 1.02487472\n",
      "Iteration 73594, loss = 1.17176836\n",
      "Iteration 73595, loss = 1.12827740\n",
      "Iteration 73596, loss = 1.10206874\n",
      "Iteration 73597, loss = 1.24872877\n",
      "Iteration 73598, loss = 1.28196841\n",
      "Iteration 73599, loss = 1.30825278\n",
      "Iteration 73600, loss = 1.54797467\n",
      "Iteration 73601, loss = 1.21382420\n",
      "Iteration 73602, loss = 1.11100126\n",
      "Iteration 73603, loss = 1.23915182\n",
      "Iteration 73604, loss = 1.60186637\n",
      "Iteration 73605, loss = 1.85708579\n",
      "Iteration 73606, loss = 1.35851514\n",
      "Iteration 73607, loss = 1.25128901\n",
      "Iteration 73608, loss = 1.86141114\n",
      "Iteration 73609, loss = 1.40382995\n",
      "Iteration 73610, loss = 1.35382103\n",
      "Iteration 73611, loss = 1.48988334\n",
      "Iteration 73612, loss = 1.35465011\n",
      "Iteration 73613, loss = 1.40980500\n",
      "Iteration 73614, loss = 1.17313071\n",
      "Iteration 73615, loss = 1.09463212\n",
      "Iteration 73616, loss = 1.08030347\n",
      "Iteration 73617, loss = 1.07743920\n",
      "Iteration 73618, loss = 1.94143800\n",
      "Iteration 73619, loss = 1.78035108\n",
      "Iteration 73620, loss = 1.52862245\n",
      "Iteration 73621, loss = 1.39180367\n",
      "Iteration 73622, loss = 1.36157120\n",
      "Iteration 73623, loss = 1.21788735\n",
      "Iteration 73624, loss = 1.11353073\n",
      "Iteration 73625, loss = 0.97349411\n",
      "Iteration 73626, loss = 1.04944182\n",
      "Iteration 73627, loss = 1.06641643\n",
      "Iteration 73628, loss = 1.04359562\n",
      "Iteration 73629, loss = 1.16844693\n",
      "Iteration 73630, loss = 1.25453176\n",
      "Iteration 73631, loss = 1.12675108\n",
      "Iteration 73632, loss = 1.31618469\n",
      "Iteration 73633, loss = 1.11124870\n",
      "Iteration 73634, loss = 1.12335980\n",
      "Iteration 73635, loss = 1.30666542\n",
      "Iteration 73636, loss = 1.51150053\n",
      "Iteration 73637, loss = 1.72724519\n",
      "Iteration 73638, loss = 1.75149582\n",
      "Iteration 73639, loss = 2.05351011\n",
      "Iteration 73640, loss = 1.98195521\n",
      "Iteration 73641, loss = 1.38568203\n",
      "Iteration 73642, loss = 1.11340595\n",
      "Iteration 73643, loss = 1.46286026\n",
      "Iteration 73644, loss = 1.40738346\n",
      "Iteration 73645, loss = 1.43394110\n",
      "Iteration 73646, loss = 1.29702834\n",
      "Iteration 73647, loss = 1.22883316\n",
      "Iteration 73648, loss = 1.06537823\n",
      "Iteration 73649, loss = 1.10790603\n",
      "Iteration 73650, loss = 1.23279815\n",
      "Iteration 73651, loss = 1.35668461\n",
      "Iteration 73652, loss = 1.66425286\n",
      "Iteration 73653, loss = 1.47088528\n",
      "Iteration 73654, loss = 1.23489259\n",
      "Iteration 73655, loss = 1.77722132\n",
      "Iteration 73656, loss = 1.30258915\n",
      "Iteration 73657, loss = 1.19344825\n",
      "Iteration 73658, loss = 1.25086741\n",
      "Iteration 73659, loss = 1.27661167\n",
      "Iteration 73660, loss = 1.14380512\n",
      "Iteration 73661, loss = 1.11197467\n",
      "Iteration 73662, loss = 1.05807456\n",
      "Iteration 73663, loss = 1.08784273\n",
      "Iteration 73664, loss = 0.98790592\n",
      "Iteration 73665, loss = 1.13022323\n",
      "Iteration 73666, loss = 1.10274453\n",
      "Iteration 73667, loss = 1.09122696\n",
      "Iteration 73668, loss = 1.26578044\n",
      "Iteration 73669, loss = 1.19604848\n",
      "Iteration 73670, loss = 1.29807950\n",
      "Iteration 73671, loss = 1.19941208\n",
      "Iteration 73672, loss = 1.16263468\n",
      "Iteration 73673, loss = 1.13402023\n",
      "Iteration 73674, loss = 1.09722385\n",
      "Iteration 73675, loss = 1.10874281\n",
      "Iteration 73676, loss = 1.40513067\n",
      "Iteration 73677, loss = 1.07658917\n",
      "Iteration 73678, loss = 1.14370220\n",
      "Iteration 73679, loss = 1.28823501\n",
      "Iteration 73680, loss = 1.31796256\n",
      "Iteration 73681, loss = 1.20927398\n",
      "Iteration 73682, loss = 0.99333752\n",
      "Iteration 73683, loss = 1.00467781\n",
      "Iteration 73684, loss = 1.07195108\n",
      "Iteration 73685, loss = 1.13024685\n",
      "Iteration 73686, loss = 1.13849481\n",
      "Iteration 73687, loss = 1.10394298\n",
      "Iteration 73688, loss = 1.11508306\n",
      "Iteration 73689, loss = 1.20157867\n",
      "Iteration 73690, loss = 1.16309393\n",
      "Iteration 73691, loss = 1.19003318\n",
      "Iteration 73692, loss = 1.36265555\n",
      "Iteration 73693, loss = 1.21477803\n",
      "Iteration 73694, loss = 1.08323306\n",
      "Iteration 73695, loss = 1.12714918\n",
      "Iteration 73696, loss = 1.28729511\n",
      "Iteration 73697, loss = 1.25213810\n",
      "Iteration 73698, loss = 1.44537290\n",
      "Iteration 73699, loss = 1.34205792\n",
      "Iteration 73700, loss = 1.61925172\n",
      "Iteration 73701, loss = 1.41021506\n",
      "Iteration 73702, loss = 1.53316356\n",
      "Iteration 73703, loss = 1.34243960\n",
      "Iteration 73704, loss = 1.15450399\n",
      "Iteration 73705, loss = 1.55889204\n",
      "Iteration 73706, loss = 1.32568105\n",
      "Iteration 73707, loss = 1.37201081\n",
      "Iteration 73708, loss = 1.32860381\n",
      "Iteration 73709, loss = 1.24225998\n",
      "Iteration 73710, loss = 1.21371992\n",
      "Iteration 73711, loss = 1.64997628\n",
      "Iteration 73712, loss = 1.84913004\n",
      "Iteration 73713, loss = 1.67089251\n",
      "Iteration 73714, loss = 1.53729454\n",
      "Iteration 73715, loss = 1.32466544\n",
      "Iteration 73716, loss = 1.24912072\n",
      "Iteration 73717, loss = 1.13056025\n",
      "Iteration 73718, loss = 1.12960077\n",
      "Iteration 73719, loss = 1.04892948\n",
      "Iteration 73720, loss = 0.97266830\n",
      "Iteration 73721, loss = 1.02862527\n",
      "Iteration 73722, loss = 1.08904787\n",
      "Iteration 73723, loss = 1.04844064\n",
      "Iteration 73724, loss = 1.08507168\n",
      "Iteration 73725, loss = 1.08982061\n",
      "Iteration 73726, loss = 1.07006916\n",
      "Iteration 73727, loss = 1.14214810\n",
      "Iteration 73728, loss = 1.36676851\n",
      "Iteration 73729, loss = 1.04443152\n",
      "Iteration 73730, loss = 1.19497911\n",
      "Iteration 73731, loss = 1.16990097\n",
      "Iteration 73732, loss = 1.27346792\n",
      "Iteration 73733, loss = 1.25477071\n",
      "Iteration 73734, loss = 1.16588881\n",
      "Iteration 73735, loss = 1.21642423\n",
      "Iteration 73736, loss = 1.26123537\n",
      "Iteration 73737, loss = 1.33063058\n",
      "Iteration 73738, loss = 1.47137287\n",
      "Iteration 73739, loss = 1.94077907\n",
      "Iteration 73740, loss = 1.86508212\n",
      "Iteration 73741, loss = 1.59712011\n",
      "Iteration 73742, loss = 1.65147279\n",
      "Iteration 73743, loss = 1.29423046\n",
      "Iteration 73744, loss = 1.11201350\n",
      "Iteration 73745, loss = 1.07159233\n",
      "Iteration 73746, loss = 1.16646299\n",
      "Iteration 73747, loss = 1.03388733\n",
      "Iteration 73748, loss = 0.96990398\n",
      "Iteration 73749, loss = 1.01546516\n",
      "Iteration 73750, loss = 1.03125852\n",
      "Iteration 73751, loss = 0.97868598\n",
      "Iteration 73752, loss = 1.07133005\n",
      "Iteration 73753, loss = 1.00570291\n",
      "Iteration 73754, loss = 1.21201924\n",
      "Iteration 73755, loss = 1.12942457\n",
      "Iteration 73756, loss = 1.07117419\n",
      "Iteration 73757, loss = 1.06785028\n",
      "Iteration 73758, loss = 1.05615381\n",
      "Iteration 73759, loss = 0.96903520\n",
      "Iteration 73760, loss = 1.01014896\n",
      "Iteration 73761, loss = 1.06515415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 73762, loss = 1.07921891\n",
      "Iteration 73763, loss = 0.98168383\n",
      "Iteration 73764, loss = 1.01787559\n",
      "Iteration 73765, loss = 0.97185082\n",
      "Iteration 73766, loss = 1.06045795\n",
      "Iteration 73767, loss = 0.92591289\n",
      "Iteration 73768, loss = 1.08407882\n",
      "Iteration 73769, loss = 1.07622265\n",
      "Iteration 73770, loss = 1.22740073\n",
      "Iteration 73771, loss = 0.98001442\n",
      "Iteration 73772, loss = 0.95792742\n",
      "Iteration 73773, loss = 0.95044928\n",
      "Iteration 73774, loss = 1.02339233\n",
      "Iteration 73775, loss = 1.02430742\n",
      "Iteration 73776, loss = 0.98358057\n",
      "Iteration 73777, loss = 1.06016828\n",
      "Iteration 73778, loss = 1.10825515\n",
      "Iteration 73779, loss = 1.20606052\n",
      "Iteration 73780, loss = 1.14124667\n",
      "Iteration 73781, loss = 1.17286714\n",
      "Iteration 73782, loss = 1.11150452\n",
      "Iteration 73783, loss = 1.03408075\n",
      "Iteration 73784, loss = 1.02174077\n",
      "Iteration 73785, loss = 1.01040890\n",
      "Iteration 73786, loss = 1.02455251\n",
      "Iteration 73787, loss = 1.36974345\n",
      "Iteration 73788, loss = 1.45221438\n",
      "Iteration 73789, loss = 1.27614541\n",
      "Iteration 73790, loss = 1.28106276\n",
      "Iteration 73791, loss = 1.17866788\n",
      "Iteration 73792, loss = 1.19010386\n",
      "Iteration 73793, loss = 1.20130595\n",
      "Iteration 73794, loss = 1.23173689\n",
      "Iteration 73795, loss = 1.13679155\n",
      "Iteration 73796, loss = 1.16483670\n",
      "Iteration 73797, loss = 1.30271502\n",
      "Iteration 73798, loss = 1.29152932\n",
      "Iteration 73799, loss = 1.10575631\n",
      "Iteration 73800, loss = 1.04735216\n",
      "Iteration 73801, loss = 0.96930320\n",
      "Iteration 73802, loss = 1.04875737\n",
      "Iteration 73803, loss = 1.08657913\n",
      "Iteration 73804, loss = 1.22228980\n",
      "Iteration 73805, loss = 1.70102626\n",
      "Iteration 73806, loss = 1.31566938\n",
      "Iteration 73807, loss = 1.48998106\n",
      "Iteration 73808, loss = 1.29042785\n",
      "Iteration 73809, loss = 1.82659272\n",
      "Iteration 73810, loss = 2.67190750\n",
      "Iteration 73811, loss = 3.21932290\n",
      "Iteration 73812, loss = 2.49893009\n",
      "Iteration 73813, loss = 1.82339523\n",
      "Iteration 73814, loss = 1.41768220\n",
      "Iteration 73815, loss = 1.34346627\n",
      "Iteration 73816, loss = 1.06121137\n",
      "Iteration 73817, loss = 0.99216851\n",
      "Iteration 73818, loss = 1.08712916\n",
      "Iteration 73819, loss = 1.01947490\n",
      "Iteration 73820, loss = 0.96308296\n",
      "Iteration 73821, loss = 1.14097879\n",
      "Iteration 73822, loss = 1.13492535\n",
      "Iteration 73823, loss = 1.05396531\n",
      "Iteration 73824, loss = 1.04115933\n",
      "Iteration 73825, loss = 0.98500677\n",
      "Iteration 73826, loss = 1.08324600\n",
      "Iteration 73827, loss = 1.38945686\n",
      "Iteration 73828, loss = 1.30484210\n",
      "Iteration 73829, loss = 1.29560453\n",
      "Iteration 73830, loss = 1.06250973\n",
      "Iteration 73831, loss = 1.18152028\n",
      "Iteration 73832, loss = 1.17229381\n",
      "Iteration 73833, loss = 0.99743525\n",
      "Iteration 73834, loss = 1.22399535\n",
      "Iteration 73835, loss = 1.37380047\n",
      "Iteration 73836, loss = 1.79649682\n",
      "Iteration 73837, loss = 1.65916971\n",
      "Iteration 73838, loss = 2.40454867\n",
      "Iteration 73839, loss = 2.38612458\n",
      "Iteration 73840, loss = 3.02896753\n",
      "Iteration 73841, loss = 2.41682589\n",
      "Iteration 73842, loss = 2.74277339\n",
      "Iteration 73843, loss = 2.30441877\n",
      "Iteration 73844, loss = 2.99011464\n",
      "Iteration 73845, loss = 2.38629408\n",
      "Iteration 73846, loss = 2.44977380\n",
      "Iteration 73847, loss = 1.96977719\n",
      "Iteration 73848, loss = 2.12931359\n",
      "Iteration 73849, loss = 2.00939242\n",
      "Iteration 73850, loss = 1.96980965\n",
      "Iteration 73851, loss = 1.41062615\n",
      "Iteration 73852, loss = 1.70585915\n",
      "Iteration 73853, loss = 1.51968793\n",
      "Iteration 73854, loss = 1.35966396\n",
      "Iteration 73855, loss = 1.16699792\n",
      "Iteration 73856, loss = 1.18339442\n",
      "Iteration 73857, loss = 1.04622098\n",
      "Iteration 73858, loss = 1.03163455\n",
      "Iteration 73859, loss = 1.03052771\n",
      "Iteration 73860, loss = 1.04516997\n",
      "Iteration 73861, loss = 1.00390270\n",
      "Iteration 73862, loss = 1.27824421\n",
      "Iteration 73863, loss = 1.27056365\n",
      "Iteration 73864, loss = 1.15832194\n",
      "Iteration 73865, loss = 1.09646597\n",
      "Iteration 73866, loss = 1.09029079\n",
      "Iteration 73867, loss = 1.46006317\n",
      "Iteration 73868, loss = 2.30860534\n",
      "Iteration 73869, loss = 1.63761100\n",
      "Iteration 73870, loss = 1.53856035\n",
      "Iteration 73871, loss = 1.43446258\n",
      "Iteration 73872, loss = 1.31267176\n",
      "Iteration 73873, loss = 1.25673684\n",
      "Iteration 73874, loss = 1.23778291\n",
      "Iteration 73875, loss = 1.03060259\n",
      "Iteration 73876, loss = 1.11340803\n",
      "Iteration 73877, loss = 1.16133110\n",
      "Iteration 73878, loss = 1.17873222\n",
      "Iteration 73879, loss = 1.15390681\n",
      "Iteration 73880, loss = 1.11809317\n",
      "Iteration 73881, loss = 1.14741070\n",
      "Iteration 73882, loss = 1.29847361\n",
      "Iteration 73883, loss = 1.58581167\n",
      "Iteration 73884, loss = 1.89692216\n",
      "Iteration 73885, loss = 1.22157819\n",
      "Iteration 73886, loss = 1.44389811\n",
      "Iteration 73887, loss = 1.72149456\n",
      "Iteration 73888, loss = 1.78474125\n",
      "Iteration 73889, loss = 1.28847277\n",
      "Iteration 73890, loss = 1.15958698\n",
      "Iteration 73891, loss = 1.27128590\n",
      "Iteration 73892, loss = 1.30126451\n",
      "Iteration 73893, loss = 1.11606783\n",
      "Iteration 73894, loss = 1.07407477\n",
      "Iteration 73895, loss = 1.07982462\n",
      "Iteration 73896, loss = 0.98196247\n",
      "Iteration 73897, loss = 1.03866074\n",
      "Iteration 73898, loss = 1.14367134\n",
      "Iteration 73899, loss = 1.12278207\n",
      "Iteration 73900, loss = 1.31608057\n",
      "Iteration 73901, loss = 1.13577577\n",
      "Iteration 73902, loss = 1.15190861\n",
      "Iteration 73903, loss = 1.14852291\n",
      "Iteration 73904, loss = 1.17182165\n",
      "Iteration 73905, loss = 1.12280533\n",
      "Iteration 73906, loss = 1.10950486\n",
      "Iteration 73907, loss = 1.18673022\n",
      "Iteration 73908, loss = 1.44112629\n",
      "Iteration 73909, loss = 1.04878128\n",
      "Iteration 73910, loss = 1.44884866\n",
      "Iteration 73911, loss = 1.92761342\n",
      "Iteration 73912, loss = 2.28805438\n",
      "Iteration 73913, loss = 1.96484970\n",
      "Iteration 73914, loss = 1.79230135\n",
      "Iteration 73915, loss = 1.27326775\n",
      "Iteration 73916, loss = 1.35933864\n",
      "Iteration 73917, loss = 1.27606041\n",
      "Iteration 73918, loss = 1.14586797\n",
      "Iteration 73919, loss = 1.17562125\n",
      "Iteration 73920, loss = 1.06766645\n",
      "Iteration 73921, loss = 1.19509319\n",
      "Iteration 73922, loss = 1.14854791\n",
      "Iteration 73923, loss = 1.13921785\n",
      "Iteration 73924, loss = 1.08417966\n",
      "Iteration 73925, loss = 1.10530076\n",
      "Iteration 73926, loss = 0.97595037\n",
      "Iteration 73927, loss = 1.02757947\n",
      "Iteration 73928, loss = 1.09352094\n",
      "Iteration 73929, loss = 1.39306710\n",
      "Iteration 73930, loss = 1.43415329\n",
      "Iteration 73931, loss = 1.44246055\n",
      "Iteration 73932, loss = 1.64201179\n",
      "Iteration 73933, loss = 1.32036218\n",
      "Iteration 73934, loss = 1.35070734\n",
      "Iteration 73935, loss = 1.59934258\n",
      "Iteration 73936, loss = 1.51838663\n",
      "Iteration 73937, loss = 1.34922363\n",
      "Iteration 73938, loss = 1.34863766\n",
      "Iteration 73939, loss = 1.11251021\n",
      "Iteration 73940, loss = 1.15357228\n",
      "Iteration 73941, loss = 1.09532347\n",
      "Iteration 73942, loss = 1.11188013\n",
      "Iteration 73943, loss = 1.08785164\n",
      "Iteration 73944, loss = 1.02039782\n",
      "Iteration 73945, loss = 1.09704631\n",
      "Iteration 73946, loss = 1.16203592\n",
      "Iteration 73947, loss = 1.22702999\n",
      "Iteration 73948, loss = 1.25418437\n",
      "Iteration 73949, loss = 1.22886191\n",
      "Iteration 73950, loss = 1.24381972\n",
      "Iteration 73951, loss = 1.75845274\n",
      "Iteration 73952, loss = 2.07845952\n",
      "Iteration 73953, loss = 1.96519805\n",
      "Iteration 73954, loss = 1.86172699\n",
      "Iteration 73955, loss = 1.56998312\n",
      "Iteration 73956, loss = 1.39314433\n",
      "Iteration 73957, loss = 1.13575801\n",
      "Iteration 73958, loss = 1.01733540\n",
      "Iteration 73959, loss = 1.17627420\n",
      "Iteration 73960, loss = 1.28795286\n",
      "Iteration 73961, loss = 1.30299959\n",
      "Iteration 73962, loss = 1.14792990\n",
      "Iteration 73963, loss = 1.11242981\n",
      "Iteration 73964, loss = 1.20669966\n",
      "Iteration 73965, loss = 1.22204942\n",
      "Iteration 73966, loss = 1.18518158\n",
      "Iteration 73967, loss = 1.15843030\n",
      "Iteration 73968, loss = 1.04390103\n",
      "Iteration 73969, loss = 1.05329270\n",
      "Iteration 73970, loss = 1.12971922\n",
      "Iteration 73971, loss = 1.21866924\n",
      "Iteration 73972, loss = 1.57618065\n",
      "Iteration 73973, loss = 1.30842356\n",
      "Iteration 73974, loss = 1.20070639\n",
      "Iteration 73975, loss = 1.24894573\n",
      "Iteration 73976, loss = 1.20439531\n",
      "Iteration 73977, loss = 1.19077574\n",
      "Iteration 73978, loss = 1.12884636\n",
      "Iteration 73979, loss = 1.18176585\n",
      "Iteration 73980, loss = 1.58004265\n",
      "Iteration 73981, loss = 1.20637917\n",
      "Iteration 73982, loss = 1.07075155\n",
      "Iteration 73983, loss = 1.19242303\n",
      "Iteration 73984, loss = 1.12752237\n",
      "Iteration 73985, loss = 1.17155057\n",
      "Iteration 73986, loss = 1.16103583\n",
      "Iteration 73987, loss = 1.56060349\n",
      "Iteration 73988, loss = 1.15089523\n",
      "Iteration 73989, loss = 1.17864820\n",
      "Iteration 73990, loss = 1.24574394\n",
      "Iteration 73991, loss = 1.45253898\n",
      "Iteration 73992, loss = 1.40071286\n",
      "Iteration 73993, loss = 1.29253242\n",
      "Iteration 73994, loss = 1.35576624\n",
      "Iteration 73995, loss = 1.17039960\n",
      "Iteration 73996, loss = 1.04889915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 73997, loss = 1.07472806\n",
      "Iteration 73998, loss = 1.14630143\n",
      "Iteration 73999, loss = 1.05605945\n",
      "Iteration 74000, loss = 1.00512395\n",
      "Iteration 74001, loss = 1.12569300\n",
      "Iteration 74002, loss = 1.02542306\n",
      "Iteration 74003, loss = 1.31387443\n",
      "Iteration 74004, loss = 1.29261463\n",
      "Iteration 74005, loss = 1.39525042\n",
      "Iteration 74006, loss = 1.63560458\n",
      "Iteration 74007, loss = 1.75578763\n",
      "Iteration 74008, loss = 1.62924026\n",
      "Iteration 74009, loss = 1.45790854\n",
      "Iteration 74010, loss = 1.23503661\n",
      "Iteration 74011, loss = 1.18189339\n",
      "Iteration 74012, loss = 1.39116010\n",
      "Iteration 74013, loss = 1.09154433\n",
      "Iteration 74014, loss = 1.17848365\n",
      "Iteration 74015, loss = 1.17660861\n",
      "Iteration 74016, loss = 1.00377863\n",
      "Iteration 74017, loss = 0.98174172\n",
      "Iteration 74018, loss = 1.00351061\n",
      "Iteration 74019, loss = 1.11657383\n",
      "Iteration 74020, loss = 1.18998001\n",
      "Iteration 74021, loss = 1.11456123\n",
      "Iteration 74022, loss = 1.30638874\n",
      "Iteration 74023, loss = 1.43954169\n",
      "Iteration 74024, loss = 1.61358925\n",
      "Iteration 74025, loss = 1.98107735\n",
      "Iteration 74026, loss = 1.70113588\n",
      "Iteration 74027, loss = 1.69368861\n",
      "Iteration 74028, loss = 1.17471233\n",
      "Iteration 74029, loss = 1.04742286\n",
      "Iteration 74030, loss = 1.20022828\n",
      "Iteration 74031, loss = 1.24028579\n",
      "Iteration 74032, loss = 1.57135563\n",
      "Iteration 74033, loss = 1.57105562\n",
      "Iteration 74034, loss = 1.36807935\n",
      "Iteration 74035, loss = 1.13256067\n",
      "Iteration 74036, loss = 1.20887455\n",
      "Iteration 74037, loss = 1.07926554\n",
      "Iteration 74038, loss = 1.11980392\n",
      "Iteration 74039, loss = 1.09605272\n",
      "Iteration 74040, loss = 1.02780794\n",
      "Iteration 74041, loss = 1.07902117\n",
      "Iteration 74042, loss = 1.12540878\n",
      "Iteration 74043, loss = 1.22442997\n",
      "Iteration 74044, loss = 1.16081069\n",
      "Iteration 74045, loss = 1.15726831\n",
      "Iteration 74046, loss = 1.15894479\n",
      "Iteration 74047, loss = 1.11952432\n",
      "Iteration 74048, loss = 1.07372657\n",
      "Iteration 74049, loss = 1.29849748\n",
      "Iteration 74050, loss = 1.20702599\n",
      "Iteration 74051, loss = 1.59239270\n",
      "Iteration 74052, loss = 1.52095947\n",
      "Iteration 74053, loss = 1.54376134\n",
      "Iteration 74054, loss = 1.47361601\n",
      "Iteration 74055, loss = 1.14672287\n",
      "Iteration 74056, loss = 1.13655369\n",
      "Iteration 74057, loss = 1.05295391\n",
      "Iteration 74058, loss = 1.15053326\n",
      "Iteration 74059, loss = 1.09720725\n",
      "Iteration 74060, loss = 1.06682975\n",
      "Iteration 74061, loss = 1.11628997\n",
      "Iteration 74062, loss = 0.94423087\n",
      "Iteration 74063, loss = 1.10723137\n",
      "Iteration 74064, loss = 1.16519057\n",
      "Iteration 74065, loss = 1.03299847\n",
      "Iteration 74066, loss = 1.07022811\n",
      "Iteration 74067, loss = 1.27893039\n",
      "Iteration 74068, loss = 1.31410467\n",
      "Iteration 74069, loss = 1.21159366\n",
      "Iteration 74070, loss = 1.03879119\n",
      "Iteration 74071, loss = 1.08401899\n",
      "Iteration 74072, loss = 1.17447241\n",
      "Iteration 74073, loss = 1.19941942\n",
      "Iteration 74074, loss = 1.16393325\n",
      "Iteration 74075, loss = 1.40533754\n",
      "Iteration 74076, loss = 1.30516788\n",
      "Iteration 74077, loss = 1.08906388\n",
      "Iteration 74078, loss = 1.07827935\n",
      "Iteration 74079, loss = 1.15693157\n",
      "Iteration 74080, loss = 1.24289488\n",
      "Iteration 74081, loss = 1.41534045\n",
      "Iteration 74082, loss = 1.11778718\n",
      "Iteration 74083, loss = 1.17878874\n",
      "Iteration 74084, loss = 0.98489778\n",
      "Iteration 74085, loss = 0.99705101\n",
      "Iteration 74086, loss = 1.00800616\n",
      "Iteration 74087, loss = 1.02846298\n",
      "Iteration 74088, loss = 1.03296457\n",
      "Iteration 74089, loss = 1.11679054\n",
      "Iteration 74090, loss = 1.26951977\n",
      "Iteration 74091, loss = 1.03072178\n",
      "Iteration 74092, loss = 1.05456259\n",
      "Iteration 74093, loss = 1.08744922\n",
      "Iteration 74094, loss = 1.13621202\n",
      "Iteration 74095, loss = 1.17027928\n",
      "Iteration 74096, loss = 1.42908518\n",
      "Iteration 74097, loss = 1.58391056\n",
      "Iteration 74098, loss = 1.32954882\n",
      "Iteration 74099, loss = 1.23897313\n",
      "Iteration 74100, loss = 1.21283612\n",
      "Iteration 74101, loss = 1.59317562\n",
      "Iteration 74102, loss = 1.35593975\n",
      "Iteration 74103, loss = 1.12731113\n",
      "Iteration 74104, loss = 0.97858501\n",
      "Iteration 74105, loss = 1.06292664\n",
      "Iteration 74106, loss = 0.99728719\n",
      "Iteration 74107, loss = 1.08535291\n",
      "Iteration 74108, loss = 1.27838796\n",
      "Iteration 74109, loss = 1.25211396\n",
      "Iteration 74110, loss = 1.24292921\n",
      "Iteration 74111, loss = 1.52352215\n",
      "Iteration 74112, loss = 2.07522337\n",
      "Iteration 74113, loss = 1.46858585\n",
      "Iteration 74114, loss = 1.43295540\n",
      "Iteration 74115, loss = 1.29603293\n",
      "Iteration 74116, loss = 1.20155466\n",
      "Iteration 74117, loss = 0.97797846\n",
      "Iteration 74118, loss = 1.04347385\n",
      "Iteration 74119, loss = 1.16864858\n",
      "Iteration 74120, loss = 1.06265495\n",
      "Iteration 74121, loss = 1.01806678\n",
      "Iteration 74122, loss = 0.96899889\n",
      "Iteration 74123, loss = 1.02862755\n",
      "Iteration 74124, loss = 1.04730053\n",
      "Iteration 74125, loss = 1.13182387\n",
      "Iteration 74126, loss = 1.08504075\n",
      "Iteration 74127, loss = 1.26711484\n",
      "Iteration 74128, loss = 1.33208123\n",
      "Iteration 74129, loss = 1.43999286\n",
      "Iteration 74130, loss = 1.65600675\n",
      "Iteration 74131, loss = 1.59978581\n",
      "Iteration 74132, loss = 1.55642409\n",
      "Iteration 74133, loss = 2.28436884\n",
      "Iteration 74134, loss = 2.02220147\n",
      "Iteration 74135, loss = 2.04137694\n",
      "Iteration 74136, loss = 2.22347952\n",
      "Iteration 74137, loss = 1.68602177\n",
      "Iteration 74138, loss = 1.45169636\n",
      "Iteration 74139, loss = 1.79367125\n",
      "Iteration 74140, loss = 1.23516940\n",
      "Iteration 74141, loss = 1.38745496\n",
      "Iteration 74142, loss = 1.28983050\n",
      "Iteration 74143, loss = 1.19484201\n",
      "Iteration 74144, loss = 1.27346907\n",
      "Iteration 74145, loss = 1.33242453\n",
      "Iteration 74146, loss = 1.20554305\n",
      "Iteration 74147, loss = 1.34356777\n",
      "Iteration 74148, loss = 1.18141003\n",
      "Iteration 74149, loss = 1.28775007\n",
      "Iteration 74150, loss = 1.67257656\n",
      "Iteration 74151, loss = 1.90447571\n",
      "Iteration 74152, loss = 1.61995240\n",
      "Iteration 74153, loss = 1.59448553\n",
      "Iteration 74154, loss = 1.47038291\n",
      "Iteration 74155, loss = 1.16546427\n",
      "Iteration 74156, loss = 1.14096552\n",
      "Iteration 74157, loss = 1.04329993\n",
      "Iteration 74158, loss = 1.15439843\n",
      "Iteration 74159, loss = 1.08104401\n",
      "Iteration 74160, loss = 1.31074301\n",
      "Iteration 74161, loss = 1.46630944\n",
      "Iteration 74162, loss = 1.35859233\n",
      "Iteration 74163, loss = 1.99158498\n",
      "Iteration 74164, loss = 2.27070766\n",
      "Iteration 74165, loss = 2.17971077\n",
      "Iteration 74166, loss = 1.51226819\n",
      "Iteration 74167, loss = 1.17054459\n",
      "Iteration 74168, loss = 1.11013803\n",
      "Iteration 74169, loss = 1.02869767\n",
      "Iteration 74170, loss = 1.05714142\n",
      "Iteration 74171, loss = 1.04384282\n",
      "Iteration 74172, loss = 1.15396850\n",
      "Iteration 74173, loss = 1.06501976\n",
      "Iteration 74174, loss = 0.98369008\n",
      "Iteration 74175, loss = 1.03032867\n",
      "Iteration 74176, loss = 1.14581124\n",
      "Iteration 74177, loss = 1.22248043\n",
      "Iteration 74178, loss = 1.14234012\n",
      "Iteration 74179, loss = 1.10006877\n",
      "Iteration 74180, loss = 1.13200200\n",
      "Iteration 74181, loss = 1.66003365\n",
      "Iteration 74182, loss = 1.48183585\n",
      "Iteration 74183, loss = 1.44427452\n",
      "Iteration 74184, loss = 1.37477033\n",
      "Iteration 74185, loss = 1.23676352\n",
      "Iteration 74186, loss = 1.27472446\n",
      "Iteration 74187, loss = 1.68619067\n",
      "Iteration 74188, loss = 1.96505825\n",
      "Iteration 74189, loss = 1.58777391\n",
      "Iteration 74190, loss = 1.75255918\n",
      "Iteration 74191, loss = 1.66209076\n",
      "Iteration 74192, loss = 1.39494595\n",
      "Iteration 74193, loss = 1.53526460\n",
      "Iteration 74194, loss = 1.47704267\n",
      "Iteration 74195, loss = 1.40509216\n",
      "Iteration 74196, loss = 1.17482714\n",
      "Iteration 74197, loss = 1.08855423\n",
      "Iteration 74198, loss = 1.12106612\n",
      "Iteration 74199, loss = 1.05683920\n",
      "Iteration 74200, loss = 1.09076655\n",
      "Iteration 74201, loss = 1.17339989\n",
      "Iteration 74202, loss = 1.22295446\n",
      "Iteration 74203, loss = 1.16175890\n",
      "Iteration 74204, loss = 1.28227424\n",
      "Iteration 74205, loss = 1.34636233\n",
      "Iteration 74206, loss = 1.10615313\n",
      "Iteration 74207, loss = 0.97540335\n",
      "Iteration 74208, loss = 1.14308314\n",
      "Iteration 74209, loss = 1.08962192\n",
      "Iteration 74210, loss = 1.00713776\n",
      "Iteration 74211, loss = 1.05817899\n",
      "Iteration 74212, loss = 0.99013716\n",
      "Iteration 74213, loss = 0.99409240\n",
      "Iteration 74214, loss = 1.01132665\n",
      "Iteration 74215, loss = 1.06504377\n",
      "Iteration 74216, loss = 0.94599655\n",
      "Iteration 74217, loss = 0.96827488\n",
      "Iteration 74218, loss = 1.07111137\n",
      "Iteration 74219, loss = 1.01969972\n",
      "Iteration 74220, loss = 1.03311535\n",
      "Iteration 74221, loss = 1.17372033\n",
      "Iteration 74222, loss = 1.06464250\n",
      "Iteration 74223, loss = 1.13732006\n",
      "Iteration 74224, loss = 1.45576723\n",
      "Iteration 74225, loss = 1.63951421\n",
      "Iteration 74226, loss = 1.63750295\n",
      "Iteration 74227, loss = 1.88424701\n",
      "Iteration 74228, loss = 1.99991384\n",
      "Iteration 74229, loss = 1.32253881\n",
      "Iteration 74230, loss = 1.29456711\n",
      "Iteration 74231, loss = 1.49095295\n",
      "Iteration 74232, loss = 1.22766861\n",
      "Iteration 74233, loss = 1.02169846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 74234, loss = 1.15160376\n",
      "Iteration 74235, loss = 1.15435703\n",
      "Iteration 74236, loss = 1.15503684\n",
      "Iteration 74237, loss = 1.09195527\n",
      "Iteration 74238, loss = 1.05050209\n",
      "Iteration 74239, loss = 1.05907455\n",
      "Iteration 74240, loss = 1.16694011\n",
      "Iteration 74241, loss = 1.05986503\n",
      "Iteration 74242, loss = 1.02252115\n",
      "Iteration 74243, loss = 1.13627495\n",
      "Iteration 74244, loss = 1.16984188\n",
      "Iteration 74245, loss = 1.17108557\n",
      "Iteration 74246, loss = 1.11160743\n",
      "Iteration 74247, loss = 0.93560508\n",
      "Iteration 74248, loss = 1.06935054\n",
      "Iteration 74249, loss = 1.10479426\n",
      "Iteration 74250, loss = 1.16119281\n",
      "Iteration 74251, loss = 1.02529121\n",
      "Iteration 74252, loss = 1.13090438\n",
      "Iteration 74253, loss = 1.15177870\n",
      "Iteration 74254, loss = 1.09321590\n",
      "Iteration 74255, loss = 1.07196943\n",
      "Iteration 74256, loss = 1.03563856\n",
      "Iteration 74257, loss = 1.12474762\n",
      "Iteration 74258, loss = 1.05167262\n",
      "Iteration 74259, loss = 1.09190048\n",
      "Iteration 74260, loss = 1.13225651\n",
      "Iteration 74261, loss = 1.13152699\n",
      "Iteration 74262, loss = 1.45335752\n",
      "Iteration 74263, loss = 1.70800148\n",
      "Iteration 74264, loss = 1.14954925\n",
      "Iteration 74265, loss = 1.18535760\n",
      "Iteration 74266, loss = 1.13013227\n",
      "Iteration 74267, loss = 1.37791026\n",
      "Iteration 74268, loss = 1.74858936\n",
      "Iteration 74269, loss = 1.58154266\n",
      "Iteration 74270, loss = 1.37531127\n",
      "Iteration 74271, loss = 1.42690487\n",
      "Iteration 74272, loss = 1.40817181\n",
      "Iteration 74273, loss = 1.28591801\n",
      "Iteration 74274, loss = 1.37382774\n",
      "Iteration 74275, loss = 1.28547202\n",
      "Iteration 74276, loss = 1.16794261\n",
      "Iteration 74277, loss = 1.19695015\n",
      "Iteration 74278, loss = 1.47122031\n",
      "Iteration 74279, loss = 1.35211254\n",
      "Iteration 74280, loss = 1.51366881\n",
      "Iteration 74281, loss = 1.63599262\n",
      "Iteration 74282, loss = 1.43271385\n",
      "Iteration 74283, loss = 1.32837191\n",
      "Iteration 74284, loss = 1.20975858\n",
      "Iteration 74285, loss = 1.23260348\n",
      "Iteration 74286, loss = 1.05837122\n",
      "Iteration 74287, loss = 1.09620227\n",
      "Iteration 74288, loss = 1.19235896\n",
      "Iteration 74289, loss = 1.27957376\n",
      "Iteration 74290, loss = 1.34252211\n",
      "Iteration 74291, loss = 1.94340883\n",
      "Iteration 74292, loss = 1.70173201\n",
      "Iteration 74293, loss = 1.75170419\n",
      "Iteration 74294, loss = 1.63398698\n",
      "Iteration 74295, loss = 1.37633543\n",
      "Iteration 74296, loss = 1.18426296\n",
      "Iteration 74297, loss = 1.11695624\n",
      "Iteration 74298, loss = 1.21307350\n",
      "Iteration 74299, loss = 1.16729119\n",
      "Iteration 74300, loss = 1.44864635\n",
      "Iteration 74301, loss = 1.43823566\n",
      "Iteration 74302, loss = 1.61449420\n",
      "Iteration 74303, loss = 1.37741565\n",
      "Iteration 74304, loss = 1.29223688\n",
      "Iteration 74305, loss = 1.14413918\n",
      "Iteration 74306, loss = 1.51086816\n",
      "Iteration 74307, loss = 1.50785156\n",
      "Iteration 74308, loss = 1.22899572\n",
      "Iteration 74309, loss = 1.40285302\n",
      "Iteration 74310, loss = 1.42321474\n",
      "Iteration 74311, loss = 1.38010546\n",
      "Iteration 74312, loss = 1.54231534\n",
      "Iteration 74313, loss = 1.54579845\n",
      "Iteration 74314, loss = 1.23038240\n",
      "Iteration 74315, loss = 1.11965375\n",
      "Iteration 74316, loss = 1.05247002\n",
      "Iteration 74317, loss = 1.17875620\n",
      "Iteration 74318, loss = 1.45924687\n",
      "Iteration 74319, loss = 1.72542904\n",
      "Iteration 74320, loss = 2.30472576\n",
      "Iteration 74321, loss = 2.29233802\n",
      "Iteration 74322, loss = 2.34961208\n",
      "Iteration 74323, loss = 1.56152080\n",
      "Iteration 74324, loss = 1.88441307\n",
      "Iteration 74325, loss = 1.69884196\n",
      "Iteration 74326, loss = 1.40662845\n",
      "Iteration 74327, loss = 1.55304043\n",
      "Iteration 74328, loss = 1.33829138\n",
      "Iteration 74329, loss = 1.38429408\n",
      "Iteration 74330, loss = 1.30818215\n",
      "Iteration 74331, loss = 1.17470748\n",
      "Iteration 74332, loss = 1.08379662\n",
      "Iteration 74333, loss = 1.16710790\n",
      "Iteration 74334, loss = 1.32810148\n",
      "Iteration 74335, loss = 1.20392060\n",
      "Iteration 74336, loss = 1.40698163\n",
      "Iteration 74337, loss = 1.19907863\n",
      "Iteration 74338, loss = 1.20924634\n",
      "Iteration 74339, loss = 0.97597000\n",
      "Iteration 74340, loss = 1.12534388\n",
      "Iteration 74341, loss = 0.98894963\n",
      "Iteration 74342, loss = 1.14378260\n",
      "Iteration 74343, loss = 1.10086488\n",
      "Iteration 74344, loss = 1.07558542\n",
      "Iteration 74345, loss = 1.07112298\n",
      "Iteration 74346, loss = 1.05160313\n",
      "Iteration 74347, loss = 1.09536021\n",
      "Iteration 74348, loss = 1.01684816\n",
      "Iteration 74349, loss = 1.09198027\n",
      "Iteration 74350, loss = 1.16230872\n",
      "Iteration 74351, loss = 1.25786610\n",
      "Iteration 74352, loss = 1.31167259\n",
      "Iteration 74353, loss = 1.07132912\n",
      "Iteration 74354, loss = 0.97726151\n",
      "Iteration 74355, loss = 1.19261565\n",
      "Iteration 74356, loss = 1.05861833\n",
      "Iteration 74357, loss = 1.07200084\n",
      "Iteration 74358, loss = 1.12369628\n",
      "Iteration 74359, loss = 1.21549690\n",
      "Iteration 74360, loss = 1.32789881\n",
      "Iteration 74361, loss = 1.19691025\n",
      "Iteration 74362, loss = 1.19658033\n",
      "Iteration 74363, loss = 1.46241263\n",
      "Iteration 74364, loss = 1.51904104\n",
      "Iteration 74365, loss = 1.74692348\n",
      "Iteration 74366, loss = 1.54668575\n",
      "Iteration 74367, loss = 1.38713169\n",
      "Iteration 74368, loss = 1.07198473\n",
      "Iteration 74369, loss = 1.14374677\n",
      "Iteration 74370, loss = 1.08837966\n",
      "Iteration 74371, loss = 1.08807097\n",
      "Iteration 74372, loss = 1.05201170\n",
      "Iteration 74373, loss = 1.19944159\n",
      "Iteration 74374, loss = 1.09781225\n",
      "Iteration 74375, loss = 1.03210528\n",
      "Iteration 74376, loss = 1.05724143\n",
      "Iteration 74377, loss = 1.10211928\n",
      "Iteration 74378, loss = 1.22047807\n",
      "Iteration 74379, loss = 1.20797711\n",
      "Iteration 74380, loss = 1.20410845\n",
      "Iteration 74381, loss = 1.28989297\n",
      "Iteration 74382, loss = 1.92722861\n",
      "Iteration 74383, loss = 1.87604920\n",
      "Iteration 74384, loss = 1.77679664\n",
      "Iteration 74385, loss = 1.68729488\n",
      "Iteration 74386, loss = 1.26688522\n",
      "Iteration 74387, loss = 1.40267739\n",
      "Iteration 74388, loss = 1.37355950\n",
      "Iteration 74389, loss = 1.52676833\n",
      "Iteration 74390, loss = 1.55627080\n",
      "Iteration 74391, loss = 1.30225325\n",
      "Iteration 74392, loss = 1.59270369\n",
      "Iteration 74393, loss = 1.53469563\n",
      "Iteration 74394, loss = 1.33148841\n",
      "Iteration 74395, loss = 1.17810961\n",
      "Iteration 74396, loss = 1.03154329\n",
      "Iteration 74397, loss = 1.08673458\n",
      "Iteration 74398, loss = 1.12015103\n",
      "Iteration 74399, loss = 1.08617111\n",
      "Iteration 74400, loss = 1.23933965\n",
      "Iteration 74401, loss = 1.19616789\n",
      "Iteration 74402, loss = 1.08199823\n",
      "Iteration 74403, loss = 1.01757648\n",
      "Iteration 74404, loss = 0.99119317\n",
      "Iteration 74405, loss = 1.14140110\n",
      "Iteration 74406, loss = 1.31815361\n",
      "Iteration 74407, loss = 1.79633716\n",
      "Iteration 74408, loss = 1.92152522\n",
      "Iteration 74409, loss = 1.69813927\n",
      "Iteration 74410, loss = 1.16732651\n",
      "Iteration 74411, loss = 1.51221183\n",
      "Iteration 74412, loss = 1.20111762\n",
      "Iteration 74413, loss = 1.23155207\n",
      "Iteration 74414, loss = 1.19523004\n",
      "Iteration 74415, loss = 1.09381246\n",
      "Iteration 74416, loss = 1.16746331\n",
      "Iteration 74417, loss = 1.35980813\n",
      "Iteration 74418, loss = 1.43469784\n",
      "Iteration 74419, loss = 1.68015052\n",
      "Iteration 74420, loss = 1.49504682\n",
      "Iteration 74421, loss = 1.70573286\n",
      "Iteration 74422, loss = 1.91839674\n",
      "Iteration 74423, loss = 1.69890915\n",
      "Iteration 74424, loss = 1.67111458\n",
      "Iteration 74425, loss = 1.86426944\n",
      "Iteration 74426, loss = 1.66559434\n",
      "Iteration 74427, loss = 1.36339931\n",
      "Iteration 74428, loss = 1.46415531\n",
      "Iteration 74429, loss = 1.56748734\n",
      "Iteration 74430, loss = 2.28315636\n",
      "Iteration 74431, loss = 2.61538394\n",
      "Iteration 74432, loss = 2.37081259\n",
      "Iteration 74433, loss = 1.77159726\n",
      "Iteration 74434, loss = 1.58538589\n",
      "Iteration 74435, loss = 1.45806767\n",
      "Iteration 74436, loss = 1.67815899\n",
      "Iteration 74437, loss = 1.28693344\n",
      "Iteration 74438, loss = 1.14810062\n",
      "Iteration 74439, loss = 1.03761513\n",
      "Iteration 74440, loss = 1.02125146\n",
      "Iteration 74441, loss = 1.14348157\n",
      "Iteration 74442, loss = 1.05088180\n",
      "Iteration 74443, loss = 1.13016695\n",
      "Iteration 74444, loss = 1.08980798\n",
      "Iteration 74445, loss = 1.18853170\n",
      "Iteration 74446, loss = 1.01671611\n",
      "Iteration 74447, loss = 1.26049901\n",
      "Iteration 74448, loss = 1.38434078\n",
      "Iteration 74449, loss = 1.31997334\n",
      "Iteration 74450, loss = 1.10796543\n",
      "Iteration 74451, loss = 1.11763418\n",
      "Iteration 74452, loss = 1.08484400\n",
      "Iteration 74453, loss = 1.00942746\n",
      "Iteration 74454, loss = 1.22886842\n",
      "Iteration 74455, loss = 1.32379714\n",
      "Iteration 74456, loss = 1.35287271\n",
      "Iteration 74457, loss = 1.44481789\n",
      "Iteration 74458, loss = 1.24264391\n",
      "Iteration 74459, loss = 1.18980631\n",
      "Iteration 74460, loss = 1.31425934\n",
      "Iteration 74461, loss = 1.22884341\n",
      "Iteration 74462, loss = 1.13561686\n",
      "Iteration 74463, loss = 0.97855640\n",
      "Iteration 74464, loss = 1.02431949\n",
      "Iteration 74465, loss = 1.19310616\n",
      "Iteration 74466, loss = 1.02494932\n",
      "Iteration 74467, loss = 1.07480127\n",
      "Iteration 74468, loss = 1.07522759\n",
      "Iteration 74469, loss = 1.05586695\n",
      "Iteration 74470, loss = 0.94588067\n",
      "Iteration 74471, loss = 0.96936986\n",
      "Iteration 74472, loss = 0.94900216\n",
      "Iteration 74473, loss = 0.94017492\n",
      "Iteration 74474, loss = 0.96649481\n",
      "Iteration 74475, loss = 0.96245377\n",
      "Iteration 74476, loss = 1.02685820\n",
      "Iteration 74477, loss = 1.09274930\n",
      "Iteration 74478, loss = 1.07177527\n",
      "Iteration 74479, loss = 1.21390279\n",
      "Iteration 74480, loss = 1.28604739\n",
      "Iteration 74481, loss = 1.43474601\n",
      "Iteration 74482, loss = 1.31030098\n",
      "Iteration 74483, loss = 1.19855707\n",
      "Iteration 74484, loss = 1.32052653\n",
      "Iteration 74485, loss = 1.32045195\n",
      "Iteration 74486, loss = 1.17842615\n",
      "Iteration 74487, loss = 1.07590569\n",
      "Iteration 74488, loss = 1.03394061\n",
      "Iteration 74489, loss = 1.05913876\n",
      "Iteration 74490, loss = 1.15832171\n",
      "Iteration 74491, loss = 1.09201270\n",
      "Iteration 74492, loss = 1.13138527\n",
      "Iteration 74493, loss = 1.02425306\n",
      "Iteration 74494, loss = 1.07301225\n",
      "Iteration 74495, loss = 1.08511868\n",
      "Iteration 74496, loss = 1.10894354\n",
      "Iteration 74497, loss = 1.18125745\n",
      "Iteration 74498, loss = 1.22811875\n",
      "Iteration 74499, loss = 1.09762280\n",
      "Iteration 74500, loss = 1.46980016\n",
      "Iteration 74501, loss = 1.47187724\n",
      "Iteration 74502, loss = 1.23013139\n",
      "Iteration 74503, loss = 1.14756698\n",
      "Iteration 74504, loss = 1.16252799\n",
      "Iteration 74505, loss = 1.34630805\n",
      "Iteration 74506, loss = 1.78595246\n",
      "Iteration 74507, loss = 1.96442116\n",
      "Iteration 74508, loss = 1.81650670\n",
      "Iteration 74509, loss = 1.39897726\n",
      "Iteration 74510, loss = 1.42838267\n",
      "Iteration 74511, loss = 1.19081537\n",
      "Iteration 74512, loss = 1.16624744\n",
      "Iteration 74513, loss = 1.15424478\n",
      "Iteration 74514, loss = 1.19896076\n",
      "Iteration 74515, loss = 1.24604046\n",
      "Iteration 74516, loss = 1.37049240\n",
      "Iteration 74517, loss = 1.39043482\n",
      "Iteration 74518, loss = 1.16962181\n",
      "Iteration 74519, loss = 1.15633667\n",
      "Iteration 74520, loss = 1.01440085\n",
      "Iteration 74521, loss = 1.14250607\n",
      "Iteration 74522, loss = 1.74243380\n",
      "Iteration 74523, loss = 1.53347754\n",
      "Iteration 74524, loss = 2.32733518\n",
      "Iteration 74525, loss = 1.70060177\n",
      "Iteration 74526, loss = 2.27185172\n",
      "Iteration 74527, loss = 1.90371338\n",
      "Iteration 74528, loss = 1.67623582\n",
      "Iteration 74529, loss = 1.44823630\n",
      "Iteration 74530, loss = 1.56197571\n",
      "Iteration 74531, loss = 1.75297103\n",
      "Iteration 74532, loss = 1.43246466\n",
      "Iteration 74533, loss = 1.20835154\n",
      "Iteration 74534, loss = 1.04251288\n",
      "Iteration 74535, loss = 1.09591237\n",
      "Iteration 74536, loss = 1.10541519\n",
      "Iteration 74537, loss = 1.16716685\n",
      "Iteration 74538, loss = 1.04312139\n",
      "Iteration 74539, loss = 1.16135279\n",
      "Iteration 74540, loss = 1.17159919\n",
      "Iteration 74541, loss = 1.07809301\n",
      "Iteration 74542, loss = 1.36471174\n",
      "Iteration 74543, loss = 1.62262756\n",
      "Iteration 74544, loss = 1.48467351\n",
      "Iteration 74545, loss = 1.38195294\n",
      "Iteration 74546, loss = 1.50640710\n",
      "Iteration 74547, loss = 1.65877339\n",
      "Iteration 74548, loss = 2.24818229\n",
      "Iteration 74549, loss = 2.16866611\n",
      "Iteration 74550, loss = 1.88768096\n",
      "Iteration 74551, loss = 1.89087648\n",
      "Iteration 74552, loss = 1.98467397\n",
      "Iteration 74553, loss = 1.67138686\n",
      "Iteration 74554, loss = 1.58624831\n",
      "Iteration 74555, loss = 1.36053588\n",
      "Iteration 74556, loss = 1.61828257\n",
      "Iteration 74557, loss = 2.35956165\n",
      "Iteration 74558, loss = 2.74579051\n",
      "Iteration 74559, loss = 2.01050708\n",
      "Iteration 74560, loss = 1.53668083\n",
      "Iteration 74561, loss = 1.51802114\n",
      "Iteration 74562, loss = 1.64453244\n",
      "Iteration 74563, loss = 1.60375282\n",
      "Iteration 74564, loss = 1.53643991\n",
      "Iteration 74565, loss = 1.28308349\n",
      "Iteration 74566, loss = 1.22263333\n",
      "Iteration 74567, loss = 1.26752967\n",
      "Iteration 74568, loss = 1.22623805\n",
      "Iteration 74569, loss = 1.10631351\n",
      "Iteration 74570, loss = 1.02171612\n",
      "Iteration 74571, loss = 1.00143469\n",
      "Iteration 74572, loss = 0.95249111\n",
      "Iteration 74573, loss = 1.00928102\n",
      "Iteration 74574, loss = 1.10515223\n",
      "Iteration 74575, loss = 0.99980425\n",
      "Iteration 74576, loss = 0.96469892\n",
      "Iteration 74577, loss = 0.92984762\n",
      "Iteration 74578, loss = 1.00431997\n",
      "Iteration 74579, loss = 1.15500432\n",
      "Iteration 74580, loss = 1.55936913\n",
      "Iteration 74581, loss = 1.61298410\n",
      "Iteration 74582, loss = 1.70826495\n",
      "Iteration 74583, loss = 1.69087314\n",
      "Iteration 74584, loss = 1.29117207\n",
      "Iteration 74585, loss = 1.17792102\n",
      "Iteration 74586, loss = 1.32318256\n",
      "Iteration 74587, loss = 1.08097395\n",
      "Iteration 74588, loss = 1.04094165\n",
      "Iteration 74589, loss = 1.01301409\n",
      "Iteration 74590, loss = 1.11631443\n",
      "Iteration 74591, loss = 1.30081924\n",
      "Iteration 74592, loss = 1.18186971\n",
      "Iteration 74593, loss = 1.34996716\n",
      "Iteration 74594, loss = 1.34414548\n",
      "Iteration 74595, loss = 1.71132975\n",
      "Iteration 74596, loss = 3.65871496\n",
      "Iteration 74597, loss = 4.17926548\n",
      "Iteration 74598, loss = 3.19664681\n",
      "Iteration 74599, loss = 2.76276631\n",
      "Iteration 74600, loss = 2.18246494\n",
      "Iteration 74601, loss = 1.53760165\n",
      "Iteration 74602, loss = 1.39487359\n",
      "Iteration 74603, loss = 1.36214336\n",
      "Iteration 74604, loss = 1.16562373\n",
      "Iteration 74605, loss = 1.31385952\n",
      "Iteration 74606, loss = 1.24713701\n",
      "Iteration 74607, loss = 1.49021278\n",
      "Iteration 74608, loss = 1.25955223\n",
      "Iteration 74609, loss = 1.12162918\n",
      "Iteration 74610, loss = 1.13632807\n",
      "Iteration 74611, loss = 1.08536507\n",
      "Iteration 74612, loss = 1.07866844\n",
      "Iteration 74613, loss = 1.08821814\n",
      "Iteration 74614, loss = 1.07615682\n",
      "Iteration 74615, loss = 1.13309786\n",
      "Iteration 74616, loss = 1.18118491\n",
      "Iteration 74617, loss = 1.13126702\n",
      "Iteration 74618, loss = 1.19563834\n",
      "Iteration 74619, loss = 1.25474791\n",
      "Iteration 74620, loss = 1.13009792\n",
      "Iteration 74621, loss = 1.02415551\n",
      "Iteration 74622, loss = 1.15026137\n",
      "Iteration 74623, loss = 1.36723545\n",
      "Iteration 74624, loss = 1.17330562\n",
      "Iteration 74625, loss = 1.13845129\n",
      "Iteration 74626, loss = 1.09015337\n",
      "Iteration 74627, loss = 1.20895757\n",
      "Iteration 74628, loss = 1.25120168\n",
      "Iteration 74629, loss = 1.20968221\n",
      "Iteration 74630, loss = 1.05567087\n",
      "Iteration 74631, loss = 1.10468615\n",
      "Iteration 74632, loss = 1.03368802\n",
      "Iteration 74633, loss = 1.10298985\n",
      "Iteration 74634, loss = 1.00127978\n",
      "Iteration 74635, loss = 1.40008696\n",
      "Iteration 74636, loss = 1.42355599\n",
      "Iteration 74637, loss = 1.40705759\n",
      "Iteration 74638, loss = 1.15128703\n",
      "Iteration 74639, loss = 1.27914736\n",
      "Iteration 74640, loss = 1.17343119\n",
      "Iteration 74641, loss = 0.98122066\n",
      "Iteration 74642, loss = 1.00185490\n",
      "Iteration 74643, loss = 1.09517451\n",
      "Iteration 74644, loss = 1.05623822\n",
      "Iteration 74645, loss = 1.05644416\n",
      "Iteration 74646, loss = 1.17099642\n",
      "Iteration 74647, loss = 1.24466794\n",
      "Iteration 74648, loss = 1.09871724\n",
      "Iteration 74649, loss = 1.18203762\n",
      "Iteration 74650, loss = 1.18979122\n",
      "Iteration 74651, loss = 1.21767767\n",
      "Iteration 74652, loss = 1.03854385\n",
      "Iteration 74653, loss = 1.12440775\n",
      "Iteration 74654, loss = 1.07040375\n",
      "Iteration 74655, loss = 1.20018644\n",
      "Iteration 74656, loss = 1.23078461\n",
      "Iteration 74657, loss = 1.43527325\n",
      "Iteration 74658, loss = 1.34264681\n",
      "Iteration 74659, loss = 1.62923516\n",
      "Iteration 74660, loss = 1.44814395\n",
      "Iteration 74661, loss = 1.63636145\n",
      "Iteration 74662, loss = 1.67299087\n",
      "Iteration 74663, loss = 1.61033390\n",
      "Iteration 74664, loss = 1.25367348\n",
      "Iteration 74665, loss = 1.18347999\n",
      "Iteration 74666, loss = 1.20210347\n",
      "Iteration 74667, loss = 1.49892418\n",
      "Iteration 74668, loss = 1.32274648\n",
      "Iteration 74669, loss = 1.17701563\n",
      "Iteration 74670, loss = 1.12518949\n",
      "Iteration 74671, loss = 1.08883416\n",
      "Iteration 74672, loss = 0.96495529\n",
      "Iteration 74673, loss = 1.05162180\n",
      "Iteration 74674, loss = 1.75873369\n",
      "Iteration 74675, loss = 1.72130996\n",
      "Iteration 74676, loss = 1.67534012\n",
      "Iteration 74677, loss = 1.86800575\n",
      "Iteration 74678, loss = 1.50931390\n",
      "Iteration 74679, loss = 1.39600112\n",
      "Iteration 74680, loss = 1.23248746\n",
      "Iteration 74681, loss = 1.15786306\n",
      "Iteration 74682, loss = 1.28752471\n",
      "Iteration 74683, loss = 1.60601934\n",
      "Iteration 74684, loss = 1.20250668\n",
      "Iteration 74685, loss = 1.33730751\n",
      "Iteration 74686, loss = 1.52658395\n",
      "Iteration 74687, loss = 1.12739548\n",
      "Iteration 74688, loss = 1.61057816\n",
      "Iteration 74689, loss = 1.29422676\n",
      "Iteration 74690, loss = 1.26998970\n",
      "Iteration 74691, loss = 1.34297323\n",
      "Iteration 74692, loss = 1.14733457\n",
      "Iteration 74693, loss = 1.18639923\n",
      "Iteration 74694, loss = 1.09488361\n",
      "Iteration 74695, loss = 1.05399963\n",
      "Iteration 74696, loss = 1.08472756\n",
      "Iteration 74697, loss = 0.96819726\n",
      "Iteration 74698, loss = 0.97542686\n",
      "Iteration 74699, loss = 0.96492142\n",
      "Iteration 74700, loss = 0.98832220\n",
      "Iteration 74701, loss = 0.97829210\n",
      "Iteration 74702, loss = 1.10733303\n",
      "Iteration 74703, loss = 1.34793188\n",
      "Iteration 74704, loss = 1.48393502\n",
      "Iteration 74705, loss = 1.22741410\n",
      "Iteration 74706, loss = 1.11354658\n",
      "Iteration 74707, loss = 1.14590415\n",
      "Iteration 74708, loss = 1.13420382\n",
      "Iteration 74709, loss = 1.26534218\n",
      "Iteration 74710, loss = 1.11395923\n",
      "Iteration 74711, loss = 1.43587262\n",
      "Iteration 74712, loss = 1.20632750\n",
      "Iteration 74713, loss = 1.08148471\n",
      "Iteration 74714, loss = 1.03913950\n",
      "Iteration 74715, loss = 0.94248893\n",
      "Iteration 74716, loss = 0.96983434\n",
      "Iteration 74717, loss = 0.93415328\n",
      "Iteration 74718, loss = 1.35926074\n",
      "Iteration 74719, loss = 1.27106682\n",
      "Iteration 74720, loss = 1.21436864\n",
      "Iteration 74721, loss = 1.41060543\n",
      "Iteration 74722, loss = 1.41291061\n",
      "Iteration 74723, loss = 2.08455529\n",
      "Iteration 74724, loss = 1.55105442\n",
      "Iteration 74725, loss = 1.40560537\n",
      "Iteration 74726, loss = 1.05601273\n",
      "Iteration 74727, loss = 1.02202113\n",
      "Iteration 74728, loss = 1.10792911\n",
      "Iteration 74729, loss = 1.13281267\n",
      "Iteration 74730, loss = 1.00948282\n",
      "Iteration 74731, loss = 1.04413195\n",
      "Iteration 74732, loss = 1.37476448\n",
      "Iteration 74733, loss = 1.38726017\n",
      "Iteration 74734, loss = 1.10109218\n",
      "Iteration 74735, loss = 1.38197252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 74736, loss = 1.27587845\n",
      "Iteration 74737, loss = 1.31533382\n",
      "Iteration 74738, loss = 1.20469247\n",
      "Iteration 74739, loss = 1.15110962\n",
      "Iteration 74740, loss = 1.10538529\n",
      "Iteration 74741, loss = 1.29003675\n",
      "Iteration 74742, loss = 1.22121988\n",
      "Iteration 74743, loss = 1.16851375\n",
      "Iteration 74744, loss = 1.06911848\n",
      "Iteration 74745, loss = 1.14844472\n",
      "Iteration 74746, loss = 1.05871617\n",
      "Iteration 74747, loss = 1.42687473\n",
      "Iteration 74748, loss = 1.15289318\n",
      "Iteration 74749, loss = 1.42570299\n",
      "Iteration 74750, loss = 1.46552412\n",
      "Iteration 74751, loss = 1.63718780\n",
      "Iteration 74752, loss = 1.56686914\n",
      "Iteration 74753, loss = 1.57127373\n",
      "Iteration 74754, loss = 1.43293362\n",
      "Iteration 74755, loss = 1.76649939\n",
      "Iteration 74756, loss = 1.47217911\n",
      "Iteration 74757, loss = 1.27569645\n",
      "Iteration 74758, loss = 1.60260562\n",
      "Iteration 74759, loss = 1.14850149\n",
      "Iteration 74760, loss = 1.10305922\n",
      "Iteration 74761, loss = 1.14841661\n",
      "Iteration 74762, loss = 1.07094840\n",
      "Iteration 74763, loss = 1.25000340\n",
      "Iteration 74764, loss = 1.43072773\n",
      "Iteration 74765, loss = 1.24967276\n",
      "Iteration 74766, loss = 1.18768052\n",
      "Iteration 74767, loss = 1.30056909\n",
      "Iteration 74768, loss = 1.11821105\n",
      "Iteration 74769, loss = 1.32204198\n",
      "Iteration 74770, loss = 1.18526450\n",
      "Iteration 74771, loss = 1.43088226\n",
      "Iteration 74772, loss = 1.04474779\n",
      "Iteration 74773, loss = 1.20310636\n",
      "Iteration 74774, loss = 1.01364209\n",
      "Iteration 74775, loss = 1.11372889\n",
      "Iteration 74776, loss = 1.06385400\n",
      "Iteration 74777, loss = 1.37970740\n",
      "Iteration 74778, loss = 1.43868679\n",
      "Iteration 74779, loss = 1.79911864\n",
      "Iteration 74780, loss = 1.72034551\n",
      "Iteration 74781, loss = 1.46238138\n",
      "Iteration 74782, loss = 1.37471812\n",
      "Iteration 74783, loss = 1.23668107\n",
      "Iteration 74784, loss = 1.26171075\n",
      "Iteration 74785, loss = 1.04170071\n",
      "Iteration 74786, loss = 1.43687950\n",
      "Iteration 74787, loss = 1.39413538\n",
      "Iteration 74788, loss = 1.26861165\n",
      "Iteration 74789, loss = 1.18430227\n",
      "Iteration 74790, loss = 1.00739013\n",
      "Iteration 74791, loss = 1.10238609\n",
      "Iteration 74792, loss = 1.17804869\n",
      "Iteration 74793, loss = 1.21260291\n",
      "Iteration 74794, loss = 1.68732854\n",
      "Iteration 74795, loss = 1.47408302\n",
      "Iteration 74796, loss = 1.50920038\n",
      "Iteration 74797, loss = 1.40491572\n",
      "Iteration 74798, loss = 1.31838707\n",
      "Iteration 74799, loss = 1.17020945\n",
      "Iteration 74800, loss = 1.08302303\n",
      "Iteration 74801, loss = 1.07680043\n",
      "Iteration 74802, loss = 1.21221572\n",
      "Iteration 74803, loss = 1.17471634\n",
      "Iteration 74804, loss = 1.06315016\n",
      "Iteration 74805, loss = 1.03650205\n",
      "Iteration 74806, loss = 1.27242289\n",
      "Iteration 74807, loss = 1.17367811\n",
      "Iteration 74808, loss = 1.20177414\n",
      "Iteration 74809, loss = 1.36920881\n",
      "Iteration 74810, loss = 1.23542342\n",
      "Iteration 74811, loss = 1.48920462\n",
      "Iteration 74812, loss = 1.64783676\n",
      "Iteration 74813, loss = 1.74324240\n",
      "Iteration 74814, loss = 1.27697034\n",
      "Iteration 74815, loss = 1.12246043\n",
      "Iteration 74816, loss = 1.10740277\n",
      "Iteration 74817, loss = 1.26404933\n",
      "Iteration 74818, loss = 1.21448787\n",
      "Iteration 74819, loss = 1.12386960\n",
      "Iteration 74820, loss = 0.97819568\n",
      "Iteration 74821, loss = 1.04559214\n",
      "Iteration 74822, loss = 1.26822244\n",
      "Iteration 74823, loss = 1.29995991\n",
      "Iteration 74824, loss = 1.54969210\n",
      "Iteration 74825, loss = 1.41655201\n",
      "Iteration 74826, loss = 1.12805525\n",
      "Iteration 74827, loss = 1.17791482\n",
      "Iteration 74828, loss = 1.17907588\n",
      "Iteration 74829, loss = 1.12782686\n",
      "Iteration 74830, loss = 1.28051777\n",
      "Iteration 74831, loss = 1.22652975\n",
      "Iteration 74832, loss = 1.79320551\n",
      "Iteration 74833, loss = 1.72282501\n",
      "Iteration 74834, loss = 1.86960628\n",
      "Iteration 74835, loss = 1.20211921\n",
      "Iteration 74836, loss = 1.36806667\n",
      "Iteration 74837, loss = 1.79156797\n",
      "Iteration 74838, loss = 1.44682105\n",
      "Iteration 74839, loss = 1.57164619\n",
      "Iteration 74840, loss = 2.07386677\n",
      "Iteration 74841, loss = 2.42113478\n",
      "Iteration 74842, loss = 1.77101192\n",
      "Iteration 74843, loss = 1.33166391\n",
      "Iteration 74844, loss = 1.14128488\n",
      "Iteration 74845, loss = 1.23314428\n",
      "Iteration 74846, loss = 1.25187464\n",
      "Iteration 74847, loss = 1.49526063\n",
      "Iteration 74848, loss = 1.29345684\n",
      "Iteration 74849, loss = 0.96436349\n",
      "Iteration 74850, loss = 1.10566671\n",
      "Iteration 74851, loss = 1.09283671\n",
      "Iteration 74852, loss = 1.08754191\n",
      "Iteration 74853, loss = 1.43811129\n",
      "Iteration 74854, loss = 2.15118363\n",
      "Iteration 74855, loss = 2.94373912\n",
      "Iteration 74856, loss = 2.12243896\n",
      "Iteration 74857, loss = 1.86603805\n",
      "Iteration 74858, loss = 1.36241823\n",
      "Iteration 74859, loss = 1.49141626\n",
      "Iteration 74860, loss = 1.70744255\n",
      "Iteration 74861, loss = 1.47781481\n",
      "Iteration 74862, loss = 1.32671945\n",
      "Iteration 74863, loss = 1.77944028\n",
      "Iteration 74864, loss = 1.59744764\n",
      "Iteration 74865, loss = 1.88202924\n",
      "Iteration 74866, loss = 1.64932582\n",
      "Iteration 74867, loss = 1.55808987\n",
      "Iteration 74868, loss = 2.26956575\n",
      "Iteration 74869, loss = 1.71005886\n",
      "Iteration 74870, loss = 1.71051755\n",
      "Iteration 74871, loss = 1.54460425\n",
      "Iteration 74872, loss = 1.16087629\n",
      "Iteration 74873, loss = 1.09867123\n",
      "Iteration 74874, loss = 1.22705205\n",
      "Iteration 74875, loss = 1.15487139\n",
      "Iteration 74876, loss = 1.08531126\n",
      "Iteration 74877, loss = 1.03774260\n",
      "Iteration 74878, loss = 0.98428496\n",
      "Iteration 74879, loss = 1.02924061\n",
      "Iteration 74880, loss = 1.04786161\n",
      "Iteration 74881, loss = 0.94230452\n",
      "Iteration 74882, loss = 1.09389941\n",
      "Iteration 74883, loss = 0.98127745\n",
      "Iteration 74884, loss = 1.09368205\n",
      "Iteration 74885, loss = 1.07875626\n",
      "Iteration 74886, loss = 1.19815792\n",
      "Iteration 74887, loss = 1.04369946\n",
      "Iteration 74888, loss = 1.04647182\n",
      "Iteration 74889, loss = 1.10172333\n",
      "Iteration 74890, loss = 1.15302803\n",
      "Iteration 74891, loss = 1.07796623\n",
      "Iteration 74892, loss = 0.97901460\n",
      "Iteration 74893, loss = 1.24868728\n",
      "Iteration 74894, loss = 1.17956719\n",
      "Iteration 74895, loss = 1.10302689\n",
      "Iteration 74896, loss = 1.16409263\n",
      "Iteration 74897, loss = 1.15107516\n",
      "Iteration 74898, loss = 1.08025275\n",
      "Iteration 74899, loss = 1.01434453\n",
      "Iteration 74900, loss = 1.11555456\n",
      "Iteration 74901, loss = 1.00420911\n",
      "Iteration 74902, loss = 1.14775097\n",
      "Iteration 74903, loss = 1.26843243\n",
      "Iteration 74904, loss = 1.67510646\n",
      "Iteration 74905, loss = 1.41627825\n",
      "Iteration 74906, loss = 1.26469138\n",
      "Iteration 74907, loss = 1.69067415\n",
      "Iteration 74908, loss = 1.43339863\n",
      "Iteration 74909, loss = 1.08468057\n",
      "Iteration 74910, loss = 1.16934638\n",
      "Iteration 74911, loss = 1.26200355\n",
      "Iteration 74912, loss = 1.56610431\n",
      "Iteration 74913, loss = 1.18038614\n",
      "Iteration 74914, loss = 1.32630115\n",
      "Iteration 74915, loss = 1.49880125\n",
      "Iteration 74916, loss = 1.19738839\n",
      "Iteration 74917, loss = 1.30553789\n",
      "Iteration 74918, loss = 1.39230816\n",
      "Iteration 74919, loss = 1.13180032\n",
      "Iteration 74920, loss = 1.23643541\n",
      "Iteration 74921, loss = 1.36592622\n",
      "Iteration 74922, loss = 1.87969089\n",
      "Iteration 74923, loss = 1.91982018\n",
      "Iteration 74924, loss = 2.09816906\n",
      "Iteration 74925, loss = 1.73646284\n",
      "Iteration 74926, loss = 1.31894270\n",
      "Iteration 74927, loss = 1.05656074\n",
      "Iteration 74928, loss = 1.06934703\n",
      "Iteration 74929, loss = 1.02746590\n",
      "Iteration 74930, loss = 1.01141821\n",
      "Iteration 74931, loss = 1.05187381\n",
      "Iteration 74932, loss = 1.16488515\n",
      "Iteration 74933, loss = 1.13847695\n",
      "Iteration 74934, loss = 1.38075150\n",
      "Iteration 74935, loss = 1.29396895\n",
      "Iteration 74936, loss = 1.05359462\n",
      "Iteration 74937, loss = 1.11737379\n",
      "Iteration 74938, loss = 1.06332994\n",
      "Iteration 74939, loss = 0.95305488\n",
      "Iteration 74940, loss = 1.07094747\n",
      "Iteration 74941, loss = 1.21080349\n",
      "Iteration 74942, loss = 1.08738462\n",
      "Iteration 74943, loss = 1.16062866\n",
      "Iteration 74944, loss = 1.34022467\n",
      "Iteration 74945, loss = 1.16229258\n",
      "Iteration 74946, loss = 1.28270900\n",
      "Iteration 74947, loss = 1.15139841\n",
      "Iteration 74948, loss = 1.14839818\n",
      "Iteration 74949, loss = 1.16566624\n",
      "Iteration 74950, loss = 1.03147891\n",
      "Iteration 74951, loss = 1.15637653\n",
      "Iteration 74952, loss = 1.28625997\n",
      "Iteration 74953, loss = 1.31459530\n",
      "Iteration 74954, loss = 1.34329277\n",
      "Iteration 74955, loss = 1.69633377\n",
      "Iteration 74956, loss = 1.68660310\n",
      "Iteration 74957, loss = 1.51118057\n",
      "Iteration 74958, loss = 1.57826265\n",
      "Iteration 74959, loss = 1.63623365\n",
      "Iteration 74960, loss = 1.43614957\n",
      "Iteration 74961, loss = 1.24005656\n",
      "Iteration 74962, loss = 1.29865603\n",
      "Iteration 74963, loss = 1.39040591\n",
      "Iteration 74964, loss = 1.26216938\n",
      "Iteration 74965, loss = 1.29724774\n",
      "Iteration 74966, loss = 1.31741610\n",
      "Iteration 74967, loss = 1.30257860\n",
      "Iteration 74968, loss = 1.09665445\n",
      "Iteration 74969, loss = 1.01610647\n",
      "Iteration 74970, loss = 1.03764445\n",
      "Iteration 74971, loss = 1.21530311\n",
      "Iteration 74972, loss = 1.06470936\n",
      "Iteration 74973, loss = 1.09347583\n",
      "Iteration 74974, loss = 1.10493281\n",
      "Iteration 74975, loss = 1.17332406\n",
      "Iteration 74976, loss = 1.09307098\n",
      "Iteration 74977, loss = 1.14443975\n",
      "Iteration 74978, loss = 1.11543573\n",
      "Iteration 74979, loss = 1.15683160\n",
      "Iteration 74980, loss = 1.05539488\n",
      "Iteration 74981, loss = 1.06151368\n",
      "Iteration 74982, loss = 1.03729511\n",
      "Iteration 74983, loss = 1.00603228\n",
      "Iteration 74984, loss = 0.96508340\n",
      "Iteration 74985, loss = 1.06482195\n",
      "Iteration 74986, loss = 0.97847926\n",
      "Iteration 74987, loss = 1.13287889\n",
      "Iteration 74988, loss = 1.07118209\n",
      "Iteration 74989, loss = 1.08186872\n",
      "Iteration 74990, loss = 1.09727774\n",
      "Iteration 74991, loss = 1.08403103\n",
      "Iteration 74992, loss = 1.15978948\n",
      "Iteration 74993, loss = 1.01361700\n",
      "Iteration 74994, loss = 1.00644537\n",
      "Iteration 74995, loss = 1.06659705\n",
      "Iteration 74996, loss = 1.12477899\n",
      "Iteration 74997, loss = 1.08604043\n",
      "Iteration 74998, loss = 1.02176444\n",
      "Iteration 74999, loss = 1.02284430\n",
      "Iteration 75000, loss = 1.02335525\n",
      "Iteration 75001, loss = 1.08167625\n",
      "Iteration 75002, loss = 0.98658282\n",
      "Iteration 75003, loss = 1.04019735\n",
      "Iteration 75004, loss = 1.05662063\n",
      "Iteration 75005, loss = 1.08722756\n",
      "Iteration 75006, loss = 0.99556829\n",
      "Iteration 75007, loss = 0.97562542\n",
      "Iteration 75008, loss = 1.26495513\n",
      "Iteration 75009, loss = 1.23867431\n",
      "Iteration 75010, loss = 1.26090049\n",
      "Iteration 75011, loss = 1.14292534\n",
      "Iteration 75012, loss = 1.45186110\n",
      "Iteration 75013, loss = 1.11110572\n",
      "Iteration 75014, loss = 0.97940543\n",
      "Iteration 75015, loss = 1.06329811\n",
      "Iteration 75016, loss = 1.19670154\n",
      "Iteration 75017, loss = 1.12661256\n",
      "Iteration 75018, loss = 1.28290746\n",
      "Iteration 75019, loss = 1.36077662\n",
      "Iteration 75020, loss = 1.81065109\n",
      "Iteration 75021, loss = 1.51860148\n",
      "Iteration 75022, loss = 1.39712135\n",
      "Iteration 75023, loss = 1.17004292\n",
      "Iteration 75024, loss = 1.43384373\n",
      "Iteration 75025, loss = 1.95288295\n",
      "Iteration 75026, loss = 1.72735356\n",
      "Iteration 75027, loss = 1.30232035\n",
      "Iteration 75028, loss = 1.42802621\n",
      "Iteration 75029, loss = 1.55424347\n",
      "Iteration 75030, loss = 1.51269104\n",
      "Iteration 75031, loss = 1.56827116\n",
      "Iteration 75032, loss = 1.27310614\n",
      "Iteration 75033, loss = 1.19399926\n",
      "Iteration 75034, loss = 1.10068739\n",
      "Iteration 75035, loss = 1.22053975\n",
      "Iteration 75036, loss = 1.15372376\n",
      "Iteration 75037, loss = 1.00612984\n",
      "Iteration 75038, loss = 1.05792316\n",
      "Iteration 75039, loss = 0.98389547\n",
      "Iteration 75040, loss = 1.02928516\n",
      "Iteration 75041, loss = 1.10318083\n",
      "Iteration 75042, loss = 1.12888643\n",
      "Iteration 75043, loss = 1.09066300\n",
      "Iteration 75044, loss = 0.99675811\n",
      "Iteration 75045, loss = 1.04253580\n",
      "Iteration 75046, loss = 1.08437623\n",
      "Iteration 75047, loss = 0.94383460\n",
      "Iteration 75048, loss = 1.02244643\n",
      "Iteration 75049, loss = 0.99412063\n",
      "Iteration 75050, loss = 0.96794041\n",
      "Iteration 75051, loss = 1.07526559\n",
      "Iteration 75052, loss = 1.17983868\n",
      "Iteration 75053, loss = 1.02809583\n",
      "Iteration 75054, loss = 1.04455265\n",
      "Iteration 75055, loss = 1.09068854\n",
      "Iteration 75056, loss = 1.03787047\n",
      "Iteration 75057, loss = 0.96464623\n",
      "Iteration 75058, loss = 0.98953222\n",
      "Iteration 75059, loss = 1.02710704\n",
      "Iteration 75060, loss = 1.01074467\n",
      "Iteration 75061, loss = 1.06930562\n",
      "Iteration 75062, loss = 1.14252052\n",
      "Iteration 75063, loss = 1.21495007\n",
      "Iteration 75064, loss = 1.09173526\n",
      "Iteration 75065, loss = 1.03372780\n",
      "Iteration 75066, loss = 1.06950780\n",
      "Iteration 75067, loss = 1.14334948\n",
      "Iteration 75068, loss = 1.10271795\n",
      "Iteration 75069, loss = 1.21563880\n",
      "Iteration 75070, loss = 1.13266416\n",
      "Iteration 75071, loss = 1.00825361\n",
      "Iteration 75072, loss = 1.04089498\n",
      "Iteration 75073, loss = 1.22510076\n",
      "Iteration 75074, loss = 1.14084151\n",
      "Iteration 75075, loss = 1.03482890\n",
      "Iteration 75076, loss = 1.34990099\n",
      "Iteration 75077, loss = 1.21702465\n",
      "Iteration 75078, loss = 1.23360462\n",
      "Iteration 75079, loss = 1.05373778\n",
      "Iteration 75080, loss = 1.10779402\n",
      "Iteration 75081, loss = 1.00365002\n",
      "Iteration 75082, loss = 0.96086822\n",
      "Iteration 75083, loss = 1.16594247\n",
      "Iteration 75084, loss = 1.39313447\n",
      "Iteration 75085, loss = 1.45101256\n",
      "Iteration 75086, loss = 1.64880309\n",
      "Iteration 75087, loss = 1.49479686\n",
      "Iteration 75088, loss = 1.36438355\n",
      "Iteration 75089, loss = 1.08497311\n",
      "Iteration 75090, loss = 1.14522777\n",
      "Iteration 75091, loss = 1.06379531\n",
      "Iteration 75092, loss = 1.05726360\n",
      "Iteration 75093, loss = 0.98348152\n",
      "Iteration 75094, loss = 1.02057553\n",
      "Iteration 75095, loss = 0.97708091\n",
      "Iteration 75096, loss = 0.99024446\n",
      "Iteration 75097, loss = 1.01586443\n",
      "Iteration 75098, loss = 1.02930997\n",
      "Iteration 75099, loss = 1.02267645\n",
      "Iteration 75100, loss = 0.95884188\n",
      "Iteration 75101, loss = 0.95679030\n",
      "Iteration 75102, loss = 1.05977438\n",
      "Iteration 75103, loss = 1.05359149\n",
      "Iteration 75104, loss = 1.16918945\n",
      "Iteration 75105, loss = 1.03237936\n",
      "Iteration 75106, loss = 1.06763742\n",
      "Iteration 75107, loss = 1.10287205\n",
      "Iteration 75108, loss = 1.12190987\n",
      "Iteration 75109, loss = 1.24246186\n",
      "Iteration 75110, loss = 1.44669297\n",
      "Iteration 75111, loss = 1.02591341\n",
      "Iteration 75112, loss = 1.01856176\n",
      "Iteration 75113, loss = 0.96036257\n",
      "Iteration 75114, loss = 1.14204666\n",
      "Iteration 75115, loss = 1.17062205\n",
      "Iteration 75116, loss = 1.13968658\n",
      "Iteration 75117, loss = 1.17836681\n",
      "Iteration 75118, loss = 1.43913688\n",
      "Iteration 75119, loss = 1.13814835\n",
      "Iteration 75120, loss = 1.24932951\n",
      "Iteration 75121, loss = 1.41660368\n",
      "Iteration 75122, loss = 1.81049211\n",
      "Iteration 75123, loss = 2.06498032\n",
      "Iteration 75124, loss = 2.07303994\n",
      "Iteration 75125, loss = 1.78417352\n",
      "Iteration 75126, loss = 1.97369166\n",
      "Iteration 75127, loss = 2.17349101\n",
      "Iteration 75128, loss = 2.16715466\n",
      "Iteration 75129, loss = 2.03461570\n",
      "Iteration 75130, loss = 1.95406437\n",
      "Iteration 75131, loss = 1.71234841\n",
      "Iteration 75132, loss = 1.93735642\n",
      "Iteration 75133, loss = 2.44845586\n",
      "Iteration 75134, loss = 1.99502457\n",
      "Iteration 75135, loss = 1.78417920\n",
      "Iteration 75136, loss = 1.38703775\n",
      "Iteration 75137, loss = 1.61868912\n",
      "Iteration 75138, loss = 1.57413055\n",
      "Iteration 75139, loss = 2.49238026\n",
      "Iteration 75140, loss = 4.34006384\n",
      "Iteration 75141, loss = 4.29050048\n",
      "Iteration 75142, loss = 2.85610843\n",
      "Iteration 75143, loss = 2.41987165\n",
      "Iteration 75144, loss = 2.42124971\n",
      "Iteration 75145, loss = 1.40392105\n",
      "Iteration 75146, loss = 1.55093452\n",
      "Iteration 75147, loss = 1.47308557\n",
      "Iteration 75148, loss = 1.37904122\n",
      "Iteration 75149, loss = 1.50088540\n",
      "Iteration 75150, loss = 1.50301663\n",
      "Iteration 75151, loss = 1.51952828\n",
      "Iteration 75152, loss = 2.44802042\n",
      "Iteration 75153, loss = 2.02898245\n",
      "Iteration 75154, loss = 1.63307064\n",
      "Iteration 75155, loss = 1.55952075\n",
      "Iteration 75156, loss = 1.58420729\n",
      "Iteration 75157, loss = 1.61287646\n",
      "Iteration 75158, loss = 1.17374484\n",
      "Iteration 75159, loss = 1.45978110\n",
      "Iteration 75160, loss = 1.49289076\n",
      "Iteration 75161, loss = 1.07858717\n",
      "Iteration 75162, loss = 1.24951063\n",
      "Iteration 75163, loss = 1.29962073\n",
      "Iteration 75164, loss = 1.21679025\n",
      "Iteration 75165, loss = 1.48281506\n",
      "Iteration 75166, loss = 1.29429766\n",
      "Iteration 75167, loss = 1.11270460\n",
      "Iteration 75168, loss = 1.20981497\n",
      "Iteration 75169, loss = 1.31207308\n",
      "Iteration 75170, loss = 1.30225948\n",
      "Iteration 75171, loss = 1.06860417\n",
      "Iteration 75172, loss = 0.99590936\n",
      "Iteration 75173, loss = 1.02764076\n",
      "Iteration 75174, loss = 1.22649122\n",
      "Iteration 75175, loss = 1.34295943\n",
      "Iteration 75176, loss = 1.27641809\n",
      "Iteration 75177, loss = 1.30355947\n",
      "Iteration 75178, loss = 1.18242767\n",
      "Iteration 75179, loss = 1.49918755\n",
      "Iteration 75180, loss = 1.47012085\n",
      "Iteration 75181, loss = 1.21541847\n",
      "Iteration 75182, loss = 1.36294704\n",
      "Iteration 75183, loss = 1.24618605\n",
      "Iteration 75184, loss = 1.17726090\n",
      "Iteration 75185, loss = 1.08945667\n",
      "Iteration 75186, loss = 1.08146475\n",
      "Iteration 75187, loss = 0.92607509\n",
      "Iteration 75188, loss = 1.04285765\n",
      "Iteration 75189, loss = 0.98391971\n",
      "Iteration 75190, loss = 0.95567369\n",
      "Iteration 75191, loss = 1.15252750\n",
      "Iteration 75192, loss = 1.22271166\n",
      "Iteration 75193, loss = 1.22842484\n",
      "Iteration 75194, loss = 1.25283660\n",
      "Iteration 75195, loss = 1.14616481\n",
      "Iteration 75196, loss = 1.07153518\n",
      "Iteration 75197, loss = 0.96769207\n",
      "Iteration 75198, loss = 1.27204135\n",
      "Iteration 75199, loss = 1.31675307\n",
      "Iteration 75200, loss = 1.12838214\n",
      "Iteration 75201, loss = 1.30213851\n",
      "Iteration 75202, loss = 1.35064462\n",
      "Iteration 75203, loss = 1.51535009\n",
      "Iteration 75204, loss = 1.19988334\n",
      "Iteration 75205, loss = 1.08129183\n",
      "Iteration 75206, loss = 1.28387809\n",
      "Iteration 75207, loss = 1.55431003\n",
      "Iteration 75208, loss = 1.32227512\n",
      "Iteration 75209, loss = 0.98586425\n",
      "Iteration 75210, loss = 1.05021771\n",
      "Iteration 75211, loss = 1.32939853\n",
      "Iteration 75212, loss = 1.25055962\n",
      "Iteration 75213, loss = 1.07766701\n",
      "Iteration 75214, loss = 0.96496089\n",
      "Iteration 75215, loss = 1.04798376\n",
      "Iteration 75216, loss = 1.00259136\n",
      "Iteration 75217, loss = 1.00010208\n",
      "Iteration 75218, loss = 0.98851087\n",
      "Iteration 75219, loss = 1.04818529\n",
      "Iteration 75220, loss = 0.96259979\n",
      "Iteration 75221, loss = 0.99554001\n",
      "Iteration 75222, loss = 1.08578112\n",
      "Iteration 75223, loss = 1.01012010\n",
      "Iteration 75224, loss = 0.99571776\n",
      "Iteration 75225, loss = 0.95365611\n",
      "Iteration 75226, loss = 1.02568910\n",
      "Iteration 75227, loss = 1.28518102\n",
      "Iteration 75228, loss = 1.09257858\n",
      "Iteration 75229, loss = 0.99979425\n",
      "Iteration 75230, loss = 1.22094939\n",
      "Iteration 75231, loss = 1.08424720\n",
      "Iteration 75232, loss = 1.01642660\n",
      "Iteration 75233, loss = 0.99200769\n",
      "Iteration 75234, loss = 1.04807882\n",
      "Iteration 75235, loss = 1.14257592\n",
      "Iteration 75236, loss = 1.05287701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 75237, loss = 1.14759334\n",
      "Iteration 75238, loss = 1.34180015\n",
      "Iteration 75239, loss = 1.52172785\n",
      "Iteration 75240, loss = 1.57734443\n",
      "Iteration 75241, loss = 1.19618899\n",
      "Iteration 75242, loss = 0.98672519\n",
      "Iteration 75243, loss = 0.95854392\n",
      "Iteration 75244, loss = 0.91086036\n",
      "Iteration 75245, loss = 1.06822091\n",
      "Iteration 75246, loss = 1.07592970\n",
      "Iteration 75247, loss = 1.00675936\n",
      "Iteration 75248, loss = 0.92215534\n",
      "Iteration 75249, loss = 0.97456368\n",
      "Iteration 75250, loss = 0.94922408\n",
      "Iteration 75251, loss = 1.14111993\n",
      "Iteration 75252, loss = 1.29005450\n",
      "Iteration 75253, loss = 1.14110759\n",
      "Iteration 75254, loss = 1.08058056\n",
      "Iteration 75255, loss = 1.00765047\n",
      "Iteration 75256, loss = 1.01844285\n",
      "Iteration 75257, loss = 1.31935532\n",
      "Iteration 75258, loss = 1.15478607\n",
      "Iteration 75259, loss = 1.21085767\n",
      "Iteration 75260, loss = 1.34355861\n",
      "Iteration 75261, loss = 1.43639063\n",
      "Iteration 75262, loss = 1.39451704\n",
      "Iteration 75263, loss = 1.37834854\n",
      "Iteration 75264, loss = 1.30685186\n",
      "Iteration 75265, loss = 1.16426572\n",
      "Iteration 75266, loss = 1.15906454\n",
      "Iteration 75267, loss = 1.09629025\n",
      "Iteration 75268, loss = 1.48386788\n",
      "Iteration 75269, loss = 1.22120598\n",
      "Iteration 75270, loss = 1.05603135\n",
      "Iteration 75271, loss = 1.17655937\n",
      "Iteration 75272, loss = 1.16193371\n",
      "Iteration 75273, loss = 1.06555999\n",
      "Iteration 75274, loss = 1.07971937\n",
      "Iteration 75275, loss = 1.15763570\n",
      "Iteration 75276, loss = 1.11220840\n",
      "Iteration 75277, loss = 1.24988009\n",
      "Iteration 75278, loss = 1.31725284\n",
      "Iteration 75279, loss = 1.32376506\n",
      "Iteration 75280, loss = 1.33684254\n",
      "Iteration 75281, loss = 1.30267291\n",
      "Iteration 75282, loss = 1.48539415\n",
      "Iteration 75283, loss = 1.55604868\n",
      "Iteration 75284, loss = 1.48020282\n",
      "Iteration 75285, loss = 1.47486247\n",
      "Iteration 75286, loss = 1.92884875\n",
      "Iteration 75287, loss = 1.77894676\n",
      "Iteration 75288, loss = 2.15913800\n",
      "Iteration 75289, loss = 1.62105835\n",
      "Iteration 75290, loss = 1.44918294\n",
      "Iteration 75291, loss = 1.37327148\n",
      "Iteration 75292, loss = 1.33712464\n",
      "Iteration 75293, loss = 1.52461495\n",
      "Iteration 75294, loss = 1.34320473\n",
      "Iteration 75295, loss = 1.32104580\n",
      "Iteration 75296, loss = 1.61984090\n",
      "Iteration 75297, loss = 2.86627717\n",
      "Iteration 75298, loss = 2.64148456\n",
      "Iteration 75299, loss = 2.43994192\n",
      "Iteration 75300, loss = 2.13087030\n",
      "Iteration 75301, loss = 1.62108022\n",
      "Iteration 75302, loss = 1.48328094\n",
      "Iteration 75303, loss = 1.18447521\n",
      "Iteration 75304, loss = 1.25678260\n",
      "Iteration 75305, loss = 1.40240626\n",
      "Iteration 75306, loss = 1.19697409\n",
      "Iteration 75307, loss = 1.28097606\n",
      "Iteration 75308, loss = 1.39999000\n",
      "Iteration 75309, loss = 1.07111204\n",
      "Iteration 75310, loss = 1.05233529\n",
      "Iteration 75311, loss = 1.04861871\n",
      "Iteration 75312, loss = 1.23188225\n",
      "Iteration 75313, loss = 1.53795317\n",
      "Iteration 75314, loss = 1.64950657\n",
      "Iteration 75315, loss = 2.10828087\n",
      "Iteration 75316, loss = 3.39566593\n",
      "Iteration 75317, loss = 2.67662019\n",
      "Iteration 75318, loss = 1.94407740\n",
      "Iteration 75319, loss = 2.12493414\n",
      "Iteration 75320, loss = 2.47000221\n",
      "Iteration 75321, loss = 2.03312338\n",
      "Iteration 75322, loss = 1.39750983\n",
      "Iteration 75323, loss = 1.55344700\n",
      "Iteration 75324, loss = 1.28326891\n",
      "Iteration 75325, loss = 1.34754117\n",
      "Iteration 75326, loss = 1.50131582\n",
      "Iteration 75327, loss = 1.98612001\n",
      "Iteration 75328, loss = 1.51445879\n",
      "Iteration 75329, loss = 1.41429589\n",
      "Iteration 75330, loss = 1.31057164\n",
      "Iteration 75331, loss = 1.15641273\n",
      "Iteration 75332, loss = 1.00432404\n",
      "Iteration 75333, loss = 1.05046239\n",
      "Iteration 75334, loss = 1.11931142\n",
      "Iteration 75335, loss = 1.08610762\n",
      "Iteration 75336, loss = 1.23619040\n",
      "Iteration 75337, loss = 1.16935066\n",
      "Iteration 75338, loss = 1.14865838\n",
      "Iteration 75339, loss = 0.99998919\n",
      "Iteration 75340, loss = 1.08768311\n",
      "Iteration 75341, loss = 0.95802025\n",
      "Iteration 75342, loss = 0.98153809\n",
      "Iteration 75343, loss = 1.07284597\n",
      "Iteration 75344, loss = 1.11143746\n",
      "Iteration 75345, loss = 1.12100601\n",
      "Iteration 75346, loss = 1.05934345\n",
      "Iteration 75347, loss = 1.02526308\n",
      "Iteration 75348, loss = 1.08751402\n",
      "Iteration 75349, loss = 1.28930348\n",
      "Iteration 75350, loss = 1.19788656\n",
      "Iteration 75351, loss = 1.29394914\n",
      "Iteration 75352, loss = 1.42882109\n",
      "Iteration 75353, loss = 1.26872524\n",
      "Iteration 75354, loss = 1.40417454\n",
      "Iteration 75355, loss = 1.11748354\n",
      "Iteration 75356, loss = 1.19222438\n",
      "Iteration 75357, loss = 1.49155974\n",
      "Iteration 75358, loss = 1.24022212\n",
      "Iteration 75359, loss = 1.19703609\n",
      "Iteration 75360, loss = 1.23713099\n",
      "Iteration 75361, loss = 1.13883404\n",
      "Iteration 75362, loss = 1.16148327\n",
      "Iteration 75363, loss = 1.24153639\n",
      "Iteration 75364, loss = 0.93814746\n",
      "Iteration 75365, loss = 1.02807662\n",
      "Iteration 75366, loss = 0.93512407\n",
      "Iteration 75367, loss = 0.93164232\n",
      "Iteration 75368, loss = 1.04542251\n",
      "Iteration 75369, loss = 1.09426205\n",
      "Iteration 75370, loss = 1.38704224\n",
      "Iteration 75371, loss = 1.41657702\n",
      "Iteration 75372, loss = 1.46472931\n",
      "Iteration 75373, loss = 1.42422771\n",
      "Iteration 75374, loss = 1.49729804\n",
      "Iteration 75375, loss = 1.08531054\n",
      "Iteration 75376, loss = 1.21750023\n",
      "Iteration 75377, loss = 1.38838291\n",
      "Iteration 75378, loss = 1.12890296\n",
      "Iteration 75379, loss = 1.34788170\n",
      "Iteration 75380, loss = 1.26875164\n",
      "Iteration 75381, loss = 1.36222222\n",
      "Iteration 75382, loss = 1.04932037\n",
      "Iteration 75383, loss = 1.03905262\n",
      "Iteration 75384, loss = 1.30280146\n",
      "Iteration 75385, loss = 1.12661805\n",
      "Iteration 75386, loss = 0.99659923\n",
      "Iteration 75387, loss = 0.96772531\n",
      "Iteration 75388, loss = 1.03185296\n",
      "Iteration 75389, loss = 0.95177737\n",
      "Iteration 75390, loss = 0.93072093\n",
      "Iteration 75391, loss = 1.03370421\n",
      "Iteration 75392, loss = 1.01229988\n",
      "Iteration 75393, loss = 0.99383072\n",
      "Iteration 75394, loss = 0.97715790\n",
      "Iteration 75395, loss = 0.99209744\n",
      "Iteration 75396, loss = 1.00375694\n",
      "Iteration 75397, loss = 1.21668896\n",
      "Iteration 75398, loss = 1.08290739\n",
      "Iteration 75399, loss = 1.05905250\n",
      "Iteration 75400, loss = 1.07846461\n",
      "Iteration 75401, loss = 0.94573995\n",
      "Iteration 75402, loss = 0.95053539\n",
      "Iteration 75403, loss = 1.02901323\n",
      "Iteration 75404, loss = 1.16696570\n",
      "Iteration 75405, loss = 1.12877681\n",
      "Iteration 75406, loss = 0.99157240\n",
      "Iteration 75407, loss = 1.05856555\n",
      "Iteration 75408, loss = 0.93843409\n",
      "Iteration 75409, loss = 0.99171354\n",
      "Iteration 75410, loss = 0.91508612\n",
      "Iteration 75411, loss = 1.00284540\n",
      "Iteration 75412, loss = 1.03471340\n",
      "Iteration 75413, loss = 1.27437079\n",
      "Iteration 75414, loss = 1.56888130\n",
      "Iteration 75415, loss = 1.50986406\n",
      "Iteration 75416, loss = 1.25815918\n",
      "Iteration 75417, loss = 1.05335589\n",
      "Iteration 75418, loss = 1.08004052\n",
      "Iteration 75419, loss = 1.22687330\n",
      "Iteration 75420, loss = 1.29213808\n",
      "Iteration 75421, loss = 1.42571681\n",
      "Iteration 75422, loss = 1.49344893\n",
      "Iteration 75423, loss = 1.48476662\n",
      "Iteration 75424, loss = 1.29635524\n",
      "Iteration 75425, loss = 1.05409881\n",
      "Iteration 75426, loss = 1.06769251\n",
      "Iteration 75427, loss = 1.21061066\n",
      "Iteration 75428, loss = 1.31160010\n",
      "Iteration 75429, loss = 1.49827037\n",
      "Iteration 75430, loss = 1.23874089\n",
      "Iteration 75431, loss = 1.16918314\n",
      "Iteration 75432, loss = 1.35191990\n",
      "Iteration 75433, loss = 1.46610910\n",
      "Iteration 75434, loss = 1.98715016\n",
      "Iteration 75435, loss = 2.02321873\n",
      "Iteration 75436, loss = 2.12370135\n",
      "Iteration 75437, loss = 1.75142501\n",
      "Iteration 75438, loss = 1.61774589\n",
      "Iteration 75439, loss = 1.20247475\n",
      "Iteration 75440, loss = 1.40048811\n",
      "Iteration 75441, loss = 1.36280295\n",
      "Iteration 75442, loss = 1.11925503\n",
      "Iteration 75443, loss = 1.16671693\n",
      "Iteration 75444, loss = 1.15407441\n",
      "Iteration 75445, loss = 1.11548499\n",
      "Iteration 75446, loss = 1.32229275\n",
      "Iteration 75447, loss = 1.38807335\n",
      "Iteration 75448, loss = 1.32448714\n",
      "Iteration 75449, loss = 1.11916228\n",
      "Iteration 75450, loss = 1.43175281\n",
      "Iteration 75451, loss = 1.42619537\n",
      "Iteration 75452, loss = 1.39611134\n",
      "Iteration 75453, loss = 1.40646454\n",
      "Iteration 75454, loss = 1.60684213\n",
      "Iteration 75455, loss = 1.48301219\n",
      "Iteration 75456, loss = 1.82600340\n",
      "Iteration 75457, loss = 1.71458232\n",
      "Iteration 75458, loss = 1.84533874\n",
      "Iteration 75459, loss = 1.76256225\n",
      "Iteration 75460, loss = 1.46684531\n",
      "Iteration 75461, loss = 1.31482065\n",
      "Iteration 75462, loss = 1.11851228\n",
      "Iteration 75463, loss = 1.09226272\n",
      "Iteration 75464, loss = 1.12658606\n",
      "Iteration 75465, loss = 1.23595860\n",
      "Iteration 75466, loss = 1.09187257\n",
      "Iteration 75467, loss = 1.10825005\n",
      "Iteration 75468, loss = 1.42802357\n",
      "Iteration 75469, loss = 1.39603764\n",
      "Iteration 75470, loss = 1.42734991\n",
      "Iteration 75471, loss = 1.24463660\n",
      "Iteration 75472, loss = 1.34990900\n",
      "Iteration 75473, loss = 1.32366924\n",
      "Iteration 75474, loss = 1.26448185\n",
      "Iteration 75475, loss = 1.18473689\n",
      "Iteration 75476, loss = 0.96243063\n",
      "Iteration 75477, loss = 1.05983314\n",
      "Iteration 75478, loss = 1.27904625\n",
      "Iteration 75479, loss = 1.14701287\n",
      "Iteration 75480, loss = 1.25074012\n",
      "Iteration 75481, loss = 1.35884272\n",
      "Iteration 75482, loss = 1.56689505\n",
      "Iteration 75483, loss = 1.31187949\n",
      "Iteration 75484, loss = 1.29377215\n",
      "Iteration 75485, loss = 1.27329137\n",
      "Iteration 75486, loss = 1.06286362\n",
      "Iteration 75487, loss = 1.04059850\n",
      "Iteration 75488, loss = 1.21371931\n",
      "Iteration 75489, loss = 1.11507257\n",
      "Iteration 75490, loss = 1.31670848\n",
      "Iteration 75491, loss = 1.40169228\n",
      "Iteration 75492, loss = 1.03634437\n",
      "Iteration 75493, loss = 1.02723682\n",
      "Iteration 75494, loss = 1.07588732\n",
      "Iteration 75495, loss = 1.11648176\n",
      "Iteration 75496, loss = 1.05797355\n",
      "Iteration 75497, loss = 1.05130808\n",
      "Iteration 75498, loss = 1.09583136\n",
      "Iteration 75499, loss = 1.14174032\n",
      "Iteration 75500, loss = 1.25221731\n",
      "Iteration 75501, loss = 1.01598871\n",
      "Iteration 75502, loss = 1.08810874\n",
      "Iteration 75503, loss = 1.47604011\n",
      "Iteration 75504, loss = 1.16523690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 75505, loss = 1.30522293\n",
      "Iteration 75506, loss = 1.25963380\n",
      "Iteration 75507, loss = 0.98375848\n",
      "Iteration 75508, loss = 1.02984665\n",
      "Iteration 75509, loss = 1.20501624\n",
      "Iteration 75510, loss = 1.26579407\n",
      "Iteration 75511, loss = 1.24389772\n",
      "Iteration 75512, loss = 1.07377628\n",
      "Iteration 75513, loss = 0.99395653\n",
      "Iteration 75514, loss = 1.06652517\n",
      "Iteration 75515, loss = 1.29141860\n",
      "Iteration 75516, loss = 1.21343225\n",
      "Iteration 75517, loss = 1.07876887\n",
      "Iteration 75518, loss = 1.19578533\n",
      "Iteration 75519, loss = 1.21913872\n",
      "Iteration 75520, loss = 1.53901048\n",
      "Iteration 75521, loss = 1.42893197\n",
      "Iteration 75522, loss = 1.31262450\n",
      "Iteration 75523, loss = 1.20733029\n",
      "Iteration 75524, loss = 1.28798578\n",
      "Iteration 75525, loss = 1.45472916\n",
      "Iteration 75526, loss = 1.38438087\n",
      "Iteration 75527, loss = 1.50439144\n",
      "Iteration 75528, loss = 1.44532546\n",
      "Iteration 75529, loss = 1.81187471\n",
      "Iteration 75530, loss = 1.25010152\n",
      "Iteration 75531, loss = 1.13342065\n",
      "Iteration 75532, loss = 1.18426215\n",
      "Iteration 75533, loss = 1.14751998\n",
      "Iteration 75534, loss = 1.31403271\n",
      "Iteration 75535, loss = 1.43072221\n",
      "Iteration 75536, loss = 1.46905514\n",
      "Iteration 75537, loss = 1.34159259\n",
      "Iteration 75538, loss = 1.15615055\n",
      "Iteration 75539, loss = 1.26881210\n",
      "Iteration 75540, loss = 1.19489119\n",
      "Iteration 75541, loss = 1.40778889\n",
      "Iteration 75542, loss = 1.62588900\n",
      "Iteration 75543, loss = 1.69926492\n",
      "Iteration 75544, loss = 1.19378674\n",
      "Iteration 75545, loss = 1.10030145\n",
      "Iteration 75546, loss = 1.04932982\n",
      "Iteration 75547, loss = 1.01648491\n",
      "Iteration 75548, loss = 0.99129836\n",
      "Iteration 75549, loss = 1.11730194\n",
      "Iteration 75550, loss = 1.17787601\n",
      "Iteration 75551, loss = 1.20304926\n",
      "Iteration 75552, loss = 1.39222809\n",
      "Iteration 75553, loss = 1.28664775\n",
      "Iteration 75554, loss = 1.36584369\n",
      "Iteration 75555, loss = 1.74412017\n",
      "Iteration 75556, loss = 1.26610796\n",
      "Iteration 75557, loss = 1.14270104\n",
      "Iteration 75558, loss = 1.07528419\n",
      "Iteration 75559, loss = 1.04965900\n",
      "Iteration 75560, loss = 1.03924203\n",
      "Iteration 75561, loss = 0.95356521\n",
      "Iteration 75562, loss = 0.92086950\n",
      "Iteration 75563, loss = 0.95558768\n",
      "Iteration 75564, loss = 1.00840421\n",
      "Iteration 75565, loss = 1.14719043\n",
      "Iteration 75566, loss = 1.22586460\n",
      "Iteration 75567, loss = 1.13765987\n",
      "Iteration 75568, loss = 1.09106176\n",
      "Iteration 75569, loss = 1.08414083\n",
      "Iteration 75570, loss = 0.96616228\n",
      "Iteration 75571, loss = 0.93822325\n",
      "Iteration 75572, loss = 0.97341831\n",
      "Iteration 75573, loss = 0.99570559\n",
      "Iteration 75574, loss = 1.12881820\n",
      "Iteration 75575, loss = 1.23695812\n",
      "Iteration 75576, loss = 1.41190181\n",
      "Iteration 75577, loss = 1.24958854\n",
      "Iteration 75578, loss = 1.27466017\n",
      "Iteration 75579, loss = 1.33869219\n",
      "Iteration 75580, loss = 1.49025385\n",
      "Iteration 75581, loss = 1.33130147\n",
      "Iteration 75582, loss = 1.09298058\n",
      "Iteration 75583, loss = 1.05611994\n",
      "Iteration 75584, loss = 0.96876802\n",
      "Iteration 75585, loss = 1.09387950\n",
      "Iteration 75586, loss = 1.01761615\n",
      "Iteration 75587, loss = 0.98248151\n",
      "Iteration 75588, loss = 1.03645126\n",
      "Iteration 75589, loss = 1.00314424\n",
      "Iteration 75590, loss = 0.99266095\n",
      "Iteration 75591, loss = 0.94285068\n",
      "Iteration 75592, loss = 0.95886873\n",
      "Iteration 75593, loss = 0.97040836\n",
      "Iteration 75594, loss = 0.96184266\n",
      "Iteration 75595, loss = 1.01361283\n",
      "Iteration 75596, loss = 1.13249933\n",
      "Iteration 75597, loss = 1.25847930\n",
      "Iteration 75598, loss = 1.57798572\n",
      "Iteration 75599, loss = 1.35002909\n",
      "Iteration 75600, loss = 1.25554925\n",
      "Iteration 75601, loss = 1.03996051\n",
      "Iteration 75602, loss = 1.24064308\n",
      "Iteration 75603, loss = 1.23429361\n",
      "Iteration 75604, loss = 1.00348193\n",
      "Iteration 75605, loss = 1.24488694\n",
      "Iteration 75606, loss = 1.35587059\n",
      "Iteration 75607, loss = 2.02815338\n",
      "Iteration 75608, loss = 2.82915471\n",
      "Iteration 75609, loss = 3.36750133\n",
      "Iteration 75610, loss = 3.43413313\n",
      "Iteration 75611, loss = 3.23008398\n",
      "Iteration 75612, loss = 3.50646975\n",
      "Iteration 75613, loss = 3.69076074\n",
      "Iteration 75614, loss = 3.10032355\n",
      "Iteration 75615, loss = 2.66142038\n",
      "Iteration 75616, loss = 2.00293395\n",
      "Iteration 75617, loss = 2.12710514\n",
      "Iteration 75618, loss = 2.39513233\n",
      "Iteration 75619, loss = 1.50979965\n",
      "Iteration 75620, loss = 1.47502818\n",
      "Iteration 75621, loss = 1.31305754\n",
      "Iteration 75622, loss = 1.50036537\n",
      "Iteration 75623, loss = 1.33703267\n",
      "Iteration 75624, loss = 1.18725848\n",
      "Iteration 75625, loss = 1.10436516\n",
      "Iteration 75626, loss = 1.20709945\n",
      "Iteration 75627, loss = 1.30030548\n",
      "Iteration 75628, loss = 1.44254656\n",
      "Iteration 75629, loss = 1.31205636\n",
      "Iteration 75630, loss = 1.09632260\n",
      "Iteration 75631, loss = 1.06221202\n",
      "Iteration 75632, loss = 1.02421626\n",
      "Iteration 75633, loss = 1.06144636\n",
      "Iteration 75634, loss = 0.98065391\n",
      "Iteration 75635, loss = 0.97104293\n",
      "Iteration 75636, loss = 1.27272790\n",
      "Iteration 75637, loss = 1.12312563\n",
      "Iteration 75638, loss = 1.22611079\n",
      "Iteration 75639, loss = 1.28362570\n",
      "Iteration 75640, loss = 1.20190629\n",
      "Iteration 75641, loss = 1.19687037\n",
      "Iteration 75642, loss = 1.29760129\n",
      "Iteration 75643, loss = 1.47750116\n",
      "Iteration 75644, loss = 1.16717924\n",
      "Iteration 75645, loss = 1.33272835\n",
      "Iteration 75646, loss = 1.23741482\n",
      "Iteration 75647, loss = 1.52076745\n",
      "Iteration 75648, loss = 1.35796486\n",
      "Iteration 75649, loss = 1.17491501\n",
      "Iteration 75650, loss = 1.19910318\n",
      "Iteration 75651, loss = 1.06993360\n",
      "Iteration 75652, loss = 1.04860194\n",
      "Iteration 75653, loss = 1.13184737\n",
      "Iteration 75654, loss = 1.04125804\n",
      "Iteration 75655, loss = 1.12823429\n",
      "Iteration 75656, loss = 0.96900504\n",
      "Iteration 75657, loss = 1.05699333\n",
      "Iteration 75658, loss = 1.14009559\n",
      "Iteration 75659, loss = 1.13776573\n",
      "Iteration 75660, loss = 1.25950671\n",
      "Iteration 75661, loss = 1.30363700\n",
      "Iteration 75662, loss = 1.09497703\n",
      "Iteration 75663, loss = 1.13148114\n",
      "Iteration 75664, loss = 1.36310310\n",
      "Iteration 75665, loss = 1.78471782\n",
      "Iteration 75666, loss = 1.68841712\n",
      "Iteration 75667, loss = 1.13571275\n",
      "Iteration 75668, loss = 1.15115104\n",
      "Iteration 75669, loss = 1.04633313\n",
      "Iteration 75670, loss = 1.01144620\n",
      "Iteration 75671, loss = 1.08637922\n",
      "Iteration 75672, loss = 1.19702068\n",
      "Iteration 75673, loss = 1.15206800\n",
      "Iteration 75674, loss = 1.04825244\n",
      "Iteration 75675, loss = 1.12865799\n",
      "Iteration 75676, loss = 1.03575045\n",
      "Iteration 75677, loss = 1.07772507\n",
      "Iteration 75678, loss = 1.12371589\n",
      "Iteration 75679, loss = 2.13486902\n",
      "Iteration 75680, loss = 1.96722910\n",
      "Iteration 75681, loss = 1.34324931\n",
      "Iteration 75682, loss = 1.21463628\n",
      "Iteration 75683, loss = 1.48323211\n",
      "Iteration 75684, loss = 1.29085618\n",
      "Iteration 75685, loss = 1.25940357\n",
      "Iteration 75686, loss = 1.11225953\n",
      "Iteration 75687, loss = 1.94858731\n",
      "Iteration 75688, loss = 2.31375929\n",
      "Iteration 75689, loss = 1.73811990\n",
      "Iteration 75690, loss = 1.45120911\n",
      "Iteration 75691, loss = 1.39629780\n",
      "Iteration 75692, loss = 1.72399547\n",
      "Iteration 75693, loss = 2.00464285\n",
      "Iteration 75694, loss = 1.79328109\n",
      "Iteration 75695, loss = 1.93319356\n",
      "Iteration 75696, loss = 1.61158852\n",
      "Iteration 75697, loss = 1.44818101\n",
      "Iteration 75698, loss = 1.31509006\n",
      "Iteration 75699, loss = 1.23568521\n",
      "Iteration 75700, loss = 1.23109798\n",
      "Iteration 75701, loss = 1.10671569\n",
      "Iteration 75702, loss = 1.08876554\n",
      "Iteration 75703, loss = 1.09104543\n",
      "Iteration 75704, loss = 0.96063501\n",
      "Iteration 75705, loss = 0.91474491\n",
      "Iteration 75706, loss = 0.98185514\n",
      "Iteration 75707, loss = 0.97436977\n",
      "Iteration 75708, loss = 0.97216184\n",
      "Iteration 75709, loss = 1.00497494\n",
      "Iteration 75710, loss = 1.10111005\n",
      "Iteration 75711, loss = 1.17396014\n",
      "Iteration 75712, loss = 1.14245401\n",
      "Iteration 75713, loss = 1.07613112\n",
      "Iteration 75714, loss = 1.11299149\n",
      "Iteration 75715, loss = 1.03431001\n",
      "Iteration 75716, loss = 1.15690087\n",
      "Iteration 75717, loss = 1.24370022\n",
      "Iteration 75718, loss = 1.27454660\n",
      "Iteration 75719, loss = 1.13482492\n",
      "Iteration 75720, loss = 1.15741447\n",
      "Iteration 75721, loss = 0.97973260\n",
      "Iteration 75722, loss = 0.98749969\n",
      "Iteration 75723, loss = 0.98243188\n",
      "Iteration 75724, loss = 0.95274828\n",
      "Iteration 75725, loss = 1.02993288\n",
      "Iteration 75726, loss = 1.08135418\n",
      "Iteration 75727, loss = 1.01388261\n",
      "Iteration 75728, loss = 1.22868623\n",
      "Iteration 75729, loss = 1.11142939\n",
      "Iteration 75730, loss = 1.05930780\n",
      "Iteration 75731, loss = 1.35693620\n",
      "Iteration 75732, loss = 1.61428670\n",
      "Iteration 75733, loss = 1.38859161\n",
      "Iteration 75734, loss = 1.40464823\n",
      "Iteration 75735, loss = 1.73320369\n",
      "Iteration 75736, loss = 1.52327165\n",
      "Iteration 75737, loss = 1.89739626\n",
      "Iteration 75738, loss = 1.73057348\n",
      "Iteration 75739, loss = 1.48299980\n",
      "Iteration 75740, loss = 1.29946893\n",
      "Iteration 75741, loss = 1.43891650\n",
      "Iteration 75742, loss = 1.24090462\n",
      "Iteration 75743, loss = 1.19617104\n",
      "Iteration 75744, loss = 1.00294106\n",
      "Iteration 75745, loss = 1.15596207\n",
      "Iteration 75746, loss = 1.33467506\n",
      "Iteration 75747, loss = 1.26442516\n",
      "Iteration 75748, loss = 1.13277666\n",
      "Iteration 75749, loss = 1.15807664\n",
      "Iteration 75750, loss = 1.03786305\n",
      "Iteration 75751, loss = 1.14141908\n",
      "Iteration 75752, loss = 1.04930122\n",
      "Iteration 75753, loss = 0.98172063\n",
      "Iteration 75754, loss = 1.03213227\n",
      "Iteration 75755, loss = 1.08923782\n",
      "Iteration 75756, loss = 1.25686797\n",
      "Iteration 75757, loss = 1.06489054\n",
      "Iteration 75758, loss = 1.15794721\n",
      "Iteration 75759, loss = 1.03648319\n",
      "Iteration 75760, loss = 1.41587546\n",
      "Iteration 75761, loss = 1.58111157\n",
      "Iteration 75762, loss = 1.30731836\n",
      "Iteration 75763, loss = 1.12454808\n",
      "Iteration 75764, loss = 1.16832031\n",
      "Iteration 75765, loss = 1.19419113\n",
      "Iteration 75766, loss = 1.35972947\n",
      "Iteration 75767, loss = 1.45576061\n",
      "Iteration 75768, loss = 1.33985574\n",
      "Iteration 75769, loss = 1.30570169\n",
      "Iteration 75770, loss = 1.43794575\n",
      "Iteration 75771, loss = 1.47677344\n",
      "Iteration 75772, loss = 1.58036235\n",
      "Iteration 75773, loss = 1.30892638\n",
      "Iteration 75774, loss = 1.24244996\n",
      "Iteration 75775, loss = 1.18961615\n",
      "Iteration 75776, loss = 1.24167805\n",
      "Iteration 75777, loss = 1.12847855\n",
      "Iteration 75778, loss = 1.16056157\n",
      "Iteration 75779, loss = 1.09386092\n",
      "Iteration 75780, loss = 1.12480572\n",
      "Iteration 75781, loss = 1.31623682\n",
      "Iteration 75782, loss = 1.26266624\n",
      "Iteration 75783, loss = 1.34270978\n",
      "Iteration 75784, loss = 1.08847493\n",
      "Iteration 75785, loss = 0.96375332\n",
      "Iteration 75786, loss = 1.00381675\n",
      "Iteration 75787, loss = 1.20296831\n",
      "Iteration 75788, loss = 1.11049475\n",
      "Iteration 75789, loss = 1.04235457\n",
      "Iteration 75790, loss = 1.14517427\n",
      "Iteration 75791, loss = 1.02624465\n",
      "Iteration 75792, loss = 1.12416850\n",
      "Iteration 75793, loss = 1.28851531\n",
      "Iteration 75794, loss = 1.23358877\n",
      "Iteration 75795, loss = 1.36247459\n",
      "Iteration 75796, loss = 1.24617689\n",
      "Iteration 75797, loss = 1.29334813\n",
      "Iteration 75798, loss = 1.35051446\n",
      "Iteration 75799, loss = 1.23802494\n",
      "Iteration 75800, loss = 1.16732396\n",
      "Iteration 75801, loss = 1.28719318\n",
      "Iteration 75802, loss = 1.40047742\n",
      "Iteration 75803, loss = 1.14062275\n",
      "Iteration 75804, loss = 1.06161697\n",
      "Iteration 75805, loss = 1.03523264\n",
      "Iteration 75806, loss = 1.11865501\n",
      "Iteration 75807, loss = 1.09580206\n",
      "Iteration 75808, loss = 0.99281944\n",
      "Iteration 75809, loss = 1.04769284\n",
      "Iteration 75810, loss = 1.19918625\n",
      "Iteration 75811, loss = 1.15617709\n",
      "Iteration 75812, loss = 1.13269634\n",
      "Iteration 75813, loss = 1.04607591\n",
      "Iteration 75814, loss = 1.01458869\n",
      "Iteration 75815, loss = 1.11561986\n",
      "Iteration 75816, loss = 1.21495615\n",
      "Iteration 75817, loss = 1.66577253\n",
      "Iteration 75818, loss = 1.97443007\n",
      "Iteration 75819, loss = 1.86833567\n",
      "Iteration 75820, loss = 1.95522587\n",
      "Iteration 75821, loss = 1.54152701\n",
      "Iteration 75822, loss = 1.86412578\n",
      "Iteration 75823, loss = 1.36295471\n",
      "Iteration 75824, loss = 1.17095354\n",
      "Iteration 75825, loss = 1.30359554\n",
      "Iteration 75826, loss = 1.27129248\n",
      "Iteration 75827, loss = 1.36166951\n",
      "Iteration 75828, loss = 1.24405838\n",
      "Iteration 75829, loss = 1.16883766\n",
      "Iteration 75830, loss = 1.31082696\n",
      "Iteration 75831, loss = 1.13681298\n",
      "Iteration 75832, loss = 1.14461030\n",
      "Iteration 75833, loss = 1.29690982\n",
      "Iteration 75834, loss = 1.07730549\n",
      "Iteration 75835, loss = 1.15651521\n",
      "Iteration 75836, loss = 1.02583015\n",
      "Iteration 75837, loss = 1.17560302\n",
      "Iteration 75838, loss = 1.37402190\n",
      "Iteration 75839, loss = 1.64716796\n",
      "Iteration 75840, loss = 1.69015632\n",
      "Iteration 75841, loss = 1.24601046\n",
      "Iteration 75842, loss = 1.27097066\n",
      "Iteration 75843, loss = 1.20208176\n",
      "Iteration 75844, loss = 1.10742828\n",
      "Iteration 75845, loss = 1.01272996\n",
      "Iteration 75846, loss = 1.14454914\n",
      "Iteration 75847, loss = 1.04448277\n",
      "Iteration 75848, loss = 1.03172859\n",
      "Iteration 75849, loss = 1.03899091\n",
      "Iteration 75850, loss = 0.98527625\n",
      "Iteration 75851, loss = 1.11119390\n",
      "Iteration 75852, loss = 1.02395564\n",
      "Iteration 75853, loss = 1.09855497\n",
      "Iteration 75854, loss = 1.29107528\n",
      "Iteration 75855, loss = 1.43040735\n",
      "Iteration 75856, loss = 1.32360395\n",
      "Iteration 75857, loss = 1.19592829\n",
      "Iteration 75858, loss = 1.11094228\n",
      "Iteration 75859, loss = 1.05050045\n",
      "Iteration 75860, loss = 1.11684860\n",
      "Iteration 75861, loss = 1.02823036\n",
      "Iteration 75862, loss = 1.00736382\n",
      "Iteration 75863, loss = 0.97047614\n",
      "Iteration 75864, loss = 1.09569224\n",
      "Iteration 75865, loss = 1.07051760\n",
      "Iteration 75866, loss = 1.06653095\n",
      "Iteration 75867, loss = 1.01035628\n",
      "Iteration 75868, loss = 1.09648056\n",
      "Iteration 75869, loss = 1.19949620\n",
      "Iteration 75870, loss = 1.10609747\n",
      "Iteration 75871, loss = 1.17951801\n",
      "Iteration 75872, loss = 1.61162259\n",
      "Iteration 75873, loss = 1.53088870\n",
      "Iteration 75874, loss = 1.36264261\n",
      "Iteration 75875, loss = 1.46495278\n",
      "Iteration 75876, loss = 1.54649007\n",
      "Iteration 75877, loss = 1.55591248\n",
      "Iteration 75878, loss = 1.40296591\n",
      "Iteration 75879, loss = 2.33788666\n",
      "Iteration 75880, loss = 1.78743566\n",
      "Iteration 75881, loss = 1.63098431\n",
      "Iteration 75882, loss = 1.61572229\n",
      "Iteration 75883, loss = 2.16583181\n",
      "Iteration 75884, loss = 1.65077052\n",
      "Iteration 75885, loss = 1.98042872\n",
      "Iteration 75886, loss = 2.48845594\n",
      "Iteration 75887, loss = 2.71576416\n",
      "Iteration 75888, loss = 2.39622717\n",
      "Iteration 75889, loss = 2.32862847\n",
      "Iteration 75890, loss = 2.61995310\n",
      "Iteration 75891, loss = 2.34539585\n",
      "Iteration 75892, loss = 2.67520834\n",
      "Iteration 75893, loss = 2.13278541\n",
      "Iteration 75894, loss = 1.73598649\n",
      "Iteration 75895, loss = 2.41144034\n",
      "Iteration 75896, loss = 1.50829944\n",
      "Iteration 75897, loss = 1.52922374\n",
      "Iteration 75898, loss = 1.41997183\n",
      "Iteration 75899, loss = 1.56161027\n",
      "Iteration 75900, loss = 2.00662260\n",
      "Iteration 75901, loss = 1.75118570\n",
      "Iteration 75902, loss = 1.51352104\n",
      "Iteration 75903, loss = 1.33869220\n",
      "Iteration 75904, loss = 1.22916569\n",
      "Iteration 75905, loss = 1.32996524\n",
      "Iteration 75906, loss = 1.32434669\n",
      "Iteration 75907, loss = 1.26347301\n",
      "Iteration 75908, loss = 1.38160555\n",
      "Iteration 75909, loss = 1.14213522\n",
      "Iteration 75910, loss = 1.14234167\n",
      "Iteration 75911, loss = 1.11603675\n",
      "Iteration 75912, loss = 1.17900425\n",
      "Iteration 75913, loss = 1.23825733\n",
      "Iteration 75914, loss = 1.31739653\n",
      "Iteration 75915, loss = 1.31005408\n",
      "Iteration 75916, loss = 1.12086202\n",
      "Iteration 75917, loss = 0.94818587\n",
      "Iteration 75918, loss = 1.02370211\n",
      "Iteration 75919, loss = 1.10953244\n",
      "Iteration 75920, loss = 1.12262017\n",
      "Iteration 75921, loss = 1.01678432\n",
      "Iteration 75922, loss = 1.06688530\n",
      "Iteration 75923, loss = 1.18915023\n",
      "Iteration 75924, loss = 1.03252657\n",
      "Iteration 75925, loss = 1.05961680\n",
      "Iteration 75926, loss = 1.08396631\n",
      "Iteration 75927, loss = 1.08988650\n",
      "Iteration 75928, loss = 1.08657326\n",
      "Iteration 75929, loss = 0.98741806\n",
      "Iteration 75930, loss = 0.95147490\n",
      "Iteration 75931, loss = 0.95577326\n",
      "Iteration 75932, loss = 0.99628389\n",
      "Iteration 75933, loss = 1.01195452\n",
      "Iteration 75934, loss = 1.06464200\n",
      "Iteration 75935, loss = 1.05381935\n",
      "Iteration 75936, loss = 1.05778209\n",
      "Iteration 75937, loss = 1.01068471\n",
      "Iteration 75938, loss = 1.00461599\n",
      "Iteration 75939, loss = 1.05062422\n",
      "Iteration 75940, loss = 1.12304398\n",
      "Iteration 75941, loss = 0.98207686\n",
      "Iteration 75942, loss = 1.02395243\n",
      "Iteration 75943, loss = 1.16077097\n",
      "Iteration 75944, loss = 1.05527286\n",
      "Iteration 75945, loss = 1.19799767\n",
      "Iteration 75946, loss = 1.08734892\n",
      "Iteration 75947, loss = 1.02209041\n",
      "Iteration 75948, loss = 0.93631094\n",
      "Iteration 75949, loss = 0.99664106\n",
      "Iteration 75950, loss = 1.14323037\n",
      "Iteration 75951, loss = 1.19850347\n",
      "Iteration 75952, loss = 1.27316030\n",
      "Iteration 75953, loss = 1.33853813\n",
      "Iteration 75954, loss = 1.31307565\n",
      "Iteration 75955, loss = 1.35023338\n",
      "Iteration 75956, loss = 1.52624438\n",
      "Iteration 75957, loss = 1.32163216\n",
      "Iteration 75958, loss = 1.13835314\n",
      "Iteration 75959, loss = 1.15473432\n",
      "Iteration 75960, loss = 1.06489626\n",
      "Iteration 75961, loss = 1.13900815\n",
      "Iteration 75962, loss = 1.09559766\n",
      "Iteration 75963, loss = 1.08145775\n",
      "Iteration 75964, loss = 1.28524928\n",
      "Iteration 75965, loss = 1.18744139\n",
      "Iteration 75966, loss = 1.06625222\n",
      "Iteration 75967, loss = 1.24652533\n",
      "Iteration 75968, loss = 1.16837025\n",
      "Iteration 75969, loss = 1.38337268\n",
      "Iteration 75970, loss = 1.72063251\n",
      "Iteration 75971, loss = 1.73669385\n",
      "Iteration 75972, loss = 1.57260382\n",
      "Iteration 75973, loss = 1.94642625\n",
      "Iteration 75974, loss = 1.95279807\n",
      "Iteration 75975, loss = 2.04700816\n",
      "Iteration 75976, loss = 1.41202297\n",
      "Iteration 75977, loss = 1.23916861\n",
      "Iteration 75978, loss = 1.10760824\n",
      "Iteration 75979, loss = 1.16288565\n",
      "Iteration 75980, loss = 0.99563972\n",
      "Iteration 75981, loss = 1.01475827\n",
      "Iteration 75982, loss = 0.93774035\n",
      "Iteration 75983, loss = 0.95750539\n",
      "Iteration 75984, loss = 1.02139978\n",
      "Iteration 75985, loss = 1.15642285\n",
      "Iteration 75986, loss = 1.09015634\n",
      "Iteration 75987, loss = 1.11994360\n",
      "Iteration 75988, loss = 1.07539796\n",
      "Iteration 75989, loss = 1.07733175\n",
      "Iteration 75990, loss = 1.41759185\n",
      "Iteration 75991, loss = 1.23505503\n",
      "Iteration 75992, loss = 1.28942463\n",
      "Iteration 75993, loss = 2.12252235\n",
      "Iteration 75994, loss = 1.87138194\n",
      "Iteration 75995, loss = 1.94508657\n",
      "Iteration 75996, loss = 1.39538671\n",
      "Iteration 75997, loss = 1.25196639\n",
      "Iteration 75998, loss = 1.28697276\n",
      "Iteration 75999, loss = 1.57334468\n",
      "Iteration 76000, loss = 1.58515501\n",
      "Iteration 76001, loss = 1.31054839\n",
      "Iteration 76002, loss = 1.24053795\n",
      "Iteration 76003, loss = 1.10923202\n",
      "Iteration 76004, loss = 1.03390645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 76005, loss = 1.06898312\n",
      "Iteration 76006, loss = 1.20041887\n",
      "Iteration 76007, loss = 1.27243948\n",
      "Iteration 76008, loss = 1.07213621\n",
      "Iteration 76009, loss = 1.03095229\n",
      "Iteration 76010, loss = 1.13158377\n",
      "Iteration 76011, loss = 1.53780069\n",
      "Iteration 76012, loss = 1.62090183\n",
      "Iteration 76013, loss = 1.14577326\n",
      "Iteration 76014, loss = 1.13466151\n",
      "Iteration 76015, loss = 1.16025743\n",
      "Iteration 76016, loss = 1.39963982\n",
      "Iteration 76017, loss = 1.65288159\n",
      "Iteration 76018, loss = 1.62719128\n",
      "Iteration 76019, loss = 1.21551609\n",
      "Iteration 76020, loss = 1.35033123\n",
      "Iteration 76021, loss = 1.16370055\n",
      "Iteration 76022, loss = 1.24999301\n",
      "Iteration 76023, loss = 1.29189109\n",
      "Iteration 76024, loss = 1.67813088\n",
      "Iteration 76025, loss = 1.32552009\n",
      "Iteration 76026, loss = 1.16687720\n",
      "Iteration 76027, loss = 1.16870871\n",
      "Iteration 76028, loss = 1.01260776\n",
      "Iteration 76029, loss = 1.01016261\n",
      "Iteration 76030, loss = 0.99369849\n",
      "Iteration 76031, loss = 1.00276094\n",
      "Iteration 76032, loss = 0.96958431\n",
      "Iteration 76033, loss = 1.02691596\n",
      "Iteration 76034, loss = 0.99973841\n",
      "Iteration 76035, loss = 0.92459960\n",
      "Iteration 76036, loss = 0.97553996\n",
      "Iteration 76037, loss = 1.03241473\n",
      "Iteration 76038, loss = 1.10861180\n",
      "Iteration 76039, loss = 1.07479684\n",
      "Iteration 76040, loss = 0.94930324\n",
      "Iteration 76041, loss = 0.94165843\n",
      "Iteration 76042, loss = 1.20455006\n",
      "Iteration 76043, loss = 1.41150793\n",
      "Iteration 76044, loss = 1.24348270\n",
      "Iteration 76045, loss = 1.11735942\n",
      "Iteration 76046, loss = 1.20933309\n",
      "Iteration 76047, loss = 1.09922290\n",
      "Iteration 76048, loss = 0.99666873\n",
      "Iteration 76049, loss = 1.30768136\n",
      "Iteration 76050, loss = 1.37016400\n",
      "Iteration 76051, loss = 1.25027095\n",
      "Iteration 76052, loss = 1.13669947\n",
      "Iteration 76053, loss = 1.26372143\n",
      "Iteration 76054, loss = 1.24870078\n",
      "Iteration 76055, loss = 1.03248392\n",
      "Iteration 76056, loss = 1.13558890\n",
      "Iteration 76057, loss = 0.96773176\n",
      "Iteration 76058, loss = 0.96776251\n",
      "Iteration 76059, loss = 1.10187207\n",
      "Iteration 76060, loss = 0.92400128\n",
      "Iteration 76061, loss = 1.03664161\n",
      "Iteration 76062, loss = 1.00531532\n",
      "Iteration 76063, loss = 0.98810409\n",
      "Iteration 76064, loss = 1.02470611\n",
      "Iteration 76065, loss = 1.00671303\n",
      "Iteration 76066, loss = 1.32926450\n",
      "Iteration 76067, loss = 1.20993399\n",
      "Iteration 76068, loss = 1.13991519\n",
      "Iteration 76069, loss = 1.16847089\n",
      "Iteration 76070, loss = 2.37123898\n",
      "Iteration 76071, loss = 3.03231641\n",
      "Iteration 76072, loss = 1.99029379\n",
      "Iteration 76073, loss = 1.84089837\n",
      "Iteration 76074, loss = 1.28856434\n",
      "Iteration 76075, loss = 1.22251366\n",
      "Iteration 76076, loss = 1.08905793\n",
      "Iteration 76077, loss = 0.98899524\n",
      "Iteration 76078, loss = 1.01245350\n",
      "Iteration 76079, loss = 1.12545726\n",
      "Iteration 76080, loss = 1.15473294\n",
      "Iteration 76081, loss = 1.24162812\n",
      "Iteration 76082, loss = 1.14745489\n",
      "Iteration 76083, loss = 1.43267956\n",
      "Iteration 76084, loss = 1.38458739\n",
      "Iteration 76085, loss = 1.36453033\n",
      "Iteration 76086, loss = 1.23848734\n",
      "Iteration 76087, loss = 1.34447570\n",
      "Iteration 76088, loss = 1.22988984\n",
      "Iteration 76089, loss = 1.39158993\n",
      "Iteration 76090, loss = 1.16917732\n",
      "Iteration 76091, loss = 1.26217184\n",
      "Iteration 76092, loss = 1.13328109\n",
      "Iteration 76093, loss = 1.12527046\n",
      "Iteration 76094, loss = 1.03924230\n",
      "Iteration 76095, loss = 1.01012744\n",
      "Iteration 76096, loss = 1.16347691\n",
      "Iteration 76097, loss = 1.25907690\n",
      "Iteration 76098, loss = 1.10433471\n",
      "Iteration 76099, loss = 1.14031407\n",
      "Iteration 76100, loss = 1.09339465\n",
      "Iteration 76101, loss = 1.28175362\n",
      "Iteration 76102, loss = 1.04976392\n",
      "Iteration 76103, loss = 0.99557675\n",
      "Iteration 76104, loss = 1.20128886\n",
      "Iteration 76105, loss = 1.18302939\n",
      "Iteration 76106, loss = 1.34472884\n",
      "Iteration 76107, loss = 1.10406463\n",
      "Iteration 76108, loss = 1.23997348\n",
      "Iteration 76109, loss = 1.17630793\n",
      "Iteration 76110, loss = 1.15839357\n",
      "Iteration 76111, loss = 1.37744066\n",
      "Iteration 76112, loss = 1.42437768\n",
      "Iteration 76113, loss = 1.78984526\n",
      "Iteration 76114, loss = 1.80479394\n",
      "Iteration 76115, loss = 1.57753174\n",
      "Iteration 76116, loss = 1.58377726\n",
      "Iteration 76117, loss = 1.55399399\n",
      "Iteration 76118, loss = 1.58580048\n",
      "Iteration 76119, loss = 1.40899962\n",
      "Iteration 76120, loss = 1.34486069\n",
      "Iteration 76121, loss = 1.51493029\n",
      "Iteration 76122, loss = 1.54881540\n",
      "Iteration 76123, loss = 1.98950528\n",
      "Iteration 76124, loss = 1.77505328\n",
      "Iteration 76125, loss = 1.58621750\n",
      "Iteration 76126, loss = 1.54995577\n",
      "Iteration 76127, loss = 2.34053957\n",
      "Iteration 76128, loss = 1.86706430\n",
      "Iteration 76129, loss = 1.63344014\n",
      "Iteration 76130, loss = 1.80032745\n",
      "Iteration 76131, loss = 1.60350408\n",
      "Iteration 76132, loss = 1.37286398\n",
      "Iteration 76133, loss = 1.19257969\n",
      "Iteration 76134, loss = 1.21757118\n",
      "Iteration 76135, loss = 1.19633552\n",
      "Iteration 76136, loss = 1.13092666\n",
      "Iteration 76137, loss = 1.12018625\n",
      "Iteration 76138, loss = 1.27151006\n",
      "Iteration 76139, loss = 1.43273490\n",
      "Iteration 76140, loss = 1.21757135\n",
      "Iteration 76141, loss = 1.10191481\n",
      "Iteration 76142, loss = 1.26208235\n",
      "Iteration 76143, loss = 1.13535384\n",
      "Iteration 76144, loss = 1.05467907\n",
      "Iteration 76145, loss = 1.12862170\n",
      "Iteration 76146, loss = 1.11616681\n",
      "Iteration 76147, loss = 1.28351722\n",
      "Iteration 76148, loss = 1.50774630\n",
      "Iteration 76149, loss = 1.34716240\n",
      "Iteration 76150, loss = 1.59583112\n",
      "Iteration 76151, loss = 1.40870878\n",
      "Iteration 76152, loss = 1.12772565\n",
      "Iteration 76153, loss = 1.02229299\n",
      "Iteration 76154, loss = 1.13513577\n",
      "Iteration 76155, loss = 1.05015104\n",
      "Iteration 76156, loss = 1.01703960\n",
      "Iteration 76157, loss = 1.12027289\n",
      "Iteration 76158, loss = 1.04434315\n",
      "Iteration 76159, loss = 1.01564462\n",
      "Iteration 76160, loss = 1.05835746\n",
      "Iteration 76161, loss = 1.15627205\n",
      "Iteration 76162, loss = 1.48255260\n",
      "Iteration 76163, loss = 1.30022183\n",
      "Iteration 76164, loss = 1.19626203\n",
      "Iteration 76165, loss = 1.22547514\n",
      "Iteration 76166, loss = 1.04419316\n",
      "Iteration 76167, loss = 1.10083554\n",
      "Iteration 76168, loss = 1.49727741\n",
      "Iteration 76169, loss = 1.57940003\n",
      "Iteration 76170, loss = 1.58132182\n",
      "Iteration 76171, loss = 1.42626811\n",
      "Iteration 76172, loss = 1.18841340\n",
      "Iteration 76173, loss = 1.03365621\n",
      "Iteration 76174, loss = 0.96032658\n",
      "Iteration 76175, loss = 1.00935773\n",
      "Iteration 76176, loss = 1.05881381\n",
      "Iteration 76177, loss = 0.97731570\n",
      "Iteration 76178, loss = 1.04758338\n",
      "Iteration 76179, loss = 1.12225445\n",
      "Iteration 76180, loss = 1.09206243\n",
      "Iteration 76181, loss = 1.25329709\n",
      "Iteration 76182, loss = 1.11212512\n",
      "Iteration 76183, loss = 1.22944617\n",
      "Iteration 76184, loss = 1.02906058\n",
      "Iteration 76185, loss = 0.99632936\n",
      "Iteration 76186, loss = 0.93419626\n",
      "Iteration 76187, loss = 0.94497923\n",
      "Iteration 76188, loss = 1.04520573\n",
      "Iteration 76189, loss = 1.10166238\n",
      "Iteration 76190, loss = 1.20897027\n",
      "Iteration 76191, loss = 1.82524534\n",
      "Iteration 76192, loss = 1.70532899\n",
      "Iteration 76193, loss = 1.64646204\n",
      "Iteration 76194, loss = 1.29444434\n",
      "Iteration 76195, loss = 1.25319176\n",
      "Iteration 76196, loss = 1.15083484\n",
      "Iteration 76197, loss = 1.45935066\n",
      "Iteration 76198, loss = 1.43936557\n",
      "Iteration 76199, loss = 1.15734804\n",
      "Iteration 76200, loss = 1.09066861\n",
      "Iteration 76201, loss = 1.16014783\n",
      "Iteration 76202, loss = 1.22744782\n",
      "Iteration 76203, loss = 1.20122130\n",
      "Iteration 76204, loss = 1.09185901\n",
      "Iteration 76205, loss = 1.26494471\n",
      "Iteration 76206, loss = 1.78306232\n",
      "Iteration 76207, loss = 1.86256686\n",
      "Iteration 76208, loss = 1.86834973\n",
      "Iteration 76209, loss = 1.62308035\n",
      "Iteration 76210, loss = 1.48176574\n",
      "Iteration 76211, loss = 1.44282915\n",
      "Iteration 76212, loss = 1.40388768\n",
      "Iteration 76213, loss = 1.20450192\n",
      "Iteration 76214, loss = 1.09760383\n",
      "Iteration 76215, loss = 1.11512022\n",
      "Iteration 76216, loss = 1.11978889\n",
      "Iteration 76217, loss = 1.11134154\n",
      "Iteration 76218, loss = 1.03662409\n",
      "Iteration 76219, loss = 1.04999910\n",
      "Iteration 76220, loss = 0.99899538\n",
      "Iteration 76221, loss = 1.03066572\n",
      "Iteration 76222, loss = 1.10977429\n",
      "Iteration 76223, loss = 1.13702610\n",
      "Iteration 76224, loss = 1.03068951\n",
      "Iteration 76225, loss = 1.01175355\n",
      "Iteration 76226, loss = 1.00053701\n",
      "Iteration 76227, loss = 0.93706888\n",
      "Iteration 76228, loss = 1.12268684\n",
      "Iteration 76229, loss = 1.39226441\n",
      "Iteration 76230, loss = 1.35916882\n",
      "Iteration 76231, loss = 1.18118301\n",
      "Iteration 76232, loss = 1.41407183\n",
      "Iteration 76233, loss = 2.40416693\n",
      "Iteration 76234, loss = 2.60102165\n",
      "Iteration 76235, loss = 1.45237166\n",
      "Iteration 76236, loss = 1.77813897\n",
      "Iteration 76237, loss = 1.24045473\n",
      "Iteration 76238, loss = 1.04514943\n",
      "Iteration 76239, loss = 1.10846995\n",
      "Iteration 76240, loss = 1.20924355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 76241, loss = 1.16615955\n",
      "Iteration 76242, loss = 1.16928702\n",
      "Iteration 76243, loss = 1.09912980\n",
      "Iteration 76244, loss = 1.05003993\n",
      "Iteration 76245, loss = 0.99856330\n",
      "Iteration 76246, loss = 1.03771285\n",
      "Iteration 76247, loss = 1.01677857\n",
      "Iteration 76248, loss = 1.29025194\n",
      "Iteration 76249, loss = 1.83325725\n",
      "Iteration 76250, loss = 2.07935210\n",
      "Iteration 76251, loss = 1.92491057\n",
      "Iteration 76252, loss = 1.59972704\n",
      "Iteration 76253, loss = 1.57501103\n",
      "Iteration 76254, loss = 1.91975347\n",
      "Iteration 76255, loss = 2.07058672\n",
      "Iteration 76256, loss = 1.91317720\n",
      "Iteration 76257, loss = 1.61800589\n",
      "Iteration 76258, loss = 1.59696408\n",
      "Iteration 76259, loss = 1.34516261\n",
      "Iteration 76260, loss = 1.53394859\n",
      "Iteration 76261, loss = 1.39966408\n",
      "Iteration 76262, loss = 1.36763765\n",
      "Iteration 76263, loss = 1.44895469\n",
      "Iteration 76264, loss = 1.13668940\n",
      "Iteration 76265, loss = 1.31001422\n",
      "Iteration 76266, loss = 1.42249976\n",
      "Iteration 76267, loss = 1.31792605\n",
      "Iteration 76268, loss = 1.17708068\n",
      "Iteration 76269, loss = 1.21278403\n",
      "Iteration 76270, loss = 1.10939473\n",
      "Iteration 76271, loss = 1.03083985\n",
      "Iteration 76272, loss = 0.97071555\n",
      "Iteration 76273, loss = 1.04656656\n",
      "Iteration 76274, loss = 0.96990766\n",
      "Iteration 76275, loss = 1.03969450\n",
      "Iteration 76276, loss = 1.06091066\n",
      "Iteration 76277, loss = 1.01022537\n",
      "Iteration 76278, loss = 0.89981873\n",
      "Iteration 76279, loss = 1.63670194\n",
      "Iteration 76280, loss = 2.19739272\n",
      "Iteration 76281, loss = 1.58553342\n",
      "Iteration 76282, loss = 1.50971625\n",
      "Iteration 76283, loss = 1.36170194\n",
      "Iteration 76284, loss = 1.21221913\n",
      "Iteration 76285, loss = 1.15223696\n",
      "Iteration 76286, loss = 1.13527146\n",
      "Iteration 76287, loss = 1.11109305\n",
      "Iteration 76288, loss = 1.39779436\n",
      "Iteration 76289, loss = 1.02201336\n",
      "Iteration 76290, loss = 1.03934048\n",
      "Iteration 76291, loss = 1.06761944\n",
      "Iteration 76292, loss = 1.14977312\n",
      "Iteration 76293, loss = 1.00667837\n",
      "Iteration 76294, loss = 1.09757531\n",
      "Iteration 76295, loss = 1.30159630\n",
      "Iteration 76296, loss = 1.57511774\n",
      "Iteration 76297, loss = 1.51702824\n",
      "Iteration 76298, loss = 1.87033845\n",
      "Iteration 76299, loss = 1.47545698\n",
      "Iteration 76300, loss = 1.88286409\n",
      "Iteration 76301, loss = 1.42397367\n",
      "Iteration 76302, loss = 1.77517107\n",
      "Iteration 76303, loss = 1.92278097\n",
      "Iteration 76304, loss = 1.94448925\n",
      "Iteration 76305, loss = 1.93707634\n",
      "Iteration 76306, loss = 2.32652063\n",
      "Iteration 76307, loss = 3.10360512\n",
      "Iteration 76308, loss = 1.74475148\n",
      "Iteration 76309, loss = 1.68532599\n",
      "Iteration 76310, loss = 1.46194136\n",
      "Iteration 76311, loss = 1.15732881\n",
      "Iteration 76312, loss = 1.06355627\n",
      "Iteration 76313, loss = 1.07290795\n",
      "Iteration 76314, loss = 1.01202371\n",
      "Iteration 76315, loss = 1.10778597\n",
      "Iteration 76316, loss = 1.11922240\n",
      "Iteration 76317, loss = 1.12093357\n",
      "Iteration 76318, loss = 1.17114401\n",
      "Iteration 76319, loss = 1.30239716\n",
      "Iteration 76320, loss = 1.09024100\n",
      "Iteration 76321, loss = 1.02109972\n",
      "Iteration 76322, loss = 0.95917349\n",
      "Iteration 76323, loss = 1.01604215\n",
      "Iteration 76324, loss = 0.98300829\n",
      "Iteration 76325, loss = 0.94651185\n",
      "Iteration 76326, loss = 0.95051811\n",
      "Iteration 76327, loss = 1.07476082\n",
      "Iteration 76328, loss = 1.04833315\n",
      "Iteration 76329, loss = 0.93804152\n",
      "Iteration 76330, loss = 0.94054031\n",
      "Iteration 76331, loss = 1.20351133\n",
      "Iteration 76332, loss = 1.07012694\n",
      "Iteration 76333, loss = 1.11900275\n",
      "Iteration 76334, loss = 1.03768884\n",
      "Iteration 76335, loss = 1.19177660\n",
      "Iteration 76336, loss = 1.32958617\n",
      "Iteration 76337, loss = 1.32842300\n",
      "Iteration 76338, loss = 1.18197906\n",
      "Iteration 76339, loss = 1.37578773\n",
      "Iteration 76340, loss = 1.20451621\n",
      "Iteration 76341, loss = 1.19685697\n",
      "Iteration 76342, loss = 1.45397819\n",
      "Iteration 76343, loss = 1.72218424\n",
      "Iteration 76344, loss = 1.62712693\n",
      "Iteration 76345, loss = 1.95841378\n",
      "Iteration 76346, loss = 1.79732779\n",
      "Iteration 76347, loss = 1.43422293\n",
      "Iteration 76348, loss = 1.13659575\n",
      "Iteration 76349, loss = 1.00521540\n",
      "Iteration 76350, loss = 0.99451560\n",
      "Iteration 76351, loss = 1.01728445\n",
      "Iteration 76352, loss = 1.07179102\n",
      "Iteration 76353, loss = 1.20029228\n",
      "Iteration 76354, loss = 1.49199371\n",
      "Iteration 76355, loss = 1.40577079\n",
      "Iteration 76356, loss = 1.25814835\n",
      "Iteration 76357, loss = 1.07121104\n",
      "Iteration 76358, loss = 1.00430421\n",
      "Iteration 76359, loss = 1.08088019\n",
      "Iteration 76360, loss = 1.09761702\n",
      "Iteration 76361, loss = 1.04255824\n",
      "Iteration 76362, loss = 1.21305751\n",
      "Iteration 76363, loss = 1.26512131\n",
      "Iteration 76364, loss = 1.18293111\n",
      "Iteration 76365, loss = 1.28988848\n",
      "Iteration 76366, loss = 1.36153486\n",
      "Iteration 76367, loss = 1.27688561\n",
      "Iteration 76368, loss = 1.30733363\n",
      "Iteration 76369, loss = 1.20116972\n",
      "Iteration 76370, loss = 1.02203329\n",
      "Iteration 76371, loss = 1.00683731\n",
      "Iteration 76372, loss = 1.23168578\n",
      "Iteration 76373, loss = 1.15525171\n",
      "Iteration 76374, loss = 1.03021743\n",
      "Iteration 76375, loss = 1.06388898\n",
      "Iteration 76376, loss = 1.11643446\n",
      "Iteration 76377, loss = 0.99224189\n",
      "Iteration 76378, loss = 0.92290937\n",
      "Iteration 76379, loss = 1.03527027\n",
      "Iteration 76380, loss = 1.14466981\n",
      "Iteration 76381, loss = 1.13281986\n",
      "Iteration 76382, loss = 1.25223170\n",
      "Iteration 76383, loss = 1.17890069\n",
      "Iteration 76384, loss = 0.96308967\n",
      "Iteration 76385, loss = 1.11457774\n",
      "Iteration 76386, loss = 1.01184116\n",
      "Iteration 76387, loss = 1.00826833\n",
      "Iteration 76388, loss = 1.19973098\n",
      "Iteration 76389, loss = 1.18134181\n",
      "Iteration 76390, loss = 1.03484951\n",
      "Iteration 76391, loss = 1.03933817\n",
      "Iteration 76392, loss = 1.17811438\n",
      "Iteration 76393, loss = 1.12866231\n",
      "Iteration 76394, loss = 1.15354437\n",
      "Iteration 76395, loss = 1.09103650\n",
      "Iteration 76396, loss = 0.97576492\n",
      "Iteration 76397, loss = 1.01520412\n",
      "Iteration 76398, loss = 1.14145522\n",
      "Iteration 76399, loss = 1.34768436\n",
      "Iteration 76400, loss = 1.35469396\n",
      "Iteration 76401, loss = 1.14745576\n",
      "Iteration 76402, loss = 1.31790013\n",
      "Iteration 76403, loss = 1.20875129\n",
      "Iteration 76404, loss = 1.25157883\n",
      "Iteration 76405, loss = 1.24049493\n",
      "Iteration 76406, loss = 1.16461653\n",
      "Iteration 76407, loss = 1.28620892\n",
      "Iteration 76408, loss = 1.21586356\n",
      "Iteration 76409, loss = 1.46240536\n",
      "Iteration 76410, loss = 1.19196820\n",
      "Iteration 76411, loss = 1.39548914\n",
      "Iteration 76412, loss = 1.29038960\n",
      "Iteration 76413, loss = 1.19637648\n",
      "Iteration 76414, loss = 1.28231481\n",
      "Iteration 76415, loss = 1.17928088\n",
      "Iteration 76416, loss = 1.42845077\n",
      "Iteration 76417, loss = 1.30347281\n",
      "Iteration 76418, loss = 1.44054676\n",
      "Iteration 76419, loss = 1.51501232\n",
      "Iteration 76420, loss = 1.58709720\n",
      "Iteration 76421, loss = 1.50484561\n",
      "Iteration 76422, loss = 1.67878861\n",
      "Iteration 76423, loss = 1.41057807\n",
      "Iteration 76424, loss = 1.14452816\n",
      "Iteration 76425, loss = 1.37622272\n",
      "Iteration 76426, loss = 1.67660254\n",
      "Iteration 76427, loss = 1.63423079\n",
      "Iteration 76428, loss = 1.98093956\n",
      "Iteration 76429, loss = 1.63879329\n",
      "Iteration 76430, loss = 1.78898367\n",
      "Iteration 76431, loss = 1.38255378\n",
      "Iteration 76432, loss = 1.63859683\n",
      "Iteration 76433, loss = 1.53979863\n",
      "Iteration 76434, loss = 1.42056092\n",
      "Iteration 76435, loss = 1.13489948\n",
      "Iteration 76436, loss = 1.34026084\n",
      "Iteration 76437, loss = 1.49689069\n",
      "Iteration 76438, loss = 1.20696611\n",
      "Iteration 76439, loss = 1.23065865\n",
      "Iteration 76440, loss = 1.16894091\n",
      "Iteration 76441, loss = 1.49122873\n",
      "Iteration 76442, loss = 1.26494196\n",
      "Iteration 76443, loss = 1.19419025\n",
      "Iteration 76444, loss = 1.09221948\n",
      "Iteration 76445, loss = 1.01335409\n",
      "Iteration 76446, loss = 1.17810149\n",
      "Iteration 76447, loss = 1.23851738\n",
      "Iteration 76448, loss = 1.52985590\n",
      "Iteration 76449, loss = 1.42250961\n",
      "Iteration 76450, loss = 1.12107365\n",
      "Iteration 76451, loss = 1.13939706\n",
      "Iteration 76452, loss = 1.16567821\n",
      "Iteration 76453, loss = 1.13032162\n",
      "Iteration 76454, loss = 1.05291875\n",
      "Iteration 76455, loss = 1.17300318\n",
      "Iteration 76456, loss = 1.12347907\n",
      "Iteration 76457, loss = 1.01833764\n",
      "Iteration 76458, loss = 1.11995019\n",
      "Iteration 76459, loss = 0.97096890\n",
      "Iteration 76460, loss = 0.96312312\n",
      "Iteration 76461, loss = 1.10507425\n",
      "Iteration 76462, loss = 1.35287902\n",
      "Iteration 76463, loss = 1.96164925\n",
      "Iteration 76464, loss = 2.30147553\n",
      "Iteration 76465, loss = 2.45670752\n",
      "Iteration 76466, loss = 2.34261641\n",
      "Iteration 76467, loss = 2.00977983\n",
      "Iteration 76468, loss = 1.85801990\n",
      "Iteration 76469, loss = 1.49905636\n",
      "Iteration 76470, loss = 1.20242276\n",
      "Iteration 76471, loss = 1.04595181\n",
      "Iteration 76472, loss = 0.95990080\n",
      "Iteration 76473, loss = 1.24187546\n",
      "Iteration 76474, loss = 1.30061176\n",
      "Iteration 76475, loss = 1.05936558\n",
      "Iteration 76476, loss = 1.31855592\n",
      "Iteration 76477, loss = 1.57001516\n",
      "Iteration 76478, loss = 1.55041176\n",
      "Iteration 76479, loss = 1.28040607\n",
      "Iteration 76480, loss = 1.28552431\n",
      "Iteration 76481, loss = 1.23242175\n",
      "Iteration 76482, loss = 1.52718475\n",
      "Iteration 76483, loss = 1.34216701\n",
      "Iteration 76484, loss = 1.29172219\n",
      "Iteration 76485, loss = 1.31945109\n",
      "Iteration 76486, loss = 1.17352885\n",
      "Iteration 76487, loss = 1.31506976\n",
      "Iteration 76488, loss = 1.29853737\n",
      "Iteration 76489, loss = 1.16147977\n",
      "Iteration 76490, loss = 1.20920485\n",
      "Iteration 76491, loss = 1.19300111\n",
      "Iteration 76492, loss = 1.29943795\n",
      "Iteration 76493, loss = 1.31315509\n",
      "Iteration 76494, loss = 1.37529080\n",
      "Iteration 76495, loss = 1.26788900\n",
      "Iteration 76496, loss = 1.30454531\n",
      "Iteration 76497, loss = 1.18463253\n",
      "Iteration 76498, loss = 1.10454585\n",
      "Iteration 76499, loss = 1.16064558\n",
      "Iteration 76500, loss = 1.15762320\n",
      "Iteration 76501, loss = 1.50100234\n",
      "Iteration 76502, loss = 1.38216075\n",
      "Iteration 76503, loss = 1.27253368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 76504, loss = 1.25500321\n",
      "Iteration 76505, loss = 0.97554282\n",
      "Iteration 76506, loss = 1.26156075\n",
      "Iteration 76507, loss = 1.24399363\n",
      "Iteration 76508, loss = 1.26447892\n",
      "Iteration 76509, loss = 1.17006325\n",
      "Iteration 76510, loss = 1.05152182\n",
      "Iteration 76511, loss = 1.35721250\n",
      "Iteration 76512, loss = 1.52193968\n",
      "Iteration 76513, loss = 1.22215671\n",
      "Iteration 76514, loss = 1.07946732\n",
      "Iteration 76515, loss = 1.01290664\n",
      "Iteration 76516, loss = 0.94588227\n",
      "Iteration 76517, loss = 0.98967716\n",
      "Iteration 76518, loss = 0.98837277\n",
      "Iteration 76519, loss = 1.23400791\n",
      "Iteration 76520, loss = 1.21873779\n",
      "Iteration 76521, loss = 1.25287657\n",
      "Iteration 76522, loss = 1.03163951\n",
      "Iteration 76523, loss = 1.32118602\n",
      "Iteration 76524, loss = 1.41068913\n",
      "Iteration 76525, loss = 1.38763580\n",
      "Iteration 76526, loss = 1.39029218\n",
      "Iteration 76527, loss = 1.34884408\n",
      "Iteration 76528, loss = 1.27403719\n",
      "Iteration 76529, loss = 1.24708137\n",
      "Iteration 76530, loss = 1.11622152\n",
      "Iteration 76531, loss = 1.12247381\n",
      "Iteration 76532, loss = 1.12906130\n",
      "Iteration 76533, loss = 1.08199556\n",
      "Iteration 76534, loss = 1.10634431\n",
      "Iteration 76535, loss = 1.37919886\n",
      "Iteration 76536, loss = 1.25273809\n",
      "Iteration 76537, loss = 1.31262164\n",
      "Iteration 76538, loss = 1.17104812\n",
      "Iteration 76539, loss = 1.24792449\n",
      "Iteration 76540, loss = 1.11041261\n",
      "Iteration 76541, loss = 1.05642357\n",
      "Iteration 76542, loss = 1.04014103\n",
      "Iteration 76543, loss = 0.98517775\n",
      "Iteration 76544, loss = 0.99111535\n",
      "Iteration 76545, loss = 1.02548699\n",
      "Iteration 76546, loss = 0.97113879\n",
      "Iteration 76547, loss = 1.01088596\n",
      "Iteration 76548, loss = 0.93977402\n",
      "Iteration 76549, loss = 0.99779241\n",
      "Iteration 76550, loss = 1.15854024\n",
      "Iteration 76551, loss = 1.93126051\n",
      "Iteration 76552, loss = 1.88801798\n",
      "Iteration 76553, loss = 2.14875831\n",
      "Iteration 76554, loss = 2.76196967\n",
      "Iteration 76555, loss = 2.50601453\n",
      "Iteration 76556, loss = 2.08043790\n",
      "Iteration 76557, loss = 2.62477192\n",
      "Iteration 76558, loss = 1.84329551\n",
      "Iteration 76559, loss = 2.21860787\n",
      "Iteration 76560, loss = 1.80925808\n",
      "Iteration 76561, loss = 1.71823976\n",
      "Iteration 76562, loss = 2.30124775\n",
      "Iteration 76563, loss = 1.90089882\n",
      "Iteration 76564, loss = 1.20791301\n",
      "Iteration 76565, loss = 1.26500110\n",
      "Iteration 76566, loss = 1.52017051\n",
      "Iteration 76567, loss = 1.23223983\n",
      "Iteration 76568, loss = 1.30567435\n",
      "Iteration 76569, loss = 1.16401215\n",
      "Iteration 76570, loss = 1.50036202\n",
      "Iteration 76571, loss = 1.31481108\n",
      "Iteration 76572, loss = 1.13526486\n",
      "Iteration 76573, loss = 1.02700358\n",
      "Iteration 76574, loss = 1.13245811\n",
      "Iteration 76575, loss = 1.09691708\n",
      "Iteration 76576, loss = 1.12988145\n",
      "Iteration 76577, loss = 1.17174702\n",
      "Iteration 76578, loss = 1.30809637\n",
      "Iteration 76579, loss = 1.14903773\n",
      "Iteration 76580, loss = 1.44849714\n",
      "Iteration 76581, loss = 1.25348379\n",
      "Iteration 76582, loss = 1.37151479\n",
      "Iteration 76583, loss = 1.37140463\n",
      "Iteration 76584, loss = 1.15193618\n",
      "Iteration 76585, loss = 1.05186427\n",
      "Iteration 76586, loss = 1.02354560\n",
      "Iteration 76587, loss = 1.06262766\n",
      "Iteration 76588, loss = 1.07071936\n",
      "Iteration 76589, loss = 1.06988311\n",
      "Iteration 76590, loss = 1.20177043\n",
      "Iteration 76591, loss = 1.28895322\n",
      "Iteration 76592, loss = 1.22776713\n",
      "Iteration 76593, loss = 1.26484864\n",
      "Iteration 76594, loss = 1.57587708\n",
      "Iteration 76595, loss = 1.49465770\n",
      "Iteration 76596, loss = 1.22268598\n",
      "Iteration 76597, loss = 1.11438715\n",
      "Iteration 76598, loss = 0.99494155\n",
      "Iteration 76599, loss = 1.05726648\n",
      "Iteration 76600, loss = 1.11018663\n",
      "Iteration 76601, loss = 1.20273352\n",
      "Iteration 76602, loss = 1.19836482\n",
      "Iteration 76603, loss = 1.21146515\n",
      "Iteration 76604, loss = 1.04015149\n",
      "Iteration 76605, loss = 1.30510253\n",
      "Iteration 76606, loss = 1.07866462\n",
      "Iteration 76607, loss = 1.07486183\n",
      "Iteration 76608, loss = 1.00030195\n",
      "Iteration 76609, loss = 1.11295681\n",
      "Iteration 76610, loss = 1.29809453\n",
      "Iteration 76611, loss = 1.51362880\n",
      "Iteration 76612, loss = 1.30138111\n",
      "Iteration 76613, loss = 1.11162876\n",
      "Iteration 76614, loss = 0.99564359\n",
      "Iteration 76615, loss = 0.99337854\n",
      "Iteration 76616, loss = 1.02117136\n",
      "Iteration 76617, loss = 1.01460852\n",
      "Iteration 76618, loss = 1.05549819\n",
      "Iteration 76619, loss = 1.33740696\n",
      "Iteration 76620, loss = 1.50736637\n",
      "Iteration 76621, loss = 1.78536359\n",
      "Iteration 76622, loss = 1.52423331\n",
      "Iteration 76623, loss = 1.41221767\n",
      "Iteration 76624, loss = 1.32005622\n",
      "Iteration 76625, loss = 1.13021736\n",
      "Iteration 76626, loss = 1.26401251\n",
      "Iteration 76627, loss = 1.29873476\n",
      "Iteration 76628, loss = 1.37164225\n",
      "Iteration 76629, loss = 1.52961327\n",
      "Iteration 76630, loss = 1.78034036\n",
      "Iteration 76631, loss = 2.36159318\n",
      "Iteration 76632, loss = 1.88437218\n",
      "Iteration 76633, loss = 1.62349660\n",
      "Iteration 76634, loss = 1.70421978\n",
      "Iteration 76635, loss = 1.91744196\n",
      "Iteration 76636, loss = 1.97703109\n",
      "Iteration 76637, loss = 1.87496563\n",
      "Iteration 76638, loss = 1.68787813\n",
      "Iteration 76639, loss = 1.42148596\n",
      "Iteration 76640, loss = 1.40295729\n",
      "Iteration 76641, loss = 1.39430461\n",
      "Iteration 76642, loss = 1.17779080\n",
      "Iteration 76643, loss = 1.40309272\n",
      "Iteration 76644, loss = 1.95459332\n",
      "Iteration 76645, loss = 1.82190942\n",
      "Iteration 76646, loss = 1.26257855\n",
      "Iteration 76647, loss = 1.26047875\n",
      "Iteration 76648, loss = 1.04851196\n",
      "Iteration 76649, loss = 1.08791239\n",
      "Iteration 76650, loss = 0.98661557\n",
      "Iteration 76651, loss = 1.17984755\n",
      "Iteration 76652, loss = 1.28538685\n",
      "Iteration 76653, loss = 1.09337050\n",
      "Iteration 76654, loss = 0.99885067\n",
      "Iteration 76655, loss = 1.08371538\n",
      "Iteration 76656, loss = 1.03645431\n",
      "Iteration 76657, loss = 1.00503840\n",
      "Iteration 76658, loss = 1.11583198\n",
      "Iteration 76659, loss = 1.38459113\n",
      "Iteration 76660, loss = 1.52448455\n",
      "Iteration 76661, loss = 1.21169303\n",
      "Iteration 76662, loss = 1.46142418\n",
      "Iteration 76663, loss = 1.34076231\n",
      "Iteration 76664, loss = 1.41800814\n",
      "Iteration 76665, loss = 1.10975594\n",
      "Iteration 76666, loss = 1.04216417\n",
      "Iteration 76667, loss = 1.14573702\n",
      "Iteration 76668, loss = 1.09901175\n",
      "Iteration 76669, loss = 1.27482918\n",
      "Iteration 76670, loss = 1.15755586\n",
      "Iteration 76671, loss = 1.25800337\n",
      "Iteration 76672, loss = 1.24043864\n",
      "Iteration 76673, loss = 1.03474521\n",
      "Iteration 76674, loss = 1.04271396\n",
      "Iteration 76675, loss = 0.99253325\n",
      "Iteration 76676, loss = 0.97514488\n",
      "Iteration 76677, loss = 1.11295035\n",
      "Iteration 76678, loss = 1.02782893\n",
      "Iteration 76679, loss = 1.00483048\n",
      "Iteration 76680, loss = 0.90378235\n",
      "Iteration 76681, loss = 0.97833572\n",
      "Iteration 76682, loss = 0.98799240\n",
      "Iteration 76683, loss = 0.96317702\n",
      "Iteration 76684, loss = 1.15557120\n",
      "Iteration 76685, loss = 1.12317792\n",
      "Iteration 76686, loss = 1.10451038\n",
      "Iteration 76687, loss = 0.97444288\n",
      "Iteration 76688, loss = 1.09135721\n",
      "Iteration 76689, loss = 1.05637089\n",
      "Iteration 76690, loss = 1.06994339\n",
      "Iteration 76691, loss = 1.10123161\n",
      "Iteration 76692, loss = 1.12300880\n",
      "Iteration 76693, loss = 1.13895792\n",
      "Iteration 76694, loss = 0.98868425\n",
      "Iteration 76695, loss = 1.20309282\n",
      "Iteration 76696, loss = 1.36164308\n",
      "Iteration 76697, loss = 1.21516091\n",
      "Iteration 76698, loss = 1.32921662\n",
      "Iteration 76699, loss = 1.04071062\n",
      "Iteration 76700, loss = 1.01794769\n",
      "Iteration 76701, loss = 1.04080845\n",
      "Iteration 76702, loss = 1.24221086\n",
      "Iteration 76703, loss = 1.39140584\n",
      "Iteration 76704, loss = 1.16348775\n",
      "Iteration 76705, loss = 1.25960818\n",
      "Iteration 76706, loss = 1.22983452\n",
      "Iteration 76707, loss = 1.40357513\n",
      "Iteration 76708, loss = 1.21233488\n",
      "Iteration 76709, loss = 1.40069071\n",
      "Iteration 76710, loss = 1.32506316\n",
      "Iteration 76711, loss = 1.19997958\n",
      "Iteration 76712, loss = 1.16277456\n",
      "Iteration 76713, loss = 1.11887755\n",
      "Iteration 76714, loss = 1.13573407\n",
      "Iteration 76715, loss = 1.05365695\n",
      "Iteration 76716, loss = 0.93414795\n",
      "Iteration 76717, loss = 0.96851421\n",
      "Iteration 76718, loss = 1.09443831\n",
      "Iteration 76719, loss = 1.09695639\n",
      "Iteration 76720, loss = 1.25189791\n",
      "Iteration 76721, loss = 1.35115413\n",
      "Iteration 76722, loss = 1.22099955\n",
      "Iteration 76723, loss = 1.09437595\n",
      "Iteration 76724, loss = 0.96402791\n",
      "Iteration 76725, loss = 1.02169560\n",
      "Iteration 76726, loss = 1.05349543\n",
      "Iteration 76727, loss = 1.26313196\n",
      "Iteration 76728, loss = 1.20455220\n",
      "Iteration 76729, loss = 1.34074015\n",
      "Iteration 76730, loss = 1.29027676\n",
      "Iteration 76731, loss = 1.45042120\n",
      "Iteration 76732, loss = 2.41442094\n",
      "Iteration 76733, loss = 1.98576919\n",
      "Iteration 76734, loss = 1.48129440\n",
      "Iteration 76735, loss = 1.44839116\n",
      "Iteration 76736, loss = 1.32502212\n",
      "Iteration 76737, loss = 1.86329480\n",
      "Iteration 76738, loss = 1.45645277\n",
      "Iteration 76739, loss = 1.53842837\n",
      "Iteration 76740, loss = 1.23310082\n",
      "Iteration 76741, loss = 1.33388651\n",
      "Iteration 76742, loss = 1.04148548\n",
      "Iteration 76743, loss = 1.37935703\n",
      "Iteration 76744, loss = 1.33271244\n",
      "Iteration 76745, loss = 1.24520852\n",
      "Iteration 76746, loss = 1.08294308\n",
      "Iteration 76747, loss = 1.18407726\n",
      "Iteration 76748, loss = 1.67136525\n",
      "Iteration 76749, loss = 1.82963240\n",
      "Iteration 76750, loss = 2.03095480\n",
      "Iteration 76751, loss = 2.31732217\n",
      "Iteration 76752, loss = 1.99363236\n",
      "Iteration 76753, loss = 2.30503682\n",
      "Iteration 76754, loss = 2.62490528\n",
      "Iteration 76755, loss = 1.68469227\n",
      "Iteration 76756, loss = 1.76916050\n",
      "Iteration 76757, loss = 1.29701326\n",
      "Iteration 76758, loss = 1.42748681\n",
      "Iteration 76759, loss = 1.53713550\n",
      "Iteration 76760, loss = 1.59860255\n",
      "Iteration 76761, loss = 1.18528065\n",
      "Iteration 76762, loss = 1.07866139\n",
      "Iteration 76763, loss = 1.03368829\n",
      "Iteration 76764, loss = 1.06507367\n",
      "Iteration 76765, loss = 1.43283832\n",
      "Iteration 76766, loss = 1.31988016\n",
      "Iteration 76767, loss = 1.01251072\n",
      "Iteration 76768, loss = 1.10767547\n",
      "Iteration 76769, loss = 1.25058104\n",
      "Iteration 76770, loss = 0.99046529\n",
      "Iteration 76771, loss = 1.01650900\n",
      "Iteration 76772, loss = 1.03600377\n",
      "Iteration 76773, loss = 1.01287843\n",
      "Iteration 76774, loss = 1.22485684\n",
      "Iteration 76775, loss = 1.16608360\n",
      "Iteration 76776, loss = 1.05968808\n",
      "Iteration 76777, loss = 0.98487566\n",
      "Iteration 76778, loss = 1.02249785\n",
      "Iteration 76779, loss = 1.33374362\n",
      "Iteration 76780, loss = 1.06945369\n",
      "Iteration 76781, loss = 1.02158277\n",
      "Iteration 76782, loss = 0.96149329\n",
      "Iteration 76783, loss = 0.95382698\n",
      "Iteration 76784, loss = 0.98547315\n",
      "Iteration 76785, loss = 1.14075803\n",
      "Iteration 76786, loss = 1.13299482\n",
      "Iteration 76787, loss = 1.01794872\n",
      "Iteration 76788, loss = 0.97629597\n",
      "Iteration 76789, loss = 1.01036172\n",
      "Iteration 76790, loss = 1.16897444\n",
      "Iteration 76791, loss = 1.13636268\n",
      "Iteration 76792, loss = 1.01363240\n",
      "Iteration 76793, loss = 1.00530346\n",
      "Iteration 76794, loss = 1.01008005\n",
      "Iteration 76795, loss = 1.01477388\n",
      "Iteration 76796, loss = 1.06505599\n",
      "Iteration 76797, loss = 1.11837451\n",
      "Iteration 76798, loss = 1.33673512\n",
      "Iteration 76799, loss = 1.17724651\n",
      "Iteration 76800, loss = 1.20581745\n",
      "Iteration 76801, loss = 1.14259756\n",
      "Iteration 76802, loss = 1.06267474\n",
      "Iteration 76803, loss = 1.05321202\n",
      "Iteration 76804, loss = 1.13149648\n",
      "Iteration 76805, loss = 1.31715817\n",
      "Iteration 76806, loss = 1.72470750\n",
      "Iteration 76807, loss = 2.31510085\n",
      "Iteration 76808, loss = 3.14312786\n",
      "Iteration 76809, loss = 2.56316935\n",
      "Iteration 76810, loss = 3.02745977\n",
      "Iteration 76811, loss = 2.40818339\n",
      "Iteration 76812, loss = 5.86423258\n",
      "Iteration 76813, loss = 6.08306287\n",
      "Iteration 76814, loss = 2.59840140\n",
      "Iteration 76815, loss = 2.69598725\n",
      "Iteration 76816, loss = 2.89352696\n",
      "Iteration 76817, loss = 2.41502425\n",
      "Iteration 76818, loss = 1.70164636\n",
      "Iteration 76819, loss = 1.84759858\n",
      "Iteration 76820, loss = 1.83569871\n",
      "Iteration 76821, loss = 1.57640706\n",
      "Iteration 76822, loss = 1.26286762\n",
      "Iteration 76823, loss = 1.09944823\n",
      "Iteration 76824, loss = 1.32884955\n",
      "Iteration 76825, loss = 1.23810832\n",
      "Iteration 76826, loss = 1.24688602\n",
      "Iteration 76827, loss = 1.05539044\n",
      "Iteration 76828, loss = 1.00971911\n",
      "Iteration 76829, loss = 1.10832310\n",
      "Iteration 76830, loss = 0.97103255\n",
      "Iteration 76831, loss = 0.94757774\n",
      "Iteration 76832, loss = 0.97467330\n",
      "Iteration 76833, loss = 0.97223196\n",
      "Iteration 76834, loss = 1.07606614\n",
      "Iteration 76835, loss = 0.95947460\n",
      "Iteration 76836, loss = 1.01451107\n",
      "Iteration 76837, loss = 0.99816058\n",
      "Iteration 76838, loss = 1.06044963\n",
      "Iteration 76839, loss = 1.04118697\n",
      "Iteration 76840, loss = 1.05939770\n",
      "Iteration 76841, loss = 1.12769283\n",
      "Iteration 76842, loss = 1.14113115\n",
      "Iteration 76843, loss = 1.22910974\n",
      "Iteration 76844, loss = 1.03009218\n",
      "Iteration 76845, loss = 1.08452431\n",
      "Iteration 76846, loss = 1.31526744\n",
      "Iteration 76847, loss = 1.15419158\n",
      "Iteration 76848, loss = 1.06409075\n",
      "Iteration 76849, loss = 1.03149369\n",
      "Iteration 76850, loss = 1.40937184\n",
      "Iteration 76851, loss = 1.26753928\n",
      "Iteration 76852, loss = 1.22340853\n",
      "Iteration 76853, loss = 1.25316368\n",
      "Iteration 76854, loss = 1.20789098\n",
      "Iteration 76855, loss = 1.24597824\n",
      "Iteration 76856, loss = 1.10577043\n",
      "Iteration 76857, loss = 1.06750799\n",
      "Iteration 76858, loss = 1.07517651\n",
      "Iteration 76859, loss = 1.21088285\n",
      "Iteration 76860, loss = 1.05944941\n",
      "Iteration 76861, loss = 1.10753579\n",
      "Iteration 76862, loss = 1.20043655\n",
      "Iteration 76863, loss = 0.99264015\n",
      "Iteration 76864, loss = 1.07467439\n",
      "Iteration 76865, loss = 1.14534895\n",
      "Iteration 76866, loss = 1.51677077\n",
      "Iteration 76867, loss = 1.58527513\n",
      "Iteration 76868, loss = 1.23386681\n",
      "Iteration 76869, loss = 1.15250567\n",
      "Iteration 76870, loss = 1.12124908\n",
      "Iteration 76871, loss = 1.24138551\n",
      "Iteration 76872, loss = 1.09089557\n",
      "Iteration 76873, loss = 1.12147429\n",
      "Iteration 76874, loss = 1.15605245\n",
      "Iteration 76875, loss = 1.16350704\n",
      "Iteration 76876, loss = 1.23355134\n",
      "Iteration 76877, loss = 1.74928567\n",
      "Iteration 76878, loss = 1.55217947\n",
      "Iteration 76879, loss = 1.51052230\n",
      "Iteration 76880, loss = 1.39458021\n",
      "Iteration 76881, loss = 1.17999350\n",
      "Iteration 76882, loss = 1.00076445\n",
      "Iteration 76883, loss = 1.41230074\n",
      "Iteration 76884, loss = 1.41117250\n",
      "Iteration 76885, loss = 1.35562119\n",
      "Iteration 76886, loss = 1.13455449\n",
      "Iteration 76887, loss = 1.41996811\n",
      "Iteration 76888, loss = 1.17001703\n",
      "Iteration 76889, loss = 1.17012285\n",
      "Iteration 76890, loss = 1.17828878\n",
      "Iteration 76891, loss = 1.06139954\n",
      "Iteration 76892, loss = 1.23115045\n",
      "Iteration 76893, loss = 1.14976792\n",
      "Iteration 76894, loss = 1.28419686\n",
      "Iteration 76895, loss = 1.29338448\n",
      "Iteration 76896, loss = 1.08531177\n",
      "Iteration 76897, loss = 1.05213127\n",
      "Iteration 76898, loss = 1.25302011\n",
      "Iteration 76899, loss = 1.22640890\n",
      "Iteration 76900, loss = 1.33627306\n",
      "Iteration 76901, loss = 1.30875106\n",
      "Iteration 76902, loss = 1.71022026\n",
      "Iteration 76903, loss = 1.53357538\n",
      "Iteration 76904, loss = 1.31207190\n",
      "Iteration 76905, loss = 1.17383497\n",
      "Iteration 76906, loss = 1.12585265\n",
      "Iteration 76907, loss = 1.09121405\n",
      "Iteration 76908, loss = 1.08329878\n",
      "Iteration 76909, loss = 1.26187913\n",
      "Iteration 76910, loss = 1.16219278\n",
      "Iteration 76911, loss = 1.09541923\n",
      "Iteration 76912, loss = 1.18822661\n",
      "Iteration 76913, loss = 1.09248318\n",
      "Iteration 76914, loss = 1.16007477\n",
      "Iteration 76915, loss = 1.11373414\n",
      "Iteration 76916, loss = 1.16713577\n",
      "Iteration 76917, loss = 1.19723645\n",
      "Iteration 76918, loss = 1.30182220\n",
      "Iteration 76919, loss = 1.51984301\n",
      "Iteration 76920, loss = 1.31108847\n",
      "Iteration 76921, loss = 1.16285670\n",
      "Iteration 76922, loss = 1.02908682\n",
      "Iteration 76923, loss = 1.10168080\n",
      "Iteration 76924, loss = 1.19536993\n",
      "Iteration 76925, loss = 1.30547459\n",
      "Iteration 76926, loss = 1.43557643\n",
      "Iteration 76927, loss = 1.26973184\n",
      "Iteration 76928, loss = 1.27698144\n",
      "Iteration 76929, loss = 1.74290579\n",
      "Iteration 76930, loss = 1.53521519\n",
      "Iteration 76931, loss = 1.34200720\n",
      "Iteration 76932, loss = 1.17215724\n",
      "Iteration 76933, loss = 1.04694085\n",
      "Iteration 76934, loss = 1.22251791\n",
      "Iteration 76935, loss = 1.49333116\n",
      "Iteration 76936, loss = 1.25338932\n",
      "Iteration 76937, loss = 1.43195050\n",
      "Iteration 76938, loss = 1.87991961\n",
      "Iteration 76939, loss = 2.02776993\n",
      "Iteration 76940, loss = 1.69639951\n",
      "Iteration 76941, loss = 1.59035204\n",
      "Iteration 76942, loss = 1.19600284\n",
      "Iteration 76943, loss = 1.06230416\n",
      "Iteration 76944, loss = 1.04307050\n",
      "Iteration 76945, loss = 1.24948638\n",
      "Iteration 76946, loss = 0.99374355\n",
      "Iteration 76947, loss = 1.15976687\n",
      "Iteration 76948, loss = 1.23974010\n",
      "Iteration 76949, loss = 0.98850248\n",
      "Iteration 76950, loss = 0.89244631\n",
      "Iteration 76951, loss = 0.98603830\n",
      "Iteration 76952, loss = 1.09010048\n",
      "Iteration 76953, loss = 1.16809316\n",
      "Iteration 76954, loss = 1.20154802\n",
      "Iteration 76955, loss = 1.18002910\n",
      "Iteration 76956, loss = 1.07112538\n",
      "Iteration 76957, loss = 1.14564373\n",
      "Iteration 76958, loss = 1.22308907\n",
      "Iteration 76959, loss = 1.24546480\n",
      "Iteration 76960, loss = 1.37407834\n",
      "Iteration 76961, loss = 1.25931543\n",
      "Iteration 76962, loss = 1.36050862\n",
      "Iteration 76963, loss = 1.19089883\n",
      "Iteration 76964, loss = 1.10084979\n",
      "Iteration 76965, loss = 1.13197009\n",
      "Iteration 76966, loss = 1.01995776\n",
      "Iteration 76967, loss = 1.04976789\n",
      "Iteration 76968, loss = 1.11210868\n",
      "Iteration 76969, loss = 0.97087385\n",
      "Iteration 76970, loss = 1.12226689\n",
      "Iteration 76971, loss = 1.04868222\n",
      "Iteration 76972, loss = 1.04392528\n",
      "Iteration 76973, loss = 1.11800380\n",
      "Iteration 76974, loss = 1.02994584\n",
      "Iteration 76975, loss = 1.14242695\n",
      "Iteration 76976, loss = 1.03952138\n",
      "Iteration 76977, loss = 0.93648414\n",
      "Iteration 76978, loss = 1.10642158\n",
      "Iteration 76979, loss = 0.95951186\n",
      "Iteration 76980, loss = 1.09319612\n",
      "Iteration 76981, loss = 1.19531001\n",
      "Iteration 76982, loss = 1.38150043\n",
      "Iteration 76983, loss = 1.21132257\n",
      "Iteration 76984, loss = 1.17795999\n",
      "Iteration 76985, loss = 1.35150519\n",
      "Iteration 76986, loss = 1.39400419\n",
      "Iteration 76987, loss = 1.04551061\n",
      "Iteration 76988, loss = 1.05760643\n",
      "Iteration 76989, loss = 0.99685810\n",
      "Iteration 76990, loss = 0.91735322\n",
      "Iteration 76991, loss = 1.03784927\n",
      "Iteration 76992, loss = 1.12787064\n",
      "Iteration 76993, loss = 1.10004814\n",
      "Iteration 76994, loss = 1.29198823\n",
      "Iteration 76995, loss = 1.11895734\n",
      "Iteration 76996, loss = 0.93687183\n",
      "Iteration 76997, loss = 0.92665551\n",
      "Iteration 76998, loss = 1.01445752\n",
      "Iteration 76999, loss = 0.98585448\n",
      "Iteration 77000, loss = 0.97227850\n",
      "Iteration 77001, loss = 1.27480907\n",
      "Iteration 77002, loss = 1.39417070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 77003, loss = 1.24312816\n",
      "Iteration 77004, loss = 1.31053004\n",
      "Iteration 77005, loss = 1.19814431\n",
      "Iteration 77006, loss = 1.50776339\n",
      "Iteration 77007, loss = 1.52864410\n",
      "Iteration 77008, loss = 1.18133895\n",
      "Iteration 77009, loss = 1.25202061\n",
      "Iteration 77010, loss = 1.39099671\n",
      "Iteration 77011, loss = 1.22270793\n",
      "Iteration 77012, loss = 1.18054029\n",
      "Iteration 77013, loss = 1.09301065\n",
      "Iteration 77014, loss = 1.30018962\n",
      "Iteration 77015, loss = 1.39754269\n",
      "Iteration 77016, loss = 1.47245867\n",
      "Iteration 77017, loss = 1.28373277\n",
      "Iteration 77018, loss = 1.22644611\n",
      "Iteration 77019, loss = 1.38613465\n",
      "Iteration 77020, loss = 1.22516811\n",
      "Iteration 77021, loss = 1.18785962\n",
      "Iteration 77022, loss = 1.03690747\n",
      "Iteration 77023, loss = 1.13005189\n",
      "Iteration 77024, loss = 1.27562911\n",
      "Iteration 77025, loss = 1.10307893\n",
      "Iteration 77026, loss = 1.05939921\n",
      "Iteration 77027, loss = 1.33740311\n",
      "Iteration 77028, loss = 1.12858587\n",
      "Iteration 77029, loss = 1.09328409\n",
      "Iteration 77030, loss = 1.12362443\n",
      "Iteration 77031, loss = 1.14140655\n",
      "Iteration 77032, loss = 1.05190910\n",
      "Iteration 77033, loss = 1.17356528\n",
      "Iteration 77034, loss = 1.69353851\n",
      "Iteration 77035, loss = 1.86279148\n",
      "Iteration 77036, loss = 1.59520235\n",
      "Iteration 77037, loss = 1.30780291\n",
      "Iteration 77038, loss = 1.21359911\n",
      "Iteration 77039, loss = 1.31811349\n",
      "Iteration 77040, loss = 1.12620109\n",
      "Iteration 77041, loss = 1.19770047\n",
      "Iteration 77042, loss = 1.12508901\n",
      "Iteration 77043, loss = 1.58140494\n",
      "Iteration 77044, loss = 1.66251396\n",
      "Iteration 77045, loss = 2.04496301\n",
      "Iteration 77046, loss = 2.41098192\n",
      "Iteration 77047, loss = 3.33074074\n",
      "Iteration 77048, loss = 2.28994903\n",
      "Iteration 77049, loss = 3.44824199\n",
      "Iteration 77050, loss = 4.92610719\n",
      "Iteration 77051, loss = 3.68714079\n",
      "Iteration 77052, loss = 2.88948657\n",
      "Iteration 77053, loss = 2.83572424\n",
      "Iteration 77054, loss = 2.11110756\n",
      "Iteration 77055, loss = 2.17779228\n",
      "Iteration 77056, loss = 2.26977365\n",
      "Iteration 77057, loss = 1.67174499\n",
      "Iteration 77058, loss = 1.43575128\n",
      "Iteration 77059, loss = 1.25333992\n",
      "Iteration 77060, loss = 1.35786587\n",
      "Iteration 77061, loss = 1.21449618\n",
      "Iteration 77062, loss = 1.49674249\n",
      "Iteration 77063, loss = 1.69774409\n",
      "Iteration 77064, loss = 1.88892815\n",
      "Iteration 77065, loss = 1.83125563\n",
      "Iteration 77066, loss = 1.49202941\n",
      "Iteration 77067, loss = 1.15190666\n",
      "Iteration 77068, loss = 1.17814860\n",
      "Iteration 77069, loss = 1.06063345\n",
      "Iteration 77070, loss = 1.00992226\n",
      "Iteration 77071, loss = 0.98766276\n",
      "Iteration 77072, loss = 0.98663150\n",
      "Iteration 77073, loss = 0.94701495\n",
      "Iteration 77074, loss = 1.10041984\n",
      "Iteration 77075, loss = 1.02373456\n",
      "Iteration 77076, loss = 1.20418411\n",
      "Iteration 77077, loss = 1.14908676\n",
      "Iteration 77078, loss = 1.08764706\n",
      "Iteration 77079, loss = 1.03595846\n",
      "Iteration 77080, loss = 1.25421640\n",
      "Iteration 77081, loss = 1.11773157\n",
      "Iteration 77082, loss = 1.03599091\n",
      "Iteration 77083, loss = 1.00352473\n",
      "Iteration 77084, loss = 1.05376483\n",
      "Iteration 77085, loss = 1.14222668\n",
      "Iteration 77086, loss = 1.31637879\n",
      "Iteration 77087, loss = 1.37414213\n",
      "Iteration 77088, loss = 1.02300719\n",
      "Iteration 77089, loss = 1.33550283\n",
      "Iteration 77090, loss = 1.01165809\n",
      "Iteration 77091, loss = 1.05081588\n",
      "Iteration 77092, loss = 0.94466635\n",
      "Iteration 77093, loss = 1.00901771\n",
      "Iteration 77094, loss = 0.93380488\n",
      "Iteration 77095, loss = 1.00330459\n",
      "Iteration 77096, loss = 0.93434109\n",
      "Iteration 77097, loss = 0.91906368\n",
      "Iteration 77098, loss = 0.94533100\n",
      "Iteration 77099, loss = 0.90062914\n",
      "Iteration 77100, loss = 0.94795560\n",
      "Iteration 77101, loss = 0.91162691\n",
      "Iteration 77102, loss = 0.98112094\n",
      "Iteration 77103, loss = 0.94937001\n",
      "Iteration 77104, loss = 0.95472252\n",
      "Iteration 77105, loss = 1.02100711\n",
      "Iteration 77106, loss = 1.03814522\n",
      "Iteration 77107, loss = 1.07784718\n",
      "Iteration 77108, loss = 1.03462839\n",
      "Iteration 77109, loss = 1.03317601\n",
      "Iteration 77110, loss = 1.00945561\n",
      "Iteration 77111, loss = 1.08454545\n",
      "Iteration 77112, loss = 1.13026442\n",
      "Iteration 77113, loss = 1.18916582\n",
      "Iteration 77114, loss = 1.14467369\n",
      "Iteration 77115, loss = 1.36489372\n",
      "Iteration 77116, loss = 1.37133353\n",
      "Iteration 77117, loss = 1.47598021\n",
      "Iteration 77118, loss = 1.25072786\n",
      "Iteration 77119, loss = 1.01673955\n",
      "Iteration 77120, loss = 0.99568696\n",
      "Iteration 77121, loss = 0.93870352\n",
      "Iteration 77122, loss = 0.96193788\n",
      "Iteration 77123, loss = 0.96387666\n",
      "Iteration 77124, loss = 1.04693739\n",
      "Iteration 77125, loss = 1.04496494\n",
      "Iteration 77126, loss = 1.28921573\n",
      "Iteration 77127, loss = 1.28539551\n",
      "Iteration 77128, loss = 1.17351844\n",
      "Iteration 77129, loss = 1.23684127\n",
      "Iteration 77130, loss = 1.20949682\n",
      "Iteration 77131, loss = 1.09091165\n",
      "Iteration 77132, loss = 1.10884858\n",
      "Iteration 77133, loss = 1.09065820\n",
      "Iteration 77134, loss = 1.03346750\n",
      "Iteration 77135, loss = 1.08217545\n",
      "Iteration 77136, loss = 1.02576628\n",
      "Iteration 77137, loss = 1.04697133\n",
      "Iteration 77138, loss = 1.01817164\n",
      "Iteration 77139, loss = 1.19555476\n",
      "Iteration 77140, loss = 1.29829924\n",
      "Iteration 77141, loss = 1.67598018\n",
      "Iteration 77142, loss = 1.42360980\n",
      "Iteration 77143, loss = 1.49840080\n",
      "Iteration 77144, loss = 1.36182380\n",
      "Iteration 77145, loss = 1.27405501\n",
      "Iteration 77146, loss = 1.26401594\n",
      "Iteration 77147, loss = 1.14948804\n",
      "Iteration 77148, loss = 1.22749551\n",
      "Iteration 77149, loss = 1.14308269\n",
      "Iteration 77150, loss = 1.16137068\n",
      "Iteration 77151, loss = 1.04663227\n",
      "Iteration 77152, loss = 1.24346784\n",
      "Iteration 77153, loss = 1.62363449\n",
      "Iteration 77154, loss = 1.41028432\n",
      "Iteration 77155, loss = 1.18625662\n",
      "Iteration 77156, loss = 1.24053939\n",
      "Iteration 77157, loss = 0.99558987\n",
      "Iteration 77158, loss = 1.04811680\n",
      "Iteration 77159, loss = 1.01784115\n",
      "Iteration 77160, loss = 1.34949489\n",
      "Iteration 77161, loss = 1.38118322\n",
      "Iteration 77162, loss = 1.49616531\n",
      "Iteration 77163, loss = 1.76699785\n",
      "Iteration 77164, loss = 2.21052685\n",
      "Iteration 77165, loss = 3.26889404\n",
      "Iteration 77166, loss = 1.94017814\n",
      "Iteration 77167, loss = 1.72538651\n",
      "Iteration 77168, loss = 1.50166094\n",
      "Iteration 77169, loss = 1.25401016\n",
      "Iteration 77170, loss = 1.18080253\n",
      "Iteration 77171, loss = 1.11580385\n",
      "Iteration 77172, loss = 1.07684337\n",
      "Iteration 77173, loss = 1.10599496\n",
      "Iteration 77174, loss = 1.29272296\n",
      "Iteration 77175, loss = 1.31383808\n",
      "Iteration 77176, loss = 1.14522961\n",
      "Iteration 77177, loss = 1.42261853\n",
      "Iteration 77178, loss = 1.63238159\n",
      "Iteration 77179, loss = 1.96282474\n",
      "Iteration 77180, loss = 1.77865442\n",
      "Iteration 77181, loss = 1.51671584\n",
      "Iteration 77182, loss = 1.40340507\n",
      "Iteration 77183, loss = 1.39199775\n",
      "Iteration 77184, loss = 1.15534143\n",
      "Iteration 77185, loss = 1.10159645\n",
      "Iteration 77186, loss = 1.11316948\n",
      "Iteration 77187, loss = 1.05637898\n",
      "Iteration 77188, loss = 0.98992547\n",
      "Iteration 77189, loss = 0.94782314\n",
      "Iteration 77190, loss = 0.94292167\n",
      "Iteration 77191, loss = 1.01662191\n",
      "Iteration 77192, loss = 0.96281761\n",
      "Iteration 77193, loss = 1.02524642\n",
      "Iteration 77194, loss = 0.91728656\n",
      "Iteration 77195, loss = 0.93260502\n",
      "Iteration 77196, loss = 0.98515108\n",
      "Iteration 77197, loss = 1.04172031\n",
      "Iteration 77198, loss = 1.10083777\n",
      "Iteration 77199, loss = 1.10444068\n",
      "Iteration 77200, loss = 1.00177733\n",
      "Iteration 77201, loss = 1.05517558\n",
      "Iteration 77202, loss = 1.07861033\n",
      "Iteration 77203, loss = 1.14464710\n",
      "Iteration 77204, loss = 1.17826936\n",
      "Iteration 77205, loss = 1.28941656\n",
      "Iteration 77206, loss = 1.12710835\n",
      "Iteration 77207, loss = 1.27554396\n",
      "Iteration 77208, loss = 1.44963765\n",
      "Iteration 77209, loss = 1.15514970\n",
      "Iteration 77210, loss = 0.99919623\n",
      "Iteration 77211, loss = 0.98756389\n",
      "Iteration 77212, loss = 1.26381905\n",
      "Iteration 77213, loss = 1.15136814\n",
      "Iteration 77214, loss = 0.99872225\n",
      "Iteration 77215, loss = 1.10423101\n",
      "Iteration 77216, loss = 1.17655203\n",
      "Iteration 77217, loss = 1.12251630\n",
      "Iteration 77218, loss = 1.20994053\n",
      "Iteration 77219, loss = 1.35747984\n",
      "Iteration 77220, loss = 1.36560732\n",
      "Iteration 77221, loss = 1.20785353\n",
      "Iteration 77222, loss = 1.67108167\n",
      "Iteration 77223, loss = 1.35302722\n",
      "Iteration 77224, loss = 1.10343215\n",
      "Iteration 77225, loss = 1.02923330\n",
      "Iteration 77226, loss = 0.93130135\n",
      "Iteration 77227, loss = 0.99863371\n",
      "Iteration 77228, loss = 1.12024961\n",
      "Iteration 77229, loss = 0.92817757\n",
      "Iteration 77230, loss = 0.98629515\n",
      "Iteration 77231, loss = 0.94211246\n",
      "Iteration 77232, loss = 0.96440923\n",
      "Iteration 77233, loss = 0.96261792\n",
      "Iteration 77234, loss = 0.98751331\n",
      "Iteration 77235, loss = 1.15057648\n",
      "Iteration 77236, loss = 1.23744806\n",
      "Iteration 77237, loss = 1.49045954\n",
      "Iteration 77238, loss = 1.68243565\n",
      "Iteration 77239, loss = 1.54723605\n",
      "Iteration 77240, loss = 1.26467500\n",
      "Iteration 77241, loss = 1.29800817\n",
      "Iteration 77242, loss = 1.62362127\n",
      "Iteration 77243, loss = 1.86527848\n",
      "Iteration 77244, loss = 2.13709991\n",
      "Iteration 77245, loss = 2.24781692\n",
      "Iteration 77246, loss = 1.66428209\n",
      "Iteration 77247, loss = 1.51508923\n",
      "Iteration 77248, loss = 1.36352673\n",
      "Iteration 77249, loss = 1.95765826\n",
      "Iteration 77250, loss = 1.60437198\n",
      "Iteration 77251, loss = 1.48129238\n",
      "Iteration 77252, loss = 1.36549471\n",
      "Iteration 77253, loss = 1.24501797\n",
      "Iteration 77254, loss = 1.22283860\n",
      "Iteration 77255, loss = 1.21974967\n",
      "Iteration 77256, loss = 1.16798289\n",
      "Iteration 77257, loss = 1.02909167\n",
      "Iteration 77258, loss = 1.34238368\n",
      "Iteration 77259, loss = 1.73560657\n",
      "Iteration 77260, loss = 1.61446290\n",
      "Iteration 77261, loss = 1.91141705\n",
      "Iteration 77262, loss = 1.66506113\n",
      "Iteration 77263, loss = 1.72113549\n",
      "Iteration 77264, loss = 2.91838121\n",
      "Iteration 77265, loss = 2.81402504\n",
      "Iteration 77266, loss = 2.07918215\n",
      "Iteration 77267, loss = 1.50108484\n",
      "Iteration 77268, loss = 1.59060437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 77269, loss = 1.60868019\n",
      "Iteration 77270, loss = 1.70222155\n",
      "Iteration 77271, loss = 1.40627913\n",
      "Iteration 77272, loss = 1.38892561\n",
      "Iteration 77273, loss = 1.34563488\n",
      "Iteration 77274, loss = 1.03055849\n",
      "Iteration 77275, loss = 1.03677508\n",
      "Iteration 77276, loss = 1.05475545\n",
      "Iteration 77277, loss = 1.00595730\n",
      "Iteration 77278, loss = 1.06065759\n",
      "Iteration 77279, loss = 1.01199098\n",
      "Iteration 77280, loss = 1.07127504\n",
      "Iteration 77281, loss = 1.02243545\n",
      "Iteration 77282, loss = 1.08415224\n",
      "Iteration 77283, loss = 1.09399079\n",
      "Iteration 77284, loss = 1.16256777\n",
      "Iteration 77285, loss = 1.24958769\n",
      "Iteration 77286, loss = 1.13985219\n",
      "Iteration 77287, loss = 1.01318151\n",
      "Iteration 77288, loss = 0.93361137\n",
      "Iteration 77289, loss = 0.92384576\n",
      "Iteration 77290, loss = 1.00985356\n",
      "Iteration 77291, loss = 1.00148476\n",
      "Iteration 77292, loss = 0.97639261\n",
      "Iteration 77293, loss = 1.13886032\n",
      "Iteration 77294, loss = 1.05447321\n",
      "Iteration 77295, loss = 1.08607880\n",
      "Iteration 77296, loss = 1.09590150\n",
      "Iteration 77297, loss = 1.33093869\n",
      "Iteration 77298, loss = 1.56341460\n",
      "Iteration 77299, loss = 2.39173922\n",
      "Iteration 77300, loss = 3.07588583\n",
      "Iteration 77301, loss = 1.93039939\n",
      "Iteration 77302, loss = 1.75675142\n",
      "Iteration 77303, loss = 1.79861149\n",
      "Iteration 77304, loss = 1.55009249\n",
      "Iteration 77305, loss = 1.45515864\n",
      "Iteration 77306, loss = 1.07407465\n",
      "Iteration 77307, loss = 1.47246935\n",
      "Iteration 77308, loss = 1.18568607\n",
      "Iteration 77309, loss = 1.25706520\n",
      "Iteration 77310, loss = 1.17675027\n",
      "Iteration 77311, loss = 1.03385702\n",
      "Iteration 77312, loss = 1.06458522\n",
      "Iteration 77313, loss = 1.12674421\n",
      "Iteration 77314, loss = 1.14706525\n",
      "Iteration 77315, loss = 1.09573760\n",
      "Iteration 77316, loss = 1.19586424\n",
      "Iteration 77317, loss = 1.43172903\n",
      "Iteration 77318, loss = 1.30941565\n",
      "Iteration 77319, loss = 1.48994490\n",
      "Iteration 77320, loss = 1.35793362\n",
      "Iteration 77321, loss = 1.63300634\n",
      "Iteration 77322, loss = 1.40399585\n",
      "Iteration 77323, loss = 1.44437037\n",
      "Iteration 77324, loss = 1.09747845\n",
      "Iteration 77325, loss = 1.05016401\n",
      "Iteration 77326, loss = 1.16694574\n",
      "Iteration 77327, loss = 1.03499947\n",
      "Iteration 77328, loss = 1.12306132\n",
      "Iteration 77329, loss = 1.02800892\n",
      "Iteration 77330, loss = 1.14249255\n",
      "Iteration 77331, loss = 1.64595970\n",
      "Iteration 77332, loss = 1.24487728\n",
      "Iteration 77333, loss = 1.60281505\n",
      "Iteration 77334, loss = 2.35258405\n",
      "Iteration 77335, loss = 1.74927967\n",
      "Iteration 77336, loss = 2.31903211\n",
      "Iteration 77337, loss = 1.94507004\n",
      "Iteration 77338, loss = 1.74918485\n",
      "Iteration 77339, loss = 1.50053254\n",
      "Iteration 77340, loss = 1.19995969\n",
      "Iteration 77341, loss = 1.00041914\n",
      "Iteration 77342, loss = 0.89063917\n",
      "Iteration 77343, loss = 1.02296345\n",
      "Iteration 77344, loss = 1.16511811\n",
      "Iteration 77345, loss = 1.12318827\n",
      "Iteration 77346, loss = 1.16803215\n",
      "Iteration 77347, loss = 1.37963473\n",
      "Iteration 77348, loss = 1.31855712\n",
      "Iteration 77349, loss = 1.30368657\n",
      "Iteration 77350, loss = 1.34052313\n",
      "Iteration 77351, loss = 1.22214810\n",
      "Iteration 77352, loss = 1.18153703\n",
      "Iteration 77353, loss = 1.16404092\n",
      "Iteration 77354, loss = 1.33533676\n",
      "Iteration 77355, loss = 1.20732208\n",
      "Iteration 77356, loss = 1.05132667\n",
      "Iteration 77357, loss = 1.04521938\n",
      "Iteration 77358, loss = 1.00190421\n",
      "Iteration 77359, loss = 1.05444867\n",
      "Iteration 77360, loss = 1.04714977\n",
      "Iteration 77361, loss = 0.94650396\n",
      "Iteration 77362, loss = 0.99423787\n",
      "Iteration 77363, loss = 0.95044255\n",
      "Iteration 77364, loss = 1.00373626\n",
      "Iteration 77365, loss = 1.00761801\n",
      "Iteration 77366, loss = 1.04576889\n",
      "Iteration 77367, loss = 0.94528107\n",
      "Iteration 77368, loss = 1.03613771\n",
      "Iteration 77369, loss = 1.09607930\n",
      "Iteration 77370, loss = 0.94479389\n",
      "Iteration 77371, loss = 0.93676277\n",
      "Iteration 77372, loss = 0.90072502\n",
      "Iteration 77373, loss = 0.93855550\n",
      "Iteration 77374, loss = 0.97386004\n",
      "Iteration 77375, loss = 0.91548995\n",
      "Iteration 77376, loss = 0.97139894\n",
      "Iteration 77377, loss = 1.14432703\n",
      "Iteration 77378, loss = 1.13235716\n",
      "Iteration 77379, loss = 1.00546938\n",
      "Iteration 77380, loss = 0.99123232\n",
      "Iteration 77381, loss = 1.24113669\n",
      "Iteration 77382, loss = 1.16185976\n",
      "Iteration 77383, loss = 0.98100945\n",
      "Iteration 77384, loss = 0.99327136\n",
      "Iteration 77385, loss = 1.00229813\n",
      "Iteration 77386, loss = 1.05778787\n",
      "Iteration 77387, loss = 1.05531600\n",
      "Iteration 77388, loss = 1.04267061\n",
      "Iteration 77389, loss = 1.05917814\n",
      "Iteration 77390, loss = 1.10740787\n",
      "Iteration 77391, loss = 1.17388748\n",
      "Iteration 77392, loss = 0.98431383\n",
      "Iteration 77393, loss = 1.08877092\n",
      "Iteration 77394, loss = 1.09095514\n",
      "Iteration 77395, loss = 1.36622463\n",
      "Iteration 77396, loss = 2.16639460\n",
      "Iteration 77397, loss = 1.83648039\n",
      "Iteration 77398, loss = 1.89825638\n",
      "Iteration 77399, loss = 1.51313401\n",
      "Iteration 77400, loss = 1.39662905\n",
      "Iteration 77401, loss = 1.29064602\n",
      "Iteration 77402, loss = 1.45636433\n",
      "Iteration 77403, loss = 1.38646889\n",
      "Iteration 77404, loss = 1.37760862\n",
      "Iteration 77405, loss = 1.28324472\n",
      "Iteration 77406, loss = 1.11647404\n",
      "Iteration 77407, loss = 1.06115123\n",
      "Iteration 77408, loss = 0.98894329\n",
      "Iteration 77409, loss = 1.36158951\n",
      "Iteration 77410, loss = 1.33866438\n",
      "Iteration 77411, loss = 1.75222465\n",
      "Iteration 77412, loss = 1.35420003\n",
      "Iteration 77413, loss = 1.39826959\n",
      "Iteration 77414, loss = 1.48026448\n",
      "Iteration 77415, loss = 1.43230465\n",
      "Iteration 77416, loss = 1.33116556\n",
      "Iteration 77417, loss = 1.17358330\n",
      "Iteration 77418, loss = 1.08359212\n",
      "Iteration 77419, loss = 0.99371615\n",
      "Iteration 77420, loss = 1.12283431\n",
      "Iteration 77421, loss = 1.07353679\n",
      "Iteration 77422, loss = 1.12893193\n",
      "Iteration 77423, loss = 1.15323278\n",
      "Iteration 77424, loss = 1.09555068\n",
      "Iteration 77425, loss = 0.97668830\n",
      "Iteration 77426, loss = 1.06399286\n",
      "Iteration 77427, loss = 1.19652044\n",
      "Iteration 77428, loss = 1.21259004\n",
      "Iteration 77429, loss = 1.22699036\n",
      "Iteration 77430, loss = 1.04906159\n",
      "Iteration 77431, loss = 1.05337376\n",
      "Iteration 77432, loss = 1.06882437\n",
      "Iteration 77433, loss = 1.11037004\n",
      "Iteration 77434, loss = 1.00177626\n",
      "Iteration 77435, loss = 1.00197629\n",
      "Iteration 77436, loss = 1.11977966\n",
      "Iteration 77437, loss = 1.19688177\n",
      "Iteration 77438, loss = 1.06417056\n",
      "Iteration 77439, loss = 0.95709583\n",
      "Iteration 77440, loss = 0.93206694\n",
      "Iteration 77441, loss = 1.11260361\n",
      "Iteration 77442, loss = 1.15060753\n",
      "Iteration 77443, loss = 1.08706936\n",
      "Iteration 77444, loss = 1.02798180\n",
      "Iteration 77445, loss = 1.00808745\n",
      "Iteration 77446, loss = 0.98040210\n",
      "Iteration 77447, loss = 0.96700994\n",
      "Iteration 77448, loss = 0.96017137\n",
      "Iteration 77449, loss = 1.00866044\n",
      "Iteration 77450, loss = 1.10562711\n",
      "Iteration 77451, loss = 1.27053899\n",
      "Iteration 77452, loss = 1.19150611\n",
      "Iteration 77453, loss = 1.40147941\n",
      "Iteration 77454, loss = 1.44604821\n",
      "Iteration 77455, loss = 1.32936246\n",
      "Iteration 77456, loss = 1.21394862\n",
      "Iteration 77457, loss = 1.15133171\n",
      "Iteration 77458, loss = 1.30413463\n",
      "Iteration 77459, loss = 1.51569587\n",
      "Iteration 77460, loss = 1.27887290\n",
      "Iteration 77461, loss = 1.41388666\n",
      "Iteration 77462, loss = 1.45065745\n",
      "Iteration 77463, loss = 1.10226558\n",
      "Iteration 77464, loss = 1.03168984\n",
      "Iteration 77465, loss = 1.08190788\n",
      "Iteration 77466, loss = 1.69314841\n",
      "Iteration 77467, loss = 2.07611337\n",
      "Iteration 77468, loss = 1.71035246\n",
      "Iteration 77469, loss = 1.75112882\n",
      "Iteration 77470, loss = 1.81935629\n",
      "Iteration 77471, loss = 1.68373849\n",
      "Iteration 77472, loss = 1.35631959\n",
      "Iteration 77473, loss = 1.35584370\n",
      "Iteration 77474, loss = 1.08109552\n",
      "Iteration 77475, loss = 1.03095934\n",
      "Iteration 77476, loss = 0.93638191\n",
      "Iteration 77477, loss = 0.90439266\n",
      "Iteration 77478, loss = 0.94291426\n",
      "Iteration 77479, loss = 0.93105974\n",
      "Iteration 77480, loss = 1.02689961\n",
      "Iteration 77481, loss = 1.04553420\n",
      "Iteration 77482, loss = 0.99701282\n",
      "Iteration 77483, loss = 1.12270165\n",
      "Iteration 77484, loss = 1.19878190\n",
      "Iteration 77485, loss = 1.03789528\n",
      "Iteration 77486, loss = 1.12031236\n",
      "Iteration 77487, loss = 1.12976944\n",
      "Iteration 77488, loss = 1.26920181\n",
      "Iteration 77489, loss = 1.19703060\n",
      "Iteration 77490, loss = 1.17448440\n",
      "Iteration 77491, loss = 1.28456337\n",
      "Iteration 77492, loss = 1.26771443\n",
      "Iteration 77493, loss = 1.24750730\n",
      "Iteration 77494, loss = 1.25547822\n",
      "Iteration 77495, loss = 1.18114333\n",
      "Iteration 77496, loss = 1.04151432\n",
      "Iteration 77497, loss = 1.17560802\n",
      "Iteration 77498, loss = 1.05727932\n",
      "Iteration 77499, loss = 1.10008359\n",
      "Iteration 77500, loss = 0.98946781\n",
      "Iteration 77501, loss = 0.99155688\n",
      "Iteration 77502, loss = 0.96973507\n",
      "Iteration 77503, loss = 1.05731996\n",
      "Iteration 77504, loss = 1.19953054\n",
      "Iteration 77505, loss = 1.04909692\n",
      "Iteration 77506, loss = 1.15658010\n",
      "Iteration 77507, loss = 0.98611745\n",
      "Iteration 77508, loss = 0.92005217\n",
      "Iteration 77509, loss = 0.98821438\n",
      "Iteration 77510, loss = 0.94615006\n",
      "Iteration 77511, loss = 0.91171485\n",
      "Iteration 77512, loss = 0.96062600\n",
      "Iteration 77513, loss = 0.97324305\n",
      "Iteration 77514, loss = 1.05000063\n",
      "Iteration 77515, loss = 1.20610027\n",
      "Iteration 77516, loss = 1.39460538\n",
      "Iteration 77517, loss = 2.00758725\n",
      "Iteration 77518, loss = 2.31149219\n",
      "Iteration 77519, loss = 1.66787614\n",
      "Iteration 77520, loss = 1.83977403\n",
      "Iteration 77521, loss = 2.46702508\n",
      "Iteration 77522, loss = 2.62068734\n",
      "Iteration 77523, loss = 1.74613460\n",
      "Iteration 77524, loss = 1.44618880\n",
      "Iteration 77525, loss = 1.29148215\n",
      "Iteration 77526, loss = 1.30225739\n",
      "Iteration 77527, loss = 1.14288336\n",
      "Iteration 77528, loss = 0.91544572\n",
      "Iteration 77529, loss = 1.32971931\n",
      "Iteration 77530, loss = 1.34692996\n",
      "Iteration 77531, loss = 1.39207224\n",
      "Iteration 77532, loss = 1.26332433\n",
      "Iteration 77533, loss = 1.08705631\n",
      "Iteration 77534, loss = 1.36913661\n",
      "Iteration 77535, loss = 1.33610185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 77536, loss = 1.37020209\n",
      "Iteration 77537, loss = 1.24099465\n",
      "Iteration 77538, loss = 1.03345760\n",
      "Iteration 77539, loss = 1.06571150\n",
      "Iteration 77540, loss = 1.13416047\n",
      "Iteration 77541, loss = 1.14870293\n",
      "Iteration 77542, loss = 1.02999731\n",
      "Iteration 77543, loss = 1.28434559\n",
      "Iteration 77544, loss = 1.10867167\n",
      "Iteration 77545, loss = 1.12189278\n",
      "Iteration 77546, loss = 1.31196178\n",
      "Iteration 77547, loss = 1.38090172\n",
      "Iteration 77548, loss = 1.42082536\n",
      "Iteration 77549, loss = 1.20883216\n",
      "Iteration 77550, loss = 1.31025477\n",
      "Iteration 77551, loss = 1.28189942\n",
      "Iteration 77552, loss = 1.16074974\n",
      "Iteration 77553, loss = 1.07411457\n",
      "Iteration 77554, loss = 1.00498428\n",
      "Iteration 77555, loss = 1.08698305\n",
      "Iteration 77556, loss = 1.15915739\n",
      "Iteration 77557, loss = 1.05275011\n",
      "Iteration 77558, loss = 1.17032005\n",
      "Iteration 77559, loss = 1.12608522\n",
      "Iteration 77560, loss = 1.01611910\n",
      "Iteration 77561, loss = 1.01947026\n",
      "Iteration 77562, loss = 1.06652590\n",
      "Iteration 77563, loss = 1.26985966\n",
      "Iteration 77564, loss = 1.35693916\n",
      "Iteration 77565, loss = 1.50505997\n",
      "Iteration 77566, loss = 1.39007339\n",
      "Iteration 77567, loss = 1.72160687\n",
      "Iteration 77568, loss = 1.49544686\n",
      "Iteration 77569, loss = 1.70536615\n",
      "Iteration 77570, loss = 1.42007512\n",
      "Iteration 77571, loss = 1.16475543\n",
      "Iteration 77572, loss = 1.14288662\n",
      "Iteration 77573, loss = 1.64261654\n",
      "Iteration 77574, loss = 2.38902950\n",
      "Iteration 77575, loss = 2.52390907\n",
      "Iteration 77576, loss = 2.44989079\n",
      "Iteration 77577, loss = 2.50913939\n",
      "Iteration 77578, loss = 3.60621517\n",
      "Iteration 77579, loss = 4.01055961\n",
      "Iteration 77580, loss = 2.41309740\n",
      "Iteration 77581, loss = 2.55685643\n",
      "Iteration 77582, loss = 3.51406299\n",
      "Iteration 77583, loss = 5.34167107\n",
      "Iteration 77584, loss = 2.85671553\n",
      "Iteration 77585, loss = 1.94378945\n",
      "Iteration 77586, loss = 1.78683080\n",
      "Iteration 77587, loss = 1.42848843\n",
      "Iteration 77588, loss = 1.21487674\n",
      "Iteration 77589, loss = 1.23601357\n",
      "Iteration 77590, loss = 1.17835012\n",
      "Iteration 77591, loss = 1.20037584\n",
      "Iteration 77592, loss = 1.18981422\n",
      "Iteration 77593, loss = 1.15225557\n",
      "Iteration 77594, loss = 1.16608953\n",
      "Iteration 77595, loss = 0.95203784\n",
      "Iteration 77596, loss = 0.97363767\n",
      "Iteration 77597, loss = 1.11256241\n",
      "Iteration 77598, loss = 1.15604052\n",
      "Iteration 77599, loss = 1.20949639\n",
      "Iteration 77600, loss = 1.16663330\n",
      "Iteration 77601, loss = 1.07898830\n",
      "Iteration 77602, loss = 1.19119202\n",
      "Iteration 77603, loss = 1.13197555\n",
      "Iteration 77604, loss = 1.12250102\n",
      "Iteration 77605, loss = 1.11524845\n",
      "Iteration 77606, loss = 1.26821535\n",
      "Iteration 77607, loss = 1.38128852\n",
      "Iteration 77608, loss = 1.46049221\n",
      "Iteration 77609, loss = 1.40415535\n",
      "Iteration 77610, loss = 1.55684257\n",
      "Iteration 77611, loss = 1.64641078\n",
      "Iteration 77612, loss = 1.30478399\n",
      "Iteration 77613, loss = 1.41601651\n",
      "Iteration 77614, loss = 1.33172921\n",
      "Iteration 77615, loss = 1.07656949\n",
      "Iteration 77616, loss = 1.18245897\n",
      "Iteration 77617, loss = 1.42209133\n",
      "Iteration 77618, loss = 1.19068574\n",
      "Iteration 77619, loss = 1.24563385\n",
      "Iteration 77620, loss = 1.12646151\n",
      "Iteration 77621, loss = 0.99910969\n",
      "Iteration 77622, loss = 1.08150987\n",
      "Iteration 77623, loss = 1.15088030\n",
      "Iteration 77624, loss = 1.06665480\n",
      "Iteration 77625, loss = 1.03232040\n",
      "Iteration 77626, loss = 1.06901879\n",
      "Iteration 77627, loss = 1.14668034\n",
      "Iteration 77628, loss = 1.21688397\n",
      "Iteration 77629, loss = 1.35963221\n",
      "Iteration 77630, loss = 1.11544980\n",
      "Iteration 77631, loss = 1.22272559\n",
      "Iteration 77632, loss = 1.22477748\n",
      "Iteration 77633, loss = 1.16152581\n",
      "Iteration 77634, loss = 1.08591111\n",
      "Iteration 77635, loss = 1.05997411\n",
      "Iteration 77636, loss = 1.04357090\n",
      "Iteration 77637, loss = 1.04104904\n",
      "Iteration 77638, loss = 1.09124146\n",
      "Iteration 77639, loss = 0.99191243\n",
      "Iteration 77640, loss = 0.98445251\n",
      "Iteration 77641, loss = 0.96038708\n",
      "Iteration 77642, loss = 1.09649270\n",
      "Iteration 77643, loss = 1.07010537\n",
      "Iteration 77644, loss = 0.98606810\n",
      "Iteration 77645, loss = 1.07582891\n",
      "Iteration 77646, loss = 1.01767292\n",
      "Iteration 77647, loss = 0.96560487\n",
      "Iteration 77648, loss = 1.10510831\n",
      "Iteration 77649, loss = 1.16680401\n",
      "Iteration 77650, loss = 1.25681823\n",
      "Iteration 77651, loss = 1.39082909\n",
      "Iteration 77652, loss = 1.12021714\n",
      "Iteration 77653, loss = 1.00914833\n",
      "Iteration 77654, loss = 1.02514546\n",
      "Iteration 77655, loss = 0.97601082\n",
      "Iteration 77656, loss = 0.96911796\n",
      "Iteration 77657, loss = 0.90928192\n",
      "Iteration 77658, loss = 0.99678177\n",
      "Iteration 77659, loss = 1.10546763\n",
      "Iteration 77660, loss = 1.06989366\n",
      "Iteration 77661, loss = 1.07544703\n",
      "Iteration 77662, loss = 1.44641370\n",
      "Iteration 77663, loss = 2.04218362\n",
      "Iteration 77664, loss = 1.72362064\n",
      "Iteration 77665, loss = 1.50759800\n",
      "Iteration 77666, loss = 1.22927727\n",
      "Iteration 77667, loss = 1.16340314\n",
      "Iteration 77668, loss = 1.11662868\n",
      "Iteration 77669, loss = 1.16533900\n",
      "Iteration 77670, loss = 1.26986030\n",
      "Iteration 77671, loss = 1.22415105\n",
      "Iteration 77672, loss = 1.09342953\n",
      "Iteration 77673, loss = 1.04890941\n",
      "Iteration 77674, loss = 1.06526876\n",
      "Iteration 77675, loss = 0.98020112\n",
      "Iteration 77676, loss = 1.11789702\n",
      "Iteration 77677, loss = 1.03472632\n",
      "Iteration 77678, loss = 1.06554749\n",
      "Iteration 77679, loss = 1.02527784\n",
      "Iteration 77680, loss = 1.01841408\n",
      "Iteration 77681, loss = 0.99428772\n",
      "Iteration 77682, loss = 1.38566871\n",
      "Iteration 77683, loss = 1.12822791\n",
      "Iteration 77684, loss = 1.27054075\n",
      "Iteration 77685, loss = 1.16215609\n",
      "Iteration 77686, loss = 1.25419450\n",
      "Iteration 77687, loss = 1.35744691\n",
      "Iteration 77688, loss = 1.72292376\n",
      "Iteration 77689, loss = 1.47943951\n",
      "Iteration 77690, loss = 1.57785052\n",
      "Iteration 77691, loss = 1.50077870\n",
      "Iteration 77692, loss = 1.20752344\n",
      "Iteration 77693, loss = 1.21933853\n",
      "Iteration 77694, loss = 1.16986447\n",
      "Iteration 77695, loss = 1.12777125\n",
      "Iteration 77696, loss = 1.14381558\n",
      "Iteration 77697, loss = 1.08893414\n",
      "Iteration 77698, loss = 0.98213950\n",
      "Iteration 77699, loss = 0.97256424\n",
      "Iteration 77700, loss = 1.01405986\n",
      "Iteration 77701, loss = 1.16676367\n",
      "Iteration 77702, loss = 1.27182402\n",
      "Iteration 77703, loss = 1.28374098\n",
      "Iteration 77704, loss = 1.05469359\n",
      "Iteration 77705, loss = 1.19750548\n",
      "Iteration 77706, loss = 1.25803749\n",
      "Iteration 77707, loss = 1.12921252\n",
      "Iteration 77708, loss = 1.02831246\n",
      "Iteration 77709, loss = 1.18104157\n",
      "Iteration 77710, loss = 1.24713299\n",
      "Iteration 77711, loss = 1.20321731\n",
      "Iteration 77712, loss = 1.36257760\n",
      "Iteration 77713, loss = 1.38593668\n",
      "Iteration 77714, loss = 1.22665724\n",
      "Iteration 77715, loss = 1.07877248\n",
      "Iteration 77716, loss = 1.23539182\n",
      "Iteration 77717, loss = 1.17177712\n",
      "Iteration 77718, loss = 1.00442259\n",
      "Iteration 77719, loss = 0.99903786\n",
      "Iteration 77720, loss = 1.00333485\n",
      "Iteration 77721, loss = 0.98292099\n",
      "Iteration 77722, loss = 1.04325082\n",
      "Iteration 77723, loss = 1.05311521\n",
      "Iteration 77724, loss = 1.19076871\n",
      "Iteration 77725, loss = 1.17751885\n",
      "Iteration 77726, loss = 1.09402021\n",
      "Iteration 77727, loss = 1.17605969\n",
      "Iteration 77728, loss = 1.03792285\n",
      "Iteration 77729, loss = 1.02315544\n",
      "Iteration 77730, loss = 0.98851002\n",
      "Iteration 77731, loss = 1.15421613\n",
      "Iteration 77732, loss = 0.94830149\n",
      "Iteration 77733, loss = 1.12690219\n",
      "Iteration 77734, loss = 1.14188285\n",
      "Iteration 77735, loss = 1.18204396\n",
      "Iteration 77736, loss = 1.17256935\n",
      "Iteration 77737, loss = 1.19104891\n",
      "Iteration 77738, loss = 1.42553536\n",
      "Iteration 77739, loss = 1.75755229\n",
      "Iteration 77740, loss = 1.70022171\n",
      "Iteration 77741, loss = 1.33801196\n",
      "Iteration 77742, loss = 1.04076713\n",
      "Iteration 77743, loss = 1.26056917\n",
      "Iteration 77744, loss = 1.11359215\n",
      "Iteration 77745, loss = 1.18043404\n",
      "Iteration 77746, loss = 1.13687140\n",
      "Iteration 77747, loss = 1.11214277\n",
      "Iteration 77748, loss = 1.29062806\n",
      "Iteration 77749, loss = 1.18464999\n",
      "Iteration 77750, loss = 1.13793056\n",
      "Iteration 77751, loss = 1.15413649\n",
      "Iteration 77752, loss = 1.12176184\n",
      "Iteration 77753, loss = 1.06826440\n",
      "Iteration 77754, loss = 1.35339245\n",
      "Iteration 77755, loss = 1.46998468\n",
      "Iteration 77756, loss = 2.02731760\n",
      "Iteration 77757, loss = 1.61628407\n",
      "Iteration 77758, loss = 1.31623938\n",
      "Iteration 77759, loss = 1.46154700\n",
      "Iteration 77760, loss = 2.08609346\n",
      "Iteration 77761, loss = 2.13448134\n",
      "Iteration 77762, loss = 1.78387956\n",
      "Iteration 77763, loss = 1.98814625\n",
      "Iteration 77764, loss = 1.81562294\n",
      "Iteration 77765, loss = 1.32089348\n",
      "Iteration 77766, loss = 1.43672457\n",
      "Iteration 77767, loss = 1.17664932\n",
      "Iteration 77768, loss = 1.43572757\n",
      "Iteration 77769, loss = 1.18993788\n",
      "Iteration 77770, loss = 1.04881790\n",
      "Iteration 77771, loss = 0.98570180\n",
      "Iteration 77772, loss = 1.06157366\n",
      "Iteration 77773, loss = 1.20233425\n",
      "Iteration 77774, loss = 1.38439899\n",
      "Iteration 77775, loss = 1.33299020\n",
      "Iteration 77776, loss = 1.15385784\n",
      "Iteration 77777, loss = 1.18008449\n",
      "Iteration 77778, loss = 1.03882361\n",
      "Iteration 77779, loss = 1.33009628\n",
      "Iteration 77780, loss = 1.54112513\n",
      "Iteration 77781, loss = 1.16335184\n",
      "Iteration 77782, loss = 1.20908381\n",
      "Iteration 77783, loss = 1.22472192\n",
      "Iteration 77784, loss = 1.31634680\n",
      "Iteration 77785, loss = 1.61699456\n",
      "Iteration 77786, loss = 1.39172761\n",
      "Iteration 77787, loss = 1.27077829\n",
      "Iteration 77788, loss = 1.04267495\n",
      "Iteration 77789, loss = 1.20478868\n",
      "Iteration 77790, loss = 1.66954113\n",
      "Iteration 77791, loss = 1.35102082\n",
      "Iteration 77792, loss = 1.47316299\n",
      "Iteration 77793, loss = 1.29752677\n",
      "Iteration 77794, loss = 1.29950494\n",
      "Iteration 77795, loss = 1.06647045\n",
      "Iteration 77796, loss = 1.19609504\n",
      "Iteration 77797, loss = 1.32731057\n",
      "Iteration 77798, loss = 1.16009688\n",
      "Iteration 77799, loss = 1.16706224\n",
      "Iteration 77800, loss = 1.31087471\n",
      "Iteration 77801, loss = 1.21065182\n",
      "Iteration 77802, loss = 1.26492487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 77803, loss = 1.07818107\n",
      "Iteration 77804, loss = 1.18430626\n",
      "Iteration 77805, loss = 1.25791910\n",
      "Iteration 77806, loss = 1.18910205\n",
      "Iteration 77807, loss = 1.11082023\n",
      "Iteration 77808, loss = 1.00461481\n",
      "Iteration 77809, loss = 1.03243632\n",
      "Iteration 77810, loss = 1.15614818\n",
      "Iteration 77811, loss = 1.17517585\n",
      "Iteration 77812, loss = 1.03193937\n",
      "Iteration 77813, loss = 1.09259956\n",
      "Iteration 77814, loss = 1.20954808\n",
      "Iteration 77815, loss = 0.99247840\n",
      "Iteration 77816, loss = 1.02508283\n",
      "Iteration 77817, loss = 1.00223636\n",
      "Iteration 77818, loss = 0.95413294\n",
      "Iteration 77819, loss = 1.04779338\n",
      "Iteration 77820, loss = 1.00815583\n",
      "Iteration 77821, loss = 1.02831664\n",
      "Iteration 77822, loss = 1.11077863\n",
      "Iteration 77823, loss = 0.91374973\n",
      "Iteration 77824, loss = 1.11182879\n",
      "Iteration 77825, loss = 1.01602930\n",
      "Iteration 77826, loss = 1.20184757\n",
      "Iteration 77827, loss = 1.32038014\n",
      "Iteration 77828, loss = 1.07265542\n",
      "Iteration 77829, loss = 1.16438823\n",
      "Iteration 77830, loss = 1.43773470\n",
      "Iteration 77831, loss = 1.37594598\n",
      "Iteration 77832, loss = 1.22862257\n",
      "Iteration 77833, loss = 1.14982879\n",
      "Iteration 77834, loss = 1.21562586\n",
      "Iteration 77835, loss = 1.16221158\n",
      "Iteration 77836, loss = 1.09788749\n",
      "Iteration 77837, loss = 1.08790015\n",
      "Iteration 77838, loss = 1.07381708\n",
      "Iteration 77839, loss = 1.06193749\n",
      "Iteration 77840, loss = 1.17639340\n",
      "Iteration 77841, loss = 1.24013577\n",
      "Iteration 77842, loss = 1.31468419\n",
      "Iteration 77843, loss = 1.42795782\n",
      "Iteration 77844, loss = 1.32040270\n",
      "Iteration 77845, loss = 1.06712182\n",
      "Iteration 77846, loss = 1.02017110\n",
      "Iteration 77847, loss = 1.16014512\n",
      "Iteration 77848, loss = 0.98694857\n",
      "Iteration 77849, loss = 0.97749884\n",
      "Iteration 77850, loss = 0.97585024\n",
      "Iteration 77851, loss = 1.17317900\n",
      "Iteration 77852, loss = 1.16689958\n",
      "Iteration 77853, loss = 1.02985789\n",
      "Iteration 77854, loss = 1.14447123\n",
      "Iteration 77855, loss = 1.54266890\n",
      "Iteration 77856, loss = 1.19045719\n",
      "Iteration 77857, loss = 1.02592722\n",
      "Iteration 77858, loss = 1.22457479\n",
      "Iteration 77859, loss = 1.24027125\n",
      "Iteration 77860, loss = 1.38358741\n",
      "Iteration 77861, loss = 1.22060359\n",
      "Iteration 77862, loss = 1.16687435\n",
      "Iteration 77863, loss = 1.23202809\n",
      "Iteration 77864, loss = 1.35502957\n",
      "Iteration 77865, loss = 1.53372867\n",
      "Iteration 77866, loss = 3.09068745\n",
      "Iteration 77867, loss = 3.07954201\n",
      "Iteration 77868, loss = 4.47294098\n",
      "Iteration 77869, loss = 2.18262473\n",
      "Iteration 77870, loss = 1.90458512\n",
      "Iteration 77871, loss = 1.90133719\n",
      "Iteration 77872, loss = 1.85876553\n",
      "Iteration 77873, loss = 1.50333464\n",
      "Iteration 77874, loss = 1.31080068\n",
      "Iteration 77875, loss = 1.46773953\n",
      "Iteration 77876, loss = 1.61169259\n",
      "Iteration 77877, loss = 1.28697708\n",
      "Iteration 77878, loss = 1.75325270\n",
      "Iteration 77879, loss = 2.07558579\n",
      "Iteration 77880, loss = 3.36699584\n",
      "Iteration 77881, loss = 2.77497631\n",
      "Iteration 77882, loss = 2.43728388\n",
      "Iteration 77883, loss = 1.85065368\n",
      "Iteration 77884, loss = 1.96772558\n",
      "Iteration 77885, loss = 1.87057306\n",
      "Iteration 77886, loss = 1.45802719\n",
      "Iteration 77887, loss = 1.73707900\n",
      "Iteration 77888, loss = 1.38254196\n",
      "Iteration 77889, loss = 1.43546464\n",
      "Iteration 77890, loss = 1.03736928\n",
      "Iteration 77891, loss = 1.06605368\n",
      "Iteration 77892, loss = 1.00499536\n",
      "Iteration 77893, loss = 1.07882244\n",
      "Iteration 77894, loss = 1.30634894\n",
      "Iteration 77895, loss = 1.49440813\n",
      "Iteration 77896, loss = 1.35198124\n",
      "Iteration 77897, loss = 1.62118468\n",
      "Iteration 77898, loss = 1.47134783\n",
      "Iteration 77899, loss = 1.17124711\n",
      "Iteration 77900, loss = 1.02554492\n",
      "Iteration 77901, loss = 1.19497650\n",
      "Iteration 77902, loss = 1.12688646\n",
      "Iteration 77903, loss = 1.06393273\n",
      "Iteration 77904, loss = 1.25027229\n",
      "Iteration 77905, loss = 0.98431281\n",
      "Iteration 77906, loss = 1.23177348\n",
      "Iteration 77907, loss = 1.11193207\n",
      "Iteration 77908, loss = 1.04944240\n",
      "Iteration 77909, loss = 1.00439027\n",
      "Iteration 77910, loss = 0.98279776\n",
      "Iteration 77911, loss = 1.24735401\n",
      "Iteration 77912, loss = 1.29186362\n",
      "Iteration 77913, loss = 1.32774216\n",
      "Iteration 77914, loss = 1.29254249\n",
      "Iteration 77915, loss = 1.37424949\n",
      "Iteration 77916, loss = 1.46013367\n",
      "Iteration 77917, loss = 1.25573422\n",
      "Iteration 77918, loss = 1.13295620\n",
      "Iteration 77919, loss = 0.97074803\n",
      "Iteration 77920, loss = 0.98388650\n",
      "Iteration 77921, loss = 1.03597060\n",
      "Iteration 77922, loss = 0.99626100\n",
      "Iteration 77923, loss = 0.91475378\n",
      "Iteration 77924, loss = 0.93205723\n",
      "Iteration 77925, loss = 0.97743628\n",
      "Iteration 77926, loss = 0.98116186\n",
      "Iteration 77927, loss = 1.01859836\n",
      "Iteration 77928, loss = 0.97118605\n",
      "Iteration 77929, loss = 0.99849200\n",
      "Iteration 77930, loss = 1.06961259\n",
      "Iteration 77931, loss = 1.09625852\n",
      "Iteration 77932, loss = 1.00790646\n",
      "Iteration 77933, loss = 0.98061772\n",
      "Iteration 77934, loss = 0.93643096\n",
      "Iteration 77935, loss = 0.92201894\n",
      "Iteration 77936, loss = 0.90309652\n",
      "Iteration 77937, loss = 0.96460817\n",
      "Iteration 77938, loss = 0.99781993\n",
      "Iteration 77939, loss = 1.12823640\n",
      "Iteration 77940, loss = 1.14469684\n",
      "Iteration 77941, loss = 1.08630713\n",
      "Iteration 77942, loss = 1.10503571\n",
      "Iteration 77943, loss = 1.05932735\n",
      "Iteration 77944, loss = 1.26147844\n",
      "Iteration 77945, loss = 1.19161390\n",
      "Iteration 77946, loss = 1.02471728\n",
      "Iteration 77947, loss = 1.02165760\n",
      "Iteration 77948, loss = 1.17282878\n",
      "Iteration 77949, loss = 1.19131727\n",
      "Iteration 77950, loss = 0.98339341\n",
      "Iteration 77951, loss = 1.03685658\n",
      "Iteration 77952, loss = 1.20421614\n",
      "Iteration 77953, loss = 1.22472410\n",
      "Iteration 77954, loss = 1.28799734\n",
      "Iteration 77955, loss = 1.29615892\n",
      "Iteration 77956, loss = 1.07153044\n",
      "Iteration 77957, loss = 1.02367891\n",
      "Iteration 77958, loss = 1.09891598\n",
      "Iteration 77959, loss = 1.20662195\n",
      "Iteration 77960, loss = 1.13168996\n",
      "Iteration 77961, loss = 1.09335848\n",
      "Iteration 77962, loss = 1.25892131\n",
      "Iteration 77963, loss = 1.24357821\n",
      "Iteration 77964, loss = 1.18422588\n",
      "Iteration 77965, loss = 1.24329789\n",
      "Iteration 77966, loss = 1.29907649\n",
      "Iteration 77967, loss = 1.23238988\n",
      "Iteration 77968, loss = 1.27981819\n",
      "Iteration 77969, loss = 1.42691294\n",
      "Iteration 77970, loss = 1.35340655\n",
      "Iteration 77971, loss = 1.41307422\n",
      "Iteration 77972, loss = 1.41519680\n",
      "Iteration 77973, loss = 1.27808585\n",
      "Iteration 77974, loss = 1.18444015\n",
      "Iteration 77975, loss = 1.27322107\n",
      "Iteration 77976, loss = 1.25195808\n",
      "Iteration 77977, loss = 1.23949706\n",
      "Iteration 77978, loss = 1.18919751\n",
      "Iteration 77979, loss = 1.30778140\n",
      "Iteration 77980, loss = 1.16146226\n",
      "Iteration 77981, loss = 1.29916456\n",
      "Iteration 77982, loss = 1.13853249\n",
      "Iteration 77983, loss = 1.14711893\n",
      "Iteration 77984, loss = 1.01710912\n",
      "Iteration 77985, loss = 1.02706182\n",
      "Iteration 77986, loss = 1.07275373\n",
      "Iteration 77987, loss = 1.06916769\n",
      "Iteration 77988, loss = 1.01185865\n",
      "Iteration 77989, loss = 1.04407017\n",
      "Iteration 77990, loss = 1.12713252\n",
      "Iteration 77991, loss = 1.09463718\n",
      "Iteration 77992, loss = 1.10006062\n",
      "Iteration 77993, loss = 1.13461491\n",
      "Iteration 77994, loss = 1.16509747\n",
      "Iteration 77995, loss = 1.09392900\n",
      "Iteration 77996, loss = 0.96458655\n",
      "Iteration 77997, loss = 0.94531635\n",
      "Iteration 77998, loss = 1.00490522\n",
      "Iteration 77999, loss = 0.97574098\n",
      "Iteration 78000, loss = 0.92554136\n",
      "Iteration 78001, loss = 0.98602475\n",
      "Iteration 78002, loss = 1.01646385\n",
      "Iteration 78003, loss = 1.19628960\n",
      "Iteration 78004, loss = 1.41817463\n",
      "Iteration 78005, loss = 1.05166143\n",
      "Iteration 78006, loss = 1.03875458\n",
      "Iteration 78007, loss = 1.21667931\n",
      "Iteration 78008, loss = 1.09834377\n",
      "Iteration 78009, loss = 0.98134088\n",
      "Iteration 78010, loss = 1.00550728\n",
      "Iteration 78011, loss = 0.95195360\n",
      "Iteration 78012, loss = 0.98171141\n",
      "Iteration 78013, loss = 1.02033279\n",
      "Iteration 78014, loss = 1.07357902\n",
      "Iteration 78015, loss = 1.07984276\n",
      "Iteration 78016, loss = 1.12279833\n",
      "Iteration 78017, loss = 1.07860306\n",
      "Iteration 78018, loss = 0.93644880\n",
      "Iteration 78019, loss = 1.00257661\n",
      "Iteration 78020, loss = 1.22213205\n",
      "Iteration 78021, loss = 1.31013240\n",
      "Iteration 78022, loss = 1.43983132\n",
      "Iteration 78023, loss = 1.32931708\n",
      "Iteration 78024, loss = 1.14352861\n",
      "Iteration 78025, loss = 0.98605548\n",
      "Iteration 78026, loss = 0.98296752\n",
      "Iteration 78027, loss = 1.12133954\n",
      "Iteration 78028, loss = 1.21159552\n",
      "Iteration 78029, loss = 1.17973730\n",
      "Iteration 78030, loss = 1.28915153\n",
      "Iteration 78031, loss = 1.22508886\n",
      "Iteration 78032, loss = 1.14866042\n",
      "Iteration 78033, loss = 1.06666426\n",
      "Iteration 78034, loss = 1.06793344\n",
      "Iteration 78035, loss = 0.96471374\n",
      "Iteration 78036, loss = 1.02148264\n",
      "Iteration 78037, loss = 0.96791331\n",
      "Iteration 78038, loss = 0.99636767\n",
      "Iteration 78039, loss = 0.96849850\n",
      "Iteration 78040, loss = 1.09872179\n",
      "Iteration 78041, loss = 1.09385630\n",
      "Iteration 78042, loss = 1.55037046\n",
      "Iteration 78043, loss = 1.74220672\n",
      "Iteration 78044, loss = 2.64279214\n",
      "Iteration 78045, loss = 3.55562547\n",
      "Iteration 78046, loss = 5.06209567\n",
      "Iteration 78047, loss = 3.52198468\n",
      "Iteration 78048, loss = 2.97720267\n",
      "Iteration 78049, loss = 2.53315245\n",
      "Iteration 78050, loss = 1.91521857\n",
      "Iteration 78051, loss = 1.93921004\n",
      "Iteration 78052, loss = 1.60201858\n",
      "Iteration 78053, loss = 1.33077689\n",
      "Iteration 78054, loss = 1.68731539\n",
      "Iteration 78055, loss = 1.64033279\n",
      "Iteration 78056, loss = 1.65113117\n",
      "Iteration 78057, loss = 1.70285042\n",
      "Iteration 78058, loss = 1.55674961\n",
      "Iteration 78059, loss = 1.17404708\n",
      "Iteration 78060, loss = 1.32976330\n",
      "Iteration 78061, loss = 1.35408049\n",
      "Iteration 78062, loss = 1.47792741\n",
      "Iteration 78063, loss = 1.23281903\n",
      "Iteration 78064, loss = 1.31015724\n",
      "Iteration 78065, loss = 1.34231617\n",
      "Iteration 78066, loss = 1.27849619\n",
      "Iteration 78067, loss = 1.13536519\n",
      "Iteration 78068, loss = 1.12552964\n",
      "Iteration 78069, loss = 1.08008635\n",
      "Iteration 78070, loss = 0.95775929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 78071, loss = 1.00319907\n",
      "Iteration 78072, loss = 0.99025106\n",
      "Iteration 78073, loss = 1.03226121\n",
      "Iteration 78074, loss = 1.08698743\n",
      "Iteration 78075, loss = 1.07692333\n",
      "Iteration 78076, loss = 1.05186512\n",
      "Iteration 78077, loss = 1.41733310\n",
      "Iteration 78078, loss = 1.14679996\n",
      "Iteration 78079, loss = 1.09889094\n",
      "Iteration 78080, loss = 1.03240459\n",
      "Iteration 78081, loss = 1.12576615\n",
      "Iteration 78082, loss = 1.37776519\n",
      "Iteration 78083, loss = 1.21699148\n",
      "Iteration 78084, loss = 1.11941439\n",
      "Iteration 78085, loss = 1.12198305\n",
      "Iteration 78086, loss = 0.97456660\n",
      "Iteration 78087, loss = 1.07655631\n",
      "Iteration 78088, loss = 1.11144581\n",
      "Iteration 78089, loss = 0.94878122\n",
      "Iteration 78090, loss = 0.91621865\n",
      "Iteration 78091, loss = 1.04674554\n",
      "Iteration 78092, loss = 1.16447999\n",
      "Iteration 78093, loss = 1.48379781\n",
      "Iteration 78094, loss = 1.41321726\n",
      "Iteration 78095, loss = 1.37861750\n",
      "Iteration 78096, loss = 1.39400600\n",
      "Iteration 78097, loss = 1.45941113\n",
      "Iteration 78098, loss = 1.62201573\n",
      "Iteration 78099, loss = 1.97653019\n",
      "Iteration 78100, loss = 2.55608922\n",
      "Iteration 78101, loss = 2.02407519\n",
      "Iteration 78102, loss = 2.22814206\n",
      "Iteration 78103, loss = 1.53079785\n",
      "Iteration 78104, loss = 1.39171358\n",
      "Iteration 78105, loss = 1.33705581\n",
      "Iteration 78106, loss = 1.57364227\n",
      "Iteration 78107, loss = 1.49558811\n",
      "Iteration 78108, loss = 1.70173003\n",
      "Iteration 78109, loss = 1.66880854\n",
      "Iteration 78110, loss = 1.23626953\n",
      "Iteration 78111, loss = 1.45613321\n",
      "Iteration 78112, loss = 1.37196709\n",
      "Iteration 78113, loss = 1.17575717\n",
      "Iteration 78114, loss = 1.06113903\n",
      "Iteration 78115, loss = 1.03252462\n",
      "Iteration 78116, loss = 1.05086891\n",
      "Iteration 78117, loss = 1.08246208\n",
      "Iteration 78118, loss = 1.27848673\n",
      "Iteration 78119, loss = 1.27327566\n",
      "Iteration 78120, loss = 0.98012056\n",
      "Iteration 78121, loss = 0.94746802\n",
      "Iteration 78122, loss = 0.96819585\n",
      "Iteration 78123, loss = 0.97699615\n",
      "Iteration 78124, loss = 0.95364408\n",
      "Iteration 78125, loss = 1.09942111\n",
      "Iteration 78126, loss = 1.13860925\n",
      "Iteration 78127, loss = 1.13538517\n",
      "Iteration 78128, loss = 1.34508649\n",
      "Iteration 78129, loss = 1.35104758\n",
      "Iteration 78130, loss = 1.38010587\n",
      "Iteration 78131, loss = 1.27862633\n",
      "Iteration 78132, loss = 1.09219110\n",
      "Iteration 78133, loss = 1.18491450\n",
      "Iteration 78134, loss = 1.33066690\n",
      "Iteration 78135, loss = 1.37446220\n",
      "Iteration 78136, loss = 1.32561207\n",
      "Iteration 78137, loss = 1.07473961\n",
      "Iteration 78138, loss = 1.25140205\n",
      "Iteration 78139, loss = 1.25254934\n",
      "Iteration 78140, loss = 1.70437251\n",
      "Iteration 78141, loss = 1.75451384\n",
      "Iteration 78142, loss = 1.62975885\n",
      "Iteration 78143, loss = 1.32756585\n",
      "Iteration 78144, loss = 1.26649702\n",
      "Iteration 78145, loss = 1.01738761\n",
      "Iteration 78146, loss = 0.97670498\n",
      "Iteration 78147, loss = 1.06942487\n",
      "Iteration 78148, loss = 1.04983906\n",
      "Iteration 78149, loss = 1.09216894\n",
      "Iteration 78150, loss = 1.11492084\n",
      "Iteration 78151, loss = 1.06808616\n",
      "Iteration 78152, loss = 1.08745334\n",
      "Iteration 78153, loss = 1.03811518\n",
      "Iteration 78154, loss = 1.00374626\n",
      "Iteration 78155, loss = 0.94900852\n",
      "Iteration 78156, loss = 0.98041298\n",
      "Iteration 78157, loss = 0.97547240\n",
      "Iteration 78158, loss = 0.94411027\n",
      "Iteration 78159, loss = 0.95544678\n",
      "Iteration 78160, loss = 1.08424140\n",
      "Iteration 78161, loss = 1.05582905\n",
      "Iteration 78162, loss = 1.04945731\n",
      "Iteration 78163, loss = 0.87936899\n",
      "Iteration 78164, loss = 0.98238838\n",
      "Iteration 78165, loss = 0.94760659\n",
      "Iteration 78166, loss = 0.91053220\n",
      "Iteration 78167, loss = 0.98663470\n",
      "Iteration 78168, loss = 0.92044709\n",
      "Iteration 78169, loss = 0.96388524\n",
      "Iteration 78170, loss = 0.98576255\n",
      "Iteration 78171, loss = 1.06915154\n",
      "Iteration 78172, loss = 1.14722458\n",
      "Iteration 78173, loss = 1.31546407\n",
      "Iteration 78174, loss = 1.29761664\n",
      "Iteration 78175, loss = 1.16185251\n",
      "Iteration 78176, loss = 1.16187792\n",
      "Iteration 78177, loss = 1.03820564\n",
      "Iteration 78178, loss = 0.95230257\n",
      "Iteration 78179, loss = 1.00655817\n",
      "Iteration 78180, loss = 1.18040346\n",
      "Iteration 78181, loss = 1.13422649\n",
      "Iteration 78182, loss = 1.30387572\n",
      "Iteration 78183, loss = 1.08514829\n",
      "Iteration 78184, loss = 1.03999984\n",
      "Iteration 78185, loss = 1.02251439\n",
      "Iteration 78186, loss = 1.16413843\n",
      "Iteration 78187, loss = 1.11461094\n",
      "Iteration 78188, loss = 1.38874930\n",
      "Iteration 78189, loss = 1.58531581\n",
      "Iteration 78190, loss = 1.29310839\n",
      "Iteration 78191, loss = 1.28483681\n",
      "Iteration 78192, loss = 1.32271996\n",
      "Iteration 78193, loss = 1.19094318\n",
      "Iteration 78194, loss = 1.17977812\n",
      "Iteration 78195, loss = 1.09487521\n",
      "Iteration 78196, loss = 1.19102307\n",
      "Iteration 78197, loss = 1.10632316\n",
      "Iteration 78198, loss = 1.23428914\n",
      "Iteration 78199, loss = 1.16866240\n",
      "Iteration 78200, loss = 1.09431922\n",
      "Iteration 78201, loss = 0.92307336\n",
      "Iteration 78202, loss = 0.96787042\n",
      "Iteration 78203, loss = 0.96660399\n",
      "Iteration 78204, loss = 1.12856627\n",
      "Iteration 78205, loss = 1.08331864\n",
      "Iteration 78206, loss = 1.17109132\n",
      "Iteration 78207, loss = 1.41479020\n",
      "Iteration 78208, loss = 1.29511871\n",
      "Iteration 78209, loss = 1.32034201\n",
      "Iteration 78210, loss = 1.47957405\n",
      "Iteration 78211, loss = 1.23615407\n",
      "Iteration 78212, loss = 1.28867571\n",
      "Iteration 78213, loss = 1.04945469\n",
      "Iteration 78214, loss = 1.08628165\n",
      "Iteration 78215, loss = 1.00446753\n",
      "Iteration 78216, loss = 1.01221473\n",
      "Iteration 78217, loss = 1.02660913\n",
      "Iteration 78218, loss = 1.07320881\n",
      "Iteration 78219, loss = 0.99255945\n",
      "Iteration 78220, loss = 1.01131287\n",
      "Iteration 78221, loss = 0.98570155\n",
      "Iteration 78222, loss = 1.00310878\n",
      "Iteration 78223, loss = 1.08612314\n",
      "Iteration 78224, loss = 1.01121647\n",
      "Iteration 78225, loss = 0.96770981\n",
      "Iteration 78226, loss = 1.03595281\n",
      "Iteration 78227, loss = 1.03593956\n",
      "Iteration 78228, loss = 1.17892091\n",
      "Iteration 78229, loss = 1.10515679\n",
      "Iteration 78230, loss = 1.05692910\n",
      "Iteration 78231, loss = 1.04704761\n",
      "Iteration 78232, loss = 1.07138450\n",
      "Iteration 78233, loss = 1.20137037\n",
      "Iteration 78234, loss = 1.01334075\n",
      "Iteration 78235, loss = 1.23278310\n",
      "Iteration 78236, loss = 1.19728327\n",
      "Iteration 78237, loss = 1.15232058\n",
      "Iteration 78238, loss = 0.96034797\n",
      "Iteration 78239, loss = 1.03621220\n",
      "Iteration 78240, loss = 1.56212488\n",
      "Iteration 78241, loss = 1.28526549\n",
      "Iteration 78242, loss = 1.37679900\n",
      "Iteration 78243, loss = 1.71453853\n",
      "Iteration 78244, loss = 1.62717968\n",
      "Iteration 78245, loss = 1.64037273\n",
      "Iteration 78246, loss = 1.55299738\n",
      "Iteration 78247, loss = 2.52181362\n",
      "Iteration 78248, loss = 2.00529573\n",
      "Iteration 78249, loss = 1.68525567\n",
      "Iteration 78250, loss = 1.44205171\n",
      "Iteration 78251, loss = 1.17705672\n",
      "Iteration 78252, loss = 1.09674034\n",
      "Iteration 78253, loss = 1.45985259\n",
      "Iteration 78254, loss = 1.23300752\n",
      "Iteration 78255, loss = 1.20537298\n",
      "Iteration 78256, loss = 1.07106500\n",
      "Iteration 78257, loss = 1.10531353\n",
      "Iteration 78258, loss = 1.08321866\n",
      "Iteration 78259, loss = 1.34355043\n",
      "Iteration 78260, loss = 1.32657835\n",
      "Iteration 78261, loss = 1.21046745\n",
      "Iteration 78262, loss = 1.11724013\n",
      "Iteration 78263, loss = 1.13399497\n",
      "Iteration 78264, loss = 0.99235020\n",
      "Iteration 78265, loss = 0.99690181\n",
      "Iteration 78266, loss = 0.95313296\n",
      "Iteration 78267, loss = 0.96051572\n",
      "Iteration 78268, loss = 0.99543292\n",
      "Iteration 78269, loss = 1.11065964\n",
      "Iteration 78270, loss = 1.12971651\n",
      "Iteration 78271, loss = 1.27040083\n",
      "Iteration 78272, loss = 1.04381388\n",
      "Iteration 78273, loss = 1.04065489\n",
      "Iteration 78274, loss = 1.00293044\n",
      "Iteration 78275, loss = 1.20523624\n",
      "Iteration 78276, loss = 1.32563960\n",
      "Iteration 78277, loss = 1.19761065\n",
      "Iteration 78278, loss = 1.37291433\n",
      "Iteration 78279, loss = 1.78789766\n",
      "Iteration 78280, loss = 1.66947380\n",
      "Iteration 78281, loss = 1.10758609\n",
      "Iteration 78282, loss = 1.45052663\n",
      "Iteration 78283, loss = 1.16715069\n",
      "Iteration 78284, loss = 1.29931362\n",
      "Iteration 78285, loss = 1.87081502\n",
      "Iteration 78286, loss = 1.47627841\n",
      "Iteration 78287, loss = 1.99679323\n",
      "Iteration 78288, loss = 2.21355117\n",
      "Iteration 78289, loss = 3.09582123\n",
      "Iteration 78290, loss = 2.87578598\n",
      "Iteration 78291, loss = 1.74669388\n",
      "Iteration 78292, loss = 1.56022287\n",
      "Iteration 78293, loss = 1.76897557\n",
      "Iteration 78294, loss = 1.47884498\n",
      "Iteration 78295, loss = 1.58541131\n",
      "Iteration 78296, loss = 1.38708449\n",
      "Iteration 78297, loss = 1.26036796\n",
      "Iteration 78298, loss = 1.26497045\n",
      "Iteration 78299, loss = 1.59535954\n",
      "Iteration 78300, loss = 1.36405416\n",
      "Iteration 78301, loss = 1.54870138\n",
      "Iteration 78302, loss = 1.56010046\n",
      "Iteration 78303, loss = 1.10463991\n",
      "Iteration 78304, loss = 1.06752671\n",
      "Iteration 78305, loss = 1.05866657\n",
      "Iteration 78306, loss = 1.11323312\n",
      "Iteration 78307, loss = 1.10724708\n",
      "Iteration 78308, loss = 1.37460035\n",
      "Iteration 78309, loss = 1.41727822\n",
      "Iteration 78310, loss = 1.48492992\n",
      "Iteration 78311, loss = 1.47707623\n",
      "Iteration 78312, loss = 1.83485419\n",
      "Iteration 78313, loss = 1.63965170\n",
      "Iteration 78314, loss = 1.43770952\n",
      "Iteration 78315, loss = 1.38513520\n",
      "Iteration 78316, loss = 1.22314689\n",
      "Iteration 78317, loss = 1.17564443\n",
      "Iteration 78318, loss = 1.22633852\n",
      "Iteration 78319, loss = 1.18281744\n",
      "Iteration 78320, loss = 1.02808201\n",
      "Iteration 78321, loss = 1.03909087\n",
      "Iteration 78322, loss = 1.03962252\n",
      "Iteration 78323, loss = 1.04322339\n",
      "Iteration 78324, loss = 1.00902350\n",
      "Iteration 78325, loss = 1.22953068\n",
      "Iteration 78326, loss = 1.03180642\n",
      "Iteration 78327, loss = 1.04464317\n",
      "Iteration 78328, loss = 0.99809490\n",
      "Iteration 78329, loss = 1.07246738\n",
      "Iteration 78330, loss = 0.91530846\n",
      "Iteration 78331, loss = 1.11875177\n",
      "Iteration 78332, loss = 1.08226569\n",
      "Iteration 78333, loss = 1.03887332\n",
      "Iteration 78334, loss = 0.91535752\n",
      "Iteration 78335, loss = 0.91725235\n",
      "Iteration 78336, loss = 0.93444128\n",
      "Iteration 78337, loss = 0.91522289\n",
      "Iteration 78338, loss = 0.93903813\n",
      "Iteration 78339, loss = 0.96842610\n",
      "Iteration 78340, loss = 0.96967987\n",
      "Iteration 78341, loss = 0.95879126\n",
      "Iteration 78342, loss = 1.02905603\n",
      "Iteration 78343, loss = 1.03392054\n",
      "Iteration 78344, loss = 0.94181219\n",
      "Iteration 78345, loss = 0.93798618\n",
      "Iteration 78346, loss = 1.04913372\n",
      "Iteration 78347, loss = 0.99308667\n",
      "Iteration 78348, loss = 0.95275939\n",
      "Iteration 78349, loss = 1.12313253\n",
      "Iteration 78350, loss = 1.04596863\n",
      "Iteration 78351, loss = 1.12500412\n",
      "Iteration 78352, loss = 1.03304030\n",
      "Iteration 78353, loss = 1.08155404\n",
      "Iteration 78354, loss = 1.33293573\n",
      "Iteration 78355, loss = 1.31752890\n",
      "Iteration 78356, loss = 1.30282321\n",
      "Iteration 78357, loss = 1.38842668\n",
      "Iteration 78358, loss = 1.08390658\n",
      "Iteration 78359, loss = 1.08359430\n",
      "Iteration 78360, loss = 0.93763276\n",
      "Iteration 78361, loss = 1.01696117\n",
      "Iteration 78362, loss = 1.23767995\n",
      "Iteration 78363, loss = 1.18528239\n",
      "Iteration 78364, loss = 1.56984779\n",
      "Iteration 78365, loss = 1.43294602\n",
      "Iteration 78366, loss = 1.56529840\n",
      "Iteration 78367, loss = 1.32729191\n",
      "Iteration 78368, loss = 1.54810681\n",
      "Iteration 78369, loss = 1.42569212\n",
      "Iteration 78370, loss = 1.31832248\n",
      "Iteration 78371, loss = 1.16842668\n",
      "Iteration 78372, loss = 1.23304962\n",
      "Iteration 78373, loss = 1.14822289\n",
      "Iteration 78374, loss = 0.97021882\n",
      "Iteration 78375, loss = 1.30912302\n",
      "Iteration 78376, loss = 1.25046828\n",
      "Iteration 78377, loss = 1.48116667\n",
      "Iteration 78378, loss = 1.35206084\n",
      "Iteration 78379, loss = 1.18605167\n",
      "Iteration 78380, loss = 1.29094197\n",
      "Iteration 78381, loss = 2.12685264\n",
      "Iteration 78382, loss = 1.83225508\n",
      "Iteration 78383, loss = 2.08359396\n",
      "Iteration 78384, loss = 1.57639697\n",
      "Iteration 78385, loss = 1.49327148\n",
      "Iteration 78386, loss = 1.31873093\n",
      "Iteration 78387, loss = 1.51783095\n",
      "Iteration 78388, loss = 1.24642777\n",
      "Iteration 78389, loss = 1.10444644\n",
      "Iteration 78390, loss = 0.98752328\n",
      "Iteration 78391, loss = 0.96503539\n",
      "Iteration 78392, loss = 0.97987441\n",
      "Iteration 78393, loss = 0.99753472\n",
      "Iteration 78394, loss = 1.14538917\n",
      "Iteration 78395, loss = 1.07191707\n",
      "Iteration 78396, loss = 1.30864140\n",
      "Iteration 78397, loss = 1.40366714\n",
      "Iteration 78398, loss = 1.96043790\n",
      "Iteration 78399, loss = 1.78738449\n",
      "Iteration 78400, loss = 1.55919150\n",
      "Iteration 78401, loss = 1.13962421\n",
      "Iteration 78402, loss = 1.11118837\n",
      "Iteration 78403, loss = 1.18971445\n",
      "Iteration 78404, loss = 1.15151784\n",
      "Iteration 78405, loss = 1.31165320\n",
      "Iteration 78406, loss = 1.27518240\n",
      "Iteration 78407, loss = 1.55470978\n",
      "Iteration 78408, loss = 1.56792504\n",
      "Iteration 78409, loss = 1.19557326\n",
      "Iteration 78410, loss = 1.04184750\n",
      "Iteration 78411, loss = 1.05233299\n",
      "Iteration 78412, loss = 1.13891328\n",
      "Iteration 78413, loss = 1.01943715\n",
      "Iteration 78414, loss = 1.09358708\n",
      "Iteration 78415, loss = 1.11979586\n",
      "Iteration 78416, loss = 1.24928897\n",
      "Iteration 78417, loss = 1.16078981\n",
      "Iteration 78418, loss = 1.18039621\n",
      "Iteration 78419, loss = 1.58429191\n",
      "Iteration 78420, loss = 1.34755852\n",
      "Iteration 78421, loss = 1.31426521\n",
      "Iteration 78422, loss = 1.05443951\n",
      "Iteration 78423, loss = 0.97036999\n",
      "Iteration 78424, loss = 1.02603342\n",
      "Iteration 78425, loss = 1.03455340\n",
      "Iteration 78426, loss = 1.00852144\n",
      "Iteration 78427, loss = 1.01771908\n",
      "Iteration 78428, loss = 1.02160138\n",
      "Iteration 78429, loss = 1.26591496\n",
      "Iteration 78430, loss = 1.08260173\n",
      "Iteration 78431, loss = 1.14396903\n",
      "Iteration 78432, loss = 2.00584984\n",
      "Iteration 78433, loss = 1.68601943\n",
      "Iteration 78434, loss = 1.35976656\n",
      "Iteration 78435, loss = 1.26742615\n",
      "Iteration 78436, loss = 1.10665475\n",
      "Iteration 78437, loss = 1.25288643\n",
      "Iteration 78438, loss = 1.26043410\n",
      "Iteration 78439, loss = 1.46168666\n",
      "Iteration 78440, loss = 1.19512402\n",
      "Iteration 78441, loss = 1.30147152\n",
      "Iteration 78442, loss = 1.39016267\n",
      "Iteration 78443, loss = 1.84533739\n",
      "Iteration 78444, loss = 1.93107470\n",
      "Iteration 78445, loss = 1.60340715\n",
      "Iteration 78446, loss = 1.81824762\n",
      "Iteration 78447, loss = 2.14961408\n",
      "Iteration 78448, loss = 2.04739090\n",
      "Iteration 78449, loss = 1.49400424\n",
      "Iteration 78450, loss = 2.75029239\n",
      "Iteration 78451, loss = 2.01448079\n",
      "Iteration 78452, loss = 2.72841741\n",
      "Iteration 78453, loss = 2.56551721\n",
      "Iteration 78454, loss = 1.84388266\n",
      "Iteration 78455, loss = 1.43123885\n",
      "Iteration 78456, loss = 1.18112207\n",
      "Iteration 78457, loss = 1.04285416\n",
      "Iteration 78458, loss = 1.25366877\n",
      "Iteration 78459, loss = 1.40982911\n",
      "Iteration 78460, loss = 1.25235693\n",
      "Iteration 78461, loss = 1.45498174\n",
      "Iteration 78462, loss = 1.37625067\n",
      "Iteration 78463, loss = 1.06613257\n",
      "Iteration 78464, loss = 1.18132197\n",
      "Iteration 78465, loss = 1.03047029\n",
      "Iteration 78466, loss = 1.16224767\n",
      "Iteration 78467, loss = 0.90724003\n",
      "Iteration 78468, loss = 1.23242162\n",
      "Iteration 78469, loss = 2.01540111\n",
      "Iteration 78470, loss = 1.93868334\n",
      "Iteration 78471, loss = 1.73035120\n",
      "Iteration 78472, loss = 1.54664606\n",
      "Iteration 78473, loss = 1.72495930\n",
      "Iteration 78474, loss = 1.43313326\n",
      "Iteration 78475, loss = 1.36781665\n",
      "Iteration 78476, loss = 1.54040062\n",
      "Iteration 78477, loss = 1.31358804\n",
      "Iteration 78478, loss = 1.17010162\n",
      "Iteration 78479, loss = 1.15759134\n",
      "Iteration 78480, loss = 1.20773510\n",
      "Iteration 78481, loss = 1.06822926\n",
      "Iteration 78482, loss = 1.26670271\n",
      "Iteration 78483, loss = 1.40392275\n",
      "Iteration 78484, loss = 1.13091890\n",
      "Iteration 78485, loss = 1.07313626\n",
      "Iteration 78486, loss = 1.19218558\n",
      "Iteration 78487, loss = 1.02219478\n",
      "Iteration 78488, loss = 0.95039025\n",
      "Iteration 78489, loss = 1.12803762\n",
      "Iteration 78490, loss = 1.55269850\n",
      "Iteration 78491, loss = 1.64163130\n",
      "Iteration 78492, loss = 1.74887564\n",
      "Iteration 78493, loss = 1.43880551\n",
      "Iteration 78494, loss = 1.50650406\n",
      "Iteration 78495, loss = 1.47838416\n",
      "Iteration 78496, loss = 1.26511991\n",
      "Iteration 78497, loss = 1.37195369\n",
      "Iteration 78498, loss = 1.18313362\n",
      "Iteration 78499, loss = 1.11346757\n",
      "Iteration 78500, loss = 0.96896924\n",
      "Iteration 78501, loss = 0.91553671\n",
      "Iteration 78502, loss = 0.94456956\n",
      "Iteration 78503, loss = 0.92941953\n",
      "Iteration 78504, loss = 1.10850575\n",
      "Iteration 78505, loss = 1.14446089\n",
      "Iteration 78506, loss = 0.98456002\n",
      "Iteration 78507, loss = 0.91838123\n",
      "Iteration 78508, loss = 0.93307231\n",
      "Iteration 78509, loss = 0.99176164\n",
      "Iteration 78510, loss = 0.95983561\n",
      "Iteration 78511, loss = 1.03413188\n",
      "Iteration 78512, loss = 1.19539740\n",
      "Iteration 78513, loss = 1.00385510\n",
      "Iteration 78514, loss = 1.09544363\n",
      "Iteration 78515, loss = 1.16996382\n",
      "Iteration 78516, loss = 1.00752383\n",
      "Iteration 78517, loss = 0.98680511\n",
      "Iteration 78518, loss = 0.96670519\n",
      "Iteration 78519, loss = 1.00103081\n",
      "Iteration 78520, loss = 0.99875096\n",
      "Iteration 78521, loss = 0.96492259\n",
      "Iteration 78522, loss = 0.99382331\n",
      "Iteration 78523, loss = 0.98916862\n",
      "Iteration 78524, loss = 1.10732037\n",
      "Iteration 78525, loss = 1.17642087\n",
      "Iteration 78526, loss = 1.37187768\n",
      "Iteration 78527, loss = 1.24489537\n",
      "Iteration 78528, loss = 1.07812509\n",
      "Iteration 78529, loss = 1.18991687\n",
      "Iteration 78530, loss = 1.16849954\n",
      "Iteration 78531, loss = 0.99798875\n",
      "Iteration 78532, loss = 0.97781263\n",
      "Iteration 78533, loss = 1.10425462\n",
      "Iteration 78534, loss = 1.03074084\n",
      "Iteration 78535, loss = 1.04876811\n",
      "Iteration 78536, loss = 1.24322892\n",
      "Iteration 78537, loss = 1.22121487\n",
      "Iteration 78538, loss = 1.10747398\n",
      "Iteration 78539, loss = 1.26225770\n",
      "Iteration 78540, loss = 1.49758174\n",
      "Iteration 78541, loss = 1.41640930\n",
      "Iteration 78542, loss = 1.37702138\n",
      "Iteration 78543, loss = 1.38380110\n",
      "Iteration 78544, loss = 1.32890188\n",
      "Iteration 78545, loss = 1.44815892\n",
      "Iteration 78546, loss = 1.54916729\n",
      "Iteration 78547, loss = 1.25828989\n",
      "Iteration 78548, loss = 1.14591567\n",
      "Iteration 78549, loss = 1.29273679\n",
      "Iteration 78550, loss = 1.25801855\n",
      "Iteration 78551, loss = 1.19440285\n",
      "Iteration 78552, loss = 1.75173473\n",
      "Iteration 78553, loss = 1.64000642\n",
      "Iteration 78554, loss = 1.48083653\n",
      "Iteration 78555, loss = 1.69368801\n",
      "Iteration 78556, loss = 1.62150512\n",
      "Iteration 78557, loss = 1.50906023\n",
      "Iteration 78558, loss = 1.57909763\n",
      "Iteration 78559, loss = 1.49147072\n",
      "Iteration 78560, loss = 1.66920872\n",
      "Iteration 78561, loss = 1.56814969\n",
      "Iteration 78562, loss = 1.49276794\n",
      "Iteration 78563, loss = 1.38623398\n",
      "Iteration 78564, loss = 1.26963868\n",
      "Iteration 78565, loss = 1.30793634\n",
      "Iteration 78566, loss = 1.72701524\n",
      "Iteration 78567, loss = 1.67156496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 78568, loss = 2.76906830\n",
      "Iteration 78569, loss = 3.16853960\n",
      "Iteration 78570, loss = 4.04624599\n",
      "Iteration 78571, loss = 4.32515878\n",
      "Iteration 78572, loss = 4.41439650\n",
      "Iteration 78573, loss = 3.73138322\n",
      "Iteration 78574, loss = 2.73492889\n",
      "Iteration 78575, loss = 2.39275422\n",
      "Iteration 78576, loss = 2.16187874\n",
      "Iteration 78577, loss = 1.90812116\n",
      "Iteration 78578, loss = 2.05830617\n",
      "Iteration 78579, loss = 1.57523197\n",
      "Iteration 78580, loss = 1.57940428\n",
      "Iteration 78581, loss = 1.27129843\n",
      "Iteration 78582, loss = 1.23259321\n",
      "Iteration 78583, loss = 1.01388523\n",
      "Iteration 78584, loss = 0.95086650\n",
      "Iteration 78585, loss = 1.04629694\n",
      "Iteration 78586, loss = 1.11530833\n",
      "Iteration 78587, loss = 1.07178528\n",
      "Iteration 78588, loss = 0.99693842\n",
      "Iteration 78589, loss = 1.04253974\n",
      "Iteration 78590, loss = 1.26421567\n",
      "Iteration 78591, loss = 1.21745901\n",
      "Iteration 78592, loss = 1.35723424\n",
      "Iteration 78593, loss = 1.28694091\n",
      "Iteration 78594, loss = 1.29145682\n",
      "Iteration 78595, loss = 1.42016659\n",
      "Iteration 78596, loss = 1.23745698\n",
      "Iteration 78597, loss = 1.15912308\n",
      "Iteration 78598, loss = 1.21943012\n",
      "Iteration 78599, loss = 1.32508899\n",
      "Iteration 78600, loss = 0.95335232\n",
      "Iteration 78601, loss = 1.11417730\n",
      "Iteration 78602, loss = 1.24872485\n",
      "Iteration 78603, loss = 1.16494901\n",
      "Iteration 78604, loss = 1.40284969\n",
      "Iteration 78605, loss = 1.57738960\n",
      "Iteration 78606, loss = 1.15712679\n",
      "Iteration 78607, loss = 1.49680323\n",
      "Iteration 78608, loss = 1.45908703\n",
      "Iteration 78609, loss = 1.40479508\n",
      "Iteration 78610, loss = 1.32762458\n",
      "Iteration 78611, loss = 1.15522374\n",
      "Iteration 78612, loss = 1.02771456\n",
      "Iteration 78613, loss = 1.00574217\n",
      "Iteration 78614, loss = 1.05759724\n",
      "Iteration 78615, loss = 1.04619679\n",
      "Iteration 78616, loss = 1.07973063\n",
      "Iteration 78617, loss = 1.05838234\n",
      "Iteration 78618, loss = 1.26835189\n",
      "Iteration 78619, loss = 1.17864100\n",
      "Iteration 78620, loss = 0.98565541\n",
      "Iteration 78621, loss = 1.06845282\n",
      "Iteration 78622, loss = 1.12677189\n",
      "Iteration 78623, loss = 1.18731343\n",
      "Iteration 78624, loss = 1.36351652\n",
      "Iteration 78625, loss = 1.81468619\n",
      "Iteration 78626, loss = 2.00059242\n",
      "Iteration 78627, loss = 2.05362758\n",
      "Iteration 78628, loss = 1.33223172\n",
      "Iteration 78629, loss = 1.42071927\n",
      "Iteration 78630, loss = 1.23972362\n",
      "Iteration 78631, loss = 1.09326080\n",
      "Iteration 78632, loss = 1.27897976\n",
      "Iteration 78633, loss = 1.32405546\n",
      "Iteration 78634, loss = 1.41377472\n",
      "Iteration 78635, loss = 1.59135837\n",
      "Iteration 78636, loss = 1.38612611\n",
      "Iteration 78637, loss = 1.35317183\n",
      "Iteration 78638, loss = 1.05551794\n",
      "Iteration 78639, loss = 1.07216709\n",
      "Iteration 78640, loss = 1.01814821\n",
      "Iteration 78641, loss = 1.17356446\n",
      "Iteration 78642, loss = 1.12570472\n",
      "Iteration 78643, loss = 1.08420757\n",
      "Iteration 78644, loss = 1.11590618\n",
      "Iteration 78645, loss = 1.32284561\n",
      "Iteration 78646, loss = 1.30125945\n",
      "Iteration 78647, loss = 1.20921856\n",
      "Iteration 78648, loss = 0.98731070\n",
      "Iteration 78649, loss = 0.92385027\n",
      "Iteration 78650, loss = 0.92057123\n",
      "Iteration 78651, loss = 0.93088008\n",
      "Iteration 78652, loss = 1.06012924\n",
      "Iteration 78653, loss = 1.30929541\n",
      "Iteration 78654, loss = 1.22407696\n",
      "Iteration 78655, loss = 1.14370225\n",
      "Iteration 78656, loss = 1.07826096\n",
      "Iteration 78657, loss = 1.23818701\n",
      "Iteration 78658, loss = 1.34125960\n",
      "Iteration 78659, loss = 1.22291493\n",
      "Iteration 78660, loss = 1.24729889\n",
      "Iteration 78661, loss = 1.14746473\n",
      "Iteration 78662, loss = 1.05723986\n",
      "Iteration 78663, loss = 1.10707088\n",
      "Iteration 78664, loss = 1.20770977\n",
      "Iteration 78665, loss = 1.43783356\n",
      "Iteration 78666, loss = 1.29004927\n",
      "Iteration 78667, loss = 1.08060422\n",
      "Iteration 78668, loss = 0.99394002\n",
      "Iteration 78669, loss = 0.98320604\n",
      "Iteration 78670, loss = 0.98084005\n",
      "Iteration 78671, loss = 1.04841780\n",
      "Iteration 78672, loss = 0.99151023\n",
      "Iteration 78673, loss = 0.92966231\n",
      "Iteration 78674, loss = 0.95025139\n",
      "Iteration 78675, loss = 1.09780878\n",
      "Iteration 78676, loss = 1.05018755\n",
      "Iteration 78677, loss = 1.06703355\n",
      "Iteration 78678, loss = 1.21121167\n",
      "Iteration 78679, loss = 1.23677335\n",
      "Iteration 78680, loss = 1.00571196\n",
      "Iteration 78681, loss = 1.09582359\n",
      "Iteration 78682, loss = 1.03675854\n",
      "Iteration 78683, loss = 1.21975103\n",
      "Iteration 78684, loss = 1.12644638\n",
      "Iteration 78685, loss = 0.92142393\n",
      "Iteration 78686, loss = 0.91491623\n",
      "Iteration 78687, loss = 1.00211587\n",
      "Iteration 78688, loss = 0.97850070\n",
      "Iteration 78689, loss = 1.06470819\n",
      "Iteration 78690, loss = 1.00933025\n",
      "Iteration 78691, loss = 1.13358032\n",
      "Iteration 78692, loss = 1.20014321\n",
      "Iteration 78693, loss = 1.19359659\n",
      "Iteration 78694, loss = 1.36633660\n",
      "Iteration 78695, loss = 1.27815717\n",
      "Iteration 78696, loss = 1.00198802\n",
      "Iteration 78697, loss = 0.97000024\n",
      "Iteration 78698, loss = 1.04556823\n",
      "Iteration 78699, loss = 0.99250869\n",
      "Iteration 78700, loss = 1.02662076\n",
      "Iteration 78701, loss = 1.06300642\n",
      "Iteration 78702, loss = 1.23341292\n",
      "Iteration 78703, loss = 1.17581715\n",
      "Iteration 78704, loss = 1.05926279\n",
      "Iteration 78705, loss = 1.10782510\n",
      "Iteration 78706, loss = 1.18165399\n",
      "Iteration 78707, loss = 1.11708170\n",
      "Iteration 78708, loss = 1.04080948\n",
      "Iteration 78709, loss = 0.94168540\n",
      "Iteration 78710, loss = 1.10795493\n",
      "Iteration 78711, loss = 1.01887842\n",
      "Iteration 78712, loss = 1.15302281\n",
      "Iteration 78713, loss = 1.20205238\n",
      "Iteration 78714, loss = 1.49461485\n",
      "Iteration 78715, loss = 1.11357522\n",
      "Iteration 78716, loss = 1.05198853\n",
      "Iteration 78717, loss = 0.92125768\n",
      "Iteration 78718, loss = 1.02510240\n",
      "Iteration 78719, loss = 1.28515980\n",
      "Iteration 78720, loss = 1.24251248\n",
      "Iteration 78721, loss = 1.22899476\n",
      "Iteration 78722, loss = 1.28438704\n",
      "Iteration 78723, loss = 1.06363204\n",
      "Iteration 78724, loss = 0.95084874\n",
      "Iteration 78725, loss = 1.11840043\n",
      "Iteration 78726, loss = 1.06334235\n",
      "Iteration 78727, loss = 1.26286859\n",
      "Iteration 78728, loss = 1.26259524\n",
      "Iteration 78729, loss = 1.21836510\n",
      "Iteration 78730, loss = 1.50304577\n",
      "Iteration 78731, loss = 1.28403772\n",
      "Iteration 78732, loss = 1.32076875\n",
      "Iteration 78733, loss = 1.26501536\n",
      "Iteration 78734, loss = 1.38040813\n",
      "Iteration 78735, loss = 1.36011689\n",
      "Iteration 78736, loss = 1.45733097\n",
      "Iteration 78737, loss = 1.24205837\n",
      "Iteration 78738, loss = 1.11113651\n",
      "Iteration 78739, loss = 1.11591200\n",
      "Iteration 78740, loss = 1.18103686\n",
      "Iteration 78741, loss = 1.53159154\n",
      "Iteration 78742, loss = 1.52017298\n",
      "Iteration 78743, loss = 1.48239120\n",
      "Iteration 78744, loss = 1.06382704\n",
      "Iteration 78745, loss = 0.95968502\n",
      "Iteration 78746, loss = 1.08070775\n",
      "Iteration 78747, loss = 0.99831518\n",
      "Iteration 78748, loss = 1.35379207\n",
      "Iteration 78749, loss = 1.37298246\n",
      "Iteration 78750, loss = 1.24162531\n",
      "Iteration 78751, loss = 1.24696175\n",
      "Iteration 78752, loss = 1.18292140\n",
      "Iteration 78753, loss = 1.02824183\n",
      "Iteration 78754, loss = 0.94075445\n",
      "Iteration 78755, loss = 0.97609461\n",
      "Iteration 78756, loss = 0.98060582\n",
      "Iteration 78757, loss = 1.36731190\n",
      "Iteration 78758, loss = 1.26449527\n",
      "Iteration 78759, loss = 1.14481198\n",
      "Iteration 78760, loss = 1.18482277\n",
      "Iteration 78761, loss = 1.19635723\n",
      "Iteration 78762, loss = 1.27132189\n",
      "Iteration 78763, loss = 1.29609429\n",
      "Iteration 78764, loss = 1.27377343\n",
      "Iteration 78765, loss = 1.30021120\n",
      "Iteration 78766, loss = 0.96959523\n",
      "Iteration 78767, loss = 1.03844768\n",
      "Iteration 78768, loss = 1.24762476\n",
      "Iteration 78769, loss = 1.26116934\n",
      "Iteration 78770, loss = 1.16608743\n",
      "Iteration 78771, loss = 1.17387291\n",
      "Iteration 78772, loss = 1.17768420\n",
      "Iteration 78773, loss = 1.18173430\n",
      "Iteration 78774, loss = 1.19373271\n",
      "Iteration 78775, loss = 1.24445986\n",
      "Iteration 78776, loss = 1.20152869\n",
      "Iteration 78777, loss = 1.05336786\n",
      "Iteration 78778, loss = 1.14486140\n",
      "Iteration 78779, loss = 1.15325137\n",
      "Iteration 78780, loss = 1.24566258\n",
      "Iteration 78781, loss = 1.11871712\n",
      "Iteration 78782, loss = 0.96537442\n",
      "Iteration 78783, loss = 1.08971742\n",
      "Iteration 78784, loss = 1.33792768\n",
      "Iteration 78785, loss = 1.63303175\n",
      "Iteration 78786, loss = 1.68665993\n",
      "Iteration 78787, loss = 1.52690526\n",
      "Iteration 78788, loss = 1.32241728\n",
      "Iteration 78789, loss = 0.95218755\n",
      "Iteration 78790, loss = 0.98204909\n",
      "Iteration 78791, loss = 1.00771996\n",
      "Iteration 78792, loss = 0.97290378\n",
      "Iteration 78793, loss = 0.92223122\n",
      "Iteration 78794, loss = 0.92449639\n",
      "Iteration 78795, loss = 0.92896841\n",
      "Iteration 78796, loss = 0.97391741\n",
      "Iteration 78797, loss = 0.96877561\n",
      "Iteration 78798, loss = 1.04429088\n",
      "Iteration 78799, loss = 1.07655040\n",
      "Iteration 78800, loss = 1.08186587\n",
      "Iteration 78801, loss = 1.12546248\n",
      "Iteration 78802, loss = 1.40988732\n",
      "Iteration 78803, loss = 1.39903410\n",
      "Iteration 78804, loss = 1.26471442\n",
      "Iteration 78805, loss = 1.17107945\n",
      "Iteration 78806, loss = 1.28060943\n",
      "Iteration 78807, loss = 1.19580050\n",
      "Iteration 78808, loss = 1.19836094\n",
      "Iteration 78809, loss = 1.62315491\n",
      "Iteration 78810, loss = 1.33252944\n",
      "Iteration 78811, loss = 1.50256820\n",
      "Iteration 78812, loss = 1.57854443\n",
      "Iteration 78813, loss = 1.93182799\n",
      "Iteration 78814, loss = 2.16160402\n",
      "Iteration 78815, loss = 2.98239997\n",
      "Iteration 78816, loss = 2.14456315\n",
      "Iteration 78817, loss = 1.69584636\n",
      "Iteration 78818, loss = 1.26468423\n",
      "Iteration 78819, loss = 1.01006772\n",
      "Iteration 78820, loss = 1.07149917\n",
      "Iteration 78821, loss = 1.03032934\n",
      "Iteration 78822, loss = 1.00809740\n",
      "Iteration 78823, loss = 1.05858507\n",
      "Iteration 78824, loss = 1.10097900\n",
      "Iteration 78825, loss = 1.36929397\n",
      "Iteration 78826, loss = 1.28682097\n",
      "Iteration 78827, loss = 1.14450203\n",
      "Iteration 78828, loss = 1.27618008\n",
      "Iteration 78829, loss = 1.43875902\n",
      "Iteration 78830, loss = 1.39074340\n",
      "Iteration 78831, loss = 1.51062046\n",
      "Iteration 78832, loss = 1.32122650\n",
      "Iteration 78833, loss = 1.22082933\n",
      "Iteration 78834, loss = 1.08552442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 78835, loss = 1.20937533\n",
      "Iteration 78836, loss = 1.11918055\n",
      "Iteration 78837, loss = 1.03448934\n",
      "Iteration 78838, loss = 1.05014257\n",
      "Iteration 78839, loss = 1.17675455\n",
      "Iteration 78840, loss = 1.02625322\n",
      "Iteration 78841, loss = 1.06526123\n",
      "Iteration 78842, loss = 1.16041892\n",
      "Iteration 78843, loss = 1.33418350\n",
      "Iteration 78844, loss = 1.01825918\n",
      "Iteration 78845, loss = 1.05321422\n",
      "Iteration 78846, loss = 1.07459271\n",
      "Iteration 78847, loss = 1.54660708\n",
      "Iteration 78848, loss = 1.54991699\n",
      "Iteration 78849, loss = 1.54773524\n",
      "Iteration 78850, loss = 1.69820557\n",
      "Iteration 78851, loss = 1.61851125\n",
      "Iteration 78852, loss = 1.32215585\n",
      "Iteration 78853, loss = 1.31692454\n",
      "Iteration 78854, loss = 1.45458969\n",
      "Iteration 78855, loss = 1.28532187\n",
      "Iteration 78856, loss = 1.16333252\n",
      "Iteration 78857, loss = 1.07223332\n",
      "Iteration 78858, loss = 1.31700451\n",
      "Iteration 78859, loss = 1.33817495\n",
      "Iteration 78860, loss = 1.22417793\n",
      "Iteration 78861, loss = 1.17753197\n",
      "Iteration 78862, loss = 0.99689047\n",
      "Iteration 78863, loss = 1.05882437\n",
      "Iteration 78864, loss = 1.03937207\n",
      "Iteration 78865, loss = 1.05157791\n",
      "Iteration 78866, loss = 1.12431778\n",
      "Iteration 78867, loss = 1.25647157\n",
      "Iteration 78868, loss = 1.06188305\n",
      "Iteration 78869, loss = 1.15371861\n",
      "Iteration 78870, loss = 1.47673303\n",
      "Iteration 78871, loss = 1.24912735\n",
      "Iteration 78872, loss = 1.11440513\n",
      "Iteration 78873, loss = 1.27030929\n",
      "Iteration 78874, loss = 1.20230814\n",
      "Iteration 78875, loss = 1.03988567\n",
      "Iteration 78876, loss = 0.94478129\n",
      "Iteration 78877, loss = 1.06215461\n",
      "Iteration 78878, loss = 1.02206684\n",
      "Iteration 78879, loss = 1.06700371\n",
      "Iteration 78880, loss = 1.40830724\n",
      "Iteration 78881, loss = 1.58734529\n",
      "Iteration 78882, loss = 1.59100655\n",
      "Iteration 78883, loss = 1.53687134\n",
      "Iteration 78884, loss = 1.38064389\n",
      "Iteration 78885, loss = 1.46845063\n",
      "Iteration 78886, loss = 1.16865488\n",
      "Iteration 78887, loss = 1.09489859\n",
      "Iteration 78888, loss = 1.13273174\n",
      "Iteration 78889, loss = 1.01276579\n",
      "Iteration 78890, loss = 0.97924482\n",
      "Iteration 78891, loss = 1.10106914\n",
      "Iteration 78892, loss = 1.03670524\n",
      "Iteration 78893, loss = 0.98275514\n",
      "Iteration 78894, loss = 0.94227934\n",
      "Iteration 78895, loss = 0.96397980\n",
      "Iteration 78896, loss = 0.98268860\n",
      "Iteration 78897, loss = 0.98176068\n",
      "Iteration 78898, loss = 1.06421459\n",
      "Iteration 78899, loss = 1.02474004\n",
      "Iteration 78900, loss = 0.96884430\n",
      "Iteration 78901, loss = 0.95274625\n",
      "Iteration 78902, loss = 1.05161824\n",
      "Iteration 78903, loss = 1.03295131\n",
      "Iteration 78904, loss = 1.03031745\n",
      "Iteration 78905, loss = 0.97544961\n",
      "Iteration 78906, loss = 1.17661501\n",
      "Iteration 78907, loss = 1.33625058\n",
      "Iteration 78908, loss = 1.08360456\n",
      "Iteration 78909, loss = 1.46032710\n",
      "Iteration 78910, loss = 1.51257853\n",
      "Iteration 78911, loss = 1.14225942\n",
      "Iteration 78912, loss = 1.06158477\n",
      "Iteration 78913, loss = 0.99750637\n",
      "Iteration 78914, loss = 0.96051632\n",
      "Iteration 78915, loss = 0.96833087\n",
      "Iteration 78916, loss = 1.08552020\n",
      "Iteration 78917, loss = 1.13422249\n",
      "Iteration 78918, loss = 1.03816307\n",
      "Iteration 78919, loss = 1.04716218\n",
      "Iteration 78920, loss = 1.07167197\n",
      "Iteration 78921, loss = 0.93381646\n",
      "Iteration 78922, loss = 0.95547143\n",
      "Iteration 78923, loss = 1.17747074\n",
      "Iteration 78924, loss = 1.29612629\n",
      "Iteration 78925, loss = 1.22197386\n",
      "Iteration 78926, loss = 1.71242164\n",
      "Iteration 78927, loss = 1.43361474\n",
      "Iteration 78928, loss = 1.48015041\n",
      "Iteration 78929, loss = 1.04553602\n",
      "Iteration 78930, loss = 1.09706840\n",
      "Iteration 78931, loss = 1.04302645\n",
      "Iteration 78932, loss = 1.01770490\n",
      "Iteration 78933, loss = 1.00259680\n",
      "Iteration 78934, loss = 0.94971504\n",
      "Iteration 78935, loss = 0.95729353\n",
      "Iteration 78936, loss = 0.96821274\n",
      "Iteration 78937, loss = 0.93322856\n",
      "Iteration 78938, loss = 1.01402555\n",
      "Iteration 78939, loss = 1.60918669\n",
      "Iteration 78940, loss = 1.29645440\n",
      "Iteration 78941, loss = 1.16093274\n",
      "Iteration 78942, loss = 1.06136621\n",
      "Iteration 78943, loss = 0.98278436\n",
      "Iteration 78944, loss = 1.06549580\n",
      "Iteration 78945, loss = 1.01995790\n",
      "Iteration 78946, loss = 1.01792483\n",
      "Iteration 78947, loss = 1.22688687\n",
      "Iteration 78948, loss = 1.23552319\n",
      "Iteration 78949, loss = 1.31431363\n",
      "Iteration 78950, loss = 1.33971242\n",
      "Iteration 78951, loss = 1.19799874\n",
      "Iteration 78952, loss = 1.34734713\n",
      "Iteration 78953, loss = 1.64009290\n",
      "Iteration 78954, loss = 1.47275104\n",
      "Iteration 78955, loss = 1.68254915\n",
      "Iteration 78956, loss = 1.95920264\n",
      "Iteration 78957, loss = 1.52441132\n",
      "Iteration 78958, loss = 1.31843910\n",
      "Iteration 78959, loss = 1.44708962\n",
      "Iteration 78960, loss = 1.97236537\n",
      "Iteration 78961, loss = 2.07546761\n",
      "Iteration 78962, loss = 1.85539304\n",
      "Iteration 78963, loss = 1.64756606\n",
      "Iteration 78964, loss = 1.53158975\n",
      "Iteration 78965, loss = 1.52899053\n",
      "Iteration 78966, loss = 1.49113037\n",
      "Iteration 78967, loss = 1.74323080\n",
      "Iteration 78968, loss = 1.19342055\n",
      "Iteration 78969, loss = 1.25100075\n",
      "Iteration 78970, loss = 1.24532480\n",
      "Iteration 78971, loss = 0.97763490\n",
      "Iteration 78972, loss = 1.03624109\n",
      "Iteration 78973, loss = 1.07060512\n",
      "Iteration 78974, loss = 1.02595570\n",
      "Iteration 78975, loss = 0.97830730\n",
      "Iteration 78976, loss = 0.99943523\n",
      "Iteration 78977, loss = 1.15074163\n",
      "Iteration 78978, loss = 0.99751689\n",
      "Iteration 78979, loss = 1.07440087\n",
      "Iteration 78980, loss = 1.08921890\n",
      "Iteration 78981, loss = 1.01029802\n",
      "Iteration 78982, loss = 0.99436053\n",
      "Iteration 78983, loss = 0.97168343\n",
      "Iteration 78984, loss = 1.05313437\n",
      "Iteration 78985, loss = 1.02440986\n",
      "Iteration 78986, loss = 1.46077961\n",
      "Iteration 78987, loss = 1.85761076\n",
      "Iteration 78988, loss = 1.59025011\n",
      "Iteration 78989, loss = 1.46207084\n",
      "Iteration 78990, loss = 1.34733546\n",
      "Iteration 78991, loss = 1.31063451\n",
      "Iteration 78992, loss = 1.46245167\n",
      "Iteration 78993, loss = 1.16030243\n",
      "Iteration 78994, loss = 1.09109130\n",
      "Iteration 78995, loss = 1.09884453\n",
      "Iteration 78996, loss = 1.12144687\n",
      "Iteration 78997, loss = 1.10157474\n",
      "Iteration 78998, loss = 1.08967044\n",
      "Iteration 78999, loss = 1.02333640\n",
      "Iteration 79000, loss = 0.97928620\n",
      "Iteration 79001, loss = 0.95013771\n",
      "Iteration 79002, loss = 0.91119573\n",
      "Iteration 79003, loss = 0.97925863\n",
      "Iteration 79004, loss = 0.92732213\n",
      "Iteration 79005, loss = 0.94666350\n",
      "Iteration 79006, loss = 0.95159478\n",
      "Iteration 79007, loss = 0.91066666\n",
      "Iteration 79008, loss = 1.33929508\n",
      "Iteration 79009, loss = 1.24023087\n",
      "Iteration 79010, loss = 1.06998127\n",
      "Iteration 79011, loss = 1.32724384\n",
      "Iteration 79012, loss = 1.37041992\n",
      "Iteration 79013, loss = 1.52354470\n",
      "Iteration 79014, loss = 1.34321684\n",
      "Iteration 79015, loss = 1.43176488\n",
      "Iteration 79016, loss = 1.28880770\n",
      "Iteration 79017, loss = 1.61438699\n",
      "Iteration 79018, loss = 1.59077661\n",
      "Iteration 79019, loss = 1.09721408\n",
      "Iteration 79020, loss = 1.24827484\n",
      "Iteration 79021, loss = 1.22055117\n",
      "Iteration 79022, loss = 1.11015295\n",
      "Iteration 79023, loss = 1.09618434\n",
      "Iteration 79024, loss = 1.02538864\n",
      "Iteration 79025, loss = 1.10293569\n",
      "Iteration 79026, loss = 1.01705748\n",
      "Iteration 79027, loss = 0.95199901\n",
      "Iteration 79028, loss = 0.93195351\n",
      "Iteration 79029, loss = 0.95813325\n",
      "Iteration 79030, loss = 0.96253083\n",
      "Iteration 79031, loss = 0.97175582\n",
      "Iteration 79032, loss = 0.94922265\n",
      "Iteration 79033, loss = 1.15740708\n",
      "Iteration 79034, loss = 1.44686629\n",
      "Iteration 79035, loss = 1.51551493\n",
      "Iteration 79036, loss = 1.38740819\n",
      "Iteration 79037, loss = 1.12129761\n",
      "Iteration 79038, loss = 1.63287824\n",
      "Iteration 79039, loss = 1.81766435\n",
      "Iteration 79040, loss = 2.27166887\n",
      "Iteration 79041, loss = 2.04530587\n",
      "Iteration 79042, loss = 2.62313793\n",
      "Iteration 79043, loss = 2.29314870\n",
      "Iteration 79044, loss = 2.19250763\n",
      "Iteration 79045, loss = 2.07757021\n",
      "Iteration 79046, loss = 1.72108974\n",
      "Iteration 79047, loss = 2.09340785\n",
      "Iteration 79048, loss = 2.28987511\n",
      "Iteration 79049, loss = 1.86843873\n",
      "Iteration 79050, loss = 1.75470983\n",
      "Iteration 79051, loss = 1.44334235\n",
      "Iteration 79052, loss = 1.27455245\n",
      "Iteration 79053, loss = 1.53562655\n",
      "Iteration 79054, loss = 1.54623678\n",
      "Iteration 79055, loss = 1.48066777\n",
      "Iteration 79056, loss = 1.50892938\n",
      "Iteration 79057, loss = 2.23051806\n",
      "Iteration 79058, loss = 3.05443051\n",
      "Iteration 79059, loss = 2.90521155\n",
      "Iteration 79060, loss = 3.36159118\n",
      "Iteration 79061, loss = 2.97010972\n",
      "Iteration 79062, loss = 3.38283570\n",
      "Iteration 79063, loss = 4.26834152\n",
      "Iteration 79064, loss = 5.12101189\n",
      "Iteration 79065, loss = 4.18040746\n",
      "Iteration 79066, loss = 3.50759558\n",
      "Iteration 79067, loss = 2.64573783\n",
      "Iteration 79068, loss = 3.23959775\n",
      "Iteration 79069, loss = 3.36177357\n",
      "Iteration 79070, loss = 3.15246897\n",
      "Iteration 79071, loss = 2.60131337\n",
      "Iteration 79072, loss = 2.28418573\n",
      "Iteration 79073, loss = 1.51368242\n",
      "Iteration 79074, loss = 1.30107687\n",
      "Iteration 79075, loss = 1.29903369\n",
      "Iteration 79076, loss = 1.31701192\n",
      "Iteration 79077, loss = 1.36235443\n",
      "Iteration 79078, loss = 1.53009319\n",
      "Iteration 79079, loss = 1.59357805\n",
      "Iteration 79080, loss = 1.45971695\n",
      "Iteration 79081, loss = 1.47429509\n",
      "Iteration 79082, loss = 1.52552899\n",
      "Iteration 79083, loss = 1.51673025\n",
      "Iteration 79084, loss = 1.32950232\n",
      "Iteration 79085, loss = 1.17181898\n",
      "Iteration 79086, loss = 1.18153876\n",
      "Iteration 79087, loss = 1.30220130\n",
      "Iteration 79088, loss = 1.28916559\n",
      "Iteration 79089, loss = 1.49987061\n",
      "Iteration 79090, loss = 1.24341644\n",
      "Iteration 79091, loss = 1.13755641\n",
      "Iteration 79092, loss = 1.33688624\n",
      "Iteration 79093, loss = 1.67255566\n",
      "Iteration 79094, loss = 1.79717217\n",
      "Iteration 79095, loss = 1.61622809\n",
      "Iteration 79096, loss = 1.22975068\n",
      "Iteration 79097, loss = 0.99225414\n",
      "Iteration 79098, loss = 1.19615852\n",
      "Iteration 79099, loss = 1.17324605\n",
      "Iteration 79100, loss = 1.23197431\n",
      "Iteration 79101, loss = 1.68629497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 79102, loss = 1.61352156\n",
      "Iteration 79103, loss = 1.32027406\n",
      "Iteration 79104, loss = 1.14431693\n",
      "Iteration 79105, loss = 1.11624007\n",
      "Iteration 79106, loss = 1.04820006\n",
      "Iteration 79107, loss = 1.23376786\n",
      "Iteration 79108, loss = 1.26441091\n",
      "Iteration 79109, loss = 1.32583066\n",
      "Iteration 79110, loss = 1.25277161\n",
      "Iteration 79111, loss = 1.18395579\n",
      "Iteration 79112, loss = 1.27416196\n",
      "Iteration 79113, loss = 1.15023742\n",
      "Iteration 79114, loss = 1.16735761\n",
      "Iteration 79115, loss = 1.05476024\n",
      "Iteration 79116, loss = 1.15678505\n",
      "Iteration 79117, loss = 1.13341547\n",
      "Iteration 79118, loss = 1.32226886\n",
      "Iteration 79119, loss = 1.21055235\n",
      "Iteration 79120, loss = 1.03172927\n",
      "Iteration 79121, loss = 1.06809846\n",
      "Iteration 79122, loss = 1.00907742\n",
      "Iteration 79123, loss = 1.08033470\n",
      "Iteration 79124, loss = 1.08369737\n",
      "Iteration 79125, loss = 1.10452113\n",
      "Iteration 79126, loss = 1.03641857\n",
      "Iteration 79127, loss = 0.95910137\n",
      "Iteration 79128, loss = 0.98725531\n",
      "Iteration 79129, loss = 1.01221123\n",
      "Iteration 79130, loss = 1.01135416\n",
      "Iteration 79131, loss = 0.97637973\n",
      "Iteration 79132, loss = 0.98452541\n",
      "Iteration 79133, loss = 0.99809764\n",
      "Iteration 79134, loss = 1.05258293\n",
      "Iteration 79135, loss = 0.99678377\n",
      "Iteration 79136, loss = 0.98803484\n",
      "Iteration 79137, loss = 1.02102161\n",
      "Iteration 79138, loss = 1.03417250\n",
      "Iteration 79139, loss = 0.91472660\n",
      "Iteration 79140, loss = 0.96577052\n",
      "Iteration 79141, loss = 1.02175502\n",
      "Iteration 79142, loss = 1.03168564\n",
      "Iteration 79143, loss = 0.97975239\n",
      "Iteration 79144, loss = 1.01573013\n",
      "Iteration 79145, loss = 1.03376243\n",
      "Iteration 79146, loss = 1.09684998\n",
      "Iteration 79147, loss = 1.14309659\n",
      "Iteration 79148, loss = 1.02161702\n",
      "Iteration 79149, loss = 1.09035615\n",
      "Iteration 79150, loss = 1.00958581\n",
      "Iteration 79151, loss = 0.94675727\n",
      "Iteration 79152, loss = 1.02391579\n",
      "Iteration 79153, loss = 1.00115123\n",
      "Iteration 79154, loss = 1.03104287\n",
      "Iteration 79155, loss = 1.19816518\n",
      "Iteration 79156, loss = 0.99540983\n",
      "Iteration 79157, loss = 0.98527201\n",
      "Iteration 79158, loss = 0.97785590\n",
      "Iteration 79159, loss = 1.06037724\n",
      "Iteration 79160, loss = 1.14803818\n",
      "Iteration 79161, loss = 1.09123780\n",
      "Iteration 79162, loss = 1.10521467\n",
      "Iteration 79163, loss = 1.08928973\n",
      "Iteration 79164, loss = 0.99805093\n",
      "Iteration 79165, loss = 0.96477194\n",
      "Iteration 79166, loss = 0.92386196\n",
      "Iteration 79167, loss = 0.99947421\n",
      "Iteration 79168, loss = 1.17023638\n",
      "Iteration 79169, loss = 1.29838866\n",
      "Iteration 79170, loss = 1.13951620\n",
      "Iteration 79171, loss = 1.13287031\n",
      "Iteration 79172, loss = 1.10110385\n",
      "Iteration 79173, loss = 1.09485641\n",
      "Iteration 79174, loss = 1.29261228\n",
      "Iteration 79175, loss = 1.61903062\n",
      "Iteration 79176, loss = 1.35194053\n",
      "Iteration 79177, loss = 1.27553675\n",
      "Iteration 79178, loss = 1.16994634\n",
      "Iteration 79179, loss = 1.26801850\n",
      "Iteration 79180, loss = 1.45811844\n",
      "Iteration 79181, loss = 1.52307890\n",
      "Iteration 79182, loss = 1.68313033\n",
      "Iteration 79183, loss = 1.47113586\n",
      "Iteration 79184, loss = 1.29381785\n",
      "Iteration 79185, loss = 1.34561552\n",
      "Iteration 79186, loss = 1.46245009\n",
      "Iteration 79187, loss = 2.22012121\n",
      "Iteration 79188, loss = 1.77826433\n",
      "Iteration 79189, loss = 1.40684899\n",
      "Iteration 79190, loss = 1.41958448\n",
      "Iteration 79191, loss = 1.49432106\n",
      "Iteration 79192, loss = 1.32086679\n",
      "Iteration 79193, loss = 1.15215622\n",
      "Iteration 79194, loss = 1.23917560\n",
      "Iteration 79195, loss = 1.30929738\n",
      "Iteration 79196, loss = 1.36596055\n",
      "Iteration 79197, loss = 1.11276616\n",
      "Iteration 79198, loss = 1.35689612\n",
      "Iteration 79199, loss = 1.25014056\n",
      "Iteration 79200, loss = 1.24669641\n",
      "Iteration 79201, loss = 1.08295078\n",
      "Iteration 79202, loss = 1.28252060\n",
      "Iteration 79203, loss = 1.09069505\n",
      "Iteration 79204, loss = 1.03381620\n",
      "Iteration 79205, loss = 1.12085319\n",
      "Iteration 79206, loss = 0.98101600\n",
      "Iteration 79207, loss = 0.94401389\n",
      "Iteration 79208, loss = 1.09163532\n",
      "Iteration 79209, loss = 1.36081013\n",
      "Iteration 79210, loss = 1.10507307\n",
      "Iteration 79211, loss = 0.97065282\n",
      "Iteration 79212, loss = 1.17592521\n",
      "Iteration 79213, loss = 1.09902339\n",
      "Iteration 79214, loss = 1.18313105\n",
      "Iteration 79215, loss = 1.42079858\n",
      "Iteration 79216, loss = 1.41385566\n",
      "Iteration 79217, loss = 1.26198666\n",
      "Iteration 79218, loss = 1.19182823\n",
      "Iteration 79219, loss = 1.13138519\n",
      "Iteration 79220, loss = 1.09100808\n",
      "Iteration 79221, loss = 1.09212729\n",
      "Iteration 79222, loss = 1.17049321\n",
      "Iteration 79223, loss = 1.13408888\n",
      "Iteration 79224, loss = 1.51236841\n",
      "Iteration 79225, loss = 1.66742072\n",
      "Iteration 79226, loss = 1.55504214\n",
      "Iteration 79227, loss = 2.05082853\n",
      "Iteration 79228, loss = 2.85169893\n",
      "Iteration 79229, loss = 3.64447349\n",
      "Iteration 79230, loss = 4.39369717\n",
      "Iteration 79231, loss = 2.40935800\n",
      "Iteration 79232, loss = 1.69260605\n",
      "Iteration 79233, loss = 1.92633805\n",
      "Iteration 79234, loss = 1.65780086\n",
      "Iteration 79235, loss = 1.57080511\n",
      "Iteration 79236, loss = 1.98096209\n",
      "Iteration 79237, loss = 1.47802315\n",
      "Iteration 79238, loss = 1.54035228\n",
      "Iteration 79239, loss = 1.42457721\n",
      "Iteration 79240, loss = 1.57657794\n",
      "Iteration 79241, loss = 1.38491693\n",
      "Iteration 79242, loss = 1.17924795\n",
      "Iteration 79243, loss = 1.10218742\n",
      "Iteration 79244, loss = 1.24893381\n",
      "Iteration 79245, loss = 1.48676801\n",
      "Iteration 79246, loss = 1.54585350\n",
      "Iteration 79247, loss = 1.35107365\n",
      "Iteration 79248, loss = 1.28161266\n",
      "Iteration 79249, loss = 1.48882725\n",
      "Iteration 79250, loss = 1.27826385\n",
      "Iteration 79251, loss = 1.20867633\n",
      "Iteration 79252, loss = 1.11032425\n",
      "Iteration 79253, loss = 1.00729331\n",
      "Iteration 79254, loss = 0.97921531\n",
      "Iteration 79255, loss = 1.19964632\n",
      "Iteration 79256, loss = 1.31668896\n",
      "Iteration 79257, loss = 1.08782691\n",
      "Iteration 79258, loss = 1.06584931\n",
      "Iteration 79259, loss = 1.33832215\n",
      "Iteration 79260, loss = 1.21694968\n",
      "Iteration 79261, loss = 1.17576420\n",
      "Iteration 79262, loss = 1.01474343\n",
      "Iteration 79263, loss = 1.00685107\n",
      "Iteration 79264, loss = 0.97812541\n",
      "Iteration 79265, loss = 1.00464008\n",
      "Iteration 79266, loss = 0.94740015\n",
      "Iteration 79267, loss = 1.13443115\n",
      "Iteration 79268, loss = 1.12661039\n",
      "Iteration 79269, loss = 0.94244599\n",
      "Iteration 79270, loss = 1.08753138\n",
      "Iteration 79271, loss = 1.12493279\n",
      "Iteration 79272, loss = 1.01361604\n",
      "Iteration 79273, loss = 1.17431566\n",
      "Iteration 79274, loss = 1.28017663\n",
      "Iteration 79275, loss = 1.19299668\n",
      "Iteration 79276, loss = 1.43343398\n",
      "Iteration 79277, loss = 1.19139038\n",
      "Iteration 79278, loss = 1.78718065\n",
      "Iteration 79279, loss = 1.76601355\n",
      "Iteration 79280, loss = 1.36952090\n",
      "Iteration 79281, loss = 1.26956560\n",
      "Iteration 79282, loss = 1.55078820\n",
      "Iteration 79283, loss = 1.52186325\n",
      "Iteration 79284, loss = 1.56521528\n",
      "Iteration 79285, loss = 1.49933773\n",
      "Iteration 79286, loss = 1.40015303\n",
      "Iteration 79287, loss = 1.28883518\n",
      "Iteration 79288, loss = 1.64483451\n",
      "Iteration 79289, loss = 1.41128074\n",
      "Iteration 79290, loss = 1.14950645\n",
      "Iteration 79291, loss = 1.06021724\n",
      "Iteration 79292, loss = 1.02207552\n",
      "Iteration 79293, loss = 1.14665994\n",
      "Iteration 79294, loss = 1.08424872\n",
      "Iteration 79295, loss = 1.22638124\n",
      "Iteration 79296, loss = 1.12527077\n",
      "Iteration 79297, loss = 1.38111727\n",
      "Iteration 79298, loss = 1.42634651\n",
      "Iteration 79299, loss = 1.42269973\n",
      "Iteration 79300, loss = 1.32445470\n",
      "Iteration 79301, loss = 1.19336322\n",
      "Iteration 79302, loss = 1.36875349\n",
      "Iteration 79303, loss = 1.44515608\n",
      "Iteration 79304, loss = 1.45825682\n",
      "Iteration 79305, loss = 1.61559032\n",
      "Iteration 79306, loss = 1.46354512\n",
      "Iteration 79307, loss = 1.27145064\n",
      "Iteration 79308, loss = 1.11557450\n",
      "Iteration 79309, loss = 1.23142529\n",
      "Iteration 79310, loss = 1.40290643\n",
      "Iteration 79311, loss = 1.36070265\n",
      "Iteration 79312, loss = 1.08262624\n",
      "Iteration 79313, loss = 1.28175323\n",
      "Iteration 79314, loss = 1.19070875\n",
      "Iteration 79315, loss = 1.19276438\n",
      "Iteration 79316, loss = 1.12677542\n",
      "Iteration 79317, loss = 1.19066967\n",
      "Iteration 79318, loss = 1.03581233\n",
      "Iteration 79319, loss = 1.08936142\n",
      "Iteration 79320, loss = 0.90381611\n",
      "Iteration 79321, loss = 1.01417644\n",
      "Iteration 79322, loss = 0.95343220\n",
      "Iteration 79323, loss = 1.01882587\n",
      "Iteration 79324, loss = 0.97147702\n",
      "Iteration 79325, loss = 0.94687930\n",
      "Iteration 79326, loss = 0.90267250\n",
      "Iteration 79327, loss = 0.92449686\n",
      "Iteration 79328, loss = 0.97926627\n",
      "Iteration 79329, loss = 1.14332830\n",
      "Iteration 79330, loss = 1.07066552\n",
      "Iteration 79331, loss = 0.91742097\n",
      "Iteration 79332, loss = 0.92382820\n",
      "Iteration 79333, loss = 0.98872364\n",
      "Iteration 79334, loss = 0.97903029\n",
      "Iteration 79335, loss = 1.03669482\n",
      "Iteration 79336, loss = 1.10357826\n",
      "Iteration 79337, loss = 1.20723640\n",
      "Iteration 79338, loss = 1.41280560\n",
      "Iteration 79339, loss = 1.18156873\n",
      "Iteration 79340, loss = 1.25169921\n",
      "Iteration 79341, loss = 1.11398834\n",
      "Iteration 79342, loss = 1.08152816\n",
      "Iteration 79343, loss = 1.00729400\n",
      "Iteration 79344, loss = 1.19740541\n",
      "Iteration 79345, loss = 1.20244629\n",
      "Iteration 79346, loss = 1.42675814\n",
      "Iteration 79347, loss = 1.29009050\n",
      "Iteration 79348, loss = 1.18473738\n",
      "Iteration 79349, loss = 1.13896560\n",
      "Iteration 79350, loss = 1.10075798\n",
      "Iteration 79351, loss = 1.16083630\n",
      "Iteration 79352, loss = 1.10876835\n",
      "Iteration 79353, loss = 1.05300920\n",
      "Iteration 79354, loss = 1.20484289\n",
      "Iteration 79355, loss = 1.58327843\n",
      "Iteration 79356, loss = 1.49276306\n",
      "Iteration 79357, loss = 1.47664444\n",
      "Iteration 79358, loss = 1.53497721\n",
      "Iteration 79359, loss = 1.59545965\n",
      "Iteration 79360, loss = 1.88852315\n",
      "Iteration 79361, loss = 1.71521669\n",
      "Iteration 79362, loss = 1.25816766\n",
      "Iteration 79363, loss = 1.15943553\n",
      "Iteration 79364, loss = 1.25161285\n",
      "Iteration 79365, loss = 1.48713613\n",
      "Iteration 79366, loss = 1.68630191\n",
      "Iteration 79367, loss = 1.36445253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 79368, loss = 1.73353398\n",
      "Iteration 79369, loss = 1.86339575\n",
      "Iteration 79370, loss = 2.54970855\n",
      "Iteration 79371, loss = 2.83975086\n",
      "Iteration 79372, loss = 1.99538151\n",
      "Iteration 79373, loss = 2.10074717\n",
      "Iteration 79374, loss = 1.84865060\n",
      "Iteration 79375, loss = 1.79747133\n",
      "Iteration 79376, loss = 1.36405044\n",
      "Iteration 79377, loss = 1.92656065\n",
      "Iteration 79378, loss = 1.56606861\n",
      "Iteration 79379, loss = 1.59349788\n",
      "Iteration 79380, loss = 1.70044234\n",
      "Iteration 79381, loss = 1.32925620\n",
      "Iteration 79382, loss = 1.29021666\n",
      "Iteration 79383, loss = 1.10209249\n",
      "Iteration 79384, loss = 1.24130825\n",
      "Iteration 79385, loss = 1.24390594\n",
      "Iteration 79386, loss = 1.24288829\n",
      "Iteration 79387, loss = 1.06128773\n",
      "Iteration 79388, loss = 1.18755349\n",
      "Iteration 79389, loss = 1.22284440\n",
      "Iteration 79390, loss = 1.13300337\n",
      "Iteration 79391, loss = 1.14928765\n",
      "Iteration 79392, loss = 1.49355090\n",
      "Iteration 79393, loss = 1.10421866\n",
      "Iteration 79394, loss = 1.00614455\n",
      "Iteration 79395, loss = 1.16393930\n",
      "Iteration 79396, loss = 1.12682749\n",
      "Iteration 79397, loss = 1.14162600\n",
      "Iteration 79398, loss = 1.43948474\n",
      "Iteration 79399, loss = 1.68300476\n",
      "Iteration 79400, loss = 1.67217946\n",
      "Iteration 79401, loss = 1.44419852\n",
      "Iteration 79402, loss = 1.75229063\n",
      "Iteration 79403, loss = 1.86310657\n",
      "Iteration 79404, loss = 1.72213038\n",
      "Iteration 79405, loss = 1.44238162\n",
      "Iteration 79406, loss = 1.09702072\n",
      "Iteration 79407, loss = 1.43087178\n",
      "Iteration 79408, loss = 1.42479012\n",
      "Iteration 79409, loss = 1.26009338\n",
      "Iteration 79410, loss = 1.14697066\n",
      "Iteration 79411, loss = 1.01472631\n",
      "Iteration 79412, loss = 0.99333782\n",
      "Iteration 79413, loss = 1.04895258\n",
      "Iteration 79414, loss = 1.15090370\n",
      "Iteration 79415, loss = 1.06270823\n",
      "Iteration 79416, loss = 1.19453653\n",
      "Iteration 79417, loss = 1.28350070\n",
      "Iteration 79418, loss = 1.17995236\n",
      "Iteration 79419, loss = 1.03646866\n",
      "Iteration 79420, loss = 1.21356188\n",
      "Iteration 79421, loss = 1.38713949\n",
      "Iteration 79422, loss = 1.74599061\n",
      "Iteration 79423, loss = 1.88372604\n",
      "Iteration 79424, loss = 1.76225270\n",
      "Iteration 79425, loss = 1.55905002\n",
      "Iteration 79426, loss = 1.37072915\n",
      "Iteration 79427, loss = 1.01978662\n",
      "Iteration 79428, loss = 1.25448448\n",
      "Iteration 79429, loss = 1.23009669\n",
      "Iteration 79430, loss = 1.47219343\n",
      "Iteration 79431, loss = 1.42185392\n",
      "Iteration 79432, loss = 1.50693106\n",
      "Iteration 79433, loss = 1.48253914\n",
      "Iteration 79434, loss = 1.25069508\n",
      "Iteration 79435, loss = 1.07586977\n",
      "Iteration 79436, loss = 1.23308378\n",
      "Iteration 79437, loss = 1.47172264\n",
      "Iteration 79438, loss = 1.56416297\n",
      "Iteration 79439, loss = 1.07445381\n",
      "Iteration 79440, loss = 1.08174465\n",
      "Iteration 79441, loss = 1.12244814\n",
      "Iteration 79442, loss = 1.33151931\n",
      "Iteration 79443, loss = 1.38966469\n",
      "Iteration 79444, loss = 1.52347705\n",
      "Iteration 79445, loss = 1.27650041\n",
      "Iteration 79446, loss = 1.22719563\n",
      "Iteration 79447, loss = 1.21734991\n",
      "Iteration 79448, loss = 1.47289333\n",
      "Iteration 79449, loss = 1.36310014\n",
      "Iteration 79450, loss = 1.16432772\n",
      "Iteration 79451, loss = 1.31502674\n",
      "Iteration 79452, loss = 1.22077333\n",
      "Iteration 79453, loss = 1.12691074\n",
      "Iteration 79454, loss = 1.22381495\n",
      "Iteration 79455, loss = 1.04250005\n",
      "Iteration 79456, loss = 1.57660633\n",
      "Iteration 79457, loss = 1.42494423\n",
      "Iteration 79458, loss = 1.42063041\n",
      "Iteration 79459, loss = 1.11854583\n",
      "Iteration 79460, loss = 1.02579627\n",
      "Iteration 79461, loss = 1.04783477\n",
      "Iteration 79462, loss = 1.10548808\n",
      "Iteration 79463, loss = 1.09995997\n",
      "Iteration 79464, loss = 1.06200745\n",
      "Iteration 79465, loss = 0.95717281\n",
      "Iteration 79466, loss = 1.04791927\n",
      "Iteration 79467, loss = 1.01412574\n",
      "Iteration 79468, loss = 1.11387211\n",
      "Iteration 79469, loss = 1.36693449\n",
      "Iteration 79470, loss = 1.49400843\n",
      "Iteration 79471, loss = 1.12415394\n",
      "Iteration 79472, loss = 1.23554289\n",
      "Iteration 79473, loss = 1.35771393\n",
      "Iteration 79474, loss = 1.21753204\n",
      "Iteration 79475, loss = 1.18301914\n",
      "Iteration 79476, loss = 1.03255145\n",
      "Iteration 79477, loss = 1.09918393\n",
      "Iteration 79478, loss = 1.38922455\n",
      "Iteration 79479, loss = 1.32225019\n",
      "Iteration 79480, loss = 1.15679370\n",
      "Iteration 79481, loss = 1.02037590\n",
      "Iteration 79482, loss = 0.93612944\n",
      "Iteration 79483, loss = 0.99087201\n",
      "Iteration 79484, loss = 1.02927808\n",
      "Iteration 79485, loss = 0.96616780\n",
      "Iteration 79486, loss = 1.02490909\n",
      "Iteration 79487, loss = 1.01774548\n",
      "Iteration 79488, loss = 1.49769520\n",
      "Iteration 79489, loss = 1.35117147\n",
      "Iteration 79490, loss = 1.08254716\n",
      "Iteration 79491, loss = 1.17149147\n",
      "Iteration 79492, loss = 1.06445638\n",
      "Iteration 79493, loss = 1.52194878\n",
      "Iteration 79494, loss = 1.39625049\n",
      "Iteration 79495, loss = 1.21356091\n",
      "Iteration 79496, loss = 1.30951681\n",
      "Iteration 79497, loss = 1.22386837\n",
      "Iteration 79498, loss = 1.71969112\n",
      "Iteration 79499, loss = 1.56129258\n",
      "Iteration 79500, loss = 1.40229528\n",
      "Iteration 79501, loss = 1.18220788\n",
      "Iteration 79502, loss = 1.17673956\n",
      "Iteration 79503, loss = 1.23372483\n",
      "Iteration 79504, loss = 1.34368473\n",
      "Iteration 79505, loss = 1.20953671\n",
      "Iteration 79506, loss = 1.05285260\n",
      "Iteration 79507, loss = 0.99196275\n",
      "Iteration 79508, loss = 0.96920469\n",
      "Iteration 79509, loss = 0.96784658\n",
      "Iteration 79510, loss = 0.92279258\n",
      "Iteration 79511, loss = 0.91928597\n",
      "Iteration 79512, loss = 0.99915260\n",
      "Iteration 79513, loss = 0.98956044\n",
      "Iteration 79514, loss = 0.99182052\n",
      "Iteration 79515, loss = 0.97767384\n",
      "Iteration 79516, loss = 1.01594482\n",
      "Iteration 79517, loss = 1.15230933\n",
      "Iteration 79518, loss = 1.17194472\n",
      "Iteration 79519, loss = 1.17075575\n",
      "Iteration 79520, loss = 1.21274990\n",
      "Iteration 79521, loss = 1.13588580\n",
      "Iteration 79522, loss = 1.19382788\n",
      "Iteration 79523, loss = 1.06770455\n",
      "Iteration 79524, loss = 1.55248556\n",
      "Iteration 79525, loss = 2.49047128\n",
      "Iteration 79526, loss = 1.82812837\n",
      "Iteration 79527, loss = 2.00036463\n",
      "Iteration 79528, loss = 1.57942407\n",
      "Iteration 79529, loss = 1.41078296\n",
      "Iteration 79530, loss = 1.27495711\n",
      "Iteration 79531, loss = 1.24287449\n",
      "Iteration 79532, loss = 1.36501621\n",
      "Iteration 79533, loss = 1.33366354\n",
      "Iteration 79534, loss = 1.38485775\n",
      "Iteration 79535, loss = 1.22444369\n",
      "Iteration 79536, loss = 1.49953441\n",
      "Iteration 79537, loss = 1.52166376\n",
      "Iteration 79538, loss = 1.38681944\n",
      "Iteration 79539, loss = 1.27637054\n",
      "Iteration 79540, loss = 1.30703874\n",
      "Iteration 79541, loss = 1.11571928\n",
      "Iteration 79542, loss = 1.04819464\n",
      "Iteration 79543, loss = 1.01861160\n",
      "Iteration 79544, loss = 1.22647168\n",
      "Iteration 79545, loss = 1.15064986\n",
      "Iteration 79546, loss = 1.18854558\n",
      "Iteration 79547, loss = 1.14070730\n",
      "Iteration 79548, loss = 1.04550579\n",
      "Iteration 79549, loss = 1.09091913\n",
      "Iteration 79550, loss = 1.04994766\n",
      "Iteration 79551, loss = 1.09795739\n",
      "Iteration 79552, loss = 1.14133428\n",
      "Iteration 79553, loss = 1.22495421\n",
      "Iteration 79554, loss = 1.40909263\n",
      "Iteration 79555, loss = 1.64907669\n",
      "Iteration 79556, loss = 1.12614642\n",
      "Iteration 79557, loss = 1.00557518\n",
      "Iteration 79558, loss = 1.05122094\n",
      "Iteration 79559, loss = 1.39212767\n",
      "Iteration 79560, loss = 1.26330420\n",
      "Iteration 79561, loss = 1.11619397\n",
      "Iteration 79562, loss = 1.10020151\n",
      "Iteration 79563, loss = 0.97512746\n",
      "Iteration 79564, loss = 0.96976315\n",
      "Iteration 79565, loss = 0.97114532\n",
      "Iteration 79566, loss = 1.01861705\n",
      "Iteration 79567, loss = 0.99705062\n",
      "Iteration 79568, loss = 0.97873054\n",
      "Iteration 79569, loss = 1.01558596\n",
      "Iteration 79570, loss = 0.92160722\n",
      "Iteration 79571, loss = 0.91176576\n",
      "Iteration 79572, loss = 0.99146666\n",
      "Iteration 79573, loss = 1.04162797\n",
      "Iteration 79574, loss = 1.06239608\n",
      "Iteration 79575, loss = 1.03823899\n",
      "Iteration 79576, loss = 1.00594923\n",
      "Iteration 79577, loss = 0.92993864\n",
      "Iteration 79578, loss = 0.94359116\n",
      "Iteration 79579, loss = 0.96910118\n",
      "Iteration 79580, loss = 0.98787711\n",
      "Iteration 79581, loss = 1.03247494\n",
      "Iteration 79582, loss = 1.01570440\n",
      "Iteration 79583, loss = 1.05810856\n",
      "Iteration 79584, loss = 1.14254677\n",
      "Iteration 79585, loss = 1.51632004\n",
      "Iteration 79586, loss = 1.27229793\n",
      "Iteration 79587, loss = 1.18021357\n",
      "Iteration 79588, loss = 0.95673144\n",
      "Iteration 79589, loss = 1.02584916\n",
      "Iteration 79590, loss = 1.06432924\n",
      "Iteration 79591, loss = 1.39343735\n",
      "Iteration 79592, loss = 1.16333444\n",
      "Iteration 79593, loss = 0.97139383\n",
      "Iteration 79594, loss = 1.00349725\n",
      "Iteration 79595, loss = 0.93330187\n",
      "Iteration 79596, loss = 0.98888663\n",
      "Iteration 79597, loss = 1.11832112\n",
      "Iteration 79598, loss = 1.18156369\n",
      "Iteration 79599, loss = 0.99615175\n",
      "Iteration 79600, loss = 1.16379542\n",
      "Iteration 79601, loss = 1.10738716\n",
      "Iteration 79602, loss = 1.11196855\n",
      "Iteration 79603, loss = 1.42063883\n",
      "Iteration 79604, loss = 1.55633663\n",
      "Iteration 79605, loss = 1.36476367\n",
      "Iteration 79606, loss = 1.16750266\n",
      "Iteration 79607, loss = 1.20167283\n",
      "Iteration 79608, loss = 1.13237091\n",
      "Iteration 79609, loss = 1.31615950\n",
      "Iteration 79610, loss = 1.22596303\n",
      "Iteration 79611, loss = 1.42836072\n",
      "Iteration 79612, loss = 1.70126840\n",
      "Iteration 79613, loss = 1.55948274\n",
      "Iteration 79614, loss = 1.44271041\n",
      "Iteration 79615, loss = 1.33763010\n",
      "Iteration 79616, loss = 1.34301254\n",
      "Iteration 79617, loss = 1.50807332\n",
      "Iteration 79618, loss = 1.79560116\n",
      "Iteration 79619, loss = 1.72293630\n",
      "Iteration 79620, loss = 1.63519999\n",
      "Iteration 79621, loss = 1.35071949\n",
      "Iteration 79622, loss = 1.36575066\n",
      "Iteration 79623, loss = 1.42129141\n",
      "Iteration 79624, loss = 1.35533714\n",
      "Iteration 79625, loss = 1.15005502\n",
      "Iteration 79626, loss = 1.53450543\n",
      "Iteration 79627, loss = 1.19997941\n",
      "Iteration 79628, loss = 1.36359371\n",
      "Iteration 79629, loss = 1.68184303\n",
      "Iteration 79630, loss = 1.40564656\n",
      "Iteration 79631, loss = 1.36214437\n",
      "Iteration 79632, loss = 1.15657653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 79633, loss = 1.19041278\n",
      "Iteration 79634, loss = 1.06484996\n",
      "Iteration 79635, loss = 1.04375031\n",
      "Iteration 79636, loss = 1.05134006\n",
      "Iteration 79637, loss = 1.10226079\n",
      "Iteration 79638, loss = 1.23752695\n",
      "Iteration 79639, loss = 1.16404607\n",
      "Iteration 79640, loss = 1.21238746\n",
      "Iteration 79641, loss = 1.17505551\n",
      "Iteration 79642, loss = 1.05935461\n",
      "Iteration 79643, loss = 0.96348643\n",
      "Iteration 79644, loss = 1.02245101\n",
      "Iteration 79645, loss = 1.07300578\n",
      "Iteration 79646, loss = 1.04324704\n",
      "Iteration 79647, loss = 1.03707686\n",
      "Iteration 79648, loss = 1.00982264\n",
      "Iteration 79649, loss = 1.09963134\n",
      "Iteration 79650, loss = 1.04114518\n",
      "Iteration 79651, loss = 1.09973115\n",
      "Iteration 79652, loss = 1.16860894\n",
      "Iteration 79653, loss = 1.18452611\n",
      "Iteration 79654, loss = 1.28045611\n",
      "Iteration 79655, loss = 1.31600228\n",
      "Iteration 79656, loss = 1.17698670\n",
      "Iteration 79657, loss = 0.96862589\n",
      "Iteration 79658, loss = 1.13247207\n",
      "Iteration 79659, loss = 1.07013513\n",
      "Iteration 79660, loss = 1.16222102\n",
      "Iteration 79661, loss = 1.22620347\n",
      "Iteration 79662, loss = 1.12109368\n",
      "Iteration 79663, loss = 1.36051762\n",
      "Iteration 79664, loss = 1.01307717\n",
      "Iteration 79665, loss = 0.98076771\n",
      "Iteration 79666, loss = 1.00839961\n",
      "Iteration 79667, loss = 1.02975357\n",
      "Iteration 79668, loss = 1.05648826\n",
      "Iteration 79669, loss = 1.25061860\n",
      "Iteration 79670, loss = 1.49595631\n",
      "Iteration 79671, loss = 1.60105624\n",
      "Iteration 79672, loss = 1.57681911\n",
      "Iteration 79673, loss = 1.59791839\n",
      "Iteration 79674, loss = 1.45049413\n",
      "Iteration 79675, loss = 1.24466117\n",
      "Iteration 79676, loss = 1.31439827\n",
      "Iteration 79677, loss = 1.08914478\n",
      "Iteration 79678, loss = 1.27278835\n",
      "Iteration 79679, loss = 1.36850863\n",
      "Iteration 79680, loss = 1.40481961\n",
      "Iteration 79681, loss = 1.15457328\n",
      "Iteration 79682, loss = 1.23725982\n",
      "Iteration 79683, loss = 1.60502070\n",
      "Iteration 79684, loss = 1.70724064\n",
      "Iteration 79685, loss = 2.28134023\n",
      "Iteration 79686, loss = 2.21370690\n",
      "Iteration 79687, loss = 2.68148385\n",
      "Iteration 79688, loss = 2.77170290\n",
      "Iteration 79689, loss = 2.08877713\n",
      "Iteration 79690, loss = 1.91341646\n",
      "Iteration 79691, loss = 2.49623019\n",
      "Iteration 79692, loss = 1.79312310\n",
      "Iteration 79693, loss = 1.49054857\n",
      "Iteration 79694, loss = 1.21315932\n",
      "Iteration 79695, loss = 1.18978702\n",
      "Iteration 79696, loss = 1.04269880\n",
      "Iteration 79697, loss = 1.13938079\n",
      "Iteration 79698, loss = 1.05227680\n",
      "Iteration 79699, loss = 1.13203643\n",
      "Iteration 79700, loss = 0.98854979\n",
      "Iteration 79701, loss = 0.98958675\n",
      "Iteration 79702, loss = 1.07902812\n",
      "Iteration 79703, loss = 1.16678189\n",
      "Iteration 79704, loss = 1.00565910\n",
      "Iteration 79705, loss = 0.93038793\n",
      "Iteration 79706, loss = 0.97772465\n",
      "Iteration 79707, loss = 1.01054048\n",
      "Iteration 79708, loss = 1.19642491\n",
      "Iteration 79709, loss = 1.20210671\n",
      "Iteration 79710, loss = 1.40570510\n",
      "Iteration 79711, loss = 1.74596285\n",
      "Iteration 79712, loss = 1.54852895\n",
      "Iteration 79713, loss = 1.11578441\n",
      "Iteration 79714, loss = 0.99107264\n",
      "Iteration 79715, loss = 0.98183205\n",
      "Iteration 79716, loss = 1.03517153\n",
      "Iteration 79717, loss = 1.07243831\n",
      "Iteration 79718, loss = 1.06175649\n",
      "Iteration 79719, loss = 1.20561659\n",
      "Iteration 79720, loss = 1.03392157\n",
      "Iteration 79721, loss = 1.02017641\n",
      "Iteration 79722, loss = 1.13719929\n",
      "Iteration 79723, loss = 1.21732586\n",
      "Iteration 79724, loss = 1.07987103\n",
      "Iteration 79725, loss = 0.94089749\n",
      "Iteration 79726, loss = 0.90347592\n",
      "Iteration 79727, loss = 0.99818669\n",
      "Iteration 79728, loss = 1.06482401\n",
      "Iteration 79729, loss = 0.94119776\n",
      "Iteration 79730, loss = 0.97717765\n",
      "Iteration 79731, loss = 1.19484822\n",
      "Iteration 79732, loss = 1.27619171\n",
      "Iteration 79733, loss = 1.16129825\n",
      "Iteration 79734, loss = 1.19624717\n",
      "Iteration 79735, loss = 1.11356274\n",
      "Iteration 79736, loss = 1.29385541\n",
      "Iteration 79737, loss = 1.71832070\n",
      "Iteration 79738, loss = 1.61330178\n",
      "Iteration 79739, loss = 1.44639367\n",
      "Iteration 79740, loss = 1.36769847\n",
      "Iteration 79741, loss = 1.38415806\n",
      "Iteration 79742, loss = 1.40567571\n",
      "Iteration 79743, loss = 1.93032618\n",
      "Iteration 79744, loss = 1.44198559\n",
      "Iteration 79745, loss = 1.46057680\n",
      "Iteration 79746, loss = 1.24701236\n",
      "Iteration 79747, loss = 1.20236750\n",
      "Iteration 79748, loss = 1.25491469\n",
      "Iteration 79749, loss = 1.12242299\n",
      "Iteration 79750, loss = 1.14508086\n",
      "Iteration 79751, loss = 1.10796849\n",
      "Iteration 79752, loss = 1.18809405\n",
      "Iteration 79753, loss = 1.29899825\n",
      "Iteration 79754, loss = 1.16902646\n",
      "Iteration 79755, loss = 1.10057457\n",
      "Iteration 79756, loss = 0.93965425\n",
      "Iteration 79757, loss = 1.20818320\n",
      "Iteration 79758, loss = 1.47924557\n",
      "Iteration 79759, loss = 1.35770197\n",
      "Iteration 79760, loss = 1.33276976\n",
      "Iteration 79761, loss = 1.38585382\n",
      "Iteration 79762, loss = 1.27004355\n",
      "Iteration 79763, loss = 1.43417399\n",
      "Iteration 79764, loss = 1.32438389\n",
      "Iteration 79765, loss = 1.08332882\n",
      "Iteration 79766, loss = 0.97070593\n",
      "Iteration 79767, loss = 1.09764686\n",
      "Iteration 79768, loss = 1.09955779\n",
      "Iteration 79769, loss = 1.13338692\n",
      "Iteration 79770, loss = 1.08341014\n",
      "Iteration 79771, loss = 1.17079733\n",
      "Iteration 79772, loss = 1.05365326\n",
      "Iteration 79773, loss = 1.05843300\n",
      "Iteration 79774, loss = 1.17195372\n",
      "Iteration 79775, loss = 1.14404617\n",
      "Iteration 79776, loss = 0.97364478\n",
      "Iteration 79777, loss = 0.98313369\n",
      "Iteration 79778, loss = 1.36207821\n",
      "Iteration 79779, loss = 1.52306989\n",
      "Iteration 79780, loss = 1.40415444\n",
      "Iteration 79781, loss = 1.36084402\n",
      "Iteration 79782, loss = 1.55645049\n",
      "Iteration 79783, loss = 1.61523063\n",
      "Iteration 79784, loss = 1.51139979\n",
      "Iteration 79785, loss = 1.39850922\n",
      "Iteration 79786, loss = 1.40660238\n",
      "Iteration 79787, loss = 1.47489802\n",
      "Iteration 79788, loss = 1.75258840\n",
      "Iteration 79789, loss = 2.16419136\n",
      "Iteration 79790, loss = 2.09552097\n",
      "Iteration 79791, loss = 1.79522066\n",
      "Iteration 79792, loss = 1.68511606\n",
      "Iteration 79793, loss = 1.44592472\n",
      "Iteration 79794, loss = 1.25359204\n",
      "Iteration 79795, loss = 1.26563675\n",
      "Iteration 79796, loss = 1.24981335\n",
      "Iteration 79797, loss = 1.38741041\n",
      "Iteration 79798, loss = 1.10512397\n",
      "Iteration 79799, loss = 1.25753462\n",
      "Iteration 79800, loss = 1.29714175\n",
      "Iteration 79801, loss = 1.33362370\n",
      "Iteration 79802, loss = 1.32650991\n",
      "Iteration 79803, loss = 1.24065052\n",
      "Iteration 79804, loss = 1.19406781\n",
      "Iteration 79805, loss = 1.02442426\n",
      "Iteration 79806, loss = 0.97820015\n",
      "Iteration 79807, loss = 0.95322063\n",
      "Iteration 79808, loss = 0.91464398\n",
      "Iteration 79809, loss = 0.94711424\n",
      "Iteration 79810, loss = 1.04368909\n",
      "Iteration 79811, loss = 0.98858442\n",
      "Iteration 79812, loss = 1.10357343\n",
      "Iteration 79813, loss = 0.99562605\n",
      "Iteration 79814, loss = 1.01016152\n",
      "Iteration 79815, loss = 0.97480353\n",
      "Iteration 79816, loss = 1.08873724\n",
      "Iteration 79817, loss = 1.03301137\n",
      "Iteration 79818, loss = 1.04626494\n",
      "Iteration 79819, loss = 0.98019654\n",
      "Iteration 79820, loss = 0.91553389\n",
      "Iteration 79821, loss = 1.00604052\n",
      "Iteration 79822, loss = 0.97165993\n",
      "Iteration 79823, loss = 0.94712056\n",
      "Iteration 79824, loss = 0.96890944\n",
      "Iteration 79825, loss = 1.09593841\n",
      "Iteration 79826, loss = 1.28270173\n",
      "Iteration 79827, loss = 1.28891145\n",
      "Iteration 79828, loss = 1.27875696\n",
      "Iteration 79829, loss = 1.21936997\n",
      "Iteration 79830, loss = 1.19456520\n",
      "Iteration 79831, loss = 1.48316270\n",
      "Iteration 79832, loss = 1.70167593\n",
      "Iteration 79833, loss = 1.81903862\n",
      "Iteration 79834, loss = 1.30152446\n",
      "Iteration 79835, loss = 1.16942488\n",
      "Iteration 79836, loss = 1.33097138\n",
      "Iteration 79837, loss = 1.22563904\n",
      "Iteration 79838, loss = 1.12357526\n",
      "Iteration 79839, loss = 0.96742206\n",
      "Iteration 79840, loss = 1.05079289\n",
      "Iteration 79841, loss = 0.96126568\n",
      "Iteration 79842, loss = 1.02678732\n",
      "Iteration 79843, loss = 1.10487016\n",
      "Iteration 79844, loss = 1.05374814\n",
      "Iteration 79845, loss = 1.02103664\n",
      "Iteration 79846, loss = 0.96947451\n",
      "Iteration 79847, loss = 0.99095601\n",
      "Iteration 79848, loss = 1.06163420\n",
      "Iteration 79849, loss = 1.10481951\n",
      "Iteration 79850, loss = 1.20642549\n",
      "Iteration 79851, loss = 1.35749653\n",
      "Iteration 79852, loss = 1.08208077\n",
      "Iteration 79853, loss = 1.03121406\n",
      "Iteration 79854, loss = 1.06534175\n",
      "Iteration 79855, loss = 0.93628490\n",
      "Iteration 79856, loss = 0.93425307\n",
      "Iteration 79857, loss = 0.93252165\n",
      "Iteration 79858, loss = 1.10834240\n",
      "Iteration 79859, loss = 1.08452265\n",
      "Iteration 79860, loss = 1.06862741\n",
      "Iteration 79861, loss = 0.93640013\n",
      "Iteration 79862, loss = 1.10396936\n",
      "Iteration 79863, loss = 1.20953706\n",
      "Iteration 79864, loss = 1.11406902\n",
      "Iteration 79865, loss = 1.10502182\n",
      "Iteration 79866, loss = 1.08213643\n",
      "Iteration 79867, loss = 1.07239817\n",
      "Iteration 79868, loss = 1.20851989\n",
      "Iteration 79869, loss = 1.16767884\n",
      "Iteration 79870, loss = 1.04093146\n",
      "Iteration 79871, loss = 1.08070083\n",
      "Iteration 79872, loss = 1.22105269\n",
      "Iteration 79873, loss = 1.19118989\n",
      "Iteration 79874, loss = 1.23439651\n",
      "Iteration 79875, loss = 1.08107283\n",
      "Iteration 79876, loss = 1.33641265\n",
      "Iteration 79877, loss = 1.38396547\n",
      "Iteration 79878, loss = 1.05478058\n",
      "Iteration 79879, loss = 0.92418736\n",
      "Iteration 79880, loss = 0.90333207\n",
      "Iteration 79881, loss = 0.90910343\n",
      "Iteration 79882, loss = 0.91655549\n",
      "Iteration 79883, loss = 0.93451340\n",
      "Iteration 79884, loss = 0.98522764\n",
      "Iteration 79885, loss = 0.98404914\n",
      "Iteration 79886, loss = 0.97483819\n",
      "Iteration 79887, loss = 1.01185689\n",
      "Iteration 79888, loss = 0.99088426\n",
      "Iteration 79889, loss = 0.95618583\n",
      "Iteration 79890, loss = 1.16012677\n",
      "Iteration 79891, loss = 1.20189575\n",
      "Iteration 79892, loss = 1.36131714\n",
      "Iteration 79893, loss = 1.18280384\n",
      "Iteration 79894, loss = 1.10911268\n",
      "Iteration 79895, loss = 1.12552891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 79896, loss = 1.03583972\n",
      "Iteration 79897, loss = 0.96896732\n",
      "Iteration 79898, loss = 0.97643570\n",
      "Iteration 79899, loss = 1.19216573\n",
      "Iteration 79900, loss = 1.11935289\n",
      "Iteration 79901, loss = 1.00826877\n",
      "Iteration 79902, loss = 1.09175847\n",
      "Iteration 79903, loss = 0.87726213\n",
      "Iteration 79904, loss = 0.96790009\n",
      "Iteration 79905, loss = 1.16964498\n",
      "Iteration 79906, loss = 1.30617055\n",
      "Iteration 79907, loss = 1.24078068\n",
      "Iteration 79908, loss = 1.09197378\n",
      "Iteration 79909, loss = 1.37045769\n",
      "Iteration 79910, loss = 2.17668880\n",
      "Iteration 79911, loss = 2.10752495\n",
      "Iteration 79912, loss = 2.11120617\n",
      "Iteration 79913, loss = 1.95959168\n",
      "Iteration 79914, loss = 1.31851386\n",
      "Iteration 79915, loss = 1.27489065\n",
      "Iteration 79916, loss = 1.11197200\n",
      "Iteration 79917, loss = 1.01714740\n",
      "Iteration 79918, loss = 1.08569067\n",
      "Iteration 79919, loss = 1.16649147\n",
      "Iteration 79920, loss = 1.34974994\n",
      "Iteration 79921, loss = 1.32468689\n",
      "Iteration 79922, loss = 1.23256822\n",
      "Iteration 79923, loss = 1.15407068\n",
      "Iteration 79924, loss = 1.11804801\n",
      "Iteration 79925, loss = 1.18736755\n",
      "Iteration 79926, loss = 1.68694355\n",
      "Iteration 79927, loss = 1.37540060\n",
      "Iteration 79928, loss = 1.20953710\n",
      "Iteration 79929, loss = 1.25699087\n",
      "Iteration 79930, loss = 1.26868699\n",
      "Iteration 79931, loss = 1.63540573\n",
      "Iteration 79932, loss = 1.51161358\n",
      "Iteration 79933, loss = 1.35103537\n",
      "Iteration 79934, loss = 1.33020988\n",
      "Iteration 79935, loss = 1.02986380\n",
      "Iteration 79936, loss = 1.06699044\n",
      "Iteration 79937, loss = 1.23031504\n",
      "Iteration 79938, loss = 1.25674780\n",
      "Iteration 79939, loss = 1.74546691\n",
      "Iteration 79940, loss = 1.38638974\n",
      "Iteration 79941, loss = 1.05523507\n",
      "Iteration 79942, loss = 1.15547478\n",
      "Iteration 79943, loss = 1.10590357\n",
      "Iteration 79944, loss = 1.06042004\n",
      "Iteration 79945, loss = 0.97176651\n",
      "Iteration 79946, loss = 0.95771281\n",
      "Iteration 79947, loss = 1.00597704\n",
      "Iteration 79948, loss = 1.32103528\n",
      "Iteration 79949, loss = 1.19319686\n",
      "Iteration 79950, loss = 1.31878130\n",
      "Iteration 79951, loss = 1.14850955\n",
      "Iteration 79952, loss = 1.18657005\n",
      "Iteration 79953, loss = 1.09847035\n",
      "Iteration 79954, loss = 1.11632101\n",
      "Iteration 79955, loss = 0.95805975\n",
      "Iteration 79956, loss = 0.93315997\n",
      "Iteration 79957, loss = 1.05551174\n",
      "Iteration 79958, loss = 0.96596246\n",
      "Iteration 79959, loss = 1.13805891\n",
      "Iteration 79960, loss = 1.01047708\n",
      "Iteration 79961, loss = 1.14863449\n",
      "Iteration 79962, loss = 1.24134390\n",
      "Iteration 79963, loss = 1.24013815\n",
      "Iteration 79964, loss = 1.34926110\n",
      "Iteration 79965, loss = 1.22323562\n",
      "Iteration 79966, loss = 1.18774965\n",
      "Iteration 79967, loss = 1.33453128\n",
      "Iteration 79968, loss = 1.09614535\n",
      "Iteration 79969, loss = 1.00686391\n",
      "Iteration 79970, loss = 0.99053917\n",
      "Iteration 79971, loss = 1.20940029\n",
      "Iteration 79972, loss = 1.12118067\n",
      "Iteration 79973, loss = 0.99381117\n",
      "Iteration 79974, loss = 1.00865746\n",
      "Iteration 79975, loss = 1.02338678\n",
      "Iteration 79976, loss = 1.03706881\n",
      "Iteration 79977, loss = 1.03750207\n",
      "Iteration 79978, loss = 1.09842335\n",
      "Iteration 79979, loss = 1.08463664\n",
      "Iteration 79980, loss = 1.11983522\n",
      "Iteration 79981, loss = 1.02670025\n",
      "Iteration 79982, loss = 0.99250990\n",
      "Iteration 79983, loss = 1.17678625\n",
      "Iteration 79984, loss = 1.22632762\n",
      "Iteration 79985, loss = 1.33133728\n",
      "Iteration 79986, loss = 1.54700630\n",
      "Iteration 79987, loss = 1.39884735\n",
      "Iteration 79988, loss = 1.46872609\n",
      "Iteration 79989, loss = 1.24108609\n",
      "Iteration 79990, loss = 1.18971588\n",
      "Iteration 79991, loss = 1.09930604\n",
      "Iteration 79992, loss = 1.01947758\n",
      "Iteration 79993, loss = 1.24111371\n",
      "Iteration 79994, loss = 1.47846586\n",
      "Iteration 79995, loss = 1.56562822\n",
      "Iteration 79996, loss = 1.64149597\n",
      "Iteration 79997, loss = 1.23106444\n",
      "Iteration 79998, loss = 1.02022007\n",
      "Iteration 79999, loss = 0.99336657\n",
      "Iteration 80000, loss = 0.96200115\n",
      "R^2 Training Score: 0.991 \n",
      "R^2 Testing Score: 0.974\n",
      "RMSE Training Score: 1.326 \n",
      "RMSE Testing Score: 0.708\n",
      "MAE Training Score: 0.805 \n",
      "MAE Testing Score: 0.539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (80000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "reg_opt = MLPRegressor(\n",
    "    activation=clf.best_params_['activation'], \n",
    "\n",
    "    hidden_layer_sizes=clf.best_params_['hidden_layer_sizes'],\n",
    "    learning_rate=clf.best_params_['learning_rate'],\n",
    "    max_iter=clf.best_params_['max_iter'],\n",
    "    solver=clf.best_params_['solver'],\n",
    "    batch_size='auto',\n",
    "    verbose=True, warm_start=False, \n",
    "    early_stopping=False,\n",
    "    alpha=0.0001,\n",
    "    learning_rate_init=0.001, shuffle=True,\n",
    "    random_state=1, tol=0.00001,\n",
    "    beta_1=0.9, beta_2=0.999, epsilon=1e-08,n_iter_no_change=10000)\n",
    "\n",
    "reg_opt.fit(X_train, y_train)\n",
    "\n",
    "print('R^2 Training Score: {:.3f} \\nR^2 Testing Score: {:.3f}'.format(reg_opt.score(X_train, y_train),reg_opt.score(X_test, y_test)))\n",
    "print('RMSE Training Score: {:.3f} \\nRMSE Testing Score: {:.3f}'.format(np.sqrt(mean_squared_error(y_train,reg_opt.predict(X_train))),np.sqrt(mean_squared_error(y_test,reg_opt.predict(X_test)))))\n",
    "print('MAE Training Score: {:.3f} \\nMAE Testing Score: {:.3f}'.format(mean_absolute_error(y_train,reg_opt.predict(X_train)),mean_absolute_error(y_test,reg_opt.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = joblib.dump(reg_opt,\"NN\"+\"_Kr_M_Langmuir.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111.97815907376096\n",
      "201.7148083815378\n",
      "217.51457956286418\n",
      "46949024.508979484\n",
      "201.67357662173126\n",
      "250714339.04153162\n",
      "185.0282200174626\n",
      "201.71276113776625\n",
      "176.2747059275518\n",
      "192.46640456182826\n",
      "Best score=111.9782\n",
      "Best parameters:\n",
      "    - C=92415\n",
      "    - gamma=0.000026\n",
      "    - epsilon=0.683005\n",
      "    - kernel=rbf\n",
      "R^2 Training Score: 0.612 \n",
      "R^2 Testing Score: 0.843\n",
      "RMSE Training Score: 8.623 \n",
      "RMSE Testing Score: 1.732\n",
      "MAE Training Score: 1.928 \n",
      "MAE Testing Score: 0.830\n"
     ]
    }
   ],
   "source": [
    "reg = SVR()\n",
    "space  = [Integer(100,100000, name='C'),\n",
    "            Real(0.000001,1,\"log-uniform\",name='gamma'),\n",
    "            Real(0.1,1,name='epsilon'),\n",
    "            Categorical(('linear','rbf','sigmoid'), name='kernel')\n",
    "         ]\n",
    "@use_named_args(space)\n",
    "\n",
    "def objective(**params):\n",
    "    reg.set_params(**params)\n",
    "    result=-np.mean(cross_val_score(reg, X_train, y_train.ravel(), cv=5, n_jobs=4,\n",
    "                                        scoring=\"neg_mean_squared_error\"))\n",
    "    \n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "res_gp = gp_minimize(objective, space, n_calls=10)\n",
    "\n",
    "print(\"Best score=%.4f\" % res_gp.fun)\n",
    "print(\"\"\"Best parameters:\n",
    "    - C=%d\n",
    "    - gamma=%f\n",
    "    - epsilon=%f\n",
    "    - kernel=%s\"\"\" % (res_gp.x[0],res_gp.x[1],\n",
    "                        res_gp.x[2],res_gp.x[3]))\n",
    "reg_opt = SVR(C=res_gp.x[0],\n",
    "                gamma=res_gp.x[1],\n",
    "                epsilon=res_gp.x[2],\n",
    "                kernel=res_gp.x[3],\n",
    "                tol=0.001,\n",
    "                verbose=False)\n",
    "\n",
    "reg_opt.fit(X_train, y_train.ravel())\n",
    "\n",
    "print('R^2 Training Score: {:.3f} \\nR^2 Testing Score: {:.3f}'.format(reg_opt.score(X_train, y_train),reg_opt.score(X_test, y_test)))\n",
    "print('RMSE Training Score: {:.3f} \\nRMSE Testing Score: {:.3f}'.format(np.sqrt(mean_squared_error(y_train,reg_opt.predict(X_train))),np.sqrt(mean_squared_error(y_test,reg_opt.predict(X_test)))))\n",
    "print('MAE Training Score: {:.3f} \\nMAE Testing Score: {:.3f}'.format(mean_absolute_error(y_train,reg_opt.predict(X_train)),mean_absolute_error(y_test,reg_opt.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = joblib.dump(reg_opt,\"SVM\"+\"_Kr_M_Langmuir.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.46505619870089\n",
      "67.36735772191808\n",
      "64.51447748707491\n",
      "71.93996539482497\n",
      "68.84781509754977\n",
      "64.66011598608574\n",
      "65.00435978560333\n",
      "85.08873969122749\n",
      "72.75845539575957\n",
      "64.72393856417713\n",
      "Best score=64.5145\n",
      "Best parameters:\n",
      "    - learning_rate=0.079881\n",
      "    - n_estimators=839\n",
      "    - loss=linear\n",
      "R^2 Training Score: 0.966 \n",
      "R^2 Testing Score: 0.786\n",
      "RMSE Training Score: 2.562 \n",
      "RMSE Testing Score: 2.022\n",
      "MAE Training Score: 1.926 \n",
      "MAE Testing Score: 1.623\n"
     ]
    }
   ],
   "source": [
    "reg = AdaBoostRegressor()\n",
    "space  = [Real(0.01,1,\"log-uniform\",name='learning_rate'),\n",
    "            Integer(1,1000,name='n_estimators'),\n",
    "            Categorical(('linear','square','exponential'), name='loss')]\n",
    "@use_named_args(space)\n",
    "\n",
    "def objective(**params):\n",
    "    reg.set_params(**params)\n",
    "    result=-np.mean(cross_val_score(reg, X_train, y_train.ravel(), cv=5, n_jobs=4,scoring=\"neg_mean_squared_error\"))\n",
    "    \n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "res_gp = gp_minimize(objective, space, n_calls=10)\n",
    "\n",
    "print(\"Best score=%.4f\" % res_gp.fun)\n",
    "print(\"\"\"Best parameters:\n",
    "    - learning_rate=%f\n",
    "    - n_estimators=%d\n",
    "    - loss=%s\"\"\" % (res_gp.x[0],res_gp.x[1],res_gp.x[2]))\n",
    "\n",
    "reg_opt = AdaBoostRegressor(learning_rate=res_gp.x[0],n_estimators=res_gp.x[1],loss=res_gp.x[2])\n",
    "reg_opt.fit(X_train, y_train.ravel())\n",
    "\n",
    "print('R^2 Training Score: {:.3f} \\nR^2 Testing Score: {:.3f}'.format(reg_opt.score(X_train, y_train),reg_opt.score(X_test, y_test)))\n",
    "print('RMSE Training Score: {:.3f} \\nRMSE Testing Score: {:.3f}'.format(np.sqrt(mean_squared_error(y_train,reg_opt.predict(X_train))),np.sqrt(mean_squared_error(y_test,reg_opt.predict(X_test)))))\n",
    "print('MAE Training Score: {:.3f} \\nMAE Testing Score: {:.3f}'.format(mean_absolute_error(y_train,reg_opt.predict(X_train)),mean_absolute_error(y_test,reg_opt.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PLD : 0.023894980920528056', 'LCD : 0.006563876237874518', 'Density : 0.5134641769216677', 'ASA_m2_cm3 : 0.012773011637887152', 'AV_VF : 0.30032783076992725', 'Kr_heat : 0.14297612351211503']\n"
     ]
    }
   ],
   "source": [
    "save_model = joblib.dump(reg_opt,\"ABR\"+\"_Kr_M_Langmuir.pkl\")\n",
    "print([feature+\" : \"+str(reg_opt.feature_importances_[i]) for i,feature in enumerate(feature_names)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184.5797119812707\n",
      "175.07605490680234\n",
      "194.01060685311856\n",
      "184.85040736820667\n",
      "184.15297735032843\n",
      "194.02162087716536\n",
      "182.88868653948123\n",
      "180.4315943917896\n",
      "182.89919152039232\n",
      "193.96602486025785\n",
      "234.59539982322798\n",
      "181.26608047691704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175.07605490680234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175.07605490680234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175.07605490680234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175.07605490680234\n",
      "177.1859155901856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175.07605490680234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175.07605490680234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175.07605490680234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175.07605490680234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175.07605490680234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175.07605490680234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175.07605490680234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175.07605490680234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175.07605490680234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175.07605490680234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175.07605490680234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175.07605490680234\n",
      "174.82005294046093\n",
      "169.0836559302521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169.0836559302521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169.0836559302521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169.0836559302521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169.0836559302521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169.0836559302521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169.0836559302521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169.0836559302521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169.0836559302521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169.0836559302521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169.0836559302521\n",
      "169.0836559302521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169.0836559302521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169.0836559302521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169.0836559302521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169.0836559302521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169.0836559302521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169.0836559302521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169.0836559302521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169.0836559302521\n",
      "Best score=169.0837\n",
      "Best parameters:\n",
      "    - weights=uniform\n",
      "    - n_neighbors=5\n",
      "    - algorithm=auto\n",
      "    - leaf_size=3\n",
      "    - p=1\n",
      "R^2 Training Score: 0.521 \n",
      "R^2 Testing Score: 0.621\n",
      "RMSE Training Score: 9.587 \n",
      "RMSE Testing Score: 2.694\n",
      "MAE Training Score: 2.965 \n",
      "MAE Testing Score: 1.317\n"
     ]
    }
   ],
   "source": [
    "reg = KNeighborsRegressor()\n",
    "space  = [Categorical(('uniform','distance'),name='weights'),\n",
    "            Integer(1,100,name='n_neighbors'),\n",
    "            Categorical(('auto','ball_tree','kd_tree','brute'), name='algorithm'),\n",
    "            Integer(1,10,name='leaf_size'),\n",
    "            Integer(1,10,name='p')\n",
    "         ]\n",
    "@use_named_args(space)\n",
    "\n",
    "def objective(**params):\n",
    "    reg.set_params(**params)\n",
    "    result=-np.mean(cross_val_score(reg, X_train, y_train, cv=5, n_jobs=4,\n",
    "                                        scoring=\"neg_mean_squared_error\"))\n",
    "    \n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "res_gp = gp_minimize(objective, space, n_calls=50)\n",
    "\n",
    "print(\"Best score=%.4f\" % res_gp.fun)\n",
    "print(\"\"\"Best parameters:\n",
    "    - weights=%s\n",
    "    - n_neighbors=%d\n",
    "    - algorithm=%s\n",
    "    - leaf_size=%d\n",
    "    - p=%d\"\"\" % (res_gp.x[0],res_gp.x[1],res_gp.x[2],res_gp.x[3],res_gp.x[4]))\n",
    "reg_opt = KNeighborsRegressor(weights=res_gp.x[0],n_neighbors=res_gp.x[1],algorithm=res_gp.x[2],leaf_size=res_gp.x[3],p=res_gp.x[4])\n",
    "\n",
    "reg_opt.fit(X_train, y_train)\n",
    "\n",
    "print('R^2 Training Score: {:.3f} \\nR^2 Testing Score: {:.3f}'.format(reg_opt.score(X_train, y_train),reg_opt.score(X_test, y_test)))\n",
    "print('RMSE Training Score: {:.3f} \\nRMSE Testing Score: {:.3f}'.format(np.sqrt(mean_squared_error(y_train,reg_opt.predict(X_train))),np.sqrt(mean_squared_error(y_test,reg_opt.predict(X_test)))))\n",
    "print('MAE Training Score: {:.3f} \\nMAE Testing Score: {:.3f}'.format(mean_absolute_error(y_train,reg_opt.predict(X_train)),mean_absolute_error(y_test,reg_opt.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = joblib.dump(reg_opt,\"KNN\"+\"_Kr_M_Langmuir.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.88299463410706\n",
      "118.79450292818838\n",
      "121.36837712775218\n",
      "113.48441106147115\n",
      "120.81642454835749\n",
      "118.79531962472095\n",
      "114.27663177620396\n",
      "118.83265753539986\n",
      "121.10865967047353\n",
      "121.4034181720809\n",
      "113.27427019620629\n",
      "112.86487170532772\n",
      "112.86487170532772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.86487170532772\n",
      "119.29168610917903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.86487170532772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.86487170532772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.86487170532772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.86487170532772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.86487170532772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.86487170532772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.86487170532772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.86487170532772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.86487170532772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.86487170532772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.86487170532772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.86487170532772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.86487170532772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.86487170532772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.86487170532772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.86487170532772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.86487170532772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.86487170532772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.86487170532772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.86487170532772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.86487170532772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.86487170532772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.86487170532772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.86487170532772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.86487170532772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.86487170532772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.86487170532772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.86487170532772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.86487170532772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.86487170532772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.86487170532772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.86487170532772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.86487170532772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.86487170532772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.86487170532772\n",
      "Best score=112.8649\n",
      "Best parameters:\n",
      "    - random_state=0\n",
      "    - alphas=0.000000\n",
      "    - tol=0.010000\n",
      "R^2 Training Score: 0.493 \n",
      "R^2 Testing Score: 0.567\n",
      "RMSE Training Score: 9.860 \n",
      "RMSE Testing Score: 2.877\n",
      "MAE Training Score: 4.238 \n",
      "MAE Testing Score: 2.027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-51-f0beb3db35d0>:25: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  reg_opt.fit(X_train, y_train)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.832e+04, tolerance: 1.907e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "reg = Lasso(max_iter=10000,fit_intercept=True)\n",
    "space  = [Integer(0,1000,name='random_state'),\n",
    "            Real(0,1,name='alpha'),\n",
    "            Real(0.0001,0.01,\"log-uniform\",name='tol')\n",
    "         ]\n",
    "@use_named_args(space)\n",
    "\n",
    "def objective(**params):\n",
    "    reg.set_params(**params)\n",
    "    result=-np.mean(cross_val_score(reg, X_train, y_train, cv=5, n_jobs=4,scoring=\"neg_mean_squared_error\"))\n",
    "    \n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "res_gp = gp_minimize(objective, space, n_calls=50)\n",
    "\n",
    "print(\"Best score=%.4f\" % res_gp.fun)\n",
    "print(\"\"\"Best parameters:\n",
    "    - random_state=%d\n",
    "    - alphas=%4f\n",
    "    - tol=%4f\"\"\" % (res_gp.x[0],res_gp.x[1],res_gp.x[2]))\n",
    "\n",
    "reg_opt = Lasso(random_state=res_gp.x[0],alpha=res_gp.x[1],tol=res_gp.x[2],max_iter=10000,fit_intercept=True)\n",
    "\n",
    "reg_opt.fit(X_train, y_train)\n",
    "\n",
    "print('R^2 Training Score: {:.3f} \\nR^2 Testing Score: {:.3f}'.format(reg_opt.score(X_train, y_train),reg_opt.score(X_test, y_test)))\n",
    "print('RMSE Training Score: {:.3f} \\nRMSE Testing Score: {:.3f}'.format(np.sqrt(mean_squared_error(y_train,reg_opt.predict(X_train))),np.sqrt(mean_squared_error(y_test,reg_opt.predict(X_test)))))\n",
    "print('MAE Training Score: {:.3f} \\nMAE Testing Score: {:.3f}'.format(mean_absolute_error(y_train,reg_opt.predict(X_train)),mean_absolute_error(y_test,reg_opt.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = joblib.dump(reg_opt,\"LASSO\"+\"_Kr_M_Langmuir.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-22T02:18:03.824690Z",
     "start_time": "2021-10-22T02:18:03.310064Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRegressor(alpha=0.019183749921977387, base_score=0.5, booster='gbtree',\n",
      "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
      "             gamma=0.001, gpu_id=-1, importance_type='gain',\n",
      "             interaction_constraints='', learning_rate=0.18800395774002038,\n",
      "             max_delta_step=0, max_depth=42, min_child_weight=1, missing=nan,\n",
      "             monotone_constraints='()', n_estimators=446, n_jobs=16,\n",
      "             num_parallel_tree=31, random_state=0, reg_alpha=0.0191837493,\n",
      "             reg_lambda=1, scale_pos_weight=1, subsample=0.27494562012538826,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "R^2 Training Score: 0.99989 \n",
      "R^2 Testing Score: 0.96040\n",
      "RMSE Training Score: 0.226 \n",
      "RMSE Testing Score: 0.753\n",
      "MAE Training Score: 0.027 \n",
      "MAE Testing Score: 0.537\n",
      "['PLD : 0.05914881', 'LCD : 0.034556314', 'Density : 0.55262065', 'ASA_m2_cm3 : 0.0028326828', 'AV_VF : 0.1259806', 'Xe_heat : 0.22486094']\n"
     ]
    }
   ],
   "source": [
    "reg_opt= joblib.load(\"GBR\"+\"_Xe_M_Langmuir.pkl\") \n",
    "print(reg_opt)\n",
    "print('R^2 Training Score: {:.5f} \\nR^2 Testing Score: {:.5f}'.format(reg_opt.score(X_train, y_train),reg_opt.score(X_test, y_test)))\n",
    "print('RMSE Training Score: {:.3f} \\nRMSE Testing Score: {:.3f}'.format(np.sqrt(mean_squared_error(y_train,reg_opt.predict(X_train))),np.sqrt(mean_squared_error(y_test,reg_opt.predict(X_test)))))\n",
    "print('MAE Training Score: {:.3f} \\nMAE Testing Score: {:.3f}'.format(mean_absolute_error(y_train,reg_opt.predict(X_train)),mean_absolute_error(y_test,reg_opt.predict(X_test))))\n",
    "print([feature+\" : \"+str(reg_opt.feature_importances_[i]) for i,feature in enumerate(feature_names)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "235px",
    "left": "590px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
