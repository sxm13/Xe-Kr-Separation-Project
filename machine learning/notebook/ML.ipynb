{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from os.path import expanduser\n",
    "home = expanduser(\"~\")\n",
    "import sys\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error,r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor,AdaBoostRegressor\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from xgboost import XGBRegressor\n",
    "from skopt.utils import use_named_args\n",
    "from skopt import gp_minimize\n",
    "import joblib\n",
    "import pickle\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR, NuSVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Load file name : divided_set_0.8_0.2_Xe_M.txt\n",
      "# of diverse set, remaining set : 864 217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-b9da5852b289>:31: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  number[i] = np.asarray(d[0],dtype=np.int)\n"
     ]
    }
   ],
   "source": [
    "diverse_set=[]\n",
    "remaining_set=[]\n",
    "\n",
    "txt = open(\"divided_set_\"+str(diverse_ratio)+\"_\"+str(\"%.1f\"%remaining_ratio)+\"_\"+\"Xe_M.txt\",'r').read()\n",
    "print(\" Load file name : divided_set_\"+str(diverse_ratio)+\"_\"+str(\"%.1f\"%remaining_ratio)+\"_\"+\"Xe_M.txt\")\n",
    "s1=txt.find(\"[\",0)\n",
    "s2=txt.find(\"]\",s1)\n",
    "diverse_set=txt[s1+1:s2].split(\", \")\n",
    "diverse_set=[int(i) for i in diverse_set]\n",
    "s3=txt.find(\"[\",s2)\n",
    "s4=txt.find(\"]\",s3)\n",
    "remaining_set=txt[s3+1:s4].split(\", \")\n",
    "remaining_set=[int(i) for i in remaining_set]\n",
    "print(\"# of diverse set, remaining set :\",len(diverse_set),len(remaining_set))\n",
    "\n",
    "\n",
    "with open(data_file_name) as f:\n",
    "        data_file = csv.reader(f)\n",
    "        temp = next(data_file)\n",
    "        N_samples = int(temp[0])\n",
    "        N_features = int(temp[1])\n",
    "        N_targets = int(temp[2])\n",
    "        number = np.empty((N_samples,))\n",
    "        structure = np.empty((N_samples,))\n",
    "        data = np.empty((N_samples, N_features))\n",
    "        target = np.empty((N_samples,N_targets))\n",
    "        temp = next(data_file)  # names of features\n",
    "        structure = []\n",
    "        feature_names=temp[2:2+N_features]\n",
    "        for i, d in enumerate(data_file):\n",
    "            number[i] = np.asarray(d[0],dtype=np.int)\n",
    "            structure.append(d[1])\n",
    "            data[i] = np.asarray(d[2:2+N_features], dtype=np.float64)\n",
    "            target[i] = np.asarray(d[-N_targets:], dtype=np.float64)\n",
    "N_materials = data.shape[0]\n",
    "\n",
    "diverse_set_total=[]\n",
    "remaining_set_total=[]\n",
    "for i,diverse in enumerate(diverse_set):\n",
    "    arridx = np.where(number == diverse)\n",
    "    for j,element_div in enumerate(arridx[0]):\n",
    "        diverse_set_total.append(element_div)\n",
    "for i,remaining in enumerate(remaining_set):\n",
    "    arridx = np.where(number == remaining)\n",
    "    for j,element_rem in enumerate(arridx[0]):\n",
    "        remaining_set_total.append(element_rem)        \n",
    "\n",
    "X_train = data[diverse_set_total]\n",
    "y_train = target[diverse_set_total]\n",
    "X_test = data[remaining_set_total]\n",
    "y_test = target[remaining_set_total]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Train and save models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.989687406303716\n",
      "16.665505607227928\n",
      "19.3286231309702\n",
      "27.321494814715436\n",
      "21.047710361294143\n",
      "19.41226274585908\n",
      "31.65140323305149\n",
      "22.221293591600766\n",
      "30.24744930659228\n",
      "28.490100491400124\n",
      "Best score=16.6655\n",
      "Best parameters:\n",
      "    - n_estimator=165\n",
      "    - max_depth=14\n",
      "    - min_samples_split=11\n",
      "    - min_samples_leaf=6\n",
      "R^2 Training Score: 0.859 \n",
      "R^2 Testing Score: 0.962\n",
      "RMSE Training Score: 3.449 \n",
      "RMSE Testing Score: 0.694\n",
      "MAE Training Score: 0.925 \n",
      "MAE Testing Score: 0.489\n"
     ]
    }
   ],
   "source": [
    "reg = RandomForestRegressor()\n",
    "space  = [Integer(1, 200, name='n_estimators'),\n",
    "          Integer(1, 30, name='max_depth'),\n",
    "         Integer(1, 30, name='min_samples_split'),\n",
    "         Integer(1, 30, name='min_samples_leaf')]\n",
    "@use_named_args(space)\n",
    "\n",
    "def objective(**params):\n",
    "    reg.set_params(**params)\n",
    "    result=-np.mean(cross_val_score(reg, X_train, y_train, cv=5, n_jobs=-1,\n",
    "                                        scoring=\"neg_mean_squared_error\"))\n",
    "    \n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "res_gp = gp_minimize(objective, space, n_calls=10)\n",
    "\n",
    "print(\"Best score=%.4f\" % res_gp.fun)\n",
    "print(\"\"\"Best parameters:\n",
    "    - n_estimator=%d\n",
    "    - max_depth=%d\n",
    "    - min_samples_split=%d\n",
    "    - min_samples_leaf=%d\"\"\" % (res_gp.x[0], res_gp.x[1],\n",
    "                                res_gp.x[2], res_gp.x[3]))\n",
    "reg_opt = RandomForestRegressor(n_estimators=res_gp.x[0],\n",
    "                                        max_depth=res_gp.x[1],\n",
    "                                       min_samples_split=res_gp.x[2],\n",
    "                                       min_samples_leaf=res_gp.x[3],\n",
    "                                       random_state=445,\n",
    "                                       max_features='sqrt')\n",
    "reg_opt.fit(X_train, y_train.ravel())\n",
    "\n",
    "print('R^2 Training Score: {:.3f} \\nR^2 Testing Score: {:.3f}'.format(reg_opt.score(X_train, y_train),reg_opt.score(X_test, y_test)))\n",
    "print('RMSE Training Score: {:.3f} \\nRMSE Testing Score: {:.3f}'.format(np.sqrt(mean_squared_error(y_train,reg_opt.predict(X_train))),np.sqrt(mean_squared_error(y_test,reg_opt.predict(X_test)))))\n",
    "print('MAE Training Score: {:.3f} \\nMAE Testing Score: {:.3f}'.format(mean_absolute_error(y_train,reg_opt.predict(X_train)),mean_absolute_error(y_test,reg_opt.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = joblib.dump(reg_opt,\"RF\"+\"_Xe_M.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 GBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.594710577459656\n",
      "51.44963813657016\n",
      "10.953791542688702\n",
      "37.19817176140925\n",
      "102.37001212656838\n",
      "28.67642949320999\n",
      "43.421028462673846\n",
      "56.40105959479371\n",
      "21.5216645473865\n",
      "134.6291998532184\n",
      "134.6291998532184\n",
      "14.435650495682244\n",
      "71.30120599402592\n",
      "21.753692039048175\n",
      "15.25400550155845\n",
      "9.236948207151395\n",
      "11.27947655662409\n",
      "10.942820195934654\n",
      "12.395067424728007\n",
      "13.696699872906843\n",
      "22.64731091895915\n",
      "11.860141347616137\n",
      "9.514937726464773\n",
      "15.615797732964513\n",
      "30.337613013390108\n",
      "85.11105157602917\n",
      "11.1630824835361\n",
      "11.77180887487714\n",
      "12.034168673692918\n",
      "9.976750274816045\n",
      "10.998882262978253\n",
      "10.035071982728928\n",
      "98.2305761868148\n",
      "10.187790214254349\n",
      "10.343494717443274\n",
      "9.457008092434021\n",
      "12.004170906013657\n",
      "10.074396499497716\n",
      "9.07222992879894\n",
      "9.149665401683373\n",
      "9.896517101974844\n",
      "11.05870831884111\n",
      "82.02064701451533\n",
      "9.577999403122629\n",
      "9.536553133804736\n",
      "10.15132449216027\n",
      "9.127602283484112\n",
      "9.394829636382415\n",
      "12.545990694169854\n",
      "10.925466886385653\n",
      "9.591493821890065\n",
      "12.424224499659596\n",
      "9.684134775599038\n",
      "11.525597765242996\n",
      "9.258435328179676\n",
      "29.607074585585554\n",
      "11.823330552076339\n",
      "9.836273866207488\n",
      "20.339337779398797\n",
      "11.033548509488682\n",
      "9.39075413680651\n",
      "12.27580846607794\n",
      "13.223859453369133\n",
      "9.551049270106025\n",
      "9.46590911782679\n",
      "11.99502398104267\n",
      "9.92478728166201\n",
      "8.863348702168585\n",
      "11.464689133514229\n",
      "13.730217176723178\n",
      "11.263903134890473\n",
      "12.913178226848846\n",
      "9.535355456609828\n",
      "11.404981604385592\n",
      "9.704114947876175\n",
      "10.471846290338458\n",
      "10.646095200761845\n",
      "11.36731107024621\n",
      "9.452353661561112\n",
      "10.804711389143977\n",
      "9.334468390185695\n",
      "9.48212372541574\n",
      "10.645664339670823\n",
      "48.80810252368432\n",
      "9.819450804090968\n",
      "9.727473042743307\n",
      "10.075072223271983\n",
      "12.047749538654948\n",
      "12.646320360117778\n",
      "11.561764664034303\n",
      "12.899617313461826\n",
      "12.112058345188787\n",
      "10.025992060712952\n",
      "9.389984761726886\n",
      "8.884429526070376\n",
      "10.053751699624922\n",
      "9.838130889830222\n",
      "9.132550814263125\n",
      "9.711266230313594\n",
      "9.30582625455242\n",
      "Best score=8.8633\n",
      "Best parameters:\n",
      "    - n_estimator=300\n",
      "    - max_depth=4\n",
      "    - num_parallel_tree=7\n",
      "    - min_child_weight=1\n",
      "    - learning_rate=0.814417\n",
      "    - subsample=0.384980\n",
      "    - gamma=0.148217\n",
      "    - alpha=0.219724\n",
      "R^2 Training Score: 1.000 \n",
      "R^2 Testing Score: 0.945\n",
      "RMSE Training Score: 0.145 \n",
      "RMSE Testing Score: 0.835\n",
      "MAE Training Score: 0.114 \n",
      "MAE Testing Score: 0.591\n"
     ]
    }
   ],
   "source": [
    "reg = XGBRegressor()\n",
    "space  = [Integer(1,300, name='n_estimators'),\n",
    "            Integer(1, 30, name='max_depth'),\n",
    "            Integer(1, 30, name='num_parallel_tree'),\n",
    "            Integer(1, 30, name='min_child_weight'),\n",
    "            Real(0.001,1,\"log-uniform\",name='learning_rate'),\n",
    "            Real(0.01,1,name='subsample'),\n",
    "            Real(0.001,30,\"log-uniform\",name='gamma'),\n",
    "            Real(0, 1, name='alpha')\n",
    "         ]\n",
    "@use_named_args(space)\n",
    "\n",
    "def objective(**params):\n",
    "    reg.set_params(**params)\n",
    "    result=-np.mean(cross_val_score(reg, X_train, y_train, cv=5, n_jobs=5,\n",
    "                                        scoring=\"neg_mean_squared_error\"))\n",
    "    \n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "res_gp = gp_minimize(objective, space, n_calls=100)\n",
    "\n",
    "print(\"Best score=%.4f\" % res_gp.fun)\n",
    "print(\"\"\"Best parameters:\n",
    "    - n_estimator=%d\n",
    "    - max_depth=%d\n",
    "    - num_parallel_tree=%d\n",
    "    - min_child_weight=%d\n",
    "    - learning_rate=%f\n",
    "    - subsample=%f\n",
    "    - gamma=%f\n",
    "    - alpha=%f\"\"\" % (res_gp.x[0],res_gp.x[1],\n",
    "                        res_gp.x[2],res_gp.x[3],\n",
    "                        res_gp.x[4],res_gp.x[5],\n",
    "                        res_gp.x[6],res_gp.x[7]\n",
    "                         ))\n",
    "reg_opt = XGBRegressor(n_estimators=res_gp.x[0],\n",
    "                                    max_depth=res_gp.x[1],\n",
    "                                    num_parallel_tree=res_gp.x[2],\n",
    "                                    min_child_weight=res_gp.x[3],\n",
    "                                    learning_rate=res_gp.x[4],\n",
    "                                    subsample=res_gp.x[5],\n",
    "                                    gamma=res_gp.x[6],\n",
    "                                   alpha=res_gp.x[7]\n",
    "                                  )\n",
    "reg_opt.fit(X_train, y_train)\n",
    "\n",
    "print('R^2 Training Score: {:.3f} \\nR^2 Testing Score: {:.3f}'.format(reg_opt.score(X_train, y_train),reg_opt.score(X_test, y_test)))\n",
    "print('RMSE Training Score: {:.3f} \\nRMSE Testing Score: {:.3f}'.format(np.sqrt(mean_squared_error(y_train,reg_opt.predict(X_train))),np.sqrt(mean_squared_error(y_test,reg_opt.predict(X_test)))))\n",
    "print('MAE Training Score: {:.3f} \\nMAE Testing Score: {:.3f}'.format(mean_absolute_error(y_train,reg_opt.predict(X_train)),mean_absolute_error(y_test,reg_opt.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = joblib.dump(reg_opt,\"GBR\"+\"_Xe_M.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PLD : 0.02831871', 'LCD : 0.009536757', 'Density : 0.17611946', 'ASA_m2_cm3 : 0.008124442', 'AV_VF : 0.5659218', 'Xe_heat : 0.21197888']\n"
     ]
    }
   ],
   "source": [
    "print([feature+\" : \"+str(reg_opt.feature_importances_[i]) for i,feature in enumerate(feature_names)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.70147084 0.48303562 0.83959607 0.48303562        nan 0.48303562\n",
      " 0.88400691 0.48303562 0.70147084 0.48303562 0.83959607 0.48303562\n",
      "        nan 0.48303562 0.88400691 0.48303562 0.62563449 0.57669138\n",
      " 0.76332103 0.57669138        nan 0.57669138 0.85829698 0.57669138\n",
      " 0.62563449 0.57669138 0.76332103 0.57669138        nan 0.57669138\n",
      " 0.85829698 0.57669138]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      " {'activation': 'relu', 'hidden_layer_sizes': (60, 60), 'learning_rate': 'constant', 'max_iter': 40000, 'solver': 'adam'}\n",
      "0.701 (+/-0.458) for {'activation': 'relu', 'hidden_layer_sizes': (60, 60), 'learning_rate': 'constant', 'max_iter': 1000, 'solver': 'adam'}\n",
      "0.483 (+/-0.084) for {'activation': 'relu', 'hidden_layer_sizes': (60, 60), 'learning_rate': 'constant', 'max_iter': 1000, 'solver': 'lbfgs'}\n",
      "0.840 (+/-0.199) for {'activation': 'relu', 'hidden_layer_sizes': (60, 60), 'learning_rate': 'constant', 'max_iter': 10000, 'solver': 'adam'}\n",
      "0.483 (+/-0.084) for {'activation': 'relu', 'hidden_layer_sizes': (60, 60), 'learning_rate': 'constant', 'max_iter': 10000, 'solver': 'lbfgs'}\n",
      "nan (+/-nan) for {'activation': 'relu', 'hidden_layer_sizes': (60, 60), 'learning_rate': 'constant', 'max_iter': 20000.3, 'solver': 'adam'}\n",
      "0.483 (+/-0.084) for {'activation': 'relu', 'hidden_layer_sizes': (60, 60), 'learning_rate': 'constant', 'max_iter': 20000.3, 'solver': 'lbfgs'}\n",
      "0.884 (+/-0.119) for {'activation': 'relu', 'hidden_layer_sizes': (60, 60), 'learning_rate': 'constant', 'max_iter': 40000, 'solver': 'adam'}\n",
      "0.483 (+/-0.084) for {'activation': 'relu', 'hidden_layer_sizes': (60, 60), 'learning_rate': 'constant', 'max_iter': 40000, 'solver': 'lbfgs'}\n",
      "0.701 (+/-0.458) for {'activation': 'relu', 'hidden_layer_sizes': (60, 60), 'learning_rate': 'adaptive', 'max_iter': 1000, 'solver': 'adam'}\n",
      "0.483 (+/-0.084) for {'activation': 'relu', 'hidden_layer_sizes': (60, 60), 'learning_rate': 'adaptive', 'max_iter': 1000, 'solver': 'lbfgs'}\n",
      "0.840 (+/-0.199) for {'activation': 'relu', 'hidden_layer_sizes': (60, 60), 'learning_rate': 'adaptive', 'max_iter': 10000, 'solver': 'adam'}\n",
      "0.483 (+/-0.084) for {'activation': 'relu', 'hidden_layer_sizes': (60, 60), 'learning_rate': 'adaptive', 'max_iter': 10000, 'solver': 'lbfgs'}\n",
      "nan (+/-nan) for {'activation': 'relu', 'hidden_layer_sizes': (60, 60), 'learning_rate': 'adaptive', 'max_iter': 20000.3, 'solver': 'adam'}\n",
      "0.483 (+/-0.084) for {'activation': 'relu', 'hidden_layer_sizes': (60, 60), 'learning_rate': 'adaptive', 'max_iter': 20000.3, 'solver': 'lbfgs'}\n",
      "0.884 (+/-0.119) for {'activation': 'relu', 'hidden_layer_sizes': (60, 60), 'learning_rate': 'adaptive', 'max_iter': 40000, 'solver': 'adam'}\n",
      "0.483 (+/-0.084) for {'activation': 'relu', 'hidden_layer_sizes': (60, 60), 'learning_rate': 'adaptive', 'max_iter': 40000, 'solver': 'lbfgs'}\n",
      "0.626 (+/-0.300) for {'activation': 'relu', 'hidden_layer_sizes': (60,), 'learning_rate': 'constant', 'max_iter': 1000, 'solver': 'adam'}\n",
      "0.577 (+/-0.289) for {'activation': 'relu', 'hidden_layer_sizes': (60,), 'learning_rate': 'constant', 'max_iter': 1000, 'solver': 'lbfgs'}\n",
      "0.763 (+/-0.229) for {'activation': 'relu', 'hidden_layer_sizes': (60,), 'learning_rate': 'constant', 'max_iter': 10000, 'solver': 'adam'}\n",
      "0.577 (+/-0.289) for {'activation': 'relu', 'hidden_layer_sizes': (60,), 'learning_rate': 'constant', 'max_iter': 10000, 'solver': 'lbfgs'}\n",
      "nan (+/-nan) for {'activation': 'relu', 'hidden_layer_sizes': (60,), 'learning_rate': 'constant', 'max_iter': 20000.3, 'solver': 'adam'}\n",
      "0.577 (+/-0.289) for {'activation': 'relu', 'hidden_layer_sizes': (60,), 'learning_rate': 'constant', 'max_iter': 20000.3, 'solver': 'lbfgs'}\n",
      "0.858 (+/-0.131) for {'activation': 'relu', 'hidden_layer_sizes': (60,), 'learning_rate': 'constant', 'max_iter': 40000, 'solver': 'adam'}\n",
      "0.577 (+/-0.289) for {'activation': 'relu', 'hidden_layer_sizes': (60,), 'learning_rate': 'constant', 'max_iter': 40000, 'solver': 'lbfgs'}\n",
      "0.626 (+/-0.300) for {'activation': 'relu', 'hidden_layer_sizes': (60,), 'learning_rate': 'adaptive', 'max_iter': 1000, 'solver': 'adam'}\n",
      "0.577 (+/-0.289) for {'activation': 'relu', 'hidden_layer_sizes': (60,), 'learning_rate': 'adaptive', 'max_iter': 1000, 'solver': 'lbfgs'}\n",
      "0.763 (+/-0.229) for {'activation': 'relu', 'hidden_layer_sizes': (60,), 'learning_rate': 'adaptive', 'max_iter': 10000, 'solver': 'adam'}\n",
      "0.577 (+/-0.289) for {'activation': 'relu', 'hidden_layer_sizes': (60,), 'learning_rate': 'adaptive', 'max_iter': 10000, 'solver': 'lbfgs'}\n",
      "nan (+/-nan) for {'activation': 'relu', 'hidden_layer_sizes': (60,), 'learning_rate': 'adaptive', 'max_iter': 20000.3, 'solver': 'adam'}\n",
      "0.577 (+/-0.289) for {'activation': 'relu', 'hidden_layer_sizes': (60,), 'learning_rate': 'adaptive', 'max_iter': 20000.3, 'solver': 'lbfgs'}\n",
      "0.858 (+/-0.131) for {'activation': 'relu', 'hidden_layer_sizes': (60,), 'learning_rate': 'adaptive', 'max_iter': 40000, 'solver': 'adam'}\n",
      "0.577 (+/-0.289) for {'activation': 'relu', 'hidden_layer_sizes': (60,), 'learning_rate': 'adaptive', 'max_iter': 40000, 'solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPRegressor(batch_size='auto',\n",
    "    verbose=False, warm_start=False, \n",
    "    early_stopping=False,\n",
    "    alpha=0.0001,\n",
    "    learning_rate_init=0.001, shuffle=True,\n",
    "    random_state=1, tol=0.00001,\n",
    "    beta_1=0.9, beta_2=0.999, epsilon=1e-08,n_iter_no_change=10000)\n",
    "# space  = [Integer(100,1000, name='hidden_neurons'),\n",
    "#         Integer(5, 10, name='layers'),\n",
    "#          Integer(5, 10, name='max_iter')]\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(60,60),(60,)],\n",
    "    'activation': ['relu'],\n",
    "    'solver': [ 'adam','lbfgs'],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "    'max_iter': [1000,10000,20000.30000,40000],\n",
    "}\n",
    "#'max_iter': [1000,10000,20000,30000,40000,80000],\n",
    "clf = GridSearchCV(mlp, parameter_space, n_jobs=4, cv=3)\n",
    "clf.fit(X_train, y_train.ravel())\n",
    "\n",
    "print('Best parameters found:\\n', clf.best_params_)\n",
    "\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 30207.08100910\n",
      "Iteration 2, loss = 1900.64036156\n",
      "Iteration 3, loss = 6743.55290865\n",
      "Iteration 4, loss = 1420.98478638\n",
      "Iteration 5, loss = 753.45367212\n",
      "Iteration 6, loss = 1405.27295004\n",
      "Iteration 7, loss = 208.76908834\n",
      "Iteration 8, loss = 311.63422655\n",
      "Iteration 9, loss = 292.50650833\n",
      "Iteration 10, loss = 55.44673828\n",
      "Iteration 11, loss = 133.73154554\n",
      "Iteration 12, loss = 54.04791461\n",
      "Iteration 13, loss = 59.75973408\n",
      "Iteration 14, loss = 50.05975877\n",
      "Iteration 15, loss = 38.81032606\n",
      "Iteration 16, loss = 41.91017085\n",
      "Iteration 17, loss = 34.02273224\n",
      "Iteration 18, loss = 33.94685129\n",
      "Iteration 19, loss = 33.53950694\n",
      "Iteration 20, loss = 33.55507452\n",
      "Iteration 21, loss = 32.86486816\n",
      "Iteration 22, loss = 32.49403581\n",
      "Iteration 23, loss = 32.51223355\n",
      "Iteration 24, loss = 32.08859701\n",
      "Iteration 25, loss = 31.90985548\n",
      "Iteration 26, loss = 31.71058148\n",
      "Iteration 27, loss = 31.50326087\n",
      "Iteration 28, loss = 31.13961925"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 29, loss = 31.40687729\n",
      "Iteration 30, loss = 30.78985595\n",
      "Iteration 31, loss = 30.64723635\n",
      "Iteration 32, loss = 30.38610419\n",
      "Iteration 33, loss = 29.94253335\n",
      "Iteration 34, loss = 29.93201515\n",
      "Iteration 35, loss = 29.22813065\n",
      "Iteration 36, loss = 28.91345546\n",
      "Iteration 37, loss = 28.73526453\n",
      "Iteration 38, loss = 29.09263876\n",
      "Iteration 39, loss = 28.29294134\n",
      "Iteration 40, loss = 27.75114326\n",
      "Iteration 41, loss = 27.58417303\n",
      "Iteration 42, loss = 27.17137824\n",
      "Iteration 43, loss = 27.12000713\n",
      "Iteration 44, loss = 27.20428475\n",
      "Iteration 45, loss = 26.49505578\n",
      "Iteration 46, loss = 26.67424974\n",
      "Iteration 47, loss = 27.43621167\n",
      "Iteration 48, loss = 26.19322843\n",
      "Iteration 49, loss = 25.58329733\n",
      "Iteration 50, loss = 25.72867951\n",
      "Iteration 51, loss = 26.40258582\n",
      "Iteration 52, loss = 25.14134787\n",
      "Iteration 53, loss = 24.98478740\n",
      "Iteration 54, loss = 24.60656206\n",
      "Iteration 55, loss = 24.34190240\n",
      "Iteration 56, loss = 24.10628808\n",
      "Iteration 57, loss = 24.15722670\n",
      "Iteration 58, loss = 23.83117353\n",
      "Iteration 59, loss = 23.90896426\n",
      "Iteration 60, loss = 23.43676623\n",
      "Iteration 61, loss = 23.46635642\n",
      "Iteration 62, loss = 23.56140146\n",
      "Iteration 63, loss = 23.19158668\n",
      "Iteration 64, loss = 23.26386082\n",
      "Iteration 65, loss = 23.46896852\n",
      "Iteration 66, loss = 23.06477045\n",
      "Iteration 67, loss = 22.89128058\n",
      "Iteration 68, loss = 22.83306168\n",
      "Iteration 69, loss = 23.43677228\n",
      "Iteration 70, loss = 23.47948332\n",
      "Iteration 71, loss = 23.84936757\n",
      "Iteration 72, loss = 23.70083371\n",
      "Iteration 73, loss = 23.68769763\n",
      "Iteration 74, loss = 22.27334763\n",
      "Iteration 75, loss = 21.91247515\n",
      "Iteration 76, loss = 22.38486301\n",
      "Iteration 77, loss = 21.71731117\n",
      "Iteration 78, loss = 21.53886625\n",
      "Iteration 79, loss = 21.33255846\n",
      "Iteration 80, loss = 21.08102219\n",
      "Iteration 81, loss = 21.07616654\n",
      "Iteration 82, loss = 20.84923772\n",
      "Iteration 83, loss = 20.96165777\n",
      "Iteration 84, loss = 20.44830915\n",
      "Iteration 85, loss = 20.79821485\n",
      "Iteration 86, loss = 20.36942380\n",
      "Iteration 87, loss = 21.15744809\n",
      "Iteration 88, loss = 20.31896417\n",
      "Iteration 89, loss = 20.07835691\n",
      "Iteration 90, loss = 19.94903656\n",
      "Iteration 91, loss = 20.53890918\n",
      "Iteration 92, loss = 20.01396603\n",
      "Iteration 93, loss = 20.11034144\n",
      "Iteration 94, loss = 20.26461123\n",
      "Iteration 95, loss = 19.79644325\n",
      "Iteration 96, loss = 19.46576642\n",
      "Iteration 97, loss = 19.39697691\n",
      "Iteration 98, loss = 20.84689471\n",
      "Iteration 99, loss = 19.62866500\n",
      "Iteration 100, loss = 19.53061666\n",
      "Iteration 101, loss = 19.74993295\n",
      "Iteration 102, loss = 19.60023230\n",
      "Iteration 103, loss = 20.40321805\n",
      "Iteration 104, loss = 19.13836373\n",
      "Iteration 105, loss = 19.20178156\n",
      "Iteration 106, loss = 19.54651943\n",
      "Iteration 107, loss = 18.22539901\n",
      "Iteration 108, loss = 19.23454792\n",
      "Iteration 109, loss = 18.28421663\n",
      "Iteration 110, loss = 18.25119969\n",
      "Iteration 111, loss = 18.10674048\n",
      "Iteration 112, loss = 18.08508742\n",
      "Iteration 113, loss = 18.04412878\n",
      "Iteration 114, loss = 17.76997462\n",
      "Iteration 115, loss = 17.94453131\n",
      "Iteration 116, loss = 18.32143482\n",
      "Iteration 117, loss = 17.41806572\n",
      "Iteration 118, loss = 17.59314850\n",
      "Iteration 119, loss = 17.81678113\n",
      "Iteration 120, loss = 17.95665105\n",
      "Iteration 121, loss = 17.33368103\n",
      "Iteration 122, loss = 17.23314897\n",
      "Iteration 123, loss = 17.23168458\n",
      "Iteration 124, loss = 17.51517851\n",
      "Iteration 125, loss = 17.11421315\n",
      "Iteration 126, loss = 16.93299741\n",
      "Iteration 127, loss = 16.95719356\n",
      "Iteration 128, loss = 16.80890029\n",
      "Iteration 129, loss = 16.74436984\n",
      "Iteration 130, loss = 16.87054222\n",
      "Iteration 131, loss = 16.56567496\n",
      "Iteration 132, loss = 16.76953603\n",
      "Iteration 133, loss = 18.12726282\n",
      "Iteration 134, loss = 16.90680558\n",
      "Iteration 135, loss = 16.60546826\n",
      "Iteration 136, loss = 16.89887905\n",
      "Iteration 137, loss = 16.58722867\n",
      "Iteration 138, loss = 16.38959439\n",
      "Iteration 139, loss = 15.97902527\n",
      "Iteration 140, loss = 16.08267458\n",
      "Iteration 141, loss = 16.10220245\n",
      "Iteration 142, loss = 16.19472756\n",
      "Iteration 143, loss = 15.95424585\n",
      "Iteration 144, loss = 16.11073783\n",
      "Iteration 145, loss = 16.22709517\n",
      "Iteration 146, loss = 15.71082276\n",
      "Iteration 147, loss = 15.57095318\n",
      "Iteration 148, loss = 16.55557353\n",
      "Iteration 149, loss = 17.22337370\n",
      "Iteration 150, loss = 17.19559500\n",
      "Iteration 151, loss = 19.05283624\n",
      "Iteration 152, loss = 16.94309987\n",
      "Iteration 153, loss = 16.00267828\n",
      "Iteration 154, loss = 15.94053116\n",
      "Iteration 155, loss = 15.10507494\n",
      "Iteration 156, loss = 15.30415590\n",
      "Iteration 157, loss = 17.32519218\n",
      "Iteration 158, loss = 17.48822864\n",
      "Iteration 159, loss = 16.42524755\n",
      "Iteration 160, loss = 15.49132493\n",
      "Iteration 161, loss = 15.05908724\n",
      "Iteration 162, loss = 15.16997365\n",
      "Iteration 163, loss = 14.88946419\n",
      "Iteration 164, loss = 15.83052199\n",
      "Iteration 165, loss = 14.93537828\n",
      "Iteration 166, loss = 14.79320044\n",
      "Iteration 167, loss = 14.71814038\n",
      "Iteration 168, loss = 14.90482110\n",
      "Iteration 169, loss = 16.56701033\n",
      "Iteration 170, loss = 15.63306736\n",
      "Iteration 171, loss = 15.78925698\n",
      "Iteration 172, loss = 15.80144220\n",
      "Iteration 173, loss = 14.91651233\n",
      "Iteration 174, loss = 14.70476736\n",
      "Iteration 175, loss = 14.90314445\n",
      "Iteration 176, loss = 15.81420704\n",
      "Iteration 177, loss = 14.62307022\n",
      "Iteration 178, loss = 14.54683669\n",
      "Iteration 179, loss = 14.89995554\n",
      "Iteration 180, loss = 14.93923905\n",
      "Iteration 181, loss = 14.95273416\n",
      "Iteration 182, loss = 14.41402142\n",
      "Iteration 183, loss = 14.03715682\n",
      "Iteration 184, loss = 13.87552700\n",
      "Iteration 185, loss = 14.24281541\n",
      "Iteration 186, loss = 14.07703804\n",
      "Iteration 187, loss = 14.06143441\n",
      "Iteration 188, loss = 14.54320060\n",
      "Iteration 189, loss = 14.29153682\n",
      "Iteration 190, loss = 14.82111410\n",
      "Iteration 191, loss = 14.09194561\n",
      "Iteration 192, loss = 13.62554668\n",
      "Iteration 193, loss = 13.81441302\n",
      "Iteration 194, loss = 13.87829875\n",
      "Iteration 195, loss = 13.62562320\n",
      "Iteration 196, loss = 13.56836296\n",
      "Iteration 197, loss = 13.85114439\n",
      "Iteration 198, loss = 13.59742715\n",
      "Iteration 199, loss = 14.21522452\n",
      "Iteration 200, loss = 14.73927207\n",
      "Iteration 201, loss = 13.83629927\n",
      "Iteration 202, loss = 13.70108341\n",
      "Iteration 203, loss = 14.23209981\n",
      "Iteration 204, loss = 13.65177001\n",
      "Iteration 205, loss = 13.35227355\n",
      "Iteration 206, loss = 13.33193160\n",
      "Iteration 207, loss = 13.23287569\n",
      "Iteration 208, loss = 13.72234530\n",
      "Iteration 209, loss = 13.55545412\n",
      "Iteration 210, loss = 13.52581752\n",
      "Iteration 211, loss = 13.56792486\n",
      "Iteration 212, loss = 13.51120363\n",
      "Iteration 213, loss = 13.11516198\n",
      "Iteration 214, loss = 13.16691755\n",
      "Iteration 215, loss = 13.08134623\n",
      "Iteration 216, loss = 15.22774417\n",
      "Iteration 217, loss = 16.52566819\n",
      "Iteration 218, loss = 16.42267586\n",
      "Iteration 219, loss = 14.66249107\n",
      "Iteration 220, loss = 15.06324746\n",
      "Iteration 221, loss = 13.88505908\n",
      "Iteration 222, loss = 13.05938794\n",
      "Iteration 223, loss = 14.01128995\n",
      "Iteration 224, loss = 14.02189589\n",
      "Iteration 225, loss = 15.10769067\n",
      "Iteration 226, loss = 15.59087253\n",
      "Iteration 227, loss = 13.28961038\n",
      "Iteration 228, loss = 13.67461796\n",
      "Iteration 229, loss = 13.84412731\n",
      "Iteration 230, loss = 14.31952959\n",
      "Iteration 231, loss = 13.05173209\n",
      "Iteration 232, loss = 12.73975529\n",
      "Iteration 233, loss = 13.50807172\n",
      "Iteration 234, loss = 12.97500920\n",
      "Iteration 235, loss = 12.89405045\n",
      "Iteration 236, loss = 12.66004466\n",
      "Iteration 237, loss = 13.05451009\n",
      "Iteration 238, loss = 13.21898803\n",
      "Iteration 239, loss = 12.70374498\n",
      "Iteration 240, loss = 12.41823654\n",
      "Iteration 241, loss = 12.48504579\n",
      "Iteration 242, loss = 12.23388410\n",
      "Iteration 243, loss = 13.06942292\n",
      "Iteration 244, loss = 13.28567018\n",
      "Iteration 245, loss = 13.90680222\n",
      "Iteration 246, loss = 12.88574487\n",
      "Iteration 247, loss = 12.24037514\n",
      "Iteration 248, loss = 12.17742999\n",
      "Iteration 249, loss = 12.74853394\n",
      "Iteration 250, loss = 13.53414340\n",
      "Iteration 251, loss = 13.17470297\n",
      "Iteration 252, loss = 13.39114740\n",
      "Iteration 253, loss = 12.63493405\n",
      "Iteration 254, loss = 15.17530057\n",
      "Iteration 255, loss = 13.90070045\n",
      "Iteration 256, loss = 13.53399283\n",
      "Iteration 257, loss = 12.70153311\n",
      "Iteration 258, loss = 13.61233947\n",
      "Iteration 259, loss = 11.97712143\n",
      "Iteration 260, loss = 12.39757272\n",
      "Iteration 261, loss = 12.67448556\n",
      "Iteration 262, loss = 12.85929286\n",
      "Iteration 263, loss = 12.39154016\n",
      "Iteration 264, loss = 12.21569749\n",
      "Iteration 265, loss = 13.11859169\n",
      "Iteration 266, loss = 12.29660060\n",
      "Iteration 267, loss = 12.10352027\n",
      "Iteration 268, loss = 12.61682575\n",
      "Iteration 269, loss = 11.82374650\n",
      "Iteration 270, loss = 11.70254710\n",
      "Iteration 271, loss = 12.55973370\n",
      "Iteration 272, loss = 13.37206453\n",
      "Iteration 273, loss = 12.92503878\n",
      "Iteration 274, loss = 12.19416788\n",
      "Iteration 275, loss = 11.76675318\n",
      "Iteration 276, loss = 11.89310537\n",
      "Iteration 277, loss = 11.82680722\n",
      "Iteration 278, loss = 11.90558852\n",
      "Iteration 279, loss = 12.32998981\n",
      "Iteration 280, loss = 13.49182470\n",
      "Iteration 281, loss = 13.52167381\n",
      "Iteration 282, loss = 11.74607557\n",
      "Iteration 283, loss = 12.10837480\n",
      "Iteration 284, loss = 11.40678993\n",
      "Iteration 285, loss = 12.04157540\n",
      "Iteration 286, loss = 14.79607777\n",
      "Iteration 287, loss = 13.83126958\n",
      "Iteration 288, loss = 15.40059036\n",
      "Iteration 289, loss = 12.92873232\n",
      "Iteration 290, loss = 12.05631061\n",
      "Iteration 291, loss = 11.41148901\n",
      "Iteration 292, loss = 11.54798421\n",
      "Iteration 293, loss = 11.80794369\n",
      "Iteration 294, loss = 14.11418567\n",
      "Iteration 295, loss = 14.34091220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 296, loss = 12.98214806\n",
      "Iteration 297, loss = 12.34533908\n",
      "Iteration 298, loss = 11.29095055\n",
      "Iteration 299, loss = 11.52873086\n",
      "Iteration 300, loss = 11.93212136\n",
      "Iteration 301, loss = 11.40248901\n",
      "Iteration 302, loss = 11.17664951\n",
      "Iteration 303, loss = 11.41935482\n",
      "Iteration 304, loss = 13.39191256\n",
      "Iteration 305, loss = 12.51657895\n",
      "Iteration 306, loss = 11.09902920\n",
      "Iteration 307, loss = 11.82421567\n",
      "Iteration 308, loss = 13.18005344\n",
      "Iteration 309, loss = 11.81734434\n",
      "Iteration 310, loss = 11.37793612\n",
      "Iteration 311, loss = 11.97039195\n",
      "Iteration 312, loss = 11.31541858\n",
      "Iteration 313, loss = 11.72107019\n",
      "Iteration 314, loss = 10.85797173\n",
      "Iteration 315, loss = 11.04906080\n",
      "Iteration 316, loss = 11.95612536\n",
      "Iteration 317, loss = 12.24899337\n",
      "Iteration 318, loss = 11.60934211\n",
      "Iteration 319, loss = 11.56505525\n",
      "Iteration 320, loss = 10.86374574\n",
      "Iteration 321, loss = 11.42303781\n",
      "Iteration 322, loss = 11.19085820\n",
      "Iteration 323, loss = 12.17265575\n",
      "Iteration 324, loss = 11.33669980\n",
      "Iteration 325, loss = 10.93837785\n",
      "Iteration 326, loss = 11.02488322\n",
      "Iteration 327, loss = 11.29557776\n",
      "Iteration 328, loss = 10.82917580\n",
      "Iteration 329, loss = 10.66285608\n",
      "Iteration 330, loss = 10.69392006\n",
      "Iteration 331, loss = 11.24263106\n",
      "Iteration 332, loss = 11.03495018\n",
      "Iteration 333, loss = 11.21967895\n",
      "Iteration 334, loss = 11.83850956\n",
      "Iteration 335, loss = 11.94987382\n",
      "Iteration 336, loss = 11.90064905\n",
      "Iteration 337, loss = 10.81815344\n",
      "Iteration 338, loss = 10.73301012\n",
      "Iteration 339, loss = 10.75942878\n",
      "Iteration 340, loss = 12.66486797\n",
      "Iteration 341, loss = 13.34829695\n",
      "Iteration 342, loss = 13.44387108\n",
      "Iteration 343, loss = 11.94923108\n",
      "Iteration 344, loss = 13.44620159\n",
      "Iteration 345, loss = 12.05249483\n",
      "Iteration 346, loss = 11.89241344\n",
      "Iteration 347, loss = 11.33344745\n",
      "Iteration 348, loss = 12.57983978\n",
      "Iteration 349, loss = 11.30004043\n",
      "Iteration 350, loss = 10.87342556\n",
      "Iteration 351, loss = 10.67667333\n",
      "Iteration 352, loss = 10.86368344\n",
      "Iteration 353, loss = 10.85624103\n",
      "Iteration 354, loss = 11.07472612\n",
      "Iteration 355, loss = 13.42557451\n",
      "Iteration 356, loss = 12.22197799\n",
      "Iteration 357, loss = 13.89091579\n",
      "Iteration 358, loss = 11.67967329\n",
      "Iteration 359, loss = 11.67506280\n",
      "Iteration 360, loss = 10.71778527\n",
      "Iteration 361, loss = 10.38365975\n",
      "Iteration 362, loss = 10.55514803\n",
      "Iteration 363, loss = 11.01616483\n",
      "Iteration 364, loss = 11.32448356\n",
      "Iteration 365, loss = 10.80727968\n",
      "Iteration 366, loss = 10.42768860\n",
      "Iteration 367, loss = 10.46237383\n",
      "Iteration 368, loss = 10.51066580\n",
      "Iteration 369, loss = 10.62165678\n",
      "Iteration 370, loss = 10.76932309\n",
      "Iteration 371, loss = 10.51726055\n",
      "Iteration 372, loss = 10.32865479\n",
      "Iteration 373, loss = 10.61174453\n",
      "Iteration 374, loss = 11.01155487\n",
      "Iteration 375, loss = 10.25762619\n",
      "Iteration 376, loss = 10.38666145\n",
      "Iteration 377, loss = 10.14586774\n",
      "Iteration 378, loss = 10.03699576\n",
      "Iteration 379, loss = 10.28886768\n",
      "Iteration 380, loss = 10.95105759\n",
      "Iteration 381, loss = 11.05987472\n",
      "Iteration 382, loss = 12.30465908\n",
      "Iteration 383, loss = 10.59296521\n",
      "Iteration 384, loss = 11.21706330\n",
      "Iteration 385, loss = 11.32835291\n",
      "Iteration 386, loss = 10.72697917\n",
      "Iteration 387, loss = 11.26590719\n",
      "Iteration 388, loss = 10.74808051\n",
      "Iteration 389, loss = 11.15144608\n",
      "Iteration 390, loss = 10.31902230\n",
      "Iteration 391, loss = 10.55898459\n",
      "Iteration 392, loss = 11.04505723\n",
      "Iteration 393, loss = 10.62636852\n",
      "Iteration 394, loss = 10.97056761\n",
      "Iteration 395, loss = 10.66940671\n",
      "Iteration 396, loss = 13.67802099\n",
      "Iteration 397, loss = 12.91482179\n",
      "Iteration 398, loss = 12.62361421\n",
      "Iteration 399, loss = 12.36641399\n",
      "Iteration 400, loss = 11.68911743\n",
      "Iteration 401, loss = 11.13274689\n",
      "Iteration 402, loss = 11.48783105\n",
      "Iteration 403, loss = 15.38673804\n",
      "Iteration 404, loss = 15.68268788\n",
      "Iteration 405, loss = 16.00598598\n",
      "Iteration 406, loss = 12.54921144\n",
      "Iteration 407, loss = 17.16191617\n",
      "Iteration 408, loss = 12.06554027\n",
      "Iteration 409, loss = 12.78291331\n",
      "Iteration 410, loss = 13.01601248\n",
      "Iteration 411, loss = 11.16851064\n",
      "Iteration 412, loss = 11.96893147\n",
      "Iteration 413, loss = 11.49791482\n",
      "Iteration 414, loss = 10.52922908\n",
      "Iteration 415, loss = 10.11961744\n",
      "Iteration 416, loss = 10.64679936\n",
      "Iteration 417, loss = 11.60895195\n",
      "Iteration 418, loss = 9.69909877\n",
      "Iteration 419, loss = 10.51121650\n",
      "Iteration 420, loss = 10.04624995\n",
      "Iteration 421, loss = 9.68926446\n",
      "Iteration 422, loss = 10.08165233\n",
      "Iteration 423, loss = 10.14944305\n",
      "Iteration 424, loss = 10.46218112\n",
      "Iteration 425, loss = 9.92569000\n",
      "Iteration 426, loss = 9.68060687\n",
      "Iteration 427, loss = 9.83053856\n",
      "Iteration 428, loss = 10.39831099\n",
      "Iteration 429, loss = 9.61190947\n",
      "Iteration 430, loss = 9.89604763\n",
      "Iteration 431, loss = 10.45051404\n",
      "Iteration 432, loss = 11.08513084\n",
      "Iteration 433, loss = 10.62407308\n",
      "Iteration 434, loss = 10.07374460\n",
      "Iteration 435, loss = 9.78893145\n",
      "Iteration 436, loss = 10.49861130\n",
      "Iteration 437, loss = 10.27528767\n",
      "Iteration 438, loss = 10.17472538\n",
      "Iteration 439, loss = 9.85227927\n",
      "Iteration 440, loss = 9.91023747\n",
      "Iteration 441, loss = 9.89768182\n",
      "Iteration 442, loss = 10.05992673\n",
      "Iteration 443, loss = 12.32018899\n",
      "Iteration 444, loss = 11.14432916\n",
      "Iteration 445, loss = 11.70490016\n",
      "Iteration 446, loss = 10.13487575\n",
      "Iteration 447, loss = 10.07466239\n",
      "Iteration 448, loss = 9.67109584\n",
      "Iteration 449, loss = 9.40629829\n",
      "Iteration 450, loss = 9.49907319\n",
      "Iteration 451, loss = 9.73616238\n",
      "Iteration 452, loss = 9.52580205\n",
      "Iteration 453, loss = 9.88492123\n",
      "Iteration 454, loss = 9.74986219\n",
      "Iteration 455, loss = 10.23929224\n",
      "Iteration 456, loss = 10.87748661\n",
      "Iteration 457, loss = 9.68512582\n",
      "Iteration 458, loss = 10.14818277\n",
      "Iteration 459, loss = 10.32074682\n",
      "Iteration 460, loss = 9.99079362\n",
      "Iteration 461, loss = 10.15011682\n",
      "Iteration 462, loss = 9.61516249\n",
      "Iteration 463, loss = 10.09945758\n",
      "Iteration 464, loss = 9.55254607\n",
      "Iteration 465, loss = 10.47841239\n",
      "Iteration 466, loss = 11.95222237\n",
      "Iteration 467, loss = 11.20262694\n",
      "Iteration 468, loss = 10.78327249\n",
      "Iteration 469, loss = 10.58739152\n",
      "Iteration 470, loss = 9.44778202\n",
      "Iteration 471, loss = 9.63468027\n",
      "Iteration 472, loss = 9.93688615\n",
      "Iteration 473, loss = 9.78135221\n",
      "Iteration 474, loss = 9.79553728\n",
      "Iteration 475, loss = 11.17250881\n",
      "Iteration 476, loss = 10.60040771\n",
      "Iteration 477, loss = 9.76506650\n",
      "Iteration 478, loss = 11.31437356\n",
      "Iteration 479, loss = 10.39771618\n",
      "Iteration 480, loss = 9.97734818\n",
      "Iteration 481, loss = 10.15667099\n",
      "Iteration 482, loss = 10.20180946\n",
      "Iteration 483, loss = 9.68098616\n",
      "Iteration 484, loss = 10.64825613\n",
      "Iteration 485, loss = 12.07617957\n",
      "Iteration 486, loss = 11.86334475\n",
      "Iteration 487, loss = 12.46861214\n",
      "Iteration 488, loss = 13.13765851\n",
      "Iteration 489, loss = 11.06515192\n",
      "Iteration 490, loss = 10.07641211\n",
      "Iteration 491, loss = 9.38976113\n",
      "Iteration 492, loss = 9.32160767\n",
      "Iteration 493, loss = 11.43097935\n",
      "Iteration 494, loss = 10.39668545\n",
      "Iteration 495, loss = 13.17224429\n",
      "Iteration 496, loss = 12.60752435\n",
      "Iteration 497, loss = 10.67269984\n",
      "Iteration 498, loss = 10.08662937\n",
      "Iteration 499, loss = 10.77411244\n",
      "Iteration 500, loss = 12.40549086\n",
      "Iteration 501, loss = 11.35878735\n",
      "Iteration 502, loss = 9.88525014\n",
      "Iteration 503, loss = 10.33270200\n",
      "Iteration 504, loss = 11.02068661\n",
      "Iteration 505, loss = 10.26665148\n",
      "Iteration 506, loss = 9.72145499\n",
      "Iteration 507, loss = 9.94464208\n",
      "Iteration 508, loss = 12.18335405\n",
      "Iteration 509, loss = 12.17243900\n",
      "Iteration 510, loss = 9.97808116\n",
      "Iteration 511, loss = 10.11961128\n",
      "Iteration 512, loss = 10.50314557\n",
      "Iteration 513, loss = 10.08851286\n",
      "Iteration 514, loss = 10.48027122\n",
      "Iteration 515, loss = 10.77432184\n",
      "Iteration 516, loss = 8.89606619\n",
      "Iteration 517, loss = 9.55976040\n",
      "Iteration 518, loss = 9.28128886\n",
      "Iteration 519, loss = 9.66001921\n",
      "Iteration 520, loss = 10.32206940\n",
      "Iteration 521, loss = 9.32133273\n",
      "Iteration 522, loss = 9.28325687\n",
      "Iteration 523, loss = 9.36746070\n",
      "Iteration 524, loss = 9.07686637\n",
      "Iteration 525, loss = 9.11844171\n",
      "Iteration 526, loss = 9.05984137\n",
      "Iteration 527, loss = 9.57114719\n",
      "Iteration 528, loss = 9.61699887\n",
      "Iteration 529, loss = 11.29231134\n",
      "Iteration 530, loss = 11.52531515\n",
      "Iteration 531, loss = 10.09853026\n",
      "Iteration 532, loss = 10.38340656\n",
      "Iteration 533, loss = 9.51062476\n",
      "Iteration 534, loss = 9.36935275\n",
      "Iteration 535, loss = 8.84515596\n",
      "Iteration 536, loss = 9.03526877\n",
      "Iteration 537, loss = 9.07003451\n",
      "Iteration 538, loss = 9.73257343\n",
      "Iteration 539, loss = 9.96384951\n",
      "Iteration 540, loss = 11.11301733\n",
      "Iteration 541, loss = 10.80277796\n",
      "Iteration 542, loss = 10.19071272\n",
      "Iteration 543, loss = 9.42196126\n",
      "Iteration 544, loss = 9.24285804\n",
      "Iteration 545, loss = 9.99783235\n",
      "Iteration 546, loss = 12.39463896\n",
      "Iteration 547, loss = 11.35461536\n",
      "Iteration 548, loss = 10.28549763\n",
      "Iteration 549, loss = 9.02943589\n",
      "Iteration 550, loss = 9.81160760\n",
      "Iteration 551, loss = 8.81240825\n",
      "Iteration 552, loss = 10.33542434\n",
      "Iteration 553, loss = 10.20130370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 554, loss = 9.80782307\n",
      "Iteration 555, loss = 9.25696490\n",
      "Iteration 556, loss = 9.20616477\n",
      "Iteration 557, loss = 10.57118999\n",
      "Iteration 558, loss = 10.49124604\n",
      "Iteration 559, loss = 11.46666179\n",
      "Iteration 560, loss = 10.04593990\n",
      "Iteration 561, loss = 11.47845230\n",
      "Iteration 562, loss = 15.16757149\n",
      "Iteration 563, loss = 17.59827854\n",
      "Iteration 564, loss = 11.77370448\n",
      "Iteration 565, loss = 11.06305067\n",
      "Iteration 566, loss = 10.09091798\n",
      "Iteration 567, loss = 11.93735809\n",
      "Iteration 568, loss = 9.69954763\n",
      "Iteration 569, loss = 10.63697804\n",
      "Iteration 570, loss = 9.82953895\n",
      "Iteration 571, loss = 11.08831485\n",
      "Iteration 572, loss = 10.13603789\n",
      "Iteration 573, loss = 12.41410646\n",
      "Iteration 574, loss = 9.58719743\n",
      "Iteration 575, loss = 9.90040825\n",
      "Iteration 576, loss = 9.22168762\n",
      "Iteration 577, loss = 9.08679649\n",
      "Iteration 578, loss = 11.52968716\n",
      "Iteration 579, loss = 12.12099202\n",
      "Iteration 580, loss = 10.53514266\n",
      "Iteration 581, loss = 10.14949306\n",
      "Iteration 582, loss = 10.33958169\n",
      "Iteration 583, loss = 9.63881012\n",
      "Iteration 584, loss = 9.25912452\n",
      "Iteration 585, loss = 9.36247797\n",
      "Iteration 586, loss = 9.43311479\n",
      "Iteration 587, loss = 10.45263014\n",
      "Iteration 588, loss = 10.86817587\n",
      "Iteration 589, loss = 10.11271792\n",
      "Iteration 590, loss = 10.96032626\n",
      "Iteration 591, loss = 12.78457629\n",
      "Iteration 592, loss = 14.09280762\n",
      "Iteration 593, loss = 12.44999446\n",
      "Iteration 594, loss = 9.68231731\n",
      "Iteration 595, loss = 9.69070512\n",
      "Iteration 596, loss = 9.34232771\n",
      "Iteration 597, loss = 9.44553065\n",
      "Iteration 598, loss = 8.83918686\n",
      "Iteration 599, loss = 10.54048453\n",
      "Iteration 600, loss = 11.97997473\n",
      "Iteration 601, loss = 14.94294228\n",
      "Iteration 602, loss = 13.80764259\n",
      "Iteration 603, loss = 12.16116339\n",
      "Iteration 604, loss = 11.29705657\n",
      "Iteration 605, loss = 10.27317369\n",
      "Iteration 606, loss = 9.39932858\n",
      "Iteration 607, loss = 9.17069562\n",
      "Iteration 608, loss = 9.33600342\n",
      "Iteration 609, loss = 10.15980728\n",
      "Iteration 610, loss = 10.16440675\n",
      "Iteration 611, loss = 10.60230305\n",
      "Iteration 612, loss = 9.09674071\n",
      "Iteration 613, loss = 9.15227686\n",
      "Iteration 614, loss = 9.13910276\n",
      "Iteration 615, loss = 10.84178538\n",
      "Iteration 616, loss = 9.93374462\n",
      "Iteration 617, loss = 12.65134985\n",
      "Iteration 618, loss = 11.71171979\n",
      "Iteration 619, loss = 9.65327035\n",
      "Iteration 620, loss = 8.65329710\n",
      "Iteration 621, loss = 10.50487965\n",
      "Iteration 622, loss = 9.62973387\n",
      "Iteration 623, loss = 9.62523731\n",
      "Iteration 624, loss = 9.27597002\n",
      "Iteration 625, loss = 9.62451991\n",
      "Iteration 626, loss = 8.56946844\n",
      "Iteration 627, loss = 9.92262071\n",
      "Iteration 628, loss = 10.29725531\n",
      "Iteration 629, loss = 8.83388329\n",
      "Iteration 630, loss = 8.96251452\n",
      "Iteration 631, loss = 10.14155913\n",
      "Iteration 632, loss = 11.43586742\n",
      "Iteration 633, loss = 10.91275687\n",
      "Iteration 634, loss = 10.89950037\n",
      "Iteration 635, loss = 8.86160193\n",
      "Iteration 636, loss = 9.36587756\n",
      "Iteration 637, loss = 8.65374096\n",
      "Iteration 638, loss = 10.18292058\n",
      "Iteration 639, loss = 9.34328110\n",
      "Iteration 640, loss = 9.63823878\n",
      "Iteration 641, loss = 10.13028750\n",
      "Iteration 642, loss = 10.12780129\n",
      "Iteration 643, loss = 10.28078249\n",
      "Iteration 644, loss = 12.11758633\n",
      "Iteration 645, loss = 10.96841378\n",
      "Iteration 646, loss = 10.03450688\n",
      "Iteration 647, loss = 11.75161530\n",
      "Iteration 648, loss = 14.89697481\n",
      "Iteration 649, loss = 10.38126040\n",
      "Iteration 650, loss = 9.11030357\n",
      "Iteration 651, loss = 10.42752662\n",
      "Iteration 652, loss = 8.90966579\n",
      "Iteration 653, loss = 8.56308477\n",
      "Iteration 654, loss = 8.84792495\n",
      "Iteration 655, loss = 9.08232748\n",
      "Iteration 656, loss = 8.91320403\n",
      "Iteration 657, loss = 8.70898075\n",
      "Iteration 658, loss = 9.27932192\n",
      "Iteration 659, loss = 9.58616102\n",
      "Iteration 660, loss = 8.78467557\n",
      "Iteration 661, loss = 10.29663762\n",
      "Iteration 662, loss = 8.89457006\n",
      "Iteration 663, loss = 8.34522940\n",
      "Iteration 664, loss = 8.40259061\n",
      "Iteration 665, loss = 9.13808955\n",
      "Iteration 666, loss = 10.05305172\n",
      "Iteration 667, loss = 8.49311523\n",
      "Iteration 668, loss = 8.81899302\n",
      "Iteration 669, loss = 8.48517253\n",
      "Iteration 670, loss = 9.76027714\n",
      "Iteration 671, loss = 10.04478504\n",
      "Iteration 672, loss = 8.37195262\n",
      "Iteration 673, loss = 8.61279666\n",
      "Iteration 674, loss = 12.11179000\n",
      "Iteration 675, loss = 13.77006256\n",
      "Iteration 676, loss = 8.64153268\n",
      "Iteration 677, loss = 8.78917662\n",
      "Iteration 678, loss = 9.44743306\n",
      "Iteration 679, loss = 9.90068325\n",
      "Iteration 680, loss = 15.58934943\n",
      "Iteration 681, loss = 14.43704652\n",
      "Iteration 682, loss = 17.78168251\n",
      "Iteration 683, loss = 13.31820131\n",
      "Iteration 684, loss = 10.26826566\n",
      "Iteration 685, loss = 11.96232368\n",
      "Iteration 686, loss = 13.14355475\n",
      "Iteration 687, loss = 11.87597880\n",
      "Iteration 688, loss = 17.22957910\n",
      "Iteration 689, loss = 14.11310648\n",
      "Iteration 690, loss = 13.97224038\n",
      "Iteration 691, loss = 18.06888799\n",
      "Iteration 692, loss = 15.52539109\n",
      "Iteration 693, loss = 15.25288612\n",
      "Iteration 694, loss = 10.58682065\n",
      "Iteration 695, loss = 10.55478391\n",
      "Iteration 696, loss = 10.77632056\n",
      "Iteration 697, loss = 9.02289382\n",
      "Iteration 698, loss = 9.00051399\n",
      "Iteration 699, loss = 9.76070042\n",
      "Iteration 700, loss = 8.79960064\n",
      "Iteration 701, loss = 9.37589099\n",
      "Iteration 702, loss = 8.54694562\n",
      "Iteration 703, loss = 8.50795345\n",
      "Iteration 704, loss = 9.08193590\n",
      "Iteration 705, loss = 8.27121431\n",
      "Iteration 706, loss = 8.99430369\n",
      "Iteration 707, loss = 12.94542110\n",
      "Iteration 708, loss = 10.44973088\n",
      "Iteration 709, loss = 11.06468146\n",
      "Iteration 710, loss = 9.81341332\n",
      "Iteration 711, loss = 9.40886147\n",
      "Iteration 712, loss = 9.42710627\n",
      "Iteration 713, loss = 9.40895146\n",
      "Iteration 714, loss = 8.42891094\n",
      "Iteration 715, loss = 9.13610505\n",
      "Iteration 716, loss = 10.69945487\n",
      "Iteration 717, loss = 13.32206712\n",
      "Iteration 718, loss = 12.59762803\n",
      "Iteration 719, loss = 11.50823032\n",
      "Iteration 720, loss = 9.51951215\n",
      "Iteration 721, loss = 10.34045873\n",
      "Iteration 722, loss = 9.71159676\n",
      "Iteration 723, loss = 11.26616767\n",
      "Iteration 724, loss = 9.05846079\n",
      "Iteration 725, loss = 8.79918186\n",
      "Iteration 726, loss = 8.41957895\n",
      "Iteration 727, loss = 10.48376781\n",
      "Iteration 728, loss = 10.32685961\n",
      "Iteration 729, loss = 10.00796847\n",
      "Iteration 730, loss = 10.04004455\n",
      "Iteration 731, loss = 9.66751143\n",
      "Iteration 732, loss = 10.82729410\n",
      "Iteration 733, loss = 9.90203567\n",
      "Iteration 734, loss = 8.55913460\n",
      "Iteration 735, loss = 8.22611583\n",
      "Iteration 736, loss = 10.56815752\n",
      "Iteration 737, loss = 8.67944507\n",
      "Iteration 738, loss = 9.91825447\n",
      "Iteration 739, loss = 9.80706156\n",
      "Iteration 740, loss = 11.40390738\n",
      "Iteration 741, loss = 12.67853164\n",
      "Iteration 742, loss = 9.36460186\n",
      "Iteration 743, loss = 8.75524710\n",
      "Iteration 744, loss = 8.09617317\n",
      "Iteration 745, loss = 8.97742328\n",
      "Iteration 746, loss = 8.48154082\n",
      "Iteration 747, loss = 8.53399274\n",
      "Iteration 748, loss = 8.39545089\n",
      "Iteration 749, loss = 8.14664180\n",
      "Iteration 750, loss = 8.41012576\n",
      "Iteration 751, loss = 8.80330393\n",
      "Iteration 752, loss = 8.37388339\n",
      "Iteration 753, loss = 9.44190421\n",
      "Iteration 754, loss = 11.18235615\n",
      "Iteration 755, loss = 11.40230130\n",
      "Iteration 756, loss = 11.20205524\n",
      "Iteration 757, loss = 16.94107806\n",
      "Iteration 758, loss = 12.25787598\n",
      "Iteration 759, loss = 10.27602408\n",
      "Iteration 760, loss = 10.19338936\n",
      "Iteration 761, loss = 11.30603608\n",
      "Iteration 762, loss = 9.78687104\n",
      "Iteration 763, loss = 8.97750075\n",
      "Iteration 764, loss = 10.72462434\n",
      "Iteration 765, loss = 8.38040438\n",
      "Iteration 766, loss = 8.54210648\n",
      "Iteration 767, loss = 8.77293422\n",
      "Iteration 768, loss = 8.06680700\n",
      "Iteration 769, loss = 9.03678802\n",
      "Iteration 770, loss = 8.31809147\n",
      "Iteration 771, loss = 8.12781270\n",
      "Iteration 772, loss = 9.06117045\n",
      "Iteration 773, loss = 11.28056122\n",
      "Iteration 774, loss = 10.37399957\n",
      "Iteration 775, loss = 9.11917108\n",
      "Iteration 776, loss = 8.08682897\n",
      "Iteration 777, loss = 9.81538382\n",
      "Iteration 778, loss = 9.00990986\n",
      "Iteration 779, loss = 9.73766573\n",
      "Iteration 780, loss = 8.60336665\n",
      "Iteration 781, loss = 8.93794709\n",
      "Iteration 782, loss = 8.15947101\n",
      "Iteration 783, loss = 7.93892974\n",
      "Iteration 784, loss = 8.70327946\n",
      "Iteration 785, loss = 8.52135834\n",
      "Iteration 786, loss = 9.26627881\n",
      "Iteration 787, loss = 8.90320892\n",
      "Iteration 788, loss = 9.14922376\n",
      "Iteration 789, loss = 10.46712266\n",
      "Iteration 790, loss = 8.50089674\n",
      "Iteration 791, loss = 8.10372227\n",
      "Iteration 792, loss = 8.31510317\n",
      "Iteration 793, loss = 9.19487223\n",
      "Iteration 794, loss = 8.71457258\n",
      "Iteration 795, loss = 10.17550542\n",
      "Iteration 796, loss = 8.93048497\n",
      "Iteration 797, loss = 9.76026680\n",
      "Iteration 798, loss = 11.19434224\n",
      "Iteration 799, loss = 9.23925982\n",
      "Iteration 800, loss = 8.63924482\n",
      "Iteration 801, loss = 8.08355142\n",
      "Iteration 802, loss = 8.52016738\n",
      "Iteration 803, loss = 10.59065299\n",
      "Iteration 804, loss = 10.16985501\n",
      "Iteration 805, loss = 10.10650225\n",
      "Iteration 806, loss = 7.94726017\n",
      "Iteration 807, loss = 8.72634746\n",
      "Iteration 808, loss = 8.59242564\n",
      "Iteration 809, loss = 8.36983397\n",
      "Iteration 810, loss = 8.00536467\n",
      "Iteration 811, loss = 9.43363501\n",
      "Iteration 812, loss = 8.71602496\n",
      "Iteration 813, loss = 8.67134279\n",
      "Iteration 814, loss = 9.61464070\n",
      "Iteration 815, loss = 8.81139549\n",
      "Iteration 816, loss = 8.82264554\n",
      "Iteration 817, loss = 9.19177864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 818, loss = 13.62496649\n",
      "Iteration 819, loss = 12.71361178\n",
      "Iteration 820, loss = 11.22461025\n",
      "Iteration 821, loss = 9.62863743\n",
      "Iteration 822, loss = 8.68792499\n",
      "Iteration 823, loss = 9.31113024\n",
      "Iteration 824, loss = 8.80326229\n",
      "Iteration 825, loss = 9.33504488\n",
      "Iteration 826, loss = 11.01033591\n",
      "Iteration 827, loss = 12.02434461\n",
      "Iteration 828, loss = 9.22201218\n",
      "Iteration 829, loss = 8.12027114\n",
      "Iteration 830, loss = 8.57869252\n",
      "Iteration 831, loss = 8.21635678\n",
      "Iteration 832, loss = 8.19428360\n",
      "Iteration 833, loss = 8.10833852\n",
      "Iteration 834, loss = 10.37534939\n",
      "Iteration 835, loss = 10.42487704\n",
      "Iteration 836, loss = 11.76938332\n",
      "Iteration 837, loss = 9.88850404\n",
      "Iteration 838, loss = 8.32509715\n",
      "Iteration 839, loss = 8.02982278\n",
      "Iteration 840, loss = 9.19653353\n",
      "Iteration 841, loss = 10.45194642\n",
      "Iteration 842, loss = 8.90502887\n",
      "Iteration 843, loss = 9.28928824\n",
      "Iteration 844, loss = 9.57510857\n",
      "Iteration 845, loss = 8.32866005\n",
      "Iteration 846, loss = 8.68697360\n",
      "Iteration 847, loss = 14.93200406\n",
      "Iteration 848, loss = 11.78867213\n",
      "Iteration 849, loss = 9.17955447\n",
      "Iteration 850, loss = 12.32738215\n",
      "Iteration 851, loss = 14.62467554\n",
      "Iteration 852, loss = 22.96528105\n",
      "Iteration 853, loss = 13.44813214\n",
      "Iteration 854, loss = 17.77086136\n",
      "Iteration 855, loss = 14.27906019\n",
      "Iteration 856, loss = 16.80951346\n",
      "Iteration 857, loss = 17.57997053\n",
      "Iteration 858, loss = 10.59749632\n",
      "Iteration 859, loss = 9.30996771\n",
      "Iteration 860, loss = 14.73999275\n",
      "Iteration 861, loss = 12.62739411\n",
      "Iteration 862, loss = 11.15733299\n",
      "Iteration 863, loss = 10.39014741\n",
      "Iteration 864, loss = 9.56530067\n",
      "Iteration 865, loss = 9.03525111\n",
      "Iteration 866, loss = 10.08648756\n",
      "Iteration 867, loss = 9.08658082\n",
      "Iteration 868, loss = 7.67970726\n",
      "Iteration 869, loss = 8.26728654\n",
      "Iteration 870, loss = 8.52579798\n",
      "Iteration 871, loss = 8.36021457\n",
      "Iteration 872, loss = 8.88176700\n",
      "Iteration 873, loss = 8.38640815\n",
      "Iteration 874, loss = 8.54000532\n",
      "Iteration 875, loss = 9.14504596\n",
      "Iteration 876, loss = 9.34851881\n",
      "Iteration 877, loss = 8.89164261\n",
      "Iteration 878, loss = 8.30157762\n",
      "Iteration 879, loss = 10.31093219\n",
      "Iteration 880, loss = 10.72215929\n",
      "Iteration 881, loss = 9.32208019\n",
      "Iteration 882, loss = 8.19785529\n",
      "Iteration 883, loss = 8.23804201\n",
      "Iteration 884, loss = 8.35358070\n",
      "Iteration 885, loss = 8.03838837\n",
      "Iteration 886, loss = 8.03302329\n",
      "Iteration 887, loss = 8.35674146\n",
      "Iteration 888, loss = 10.13131670\n",
      "Iteration 889, loss = 9.78067558\n",
      "Iteration 890, loss = 9.22458862\n",
      "Iteration 891, loss = 9.66500458\n",
      "Iteration 892, loss = 9.75371871\n",
      "Iteration 893, loss = 9.98199552\n",
      "Iteration 894, loss = 11.83336396\n",
      "Iteration 895, loss = 8.84860468\n",
      "Iteration 896, loss = 9.27367034\n",
      "Iteration 897, loss = 11.10231152\n",
      "Iteration 898, loss = 10.08446192\n",
      "Iteration 899, loss = 9.36625261\n",
      "Iteration 900, loss = 7.79979799\n",
      "Iteration 901, loss = 14.75864832\n",
      "Iteration 902, loss = 10.22134563\n",
      "Iteration 903, loss = 9.07789312\n",
      "Iteration 904, loss = 9.05029247\n",
      "Iteration 905, loss = 10.18601597\n",
      "Iteration 906, loss = 11.06066122\n",
      "Iteration 907, loss = 10.01850940\n",
      "Iteration 908, loss = 12.06519912\n",
      "Iteration 909, loss = 10.96005567\n",
      "Iteration 910, loss = 8.06393231\n",
      "Iteration 911, loss = 8.62392786\n",
      "Iteration 912, loss = 9.10861956\n",
      "Iteration 913, loss = 9.53591865\n",
      "Iteration 914, loss = 9.84802932\n",
      "Iteration 915, loss = 9.99717987\n",
      "Iteration 916, loss = 9.33785185\n",
      "Iteration 917, loss = 8.58415559\n",
      "Iteration 918, loss = 7.97986908\n",
      "Iteration 919, loss = 10.80896170\n",
      "Iteration 920, loss = 8.62352488\n",
      "Iteration 921, loss = 10.09565172\n",
      "Iteration 922, loss = 14.66576190\n",
      "Iteration 923, loss = 12.15109625\n",
      "Iteration 924, loss = 9.16155208\n",
      "Iteration 925, loss = 9.79429626\n",
      "Iteration 926, loss = 12.14676209\n",
      "Iteration 927, loss = 11.22041044\n",
      "Iteration 928, loss = 15.01142331\n",
      "Iteration 929, loss = 11.29456150\n",
      "Iteration 930, loss = 11.34591461\n",
      "Iteration 931, loss = 18.21274270\n",
      "Iteration 932, loss = 12.31959701\n",
      "Iteration 933, loss = 9.99908448\n",
      "Iteration 934, loss = 8.81546562\n",
      "Iteration 935, loss = 7.90152076\n",
      "Iteration 936, loss = 8.84946623\n",
      "Iteration 937, loss = 7.63541620\n",
      "Iteration 938, loss = 8.03326868\n",
      "Iteration 939, loss = 7.76072434\n",
      "Iteration 940, loss = 9.47274876\n",
      "Iteration 941, loss = 12.16212975\n",
      "Iteration 942, loss = 10.43758899\n",
      "Iteration 943, loss = 11.79357587\n",
      "Iteration 944, loss = 8.92392832\n",
      "Iteration 945, loss = 10.27214045\n",
      "Iteration 946, loss = 9.20834148\n",
      "Iteration 947, loss = 10.75860822\n",
      "Iteration 948, loss = 13.61230774\n",
      "Iteration 949, loss = 16.12132806\n",
      "Iteration 950, loss = 11.07612754\n",
      "Iteration 951, loss = 11.09106395\n",
      "Iteration 952, loss = 13.65880330\n",
      "Iteration 953, loss = 9.66941499\n",
      "Iteration 954, loss = 8.18125581\n",
      "Iteration 955, loss = 9.76789329\n",
      "Iteration 956, loss = 9.22353620\n",
      "Iteration 957, loss = 8.33293182\n",
      "Iteration 958, loss = 9.74780255\n",
      "Iteration 959, loss = 9.16177914\n",
      "Iteration 960, loss = 10.83555152\n",
      "Iteration 961, loss = 9.48538451\n",
      "Iteration 962, loss = 9.49162677\n",
      "Iteration 963, loss = 12.25667513\n",
      "Iteration 964, loss = 14.05822809\n",
      "Iteration 965, loss = 13.55084073\n",
      "Iteration 966, loss = 17.88201529\n",
      "Iteration 967, loss = 16.16096374\n",
      "Iteration 968, loss = 9.98773601\n",
      "Iteration 969, loss = 8.39416654\n",
      "Iteration 970, loss = 8.78331441\n",
      "Iteration 971, loss = 8.79219375\n",
      "Iteration 972, loss = 8.02208588\n",
      "Iteration 973, loss = 8.44183987\n",
      "Iteration 974, loss = 8.64641412\n",
      "Iteration 975, loss = 8.85382105\n",
      "Iteration 976, loss = 8.07526859\n",
      "Iteration 977, loss = 9.32994895\n",
      "Iteration 978, loss = 8.99088043\n",
      "Iteration 979, loss = 9.67821778\n",
      "Iteration 980, loss = 8.29042426\n",
      "Iteration 981, loss = 8.00439115\n",
      "Iteration 982, loss = 8.03411220\n",
      "Iteration 983, loss = 7.59493108\n",
      "Iteration 984, loss = 7.92327092\n",
      "Iteration 985, loss = 9.63478072\n",
      "Iteration 986, loss = 8.33345237\n",
      "Iteration 987, loss = 9.57651105\n",
      "Iteration 988, loss = 8.39493187\n",
      "Iteration 989, loss = 14.51526876\n",
      "Iteration 990, loss = 14.34013244\n",
      "Iteration 991, loss = 21.91706758\n",
      "Iteration 992, loss = 15.32303999\n",
      "Iteration 993, loss = 10.55971445\n",
      "Iteration 994, loss = 13.58517577\n",
      "Iteration 995, loss = 11.83720608\n",
      "Iteration 996, loss = 13.95530086\n",
      "Iteration 997, loss = 9.86210449\n",
      "Iteration 998, loss = 8.49725659\n",
      "Iteration 999, loss = 9.45395562\n",
      "Iteration 1000, loss = 10.78222072\n",
      "Iteration 1001, loss = 10.68234345\n",
      "Iteration 1002, loss = 9.09754543\n",
      "Iteration 1003, loss = 10.73295989\n",
      "Iteration 1004, loss = 10.78598625\n",
      "Iteration 1005, loss = 8.64883351\n",
      "Iteration 1006, loss = 14.07034510\n",
      "Iteration 1007, loss = 11.95546102\n",
      "Iteration 1008, loss = 10.29944371\n",
      "Iteration 1009, loss = 10.19059609\n",
      "Iteration 1010, loss = 9.98235044\n",
      "Iteration 1011, loss = 8.96476821\n",
      "Iteration 1012, loss = 9.61430337\n",
      "Iteration 1013, loss = 10.73451679\n",
      "Iteration 1014, loss = 8.75198259\n",
      "Iteration 1015, loss = 9.86449700\n",
      "Iteration 1016, loss = 9.31528887\n",
      "Iteration 1017, loss = 7.94817759\n",
      "Iteration 1018, loss = 8.29232526\n",
      "Iteration 1019, loss = 8.28260339\n",
      "Iteration 1020, loss = 9.13572272\n",
      "Iteration 1021, loss = 9.31334088\n",
      "Iteration 1022, loss = 12.23360783\n",
      "Iteration 1023, loss = 12.65330708\n",
      "Iteration 1024, loss = 11.86052032\n",
      "Iteration 1025, loss = 12.52471625\n",
      "Iteration 1026, loss = 11.18110856\n",
      "Iteration 1027, loss = 9.74849393\n",
      "Iteration 1028, loss = 8.78362855\n",
      "Iteration 1029, loss = 8.73268355\n",
      "Iteration 1030, loss = 9.38118813\n",
      "Iteration 1031, loss = 12.13310425\n",
      "Iteration 1032, loss = 16.97633055\n",
      "Iteration 1033, loss = 17.01483574\n",
      "Iteration 1034, loss = 14.48226533\n",
      "Iteration 1035, loss = 11.81660547\n",
      "Iteration 1036, loss = 10.36030033\n",
      "Iteration 1037, loss = 8.15054668\n",
      "Iteration 1038, loss = 17.89743763\n",
      "Iteration 1039, loss = 12.97040883\n",
      "Iteration 1040, loss = 14.71872395\n",
      "Iteration 1041, loss = 10.50690419\n",
      "Iteration 1042, loss = 12.82408794\n",
      "Iteration 1043, loss = 12.40418708\n",
      "Iteration 1044, loss = 8.31328208\n",
      "Iteration 1045, loss = 8.06758322\n",
      "Iteration 1046, loss = 8.81962613\n",
      "Iteration 1047, loss = 7.64011695\n",
      "Iteration 1048, loss = 8.33534322\n",
      "Iteration 1049, loss = 8.95739948\n",
      "Iteration 1050, loss = 8.36689534\n",
      "Iteration 1051, loss = 7.89960967\n",
      "Iteration 1052, loss = 8.30013783\n",
      "Iteration 1053, loss = 7.55072009\n",
      "Iteration 1054, loss = 7.75945457\n",
      "Iteration 1055, loss = 9.60808539\n",
      "Iteration 1056, loss = 7.41201040\n",
      "Iteration 1057, loss = 8.24940182\n",
      "Iteration 1058, loss = 9.71654912\n",
      "Iteration 1059, loss = 8.11284129\n",
      "Iteration 1060, loss = 8.12629343\n",
      "Iteration 1061, loss = 12.33553879\n",
      "Iteration 1062, loss = 15.48953324\n",
      "Iteration 1063, loss = 10.86488575\n",
      "Iteration 1064, loss = 10.62422016\n",
      "Iteration 1065, loss = 7.97101115\n",
      "Iteration 1066, loss = 7.85591558\n",
      "Iteration 1067, loss = 7.90290812\n",
      "Iteration 1068, loss = 8.72060697\n",
      "Iteration 1069, loss = 8.77882952\n",
      "Iteration 1070, loss = 7.77935227\n",
      "Iteration 1071, loss = 8.90516877\n",
      "Iteration 1072, loss = 9.31525862\n",
      "Iteration 1073, loss = 8.35714667\n",
      "Iteration 1074, loss = 10.58126325\n",
      "Iteration 1075, loss = 9.36196418\n",
      "Iteration 1076, loss = 10.68402941\n",
      "Iteration 1077, loss = 8.06919929\n",
      "Iteration 1078, loss = 8.18089115\n",
      "Iteration 1079, loss = 7.68070461\n",
      "Iteration 1080, loss = 9.19738018\n",
      "Iteration 1081, loss = 12.78140190\n",
      "Iteration 1082, loss = 11.87718068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1083, loss = 11.54206773\n",
      "Iteration 1084, loss = 10.89294856\n",
      "Iteration 1085, loss = 11.10059191\n",
      "Iteration 1086, loss = 10.09854194\n",
      "Iteration 1087, loss = 14.70908488\n",
      "Iteration 1088, loss = 12.94897708\n",
      "Iteration 1089, loss = 11.45331903\n",
      "Iteration 1090, loss = 8.89021519\n",
      "Iteration 1091, loss = 9.21028306\n",
      "Iteration 1092, loss = 8.69821241\n",
      "Iteration 1093, loss = 8.09877891\n",
      "Iteration 1094, loss = 8.82418190\n",
      "Iteration 1095, loss = 10.50044424\n",
      "Iteration 1096, loss = 12.06288819\n",
      "Iteration 1097, loss = 14.76425586\n",
      "Iteration 1098, loss = 10.81629046\n",
      "Iteration 1099, loss = 11.00022595\n",
      "Iteration 1100, loss = 9.72496246\n",
      "Iteration 1101, loss = 12.71072408\n",
      "Iteration 1102, loss = 10.59764060\n",
      "Iteration 1103, loss = 15.50397491\n",
      "Iteration 1104, loss = 9.07266109\n",
      "Iteration 1105, loss = 7.58372817\n",
      "Iteration 1106, loss = 13.58971780\n",
      "Iteration 1107, loss = 11.74571465\n",
      "Iteration 1108, loss = 12.31463446\n",
      "Iteration 1109, loss = 11.52474349\n",
      "Iteration 1110, loss = 10.75127738\n",
      "Iteration 1111, loss = 8.53053173\n",
      "Iteration 1112, loss = 7.51462158\n",
      "Iteration 1113, loss = 8.88939278\n",
      "Iteration 1114, loss = 8.36572101\n",
      "Iteration 1115, loss = 8.71061627\n",
      "Iteration 1116, loss = 7.92208085\n",
      "Iteration 1117, loss = 8.87166005\n",
      "Iteration 1118, loss = 9.17128345\n",
      "Iteration 1119, loss = 8.44809956\n",
      "Iteration 1120, loss = 8.29256577\n",
      "Iteration 1121, loss = 7.73801911\n",
      "Iteration 1122, loss = 9.69933946\n",
      "Iteration 1123, loss = 9.04319802\n",
      "Iteration 1124, loss = 8.59729052\n",
      "Iteration 1125, loss = 10.51679897\n",
      "Iteration 1126, loss = 10.83742512\n",
      "Iteration 1127, loss = 8.67061988\n",
      "Iteration 1128, loss = 9.20730886\n",
      "Iteration 1129, loss = 8.34648824\n",
      "Iteration 1130, loss = 12.47567881\n",
      "Iteration 1131, loss = 12.75164022\n",
      "Iteration 1132, loss = 10.76871727\n",
      "Iteration 1133, loss = 8.33037364\n",
      "Iteration 1134, loss = 8.44365533\n",
      "Iteration 1135, loss = 8.35966840\n",
      "Iteration 1136, loss = 7.53056807\n",
      "Iteration 1137, loss = 9.72428320\n",
      "Iteration 1138, loss = 10.20586782\n",
      "Iteration 1139, loss = 8.74511797\n",
      "Iteration 1140, loss = 8.00744145\n",
      "Iteration 1141, loss = 9.28307338\n",
      "Iteration 1142, loss = 8.91289272\n",
      "Iteration 1143, loss = 9.76346769\n",
      "Iteration 1144, loss = 10.58969133\n",
      "Iteration 1145, loss = 11.20055411\n",
      "Iteration 1146, loss = 10.88364704\n",
      "Iteration 1147, loss = 8.24268416\n",
      "Iteration 1148, loss = 7.46317980\n",
      "Iteration 1149, loss = 8.28404614\n",
      "Iteration 1150, loss = 11.70825728\n",
      "Iteration 1151, loss = 12.42699126\n",
      "Iteration 1152, loss = 9.75978842\n",
      "Iteration 1153, loss = 8.34989832\n",
      "Iteration 1154, loss = 7.53292096\n",
      "Iteration 1155, loss = 8.07820744\n",
      "Iteration 1156, loss = 9.06588495\n",
      "Iteration 1157, loss = 8.79905811\n",
      "Iteration 1158, loss = 9.23743171\n",
      "Iteration 1159, loss = 12.52309790\n",
      "Iteration 1160, loss = 13.51548719\n",
      "Iteration 1161, loss = 13.44374052\n",
      "Iteration 1162, loss = 15.46655771\n",
      "Iteration 1163, loss = 12.09981645\n",
      "Iteration 1164, loss = 13.62283834\n",
      "Iteration 1165, loss = 10.29453159\n",
      "Iteration 1166, loss = 7.86961074\n",
      "Iteration 1167, loss = 7.38116086\n",
      "Iteration 1168, loss = 8.06278099\n",
      "Iteration 1169, loss = 7.43735909\n",
      "Iteration 1170, loss = 7.77812593\n",
      "Iteration 1171, loss = 7.13516849\n",
      "Iteration 1172, loss = 8.40788431\n",
      "Iteration 1173, loss = 7.13413314\n",
      "Iteration 1174, loss = 13.09817291\n",
      "Iteration 1175, loss = 15.97092189\n",
      "Iteration 1176, loss = 11.67297306\n",
      "Iteration 1177, loss = 12.58033997\n",
      "Iteration 1178, loss = 13.58658859\n",
      "Iteration 1179, loss = 10.68450641\n",
      "Iteration 1180, loss = 8.84848394\n",
      "Iteration 1181, loss = 9.45117944\n",
      "Iteration 1182, loss = 9.20689402\n",
      "Iteration 1183, loss = 7.55652494\n",
      "Iteration 1184, loss = 7.57730105\n",
      "Iteration 1185, loss = 6.99029095\n",
      "Iteration 1186, loss = 7.32050485\n",
      "Iteration 1187, loss = 10.33390858\n",
      "Iteration 1188, loss = 8.65312127\n",
      "Iteration 1189, loss = 9.32755271\n",
      "Iteration 1190, loss = 8.82491743\n",
      "Iteration 1191, loss = 7.39150203\n",
      "Iteration 1192, loss = 8.80287930\n",
      "Iteration 1193, loss = 7.35483944\n",
      "Iteration 1194, loss = 7.66007743\n",
      "Iteration 1195, loss = 8.14057986\n",
      "Iteration 1196, loss = 8.31721645\n",
      "Iteration 1197, loss = 8.90850238\n",
      "Iteration 1198, loss = 9.58965594\n",
      "Iteration 1199, loss = 8.90211247\n",
      "Iteration 1200, loss = 10.52043574\n",
      "Iteration 1201, loss = 10.42147050\n",
      "Iteration 1202, loss = 7.68765756\n",
      "Iteration 1203, loss = 9.42535056\n",
      "Iteration 1204, loss = 8.51524940\n",
      "Iteration 1205, loss = 9.39041507\n",
      "Iteration 1206, loss = 10.09926595\n",
      "Iteration 1207, loss = 9.50004017\n",
      "Iteration 1208, loss = 9.09497284\n",
      "Iteration 1209, loss = 8.15016493\n",
      "Iteration 1210, loss = 9.18970231\n",
      "Iteration 1211, loss = 7.70569835\n",
      "Iteration 1212, loss = 7.77568687\n",
      "Iteration 1213, loss = 7.53403239\n",
      "Iteration 1214, loss = 8.42590654\n",
      "Iteration 1215, loss = 9.95881392\n",
      "Iteration 1216, loss = 10.52557151\n",
      "Iteration 1217, loss = 8.01207556\n",
      "Iteration 1218, loss = 8.16161146\n",
      "Iteration 1219, loss = 7.94826378\n",
      "Iteration 1220, loss = 8.16973263\n",
      "Iteration 1221, loss = 9.32415294\n",
      "Iteration 1222, loss = 12.87895488\n",
      "Iteration 1223, loss = 10.24526053\n",
      "Iteration 1224, loss = 12.51087257\n",
      "Iteration 1225, loss = 9.74142381\n",
      "Iteration 1226, loss = 8.68598005\n",
      "Iteration 1227, loss = 10.23577516\n",
      "Iteration 1228, loss = 8.06412431\n",
      "Iteration 1229, loss = 8.98979903\n",
      "Iteration 1230, loss = 12.31472659\n",
      "Iteration 1231, loss = 13.32742696\n",
      "Iteration 1232, loss = 14.66696853\n",
      "Iteration 1233, loss = 12.48647206\n",
      "Iteration 1234, loss = 12.31984529\n",
      "Iteration 1235, loss = 12.42772826\n",
      "Iteration 1236, loss = 11.30659536\n",
      "Iteration 1237, loss = 10.17256621\n",
      "Iteration 1238, loss = 9.93334387\n",
      "Iteration 1239, loss = 9.22542259\n",
      "Iteration 1240, loss = 11.08398513\n",
      "Iteration 1241, loss = 8.45253079\n",
      "Iteration 1242, loss = 7.74609141\n",
      "Iteration 1243, loss = 8.20953948\n",
      "Iteration 1244, loss = 12.91136402\n",
      "Iteration 1245, loss = 15.26407888\n",
      "Iteration 1246, loss = 14.95725667\n",
      "Iteration 1247, loss = 13.93930813\n",
      "Iteration 1248, loss = 22.05468174\n",
      "Iteration 1249, loss = 16.50047252\n",
      "Iteration 1250, loss = 15.59627734\n",
      "Iteration 1251, loss = 14.08544678\n",
      "Iteration 1252, loss = 7.86732181\n",
      "Iteration 1253, loss = 7.73398672\n",
      "Iteration 1254, loss = 8.35627453\n",
      "Iteration 1255, loss = 13.80547933\n",
      "Iteration 1256, loss = 14.22314114\n",
      "Iteration 1257, loss = 10.81936746\n",
      "Iteration 1258, loss = 11.17904965\n",
      "Iteration 1259, loss = 7.70026634\n",
      "Iteration 1260, loss = 9.62339586\n",
      "Iteration 1261, loss = 9.62513172\n",
      "Iteration 1262, loss = 8.87493299\n",
      "Iteration 1263, loss = 14.62115640\n",
      "Iteration 1264, loss = 14.34105927\n",
      "Iteration 1265, loss = 15.05881877\n",
      "Iteration 1266, loss = 15.98705932\n",
      "Iteration 1267, loss = 13.42446560\n",
      "Iteration 1268, loss = 14.28698295\n",
      "Iteration 1269, loss = 11.18692006\n",
      "Iteration 1270, loss = 8.17522041\n",
      "Iteration 1271, loss = 6.76893770\n",
      "Iteration 1272, loss = 8.28460627\n",
      "Iteration 1273, loss = 7.23645188\n",
      "Iteration 1274, loss = 7.86168543\n",
      "Iteration 1275, loss = 8.68381511\n",
      "Iteration 1276, loss = 8.63810875\n",
      "Iteration 1277, loss = 10.90664423\n",
      "Iteration 1278, loss = 11.06959655\n",
      "Iteration 1279, loss = 9.15590486\n",
      "Iteration 1280, loss = 8.53455636\n",
      "Iteration 1281, loss = 9.61915488\n",
      "Iteration 1282, loss = 7.00670307\n",
      "Iteration 1283, loss = 7.60695032\n",
      "Iteration 1284, loss = 8.07642503\n",
      "Iteration 1285, loss = 10.49781776\n",
      "Iteration 1286, loss = 8.94326598\n",
      "Iteration 1287, loss = 9.99683717\n",
      "Iteration 1288, loss = 10.76300758\n",
      "Iteration 1289, loss = 9.89887691\n",
      "Iteration 1290, loss = 9.69656093\n",
      "Iteration 1291, loss = 7.49678962\n",
      "Iteration 1292, loss = 8.44795605\n",
      "Iteration 1293, loss = 7.37466625\n",
      "Iteration 1294, loss = 7.33707004\n",
      "Iteration 1295, loss = 8.20496874\n",
      "Iteration 1296, loss = 8.12627193\n",
      "Iteration 1297, loss = 8.93960727\n",
      "Iteration 1298, loss = 9.80385053\n",
      "Iteration 1299, loss = 9.00059312\n",
      "Iteration 1300, loss = 7.68090911\n",
      "Iteration 1301, loss = 8.02462531\n",
      "Iteration 1302, loss = 8.60038657\n",
      "Iteration 1303, loss = 6.92022189\n",
      "Iteration 1304, loss = 7.29625970\n",
      "Iteration 1305, loss = 9.11346109\n",
      "Iteration 1306, loss = 8.51251697\n",
      "Iteration 1307, loss = 8.55665221\n",
      "Iteration 1308, loss = 8.30943170\n",
      "Iteration 1309, loss = 7.07172375\n",
      "Iteration 1310, loss = 7.51514532\n",
      "Iteration 1311, loss = 7.21528358\n",
      "Iteration 1312, loss = 8.29982390\n",
      "Iteration 1313, loss = 10.25736528\n",
      "Iteration 1314, loss = 8.97287789\n",
      "Iteration 1315, loss = 12.04081722\n",
      "Iteration 1316, loss = 11.91109685\n",
      "Iteration 1317, loss = 11.56669855\n",
      "Iteration 1318, loss = 7.95745377\n",
      "Iteration 1319, loss = 11.92659935\n",
      "Iteration 1320, loss = 7.69565728\n",
      "Iteration 1321, loss = 7.42188184\n",
      "Iteration 1322, loss = 6.97000613\n",
      "Iteration 1323, loss = 6.89274313\n",
      "Iteration 1324, loss = 6.60919563\n",
      "Iteration 1325, loss = 7.32041641\n",
      "Iteration 1326, loss = 7.92972184\n",
      "Iteration 1327, loss = 8.69087473\n",
      "Iteration 1328, loss = 9.39895632\n",
      "Iteration 1329, loss = 8.51539185\n",
      "Iteration 1330, loss = 6.68191849\n",
      "Iteration 1331, loss = 7.75436354\n",
      "Iteration 1332, loss = 9.36588249\n",
      "Iteration 1333, loss = 8.76539734\n",
      "Iteration 1334, loss = 10.58231921\n",
      "Iteration 1335, loss = 7.53023368\n",
      "Iteration 1336, loss = 8.04527807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1337, loss = 6.97189247\n",
      "Iteration 1338, loss = 9.31862445\n",
      "Iteration 1339, loss = 8.10353932\n",
      "Iteration 1340, loss = 9.64807490\n",
      "Iteration 1341, loss = 10.07515841\n",
      "Iteration 1342, loss = 7.22451007\n",
      "Iteration 1343, loss = 10.00894017\n",
      "Iteration 1344, loss = 9.96767885\n",
      "Iteration 1345, loss = 7.32731255\n",
      "Iteration 1346, loss = 6.99106087\n",
      "Iteration 1347, loss = 7.67250054\n",
      "Iteration 1348, loss = 8.08443073\n",
      "Iteration 1349, loss = 8.73800833\n",
      "Iteration 1350, loss = 7.39055185\n",
      "Iteration 1351, loss = 6.90966894\n",
      "Iteration 1352, loss = 14.22985670\n",
      "Iteration 1353, loss = 11.57894077\n",
      "Iteration 1354, loss = 11.25469100\n",
      "Iteration 1355, loss = 8.60616686\n",
      "Iteration 1356, loss = 6.50910903\n",
      "Iteration 1357, loss = 7.12448534\n",
      "Iteration 1358, loss = 7.77382678\n",
      "Iteration 1359, loss = 8.41581005\n",
      "Iteration 1360, loss = 8.41650107\n",
      "Iteration 1361, loss = 8.63067409\n",
      "Iteration 1362, loss = 6.65009756\n",
      "Iteration 1363, loss = 9.37542205\n",
      "Iteration 1364, loss = 10.33312715\n",
      "Iteration 1365, loss = 11.14756474\n",
      "Iteration 1366, loss = 10.49071550\n",
      "Iteration 1367, loss = 12.09013881\n",
      "Iteration 1368, loss = 9.22132508\n",
      "Iteration 1369, loss = 6.71948877\n",
      "Iteration 1370, loss = 6.86273251\n",
      "Iteration 1371, loss = 6.83768225\n",
      "Iteration 1372, loss = 7.72176709\n",
      "Iteration 1373, loss = 8.06872931\n",
      "Iteration 1374, loss = 6.74277294\n",
      "Iteration 1375, loss = 6.70077833\n",
      "Iteration 1376, loss = 6.92709601\n",
      "Iteration 1377, loss = 6.68477932\n",
      "Iteration 1378, loss = 7.37738824\n",
      "Iteration 1379, loss = 6.41131708\n",
      "Iteration 1380, loss = 6.81241319\n",
      "Iteration 1381, loss = 6.97997993\n",
      "Iteration 1382, loss = 7.44414801\n",
      "Iteration 1383, loss = 10.87325827\n",
      "Iteration 1384, loss = 8.20106024\n",
      "Iteration 1385, loss = 7.29667931\n",
      "Iteration 1386, loss = 6.59087784\n",
      "Iteration 1387, loss = 7.84044161\n",
      "Iteration 1388, loss = 6.84209760\n",
      "Iteration 1389, loss = 8.02230930\n",
      "Iteration 1390, loss = 8.15046234\n",
      "Iteration 1391, loss = 6.78889471\n",
      "Iteration 1392, loss = 6.51102669\n",
      "Iteration 1393, loss = 6.59113986\n",
      "Iteration 1394, loss = 6.70799071\n",
      "Iteration 1395, loss = 6.22745492\n",
      "Iteration 1396, loss = 9.85780531\n",
      "Iteration 1397, loss = 18.62210628\n",
      "Iteration 1398, loss = 21.03857423\n",
      "Iteration 1399, loss = 16.50518632\n",
      "Iteration 1400, loss = 12.10486672\n",
      "Iteration 1401, loss = 13.00155215\n",
      "Iteration 1402, loss = 17.09599756\n",
      "Iteration 1403, loss = 9.65453058\n",
      "Iteration 1404, loss = 7.27122064\n",
      "Iteration 1405, loss = 7.21705033\n",
      "Iteration 1406, loss = 8.22964356\n",
      "Iteration 1407, loss = 6.69735012\n",
      "Iteration 1408, loss = 6.55626707\n",
      "Iteration 1409, loss = 7.38967369\n",
      "Iteration 1410, loss = 7.21079633\n",
      "Iteration 1411, loss = 7.03160117\n",
      "Iteration 1412, loss = 7.98425342\n",
      "Iteration 1413, loss = 8.69957099\n",
      "Iteration 1414, loss = 7.13747023\n",
      "Iteration 1415, loss = 6.35832826\n",
      "Iteration 1416, loss = 6.96194552\n",
      "Iteration 1417, loss = 6.64824804\n",
      "Iteration 1418, loss = 7.04193685\n",
      "Iteration 1419, loss = 6.37808878\n",
      "Iteration 1420, loss = 6.44282648\n",
      "Iteration 1421, loss = 7.44949221\n",
      "Iteration 1422, loss = 7.02452857\n",
      "Iteration 1423, loss = 6.51997156\n",
      "Iteration 1424, loss = 6.91712018\n",
      "Iteration 1425, loss = 6.70084353\n",
      "Iteration 1426, loss = 7.94777979\n",
      "Iteration 1427, loss = 9.22670278\n",
      "Iteration 1428, loss = 8.70540309\n",
      "Iteration 1429, loss = 7.08998914\n",
      "Iteration 1430, loss = 8.03347075\n",
      "Iteration 1431, loss = 11.50913922\n",
      "Iteration 1432, loss = 9.54466157\n",
      "Iteration 1433, loss = 10.63338811\n",
      "Iteration 1434, loss = 9.87428598\n",
      "Iteration 1435, loss = 10.92774002\n",
      "Iteration 1436, loss = 9.15819220\n",
      "Iteration 1437, loss = 9.72656644\n",
      "Iteration 1438, loss = 9.91348067\n",
      "Iteration 1439, loss = 11.09300627\n",
      "Iteration 1440, loss = 10.80816172\n",
      "Iteration 1441, loss = 7.95815201\n",
      "Iteration 1442, loss = 7.83691811\n",
      "Iteration 1443, loss = 9.13125814\n",
      "Iteration 1444, loss = 12.71673665\n",
      "Iteration 1445, loss = 6.97088322\n",
      "Iteration 1446, loss = 6.40945475\n",
      "Iteration 1447, loss = 7.55718109\n",
      "Iteration 1448, loss = 6.51497131\n",
      "Iteration 1449, loss = 9.76836126\n",
      "Iteration 1450, loss = 8.45879193\n",
      "Iteration 1451, loss = 6.96486706\n",
      "Iteration 1452, loss = 7.53242887\n",
      "Iteration 1453, loss = 21.37569261\n",
      "Iteration 1454, loss = 27.69979340\n",
      "Iteration 1455, loss = 13.89582444\n",
      "Iteration 1456, loss = 10.65178719\n",
      "Iteration 1457, loss = 13.89057643\n",
      "Iteration 1458, loss = 9.50200355\n",
      "Iteration 1459, loss = 7.80169193\n",
      "Iteration 1460, loss = 7.47187410\n",
      "Iteration 1461, loss = 8.13777362\n",
      "Iteration 1462, loss = 8.38453087\n",
      "Iteration 1463, loss = 8.50859295\n",
      "Iteration 1464, loss = 6.85480057\n",
      "Iteration 1465, loss = 7.55703307\n",
      "Iteration 1466, loss = 7.72153560\n",
      "Iteration 1467, loss = 7.68190054\n",
      "Iteration 1468, loss = 7.91685919\n",
      "Iteration 1469, loss = 10.39531188\n",
      "Iteration 1470, loss = 7.21154383\n",
      "Iteration 1471, loss = 8.01561900\n",
      "Iteration 1472, loss = 11.01366232\n",
      "Iteration 1473, loss = 9.84840783\n",
      "Iteration 1474, loss = 10.85965368\n",
      "Iteration 1475, loss = 11.99669925\n",
      "Iteration 1476, loss = 14.49353517\n",
      "Iteration 1477, loss = 12.97333744\n",
      "Iteration 1478, loss = 9.27953779\n",
      "Iteration 1479, loss = 8.76102134\n",
      "Iteration 1480, loss = 6.34150493\n",
      "Iteration 1481, loss = 6.16350975\n",
      "Iteration 1482, loss = 6.49739567\n",
      "Iteration 1483, loss = 6.77109795\n",
      "Iteration 1484, loss = 7.01632004\n",
      "Iteration 1485, loss = 6.81501874\n",
      "Iteration 1486, loss = 7.09222730\n",
      "Iteration 1487, loss = 6.42903204\n",
      "Iteration 1488, loss = 7.72410474\n",
      "Iteration 1489, loss = 8.88785983\n",
      "Iteration 1490, loss = 7.67564989\n",
      "Iteration 1491, loss = 6.61218573\n",
      "Iteration 1492, loss = 6.46849036\n",
      "Iteration 1493, loss = 7.04954450\n",
      "Iteration 1494, loss = 6.30909818\n",
      "Iteration 1495, loss = 6.53111725\n",
      "Iteration 1496, loss = 7.54750718\n",
      "Iteration 1497, loss = 6.73684463\n",
      "Iteration 1498, loss = 6.79479109\n",
      "Iteration 1499, loss = 6.93479852\n",
      "Iteration 1500, loss = 6.73285886\n",
      "Iteration 1501, loss = 6.95451455\n",
      "Iteration 1502, loss = 7.49036566\n",
      "Iteration 1503, loss = 7.22630896\n",
      "Iteration 1504, loss = 8.02479745\n",
      "Iteration 1505, loss = 8.43330187\n",
      "Iteration 1506, loss = 6.37794662\n",
      "Iteration 1507, loss = 6.99540981\n",
      "Iteration 1508, loss = 7.10207168\n",
      "Iteration 1509, loss = 9.24560811\n",
      "Iteration 1510, loss = 8.07980084\n",
      "Iteration 1511, loss = 9.06394459\n",
      "Iteration 1512, loss = 10.32690370\n",
      "Iteration 1513, loss = 9.54735919\n",
      "Iteration 1514, loss = 23.72650593\n",
      "Iteration 1515, loss = 16.39687518\n",
      "Iteration 1516, loss = 15.67393385\n",
      "Iteration 1517, loss = 10.87320848\n",
      "Iteration 1518, loss = 12.12055653\n",
      "Iteration 1519, loss = 9.49604801\n",
      "Iteration 1520, loss = 10.02315980\n",
      "Iteration 1521, loss = 7.15204311\n",
      "Iteration 1522, loss = 7.30432433\n",
      "Iteration 1523, loss = 7.54916651\n",
      "Iteration 1524, loss = 7.62640326\n",
      "Iteration 1525, loss = 7.74518728\n",
      "Iteration 1526, loss = 8.35835743\n",
      "Iteration 1527, loss = 10.41927007\n",
      "Iteration 1528, loss = 8.08504931\n",
      "Iteration 1529, loss = 8.97534701\n",
      "Iteration 1530, loss = 11.51910216\n",
      "Iteration 1531, loss = 8.44170343\n",
      "Iteration 1532, loss = 6.15732972\n",
      "Iteration 1533, loss = 8.57304085\n",
      "Iteration 1534, loss = 8.22247121\n",
      "Iteration 1535, loss = 9.99419705\n",
      "Iteration 1536, loss = 7.67735118\n",
      "Iteration 1537, loss = 7.40203119\n",
      "Iteration 1538, loss = 14.02038306\n",
      "Iteration 1539, loss = 8.43226104\n",
      "Iteration 1540, loss = 8.39776753\n",
      "Iteration 1541, loss = 6.97696767\n",
      "Iteration 1542, loss = 7.10567709\n",
      "Iteration 1543, loss = 7.05430007\n",
      "Iteration 1544, loss = 14.95013520\n",
      "Iteration 1545, loss = 17.52949019\n",
      "Iteration 1546, loss = 19.73947374\n",
      "Iteration 1547, loss = 12.02475217\n",
      "Iteration 1548, loss = 7.02749145\n",
      "Iteration 1549, loss = 7.52681311\n",
      "Iteration 1550, loss = 6.87265359\n",
      "Iteration 1551, loss = 7.51324692\n",
      "Iteration 1552, loss = 6.33888770\n",
      "Iteration 1553, loss = 6.74409598\n",
      "Iteration 1554, loss = 7.46513387\n",
      "Iteration 1555, loss = 8.44607778\n",
      "Iteration 1556, loss = 11.49883038\n",
      "Iteration 1557, loss = 20.84673855\n",
      "Iteration 1558, loss = 13.50989298\n",
      "Iteration 1559, loss = 8.32187836\n",
      "Iteration 1560, loss = 7.52582523\n",
      "Iteration 1561, loss = 7.32194890\n",
      "Iteration 1562, loss = 7.32919718\n",
      "Iteration 1563, loss = 6.73748464\n",
      "Iteration 1564, loss = 6.20895090\n",
      "Iteration 1565, loss = 6.97623752\n",
      "Iteration 1566, loss = 7.39280645\n",
      "Iteration 1567, loss = 8.01956245\n",
      "Iteration 1568, loss = 7.01879758\n",
      "Iteration 1569, loss = 9.09727792\n",
      "Iteration 1570, loss = 8.71768638\n",
      "Iteration 1571, loss = 7.28056182\n",
      "Iteration 1572, loss = 6.24006534\n",
      "Iteration 1573, loss = 5.95736400\n",
      "Iteration 1574, loss = 6.80982298\n",
      "Iteration 1575, loss = 6.03485043\n",
      "Iteration 1576, loss = 7.48148221\n",
      "Iteration 1577, loss = 6.64749396\n",
      "Iteration 1578, loss = 6.21456431\n",
      "Iteration 1579, loss = 6.79450745\n",
      "Iteration 1580, loss = 6.41332824\n",
      "Iteration 1581, loss = 8.30262209\n",
      "Iteration 1582, loss = 8.23607269\n",
      "Iteration 1583, loss = 11.67608877\n",
      "Iteration 1584, loss = 16.72878840\n",
      "Iteration 1585, loss = 20.24529688\n",
      "Iteration 1586, loss = 15.10460566\n",
      "Iteration 1587, loss = 8.86400398\n",
      "Iteration 1588, loss = 8.23539206\n",
      "Iteration 1589, loss = 12.12644695\n",
      "Iteration 1590, loss = 11.19230124\n",
      "Iteration 1591, loss = 10.83962612\n",
      "Iteration 1592, loss = 13.31188620\n",
      "Iteration 1593, loss = 8.10254648\n",
      "Iteration 1594, loss = 6.89787772\n",
      "Iteration 1595, loss = 7.11683370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1596, loss = 6.81022614\n",
      "Iteration 1597, loss = 6.73469415\n",
      "Iteration 1598, loss = 6.10595411\n",
      "Iteration 1599, loss = 5.89387213\n",
      "Iteration 1600, loss = 6.90521058\n",
      "Iteration 1601, loss = 6.70389385\n",
      "Iteration 1602, loss = 5.65355846\n",
      "Iteration 1603, loss = 9.12360962\n",
      "Iteration 1604, loss = 9.11733209\n",
      "Iteration 1605, loss = 7.32062878\n",
      "Iteration 1606, loss = 6.22602080\n",
      "Iteration 1607, loss = 6.90530631\n",
      "Iteration 1608, loss = 6.19811221\n",
      "Iteration 1609, loss = 8.35524494\n",
      "Iteration 1610, loss = 6.07558993\n",
      "Iteration 1611, loss = 7.32185958\n",
      "Iteration 1612, loss = 6.31179692\n",
      "Iteration 1613, loss = 6.61377053\n",
      "Iteration 1614, loss = 10.33442157\n",
      "Iteration 1615, loss = 12.82874056\n",
      "Iteration 1616, loss = 8.08476999\n",
      "Iteration 1617, loss = 8.58109099\n",
      "Iteration 1618, loss = 8.53234355\n",
      "Iteration 1619, loss = 7.18778186\n",
      "Iteration 1620, loss = 6.36995959\n",
      "Iteration 1621, loss = 6.98099435\n",
      "Iteration 1622, loss = 10.91728791\n",
      "Iteration 1623, loss = 10.09976871\n",
      "Iteration 1624, loss = 7.90120485\n",
      "Iteration 1625, loss = 6.23536661\n",
      "Iteration 1626, loss = 7.63709149\n",
      "Iteration 1627, loss = 7.71736240\n",
      "Iteration 1628, loss = 6.67411922\n",
      "Iteration 1629, loss = 5.79534659\n",
      "Iteration 1630, loss = 8.00734033\n",
      "Iteration 1631, loss = 8.10113915\n",
      "Iteration 1632, loss = 6.99268300\n",
      "Iteration 1633, loss = 7.44038627\n",
      "Iteration 1634, loss = 6.81366969\n",
      "Iteration 1635, loss = 8.13173700\n",
      "Iteration 1636, loss = 7.43836063\n",
      "Iteration 1637, loss = 7.46607186\n",
      "Iteration 1638, loss = 6.61818457\n",
      "Iteration 1639, loss = 6.74369985\n",
      "Iteration 1640, loss = 10.31287243\n",
      "Iteration 1641, loss = 8.53716787\n",
      "Iteration 1642, loss = 9.59484524\n",
      "Iteration 1643, loss = 11.60909289\n",
      "Iteration 1644, loss = 9.62074845\n",
      "Iteration 1645, loss = 11.34096301\n",
      "Iteration 1646, loss = 11.76725359\n",
      "Iteration 1647, loss = 10.16817450\n",
      "Iteration 1648, loss = 7.90535010\n",
      "Iteration 1649, loss = 6.62602374\n",
      "Iteration 1650, loss = 6.08207097\n",
      "Iteration 1651, loss = 7.74398576\n",
      "Iteration 1652, loss = 7.48148719\n",
      "Iteration 1653, loss = 7.15168497\n",
      "Iteration 1654, loss = 6.41016674\n",
      "Iteration 1655, loss = 6.99077227\n",
      "Iteration 1656, loss = 6.39924322\n",
      "Iteration 1657, loss = 6.13532405\n",
      "Iteration 1658, loss = 6.25162865\n",
      "Iteration 1659, loss = 5.94057436\n",
      "Iteration 1660, loss = 6.11859580\n",
      "Iteration 1661, loss = 6.46348203\n",
      "Iteration 1662, loss = 7.96511043\n",
      "Iteration 1663, loss = 7.39263630\n",
      "Iteration 1664, loss = 8.06285084\n",
      "Iteration 1665, loss = 7.94712606\n",
      "Iteration 1666, loss = 11.21515148\n",
      "Iteration 1667, loss = 9.39572795\n",
      "Iteration 1668, loss = 9.61418495\n",
      "Iteration 1669, loss = 6.99853079\n",
      "Iteration 1670, loss = 8.03567661\n",
      "Iteration 1671, loss = 10.52233728\n",
      "Iteration 1672, loss = 8.31304238\n",
      "Iteration 1673, loss = 9.44191452\n",
      "Iteration 1674, loss = 6.42598178\n",
      "Iteration 1675, loss = 7.19823022\n",
      "Iteration 1676, loss = 9.86941910\n",
      "Iteration 1677, loss = 12.72027699\n",
      "Iteration 1678, loss = 12.33835203\n",
      "Iteration 1679, loss = 20.59725386\n",
      "Iteration 1680, loss = 25.92785295\n",
      "Iteration 1681, loss = 12.31655559\n",
      "Iteration 1682, loss = 8.28204640\n",
      "Iteration 1683, loss = 9.35301470\n",
      "Iteration 1684, loss = 7.19400583\n",
      "Iteration 1685, loss = 7.26946825\n",
      "Iteration 1686, loss = 7.00082715\n",
      "Iteration 1687, loss = 6.13672504\n",
      "Iteration 1688, loss = 6.84920395\n",
      "Iteration 1689, loss = 8.46269052\n",
      "Iteration 1690, loss = 8.15325844\n",
      "Iteration 1691, loss = 7.16703833\n",
      "Iteration 1692, loss = 8.87054179\n",
      "Iteration 1693, loss = 7.85727416\n",
      "Iteration 1694, loss = 11.43345295\n",
      "Iteration 1695, loss = 9.96531845\n",
      "Iteration 1696, loss = 17.64244434\n",
      "Iteration 1697, loss = 11.69202399\n",
      "Iteration 1698, loss = 12.74352210\n",
      "Iteration 1699, loss = 9.62133879\n",
      "Iteration 1700, loss = 11.83160560\n",
      "Iteration 1701, loss = 13.00634781\n",
      "Iteration 1702, loss = 9.34439651\n",
      "Iteration 1703, loss = 8.75537881\n",
      "Iteration 1704, loss = 7.57770611\n",
      "Iteration 1705, loss = 7.38778001\n",
      "Iteration 1706, loss = 6.35392106\n",
      "Iteration 1707, loss = 6.12779058\n",
      "Iteration 1708, loss = 10.86574073\n",
      "Iteration 1709, loss = 10.22218531\n",
      "Iteration 1710, loss = 10.37918012\n",
      "Iteration 1711, loss = 6.15377586\n",
      "Iteration 1712, loss = 8.18948012\n",
      "Iteration 1713, loss = 7.73790791\n",
      "Iteration 1714, loss = 8.70615350\n",
      "Iteration 1715, loss = 8.79706912\n",
      "Iteration 1716, loss = 6.77894936\n",
      "Iteration 1717, loss = 9.41300539\n",
      "Iteration 1718, loss = 7.76992216\n",
      "Iteration 1719, loss = 6.99436033\n",
      "Iteration 1720, loss = 6.18440521\n",
      "Iteration 1721, loss = 7.02421479\n",
      "Iteration 1722, loss = 7.02771079\n",
      "Iteration 1723, loss = 8.99022764\n",
      "Iteration 1724, loss = 7.21808245\n",
      "Iteration 1725, loss = 7.49439352\n",
      "Iteration 1726, loss = 6.68842505\n",
      "Iteration 1727, loss = 6.76179610\n",
      "Iteration 1728, loss = 6.41847651\n",
      "Iteration 1729, loss = 9.45986065\n",
      "Iteration 1730, loss = 8.19238957\n",
      "Iteration 1731, loss = 8.97511743\n",
      "Iteration 1732, loss = 8.08620160\n",
      "Iteration 1733, loss = 5.98650149\n",
      "Iteration 1734, loss = 7.09313897\n",
      "Iteration 1735, loss = 6.52763082\n",
      "Iteration 1736, loss = 6.46064933\n",
      "Iteration 1737, loss = 6.18993594\n",
      "Iteration 1738, loss = 6.48438393\n",
      "Iteration 1739, loss = 12.28519616\n",
      "Iteration 1740, loss = 12.70748476\n",
      "Iteration 1741, loss = 8.39361338\n",
      "Iteration 1742, loss = 9.33382981\n",
      "Iteration 1743, loss = 6.50812470\n",
      "Iteration 1744, loss = 6.40095347\n",
      "Iteration 1745, loss = 6.82906642\n",
      "Iteration 1746, loss = 6.02722670\n",
      "Iteration 1747, loss = 6.69335855\n",
      "Iteration 1748, loss = 6.06372138\n",
      "Iteration 1749, loss = 5.81578546\n",
      "Iteration 1750, loss = 6.28241557\n",
      "Iteration 1751, loss = 7.55318819\n",
      "Iteration 1752, loss = 10.13361208\n",
      "Iteration 1753, loss = 7.16721677\n",
      "Iteration 1754, loss = 7.02794672\n",
      "Iteration 1755, loss = 14.18118289\n",
      "Iteration 1756, loss = 8.95087245\n",
      "Iteration 1757, loss = 7.79110959\n",
      "Iteration 1758, loss = 8.38798804\n",
      "Iteration 1759, loss = 11.00357640\n",
      "Iteration 1760, loss = 10.77118576\n",
      "Iteration 1761, loss = 10.71803780\n",
      "Iteration 1762, loss = 11.18965884\n",
      "Iteration 1763, loss = 14.21771919\n",
      "Iteration 1764, loss = 12.17247853\n",
      "Iteration 1765, loss = 7.42026315\n",
      "Iteration 1766, loss = 8.99942964\n",
      "Iteration 1767, loss = 6.83569747\n",
      "Iteration 1768, loss = 7.54633883\n",
      "Iteration 1769, loss = 8.79046975\n",
      "Iteration 1770, loss = 7.06474696\n",
      "Iteration 1771, loss = 7.21734724\n",
      "Iteration 1772, loss = 8.46182665\n",
      "Iteration 1773, loss = 6.20649029\n",
      "Iteration 1774, loss = 6.95343733\n",
      "Iteration 1775, loss = 5.63164872\n",
      "Iteration 1776, loss = 6.27177869\n",
      "Iteration 1777, loss = 5.98811160\n",
      "Iteration 1778, loss = 13.62428000\n",
      "Iteration 1779, loss = 14.80460901\n",
      "Iteration 1780, loss = 9.32078949\n",
      "Iteration 1781, loss = 8.06963819\n",
      "Iteration 1782, loss = 9.62861852\n",
      "Iteration 1783, loss = 7.84069672\n",
      "Iteration 1784, loss = 7.62678382\n",
      "Iteration 1785, loss = 6.96776471\n",
      "Iteration 1786, loss = 7.23784268\n",
      "Iteration 1787, loss = 6.95481682\n",
      "Iteration 1788, loss = 8.74882139\n",
      "Iteration 1789, loss = 7.00005608\n",
      "Iteration 1790, loss = 7.66789173\n",
      "Iteration 1791, loss = 10.72437132\n",
      "Iteration 1792, loss = 10.33907946\n",
      "Iteration 1793, loss = 11.03076701\n",
      "Iteration 1794, loss = 18.37817547\n",
      "Iteration 1795, loss = 16.77163687\n",
      "Iteration 1796, loss = 8.33016938\n",
      "Iteration 1797, loss = 7.49861149\n",
      "Iteration 1798, loss = 11.16832405\n",
      "Iteration 1799, loss = 11.47605688\n",
      "Iteration 1800, loss = 6.48318917\n",
      "Iteration 1801, loss = 6.30744885\n",
      "Iteration 1802, loss = 6.44975851\n",
      "Iteration 1803, loss = 5.87779714\n",
      "Iteration 1804, loss = 5.95988545\n",
      "Iteration 1805, loss = 6.51629573\n",
      "Iteration 1806, loss = 12.53638310\n",
      "Iteration 1807, loss = 7.70905416\n",
      "Iteration 1808, loss = 6.63740256\n",
      "Iteration 1809, loss = 6.02584098\n",
      "Iteration 1810, loss = 6.42433835\n",
      "Iteration 1811, loss = 8.17538314\n",
      "Iteration 1812, loss = 6.85718224\n",
      "Iteration 1813, loss = 8.22745921\n",
      "Iteration 1814, loss = 6.99373265\n",
      "Iteration 1815, loss = 6.89367147\n",
      "Iteration 1816, loss = 13.64438615\n",
      "Iteration 1817, loss = 11.46163424\n",
      "Iteration 1818, loss = 9.71602814\n",
      "Iteration 1819, loss = 7.83063264\n",
      "Iteration 1820, loss = 8.07871894\n",
      "Iteration 1821, loss = 6.81377069\n",
      "Iteration 1822, loss = 6.17166976\n",
      "Iteration 1823, loss = 7.21740017\n",
      "Iteration 1824, loss = 5.96807225\n",
      "Iteration 1825, loss = 6.26770902\n",
      "Iteration 1826, loss = 6.66463309\n",
      "Iteration 1827, loss = 6.40318424\n",
      "Iteration 1828, loss = 7.50578080\n",
      "Iteration 1829, loss = 7.61602775\n",
      "Iteration 1830, loss = 16.54025201\n",
      "Iteration 1831, loss = 13.92299898\n",
      "Iteration 1832, loss = 11.27897498\n",
      "Iteration 1833, loss = 11.19034902\n",
      "Iteration 1834, loss = 7.12439911\n",
      "Iteration 1835, loss = 7.73981952\n",
      "Iteration 1836, loss = 6.33537156\n",
      "Iteration 1837, loss = 6.07453484\n",
      "Iteration 1838, loss = 7.12646311\n",
      "Iteration 1839, loss = 13.56270735\n",
      "Iteration 1840, loss = 11.78048791\n",
      "Iteration 1841, loss = 11.74957382\n",
      "Iteration 1842, loss = 10.36854207\n",
      "Iteration 1843, loss = 7.41469109\n",
      "Iteration 1844, loss = 8.51815281\n",
      "Iteration 1845, loss = 7.98648388\n",
      "Iteration 1846, loss = 6.82660350\n",
      "Iteration 1847, loss = 7.19292754\n",
      "Iteration 1848, loss = 7.36706629\n",
      "Iteration 1849, loss = 6.19517144\n",
      "Iteration 1850, loss = 5.83678165\n",
      "Iteration 1851, loss = 5.97296117\n",
      "Iteration 1852, loss = 6.39422968\n",
      "Iteration 1853, loss = 7.66163160\n",
      "Iteration 1854, loss = 9.23024366\n",
      "Iteration 1855, loss = 7.15306034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1856, loss = 6.94370760\n",
      "Iteration 1857, loss = 6.58044038\n",
      "Iteration 1858, loss = 6.81987194\n",
      "Iteration 1859, loss = 5.83439501\n",
      "Iteration 1860, loss = 6.01630496\n",
      "Iteration 1861, loss = 6.15717880\n",
      "Iteration 1862, loss = 5.86302406\n",
      "Iteration 1863, loss = 6.86313414\n",
      "Iteration 1864, loss = 7.51420858\n",
      "Iteration 1865, loss = 7.52074138\n",
      "Iteration 1866, loss = 7.70764353\n",
      "Iteration 1867, loss = 8.14745183\n",
      "Iteration 1868, loss = 10.76346724\n",
      "Iteration 1869, loss = 7.90443753\n",
      "Iteration 1870, loss = 6.63584764\n",
      "Iteration 1871, loss = 6.16686123\n",
      "Iteration 1872, loss = 5.71255209\n",
      "Iteration 1873, loss = 6.04259660\n",
      "Iteration 1874, loss = 6.93434544\n",
      "Iteration 1875, loss = 5.89737310\n",
      "Iteration 1876, loss = 6.43781481\n",
      "Iteration 1877, loss = 7.05303643\n",
      "Iteration 1878, loss = 6.81858642\n",
      "Iteration 1879, loss = 7.72651643\n",
      "Iteration 1880, loss = 6.20805431\n",
      "Iteration 1881, loss = 5.88879580\n",
      "Iteration 1882, loss = 5.67989160\n",
      "Iteration 1883, loss = 5.67617033\n",
      "Iteration 1884, loss = 5.60996550\n",
      "Iteration 1885, loss = 6.64833109\n",
      "Iteration 1886, loss = 5.88906199\n",
      "Iteration 1887, loss = 10.45849382\n",
      "Iteration 1888, loss = 7.48504464\n",
      "Iteration 1889, loss = 6.71945698\n",
      "Iteration 1890, loss = 8.14293581\n",
      "Iteration 1891, loss = 7.34346418\n",
      "Iteration 1892, loss = 5.65281637\n",
      "Iteration 1893, loss = 5.71289383\n",
      "Iteration 1894, loss = 5.68036333\n",
      "Iteration 1895, loss = 5.92830765\n",
      "Iteration 1896, loss = 7.66552030\n",
      "Iteration 1897, loss = 6.17403257\n",
      "Iteration 1898, loss = 7.14062166\n",
      "Iteration 1899, loss = 6.66025363\n",
      "Iteration 1900, loss = 9.02519491\n",
      "Iteration 1901, loss = 6.68408967\n",
      "Iteration 1902, loss = 6.98543628\n",
      "Iteration 1903, loss = 7.32279006\n",
      "Iteration 1904, loss = 8.33260443\n",
      "Iteration 1905, loss = 6.59186147\n",
      "Iteration 1906, loss = 10.38404332\n",
      "Iteration 1907, loss = 18.58038089\n",
      "Iteration 1908, loss = 9.66355306\n",
      "Iteration 1909, loss = 7.63436415\n",
      "Iteration 1910, loss = 6.45906633\n",
      "Iteration 1911, loss = 7.01916472\n",
      "Iteration 1912, loss = 7.23005774\n",
      "Iteration 1913, loss = 9.49390316\n",
      "Iteration 1914, loss = 6.83223475\n",
      "Iteration 1915, loss = 6.57693721\n",
      "Iteration 1916, loss = 6.11994838\n",
      "Iteration 1917, loss = 5.79795545\n",
      "Iteration 1918, loss = 6.17898693\n",
      "Iteration 1919, loss = 6.51061564\n",
      "Iteration 1920, loss = 6.60147144\n",
      "Iteration 1921, loss = 5.86364859\n",
      "Iteration 1922, loss = 6.37019183\n",
      "Iteration 1923, loss = 6.13038909\n",
      "Iteration 1924, loss = 7.11254946\n",
      "Iteration 1925, loss = 7.34476145\n",
      "Iteration 1926, loss = 9.26662963\n",
      "Iteration 1927, loss = 7.07153940\n",
      "Iteration 1928, loss = 7.33746047\n",
      "Iteration 1929, loss = 9.49048533\n",
      "Iteration 1930, loss = 7.29450782\n",
      "Iteration 1931, loss = 7.61405783\n",
      "Iteration 1932, loss = 8.29229066\n",
      "Iteration 1933, loss = 6.46330645\n",
      "Iteration 1934, loss = 6.02194121\n",
      "Iteration 1935, loss = 5.69876617\n",
      "Iteration 1936, loss = 6.35597801\n",
      "Iteration 1937, loss = 7.30793648\n",
      "Iteration 1938, loss = 6.94771320\n",
      "Iteration 1939, loss = 6.72406524\n",
      "Iteration 1940, loss = 6.16653334\n",
      "Iteration 1941, loss = 8.03176834\n",
      "Iteration 1942, loss = 7.11994536\n",
      "Iteration 1943, loss = 6.30190361\n",
      "Iteration 1944, loss = 8.21881152\n",
      "Iteration 1945, loss = 5.60968526\n",
      "Iteration 1946, loss = 6.44453044\n",
      "Iteration 1947, loss = 7.11572650\n",
      "Iteration 1948, loss = 13.54048667\n",
      "Iteration 1949, loss = 10.70917357\n",
      "Iteration 1950, loss = 15.53681684\n",
      "Iteration 1951, loss = 15.37582264\n",
      "Iteration 1952, loss = 11.72691157\n",
      "Iteration 1953, loss = 10.08461196\n",
      "Iteration 1954, loss = 10.24049855\n",
      "Iteration 1955, loss = 7.34445546\n",
      "Iteration 1956, loss = 8.75195007\n",
      "Iteration 1957, loss = 6.58958225\n",
      "Iteration 1958, loss = 7.39296315\n",
      "Iteration 1959, loss = 7.11928417\n",
      "Iteration 1960, loss = 8.19734450\n",
      "Iteration 1961, loss = 10.95683881\n",
      "Iteration 1962, loss = 10.58076606\n",
      "Iteration 1963, loss = 9.41949057\n",
      "Iteration 1964, loss = 9.16139541\n",
      "Iteration 1965, loss = 6.86990332\n",
      "Iteration 1966, loss = 7.30984605\n",
      "Iteration 1967, loss = 8.72564725\n",
      "Iteration 1968, loss = 5.89518646\n",
      "Iteration 1969, loss = 7.30679595\n",
      "Iteration 1970, loss = 6.76444224\n",
      "Iteration 1971, loss = 7.23872511\n",
      "Iteration 1972, loss = 7.18749849\n",
      "Iteration 1973, loss = 7.17811657\n",
      "Iteration 1974, loss = 6.19702020\n",
      "Iteration 1975, loss = 5.63347835\n",
      "Iteration 1976, loss = 6.93389750\n",
      "Iteration 1977, loss = 7.67166026\n",
      "Iteration 1978, loss = 5.93059160\n",
      "Iteration 1979, loss = 6.78420255\n",
      "Iteration 1980, loss = 6.75059617\n",
      "Iteration 1981, loss = 5.96126765\n",
      "Iteration 1982, loss = 6.30013954\n",
      "Iteration 1983, loss = 8.20737201\n",
      "Iteration 1984, loss = 7.58817742\n",
      "Iteration 1985, loss = 8.77166681\n",
      "Iteration 1986, loss = 5.68676430\n",
      "Iteration 1987, loss = 5.90964160\n",
      "Iteration 1988, loss = 6.41636153\n",
      "Iteration 1989, loss = 7.51119905\n",
      "Iteration 1990, loss = 15.11390893\n",
      "Iteration 1991, loss = 18.40857979\n",
      "Iteration 1992, loss = 14.60249609\n",
      "Iteration 1993, loss = 15.66082724\n",
      "Iteration 1994, loss = 16.71374639\n",
      "Iteration 1995, loss = 12.94235907\n",
      "Iteration 1996, loss = 7.14829326\n",
      "Iteration 1997, loss = 8.80176981\n",
      "Iteration 1998, loss = 9.17687373\n",
      "Iteration 1999, loss = 7.67794178\n",
      "Iteration 2000, loss = 8.14164312\n",
      "Iteration 2001, loss = 8.39424088\n",
      "Iteration 2002, loss = 6.75726330\n",
      "Iteration 2003, loss = 6.20369277\n",
      "Iteration 2004, loss = 6.46606735\n",
      "Iteration 2005, loss = 6.10605357\n",
      "Iteration 2006, loss = 5.93362412\n",
      "Iteration 2007, loss = 5.70843762\n",
      "Iteration 2008, loss = 6.60824750\n",
      "Iteration 2009, loss = 5.93875053\n",
      "Iteration 2010, loss = 6.76369494\n",
      "Iteration 2011, loss = 5.92051346\n",
      "Iteration 2012, loss = 6.33300049\n",
      "Iteration 2013, loss = 5.42212074\n",
      "Iteration 2014, loss = 6.16183780\n",
      "Iteration 2015, loss = 5.41934460\n",
      "Iteration 2016, loss = 5.88556808\n",
      "Iteration 2017, loss = 5.74990072\n",
      "Iteration 2018, loss = 7.57170830\n",
      "Iteration 2019, loss = 7.51283306\n",
      "Iteration 2020, loss = 6.92462783\n",
      "Iteration 2021, loss = 6.26397866\n",
      "Iteration 2022, loss = 9.00232223\n",
      "Iteration 2023, loss = 6.35244660\n",
      "Iteration 2024, loss = 5.54340147\n",
      "Iteration 2025, loss = 6.37818608\n",
      "Iteration 2026, loss = 6.48058168\n",
      "Iteration 2027, loss = 5.64603051\n",
      "Iteration 2028, loss = 5.81095048\n",
      "Iteration 2029, loss = 6.41965314\n",
      "Iteration 2030, loss = 7.15163977\n",
      "Iteration 2031, loss = 7.38598093\n",
      "Iteration 2032, loss = 8.80732974\n",
      "Iteration 2033, loss = 11.58895613\n",
      "Iteration 2034, loss = 16.00006207\n",
      "Iteration 2035, loss = 13.07744089\n",
      "Iteration 2036, loss = 7.41919818\n",
      "Iteration 2037, loss = 7.01547785\n",
      "Iteration 2038, loss = 9.85040109\n",
      "Iteration 2039, loss = 10.96381576\n",
      "Iteration 2040, loss = 11.94021211\n",
      "Iteration 2041, loss = 10.90107309\n",
      "Iteration 2042, loss = 10.03486897\n",
      "Iteration 2043, loss = 7.39034890\n",
      "Iteration 2044, loss = 7.39417499\n",
      "Iteration 2045, loss = 6.60886208\n",
      "Iteration 2046, loss = 5.35795381\n",
      "Iteration 2047, loss = 6.48553749\n",
      "Iteration 2048, loss = 6.75488035\n",
      "Iteration 2049, loss = 8.52995795\n",
      "Iteration 2050, loss = 6.75307876\n",
      "Iteration 2051, loss = 5.36040284\n",
      "Iteration 2052, loss = 5.65046356\n",
      "Iteration 2053, loss = 5.88228032\n",
      "Iteration 2054, loss = 5.49168635\n",
      "Iteration 2055, loss = 5.81557576\n",
      "Iteration 2056, loss = 6.32474483\n",
      "Iteration 2057, loss = 9.10016553\n",
      "Iteration 2058, loss = 7.60611548\n",
      "Iteration 2059, loss = 5.64046323\n",
      "Iteration 2060, loss = 6.32705342\n",
      "Iteration 2061, loss = 5.65027084\n",
      "Iteration 2062, loss = 5.32622441\n",
      "Iteration 2063, loss = 5.35762315\n",
      "Iteration 2064, loss = 6.26205968\n",
      "Iteration 2065, loss = 5.70761574\n",
      "Iteration 2066, loss = 6.91483639\n",
      "Iteration 2067, loss = 5.63245008\n",
      "Iteration 2068, loss = 5.81854959\n",
      "Iteration 2069, loss = 6.56777460\n",
      "Iteration 2070, loss = 5.74344042\n",
      "Iteration 2071, loss = 6.75570061\n",
      "Iteration 2072, loss = 7.80382670\n",
      "Iteration 2073, loss = 8.78668556\n",
      "Iteration 2074, loss = 8.35593389\n",
      "Iteration 2075, loss = 7.42553692\n",
      "Iteration 2076, loss = 6.56488396\n",
      "Iteration 2077, loss = 8.06592253\n",
      "Iteration 2078, loss = 6.38283378\n",
      "Iteration 2079, loss = 5.84822227\n",
      "Iteration 2080, loss = 5.64160694\n",
      "Iteration 2081, loss = 6.18911817\n",
      "Iteration 2082, loss = 7.07688918\n",
      "Iteration 2083, loss = 10.95487108\n",
      "Iteration 2084, loss = 7.80661706\n",
      "Iteration 2085, loss = 6.11023774\n",
      "Iteration 2086, loss = 5.62856722\n",
      "Iteration 2087, loss = 7.03577136\n",
      "Iteration 2088, loss = 7.78192807\n",
      "Iteration 2089, loss = 6.93523268\n",
      "Iteration 2090, loss = 6.51544552\n",
      "Iteration 2091, loss = 8.18282282\n",
      "Iteration 2092, loss = 5.49611551\n",
      "Iteration 2093, loss = 6.03209098\n",
      "Iteration 2094, loss = 6.85137982\n",
      "Iteration 2095, loss = 6.05507560\n",
      "Iteration 2096, loss = 6.43397559\n",
      "Iteration 2097, loss = 10.75433156\n",
      "Iteration 2098, loss = 13.40156866\n",
      "Iteration 2099, loss = 11.20667843\n",
      "Iteration 2100, loss = 21.17830116\n",
      "Iteration 2101, loss = 19.41911959\n",
      "Iteration 2102, loss = 16.40021004\n",
      "Iteration 2103, loss = 12.77444488\n",
      "Iteration 2104, loss = 9.55256390\n",
      "Iteration 2105, loss = 13.06299808\n",
      "Iteration 2106, loss = 10.86902878\n",
      "Iteration 2107, loss = 9.41920247\n",
      "Iteration 2108, loss = 7.52213332\n",
      "Iteration 2109, loss = 6.29947984\n",
      "Iteration 2110, loss = 5.68321430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2111, loss = 5.27977911\n",
      "Iteration 2112, loss = 5.79448215\n",
      "Iteration 2113, loss = 6.66260818\n",
      "Iteration 2114, loss = 6.05675724\n",
      "Iteration 2115, loss = 5.49175634\n",
      "Iteration 2116, loss = 7.64172393\n",
      "Iteration 2117, loss = 6.46076082\n",
      "Iteration 2118, loss = 7.69392617\n",
      "Iteration 2119, loss = 11.47078449\n",
      "Iteration 2120, loss = 8.12933423\n",
      "Iteration 2121, loss = 6.35274168\n",
      "Iteration 2122, loss = 8.99303278\n",
      "Iteration 2123, loss = 9.36788233\n",
      "Iteration 2124, loss = 6.51980553\n",
      "Iteration 2125, loss = 6.32239462\n",
      "Iteration 2126, loss = 5.79196978\n",
      "Iteration 2127, loss = 5.69306468\n",
      "Iteration 2128, loss = 5.64915876\n",
      "Iteration 2129, loss = 5.66423886\n",
      "Iteration 2130, loss = 5.64817218\n",
      "Iteration 2131, loss = 11.21244475\n",
      "Iteration 2132, loss = 6.89844321\n",
      "Iteration 2133, loss = 6.80129782\n",
      "Iteration 2134, loss = 6.59599447\n",
      "Iteration 2135, loss = 5.94790646\n",
      "Iteration 2136, loss = 5.94517926\n",
      "Iteration 2137, loss = 7.11999359\n",
      "Iteration 2138, loss = 6.37896667\n",
      "Iteration 2139, loss = 5.92769762\n",
      "Iteration 2140, loss = 6.05684616\n",
      "Iteration 2141, loss = 6.67717137\n",
      "Iteration 2142, loss = 6.26090853\n",
      "Iteration 2143, loss = 6.29505556\n",
      "Iteration 2144, loss = 6.57415490\n",
      "Iteration 2145, loss = 8.18683386\n",
      "Iteration 2146, loss = 8.52373894\n",
      "Iteration 2147, loss = 6.73506852\n",
      "Iteration 2148, loss = 6.50453215\n",
      "Iteration 2149, loss = 6.77373507\n",
      "Iteration 2150, loss = 7.02143240\n",
      "Iteration 2151, loss = 6.68805592\n",
      "Iteration 2152, loss = 11.88640987\n",
      "Iteration 2153, loss = 14.99528831\n",
      "Iteration 2154, loss = 11.56058579\n",
      "Iteration 2155, loss = 12.40442102\n",
      "Iteration 2156, loss = 13.15805289\n",
      "Iteration 2157, loss = 11.87045435\n",
      "Iteration 2158, loss = 9.75511455\n",
      "Iteration 2159, loss = 8.19550800\n",
      "Iteration 2160, loss = 6.40619703\n",
      "Iteration 2161, loss = 6.69258664\n",
      "Iteration 2162, loss = 8.34926274\n",
      "Iteration 2163, loss = 7.72313828\n",
      "Iteration 2164, loss = 6.77229655\n",
      "Iteration 2165, loss = 7.72244621\n",
      "Iteration 2166, loss = 6.13733250\n",
      "Iteration 2167, loss = 6.62005166\n",
      "Iteration 2168, loss = 6.33773120\n",
      "Iteration 2169, loss = 7.11752367\n",
      "Iteration 2170, loss = 6.57504609\n",
      "Iteration 2171, loss = 10.51943634\n",
      "Iteration 2172, loss = 7.36835079\n",
      "Iteration 2173, loss = 7.80175725\n",
      "Iteration 2174, loss = 6.81458539\n",
      "Iteration 2175, loss = 7.72251394\n",
      "Iteration 2176, loss = 7.75811957\n",
      "Iteration 2177, loss = 10.65480052\n",
      "Iteration 2178, loss = 7.02488655\n",
      "Iteration 2179, loss = 6.90937817\n",
      "Iteration 2180, loss = 5.45615719\n",
      "Iteration 2181, loss = 6.00782827\n",
      "Iteration 2182, loss = 5.83954726\n",
      "Iteration 2183, loss = 8.71439182\n",
      "Iteration 2184, loss = 5.93490313\n",
      "Iteration 2185, loss = 6.27079544\n",
      "Iteration 2186, loss = 5.40643454\n",
      "Iteration 2187, loss = 5.50454099\n",
      "Iteration 2188, loss = 6.24627274\n",
      "Iteration 2189, loss = 7.81960155\n",
      "Iteration 2190, loss = 5.98390636\n",
      "Iteration 2191, loss = 11.44965270\n",
      "Iteration 2192, loss = 10.09827163\n",
      "Iteration 2193, loss = 7.55951266\n",
      "Iteration 2194, loss = 6.57827143\n",
      "Iteration 2195, loss = 9.43401431\n",
      "Iteration 2196, loss = 15.04019685\n",
      "Iteration 2197, loss = 12.38835305\n",
      "Iteration 2198, loss = 10.12614820\n",
      "Iteration 2199, loss = 12.86845839\n",
      "Iteration 2200, loss = 13.08677065\n",
      "Iteration 2201, loss = 10.11055516\n",
      "Iteration 2202, loss = 9.31327705\n",
      "Iteration 2203, loss = 9.84933493\n",
      "Iteration 2204, loss = 7.77231487\n",
      "Iteration 2205, loss = 6.59425513\n",
      "Iteration 2206, loss = 6.11328522\n",
      "Iteration 2207, loss = 6.11976221\n",
      "Iteration 2208, loss = 5.34836823\n",
      "Iteration 2209, loss = 5.25412050\n",
      "Iteration 2210, loss = 5.59011627\n",
      "Iteration 2211, loss = 6.37239989\n",
      "Iteration 2212, loss = 6.28069103\n",
      "Iteration 2213, loss = 5.71952929\n",
      "Iteration 2214, loss = 6.10655895\n",
      "Iteration 2215, loss = 6.24791250\n",
      "Iteration 2216, loss = 8.57953181\n",
      "Iteration 2217, loss = 5.96092801\n",
      "Iteration 2218, loss = 5.75647092\n",
      "Iteration 2219, loss = 5.57383646\n",
      "Iteration 2220, loss = 5.88297858\n",
      "Iteration 2221, loss = 5.68561167\n",
      "Iteration 2222, loss = 5.94845529\n",
      "Iteration 2223, loss = 6.21255990\n",
      "Iteration 2224, loss = 5.70788961\n",
      "Iteration 2225, loss = 5.96355699\n",
      "Iteration 2226, loss = 6.73337333\n",
      "Iteration 2227, loss = 8.01365089\n",
      "Iteration 2228, loss = 7.82718260\n",
      "Iteration 2229, loss = 6.99337969\n",
      "Iteration 2230, loss = 7.98647379\n",
      "Iteration 2231, loss = 8.27194094\n",
      "Iteration 2232, loss = 5.69465631\n",
      "Iteration 2233, loss = 5.82299838\n",
      "Iteration 2234, loss = 5.70032760\n",
      "Iteration 2235, loss = 6.95037033\n",
      "Iteration 2236, loss = 6.52863150\n",
      "Iteration 2237, loss = 8.15242228\n",
      "Iteration 2238, loss = 10.69249714\n",
      "Iteration 2239, loss = 7.88805854\n",
      "Iteration 2240, loss = 6.54509772\n",
      "Iteration 2241, loss = 8.19579801\n",
      "Iteration 2242, loss = 6.42878519\n",
      "Iteration 2243, loss = 7.04616951\n",
      "Iteration 2244, loss = 5.54240531\n",
      "Iteration 2245, loss = 5.94061474\n",
      "Iteration 2246, loss = 5.67340154\n",
      "Iteration 2247, loss = 5.31427491\n",
      "Iteration 2248, loss = 5.67300107\n",
      "Iteration 2249, loss = 6.89247170\n",
      "Iteration 2250, loss = 6.34314554\n",
      "Iteration 2251, loss = 5.57299205\n",
      "Iteration 2252, loss = 5.84614346\n",
      "Iteration 2253, loss = 6.39538214\n",
      "Iteration 2254, loss = 5.72019019\n",
      "Iteration 2255, loss = 5.83481849\n",
      "Iteration 2256, loss = 7.30165155\n",
      "Iteration 2257, loss = 10.56833597\n",
      "Iteration 2258, loss = 6.08626430\n",
      "Iteration 2259, loss = 6.52932045\n",
      "Iteration 2260, loss = 5.96028246\n",
      "Iteration 2261, loss = 6.53199588\n",
      "Iteration 2262, loss = 6.60302653\n",
      "Iteration 2263, loss = 6.31215225\n",
      "Iteration 2264, loss = 8.40803871\n",
      "Iteration 2265, loss = 7.76337713\n",
      "Iteration 2266, loss = 8.65616272\n",
      "Iteration 2267, loss = 7.05389969\n",
      "Iteration 2268, loss = 5.85388915\n",
      "Iteration 2269, loss = 7.32985022\n",
      "Iteration 2270, loss = 7.78299647\n",
      "Iteration 2271, loss = 8.42261290\n",
      "Iteration 2272, loss = 7.41849787\n",
      "Iteration 2273, loss = 8.50935615\n",
      "Iteration 2274, loss = 6.55546522\n",
      "Iteration 2275, loss = 8.67365596\n",
      "Iteration 2276, loss = 10.41727691\n",
      "Iteration 2277, loss = 7.50086334\n",
      "Iteration 2278, loss = 7.52332478\n",
      "Iteration 2279, loss = 5.82261775\n",
      "Iteration 2280, loss = 6.73630405\n",
      "Iteration 2281, loss = 6.01501252\n",
      "Iteration 2282, loss = 5.60476432\n",
      "Iteration 2283, loss = 5.37687739\n",
      "Iteration 2284, loss = 6.11627063\n",
      "Iteration 2285, loss = 6.66637017\n",
      "Iteration 2286, loss = 6.64320760\n",
      "Iteration 2287, loss = 6.73315806\n",
      "Iteration 2288, loss = 7.31266444\n",
      "Iteration 2289, loss = 6.94949079\n",
      "Iteration 2290, loss = 6.81873683\n",
      "Iteration 2291, loss = 6.84007579\n",
      "Iteration 2292, loss = 8.59005688\n",
      "Iteration 2293, loss = 10.11682974\n",
      "Iteration 2294, loss = 6.28585495\n",
      "Iteration 2295, loss = 7.63987124\n",
      "Iteration 2296, loss = 6.52045107\n",
      "Iteration 2297, loss = 6.65773596\n",
      "Iteration 2298, loss = 5.48953568\n",
      "Iteration 2299, loss = 6.43435487\n",
      "Iteration 2300, loss = 7.17377450\n",
      "Iteration 2301, loss = 6.84202184\n",
      "Iteration 2302, loss = 6.20259994\n",
      "Iteration 2303, loss = 5.32541794\n",
      "Iteration 2304, loss = 5.13441496\n",
      "Iteration 2305, loss = 5.79935903\n",
      "Iteration 2306, loss = 5.61646092\n",
      "Iteration 2307, loss = 7.23315989\n",
      "Iteration 2308, loss = 5.94500517\n",
      "Iteration 2309, loss = 7.03487298\n",
      "Iteration 2310, loss = 7.40275608\n",
      "Iteration 2311, loss = 5.39868103\n",
      "Iteration 2312, loss = 5.18375419\n",
      "Iteration 2313, loss = 6.88663685\n",
      "Iteration 2314, loss = 7.46606761\n",
      "Iteration 2315, loss = 5.52367574\n",
      "Iteration 2316, loss = 6.82463041\n",
      "Iteration 2317, loss = 10.31348272\n",
      "Iteration 2318, loss = 8.30920136\n",
      "Iteration 2319, loss = 9.26747673\n",
      "Iteration 2320, loss = 11.37727522\n",
      "Iteration 2321, loss = 8.81071540\n",
      "Iteration 2322, loss = 7.18649229\n",
      "Iteration 2323, loss = 7.07821361\n",
      "Iteration 2324, loss = 6.45245452\n",
      "Iteration 2325, loss = 5.50469479\n",
      "Iteration 2326, loss = 5.98537927\n",
      "Iteration 2327, loss = 7.26181215\n",
      "Iteration 2328, loss = 5.66741576\n",
      "Iteration 2329, loss = 6.33437519\n",
      "Iteration 2330, loss = 7.00609774\n",
      "Iteration 2331, loss = 5.20078383\n",
      "Iteration 2332, loss = 5.55789413\n",
      "Iteration 2333, loss = 5.12348442\n",
      "Iteration 2334, loss = 5.81958636\n",
      "Iteration 2335, loss = 6.35387939\n",
      "Iteration 2336, loss = 5.25568899\n",
      "Iteration 2337, loss = 6.22408704\n",
      "Iteration 2338, loss = 6.78146743\n",
      "Iteration 2339, loss = 6.84107502\n",
      "Iteration 2340, loss = 5.81584694\n",
      "Iteration 2341, loss = 5.40567560\n",
      "Iteration 2342, loss = 5.86011548\n",
      "Iteration 2343, loss = 5.29564838\n",
      "Iteration 2344, loss = 5.20690734\n",
      "Iteration 2345, loss = 8.76677783\n",
      "Iteration 2346, loss = 5.99266568\n",
      "Iteration 2347, loss = 5.57586532\n",
      "Iteration 2348, loss = 6.02873210\n",
      "Iteration 2349, loss = 5.22942238\n",
      "Iteration 2350, loss = 5.26112471\n",
      "Iteration 2351, loss = 5.40951343\n",
      "Iteration 2352, loss = 8.01412244\n",
      "Iteration 2353, loss = 5.98819525\n",
      "Iteration 2354, loss = 11.32952670\n",
      "Iteration 2355, loss = 10.00807305\n",
      "Iteration 2356, loss = 8.99617973\n",
      "Iteration 2357, loss = 9.55794257\n",
      "Iteration 2358, loss = 6.26406795\n",
      "Iteration 2359, loss = 5.65272399\n",
      "Iteration 2360, loss = 6.03581356\n",
      "Iteration 2361, loss = 6.30601318\n",
      "Iteration 2362, loss = 5.49229110\n",
      "Iteration 2363, loss = 6.26355643\n",
      "Iteration 2364, loss = 6.54213123\n",
      "Iteration 2365, loss = 6.43266110\n",
      "Iteration 2366, loss = 5.55133891\n",
      "Iteration 2367, loss = 5.25834631\n",
      "Iteration 2368, loss = 5.98566912\n",
      "Iteration 2369, loss = 7.69512550\n",
      "Iteration 2370, loss = 5.26766146\n",
      "Iteration 2371, loss = 6.82436937\n",
      "Iteration 2372, loss = 6.25952686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2373, loss = 5.25959035\n",
      "Iteration 2374, loss = 6.08596215\n",
      "Iteration 2375, loss = 6.25187864\n",
      "Iteration 2376, loss = 5.92551034\n",
      "Iteration 2377, loss = 5.51481269\n",
      "Iteration 2378, loss = 5.24415620\n",
      "Iteration 2379, loss = 5.12871196\n",
      "Iteration 2380, loss = 5.52663639\n",
      "Iteration 2381, loss = 5.46224917\n",
      "Iteration 2382, loss = 5.24712640\n",
      "Iteration 2383, loss = 6.83491092\n",
      "Iteration 2384, loss = 7.23891759\n",
      "Iteration 2385, loss = 5.81722033\n",
      "Iteration 2386, loss = 6.17600423\n",
      "Iteration 2387, loss = 7.04814380\n",
      "Iteration 2388, loss = 6.99990540\n",
      "Iteration 2389, loss = 15.70885536\n",
      "Iteration 2390, loss = 10.51139127\n",
      "Iteration 2391, loss = 9.08794671\n",
      "Iteration 2392, loss = 7.34873383\n",
      "Iteration 2393, loss = 9.07192455\n",
      "Iteration 2394, loss = 8.25883133\n",
      "Iteration 2395, loss = 10.09981453\n",
      "Iteration 2396, loss = 7.80647834\n",
      "Iteration 2397, loss = 6.83799790\n",
      "Iteration 2398, loss = 6.90483385\n",
      "Iteration 2399, loss = 6.33814670\n",
      "Iteration 2400, loss = 5.07386199\n",
      "Iteration 2401, loss = 5.57309390\n",
      "Iteration 2402, loss = 7.12994296\n",
      "Iteration 2403, loss = 5.78579798\n",
      "Iteration 2404, loss = 6.51326321\n",
      "Iteration 2405, loss = 5.76230959\n",
      "Iteration 2406, loss = 5.96781805\n",
      "Iteration 2407, loss = 6.29425349\n",
      "Iteration 2408, loss = 8.19136909\n",
      "Iteration 2409, loss = 9.74897129\n",
      "Iteration 2410, loss = 6.22124759\n",
      "Iteration 2411, loss = 6.32106727\n",
      "Iteration 2412, loss = 5.56914761\n",
      "Iteration 2413, loss = 5.51431419\n",
      "Iteration 2414, loss = 6.23720619\n",
      "Iteration 2415, loss = 10.73725693\n",
      "Iteration 2416, loss = 8.03460169\n",
      "Iteration 2417, loss = 8.67548191\n",
      "Iteration 2418, loss = 8.12128544\n",
      "Iteration 2419, loss = 7.57387286\n",
      "Iteration 2420, loss = 9.44865179\n",
      "Iteration 2421, loss = 12.18797616\n",
      "Iteration 2422, loss = 5.03816657\n",
      "Iteration 2423, loss = 5.42215302\n",
      "Iteration 2424, loss = 7.06161859\n",
      "Iteration 2425, loss = 10.08106696\n",
      "Iteration 2426, loss = 7.16942335\n",
      "Iteration 2427, loss = 6.02929962\n",
      "Iteration 2428, loss = 8.44379649\n",
      "Iteration 2429, loss = 9.77080892\n",
      "Iteration 2430, loss = 7.01521781\n",
      "Iteration 2431, loss = 5.10156222\n",
      "Iteration 2432, loss = 6.41343354\n",
      "Iteration 2433, loss = 5.71260556\n",
      "Iteration 2434, loss = 6.13001521\n",
      "Iteration 2435, loss = 7.47268212\n",
      "Iteration 2436, loss = 6.27444437\n",
      "Iteration 2437, loss = 5.76941673\n",
      "Iteration 2438, loss = 6.35214132\n",
      "Iteration 2439, loss = 5.41120179\n",
      "Iteration 2440, loss = 5.06878494\n",
      "Iteration 2441, loss = 5.47578722\n",
      "Iteration 2442, loss = 5.80566117\n",
      "Iteration 2443, loss = 6.47593203\n",
      "Iteration 2444, loss = 5.47508986\n",
      "Iteration 2445, loss = 8.07385105\n",
      "Iteration 2446, loss = 6.78630148\n",
      "Iteration 2447, loss = 5.99209737\n",
      "Iteration 2448, loss = 10.45860920\n",
      "Iteration 2449, loss = 7.98570373\n",
      "Iteration 2450, loss = 7.16307541\n",
      "Iteration 2451, loss = 7.26693070\n",
      "Iteration 2452, loss = 9.26851470\n",
      "Iteration 2453, loss = 8.80939771\n",
      "Iteration 2454, loss = 6.23064521\n",
      "Iteration 2455, loss = 7.29312453\n",
      "Iteration 2456, loss = 9.55260150\n",
      "Iteration 2457, loss = 13.15481493\n",
      "Iteration 2458, loss = 10.11005887\n",
      "Iteration 2459, loss = 11.21507070\n",
      "Iteration 2460, loss = 8.03074766\n",
      "Iteration 2461, loss = 5.46511653\n",
      "Iteration 2462, loss = 6.43654054\n",
      "Iteration 2463, loss = 8.15190906\n",
      "Iteration 2464, loss = 6.16975915\n",
      "Iteration 2465, loss = 5.28319379\n",
      "Iteration 2466, loss = 5.48234081\n",
      "Iteration 2467, loss = 5.75161346\n",
      "Iteration 2468, loss = 5.59167362\n",
      "Iteration 2469, loss = 6.14326235\n",
      "Iteration 2470, loss = 6.21388348\n",
      "Iteration 2471, loss = 5.49678428\n",
      "Iteration 2472, loss = 7.22387412\n",
      "Iteration 2473, loss = 5.75499981\n",
      "Iteration 2474, loss = 5.30300112\n",
      "Iteration 2475, loss = 5.51628812\n",
      "Iteration 2476, loss = 5.10258179\n",
      "Iteration 2477, loss = 6.02288854\n",
      "Iteration 2478, loss = 7.76712963\n",
      "Iteration 2479, loss = 5.94711595\n",
      "Iteration 2480, loss = 10.18606692\n",
      "Iteration 2481, loss = 9.08977220\n",
      "Iteration 2482, loss = 7.61744769\n",
      "Iteration 2483, loss = 6.56394114\n",
      "Iteration 2484, loss = 5.27954132\n",
      "Iteration 2485, loss = 5.03284865\n",
      "Iteration 2486, loss = 5.16885727\n",
      "Iteration 2487, loss = 5.43811354\n",
      "Iteration 2488, loss = 6.45248534\n",
      "Iteration 2489, loss = 5.80731180\n",
      "Iteration 2490, loss = 4.75145236\n",
      "Iteration 2491, loss = 7.29314234\n",
      "Iteration 2492, loss = 7.51110807\n",
      "Iteration 2493, loss = 6.56502226\n",
      "Iteration 2494, loss = 5.57882245\n",
      "Iteration 2495, loss = 6.26514826\n",
      "Iteration 2496, loss = 7.05667667\n",
      "Iteration 2497, loss = 7.97330171\n",
      "Iteration 2498, loss = 6.95247379\n",
      "Iteration 2499, loss = 7.47546776\n",
      "Iteration 2500, loss = 11.45659796\n",
      "Iteration 2501, loss = 10.70074503\n",
      "Iteration 2502, loss = 11.27038158\n",
      "Iteration 2503, loss = 8.65820417\n",
      "Iteration 2504, loss = 9.22682678\n",
      "Iteration 2505, loss = 5.96535143\n",
      "Iteration 2506, loss = 5.26356116\n",
      "Iteration 2507, loss = 5.11888665\n",
      "Iteration 2508, loss = 5.19011713\n",
      "Iteration 2509, loss = 5.05161502\n",
      "Iteration 2510, loss = 5.23743202\n",
      "Iteration 2511, loss = 5.54478420\n",
      "Iteration 2512, loss = 5.91964736\n",
      "Iteration 2513, loss = 5.18804246\n",
      "Iteration 2514, loss = 4.85730046\n",
      "Iteration 2515, loss = 5.74619227\n",
      "Iteration 2516, loss = 5.79672853\n",
      "Iteration 2517, loss = 7.52712634\n",
      "Iteration 2518, loss = 6.46597240\n",
      "Iteration 2519, loss = 5.96756717\n",
      "Iteration 2520, loss = 5.91194741\n",
      "Iteration 2521, loss = 5.30374831\n",
      "Iteration 2522, loss = 5.43024334\n",
      "Iteration 2523, loss = 11.08441625\n",
      "Iteration 2524, loss = 7.14715647\n",
      "Iteration 2525, loss = 9.96195311\n",
      "Iteration 2526, loss = 5.56253980\n",
      "Iteration 2527, loss = 5.67614920\n",
      "Iteration 2528, loss = 5.06881917\n",
      "Iteration 2529, loss = 4.98265916\n",
      "Iteration 2530, loss = 5.10937698\n",
      "Iteration 2531, loss = 6.46974653\n",
      "Iteration 2532, loss = 5.37971869\n",
      "Iteration 2533, loss = 6.14405130\n",
      "Iteration 2534, loss = 6.23189895\n",
      "Iteration 2535, loss = 6.29807914\n",
      "Iteration 2536, loss = 5.77742662\n",
      "Iteration 2537, loss = 5.75084374\n",
      "Iteration 2538, loss = 5.80011551\n",
      "Iteration 2539, loss = 5.95063798\n",
      "Iteration 2540, loss = 8.96487795\n",
      "Iteration 2541, loss = 9.57149743\n",
      "Iteration 2542, loss = 8.57229424\n",
      "Iteration 2543, loss = 13.45977269\n",
      "Iteration 2544, loss = 6.27742709\n",
      "Iteration 2545, loss = 9.47263025\n",
      "Iteration 2546, loss = 8.20236321\n",
      "Iteration 2547, loss = 6.67449768\n",
      "Iteration 2548, loss = 6.97100396\n",
      "Iteration 2549, loss = 6.02997127\n",
      "Iteration 2550, loss = 7.06134831\n",
      "Iteration 2551, loss = 6.34178386\n",
      "Iteration 2552, loss = 5.58841442\n",
      "Iteration 2553, loss = 9.48938995\n",
      "Iteration 2554, loss = 12.80577533\n",
      "Iteration 2555, loss = 8.20487172\n",
      "Iteration 2556, loss = 11.36562954\n",
      "Iteration 2557, loss = 8.38142821\n",
      "Iteration 2558, loss = 8.44033292\n",
      "Iteration 2559, loss = 5.93154568\n",
      "Iteration 2560, loss = 6.46028711\n",
      "Iteration 2561, loss = 7.35463025\n",
      "Iteration 2562, loss = 8.10736518\n",
      "Iteration 2563, loss = 8.64167794\n",
      "Iteration 2564, loss = 9.27219356\n",
      "Iteration 2565, loss = 10.98330911\n",
      "Iteration 2566, loss = 10.56555806\n",
      "Iteration 2567, loss = 7.57048248\n",
      "Iteration 2568, loss = 8.45854614\n",
      "Iteration 2569, loss = 12.07377881\n",
      "Iteration 2570, loss = 9.32841852\n",
      "Iteration 2571, loss = 10.51883261\n",
      "Iteration 2572, loss = 6.83399145\n",
      "Iteration 2573, loss = 7.15262449\n",
      "Iteration 2574, loss = 6.19363522\n",
      "Iteration 2575, loss = 5.36544682\n",
      "Iteration 2576, loss = 4.94459451\n",
      "Iteration 2577, loss = 7.08162529\n",
      "Iteration 2578, loss = 7.24020957\n",
      "Iteration 2579, loss = 9.94647211\n",
      "Iteration 2580, loss = 12.05651212\n",
      "Iteration 2581, loss = 7.09988125\n",
      "Iteration 2582, loss = 6.51665456\n",
      "Iteration 2583, loss = 7.95942189\n",
      "Iteration 2584, loss = 7.61278022\n",
      "Iteration 2585, loss = 6.00185908\n",
      "Iteration 2586, loss = 5.67079469\n",
      "Iteration 2587, loss = 5.27206772\n",
      "Iteration 2588, loss = 4.86878364\n",
      "Iteration 2589, loss = 5.24840272\n",
      "Iteration 2590, loss = 4.85691585\n",
      "Iteration 2591, loss = 5.81315742\n",
      "Iteration 2592, loss = 6.10428884\n",
      "Iteration 2593, loss = 6.21446855\n",
      "Iteration 2594, loss = 6.76257325\n",
      "Iteration 2595, loss = 6.20535106\n",
      "Iteration 2596, loss = 6.41147606\n",
      "Iteration 2597, loss = 6.82993088\n",
      "Iteration 2598, loss = 5.15618887\n",
      "Iteration 2599, loss = 5.81126242\n",
      "Iteration 2600, loss = 5.49584202\n",
      "Iteration 2601, loss = 5.70403338\n",
      "Iteration 2602, loss = 6.46769012\n",
      "Iteration 2603, loss = 5.63821632\n",
      "Iteration 2604, loss = 6.73633438\n",
      "Iteration 2605, loss = 8.05579697\n",
      "Iteration 2606, loss = 7.91859050\n",
      "Iteration 2607, loss = 7.00306837\n",
      "Iteration 2608, loss = 5.13604576\n",
      "Iteration 2609, loss = 5.33496337\n",
      "Iteration 2610, loss = 4.84313753\n",
      "Iteration 2611, loss = 4.97488856\n",
      "Iteration 2612, loss = 5.94198295\n",
      "Iteration 2613, loss = 7.49321735\n",
      "Iteration 2614, loss = 7.49061810\n",
      "Iteration 2615, loss = 5.00809700\n",
      "Iteration 2616, loss = 5.69392407\n",
      "Iteration 2617, loss = 5.89565859\n",
      "Iteration 2618, loss = 5.80010783\n",
      "Iteration 2619, loss = 5.38502351\n",
      "Iteration 2620, loss = 9.86038304\n",
      "Iteration 2621, loss = 8.06400514\n",
      "Iteration 2622, loss = 19.92157959\n",
      "Iteration 2623, loss = 15.92078637\n",
      "Iteration 2624, loss = 12.48633170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2625, loss = 10.37247599\n",
      "Iteration 2626, loss = 8.51373535\n",
      "Iteration 2627, loss = 6.79047092\n",
      "Iteration 2628, loss = 6.22856643\n",
      "Iteration 2629, loss = 8.07715045\n",
      "Iteration 2630, loss = 6.95981648\n",
      "Iteration 2631, loss = 6.49040728\n",
      "Iteration 2632, loss = 6.26875288\n",
      "Iteration 2633, loss = 5.17159311\n",
      "Iteration 2634, loss = 5.94653920\n",
      "Iteration 2635, loss = 4.94605242\n",
      "Iteration 2636, loss = 5.70522661\n",
      "Iteration 2637, loss = 5.90410056\n",
      "Iteration 2638, loss = 5.73466541\n",
      "Iteration 2639, loss = 6.12661321\n",
      "Iteration 2640, loss = 5.45867777\n",
      "Iteration 2641, loss = 6.34706278\n",
      "Iteration 2642, loss = 7.64443035\n",
      "Iteration 2643, loss = 6.47868324\n",
      "Iteration 2644, loss = 5.65586049\n",
      "Iteration 2645, loss = 4.78740521\n",
      "Iteration 2646, loss = 8.29702048\n",
      "Iteration 2647, loss = 7.58820682\n",
      "Iteration 2648, loss = 8.83556366\n",
      "Iteration 2649, loss = 7.83025616\n",
      "Iteration 2650, loss = 6.42092943\n",
      "Iteration 2651, loss = 7.42358336\n",
      "Iteration 2652, loss = 5.53176519\n",
      "Iteration 2653, loss = 5.24299829\n",
      "Iteration 2654, loss = 5.55802593\n",
      "Iteration 2655, loss = 6.99095720\n",
      "Iteration 2656, loss = 6.51746978\n",
      "Iteration 2657, loss = 7.53323799\n",
      "Iteration 2658, loss = 5.21086004\n",
      "Iteration 2659, loss = 5.35194976\n",
      "Iteration 2660, loss = 5.17982745\n",
      "Iteration 2661, loss = 5.87410635\n",
      "Iteration 2662, loss = 5.80097502\n",
      "Iteration 2663, loss = 6.26734483\n",
      "Iteration 2664, loss = 7.14387043\n",
      "Iteration 2665, loss = 7.05392739\n",
      "Iteration 2666, loss = 5.66146597\n",
      "Iteration 2667, loss = 6.60088820\n",
      "Iteration 2668, loss = 7.24807700\n",
      "Iteration 2669, loss = 7.66878630\n",
      "Iteration 2670, loss = 5.86553430\n",
      "Iteration 2671, loss = 6.46417106\n",
      "Iteration 2672, loss = 5.44979219\n",
      "Iteration 2673, loss = 5.44445915\n",
      "Iteration 2674, loss = 5.12486030\n",
      "Iteration 2675, loss = 5.89066030\n",
      "Iteration 2676, loss = 5.85630250\n",
      "Iteration 2677, loss = 6.54185321\n",
      "Iteration 2678, loss = 5.14650355\n",
      "Iteration 2679, loss = 5.33599358\n",
      "Iteration 2680, loss = 6.94328971\n",
      "Iteration 2681, loss = 6.37570822\n",
      "Iteration 2682, loss = 8.40498199\n",
      "Iteration 2683, loss = 7.19270339\n",
      "Iteration 2684, loss = 5.14989447\n",
      "Iteration 2685, loss = 5.41385932\n",
      "Iteration 2686, loss = 6.00753669\n",
      "Iteration 2687, loss = 5.68306063\n",
      "Iteration 2688, loss = 4.99397135\n",
      "Iteration 2689, loss = 5.10950603\n",
      "Iteration 2690, loss = 7.52837196\n",
      "Iteration 2691, loss = 5.28892530\n",
      "Iteration 2692, loss = 5.16106482\n",
      "Iteration 2693, loss = 5.20003082\n",
      "Iteration 2694, loss = 4.87181814\n",
      "Iteration 2695, loss = 5.21772698\n",
      "Iteration 2696, loss = 6.19117053\n",
      "Iteration 2697, loss = 8.11696563\n",
      "Iteration 2698, loss = 9.62430766\n",
      "Iteration 2699, loss = 8.22596027\n",
      "Iteration 2700, loss = 4.45643727\n",
      "Iteration 2701, loss = 5.49599750\n",
      "Iteration 2702, loss = 6.06973449\n",
      "Iteration 2703, loss = 8.28665488\n",
      "Iteration 2704, loss = 5.83891535\n",
      "Iteration 2705, loss = 6.90483267\n",
      "Iteration 2706, loss = 8.07638734\n",
      "Iteration 2707, loss = 6.49085866\n",
      "Iteration 2708, loss = 7.74096888\n",
      "Iteration 2709, loss = 7.54585365\n",
      "Iteration 2710, loss = 8.51527817\n",
      "Iteration 2711, loss = 7.70505667\n",
      "Iteration 2712, loss = 6.10251576\n",
      "Iteration 2713, loss = 5.12167972\n",
      "Iteration 2714, loss = 5.37465127\n",
      "Iteration 2715, loss = 5.13042682\n",
      "Iteration 2716, loss = 4.67228066\n",
      "Iteration 2717, loss = 4.65479544\n",
      "Iteration 2718, loss = 6.47454247\n",
      "Iteration 2719, loss = 8.02866790\n",
      "Iteration 2720, loss = 8.12393974\n",
      "Iteration 2721, loss = 9.20680275\n",
      "Iteration 2722, loss = 9.26035027\n",
      "Iteration 2723, loss = 8.61636023\n",
      "Iteration 2724, loss = 8.90930358\n",
      "Iteration 2725, loss = 8.83278450\n",
      "Iteration 2726, loss = 6.83027604\n",
      "Iteration 2727, loss = 6.90893496\n",
      "Iteration 2728, loss = 4.83194082\n",
      "Iteration 2729, loss = 4.69667313\n",
      "Iteration 2730, loss = 5.00301403\n",
      "Iteration 2731, loss = 4.63423666\n",
      "Iteration 2732, loss = 4.65784461\n",
      "Iteration 2733, loss = 5.35807560\n",
      "Iteration 2734, loss = 8.59920377\n",
      "Iteration 2735, loss = 7.19613974\n",
      "Iteration 2736, loss = 6.66555264\n",
      "Iteration 2737, loss = 9.70127252\n",
      "Iteration 2738, loss = 7.11397278\n",
      "Iteration 2739, loss = 6.72065410\n",
      "Iteration 2740, loss = 5.64904362\n",
      "Iteration 2741, loss = 5.95670448\n",
      "Iteration 2742, loss = 7.98627166\n",
      "Iteration 2743, loss = 5.81930022\n",
      "Iteration 2744, loss = 5.75788571\n",
      "Iteration 2745, loss = 6.45238129\n",
      "Iteration 2746, loss = 5.47087952\n",
      "Iteration 2747, loss = 5.16320852\n",
      "Iteration 2748, loss = 9.29157322\n",
      "Iteration 2749, loss = 9.63495386\n",
      "Iteration 2750, loss = 7.72924527\n",
      "Iteration 2751, loss = 8.08746348\n",
      "Iteration 2752, loss = 8.87898372\n",
      "Iteration 2753, loss = 8.01787867\n",
      "Iteration 2754, loss = 6.71441503\n",
      "Iteration 2755, loss = 5.99971219\n",
      "Iteration 2756, loss = 7.25181260\n",
      "Iteration 2757, loss = 7.20392528\n",
      "Iteration 2758, loss = 6.59538192\n",
      "Iteration 2759, loss = 6.53131174\n",
      "Iteration 2760, loss = 5.75918556\n",
      "Iteration 2761, loss = 5.04622367\n",
      "Iteration 2762, loss = 5.29676607\n",
      "Iteration 2763, loss = 6.03353980\n",
      "Iteration 2764, loss = 8.15119891\n",
      "Iteration 2765, loss = 7.02352584\n",
      "Iteration 2766, loss = 5.09280692\n",
      "Iteration 2767, loss = 12.24650694\n",
      "Iteration 2768, loss = 6.81454567\n",
      "Iteration 2769, loss = 7.30252863\n",
      "Iteration 2770, loss = 6.54995228\n",
      "Iteration 2771, loss = 5.78091572\n",
      "Iteration 2772, loss = 5.91999184\n",
      "Iteration 2773, loss = 6.79872155\n",
      "Iteration 2774, loss = 5.63139819\n",
      "Iteration 2775, loss = 7.54929957\n",
      "Iteration 2776, loss = 5.65930928\n",
      "Iteration 2777, loss = 6.14070361\n",
      "Iteration 2778, loss = 5.80208905\n",
      "Iteration 2779, loss = 6.15812859\n",
      "Iteration 2780, loss = 6.12758635\n",
      "Iteration 2781, loss = 5.32317042\n",
      "Iteration 2782, loss = 9.29025612\n",
      "Iteration 2783, loss = 4.98084838\n",
      "Iteration 2784, loss = 6.71873569\n",
      "Iteration 2785, loss = 6.75905074\n",
      "Iteration 2786, loss = 5.43404570\n",
      "Iteration 2787, loss = 5.32135570\n",
      "Iteration 2788, loss = 5.58965165\n",
      "Iteration 2789, loss = 6.40221865\n",
      "Iteration 2790, loss = 7.71505359\n",
      "Iteration 2791, loss = 6.61611017\n",
      "Iteration 2792, loss = 5.50093054\n",
      "Iteration 2793, loss = 5.68242530\n",
      "Iteration 2794, loss = 6.84167291\n",
      "Iteration 2795, loss = 6.20493604\n",
      "Iteration 2796, loss = 8.51638144\n",
      "Iteration 2797, loss = 7.59257246\n",
      "Iteration 2798, loss = 5.77785016\n",
      "Iteration 2799, loss = 6.11456421\n",
      "Iteration 2800, loss = 5.73948062\n",
      "Iteration 2801, loss = 4.97106989\n",
      "Iteration 2802, loss = 4.74276733\n",
      "Iteration 2803, loss = 5.28865472\n",
      "Iteration 2804, loss = 5.30409594\n",
      "Iteration 2805, loss = 6.12017638\n",
      "Iteration 2806, loss = 6.34469676\n",
      "Iteration 2807, loss = 6.04763225\n",
      "Iteration 2808, loss = 8.92249944\n",
      "Iteration 2809, loss = 10.71354043\n",
      "Iteration 2810, loss = 6.40547768\n",
      "Iteration 2811, loss = 6.88652159\n",
      "Iteration 2812, loss = 6.76621700\n",
      "Iteration 2813, loss = 9.25753545\n",
      "Iteration 2814, loss = 8.48575271\n",
      "Iteration 2815, loss = 7.38535895\n",
      "Iteration 2816, loss = 9.70553494\n",
      "Iteration 2817, loss = 6.54295824\n",
      "Iteration 2818, loss = 6.31244196\n",
      "Iteration 2819, loss = 5.28572644\n",
      "Iteration 2820, loss = 5.06332310\n",
      "Iteration 2821, loss = 5.64283335\n",
      "Iteration 2822, loss = 5.65601115\n",
      "Iteration 2823, loss = 5.55032862\n",
      "Iteration 2824, loss = 5.15021153\n",
      "Iteration 2825, loss = 6.21773685\n",
      "Iteration 2826, loss = 7.34225382\n",
      "Iteration 2827, loss = 6.13881351\n",
      "Iteration 2828, loss = 6.07720879\n",
      "Iteration 2829, loss = 6.16683480\n",
      "Iteration 2830, loss = 5.15960976\n",
      "Iteration 2831, loss = 5.17738216\n",
      "Iteration 2832, loss = 4.93000033\n",
      "Iteration 2833, loss = 7.75439893\n",
      "Iteration 2834, loss = 5.98546287\n",
      "Iteration 2835, loss = 6.11923205\n",
      "Iteration 2836, loss = 6.08887500\n",
      "Iteration 2837, loss = 7.08570230\n",
      "Iteration 2838, loss = 7.34574526\n",
      "Iteration 2839, loss = 7.28616390\n",
      "Iteration 2840, loss = 5.87865975\n",
      "Iteration 2841, loss = 5.04873882\n",
      "Iteration 2842, loss = 5.65517599\n",
      "Iteration 2843, loss = 5.51382389\n",
      "Iteration 2844, loss = 5.48108654\n",
      "Iteration 2845, loss = 4.59689978\n",
      "Iteration 2846, loss = 4.91947671\n",
      "Iteration 2847, loss = 5.44889033\n",
      "Iteration 2848, loss = 5.07143647\n",
      "Iteration 2849, loss = 4.70696777\n",
      "Iteration 2850, loss = 5.10670693\n",
      "Iteration 2851, loss = 4.65944190\n",
      "Iteration 2852, loss = 6.44421379\n",
      "Iteration 2853, loss = 6.70494396\n",
      "Iteration 2854, loss = 4.87994357\n",
      "Iteration 2855, loss = 5.08643097\n",
      "Iteration 2856, loss = 7.56357452\n",
      "Iteration 2857, loss = 6.09134449\n",
      "Iteration 2858, loss = 6.21319431\n",
      "Iteration 2859, loss = 7.01844711\n",
      "Iteration 2860, loss = 6.14502081\n",
      "Iteration 2861, loss = 6.16305931\n",
      "Iteration 2862, loss = 6.23487263\n",
      "Iteration 2863, loss = 4.98708098\n",
      "Iteration 2864, loss = 5.19186822\n",
      "Iteration 2865, loss = 5.42671934\n",
      "Iteration 2866, loss = 5.18460640\n",
      "Iteration 2867, loss = 4.77773625\n",
      "Iteration 2868, loss = 7.97130939\n",
      "Iteration 2869, loss = 12.55657514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2870, loss = 10.52802423\n",
      "Iteration 2871, loss = 6.46975419\n",
      "Iteration 2872, loss = 5.23267876\n",
      "Iteration 2873, loss = 7.51452087\n",
      "Iteration 2874, loss = 8.08876095\n",
      "Iteration 2875, loss = 6.18035788\n",
      "Iteration 2876, loss = 5.23642304\n",
      "Iteration 2877, loss = 5.65957420\n",
      "Iteration 2878, loss = 6.58991320\n",
      "Iteration 2879, loss = 9.37193082\n",
      "Iteration 2880, loss = 6.22722340\n",
      "Iteration 2881, loss = 12.62242101\n",
      "Iteration 2882, loss = 6.98850031\n",
      "Iteration 2883, loss = 5.64770272\n",
      "Iteration 2884, loss = 5.55313900\n",
      "Iteration 2885, loss = 6.35200986\n",
      "Iteration 2886, loss = 4.91059640\n",
      "Iteration 2887, loss = 4.44601356\n",
      "Iteration 2888, loss = 4.69157627\n",
      "Iteration 2889, loss = 4.89541124\n",
      "Iteration 2890, loss = 5.06248157\n",
      "Iteration 2891, loss = 5.04496296\n",
      "Iteration 2892, loss = 4.63683685\n",
      "Iteration 2893, loss = 4.72293506\n",
      "Iteration 2894, loss = 5.24911476\n",
      "Iteration 2895, loss = 4.96141198\n",
      "Iteration 2896, loss = 5.40226421\n",
      "Iteration 2897, loss = 4.73544524\n",
      "Iteration 2898, loss = 4.74138244\n",
      "Iteration 2899, loss = 6.01586327\n",
      "Iteration 2900, loss = 5.29331861\n",
      "Iteration 2901, loss = 5.12756299\n",
      "Iteration 2902, loss = 5.73097371\n",
      "Iteration 2903, loss = 5.10986061\n",
      "Iteration 2904, loss = 5.24211921\n",
      "Iteration 2905, loss = 6.09698316\n",
      "Iteration 2906, loss = 8.75914821\n",
      "Iteration 2907, loss = 5.79489197\n",
      "Iteration 2908, loss = 4.92801429\n",
      "Iteration 2909, loss = 4.50769494\n",
      "Iteration 2910, loss = 4.55253836\n",
      "Iteration 2911, loss = 5.39195794\n",
      "Iteration 2912, loss = 5.59936686\n",
      "Iteration 2913, loss = 7.69079269\n",
      "Iteration 2914, loss = 5.92523312\n",
      "Iteration 2915, loss = 4.63643213\n",
      "Iteration 2916, loss = 10.65422693\n",
      "Iteration 2917, loss = 6.35743642\n",
      "Iteration 2918, loss = 8.46181990\n",
      "Iteration 2919, loss = 6.63880830\n",
      "Iteration 2920, loss = 5.79543231\n",
      "Iteration 2921, loss = 4.73834855\n",
      "Iteration 2922, loss = 5.12021611\n",
      "Iteration 2923, loss = 5.13187951\n",
      "Iteration 2924, loss = 4.58615919\n",
      "Iteration 2925, loss = 4.63499460\n",
      "Iteration 2926, loss = 5.69777122\n",
      "Iteration 2927, loss = 5.47961409\n",
      "Iteration 2928, loss = 6.77708019\n",
      "Iteration 2929, loss = 7.88271826\n",
      "Iteration 2930, loss = 8.20244932\n",
      "Iteration 2931, loss = 6.88070117\n",
      "Iteration 2932, loss = 5.77570875\n",
      "Iteration 2933, loss = 6.98939252\n",
      "Iteration 2934, loss = 5.37257623\n",
      "Iteration 2935, loss = 6.22023840\n",
      "Iteration 2936, loss = 5.10987553\n",
      "Iteration 2937, loss = 5.17457254\n",
      "Iteration 2938, loss = 4.80136170\n",
      "Iteration 2939, loss = 5.32823491\n",
      "Iteration 2940, loss = 4.72609001\n",
      "Iteration 2941, loss = 5.61170314\n",
      "Iteration 2942, loss = 5.57608172\n",
      "Iteration 2943, loss = 6.37141779\n",
      "Iteration 2944, loss = 5.24787946\n",
      "Iteration 2945, loss = 5.44290584\n",
      "Iteration 2946, loss = 5.23930485\n",
      "Iteration 2947, loss = 6.04986971\n",
      "Iteration 2948, loss = 6.17925773\n",
      "Iteration 2949, loss = 7.67820695\n",
      "Iteration 2950, loss = 4.73111230\n",
      "Iteration 2951, loss = 5.27200469\n",
      "Iteration 2952, loss = 7.04492674\n",
      "Iteration 2953, loss = 6.60018506\n",
      "Iteration 2954, loss = 5.42156549\n",
      "Iteration 2955, loss = 6.29166252\n",
      "Iteration 2956, loss = 5.15398127\n",
      "Iteration 2957, loss = 5.57538341\n",
      "Iteration 2958, loss = 4.84007760\n",
      "Iteration 2959, loss = 5.04114054\n",
      "Iteration 2960, loss = 6.76347264\n",
      "Iteration 2961, loss = 8.89169119\n",
      "Iteration 2962, loss = 6.40977542\n",
      "Iteration 2963, loss = 6.04086002\n",
      "Iteration 2964, loss = 5.02738328\n",
      "Iteration 2965, loss = 5.81761722\n",
      "Iteration 2966, loss = 5.50172289\n",
      "Iteration 2967, loss = 4.80373159\n",
      "Iteration 2968, loss = 4.89727716\n",
      "Iteration 2969, loss = 5.04446875\n",
      "Iteration 2970, loss = 5.19643630\n",
      "Iteration 2971, loss = 4.68179379\n",
      "Iteration 2972, loss = 4.69584824\n",
      "Iteration 2973, loss = 5.06998130\n",
      "Iteration 2974, loss = 5.70283186\n",
      "Iteration 2975, loss = 5.25046047\n",
      "Iteration 2976, loss = 12.74858839\n",
      "Iteration 2977, loss = 8.31712683\n",
      "Iteration 2978, loss = 5.06574984\n",
      "Iteration 2979, loss = 5.36544426\n",
      "Iteration 2980, loss = 6.00011010\n",
      "Iteration 2981, loss = 6.66099761\n",
      "Iteration 2982, loss = 5.14759384\n",
      "Iteration 2983, loss = 4.37356346\n",
      "Iteration 2984, loss = 5.11493896\n",
      "Iteration 2985, loss = 5.30575454\n",
      "Iteration 2986, loss = 5.36207988\n",
      "Iteration 2987, loss = 5.70791331\n",
      "Iteration 2988, loss = 4.75178415\n",
      "Iteration 2989, loss = 4.36295913\n",
      "Iteration 2990, loss = 4.60945137\n",
      "Iteration 2991, loss = 6.09447663\n",
      "Iteration 2992, loss = 6.64809562\n",
      "Iteration 2993, loss = 7.19400426\n",
      "Iteration 2994, loss = 6.07661101\n",
      "Iteration 2995, loss = 5.26124399\n",
      "Iteration 2996, loss = 5.77997086\n",
      "Iteration 2997, loss = 6.88805517\n",
      "Iteration 2998, loss = 7.65582035\n",
      "Iteration 2999, loss = 4.69032264\n",
      "Iteration 3000, loss = 4.87388452\n",
      "Iteration 3001, loss = 8.21076511\n",
      "Iteration 3002, loss = 7.52449996\n",
      "Iteration 3003, loss = 8.14872036\n",
      "Iteration 3004, loss = 7.06316908\n",
      "Iteration 3005, loss = 6.71473816\n",
      "Iteration 3006, loss = 5.22879340\n",
      "Iteration 3007, loss = 6.54773269\n",
      "Iteration 3008, loss = 13.33810555\n",
      "Iteration 3009, loss = 11.07555762\n",
      "Iteration 3010, loss = 23.57847278\n",
      "Iteration 3011, loss = 25.46695599\n",
      "Iteration 3012, loss = 12.25321278\n",
      "Iteration 3013, loss = 12.56047542\n",
      "Iteration 3014, loss = 11.03206096\n",
      "Iteration 3015, loss = 9.80774124\n",
      "Iteration 3016, loss = 6.95456047\n",
      "Iteration 3017, loss = 4.65868964\n",
      "Iteration 3018, loss = 6.58851802\n",
      "Iteration 3019, loss = 6.02475782\n",
      "Iteration 3020, loss = 6.46268222\n",
      "Iteration 3021, loss = 5.31355677\n",
      "Iteration 3022, loss = 5.11213154\n",
      "Iteration 3023, loss = 5.12128480\n",
      "Iteration 3024, loss = 9.05127360\n",
      "Iteration 3025, loss = 8.18026029\n",
      "Iteration 3026, loss = 8.82916526\n",
      "Iteration 3027, loss = 7.08742971\n",
      "Iteration 3028, loss = 7.27760794\n",
      "Iteration 3029, loss = 5.66376375\n",
      "Iteration 3030, loss = 5.71326343\n",
      "Iteration 3031, loss = 5.28276255\n",
      "Iteration 3032, loss = 5.04035253\n",
      "Iteration 3033, loss = 5.30398741\n",
      "Iteration 3034, loss = 5.67683174\n",
      "Iteration 3035, loss = 4.58826647\n",
      "Iteration 3036, loss = 4.88677907\n",
      "Iteration 3037, loss = 5.10993064\n",
      "Iteration 3038, loss = 5.16498691\n",
      "Iteration 3039, loss = 5.43352378\n",
      "Iteration 3040, loss = 4.77603102\n",
      "Iteration 3041, loss = 4.60957290\n",
      "Iteration 3042, loss = 5.76987248\n",
      "Iteration 3043, loss = 5.27561469\n",
      "Iteration 3044, loss = 5.35228256\n",
      "Iteration 3045, loss = 5.06054458\n",
      "Iteration 3046, loss = 4.56844284\n",
      "Iteration 3047, loss = 4.36050349\n",
      "Iteration 3048, loss = 4.69699203\n",
      "Iteration 3049, loss = 4.54684165\n",
      "Iteration 3050, loss = 5.66749860\n",
      "Iteration 3051, loss = 5.02033781\n",
      "Iteration 3052, loss = 4.39541598\n",
      "Iteration 3053, loss = 4.98533280\n",
      "Iteration 3054, loss = 6.04955418\n",
      "Iteration 3055, loss = 5.96086489\n",
      "Iteration 3056, loss = 6.74577509\n",
      "Iteration 3057, loss = 8.74910801\n",
      "Iteration 3058, loss = 9.80353072\n",
      "Iteration 3059, loss = 5.37031894\n",
      "Iteration 3060, loss = 4.44482462\n",
      "Iteration 3061, loss = 4.44811993\n",
      "Iteration 3062, loss = 4.26303074\n",
      "Iteration 3063, loss = 4.57921665\n",
      "Iteration 3064, loss = 7.83880801\n",
      "Iteration 3065, loss = 8.18049109\n",
      "Iteration 3066, loss = 8.66102432\n",
      "Iteration 3067, loss = 13.03446561\n",
      "Iteration 3068, loss = 10.62898290\n",
      "Iteration 3069, loss = 8.16663417\n",
      "Iteration 3070, loss = 6.90765454\n",
      "Iteration 3071, loss = 6.22524955\n",
      "Iteration 3072, loss = 6.28733058\n",
      "Iteration 3073, loss = 5.36969475\n",
      "Iteration 3074, loss = 5.07793132\n",
      "Iteration 3075, loss = 5.15177909\n",
      "Iteration 3076, loss = 4.82618401\n",
      "Iteration 3077, loss = 5.19149390\n",
      "Iteration 3078, loss = 6.85703133\n",
      "Iteration 3079, loss = 6.49172542\n",
      "Iteration 3080, loss = 5.82281703\n",
      "Iteration 3081, loss = 4.75645898\n",
      "Iteration 3082, loss = 5.02652161\n",
      "Iteration 3083, loss = 4.95159924\n",
      "Iteration 3084, loss = 4.71672651\n",
      "Iteration 3085, loss = 5.18488205\n",
      "Iteration 3086, loss = 5.15246491\n",
      "Iteration 3087, loss = 4.59526603\n",
      "Iteration 3088, loss = 4.83178274\n",
      "Iteration 3089, loss = 4.48852298\n",
      "Iteration 3090, loss = 4.41851222\n",
      "Iteration 3091, loss = 4.83672582\n",
      "Iteration 3092, loss = 6.57049017\n",
      "Iteration 3093, loss = 7.76436700\n",
      "Iteration 3094, loss = 5.41102651\n",
      "Iteration 3095, loss = 5.15900481\n",
      "Iteration 3096, loss = 6.04162228\n",
      "Iteration 3097, loss = 5.06699679\n",
      "Iteration 3098, loss = 9.55050637\n",
      "Iteration 3099, loss = 7.02844249\n",
      "Iteration 3100, loss = 5.58169501\n",
      "Iteration 3101, loss = 4.98429914\n",
      "Iteration 3102, loss = 5.94862746\n",
      "Iteration 3103, loss = 6.69835791\n",
      "Iteration 3104, loss = 8.71842467\n",
      "Iteration 3105, loss = 6.25079422\n",
      "Iteration 3106, loss = 5.14119187\n",
      "Iteration 3107, loss = 4.80692471\n",
      "Iteration 3108, loss = 6.25900440\n",
      "Iteration 3109, loss = 4.50844283\n",
      "Iteration 3110, loss = 4.74257721\n",
      "Iteration 3111, loss = 5.23280520\n",
      "Iteration 3112, loss = 5.07360361\n",
      "Iteration 3113, loss = 5.91482121\n",
      "Iteration 3114, loss = 4.92212142\n",
      "Iteration 3115, loss = 6.32201546\n",
      "Iteration 3116, loss = 4.91340827\n",
      "Iteration 3117, loss = 5.23407078\n",
      "Iteration 3118, loss = 5.36620435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3119, loss = 4.80675975\n",
      "Iteration 3120, loss = 5.17263359\n",
      "Iteration 3121, loss = 5.42482672\n",
      "Iteration 3122, loss = 5.11660496\n",
      "Iteration 3123, loss = 4.92849775\n",
      "Iteration 3124, loss = 5.29506787\n",
      "Iteration 3125, loss = 4.99068073\n",
      "Iteration 3126, loss = 7.17219181\n",
      "Iteration 3127, loss = 5.67580936\n",
      "Iteration 3128, loss = 4.75720425\n",
      "Iteration 3129, loss = 4.34719385\n",
      "Iteration 3130, loss = 8.48029264\n",
      "Iteration 3131, loss = 5.63457255\n",
      "Iteration 3132, loss = 5.01020977\n",
      "Iteration 3133, loss = 5.92661465\n",
      "Iteration 3134, loss = 5.19198482\n",
      "Iteration 3135, loss = 5.52437990\n",
      "Iteration 3136, loss = 5.71012220\n",
      "Iteration 3137, loss = 5.48283566\n",
      "Iteration 3138, loss = 11.75286626\n",
      "Iteration 3139, loss = 9.21836705\n",
      "Iteration 3140, loss = 7.79752702\n",
      "Iteration 3141, loss = 4.98823067\n",
      "Iteration 3142, loss = 7.38533864\n",
      "Iteration 3143, loss = 5.43326281\n",
      "Iteration 3144, loss = 6.17303500\n",
      "Iteration 3145, loss = 6.20031414\n",
      "Iteration 3146, loss = 6.39717805\n",
      "Iteration 3147, loss = 5.08819039\n",
      "Iteration 3148, loss = 5.38173123\n",
      "Iteration 3149, loss = 4.73081354\n",
      "Iteration 3150, loss = 5.58270534\n",
      "Iteration 3151, loss = 4.97759058\n",
      "Iteration 3152, loss = 5.11550552\n",
      "Iteration 3153, loss = 5.83720047\n",
      "Iteration 3154, loss = 6.70113385\n",
      "Iteration 3155, loss = 7.47058740\n",
      "Iteration 3156, loss = 6.97009947\n",
      "Iteration 3157, loss = 5.67649171\n",
      "Iteration 3158, loss = 4.90975560\n",
      "Iteration 3159, loss = 4.48182970\n",
      "Iteration 3160, loss = 4.71617070\n",
      "Iteration 3161, loss = 4.77431821\n",
      "Iteration 3162, loss = 5.39253480\n",
      "Iteration 3163, loss = 5.58206259\n",
      "Iteration 3164, loss = 5.23390074\n",
      "Iteration 3165, loss = 5.46580711\n",
      "Iteration 3166, loss = 6.84632834\n",
      "Iteration 3167, loss = 6.45154356\n",
      "Iteration 3168, loss = 6.26249832\n",
      "Iteration 3169, loss = 4.93810579\n",
      "Iteration 3170, loss = 5.27021984\n",
      "Iteration 3171, loss = 5.27801458\n",
      "Iteration 3172, loss = 4.89137468\n",
      "Iteration 3173, loss = 5.80468930\n",
      "Iteration 3174, loss = 5.85092043\n",
      "Iteration 3175, loss = 6.35323708\n",
      "Iteration 3176, loss = 4.74943452\n",
      "Iteration 3177, loss = 8.82398952\n",
      "Iteration 3178, loss = 7.14740168\n",
      "Iteration 3179, loss = 5.57311850\n",
      "Iteration 3180, loss = 6.70026395\n",
      "Iteration 3181, loss = 5.80371255\n",
      "Iteration 3182, loss = 4.72060500\n",
      "Iteration 3183, loss = 5.30922565\n",
      "Iteration 3184, loss = 4.53857060\n",
      "Iteration 3185, loss = 4.92269054\n",
      "Iteration 3186, loss = 4.95819256\n",
      "Iteration 3187, loss = 5.45582766\n",
      "Iteration 3188, loss = 4.88918126\n",
      "Iteration 3189, loss = 4.49431403\n",
      "Iteration 3190, loss = 6.25848281\n",
      "Iteration 3191, loss = 8.87541438\n",
      "Iteration 3192, loss = 7.19322641\n",
      "Iteration 3193, loss = 5.23792859\n",
      "Iteration 3194, loss = 5.44769127\n",
      "Iteration 3195, loss = 7.19029484\n",
      "Iteration 3196, loss = 5.23168404\n",
      "Iteration 3197, loss = 6.96292377\n",
      "Iteration 3198, loss = 6.01731634\n",
      "Iteration 3199, loss = 7.04153890\n",
      "Iteration 3200, loss = 6.32737966\n",
      "Iteration 3201, loss = 6.07248838\n",
      "Iteration 3202, loss = 7.45293433\n",
      "Iteration 3203, loss = 5.87145180\n",
      "Iteration 3204, loss = 6.01224052\n",
      "Iteration 3205, loss = 6.15464339\n",
      "Iteration 3206, loss = 6.05409510\n",
      "Iteration 3207, loss = 5.32785038\n",
      "Iteration 3208, loss = 5.96302701\n",
      "Iteration 3209, loss = 5.23026111\n",
      "Iteration 3210, loss = 5.35371963\n",
      "Iteration 3211, loss = 5.03265736\n",
      "Iteration 3212, loss = 4.35668312\n",
      "Iteration 3213, loss = 4.59674546\n",
      "Iteration 3214, loss = 5.25507028\n",
      "Iteration 3215, loss = 4.73662466\n",
      "Iteration 3216, loss = 4.84694898\n",
      "Iteration 3217, loss = 4.62354062\n",
      "Iteration 3218, loss = 4.57303598\n",
      "Iteration 3219, loss = 4.50981561\n",
      "Iteration 3220, loss = 5.00577815\n",
      "Iteration 3221, loss = 5.72894244\n",
      "Iteration 3222, loss = 5.12956128\n",
      "Iteration 3223, loss = 4.53158491\n",
      "Iteration 3224, loss = 5.15262357\n",
      "Iteration 3225, loss = 5.33011144\n",
      "Iteration 3226, loss = 4.78580560\n",
      "Iteration 3227, loss = 4.95570305\n",
      "Iteration 3228, loss = 7.00398048\n",
      "Iteration 3229, loss = 5.77251104\n",
      "Iteration 3230, loss = 6.14494529\n",
      "Iteration 3231, loss = 8.08414035\n",
      "Iteration 3232, loss = 9.26945787\n",
      "Iteration 3233, loss = 6.25756800\n",
      "Iteration 3234, loss = 6.28941221\n",
      "Iteration 3235, loss = 5.02124701\n",
      "Iteration 3236, loss = 4.69869871\n",
      "Iteration 3237, loss = 4.59627671\n",
      "Iteration 3238, loss = 4.53240669\n",
      "Iteration 3239, loss = 5.30681800\n",
      "Iteration 3240, loss = 5.20657736\n",
      "Iteration 3241, loss = 7.00800271\n",
      "Iteration 3242, loss = 6.80875496\n",
      "Iteration 3243, loss = 5.06634202\n",
      "Iteration 3244, loss = 4.98636102\n",
      "Iteration 3245, loss = 8.69834625\n",
      "Iteration 3246, loss = 8.24188178\n",
      "Iteration 3247, loss = 11.06132425\n",
      "Iteration 3248, loss = 8.49194193\n",
      "Iteration 3249, loss = 8.92914199\n",
      "Iteration 3250, loss = 10.69029653\n",
      "Iteration 3251, loss = 7.29253887\n",
      "Iteration 3252, loss = 10.81627333\n",
      "Iteration 3253, loss = 10.85863934\n",
      "Iteration 3254, loss = 8.79946705\n",
      "Iteration 3255, loss = 7.90701655\n",
      "Iteration 3256, loss = 6.57824610\n",
      "Iteration 3257, loss = 8.32574322\n",
      "Iteration 3258, loss = 6.48646884\n",
      "Iteration 3259, loss = 4.73621467\n",
      "Iteration 3260, loss = 4.97266483\n",
      "Iteration 3261, loss = 4.63397565\n",
      "Iteration 3262, loss = 4.88336794\n",
      "Iteration 3263, loss = 6.70922389\n",
      "Iteration 3264, loss = 6.32274911\n",
      "Iteration 3265, loss = 4.85939554\n",
      "Iteration 3266, loss = 5.44623366\n",
      "Iteration 3267, loss = 4.52990418\n",
      "Iteration 3268, loss = 5.06222324\n",
      "Iteration 3269, loss = 5.65370615\n",
      "Iteration 3270, loss = 10.73442494\n",
      "Iteration 3271, loss = 4.40307811\n",
      "Iteration 3272, loss = 5.42121366\n",
      "Iteration 3273, loss = 5.06934706\n",
      "Iteration 3274, loss = 4.70072963\n",
      "Iteration 3275, loss = 5.18443781\n",
      "Iteration 3276, loss = 4.83179765\n",
      "Iteration 3277, loss = 6.99612425\n",
      "Iteration 3278, loss = 6.07574916\n",
      "Iteration 3279, loss = 5.49965356\n",
      "Iteration 3280, loss = 5.05351389\n",
      "Iteration 3281, loss = 4.95463489\n",
      "Iteration 3282, loss = 5.45762527\n",
      "Iteration 3283, loss = 4.53261442\n",
      "Iteration 3284, loss = 5.57603884\n",
      "Iteration 3285, loss = 5.08619505\n",
      "Iteration 3286, loss = 5.30791714\n",
      "Iteration 3287, loss = 5.20137511\n",
      "Iteration 3288, loss = 4.75175282\n",
      "Iteration 3289, loss = 5.18370202\n",
      "Iteration 3290, loss = 5.04344341\n",
      "Iteration 3291, loss = 4.85116980\n",
      "Iteration 3292, loss = 5.80551026\n",
      "Iteration 3293, loss = 5.93133441\n",
      "Iteration 3294, loss = 5.18468282\n",
      "Iteration 3295, loss = 4.84671964\n",
      "Iteration 3296, loss = 5.31357807\n",
      "Iteration 3297, loss = 5.33544591\n",
      "Iteration 3298, loss = 5.68410006\n",
      "Iteration 3299, loss = 5.72480271\n",
      "Iteration 3300, loss = 5.49936205\n",
      "Iteration 3301, loss = 5.05079042\n",
      "Iteration 3302, loss = 5.01583503\n",
      "Iteration 3303, loss = 6.04165930\n",
      "Iteration 3304, loss = 7.89155487\n",
      "Iteration 3305, loss = 5.36524794\n",
      "Iteration 3306, loss = 6.43676530\n",
      "Iteration 3307, loss = 7.26200079\n",
      "Iteration 3308, loss = 5.69338917\n",
      "Iteration 3309, loss = 6.82668975\n",
      "Iteration 3310, loss = 6.54881757\n",
      "Iteration 3311, loss = 9.72769620\n",
      "Iteration 3312, loss = 9.87727802\n",
      "Iteration 3313, loss = 8.67883433\n",
      "Iteration 3314, loss = 17.31340867\n",
      "Iteration 3315, loss = 12.70473157\n",
      "Iteration 3316, loss = 14.50549412\n",
      "Iteration 3317, loss = 16.48975268\n",
      "Iteration 3318, loss = 16.93745076\n",
      "Iteration 3319, loss = 10.60174805\n",
      "Iteration 3320, loss = 7.58459799\n",
      "Iteration 3321, loss = 6.44411274\n",
      "Iteration 3322, loss = 5.43814436\n",
      "Iteration 3323, loss = 5.30512719\n",
      "Iteration 3324, loss = 5.53451305\n",
      "Iteration 3325, loss = 5.65375674\n",
      "Iteration 3326, loss = 5.81202121\n",
      "Iteration 3327, loss = 5.35455156\n",
      "Iteration 3328, loss = 6.23000559\n",
      "Iteration 3329, loss = 9.62879338\n",
      "Iteration 3330, loss = 6.18829141\n",
      "Iteration 3331, loss = 4.73271173\n",
      "Iteration 3332, loss = 5.47543422\n",
      "Iteration 3333, loss = 4.15732327\n",
      "Iteration 3334, loss = 4.60166429\n",
      "Iteration 3335, loss = 5.76147445\n",
      "Iteration 3336, loss = 5.86589359\n",
      "Iteration 3337, loss = 5.34839731\n",
      "Iteration 3338, loss = 4.60654999\n",
      "Iteration 3339, loss = 7.38969079\n",
      "Iteration 3340, loss = 6.94620449\n",
      "Iteration 3341, loss = 5.88780006\n",
      "Iteration 3342, loss = 5.29316592\n",
      "Iteration 3343, loss = 4.84113457\n",
      "Iteration 3344, loss = 4.55736610\n",
      "Iteration 3345, loss = 6.02965502\n",
      "Iteration 3346, loss = 6.51450466\n",
      "Iteration 3347, loss = 5.07641754\n",
      "Iteration 3348, loss = 4.78743522\n",
      "Iteration 3349, loss = 5.65421799\n",
      "Iteration 3350, loss = 6.53714094\n",
      "Iteration 3351, loss = 5.44749001\n",
      "Iteration 3352, loss = 7.27772785\n",
      "Iteration 3353, loss = 8.14510360\n",
      "Iteration 3354, loss = 8.14139626\n",
      "Iteration 3355, loss = 6.33083445\n",
      "Iteration 3356, loss = 10.19399576\n",
      "Iteration 3357, loss = 7.28775240\n",
      "Iteration 3358, loss = 5.40112359\n",
      "Iteration 3359, loss = 5.12544036\n",
      "Iteration 3360, loss = 5.53732248\n",
      "Iteration 3361, loss = 6.09445806\n",
      "Iteration 3362, loss = 4.79446760\n",
      "Iteration 3363, loss = 5.58316376\n",
      "Iteration 3364, loss = 5.45803379\n",
      "Iteration 3365, loss = 7.80623079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3366, loss = 10.91381779\n",
      "Iteration 3367, loss = 10.19168478\n",
      "Iteration 3368, loss = 7.88850833\n",
      "Iteration 3369, loss = 8.29854957\n",
      "Iteration 3370, loss = 5.69154724\n",
      "Iteration 3371, loss = 5.54023206\n",
      "Iteration 3372, loss = 5.41575355\n",
      "Iteration 3373, loss = 5.68730423\n",
      "Iteration 3374, loss = 5.92594064\n",
      "Iteration 3375, loss = 7.58861926\n",
      "Iteration 3376, loss = 11.68033051\n",
      "Iteration 3377, loss = 7.29077269\n",
      "Iteration 3378, loss = 8.29483616\n",
      "Iteration 3379, loss = 8.11300741\n",
      "Iteration 3380, loss = 9.57278735\n",
      "Iteration 3381, loss = 5.75702796\n",
      "Iteration 3382, loss = 8.23684682\n",
      "Iteration 3383, loss = 6.37414061\n",
      "Iteration 3384, loss = 6.76223236\n",
      "Iteration 3385, loss = 7.69173375\n",
      "Iteration 3386, loss = 4.97463986\n",
      "Iteration 3387, loss = 5.00657058\n",
      "Iteration 3388, loss = 4.63451334\n",
      "Iteration 3389, loss = 4.58013196\n",
      "Iteration 3390, loss = 4.50139394\n",
      "Iteration 3391, loss = 4.38856945\n",
      "Iteration 3392, loss = 6.20231213\n",
      "Iteration 3393, loss = 5.45563573\n",
      "Iteration 3394, loss = 6.53493709\n",
      "Iteration 3395, loss = 10.28931418\n",
      "Iteration 3396, loss = 7.43804709\n",
      "Iteration 3397, loss = 7.42952575\n",
      "Iteration 3398, loss = 7.62714756\n",
      "Iteration 3399, loss = 9.76544053\n",
      "Iteration 3400, loss = 4.80658842\n",
      "Iteration 3401, loss = 5.89017230\n",
      "Iteration 3402, loss = 6.62993798\n",
      "Iteration 3403, loss = 5.71698830\n",
      "Iteration 3404, loss = 4.53644905\n",
      "Iteration 3405, loss = 4.36603729\n",
      "Iteration 3406, loss = 5.07315093\n",
      "Iteration 3407, loss = 4.33200226\n",
      "Iteration 3408, loss = 4.47821279\n",
      "Iteration 3409, loss = 5.33513831\n",
      "Iteration 3410, loss = 4.50555088\n",
      "Iteration 3411, loss = 5.59225613\n",
      "Iteration 3412, loss = 4.51536772\n",
      "Iteration 3413, loss = 5.12564908\n",
      "Iteration 3414, loss = 5.27447730\n",
      "Iteration 3415, loss = 5.73020707\n",
      "Iteration 3416, loss = 4.87603604\n",
      "Iteration 3417, loss = 4.80994873\n",
      "Iteration 3418, loss = 4.29363449\n",
      "Iteration 3419, loss = 4.44057101\n",
      "Iteration 3420, loss = 4.66632515\n",
      "Iteration 3421, loss = 5.85977027\n",
      "Iteration 3422, loss = 5.48490962\n",
      "Iteration 3423, loss = 6.51240243\n",
      "Iteration 3424, loss = 7.37963705\n",
      "Iteration 3425, loss = 6.35045465\n",
      "Iteration 3426, loss = 4.15531485\n",
      "Iteration 3427, loss = 7.25160457\n",
      "Iteration 3428, loss = 5.55926134\n",
      "Iteration 3429, loss = 5.31834217\n",
      "Iteration 3430, loss = 5.40669950\n",
      "Iteration 3431, loss = 4.83191658\n",
      "Iteration 3432, loss = 6.37036852\n",
      "Iteration 3433, loss = 4.64837854\n",
      "Iteration 3434, loss = 4.37455671\n",
      "Iteration 3435, loss = 5.34420648\n",
      "Iteration 3436, loss = 6.28900059\n",
      "Iteration 3437, loss = 6.24045936\n",
      "Iteration 3438, loss = 4.92298022\n",
      "Iteration 3439, loss = 4.55450179\n",
      "Iteration 3440, loss = 5.19275756\n",
      "Iteration 3441, loss = 5.63831801\n",
      "Iteration 3442, loss = 6.31687393\n",
      "Iteration 3443, loss = 4.75068292\n",
      "Iteration 3444, loss = 4.60455966\n",
      "Iteration 3445, loss = 5.07973489\n",
      "Iteration 3446, loss = 4.47468225\n",
      "Iteration 3447, loss = 7.57154859\n",
      "Iteration 3448, loss = 4.82686896\n",
      "Iteration 3449, loss = 5.22900398\n",
      "Iteration 3450, loss = 6.29995347\n",
      "Iteration 3451, loss = 6.39519752\n",
      "Iteration 3452, loss = 5.41958397\n",
      "Iteration 3453, loss = 6.08244040\n",
      "Iteration 3454, loss = 5.76059950\n",
      "Iteration 3455, loss = 5.16633436\n",
      "Iteration 3456, loss = 7.30442204\n",
      "Iteration 3457, loss = 5.15225018\n",
      "Iteration 3458, loss = 7.15975247\n",
      "Iteration 3459, loss = 6.51105679\n",
      "Iteration 3460, loss = 6.24465933\n",
      "Iteration 3461, loss = 7.48154972\n",
      "Iteration 3462, loss = 7.47854957\n",
      "Iteration 3463, loss = 6.17356616\n",
      "Iteration 3464, loss = 5.42930754\n",
      "Iteration 3465, loss = 8.67610164\n",
      "Iteration 3466, loss = 4.85410209\n",
      "Iteration 3467, loss = 5.80885886\n",
      "Iteration 3468, loss = 4.61944417\n",
      "Iteration 3469, loss = 4.88958604\n",
      "Iteration 3470, loss = 4.91439818\n",
      "Iteration 3471, loss = 4.93127591\n",
      "Iteration 3472, loss = 4.92205756\n",
      "Iteration 3473, loss = 4.74813028\n",
      "Iteration 3474, loss = 5.00122356\n",
      "Iteration 3475, loss = 4.65722840\n",
      "Iteration 3476, loss = 4.39052737\n",
      "Iteration 3477, loss = 4.31694075\n",
      "Iteration 3478, loss = 4.27188808\n",
      "Iteration 3479, loss = 4.30345498\n",
      "Iteration 3480, loss = 4.35026602\n",
      "Iteration 3481, loss = 4.32951336\n",
      "Iteration 3482, loss = 4.38353009\n",
      "Iteration 3483, loss = 6.11912098\n",
      "Iteration 3484, loss = 6.18161712\n",
      "Iteration 3485, loss = 8.50949853\n",
      "Iteration 3486, loss = 5.98518859\n",
      "Iteration 3487, loss = 5.09264271\n",
      "Iteration 3488, loss = 6.25315095\n",
      "Iteration 3489, loss = 5.07748015\n",
      "Iteration 3490, loss = 8.62949503\n",
      "Iteration 3491, loss = 12.23384877\n",
      "Iteration 3492, loss = 4.99945581\n",
      "Iteration 3493, loss = 6.63067919\n",
      "Iteration 3494, loss = 7.83014680\n",
      "Iteration 3495, loss = 7.66099603\n",
      "Iteration 3496, loss = 6.79882418\n",
      "Iteration 3497, loss = 7.09884842\n",
      "Iteration 3498, loss = 8.42715813\n",
      "Iteration 3499, loss = 8.19615597\n",
      "Iteration 3500, loss = 5.66985154\n",
      "Iteration 3501, loss = 7.86825951\n",
      "Iteration 3502, loss = 5.08917425\n",
      "Iteration 3503, loss = 5.92807063\n",
      "Iteration 3504, loss = 7.36930886\n",
      "Iteration 3505, loss = 5.86594825\n",
      "Iteration 3506, loss = 5.52188730\n",
      "Iteration 3507, loss = 9.14380447\n",
      "Iteration 3508, loss = 6.62198455\n",
      "Iteration 3509, loss = 6.66471438\n",
      "Iteration 3510, loss = 5.60537986\n",
      "Iteration 3511, loss = 4.81364405\n",
      "Iteration 3512, loss = 4.96583888\n",
      "Iteration 3513, loss = 4.51614115\n",
      "Iteration 3514, loss = 4.55128997\n",
      "Iteration 3515, loss = 5.16301822\n",
      "Iteration 3516, loss = 5.26947531\n",
      "Iteration 3517, loss = 4.48347675\n",
      "Iteration 3518, loss = 5.04159934\n",
      "Iteration 3519, loss = 4.46419080\n",
      "Iteration 3520, loss = 5.92058191\n",
      "Iteration 3521, loss = 6.10877358\n",
      "Iteration 3522, loss = 4.68118143\n",
      "Iteration 3523, loss = 5.94471612\n",
      "Iteration 3524, loss = 6.62376464\n",
      "Iteration 3525, loss = 8.28730104\n",
      "Iteration 3526, loss = 6.88942837\n",
      "Iteration 3527, loss = 6.30931997\n",
      "Iteration 3528, loss = 6.22231639\n",
      "Iteration 3529, loss = 5.87347673\n",
      "Iteration 3530, loss = 4.75487663\n",
      "Iteration 3531, loss = 5.29051972\n",
      "Iteration 3532, loss = 6.10883675\n",
      "Iteration 3533, loss = 5.14011614\n",
      "Iteration 3534, loss = 5.75422013\n",
      "Iteration 3535, loss = 6.23933837\n",
      "Iteration 3536, loss = 5.08066720\n",
      "Iteration 3537, loss = 5.45203086\n",
      "Iteration 3538, loss = 4.60230156\n",
      "Iteration 3539, loss = 4.48936494\n",
      "Iteration 3540, loss = 5.41403664\n",
      "Iteration 3541, loss = 5.59232260\n",
      "Iteration 3542, loss = 6.70738370\n",
      "Iteration 3543, loss = 6.99435927\n",
      "Iteration 3544, loss = 5.58223845\n",
      "Iteration 3545, loss = 6.23603807\n",
      "Iteration 3546, loss = 4.83874616\n",
      "Iteration 3547, loss = 4.62106166\n",
      "Iteration 3548, loss = 5.76554140\n",
      "Iteration 3549, loss = 5.23050810\n",
      "Iteration 3550, loss = 5.12657611\n",
      "Iteration 3551, loss = 4.46975965\n",
      "Iteration 3552, loss = 4.74506615\n",
      "Iteration 3553, loss = 4.93114360\n",
      "Iteration 3554, loss = 4.53890334\n",
      "Iteration 3555, loss = 7.00649598\n",
      "Iteration 3556, loss = 6.30076906\n",
      "Iteration 3557, loss = 5.80351369\n",
      "Iteration 3558, loss = 5.04658951\n",
      "Iteration 3559, loss = 6.18870947\n",
      "Iteration 3560, loss = 4.82774637\n",
      "Iteration 3561, loss = 5.54344701\n",
      "Iteration 3562, loss = 6.33624065\n",
      "Iteration 3563, loss = 7.08937687\n",
      "Iteration 3564, loss = 6.45766373\n",
      "Iteration 3565, loss = 5.43014460\n",
      "Iteration 3566, loss = 4.70117517\n",
      "Iteration 3567, loss = 5.00692027\n",
      "Iteration 3568, loss = 5.10507234\n",
      "Iteration 3569, loss = 5.08825383\n",
      "Iteration 3570, loss = 6.70505488\n",
      "Iteration 3571, loss = 7.85225803\n",
      "Iteration 3572, loss = 4.41689734\n",
      "Iteration 3573, loss = 5.02041270\n",
      "Iteration 3574, loss = 4.65469598\n",
      "Iteration 3575, loss = 4.66382932\n",
      "Iteration 3576, loss = 4.41216500\n",
      "Iteration 3577, loss = 4.64585161\n",
      "Iteration 3578, loss = 4.81119516\n",
      "Iteration 3579, loss = 4.56039374\n",
      "Iteration 3580, loss = 6.85138819\n",
      "Iteration 3581, loss = 9.10243551\n",
      "Iteration 3582, loss = 9.36079862\n",
      "Iteration 3583, loss = 5.75252403\n",
      "Iteration 3584, loss = 7.64118428\n",
      "Iteration 3585, loss = 5.75214528\n",
      "Iteration 3586, loss = 5.40390511\n",
      "Iteration 3587, loss = 4.59280735\n",
      "Iteration 3588, loss = 9.83233179\n",
      "Iteration 3589, loss = 7.57142905\n",
      "Iteration 3590, loss = 7.79062969\n",
      "Iteration 3591, loss = 4.72947178\n",
      "Iteration 3592, loss = 6.30875643\n",
      "Iteration 3593, loss = 6.36553855\n",
      "Iteration 3594, loss = 5.22976365\n",
      "Iteration 3595, loss = 5.01595438\n",
      "Iteration 3596, loss = 4.89039582\n",
      "Iteration 3597, loss = 9.00666150\n",
      "Iteration 3598, loss = 8.13058504\n",
      "Iteration 3599, loss = 8.40822209\n",
      "Iteration 3600, loss = 5.89845587\n",
      "Iteration 3601, loss = 6.43671715\n",
      "Iteration 3602, loss = 8.00365922\n",
      "Iteration 3603, loss = 7.50917730\n",
      "Iteration 3604, loss = 7.61814767\n",
      "Iteration 3605, loss = 9.97329887\n",
      "Iteration 3606, loss = 8.12587815\n",
      "Iteration 3607, loss = 11.52189159\n",
      "Iteration 3608, loss = 8.26390723\n",
      "Iteration 3609, loss = 8.18906680\n",
      "Iteration 3610, loss = 6.87977820\n",
      "Iteration 3611, loss = 9.08855851\n",
      "Iteration 3612, loss = 8.11963124\n",
      "Iteration 3613, loss = 7.07315860\n",
      "Iteration 3614, loss = 4.84214749\n",
      "Iteration 3615, loss = 5.85801618\n",
      "Iteration 3616, loss = 4.69058004\n",
      "Iteration 3617, loss = 4.56198852\n",
      "Iteration 3618, loss = 5.09993765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3619, loss = 5.04671083\n",
      "Iteration 3620, loss = 5.20624723\n",
      "Iteration 3621, loss = 6.65984153\n",
      "Iteration 3622, loss = 5.67729857\n",
      "Iteration 3623, loss = 7.01030680\n",
      "Iteration 3624, loss = 8.45435416\n",
      "Iteration 3625, loss = 5.74587065\n",
      "Iteration 3626, loss = 4.75296655\n",
      "Iteration 3627, loss = 4.94517148\n",
      "Iteration 3628, loss = 5.87956720\n",
      "Iteration 3629, loss = 5.72582161\n",
      "Iteration 3630, loss = 4.28143313\n",
      "Iteration 3631, loss = 5.18775163\n",
      "Iteration 3632, loss = 5.43825072\n",
      "Iteration 3633, loss = 4.96250535\n",
      "Iteration 3634, loss = 5.21987551\n",
      "Iteration 3635, loss = 6.65607184\n",
      "Iteration 3636, loss = 6.28959216\n",
      "Iteration 3637, loss = 6.90238710\n",
      "Iteration 3638, loss = 4.45316824\n",
      "Iteration 3639, loss = 4.90200434\n",
      "Iteration 3640, loss = 4.21449312\n",
      "Iteration 3641, loss = 4.39112710\n",
      "Iteration 3642, loss = 4.60979468\n",
      "Iteration 3643, loss = 4.43988635\n",
      "Iteration 3644, loss = 4.53170434\n",
      "Iteration 3645, loss = 4.86308321\n",
      "Iteration 3646, loss = 5.49301912\n",
      "Iteration 3647, loss = 4.70105305\n",
      "Iteration 3648, loss = 4.43325544\n",
      "Iteration 3649, loss = 4.36829924\n",
      "Iteration 3650, loss = 4.99697393\n",
      "Iteration 3651, loss = 4.30640533\n",
      "Iteration 3652, loss = 4.74794985\n",
      "Iteration 3653, loss = 5.38718503\n",
      "Iteration 3654, loss = 5.30502719\n",
      "Iteration 3655, loss = 5.50661220\n",
      "Iteration 3656, loss = 4.42741240\n",
      "Iteration 3657, loss = 5.03516729\n",
      "Iteration 3658, loss = 5.84395964\n",
      "Iteration 3659, loss = 6.05630856\n",
      "Iteration 3660, loss = 5.80051302\n",
      "Iteration 3661, loss = 4.42241088\n",
      "Iteration 3662, loss = 5.41407879\n",
      "Iteration 3663, loss = 5.28397237\n",
      "Iteration 3664, loss = 4.97827189\n",
      "Iteration 3665, loss = 5.04824092\n",
      "Iteration 3666, loss = 6.95935386\n",
      "Iteration 3667, loss = 5.48463518\n",
      "Iteration 3668, loss = 4.28354450\n",
      "Iteration 3669, loss = 5.65150581\n",
      "Iteration 3670, loss = 4.99093608\n",
      "Iteration 3671, loss = 5.23468377\n",
      "Iteration 3672, loss = 6.08845667\n",
      "Iteration 3673, loss = 5.35326281\n",
      "Iteration 3674, loss = 7.19302640\n",
      "Iteration 3675, loss = 8.63262593\n",
      "Iteration 3676, loss = 10.05074895\n",
      "Iteration 3677, loss = 8.17232327\n",
      "Iteration 3678, loss = 5.66094381\n",
      "Iteration 3679, loss = 5.19361700\n",
      "Iteration 3680, loss = 4.87451120\n",
      "Iteration 3681, loss = 7.36657677\n",
      "Iteration 3682, loss = 7.34885735\n",
      "Iteration 3683, loss = 6.05397126\n",
      "Iteration 3684, loss = 5.91166279\n",
      "Iteration 3685, loss = 4.60814813\n",
      "Iteration 3686, loss = 5.48689639\n",
      "Iteration 3687, loss = 5.90329791\n",
      "Iteration 3688, loss = 5.36364624\n",
      "Iteration 3689, loss = 4.10588670\n",
      "Iteration 3690, loss = 4.27993480\n",
      "Iteration 3691, loss = 4.22112394\n",
      "Iteration 3692, loss = 4.21315572\n",
      "Iteration 3693, loss = 4.60562592\n",
      "Iteration 3694, loss = 4.32495026\n",
      "Iteration 3695, loss = 4.45712306\n",
      "Iteration 3696, loss = 5.25780439\n",
      "Iteration 3697, loss = 9.19570395\n",
      "Iteration 3698, loss = 6.20219163\n",
      "Iteration 3699, loss = 5.78557910\n",
      "Iteration 3700, loss = 4.85424622\n",
      "Iteration 3701, loss = 5.23350146\n",
      "Iteration 3702, loss = 4.92718941\n",
      "Iteration 3703, loss = 5.02345529\n",
      "Iteration 3704, loss = 4.99020446\n",
      "Iteration 3705, loss = 5.42770736\n",
      "Iteration 3706, loss = 4.10773340\n",
      "Iteration 3707, loss = 4.39644170\n",
      "Iteration 3708, loss = 4.33542196\n",
      "Iteration 3709, loss = 4.28227435\n",
      "Iteration 3710, loss = 5.32356514\n",
      "Iteration 3711, loss = 5.14251872\n",
      "Iteration 3712, loss = 4.44634451\n",
      "Iteration 3713, loss = 5.07233770\n",
      "Iteration 3714, loss = 5.28358020\n",
      "Iteration 3715, loss = 5.37000869\n",
      "Iteration 3716, loss = 4.76328706\n",
      "Iteration 3717, loss = 4.80320139\n",
      "Iteration 3718, loss = 5.48729015\n",
      "Iteration 3719, loss = 4.71503712\n",
      "Iteration 3720, loss = 4.30571705\n",
      "Iteration 3721, loss = 4.17017517\n",
      "Iteration 3722, loss = 5.57862168\n",
      "Iteration 3723, loss = 7.02120834\n",
      "Iteration 3724, loss = 4.82866145\n",
      "Iteration 3725, loss = 5.48713495\n",
      "Iteration 3726, loss = 7.85521170\n",
      "Iteration 3727, loss = 7.86439326\n",
      "Iteration 3728, loss = 8.67874792\n",
      "Iteration 3729, loss = 9.07842506\n",
      "Iteration 3730, loss = 11.59591406\n",
      "Iteration 3731, loss = 11.54999045\n",
      "Iteration 3732, loss = 11.97046572\n",
      "Iteration 3733, loss = 12.87790526\n",
      "Iteration 3734, loss = 10.49085130\n",
      "Iteration 3735, loss = 9.84008419\n",
      "Iteration 3736, loss = 8.88428803\n",
      "Iteration 3737, loss = 8.24838827\n",
      "Iteration 3738, loss = 12.68716061\n",
      "Iteration 3739, loss = 16.76695610\n",
      "Iteration 3740, loss = 7.20270248\n",
      "Iteration 3741, loss = 5.95468051\n",
      "Iteration 3742, loss = 4.66023083\n",
      "Iteration 3743, loss = 5.93928530\n",
      "Iteration 3744, loss = 7.17367850\n",
      "Iteration 3745, loss = 7.47689238\n",
      "Iteration 3746, loss = 4.73009885\n",
      "Iteration 3747, loss = 4.80684611\n",
      "Iteration 3748, loss = 5.22317092\n",
      "Iteration 3749, loss = 5.38428540\n",
      "Iteration 3750, loss = 7.37264811\n",
      "Iteration 3751, loss = 9.39187663\n",
      "Iteration 3752, loss = 6.16992663\n",
      "Iteration 3753, loss = 6.85245098\n",
      "Iteration 3754, loss = 4.72000451\n",
      "Iteration 3755, loss = 7.93198934\n",
      "Iteration 3756, loss = 7.73431433\n",
      "Iteration 3757, loss = 6.14469406\n",
      "Iteration 3758, loss = 7.37411604\n",
      "Iteration 3759, loss = 6.85752172\n",
      "Iteration 3760, loss = 5.53246443\n",
      "Iteration 3761, loss = 5.92906608\n",
      "Iteration 3762, loss = 5.40476708\n",
      "Iteration 3763, loss = 5.83089476\n",
      "Iteration 3764, loss = 5.84280351\n",
      "Iteration 3765, loss = 5.02757324\n",
      "Iteration 3766, loss = 4.15039925\n",
      "Iteration 3767, loss = 4.66698892\n",
      "Iteration 3768, loss = 4.33132481\n",
      "Iteration 3769, loss = 4.29374050\n",
      "Iteration 3770, loss = 4.99708537\n",
      "Iteration 3771, loss = 5.37269811\n",
      "Iteration 3772, loss = 5.89786438\n",
      "Iteration 3773, loss = 4.35152877\n",
      "Iteration 3774, loss = 5.17470167\n",
      "Iteration 3775, loss = 4.95378452\n",
      "Iteration 3776, loss = 5.15455055\n",
      "Iteration 3777, loss = 5.03358335\n",
      "Iteration 3778, loss = 6.09746341\n",
      "Iteration 3779, loss = 5.75297570\n",
      "Iteration 3780, loss = 5.62765260\n",
      "Iteration 3781, loss = 5.66720477\n",
      "Iteration 3782, loss = 5.63398164\n",
      "Iteration 3783, loss = 5.37890733\n",
      "Iteration 3784, loss = 5.01804036\n",
      "Iteration 3785, loss = 4.82865836\n",
      "Iteration 3786, loss = 5.01289009\n",
      "Iteration 3787, loss = 4.52264766\n",
      "Iteration 3788, loss = 4.55826222\n",
      "Iteration 3789, loss = 8.25276917\n",
      "Iteration 3790, loss = 5.54358129\n",
      "Iteration 3791, loss = 5.66143131\n",
      "Iteration 3792, loss = 5.14412676\n",
      "Iteration 3793, loss = 5.21039633\n",
      "Iteration 3794, loss = 4.22830511\n",
      "Iteration 3795, loss = 4.28291964\n",
      "Iteration 3796, loss = 4.52718171\n",
      "Iteration 3797, loss = 4.23670636\n",
      "Iteration 3798, loss = 4.91708183\n",
      "Iteration 3799, loss = 5.67166282\n",
      "Iteration 3800, loss = 5.55506535\n",
      "Iteration 3801, loss = 6.77432001\n",
      "Iteration 3802, loss = 5.96215542\n",
      "Iteration 3803, loss = 6.87297139\n",
      "Iteration 3804, loss = 6.02942633\n",
      "Iteration 3805, loss = 5.45948162\n",
      "Iteration 3806, loss = 8.40298481\n",
      "Iteration 3807, loss = 5.05363489\n",
      "Iteration 3808, loss = 5.09474412\n",
      "Iteration 3809, loss = 6.47954276\n",
      "Iteration 3810, loss = 6.66608909\n",
      "Iteration 3811, loss = 5.76257837\n",
      "Iteration 3812, loss = 5.76036452\n",
      "Iteration 3813, loss = 6.99222231\n",
      "Iteration 3814, loss = 4.86769152\n",
      "Iteration 3815, loss = 4.02009390\n",
      "Iteration 3816, loss = 4.47263429\n",
      "Iteration 3817, loss = 4.66961653\n",
      "Iteration 3818, loss = 4.84161995\n",
      "Iteration 3819, loss = 4.77846076\n",
      "Iteration 3820, loss = 7.19910667\n",
      "Iteration 3821, loss = 12.80790298\n",
      "Iteration 3822, loss = 6.47356768\n",
      "Iteration 3823, loss = 4.43499738\n",
      "Iteration 3824, loss = 4.22785519\n",
      "Iteration 3825, loss = 4.27462467\n",
      "Iteration 3826, loss = 4.87413847\n",
      "Iteration 3827, loss = 6.72781819\n",
      "Iteration 3828, loss = 7.30282323\n",
      "Iteration 3829, loss = 6.68517205\n",
      "Iteration 3830, loss = 5.86780801\n",
      "Iteration 3831, loss = 6.44200724\n",
      "Iteration 3832, loss = 6.74396241\n",
      "Iteration 3833, loss = 5.55205777\n",
      "Iteration 3834, loss = 4.29996774\n",
      "Iteration 3835, loss = 5.00738179\n",
      "Iteration 3836, loss = 5.29441428\n",
      "Iteration 3837, loss = 4.76167590\n",
      "Iteration 3838, loss = 6.91443429\n",
      "Iteration 3839, loss = 6.64673806\n",
      "Iteration 3840, loss = 4.49499751\n",
      "Iteration 3841, loss = 5.69217366\n",
      "Iteration 3842, loss = 5.58316716\n",
      "Iteration 3843, loss = 5.44733442\n",
      "Iteration 3844, loss = 4.98703023\n",
      "Iteration 3845, loss = 4.21671569\n",
      "Iteration 3846, loss = 4.83659496\n",
      "Iteration 3847, loss = 6.44313957\n",
      "Iteration 3848, loss = 4.95656999\n",
      "Iteration 3849, loss = 5.44575564\n",
      "Iteration 3850, loss = 4.64303848\n",
      "Iteration 3851, loss = 5.18163671\n",
      "Iteration 3852, loss = 9.56619609\n",
      "Iteration 3853, loss = 7.67508853\n",
      "Iteration 3854, loss = 7.55462927\n",
      "Iteration 3855, loss = 6.80765889\n",
      "Iteration 3856, loss = 6.37535475\n",
      "Iteration 3857, loss = 5.16692487\n",
      "Iteration 3858, loss = 4.94005483\n",
      "Iteration 3859, loss = 4.89192932\n",
      "Iteration 3860, loss = 5.12522848\n",
      "Iteration 3861, loss = 5.30564499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3862, loss = 4.77258913\n",
      "Iteration 3863, loss = 4.94893142\n",
      "Iteration 3864, loss = 6.02809792\n",
      "Iteration 3865, loss = 6.42083546\n",
      "Iteration 3866, loss = 6.46460350\n",
      "Iteration 3867, loss = 6.07749693\n",
      "Iteration 3868, loss = 4.77705793\n",
      "Iteration 3869, loss = 5.11629472\n",
      "Iteration 3870, loss = 4.75896131\n",
      "Iteration 3871, loss = 4.82316996\n",
      "Iteration 3872, loss = 4.61472909\n",
      "Iteration 3873, loss = 4.04835259\n",
      "Iteration 3874, loss = 4.10680362\n",
      "Iteration 3875, loss = 4.92131733\n",
      "Iteration 3876, loss = 4.35451437\n",
      "Iteration 3877, loss = 4.11281938\n",
      "Iteration 3878, loss = 3.95593952\n",
      "Iteration 3879, loss = 4.72461165\n",
      "Iteration 3880, loss = 4.97912714\n",
      "Iteration 3881, loss = 6.00553042\n",
      "Iteration 3882, loss = 5.14913103\n",
      "Iteration 3883, loss = 6.25697239\n",
      "Iteration 3884, loss = 5.81168073\n",
      "Iteration 3885, loss = 6.25085544\n",
      "Iteration 3886, loss = 5.76316213\n",
      "Iteration 3887, loss = 5.13182755\n",
      "Iteration 3888, loss = 4.53988828\n",
      "Iteration 3889, loss = 5.89497533\n",
      "Iteration 3890, loss = 4.94108146\n",
      "Iteration 3891, loss = 4.57179179\n",
      "Iteration 3892, loss = 4.94428337\n",
      "Iteration 3893, loss = 7.18412183\n",
      "Iteration 3894, loss = 6.83315527\n",
      "Iteration 3895, loss = 5.66969127\n",
      "Iteration 3896, loss = 5.26875463\n",
      "Iteration 3897, loss = 4.59734458\n",
      "Iteration 3898, loss = 4.76799705\n",
      "Iteration 3899, loss = 4.14540491\n",
      "Iteration 3900, loss = 4.22011996\n",
      "Iteration 3901, loss = 4.57139700\n",
      "Iteration 3902, loss = 4.00818601\n",
      "Iteration 3903, loss = 5.38494299\n",
      "Iteration 3904, loss = 5.42283635\n",
      "Iteration 3905, loss = 5.91764023\n",
      "Iteration 3906, loss = 4.43972223\n",
      "Iteration 3907, loss = 4.06192770\n",
      "Iteration 3908, loss = 4.16400491\n",
      "Iteration 3909, loss = 4.28329928\n",
      "Iteration 3910, loss = 4.68851325\n",
      "Iteration 3911, loss = 7.60064271\n",
      "Iteration 3912, loss = 5.71433675\n",
      "Iteration 3913, loss = 4.21442357\n",
      "Iteration 3914, loss = 3.95821683\n",
      "Iteration 3915, loss = 4.26313009\n",
      "Iteration 3916, loss = 5.07205571\n",
      "Iteration 3917, loss = 4.29799744\n",
      "Iteration 3918, loss = 4.91002613\n",
      "Iteration 3919, loss = 4.94681460\n",
      "Iteration 3920, loss = 6.17779520\n",
      "Iteration 3921, loss = 6.22056148\n",
      "Iteration 3922, loss = 5.53345425\n",
      "Iteration 3923, loss = 7.59102629\n",
      "Iteration 3924, loss = 5.96234649\n",
      "Iteration 3925, loss = 5.72603869\n",
      "Iteration 3926, loss = 5.94596136\n",
      "Iteration 3927, loss = 5.14893361\n",
      "Iteration 3928, loss = 5.12489020\n",
      "Iteration 3929, loss = 5.87087206\n",
      "Iteration 3930, loss = 4.76350085\n",
      "Iteration 3931, loss = 4.72720126\n",
      "Iteration 3932, loss = 4.39441549\n",
      "Iteration 3933, loss = 4.18886134\n",
      "Iteration 3934, loss = 4.17039757\n",
      "Iteration 3935, loss = 4.91279666\n",
      "Iteration 3936, loss = 4.98214181\n",
      "Iteration 3937, loss = 4.88831305\n",
      "Iteration 3938, loss = 4.52413712\n",
      "Iteration 3939, loss = 5.35581027\n",
      "Iteration 3940, loss = 6.26873717\n",
      "Iteration 3941, loss = 5.24135811\n",
      "Iteration 3942, loss = 6.39486091\n",
      "Iteration 3943, loss = 6.71314650\n",
      "Iteration 3944, loss = 5.80909579\n",
      "Iteration 3945, loss = 6.26438094\n",
      "Iteration 3946, loss = 6.37341688\n",
      "Iteration 3947, loss = 6.88543124\n",
      "Iteration 3948, loss = 8.03871743\n",
      "Iteration 3949, loss = 8.13623404\n",
      "Iteration 3950, loss = 5.27125695\n",
      "Iteration 3951, loss = 5.51089841\n",
      "Iteration 3952, loss = 8.10758125\n",
      "Iteration 3953, loss = 8.74048491\n",
      "Iteration 3954, loss = 10.08771512\n",
      "Iteration 3955, loss = 8.23808248\n",
      "Iteration 3956, loss = 5.98790255\n",
      "Iteration 3957, loss = 6.36501522\n",
      "Iteration 3958, loss = 4.98471742\n",
      "Iteration 3959, loss = 5.31566976\n",
      "Iteration 3960, loss = 5.64697736\n",
      "Iteration 3961, loss = 5.31138994\n",
      "Iteration 3962, loss = 5.78169182\n",
      "Iteration 3963, loss = 6.00161101\n",
      "Iteration 3964, loss = 4.98590862\n",
      "Iteration 3965, loss = 7.42228553\n",
      "Iteration 3966, loss = 4.42437898\n",
      "Iteration 3967, loss = 5.65377099\n",
      "Iteration 3968, loss = 4.68631389\n",
      "Iteration 3969, loss = 4.79474022\n",
      "Iteration 3970, loss = 4.93477829\n",
      "Iteration 3971, loss = 4.42735937\n",
      "Iteration 3972, loss = 4.25898335\n",
      "Iteration 3973, loss = 4.45535116\n",
      "Iteration 3974, loss = 6.10107320\n",
      "Iteration 3975, loss = 5.61192384\n",
      "Iteration 3976, loss = 5.60265926\n",
      "Iteration 3977, loss = 6.39200938\n",
      "Iteration 3978, loss = 5.28579484\n",
      "Iteration 3979, loss = 6.59614126\n",
      "Iteration 3980, loss = 6.22325264\n",
      "Iteration 3981, loss = 6.53017758\n",
      "Iteration 3982, loss = 8.49136814\n",
      "Iteration 3983, loss = 7.86491851\n",
      "Iteration 3984, loss = 6.55280074\n",
      "Iteration 3985, loss = 6.87591101\n",
      "Iteration 3986, loss = 7.19797384\n",
      "Iteration 3987, loss = 7.08868967\n",
      "Iteration 3988, loss = 6.54529368\n",
      "Iteration 3989, loss = 6.34675498\n",
      "Iteration 3990, loss = 4.99133644\n",
      "Iteration 3991, loss = 5.34306592\n",
      "Iteration 3992, loss = 4.61629452\n",
      "Iteration 3993, loss = 4.86401407\n",
      "Iteration 3994, loss = 4.42393630\n",
      "Iteration 3995, loss = 3.96695168\n",
      "Iteration 3996, loss = 4.08163769\n",
      "Iteration 3997, loss = 4.26432510\n",
      "Iteration 3998, loss = 6.34769243\n",
      "Iteration 3999, loss = 4.64861614\n",
      "Iteration 4000, loss = 5.91718115\n",
      "Iteration 4001, loss = 6.36931858\n",
      "Iteration 4002, loss = 6.44468673\n",
      "Iteration 4003, loss = 9.17720410\n",
      "Iteration 4004, loss = 6.91739590\n",
      "Iteration 4005, loss = 6.07289469\n",
      "Iteration 4006, loss = 4.92301077\n",
      "Iteration 4007, loss = 4.56246113\n",
      "Iteration 4008, loss = 5.60222445\n",
      "Iteration 4009, loss = 4.68215449\n",
      "Iteration 4010, loss = 5.05069867\n",
      "Iteration 4011, loss = 4.25278602\n",
      "Iteration 4012, loss = 5.77402101\n",
      "Iteration 4013, loss = 4.06043084\n",
      "Iteration 4014, loss = 4.34878964\n",
      "Iteration 4015, loss = 4.24701361\n",
      "Iteration 4016, loss = 4.18568702\n",
      "Iteration 4017, loss = 4.18008187\n",
      "Iteration 4018, loss = 5.12688510\n",
      "Iteration 4019, loss = 5.77314929\n",
      "Iteration 4020, loss = 6.10989094\n",
      "Iteration 4021, loss = 4.53018604\n",
      "Iteration 4022, loss = 4.73290733\n",
      "Iteration 4023, loss = 4.88900746\n",
      "Iteration 4024, loss = 4.89607716\n",
      "Iteration 4025, loss = 6.30125578\n",
      "Iteration 4026, loss = 5.26566582\n",
      "Iteration 4027, loss = 5.29717747\n",
      "Iteration 4028, loss = 6.68560485\n",
      "Iteration 4029, loss = 6.38766127\n",
      "Iteration 4030, loss = 14.11820577\n",
      "Iteration 4031, loss = 6.89732033\n",
      "Iteration 4032, loss = 5.82228603\n",
      "Iteration 4033, loss = 4.49344046\n",
      "Iteration 4034, loss = 4.64826405\n",
      "Iteration 4035, loss = 4.13542663\n",
      "Iteration 4036, loss = 4.30555005\n",
      "Iteration 4037, loss = 4.80451785\n",
      "Iteration 4038, loss = 5.10688109\n",
      "Iteration 4039, loss = 4.91421988\n",
      "Iteration 4040, loss = 6.53492213\n",
      "Iteration 4041, loss = 7.36023282\n",
      "Iteration 4042, loss = 5.48461847\n",
      "Iteration 4043, loss = 5.47140687\n",
      "Iteration 4044, loss = 4.74926860\n",
      "Iteration 4045, loss = 4.30214871\n",
      "Iteration 4046, loss = 5.33688556\n",
      "Iteration 4047, loss = 4.89715581\n",
      "Iteration 4048, loss = 4.47954295\n",
      "Iteration 4049, loss = 3.91321050\n",
      "Iteration 4050, loss = 4.24807202\n",
      "Iteration 4051, loss = 3.99985554\n",
      "Iteration 4052, loss = 4.22771536\n",
      "Iteration 4053, loss = 4.38266763\n",
      "Iteration 4054, loss = 4.31982108\n",
      "Iteration 4055, loss = 4.44359501\n",
      "Iteration 4056, loss = 4.79162012\n",
      "Iteration 4057, loss = 4.58386568\n",
      "Iteration 4058, loss = 4.51659431\n",
      "Iteration 4059, loss = 4.69431199\n",
      "Iteration 4060, loss = 4.71311200\n",
      "Iteration 4061, loss = 4.59612087\n",
      "Iteration 4062, loss = 5.47373157\n",
      "Iteration 4063, loss = 5.80964339\n",
      "Iteration 4064, loss = 5.32275070\n",
      "Iteration 4065, loss = 5.48999303\n",
      "Iteration 4066, loss = 4.87275926\n",
      "Iteration 4067, loss = 5.42445141\n",
      "Iteration 4068, loss = 4.65870908\n",
      "Iteration 4069, loss = 4.29226890\n",
      "Iteration 4070, loss = 4.77970686\n",
      "Iteration 4071, loss = 7.13945691\n",
      "Iteration 4072, loss = 5.03477143\n",
      "Iteration 4073, loss = 6.71322120\n",
      "Iteration 4074, loss = 7.00152068\n",
      "Iteration 4075, loss = 7.96566963\n",
      "Iteration 4076, loss = 10.02333482\n",
      "Iteration 4077, loss = 7.94734385\n",
      "Iteration 4078, loss = 8.42230959\n",
      "Iteration 4079, loss = 6.45851084\n",
      "Iteration 4080, loss = 6.79872455\n",
      "Iteration 4081, loss = 4.93061192\n",
      "Iteration 4082, loss = 4.89686617\n",
      "Iteration 4083, loss = 4.25512230\n",
      "Iteration 4084, loss = 7.24181516\n",
      "Iteration 4085, loss = 6.07352376\n",
      "Iteration 4086, loss = 6.81858377\n",
      "Iteration 4087, loss = 6.78847142\n",
      "Iteration 4088, loss = 7.26075678\n",
      "Iteration 4089, loss = 7.02551191\n",
      "Iteration 4090, loss = 5.51140759\n",
      "Iteration 4091, loss = 5.61446098\n",
      "Iteration 4092, loss = 5.85216113\n",
      "Iteration 4093, loss = 6.44814512\n",
      "Iteration 4094, loss = 6.48532196\n",
      "Iteration 4095, loss = 7.00142111\n",
      "Iteration 4096, loss = 5.59932742\n",
      "Iteration 4097, loss = 4.21183697\n",
      "Iteration 4098, loss = 4.97674838\n",
      "Iteration 4099, loss = 5.60097879\n",
      "Iteration 4100, loss = 4.83012855\n",
      "Iteration 4101, loss = 6.45851800\n",
      "Iteration 4102, loss = 6.24542417\n",
      "Iteration 4103, loss = 6.26739178\n",
      "Iteration 4104, loss = 5.86586571\n",
      "Iteration 4105, loss = 5.53223170\n",
      "Iteration 4106, loss = 4.96550653\n",
      "Iteration 4107, loss = 5.96564573\n",
      "Iteration 4108, loss = 5.60078506\n",
      "Iteration 4109, loss = 7.55800285\n",
      "Iteration 4110, loss = 7.62632047\n",
      "Iteration 4111, loss = 6.79390237\n",
      "Iteration 4112, loss = 4.56355471\n",
      "Iteration 4113, loss = 6.16981511\n",
      "Iteration 4114, loss = 6.52530486\n",
      "Iteration 4115, loss = 10.77969656\n",
      "Iteration 4116, loss = 9.24380399\n",
      "Iteration 4117, loss = 5.95264325\n",
      "Iteration 4118, loss = 7.49329150\n",
      "Iteration 4119, loss = 9.06553166\n",
      "Iteration 4120, loss = 8.42760156\n",
      "Iteration 4121, loss = 9.24756731\n",
      "Iteration 4122, loss = 6.57229177\n",
      "Iteration 4123, loss = 5.11439420\n",
      "Iteration 4124, loss = 4.72343858\n",
      "Iteration 4125, loss = 5.54121564\n",
      "Iteration 4126, loss = 5.63888513\n",
      "Iteration 4127, loss = 6.48778809\n",
      "Iteration 4128, loss = 4.82162129\n",
      "Iteration 4129, loss = 6.12231256\n",
      "Iteration 4130, loss = 4.27936290\n",
      "Iteration 4131, loss = 4.18512307\n",
      "Iteration 4132, loss = 4.21804602\n",
      "Iteration 4133, loss = 4.02423001\n",
      "Iteration 4134, loss = 3.87938247\n",
      "Iteration 4135, loss = 5.18439387\n",
      "Iteration 4136, loss = 4.52764154\n",
      "Iteration 4137, loss = 4.72643486\n",
      "Iteration 4138, loss = 4.50044022\n",
      "Iteration 4139, loss = 4.73612220\n",
      "Iteration 4140, loss = 4.27756180\n",
      "Iteration 4141, loss = 4.39264215\n",
      "Iteration 4142, loss = 3.98342761\n",
      "Iteration 4143, loss = 4.09314899\n",
      "Iteration 4144, loss = 4.52815453\n",
      "Iteration 4145, loss = 4.15104468\n",
      "Iteration 4146, loss = 4.22558265\n",
      "Iteration 4147, loss = 4.12255665\n",
      "Iteration 4148, loss = 4.39384939\n",
      "Iteration 4149, loss = 8.28860632\n",
      "Iteration 4150, loss = 5.68121101\n",
      "Iteration 4151, loss = 5.37972277\n",
      "Iteration 4152, loss = 6.48755873\n",
      "Iteration 4153, loss = 6.01659896\n",
      "Iteration 4154, loss = 6.76996101\n",
      "Iteration 4155, loss = 8.26548058\n",
      "Iteration 4156, loss = 6.70334326\n",
      "Iteration 4157, loss = 5.68321003\n",
      "Iteration 4158, loss = 4.72650685\n",
      "Iteration 4159, loss = 4.21539542\n",
      "Iteration 4160, loss = 4.24953954\n",
      "Iteration 4161, loss = 4.77494221\n",
      "Iteration 4162, loss = 4.18656052\n",
      "Iteration 4163, loss = 4.55412774\n",
      "Iteration 4164, loss = 5.58716102\n",
      "Iteration 4165, loss = 7.58324226\n",
      "Iteration 4166, loss = 6.41792404\n",
      "Iteration 4167, loss = 4.90020430\n",
      "Iteration 4168, loss = 3.87452173\n",
      "Iteration 4169, loss = 4.44378916\n",
      "Iteration 4170, loss = 5.16479999\n",
      "Iteration 4171, loss = 4.96411848\n",
      "Iteration 4172, loss = 4.12567694\n",
      "Iteration 4173, loss = 4.60351828\n",
      "Iteration 4174, loss = 5.12324666\n",
      "Iteration 4175, loss = 4.74429161\n",
      "Iteration 4176, loss = 3.72378564\n",
      "Iteration 4177, loss = 4.44942344\n",
      "Iteration 4178, loss = 4.08824309\n",
      "Iteration 4179, loss = 4.55452022\n",
      "Iteration 4180, loss = 5.02292096\n",
      "Iteration 4181, loss = 5.71070302\n",
      "Iteration 4182, loss = 6.51768437\n",
      "Iteration 4183, loss = 9.97529080\n",
      "Iteration 4184, loss = 5.79917109\n",
      "Iteration 4185, loss = 4.39501317\n",
      "Iteration 4186, loss = 4.33794088\n",
      "Iteration 4187, loss = 7.27853053\n",
      "Iteration 4188, loss = 5.69881063\n",
      "Iteration 4189, loss = 5.80027195\n",
      "Iteration 4190, loss = 4.88000181\n",
      "Iteration 4191, loss = 4.80030149\n",
      "Iteration 4192, loss = 5.37352495\n",
      "Iteration 4193, loss = 5.62465554\n",
      "Iteration 4194, loss = 5.00037134\n",
      "Iteration 4195, loss = 4.91968232\n",
      "Iteration 4196, loss = 4.23915895\n",
      "Iteration 4197, loss = 5.58186191\n",
      "Iteration 4198, loss = 5.71672868\n",
      "Iteration 4199, loss = 6.23122715\n",
      "Iteration 4200, loss = 5.22734522\n",
      "Iteration 4201, loss = 4.43961966\n",
      "Iteration 4202, loss = 4.32728705\n",
      "Iteration 4203, loss = 4.18480897\n",
      "Iteration 4204, loss = 4.00542168\n",
      "Iteration 4205, loss = 4.05195000\n",
      "Iteration 4206, loss = 4.73092719\n",
      "Iteration 4207, loss = 4.03221856\n",
      "Iteration 4208, loss = 3.96043273\n",
      "Iteration 4209, loss = 4.13069206\n",
      "Iteration 4210, loss = 4.32804690\n",
      "Iteration 4211, loss = 7.41685587\n",
      "Iteration 4212, loss = 5.78884389\n",
      "Iteration 4213, loss = 5.63235440\n",
      "Iteration 4214, loss = 5.38675690\n",
      "Iteration 4215, loss = 4.10244926\n",
      "Iteration 4216, loss = 5.16577471\n",
      "Iteration 4217, loss = 5.06126734\n",
      "Iteration 4218, loss = 4.17502837\n",
      "Iteration 4219, loss = 4.14097527\n",
      "Iteration 4220, loss = 4.23557895\n",
      "Iteration 4221, loss = 6.37010502\n",
      "Iteration 4222, loss = 5.13908282\n",
      "Iteration 4223, loss = 4.52769739\n",
      "Iteration 4224, loss = 4.77724911\n",
      "Iteration 4225, loss = 4.12178354\n",
      "Iteration 4226, loss = 4.05063387\n",
      "Iteration 4227, loss = 4.45381499\n",
      "Iteration 4228, loss = 7.50860017\n",
      "Iteration 4229, loss = 8.55591699\n",
      "Iteration 4230, loss = 6.92505861\n",
      "Iteration 4231, loss = 7.32951522\n",
      "Iteration 4232, loss = 5.23153475\n",
      "Iteration 4233, loss = 7.79024296\n",
      "Iteration 4234, loss = 5.49300334\n",
      "Iteration 4235, loss = 4.80281675\n",
      "Iteration 4236, loss = 5.12867631\n",
      "Iteration 4237, loss = 5.52797827\n",
      "Iteration 4238, loss = 4.39421233\n",
      "Iteration 4239, loss = 5.13694918\n",
      "Iteration 4240, loss = 4.32554063\n",
      "Iteration 4241, loss = 4.11031234\n",
      "Iteration 4242, loss = 4.60818924\n",
      "Iteration 4243, loss = 4.70969733\n",
      "Iteration 4244, loss = 4.68951398\n",
      "Iteration 4245, loss = 4.34518693\n",
      "Iteration 4246, loss = 4.20600158\n",
      "Iteration 4247, loss = 4.37370426\n",
      "Iteration 4248, loss = 3.89243926\n",
      "Iteration 4249, loss = 4.04865819\n",
      "Iteration 4250, loss = 4.61750207\n",
      "Iteration 4251, loss = 4.28154390\n",
      "Iteration 4252, loss = 4.09156704\n",
      "Iteration 4253, loss = 4.77973374\n",
      "Iteration 4254, loss = 5.25691790\n",
      "Iteration 4255, loss = 5.32377959\n",
      "Iteration 4256, loss = 4.63138727\n",
      "Iteration 4257, loss = 4.69888657\n",
      "Iteration 4258, loss = 5.48169478\n",
      "Iteration 4259, loss = 6.17464430\n",
      "Iteration 4260, loss = 5.19976850\n",
      "Iteration 4261, loss = 5.99523166\n",
      "Iteration 4262, loss = 5.72831384\n",
      "Iteration 4263, loss = 4.84712392\n",
      "Iteration 4264, loss = 5.01665898\n",
      "Iteration 4265, loss = 5.37816094\n",
      "Iteration 4266, loss = 4.64854180\n",
      "Iteration 4267, loss = 6.59350999\n",
      "Iteration 4268, loss = 7.68936579\n",
      "Iteration 4269, loss = 5.93475662\n",
      "Iteration 4270, loss = 7.18416397\n",
      "Iteration 4271, loss = 6.23753422\n",
      "Iteration 4272, loss = 7.33131552\n",
      "Iteration 4273, loss = 6.37515194\n",
      "Iteration 4274, loss = 5.84028906\n",
      "Iteration 4275, loss = 6.06814977\n",
      "Iteration 4276, loss = 6.13224268\n",
      "Iteration 4277, loss = 5.30879614\n",
      "Iteration 4278, loss = 4.60656375\n",
      "Iteration 4279, loss = 4.36080441\n",
      "Iteration 4280, loss = 3.80024143\n",
      "Iteration 4281, loss = 4.59908534\n",
      "Iteration 4282, loss = 4.29533076\n",
      "Iteration 4283, loss = 4.24273890\n",
      "Iteration 4284, loss = 4.06647915\n",
      "Iteration 4285, loss = 4.09776791\n",
      "Iteration 4286, loss = 4.38848144\n",
      "Iteration 4287, loss = 4.57861539\n",
      "Iteration 4288, loss = 4.48444164\n",
      "Iteration 4289, loss = 4.03629487\n",
      "Iteration 4290, loss = 4.43116383\n",
      "Iteration 4291, loss = 4.95950009\n",
      "Iteration 4292, loss = 4.21444782\n",
      "Iteration 4293, loss = 4.01130500\n",
      "Iteration 4294, loss = 4.08708390\n",
      "Iteration 4295, loss = 4.56981822\n",
      "Iteration 4296, loss = 5.20179962\n",
      "Iteration 4297, loss = 4.24843925\n",
      "Iteration 4298, loss = 4.29010636\n",
      "Iteration 4299, loss = 4.54805809\n",
      "Iteration 4300, loss = 4.68261250\n",
      "Iteration 4301, loss = 9.52477691\n",
      "Iteration 4302, loss = 8.11917021\n",
      "Iteration 4303, loss = 7.10591262\n",
      "Iteration 4304, loss = 7.03881347\n",
      "Iteration 4305, loss = 4.93935688\n",
      "Iteration 4306, loss = 5.84335109\n",
      "Iteration 4307, loss = 4.46051286\n",
      "Iteration 4308, loss = 5.69089840\n",
      "Iteration 4309, loss = 5.43072469\n",
      "Iteration 4310, loss = 4.28118455\n",
      "Iteration 4311, loss = 3.80081629\n",
      "Iteration 4312, loss = 3.77244750\n",
      "Iteration 4313, loss = 3.74272634\n",
      "Iteration 4314, loss = 3.91861116\n",
      "Iteration 4315, loss = 4.00853746\n",
      "Iteration 4316, loss = 4.02643063\n",
      "Iteration 4317, loss = 3.96410193\n",
      "Iteration 4318, loss = 6.31684205\n",
      "Iteration 4319, loss = 5.40173271\n",
      "Iteration 4320, loss = 6.92888289\n",
      "Iteration 4321, loss = 5.30955768\n",
      "Iteration 4322, loss = 4.60423668\n",
      "Iteration 4323, loss = 8.20817664\n",
      "Iteration 4324, loss = 6.10232465\n",
      "Iteration 4325, loss = 4.98571410\n",
      "Iteration 4326, loss = 4.82804524\n",
      "Iteration 4327, loss = 4.79442925\n",
      "Iteration 4328, loss = 4.21707842\n",
      "Iteration 4329, loss = 3.98996252\n",
      "Iteration 4330, loss = 4.60543174\n",
      "Iteration 4331, loss = 5.39935004\n",
      "Iteration 4332, loss = 4.61015931\n",
      "Iteration 4333, loss = 4.88823855\n",
      "Iteration 4334, loss = 5.85534465\n",
      "Iteration 4335, loss = 5.14115206\n",
      "Iteration 4336, loss = 4.76174115\n",
      "Iteration 4337, loss = 4.57985827\n",
      "Iteration 4338, loss = 4.09693156\n",
      "Iteration 4339, loss = 4.33894251\n",
      "Iteration 4340, loss = 3.80974299\n",
      "Iteration 4341, loss = 4.43353976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4342, loss = 4.86565916\n",
      "Iteration 4343, loss = 4.71120165\n",
      "Iteration 4344, loss = 5.38495127\n",
      "Iteration 4345, loss = 5.04431527\n",
      "Iteration 4346, loss = 6.41503602\n",
      "Iteration 4347, loss = 6.59608305\n",
      "Iteration 4348, loss = 5.27279762\n",
      "Iteration 4349, loss = 4.28488601\n",
      "Iteration 4350, loss = 3.97540097\n",
      "Iteration 4351, loss = 4.87588036\n",
      "Iteration 4352, loss = 4.34808823\n",
      "Iteration 4353, loss = 4.65900232\n",
      "Iteration 4354, loss = 4.42815536\n",
      "Iteration 4355, loss = 4.44889352\n",
      "Iteration 4356, loss = 4.63209487\n",
      "Iteration 4357, loss = 3.71732128\n",
      "Iteration 4358, loss = 3.86549226\n",
      "Iteration 4359, loss = 4.46702773\n",
      "Iteration 4360, loss = 3.89543088\n",
      "Iteration 4361, loss = 3.64136355\n",
      "Iteration 4362, loss = 4.99055180\n",
      "Iteration 4363, loss = 7.05755256\n",
      "Iteration 4364, loss = 6.03114022\n",
      "Iteration 4365, loss = 6.20530641\n",
      "Iteration 4366, loss = 5.38462856\n",
      "Iteration 4367, loss = 5.28841481\n",
      "Iteration 4368, loss = 4.49526226\n",
      "Iteration 4369, loss = 4.91308048\n",
      "Iteration 4370, loss = 4.53784266\n",
      "Iteration 4371, loss = 6.66987575\n",
      "Iteration 4372, loss = 4.55939865\n",
      "Iteration 4373, loss = 4.51838781\n",
      "Iteration 4374, loss = 4.18847374\n",
      "Iteration 4375, loss = 6.07356853\n",
      "Iteration 4376, loss = 6.42129243\n",
      "Iteration 4377, loss = 5.90831142\n",
      "Iteration 4378, loss = 4.33309652\n",
      "Iteration 4379, loss = 4.07457641\n",
      "Iteration 4380, loss = 4.33254623\n",
      "Iteration 4381, loss = 3.88518320\n",
      "Iteration 4382, loss = 4.16426498\n",
      "Iteration 4383, loss = 4.43207946\n",
      "Iteration 4384, loss = 4.20859031\n",
      "Iteration 4385, loss = 3.98645034\n",
      "Iteration 4386, loss = 3.74502229\n",
      "Iteration 4387, loss = 3.91672136\n",
      "Iteration 4388, loss = 4.28411861\n",
      "Iteration 4389, loss = 4.24723906\n",
      "Iteration 4390, loss = 3.96873998\n",
      "Iteration 4391, loss = 7.39136546\n",
      "Iteration 4392, loss = 7.38776862\n",
      "Iteration 4393, loss = 9.70056769\n",
      "Iteration 4394, loss = 8.70703573\n",
      "Iteration 4395, loss = 6.92998501\n",
      "Iteration 4396, loss = 6.28860288\n",
      "Iteration 4397, loss = 4.16082345\n",
      "Iteration 4398, loss = 5.58754252\n",
      "Iteration 4399, loss = 5.51825134\n",
      "Iteration 4400, loss = 4.69708528\n",
      "Iteration 4401, loss = 5.71901481\n",
      "Iteration 4402, loss = 4.89948127\n",
      "Iteration 4403, loss = 3.84641023\n",
      "Iteration 4404, loss = 4.90166472\n",
      "Iteration 4405, loss = 4.30273090\n",
      "Iteration 4406, loss = 4.12863380\n",
      "Iteration 4407, loss = 4.39076531\n",
      "Iteration 4408, loss = 6.06410922\n",
      "Iteration 4409, loss = 6.82065961\n",
      "Iteration 4410, loss = 7.77985861\n",
      "Iteration 4411, loss = 4.88424137\n",
      "Iteration 4412, loss = 5.03787249\n",
      "Iteration 4413, loss = 4.40149868\n",
      "Iteration 4414, loss = 4.71281983\n",
      "Iteration 4415, loss = 5.80187223\n",
      "Iteration 4416, loss = 5.53876512\n",
      "Iteration 4417, loss = 5.92193156\n",
      "Iteration 4418, loss = 5.73000138\n",
      "Iteration 4419, loss = 5.93853689\n",
      "Iteration 4420, loss = 7.54662073\n",
      "Iteration 4421, loss = 5.83834964\n",
      "Iteration 4422, loss = 7.76140747\n",
      "Iteration 4423, loss = 6.35404531\n",
      "Iteration 4424, loss = 4.64251123\n",
      "Iteration 4425, loss = 4.74759917\n",
      "Iteration 4426, loss = 4.64646660\n",
      "Iteration 4427, loss = 3.61189374\n",
      "Iteration 4428, loss = 4.12006944\n",
      "Iteration 4429, loss = 3.95972877\n",
      "Iteration 4430, loss = 3.98599727\n",
      "Iteration 4431, loss = 4.10911941\n",
      "Iteration 4432, loss = 4.96680796\n",
      "Iteration 4433, loss = 4.20662819\n",
      "Iteration 4434, loss = 4.14821915\n",
      "Iteration 4435, loss = 4.23792579\n",
      "Iteration 4436, loss = 5.40072439\n",
      "Iteration 4437, loss = 4.41300183\n",
      "Iteration 4438, loss = 3.99434456\n",
      "Iteration 4439, loss = 4.27459965\n",
      "Iteration 4440, loss = 4.59376483\n",
      "Iteration 4441, loss = 3.75042666\n",
      "Iteration 4442, loss = 3.91645441\n",
      "Iteration 4443, loss = 5.81612716\n",
      "Iteration 4444, loss = 4.87536644\n",
      "Iteration 4445, loss = 4.59792528\n",
      "Iteration 4446, loss = 4.13207513\n",
      "Iteration 4447, loss = 3.98280464\n",
      "Iteration 4448, loss = 4.14851905\n",
      "Iteration 4449, loss = 4.74561015\n",
      "Iteration 4450, loss = 4.33922948\n",
      "Iteration 4451, loss = 5.74431282\n",
      "Iteration 4452, loss = 4.34310562\n",
      "Iteration 4453, loss = 4.58443134\n",
      "Iteration 4454, loss = 8.53026438\n",
      "Iteration 4455, loss = 7.47886591\n",
      "Iteration 4456, loss = 6.39838372\n",
      "Iteration 4457, loss = 7.49695592\n",
      "Iteration 4458, loss = 3.91510287\n",
      "Iteration 4459, loss = 3.77655769\n",
      "Iteration 4460, loss = 3.94153906\n",
      "Iteration 4461, loss = 3.89054485\n",
      "Iteration 4462, loss = 5.38752184\n",
      "Iteration 4463, loss = 4.90043594\n",
      "Iteration 4464, loss = 5.02340657\n",
      "Iteration 4465, loss = 4.67491989\n",
      "Iteration 4466, loss = 4.29663059\n",
      "Iteration 4467, loss = 4.26634406\n",
      "Iteration 4468, loss = 5.02980978\n",
      "Iteration 4469, loss = 4.31697829\n",
      "Iteration 4470, loss = 4.06182211\n",
      "Iteration 4471, loss = 5.40183518\n",
      "Iteration 4472, loss = 5.38619415\n",
      "Iteration 4473, loss = 6.38533702\n",
      "Iteration 4474, loss = 4.86193604\n",
      "Iteration 4475, loss = 5.08810778\n",
      "Iteration 4476, loss = 3.97795594\n",
      "Iteration 4477, loss = 4.39584626\n",
      "Iteration 4478, loss = 4.68749822\n",
      "Iteration 4479, loss = 3.37300764\n",
      "Iteration 4480, loss = 5.49776920\n",
      "Iteration 4481, loss = 3.96995601\n",
      "Iteration 4482, loss = 4.91484700\n",
      "Iteration 4483, loss = 4.13220367\n",
      "Iteration 4484, loss = 4.29099090\n",
      "Iteration 4485, loss = 4.99973892\n",
      "Iteration 4486, loss = 5.06381930\n",
      "Iteration 4487, loss = 4.89719591\n",
      "Iteration 4488, loss = 4.82887466\n",
      "Iteration 4489, loss = 4.78482058\n",
      "Iteration 4490, loss = 4.32925105\n",
      "Iteration 4491, loss = 3.96596736\n",
      "Iteration 4492, loss = 4.05842237\n",
      "Iteration 4493, loss = 4.20294091\n",
      "Iteration 4494, loss = 4.57840079\n",
      "Iteration 4495, loss = 4.96415718\n",
      "Iteration 4496, loss = 4.71404028\n",
      "Iteration 4497, loss = 3.99590190\n",
      "Iteration 4498, loss = 6.58457641\n",
      "Iteration 4499, loss = 7.13123830\n",
      "Iteration 4500, loss = 5.83043627\n",
      "Iteration 4501, loss = 4.32206720\n",
      "Iteration 4502, loss = 5.28159677\n",
      "Iteration 4503, loss = 5.22051052\n",
      "Iteration 4504, loss = 5.38198864\n",
      "Iteration 4505, loss = 5.95975413\n",
      "Iteration 4506, loss = 4.78977704\n",
      "Iteration 4507, loss = 4.39690370\n",
      "Iteration 4508, loss = 5.81214350\n",
      "Iteration 4509, loss = 4.40686503\n",
      "Iteration 4510, loss = 3.87718549\n",
      "Iteration 4511, loss = 3.98579943\n",
      "Iteration 4512, loss = 6.13021067\n",
      "Iteration 4513, loss = 7.62647459\n",
      "Iteration 4514, loss = 12.94047724\n",
      "Iteration 4515, loss = 12.71243794\n",
      "Iteration 4516, loss = 9.89146174\n",
      "Iteration 4517, loss = 7.19268687\n",
      "Iteration 4518, loss = 7.36434933\n",
      "Iteration 4519, loss = 8.23675111\n",
      "Iteration 4520, loss = 5.98319100\n",
      "Iteration 4521, loss = 4.37347193\n",
      "Iteration 4522, loss = 4.48023382\n",
      "Iteration 4523, loss = 4.09372070\n",
      "Iteration 4524, loss = 4.16717558\n",
      "Iteration 4525, loss = 3.76629029\n",
      "Iteration 4526, loss = 3.70294439\n",
      "Iteration 4527, loss = 5.95459477\n",
      "Iteration 4528, loss = 6.11293526\n",
      "Iteration 4529, loss = 6.45893964\n",
      "Iteration 4530, loss = 5.42331376\n",
      "Iteration 4531, loss = 5.92456729\n",
      "Iteration 4532, loss = 5.66646029\n",
      "Iteration 4533, loss = 6.61192896\n",
      "Iteration 4534, loss = 5.74155886\n",
      "Iteration 4535, loss = 5.05305838\n",
      "Iteration 4536, loss = 4.19826164\n",
      "Iteration 4537, loss = 3.88390953\n",
      "Iteration 4538, loss = 5.56357614\n",
      "Iteration 4539, loss = 5.58916613\n",
      "Iteration 4540, loss = 10.17996688\n",
      "Iteration 4541, loss = 6.75389231\n",
      "Iteration 4542, loss = 9.55774526\n",
      "Iteration 4543, loss = 7.60203151\n",
      "Iteration 4544, loss = 6.46441013\n",
      "Iteration 4545, loss = 4.76966150\n",
      "Iteration 4546, loss = 4.44082417\n",
      "Iteration 4547, loss = 4.47119621\n",
      "Iteration 4548, loss = 3.84377630\n",
      "Iteration 4549, loss = 4.74378142\n",
      "Iteration 4550, loss = 4.37162993\n",
      "Iteration 4551, loss = 4.86716452\n",
      "Iteration 4552, loss = 4.19632765\n",
      "Iteration 4553, loss = 4.63050685\n",
      "Iteration 4554, loss = 5.00937338\n",
      "Iteration 4555, loss = 4.55102154\n",
      "Iteration 4556, loss = 4.62776913\n",
      "Iteration 4557, loss = 4.46647333\n",
      "Iteration 4558, loss = 4.67027888\n",
      "Iteration 4559, loss = 4.10970807\n",
      "Iteration 4560, loss = 4.02672252\n",
      "Iteration 4561, loss = 4.66955749\n",
      "Iteration 4562, loss = 4.50941159\n",
      "Iteration 4563, loss = 4.97706653\n",
      "Iteration 4564, loss = 4.31819042\n",
      "Iteration 4565, loss = 4.51064667\n",
      "Iteration 4566, loss = 4.47594776\n",
      "Iteration 4567, loss = 5.40069087\n",
      "Iteration 4568, loss = 5.06501171\n",
      "Iteration 4569, loss = 4.35561045\n",
      "Iteration 4570, loss = 5.41741608\n",
      "Iteration 4571, loss = 4.10772960\n",
      "Iteration 4572, loss = 3.96501419\n",
      "Iteration 4573, loss = 3.96074874\n",
      "Iteration 4574, loss = 3.92888968\n",
      "Iteration 4575, loss = 4.19923805\n",
      "Iteration 4576, loss = 4.28812084\n",
      "Iteration 4577, loss = 3.62376778\n",
      "Iteration 4578, loss = 3.81415369\n",
      "Iteration 4579, loss = 4.75952949\n",
      "Iteration 4580, loss = 4.47483961\n",
      "Iteration 4581, loss = 4.93228504\n",
      "Iteration 4582, loss = 3.99086658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4583, loss = 4.07504273\n",
      "Iteration 4584, loss = 4.52804197\n",
      "Iteration 4585, loss = 8.02050312\n",
      "Iteration 4586, loss = 4.01077111\n",
      "Iteration 4587, loss = 3.71519598\n",
      "Iteration 4588, loss = 5.29709780\n",
      "Iteration 4589, loss = 4.10596661\n",
      "Iteration 4590, loss = 4.34725553\n",
      "Iteration 4591, loss = 4.44541194\n",
      "Iteration 4592, loss = 4.29237393\n",
      "Iteration 4593, loss = 5.67810830\n",
      "Iteration 4594, loss = 5.53520329\n",
      "Iteration 4595, loss = 4.42761987\n",
      "Iteration 4596, loss = 5.11288416\n",
      "Iteration 4597, loss = 4.61467253\n",
      "Iteration 4598, loss = 5.35406898\n",
      "Iteration 4599, loss = 6.65689931\n",
      "Iteration 4600, loss = 6.28117869\n",
      "Iteration 4601, loss = 8.49973813\n",
      "Iteration 4602, loss = 5.67930824\n",
      "Iteration 4603, loss = 6.25574803\n",
      "Iteration 4604, loss = 6.47978477\n",
      "Iteration 4605, loss = 6.05181352\n",
      "Iteration 4606, loss = 5.09217624\n",
      "Iteration 4607, loss = 4.95305485\n",
      "Iteration 4608, loss = 5.75677580\n",
      "Iteration 4609, loss = 3.91619899\n",
      "Iteration 4610, loss = 3.72625443\n",
      "Iteration 4611, loss = 3.97728862\n",
      "Iteration 4612, loss = 4.53482764\n",
      "Iteration 4613, loss = 8.03686789\n",
      "Iteration 4614, loss = 4.87046538\n",
      "Iteration 4615, loss = 4.67138290\n",
      "Iteration 4616, loss = 4.66426950\n",
      "Iteration 4617, loss = 4.85343080\n",
      "Iteration 4618, loss = 4.66299868\n",
      "Iteration 4619, loss = 3.94489759\n",
      "Iteration 4620, loss = 4.27905218\n",
      "Iteration 4621, loss = 3.97858685\n",
      "Iteration 4622, loss = 3.81488367\n",
      "Iteration 4623, loss = 3.80464905\n",
      "Iteration 4624, loss = 3.62649677\n",
      "Iteration 4625, loss = 5.01827613\n",
      "Iteration 4626, loss = 4.14442679\n",
      "Iteration 4627, loss = 4.12255553\n",
      "Iteration 4628, loss = 5.07990581\n",
      "Iteration 4629, loss = 4.41160119\n",
      "Iteration 4630, loss = 3.92089555\n",
      "Iteration 4631, loss = 4.28389117\n",
      "Iteration 4632, loss = 4.53807341\n",
      "Iteration 4633, loss = 4.96767248\n",
      "Iteration 4634, loss = 4.14184759\n",
      "Iteration 4635, loss = 3.86105578\n",
      "Iteration 4636, loss = 3.86216126\n",
      "Iteration 4637, loss = 4.13733389\n",
      "Iteration 4638, loss = 3.92802382\n",
      "Iteration 4639, loss = 4.34912633\n",
      "Iteration 4640, loss = 4.34181693\n",
      "Iteration 4641, loss = 4.58247213\n",
      "Iteration 4642, loss = 4.71594260\n",
      "Iteration 4643, loss = 6.98514569\n",
      "Iteration 4644, loss = 4.83774908\n",
      "Iteration 4645, loss = 5.12957493\n",
      "Iteration 4646, loss = 4.26459252\n",
      "Iteration 4647, loss = 3.71309689\n",
      "Iteration 4648, loss = 4.02092770\n",
      "Iteration 4649, loss = 3.84398531\n",
      "Iteration 4650, loss = 3.93113162\n",
      "Iteration 4651, loss = 4.36386124\n",
      "Iteration 4652, loss = 3.77442293\n",
      "Iteration 4653, loss = 3.56322382\n",
      "Iteration 4654, loss = 4.20208268\n",
      "Iteration 4655, loss = 4.78138270\n",
      "Iteration 4656, loss = 4.58290386\n",
      "Iteration 4657, loss = 4.24382819\n",
      "Iteration 4658, loss = 4.74294556\n",
      "Iteration 4659, loss = 4.29815807\n",
      "Iteration 4660, loss = 4.79736386\n",
      "Iteration 4661, loss = 3.78845933\n",
      "Iteration 4662, loss = 3.78764472\n",
      "Iteration 4663, loss = 3.91260562\n",
      "Iteration 4664, loss = 4.84711876\n",
      "Iteration 4665, loss = 3.95210782\n",
      "Iteration 4666, loss = 3.77116295\n",
      "Iteration 4667, loss = 4.36081504\n",
      "Iteration 4668, loss = 3.94566840\n",
      "Iteration 4669, loss = 4.31986814\n",
      "Iteration 4670, loss = 10.51023716\n",
      "Iteration 4671, loss = 6.44923488\n",
      "Iteration 4672, loss = 5.39771920\n",
      "Iteration 4673, loss = 4.61252670\n",
      "Iteration 4674, loss = 5.67245623\n",
      "Iteration 4675, loss = 5.21375485\n",
      "Iteration 4676, loss = 5.33132767\n",
      "Iteration 4677, loss = 6.80521102\n",
      "Iteration 4678, loss = 6.51779504\n",
      "Iteration 4679, loss = 5.61220466\n",
      "Iteration 4680, loss = 6.63257853\n",
      "Iteration 4681, loss = 6.16097885\n",
      "Iteration 4682, loss = 4.55486401\n",
      "Iteration 4683, loss = 5.03685901\n",
      "Iteration 4684, loss = 4.71220116\n",
      "Iteration 4685, loss = 4.54327561\n",
      "Iteration 4686, loss = 5.94888106\n",
      "Iteration 4687, loss = 4.68568015\n",
      "Iteration 4688, loss = 5.60772672\n",
      "Iteration 4689, loss = 4.66368146\n",
      "Iteration 4690, loss = 3.92874824\n",
      "Iteration 4691, loss = 5.58380513\n",
      "Iteration 4692, loss = 5.35906116\n",
      "Iteration 4693, loss = 5.31641900\n",
      "Iteration 4694, loss = 6.66413493\n",
      "Iteration 4695, loss = 5.31915465\n",
      "Iteration 4696, loss = 4.78424203\n",
      "Iteration 4697, loss = 5.99714338\n",
      "Iteration 4698, loss = 5.81091331\n",
      "Iteration 4699, loss = 5.76224816\n",
      "Iteration 4700, loss = 4.31891007\n",
      "Iteration 4701, loss = 3.80686597\n",
      "Iteration 4702, loss = 3.81180074\n",
      "Iteration 4703, loss = 3.76695803\n",
      "Iteration 4704, loss = 3.83594433\n",
      "Iteration 4705, loss = 6.29031718\n",
      "Iteration 4706, loss = 4.46268193\n",
      "Iteration 4707, loss = 4.94272765\n",
      "Iteration 4708, loss = 4.06813122\n",
      "Iteration 4709, loss = 4.00989656\n",
      "Iteration 4710, loss = 3.75008568\n",
      "Iteration 4711, loss = 6.07531452\n",
      "Iteration 4712, loss = 3.98831280\n",
      "Iteration 4713, loss = 4.67008159\n",
      "Iteration 4714, loss = 5.96465468\n",
      "Iteration 4715, loss = 5.19709204\n",
      "Iteration 4716, loss = 5.01840356\n",
      "Iteration 4717, loss = 5.33026512\n",
      "Iteration 4718, loss = 5.87694354\n",
      "Iteration 4719, loss = 6.18770960\n",
      "Iteration 4720, loss = 5.92661236\n",
      "Iteration 4721, loss = 7.39831204\n",
      "Iteration 4722, loss = 7.66662706\n",
      "Iteration 4723, loss = 7.20658900\n",
      "Iteration 4724, loss = 7.98237228\n",
      "Iteration 4725, loss = 8.97046187\n",
      "Iteration 4726, loss = 9.94251754\n",
      "Iteration 4727, loss = 7.73734518\n",
      "Iteration 4728, loss = 5.84635510\n",
      "Iteration 4729, loss = 8.77330137\n",
      "Iteration 4730, loss = 6.30797272\n",
      "Iteration 4731, loss = 6.36908091\n",
      "Iteration 4732, loss = 5.97002058\n",
      "Iteration 4733, loss = 5.90378278\n",
      "Iteration 4734, loss = 4.96755821\n",
      "Iteration 4735, loss = 4.86878417\n",
      "Iteration 4736, loss = 4.53599373\n",
      "Iteration 4737, loss = 4.78159893\n",
      "Iteration 4738, loss = 6.08607921\n",
      "Iteration 4739, loss = 5.71454961\n",
      "Iteration 4740, loss = 3.77137269\n",
      "Iteration 4741, loss = 4.44083654\n",
      "Iteration 4742, loss = 3.85621324\n",
      "Iteration 4743, loss = 4.40425619\n",
      "Iteration 4744, loss = 4.45288906\n",
      "Iteration 4745, loss = 3.99921913\n",
      "Iteration 4746, loss = 4.08780363\n",
      "Iteration 4747, loss = 3.65614137\n",
      "Iteration 4748, loss = 3.71218549\n",
      "Iteration 4749, loss = 3.70301547\n",
      "Iteration 4750, loss = 3.49127158\n",
      "Iteration 4751, loss = 3.75079074\n",
      "Iteration 4752, loss = 4.32308540\n",
      "Iteration 4753, loss = 5.52458727\n",
      "Iteration 4754, loss = 5.23740593\n",
      "Iteration 4755, loss = 4.23090723\n",
      "Iteration 4756, loss = 4.74713272\n",
      "Iteration 4757, loss = 3.71384161\n",
      "Iteration 4758, loss = 4.13333832\n",
      "Iteration 4759, loss = 3.72637289\n",
      "Iteration 4760, loss = 3.93040321\n",
      "Iteration 4761, loss = 4.76094549\n",
      "Iteration 4762, loss = 4.38516230\n",
      "Iteration 4763, loss = 4.74119869\n",
      "Iteration 4764, loss = 4.11582764\n",
      "Iteration 4765, loss = 4.51077381\n",
      "Iteration 4766, loss = 6.34226594\n",
      "Iteration 4767, loss = 6.29175096\n",
      "Iteration 4768, loss = 7.05050458\n",
      "Iteration 4769, loss = 4.94895719\n",
      "Iteration 4770, loss = 4.47660452\n",
      "Iteration 4771, loss = 4.55192918\n",
      "Iteration 4772, loss = 8.11975464\n",
      "Iteration 4773, loss = 6.51518432\n",
      "Iteration 4774, loss = 5.14166759\n",
      "Iteration 4775, loss = 4.57553964\n",
      "Iteration 4776, loss = 6.46565563\n",
      "Iteration 4777, loss = 7.03271647\n",
      "Iteration 4778, loss = 13.58363443\n",
      "Iteration 4779, loss = 10.15791631\n",
      "Iteration 4780, loss = 6.52766736\n",
      "Iteration 4781, loss = 9.19970953\n",
      "Iteration 4782, loss = 6.76716426\n",
      "Iteration 4783, loss = 8.46652182\n",
      "Iteration 4784, loss = 8.88524960\n",
      "Iteration 4785, loss = 7.69089293\n",
      "Iteration 4786, loss = 5.17643587\n",
      "Iteration 4787, loss = 5.85657353\n",
      "Iteration 4788, loss = 4.23676262\n",
      "Iteration 4789, loss = 3.79382001\n",
      "Iteration 4790, loss = 3.70032597\n",
      "Iteration 4791, loss = 4.05213250\n",
      "Iteration 4792, loss = 3.98997904\n",
      "Iteration 4793, loss = 4.38752031\n",
      "Iteration 4794, loss = 7.21072199\n",
      "Iteration 4795, loss = 5.54070302\n",
      "Iteration 4796, loss = 6.04055349\n",
      "Iteration 4797, loss = 4.74510859\n",
      "Iteration 4798, loss = 5.04050902\n",
      "Iteration 4799, loss = 3.96000372\n",
      "Iteration 4800, loss = 4.40101109\n",
      "Iteration 4801, loss = 5.36247828\n",
      "Iteration 4802, loss = 4.36666865\n",
      "Iteration 4803, loss = 4.36671217\n",
      "Iteration 4804, loss = 4.06049806\n",
      "Iteration 4805, loss = 3.55609790\n",
      "Iteration 4806, loss = 4.73821346\n",
      "Iteration 4807, loss = 4.32748804\n",
      "Iteration 4808, loss = 5.12291304\n",
      "Iteration 4809, loss = 4.17287504\n",
      "Iteration 4810, loss = 4.53702309\n",
      "Iteration 4811, loss = 4.21494253\n",
      "Iteration 4812, loss = 4.22353462\n",
      "Iteration 4813, loss = 3.88792697\n",
      "Iteration 4814, loss = 4.46238463\n",
      "Iteration 4815, loss = 3.64397721\n",
      "Iteration 4816, loss = 3.91407915\n",
      "Iteration 4817, loss = 3.72634504\n",
      "Iteration 4818, loss = 3.86835185\n",
      "Iteration 4819, loss = 3.86873094\n",
      "Iteration 4820, loss = 4.23083740\n",
      "Iteration 4821, loss = 4.57632903\n",
      "Iteration 4822, loss = 4.16595545\n",
      "Iteration 4823, loss = 4.29936405\n",
      "Iteration 4824, loss = 5.43595782\n",
      "Iteration 4825, loss = 4.09059325\n",
      "Iteration 4826, loss = 6.99459434\n",
      "Iteration 4827, loss = 5.48749612\n",
      "Iteration 4828, loss = 5.23076300\n",
      "Iteration 4829, loss = 4.03432858\n",
      "Iteration 4830, loss = 3.56978683\n",
      "Iteration 4831, loss = 3.86197970\n",
      "Iteration 4832, loss = 3.45553058\n",
      "Iteration 4833, loss = 3.64725304\n",
      "Iteration 4834, loss = 3.59707056\n",
      "Iteration 4835, loss = 3.66099371\n",
      "Iteration 4836, loss = 4.34893418\n",
      "Iteration 4837, loss = 3.99167535\n",
      "Iteration 4838, loss = 4.17871191\n",
      "Iteration 4839, loss = 3.57870554\n",
      "Iteration 4840, loss = 3.87944170\n",
      "Iteration 4841, loss = 3.73815125\n",
      "Iteration 4842, loss = 3.88563948\n",
      "Iteration 4843, loss = 4.03659562\n",
      "Iteration 4844, loss = 3.65668076\n",
      "Iteration 4845, loss = 3.91353661\n",
      "Iteration 4846, loss = 4.73917415\n",
      "Iteration 4847, loss = 4.07654417\n",
      "Iteration 4848, loss = 4.28121530\n",
      "Iteration 4849, loss = 4.34801209\n",
      "Iteration 4850, loss = 5.39760928\n",
      "Iteration 4851, loss = 5.12129451\n",
      "Iteration 4852, loss = 5.62306322\n",
      "Iteration 4853, loss = 4.26232380\n",
      "Iteration 4854, loss = 4.13029004\n",
      "Iteration 4855, loss = 3.96644701\n",
      "Iteration 4856, loss = 3.76743387\n",
      "Iteration 4857, loss = 3.89574121\n",
      "Iteration 4858, loss = 3.53740373\n",
      "Iteration 4859, loss = 3.67245487\n",
      "Iteration 4860, loss = 3.61365931\n",
      "Iteration 4861, loss = 3.60376516\n",
      "Iteration 4862, loss = 3.96562376\n",
      "Iteration 4863, loss = 5.66972053\n",
      "Iteration 4864, loss = 4.16817103\n",
      "Iteration 4865, loss = 4.25629271\n",
      "Iteration 4866, loss = 4.23935049\n",
      "Iteration 4867, loss = 3.68402877\n",
      "Iteration 4868, loss = 3.97176105\n",
      "Iteration 4869, loss = 5.45191809\n",
      "Iteration 4870, loss = 3.99404876\n",
      "Iteration 4871, loss = 3.79859720\n",
      "Iteration 4872, loss = 3.88405160\n",
      "Iteration 4873, loss = 3.81736838\n",
      "Iteration 4874, loss = 4.36227460\n",
      "Iteration 4875, loss = 4.47073799\n",
      "Iteration 4876, loss = 3.77719296\n",
      "Iteration 4877, loss = 4.20838898\n",
      "Iteration 4878, loss = 3.58556660\n",
      "Iteration 4879, loss = 3.97518532\n",
      "Iteration 4880, loss = 3.61462376\n",
      "Iteration 4881, loss = 3.74439114\n",
      "Iteration 4882, loss = 3.97448851\n",
      "Iteration 4883, loss = 5.15893936\n",
      "Iteration 4884, loss = 3.95188726\n",
      "Iteration 4885, loss = 3.75780707\n",
      "Iteration 4886, loss = 4.53600701\n",
      "Iteration 4887, loss = 5.01348251\n",
      "Iteration 4888, loss = 4.63583983\n",
      "Iteration 4889, loss = 4.85574872\n",
      "Iteration 4890, loss = 4.41161333\n",
      "Iteration 4891, loss = 4.12161204\n",
      "Iteration 4892, loss = 5.22027458\n",
      "Iteration 4893, loss = 4.81102071\n",
      "Iteration 4894, loss = 4.33989457\n",
      "Iteration 4895, loss = 4.35128123\n",
      "Iteration 4896, loss = 3.86555772\n",
      "Iteration 4897, loss = 4.09011551\n",
      "Iteration 4898, loss = 5.01114508\n",
      "Iteration 4899, loss = 3.73927695\n",
      "Iteration 4900, loss = 3.99863642\n",
      "Iteration 4901, loss = 4.06652762\n",
      "Iteration 4902, loss = 3.50937758\n",
      "Iteration 4903, loss = 3.64577000\n",
      "Iteration 4904, loss = 4.67535826\n",
      "Iteration 4905, loss = 4.38186388\n",
      "Iteration 4906, loss = 7.69262754\n",
      "Iteration 4907, loss = 8.82973852\n",
      "Iteration 4908, loss = 7.58270005\n",
      "Iteration 4909, loss = 6.45842748\n",
      "Iteration 4910, loss = 4.15998334\n",
      "Iteration 4911, loss = 4.08348342\n",
      "Iteration 4912, loss = 5.21056700\n",
      "Iteration 4913, loss = 3.60178227\n",
      "Iteration 4914, loss = 3.56401836\n",
      "Iteration 4915, loss = 3.89380343\n",
      "Iteration 4916, loss = 6.80365494\n",
      "Iteration 4917, loss = 3.52243469\n",
      "Iteration 4918, loss = 3.93882738\n",
      "Iteration 4919, loss = 4.02678155\n",
      "Iteration 4920, loss = 3.60576207\n",
      "Iteration 4921, loss = 3.87589959\n",
      "Iteration 4922, loss = 4.47271800\n",
      "Iteration 4923, loss = 4.13081166\n",
      "Iteration 4924, loss = 6.19063995\n",
      "Iteration 4925, loss = 7.52195367\n",
      "Iteration 4926, loss = 6.67048174\n",
      "Iteration 4927, loss = 4.41751961\n",
      "Iteration 4928, loss = 4.91269969\n",
      "Iteration 4929, loss = 4.10980884\n",
      "Iteration 4930, loss = 4.52627811\n",
      "Iteration 4931, loss = 4.89943220\n",
      "Iteration 4932, loss = 6.44132404\n",
      "Iteration 4933, loss = 5.16777291\n",
      "Iteration 4934, loss = 4.42336557\n",
      "Iteration 4935, loss = 3.87450807\n",
      "Iteration 4936, loss = 3.88291657\n",
      "Iteration 4937, loss = 4.31969173\n",
      "Iteration 4938, loss = 4.53637178\n",
      "Iteration 4939, loss = 4.99562662\n",
      "Iteration 4940, loss = 4.21324298\n",
      "Iteration 4941, loss = 5.54217322\n",
      "Iteration 4942, loss = 5.48219729\n",
      "Iteration 4943, loss = 5.38785628\n",
      "Iteration 4944, loss = 5.98280356\n",
      "Iteration 4945, loss = 6.81060315\n",
      "Iteration 4946, loss = 4.36111232\n",
      "Iteration 4947, loss = 4.05400867\n",
      "Iteration 4948, loss = 4.74684664\n",
      "Iteration 4949, loss = 3.94867975\n",
      "Iteration 4950, loss = 4.91879539\n",
      "Iteration 4951, loss = 6.51847687\n",
      "Iteration 4952, loss = 4.18203482\n",
      "Iteration 4953, loss = 4.34726660\n",
      "Iteration 4954, loss = 3.60840537\n",
      "Iteration 4955, loss = 4.16195589\n",
      "Iteration 4956, loss = 4.53274829\n",
      "Iteration 4957, loss = 4.90636690\n",
      "Iteration 4958, loss = 4.28709261\n",
      "Iteration 4959, loss = 3.98083341\n",
      "Iteration 4960, loss = 6.01394790\n",
      "Iteration 4961, loss = 4.40389798\n",
      "Iteration 4962, loss = 4.12553088\n",
      "Iteration 4963, loss = 4.26286188\n",
      "Iteration 4964, loss = 4.53557033\n",
      "Iteration 4965, loss = 5.53708375\n",
      "Iteration 4966, loss = 5.07239415\n",
      "Iteration 4967, loss = 4.24166162\n",
      "Iteration 4968, loss = 4.61561700\n",
      "Iteration 4969, loss = 4.02114435\n",
      "Iteration 4970, loss = 5.33249225\n",
      "Iteration 4971, loss = 7.28470583\n",
      "Iteration 4972, loss = 4.73812247\n",
      "Iteration 4973, loss = 3.79493502\n",
      "Iteration 4974, loss = 3.58837534\n",
      "Iteration 4975, loss = 3.91309930\n",
      "Iteration 4976, loss = 4.09898022\n",
      "Iteration 4977, loss = 3.69882938\n",
      "Iteration 4978, loss = 3.68086489\n",
      "Iteration 4979, loss = 4.30997271\n",
      "Iteration 4980, loss = 4.22580252\n",
      "Iteration 4981, loss = 4.06850240\n",
      "Iteration 4982, loss = 4.26754342\n",
      "Iteration 4983, loss = 4.93586749\n",
      "Iteration 4984, loss = 5.94824682\n",
      "Iteration 4985, loss = 4.88137779\n",
      "Iteration 4986, loss = 9.54581579\n",
      "Iteration 4987, loss = 14.44593754\n",
      "Iteration 4988, loss = 8.64855961\n",
      "Iteration 4989, loss = 8.71648239\n",
      "Iteration 4990, loss = 9.19201562\n",
      "Iteration 4991, loss = 4.45293516\n",
      "Iteration 4992, loss = 3.87641542\n",
      "Iteration 4993, loss = 4.10792786\n",
      "Iteration 4994, loss = 3.80962008\n",
      "Iteration 4995, loss = 3.82466087\n",
      "Iteration 4996, loss = 3.91587971\n",
      "Iteration 4997, loss = 3.53664247\n",
      "Iteration 4998, loss = 3.57896157\n",
      "Iteration 4999, loss = 3.75556519\n",
      "Iteration 5000, loss = 4.27965578\n",
      "Iteration 5001, loss = 4.53069193\n",
      "Iteration 5002, loss = 4.70136241\n",
      "Iteration 5003, loss = 4.23505383\n",
      "Iteration 5004, loss = 5.38602143\n",
      "Iteration 5005, loss = 6.33430647\n",
      "Iteration 5006, loss = 4.83726083\n",
      "Iteration 5007, loss = 5.77557599\n",
      "Iteration 5008, loss = 5.34846757\n",
      "Iteration 5009, loss = 4.72697652\n",
      "Iteration 5010, loss = 9.78306107\n",
      "Iteration 5011, loss = 8.59674312\n",
      "Iteration 5012, loss = 8.11210233\n",
      "Iteration 5013, loss = 6.43640520\n",
      "Iteration 5014, loss = 6.28773920\n",
      "Iteration 5015, loss = 5.36532918\n",
      "Iteration 5016, loss = 5.44391036\n",
      "Iteration 5017, loss = 5.68387281\n",
      "Iteration 5018, loss = 6.66582219\n",
      "Iteration 5019, loss = 6.88749249\n",
      "Iteration 5020, loss = 6.91724199\n",
      "Iteration 5021, loss = 6.66497369\n",
      "Iteration 5022, loss = 5.09632791\n",
      "Iteration 5023, loss = 4.67875185\n",
      "Iteration 5024, loss = 5.44824068\n",
      "Iteration 5025, loss = 5.04151826\n",
      "Iteration 5026, loss = 5.18763118\n",
      "Iteration 5027, loss = 3.99826202\n",
      "Iteration 5028, loss = 4.27723439\n",
      "Iteration 5029, loss = 3.84362776\n",
      "Iteration 5030, loss = 3.75711862\n",
      "Iteration 5031, loss = 3.49333069\n",
      "Iteration 5032, loss = 3.58890838\n",
      "Iteration 5033, loss = 3.88380131\n",
      "Iteration 5034, loss = 3.55253023\n",
      "Iteration 5035, loss = 3.91562001\n",
      "Iteration 5036, loss = 7.53259156\n",
      "Iteration 5037, loss = 3.63501324\n",
      "Iteration 5038, loss = 4.73965879\n",
      "Iteration 5039, loss = 4.28414873\n",
      "Iteration 5040, loss = 4.58113260\n",
      "Iteration 5041, loss = 4.51576910\n",
      "Iteration 5042, loss = 5.80277293\n",
      "Iteration 5043, loss = 4.90106725\n",
      "Iteration 5044, loss = 4.57641234\n",
      "Iteration 5045, loss = 4.13826648\n",
      "Iteration 5046, loss = 4.15750198\n",
      "Iteration 5047, loss = 5.72310875\n",
      "Iteration 5048, loss = 4.29972122\n",
      "Iteration 5049, loss = 3.85243990\n",
      "Iteration 5050, loss = 3.64705056\n",
      "Iteration 5051, loss = 4.21830779\n",
      "Iteration 5052, loss = 5.05515626\n",
      "Iteration 5053, loss = 4.38204876\n",
      "Iteration 5054, loss = 3.56332784\n",
      "Iteration 5055, loss = 3.90006965\n",
      "Iteration 5056, loss = 3.70860701\n",
      "Iteration 5057, loss = 4.69097149\n",
      "Iteration 5058, loss = 4.95002559\n",
      "Iteration 5059, loss = 4.48185112\n",
      "Iteration 5060, loss = 4.56120575\n",
      "Iteration 5061, loss = 4.55700329\n",
      "Iteration 5062, loss = 4.15237895\n",
      "Iteration 5063, loss = 4.20456957\n",
      "Iteration 5064, loss = 3.98700590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5065, loss = 6.14018277\n",
      "Iteration 5066, loss = 4.90788514\n",
      "Iteration 5067, loss = 4.15780131\n",
      "Iteration 5068, loss = 3.97554682\n",
      "Iteration 5069, loss = 3.91603747\n",
      "Iteration 5070, loss = 4.66047635\n",
      "Iteration 5071, loss = 3.85852702\n",
      "Iteration 5072, loss = 3.85537482\n",
      "Iteration 5073, loss = 3.98242882\n",
      "Iteration 5074, loss = 3.47257892\n",
      "Iteration 5075, loss = 3.47227634\n",
      "Iteration 5076, loss = 3.59444918\n",
      "Iteration 5077, loss = 3.78500461\n",
      "Iteration 5078, loss = 4.30086336\n",
      "Iteration 5079, loss = 5.04204357\n",
      "Iteration 5080, loss = 4.12586746\n",
      "Iteration 5081, loss = 5.06476514\n",
      "Iteration 5082, loss = 3.63160819\n",
      "Iteration 5083, loss = 3.45019756\n",
      "Iteration 5084, loss = 3.62591762\n",
      "Iteration 5085, loss = 3.38795203\n",
      "Iteration 5086, loss = 3.75010294\n",
      "Iteration 5087, loss = 3.40029540\n",
      "Iteration 5088, loss = 3.55221720\n",
      "Iteration 5089, loss = 3.54512522\n",
      "Iteration 5090, loss = 3.61961646\n",
      "Iteration 5091, loss = 3.46961195\n",
      "Iteration 5092, loss = 3.55412342\n",
      "Iteration 5093, loss = 3.50996383\n",
      "Iteration 5094, loss = 4.14864265\n",
      "Iteration 5095, loss = 4.15537873\n",
      "Iteration 5096, loss = 4.43910405\n",
      "Iteration 5097, loss = 4.15964708\n",
      "Iteration 5098, loss = 4.25619540\n",
      "Iteration 5099, loss = 4.41256723\n",
      "Iteration 5100, loss = 4.12777171\n",
      "Iteration 5101, loss = 3.79855730\n",
      "Iteration 5102, loss = 3.86210135\n",
      "Iteration 5103, loss = 3.62827561\n",
      "Iteration 5104, loss = 3.64434809\n",
      "Iteration 5105, loss = 3.66077578\n",
      "Iteration 5106, loss = 3.80782072\n",
      "Iteration 5107, loss = 3.62867496\n",
      "Iteration 5108, loss = 3.99406135\n",
      "Iteration 5109, loss = 3.63862828\n",
      "Iteration 5110, loss = 4.37535385\n",
      "Iteration 5111, loss = 5.07970172\n",
      "Iteration 5112, loss = 5.25072588\n",
      "Iteration 5113, loss = 4.77410349\n",
      "Iteration 5114, loss = 4.38706745\n",
      "Iteration 5115, loss = 3.70549639\n",
      "Iteration 5116, loss = 4.23892085\n",
      "Iteration 5117, loss = 3.79108978\n",
      "Iteration 5118, loss = 3.58044021\n",
      "Iteration 5119, loss = 3.77613449\n",
      "Iteration 5120, loss = 4.02019288\n",
      "Iteration 5121, loss = 3.83125371\n",
      "Iteration 5122, loss = 3.58803978\n",
      "Iteration 5123, loss = 3.64550842\n",
      "Iteration 5124, loss = 3.39231239\n",
      "Iteration 5125, loss = 3.56499041\n",
      "Iteration 5126, loss = 6.02951275\n",
      "Iteration 5127, loss = 4.37948769\n",
      "Iteration 5128, loss = 3.77416276\n",
      "Iteration 5129, loss = 4.87673517\n",
      "Iteration 5130, loss = 4.41529381\n",
      "Iteration 5131, loss = 3.64285013\n",
      "Iteration 5132, loss = 4.06438162\n",
      "Iteration 5133, loss = 4.88537546\n",
      "Iteration 5134, loss = 4.37382766\n",
      "Iteration 5135, loss = 3.95010182\n",
      "Iteration 5136, loss = 3.93077354\n",
      "Iteration 5137, loss = 3.54690235\n",
      "Iteration 5138, loss = 3.75256804\n",
      "Iteration 5139, loss = 4.63914366\n",
      "Iteration 5140, loss = 4.95236735\n",
      "Iteration 5141, loss = 3.78977431\n",
      "Iteration 5142, loss = 4.23212211\n",
      "Iteration 5143, loss = 4.31722681\n",
      "Iteration 5144, loss = 3.82267125\n",
      "Iteration 5145, loss = 3.75568214\n",
      "Iteration 5146, loss = 4.34490011\n",
      "Iteration 5147, loss = 3.87227008\n",
      "Iteration 5148, loss = 4.12529090\n",
      "Iteration 5149, loss = 4.22298087\n",
      "Iteration 5150, loss = 4.35336426\n",
      "Iteration 5151, loss = 4.15420385\n",
      "Iteration 5152, loss = 4.78126424\n",
      "Iteration 5153, loss = 4.48632203\n",
      "Iteration 5154, loss = 4.91423702\n",
      "Iteration 5155, loss = 6.71732128\n",
      "Iteration 5156, loss = 3.56688786\n",
      "Iteration 5157, loss = 4.26072225\n",
      "Iteration 5158, loss = 4.96342773\n",
      "Iteration 5159, loss = 6.14149125\n",
      "Iteration 5160, loss = 4.82165336\n",
      "Iteration 5161, loss = 5.21327444\n",
      "Iteration 5162, loss = 3.90248335\n",
      "Iteration 5163, loss = 3.95183845\n",
      "Iteration 5164, loss = 5.11574272\n",
      "Iteration 5165, loss = 4.75708589\n",
      "Iteration 5166, loss = 3.94245762\n",
      "Iteration 5167, loss = 5.53734576\n",
      "Iteration 5168, loss = 5.50756227\n",
      "Iteration 5169, loss = 6.90917483\n",
      "Iteration 5170, loss = 7.20533920\n",
      "Iteration 5171, loss = 5.71453718\n",
      "Iteration 5172, loss = 5.46185081\n",
      "Iteration 5173, loss = 9.73779977\n",
      "Iteration 5174, loss = 9.11371982\n",
      "Iteration 5175, loss = 7.46202829\n",
      "Iteration 5176, loss = 9.29032542\n",
      "Iteration 5177, loss = 8.39443787\n",
      "Iteration 5178, loss = 7.33214076\n",
      "Iteration 5179, loss = 9.15357140\n",
      "Iteration 5180, loss = 6.91494883\n",
      "Iteration 5181, loss = 5.74079323\n",
      "Iteration 5182, loss = 5.71933241\n",
      "Iteration 5183, loss = 4.91214082\n",
      "Iteration 5184, loss = 4.45764834\n",
      "Iteration 5185, loss = 4.16386045\n",
      "Iteration 5186, loss = 3.94635989\n",
      "Iteration 5187, loss = 3.72213746\n",
      "Iteration 5188, loss = 3.62878863\n",
      "Iteration 5189, loss = 4.35930452\n",
      "Iteration 5190, loss = 4.57102694\n",
      "Iteration 5191, loss = 3.89494948\n",
      "Iteration 5192, loss = 4.25918925\n",
      "Iteration 5193, loss = 3.94339576\n",
      "Iteration 5194, loss = 4.71762767\n",
      "Iteration 5195, loss = 4.53196558\n",
      "Iteration 5196, loss = 5.00297900\n",
      "Iteration 5197, loss = 5.40667627\n",
      "Iteration 5198, loss = 4.86361057\n",
      "Iteration 5199, loss = 7.18437255\n",
      "Iteration 5200, loss = 6.16647690\n",
      "Iteration 5201, loss = 7.17701962\n",
      "Iteration 5202, loss = 7.91599694\n",
      "Iteration 5203, loss = 7.84828847\n",
      "Iteration 5204, loss = 7.37592705\n",
      "Iteration 5205, loss = 4.64152003\n",
      "Iteration 5206, loss = 3.62850336\n",
      "Iteration 5207, loss = 3.75077346\n",
      "Iteration 5208, loss = 4.26107602\n",
      "Iteration 5209, loss = 3.51742949\n",
      "Iteration 5210, loss = 3.52352679\n",
      "Iteration 5211, loss = 3.97704072\n",
      "Iteration 5212, loss = 4.32909846\n",
      "Iteration 5213, loss = 4.89335069\n",
      "Iteration 5214, loss = 5.49503745\n",
      "Iteration 5215, loss = 4.16983654\n",
      "Iteration 5216, loss = 3.88547135\n",
      "Iteration 5217, loss = 4.30796780\n",
      "Iteration 5218, loss = 3.83330981\n",
      "Iteration 5219, loss = 3.52055092\n",
      "Iteration 5220, loss = 3.58715229\n",
      "Iteration 5221, loss = 4.17700460\n",
      "Iteration 5222, loss = 4.09296738\n",
      "Iteration 5223, loss = 3.55935916\n",
      "Iteration 5224, loss = 3.65024343\n",
      "Iteration 5225, loss = 3.98069758\n",
      "Iteration 5226, loss = 4.19061762\n",
      "Iteration 5227, loss = 4.82989128\n",
      "Iteration 5228, loss = 5.28080090\n",
      "Iteration 5229, loss = 5.73897609\n",
      "Iteration 5230, loss = 4.47006193\n",
      "Iteration 5231, loss = 3.81091241\n",
      "Iteration 5232, loss = 3.76294267\n",
      "Iteration 5233, loss = 3.61306870\n",
      "Iteration 5234, loss = 5.13165957\n",
      "Iteration 5235, loss = 5.72448218\n",
      "Iteration 5236, loss = 5.34600690\n",
      "Iteration 5237, loss = 4.60271847\n",
      "Iteration 5238, loss = 4.91118948\n",
      "Iteration 5239, loss = 5.51282276\n",
      "Iteration 5240, loss = 5.94102232\n",
      "Iteration 5241, loss = 5.86461468\n",
      "Iteration 5242, loss = 5.66115084\n",
      "Iteration 5243, loss = 5.93796246\n",
      "Iteration 5244, loss = 3.92375266\n",
      "Iteration 5245, loss = 5.16692210\n",
      "Iteration 5246, loss = 4.50065537\n",
      "Iteration 5247, loss = 4.31623875\n",
      "Iteration 5248, loss = 3.63349873\n",
      "Iteration 5249, loss = 4.28149360\n",
      "Iteration 5250, loss = 3.80948227\n",
      "Iteration 5251, loss = 3.60419875\n",
      "Iteration 5252, loss = 5.15481460\n",
      "Iteration 5253, loss = 4.23050554\n",
      "Iteration 5254, loss = 4.56879846\n",
      "Iteration 5255, loss = 3.88125725\n",
      "Iteration 5256, loss = 5.20768722\n",
      "Iteration 5257, loss = 4.50330551\n",
      "Iteration 5258, loss = 4.16735593\n",
      "Iteration 5259, loss = 5.45067628\n",
      "Iteration 5260, loss = 5.22021409\n",
      "Iteration 5261, loss = 5.94801405\n",
      "Iteration 5262, loss = 4.09016339\n",
      "Iteration 5263, loss = 3.74978903\n",
      "Iteration 5264, loss = 5.44756895\n",
      "Iteration 5265, loss = 12.29353044\n",
      "Iteration 5266, loss = 8.98024530\n",
      "Iteration 5267, loss = 8.08708144\n",
      "Iteration 5268, loss = 4.97404236\n",
      "Iteration 5269, loss = 3.84144929\n",
      "Iteration 5270, loss = 3.43726474\n",
      "Iteration 5271, loss = 3.63365450\n",
      "Iteration 5272, loss = 3.43225080\n",
      "Iteration 5273, loss = 3.74784297\n",
      "Iteration 5274, loss = 3.44545434\n",
      "Iteration 5275, loss = 3.55531198\n",
      "Iteration 5276, loss = 3.92370338\n",
      "Iteration 5277, loss = 3.53359090\n",
      "Iteration 5278, loss = 3.31995880\n",
      "Iteration 5279, loss = 3.78324828\n",
      "Iteration 5280, loss = 4.13760244\n",
      "Iteration 5281, loss = 4.46022402\n",
      "Iteration 5282, loss = 4.53951364\n",
      "Iteration 5283, loss = 5.91811992\n",
      "Iteration 5284, loss = 5.06716446\n",
      "Iteration 5285, loss = 4.84095303\n",
      "Iteration 5286, loss = 4.75956832\n",
      "Iteration 5287, loss = 4.27195717\n",
      "Iteration 5288, loss = 3.73916121\n",
      "Iteration 5289, loss = 4.38053498\n",
      "Iteration 5290, loss = 4.56450597\n",
      "Iteration 5291, loss = 4.84468066\n",
      "Iteration 5292, loss = 4.23844126\n",
      "Iteration 5293, loss = 4.27053098\n",
      "Iteration 5294, loss = 4.20949756\n",
      "Iteration 5295, loss = 3.68181277\n",
      "Iteration 5296, loss = 3.92053222\n",
      "Iteration 5297, loss = 3.63851831\n",
      "Iteration 5298, loss = 4.08057935\n",
      "Iteration 5299, loss = 4.62443350\n",
      "Iteration 5300, loss = 3.85732393\n",
      "Iteration 5301, loss = 4.33226985\n",
      "Iteration 5302, loss = 4.60720261\n",
      "Iteration 5303, loss = 4.80180649\n",
      "Iteration 5304, loss = 3.25649775\n",
      "Iteration 5305, loss = 3.61202048\n",
      "Iteration 5306, loss = 3.50161255\n",
      "Iteration 5307, loss = 3.38943750\n",
      "Iteration 5308, loss = 3.74960204\n",
      "Iteration 5309, loss = 6.89970512\n",
      "Iteration 5310, loss = 4.72910354\n",
      "Iteration 5311, loss = 4.62723335\n",
      "Iteration 5312, loss = 5.50124617\n",
      "Iteration 5313, loss = 6.41921821\n",
      "Iteration 5314, loss = 7.04268663\n",
      "Iteration 5315, loss = 4.96124945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5316, loss = 4.20464960\n",
      "Iteration 5317, loss = 4.25015964\n",
      "Iteration 5318, loss = 3.71338698\n",
      "Iteration 5319, loss = 3.51182358\n",
      "Iteration 5320, loss = 3.62036618\n",
      "Iteration 5321, loss = 3.80363027\n",
      "Iteration 5322, loss = 4.07027905\n",
      "Iteration 5323, loss = 4.84845584\n",
      "Iteration 5324, loss = 5.76866667\n",
      "Iteration 5325, loss = 6.84879371\n",
      "Iteration 5326, loss = 7.69324588\n",
      "Iteration 5327, loss = 6.04382147\n",
      "Iteration 5328, loss = 5.01914643\n",
      "Iteration 5329, loss = 4.25785389\n",
      "Iteration 5330, loss = 4.89954676\n",
      "Iteration 5331, loss = 4.85638468\n",
      "Iteration 5332, loss = 4.37336127\n",
      "Iteration 5333, loss = 4.39238626\n",
      "Iteration 5334, loss = 4.27953380\n",
      "Iteration 5335, loss = 3.62886811\n",
      "Iteration 5336, loss = 3.96285216\n",
      "Iteration 5337, loss = 3.35067141\n",
      "Iteration 5338, loss = 3.54243931\n",
      "Iteration 5339, loss = 3.51787766\n",
      "Iteration 5340, loss = 3.34388532\n",
      "Iteration 5341, loss = 3.65335660\n",
      "Iteration 5342, loss = 3.61424668\n",
      "Iteration 5343, loss = 3.46990541\n",
      "Iteration 5344, loss = 3.66266985\n",
      "Iteration 5345, loss = 4.23040803\n",
      "Iteration 5346, loss = 4.60314609\n",
      "Iteration 5347, loss = 5.91173067\n",
      "Iteration 5348, loss = 4.01828236\n",
      "Iteration 5349, loss = 4.64640502\n",
      "Iteration 5350, loss = 4.23229653\n",
      "Iteration 5351, loss = 3.63610636\n",
      "Iteration 5352, loss = 4.54594171\n",
      "Iteration 5353, loss = 4.82070217\n",
      "Iteration 5354, loss = 3.42494508\n",
      "Iteration 5355, loss = 4.71711041\n",
      "Iteration 5356, loss = 4.00853593\n",
      "Iteration 5357, loss = 4.34207187\n",
      "Iteration 5358, loss = 3.66710630\n",
      "Iteration 5359, loss = 3.60964038\n",
      "Iteration 5360, loss = 3.73034306\n",
      "Iteration 5361, loss = 4.17968114\n",
      "Iteration 5362, loss = 3.52779775\n",
      "Iteration 5363, loss = 3.84473512\n",
      "Iteration 5364, loss = 3.80879695\n",
      "Iteration 5365, loss = 4.20235247\n",
      "Iteration 5366, loss = 4.08692554\n",
      "Iteration 5367, loss = 5.72971876\n",
      "Iteration 5368, loss = 4.51957825\n",
      "Iteration 5369, loss = 4.75736878\n",
      "Iteration 5370, loss = 3.65305629\n",
      "Iteration 5371, loss = 3.40363361\n",
      "Iteration 5372, loss = 3.43001864\n",
      "Iteration 5373, loss = 3.39230313\n",
      "Iteration 5374, loss = 3.56586854\n",
      "Iteration 5375, loss = 3.90712424\n",
      "Iteration 5376, loss = 3.42603922\n",
      "Iteration 5377, loss = 3.37609450\n",
      "Iteration 5378, loss = 3.58084018\n",
      "Iteration 5379, loss = 3.29881928\n",
      "Iteration 5380, loss = 4.05543384\n",
      "Iteration 5381, loss = 4.44806907\n",
      "Iteration 5382, loss = 3.83663232\n",
      "Iteration 5383, loss = 3.64637844\n",
      "Iteration 5384, loss = 4.58691959\n",
      "Iteration 5385, loss = 4.91355933\n",
      "Iteration 5386, loss = 4.67977026\n",
      "Iteration 5387, loss = 5.11176538\n",
      "Iteration 5388, loss = 5.53527852\n",
      "Iteration 5389, loss = 4.89031771\n",
      "Iteration 5390, loss = 4.52561457\n",
      "Iteration 5391, loss = 3.95392471\n",
      "Iteration 5392, loss = 4.17865290\n",
      "Iteration 5393, loss = 4.23497781\n",
      "Iteration 5394, loss = 3.82542921\n",
      "Iteration 5395, loss = 3.85650004\n",
      "Iteration 5396, loss = 3.43166105\n",
      "Iteration 5397, loss = 3.56181183\n",
      "Iteration 5398, loss = 3.95035931\n",
      "Iteration 5399, loss = 5.57263292\n",
      "Iteration 5400, loss = 4.05735445\n",
      "Iteration 5401, loss = 3.88780515\n",
      "Iteration 5402, loss = 3.99505106\n",
      "Iteration 5403, loss = 5.17847905\n",
      "Iteration 5404, loss = 5.17623632\n",
      "Iteration 5405, loss = 4.49880712\n",
      "Iteration 5406, loss = 4.16061382\n",
      "Iteration 5407, loss = 5.65636946\n",
      "Iteration 5408, loss = 4.90816239\n",
      "Iteration 5409, loss = 5.31256329\n",
      "Iteration 5410, loss = 4.24224294\n",
      "Iteration 5411, loss = 3.93283525\n",
      "Iteration 5412, loss = 5.41488679\n",
      "Iteration 5413, loss = 6.70523030\n",
      "Iteration 5414, loss = 8.63919204\n",
      "Iteration 5415, loss = 7.91163730\n",
      "Iteration 5416, loss = 5.36721771\n",
      "Iteration 5417, loss = 5.60866305\n",
      "Iteration 5418, loss = 5.74586008\n",
      "Iteration 5419, loss = 5.98671980\n",
      "Iteration 5420, loss = 6.40851407\n",
      "Iteration 5421, loss = 4.92363113\n",
      "Iteration 5422, loss = 4.96989384\n",
      "Iteration 5423, loss = 3.68398328\n",
      "Iteration 5424, loss = 3.62370459\n",
      "Iteration 5425, loss = 3.86627075\n",
      "Iteration 5426, loss = 3.56624961\n",
      "Iteration 5427, loss = 3.56517542\n",
      "Iteration 5428, loss = 5.18350023\n",
      "Iteration 5429, loss = 4.95596187\n",
      "Iteration 5430, loss = 4.09795331\n",
      "Iteration 5431, loss = 4.58984433\n",
      "Iteration 5432, loss = 4.39066707\n",
      "Iteration 5433, loss = 4.78844717\n",
      "Iteration 5434, loss = 5.23411618\n",
      "Iteration 5435, loss = 4.70965755\n",
      "Iteration 5436, loss = 4.49763575\n",
      "Iteration 5437, loss = 4.18372799\n",
      "Iteration 5438, loss = 4.19199582\n",
      "Iteration 5439, loss = 6.27044389\n",
      "Iteration 5440, loss = 4.61878277\n",
      "Iteration 5441, loss = 3.54800860\n",
      "Iteration 5442, loss = 4.09489105\n",
      "Iteration 5443, loss = 4.35013347\n",
      "Iteration 5444, loss = 3.88889659\n",
      "Iteration 5445, loss = 4.99257862\n",
      "Iteration 5446, loss = 3.45681536\n",
      "Iteration 5447, loss = 3.91376979\n",
      "Iteration 5448, loss = 4.34695133\n",
      "Iteration 5449, loss = 3.84101596\n",
      "Iteration 5450, loss = 5.04940573\n",
      "Iteration 5451, loss = 6.56854701\n",
      "Iteration 5452, loss = 4.93415888\n",
      "Iteration 5453, loss = 5.20122510\n",
      "Iteration 5454, loss = 5.30680993\n",
      "Iteration 5455, loss = 4.85643201\n",
      "Iteration 5456, loss = 4.18619565\n",
      "Iteration 5457, loss = 4.06600971\n",
      "Iteration 5458, loss = 3.83093792\n",
      "Iteration 5459, loss = 3.61006371\n",
      "Iteration 5460, loss = 4.06191064\n",
      "Iteration 5461, loss = 4.04978370\n",
      "Iteration 5462, loss = 3.54417272\n",
      "Iteration 5463, loss = 3.92440305\n",
      "Iteration 5464, loss = 6.76691917\n",
      "Iteration 5465, loss = 4.30133643\n",
      "Iteration 5466, loss = 3.78807157\n",
      "Iteration 5467, loss = 3.81530696\n",
      "Iteration 5468, loss = 3.57768265\n",
      "Iteration 5469, loss = 4.09377892\n",
      "Iteration 5470, loss = 4.57285398\n",
      "Iteration 5471, loss = 4.35804559\n",
      "Iteration 5472, loss = 4.58659766\n",
      "Iteration 5473, loss = 3.98700789\n",
      "Iteration 5474, loss = 3.50896803\n",
      "Iteration 5475, loss = 4.38447888\n",
      "Iteration 5476, loss = 5.01616538\n",
      "Iteration 5477, loss = 4.72670622\n",
      "Iteration 5478, loss = 4.18476102\n",
      "Iteration 5479, loss = 4.34038629\n",
      "Iteration 5480, loss = 4.36115023\n",
      "Iteration 5481, loss = 4.19358184\n",
      "Iteration 5482, loss = 3.82496202\n",
      "Iteration 5483, loss = 5.01165105\n",
      "Iteration 5484, loss = 4.55477334\n",
      "Iteration 5485, loss = 5.38458045\n",
      "Iteration 5486, loss = 3.93376924\n",
      "Iteration 5487, loss = 3.64596357\n",
      "Iteration 5488, loss = 3.46424542\n",
      "Iteration 5489, loss = 3.70646733\n",
      "Iteration 5490, loss = 3.84823179\n",
      "Iteration 5491, loss = 3.51999927\n",
      "Iteration 5492, loss = 3.46451767\n",
      "Iteration 5493, loss = 3.41606482\n",
      "Iteration 5494, loss = 3.31337665\n",
      "Iteration 5495, loss = 3.45857786\n",
      "Iteration 5496, loss = 3.58436283\n",
      "Iteration 5497, loss = 4.10309763\n",
      "Iteration 5498, loss = 3.95543260\n",
      "Iteration 5499, loss = 3.42777716\n",
      "Iteration 5500, loss = 3.57399564\n",
      "Iteration 5501, loss = 3.52686041\n",
      "Iteration 5502, loss = 4.34103283\n",
      "Iteration 5503, loss = 4.24825485\n",
      "Iteration 5504, loss = 4.40393978\n",
      "Iteration 5505, loss = 4.05441316\n",
      "Iteration 5506, loss = 4.44468263\n",
      "Iteration 5507, loss = 4.65343212\n",
      "Iteration 5508, loss = 4.81128619\n",
      "Iteration 5509, loss = 3.88428025\n",
      "Iteration 5510, loss = 4.27151109\n",
      "Iteration 5511, loss = 5.24686332\n",
      "Iteration 5512, loss = 5.86677570\n",
      "Iteration 5513, loss = 5.69951122\n",
      "Iteration 5514, loss = 6.59541103\n",
      "Iteration 5515, loss = 4.34823947\n",
      "Iteration 5516, loss = 3.73448759\n",
      "Iteration 5517, loss = 3.83491845\n",
      "Iteration 5518, loss = 3.98665236\n",
      "Iteration 5519, loss = 3.94164741\n",
      "Iteration 5520, loss = 4.75845505\n",
      "Iteration 5521, loss = 4.58657071\n",
      "Iteration 5522, loss = 3.59892864\n",
      "Iteration 5523, loss = 3.75032868\n",
      "Iteration 5524, loss = 4.67054304\n",
      "Iteration 5525, loss = 3.68448967\n",
      "Iteration 5526, loss = 3.66901536\n",
      "Iteration 5527, loss = 3.42288791\n",
      "Iteration 5528, loss = 3.40888945\n",
      "Iteration 5529, loss = 5.21587030\n",
      "Iteration 5530, loss = 4.68929873\n",
      "Iteration 5531, loss = 4.64378975\n",
      "Iteration 5532, loss = 4.28896301\n",
      "Iteration 5533, loss = 3.77191613\n",
      "Iteration 5534, loss = 3.63589916\n",
      "Iteration 5535, loss = 3.81108090\n",
      "Iteration 5536, loss = 4.27843742\n",
      "Iteration 5537, loss = 5.68558160\n",
      "Iteration 5538, loss = 4.43992036\n",
      "Iteration 5539, loss = 4.70163234\n",
      "Iteration 5540, loss = 3.74579362\n",
      "Iteration 5541, loss = 3.76364782\n",
      "Iteration 5542, loss = 3.71267504\n",
      "Iteration 5543, loss = 3.37281972\n",
      "Iteration 5544, loss = 3.66569139\n",
      "Iteration 5545, loss = 3.80889301\n",
      "Iteration 5546, loss = 3.37721904\n",
      "Iteration 5547, loss = 3.60306622\n",
      "Iteration 5548, loss = 4.38497493\n",
      "Iteration 5549, loss = 4.83881477\n",
      "Iteration 5550, loss = 3.94683472\n",
      "Iteration 5551, loss = 3.52629603\n",
      "Iteration 5552, loss = 4.03394387\n",
      "Iteration 5553, loss = 4.35948702\n",
      "Iteration 5554, loss = 4.79972783\n",
      "Iteration 5555, loss = 4.67136434\n",
      "Iteration 5556, loss = 5.01212888\n",
      "Iteration 5557, loss = 5.75726484\n",
      "Iteration 5558, loss = 4.80309392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5559, loss = 4.34190527\n",
      "Iteration 5560, loss = 4.16257901\n",
      "Iteration 5561, loss = 4.04488718\n",
      "Iteration 5562, loss = 4.50115547\n",
      "Iteration 5563, loss = 4.13353808\n",
      "Iteration 5564, loss = 3.81396172\n",
      "Iteration 5565, loss = 5.31244733\n",
      "Iteration 5566, loss = 5.89352408\n",
      "Iteration 5567, loss = 5.22476382\n",
      "Iteration 5568, loss = 5.43622462\n",
      "Iteration 5569, loss = 5.84059412\n",
      "Iteration 5570, loss = 4.56383359\n",
      "Iteration 5571, loss = 3.81265953\n",
      "Iteration 5572, loss = 4.25568289\n",
      "Iteration 5573, loss = 4.26831728\n",
      "Iteration 5574, loss = 4.16500396\n",
      "Iteration 5575, loss = 3.80704573\n",
      "Iteration 5576, loss = 4.47710304\n",
      "Iteration 5577, loss = 4.26138784\n",
      "Iteration 5578, loss = 5.68779549\n",
      "Iteration 5579, loss = 6.06129567\n",
      "Iteration 5580, loss = 5.23736317\n",
      "Iteration 5581, loss = 5.33551969\n",
      "Iteration 5582, loss = 4.04282743\n",
      "Iteration 5583, loss = 3.71701542\n",
      "Iteration 5584, loss = 3.76167774\n",
      "Iteration 5585, loss = 3.49991427\n",
      "Iteration 5586, loss = 3.99712544\n",
      "Iteration 5587, loss = 4.00270489\n",
      "Iteration 5588, loss = 3.91765157\n",
      "Iteration 5589, loss = 3.56660959\n",
      "Iteration 5590, loss = 3.91794898\n",
      "Iteration 5591, loss = 3.51070682\n",
      "Iteration 5592, loss = 3.57019210\n",
      "Iteration 5593, loss = 3.71028931\n",
      "Iteration 5594, loss = 3.54423979\n",
      "Iteration 5595, loss = 3.47420449\n",
      "Iteration 5596, loss = 3.22422330\n",
      "Iteration 5597, loss = 3.43387011\n",
      "Iteration 5598, loss = 3.49071385\n",
      "Iteration 5599, loss = 4.64776740\n",
      "Iteration 5600, loss = 6.54834195\n",
      "Iteration 5601, loss = 4.32021333\n",
      "Iteration 5602, loss = 4.03014044\n",
      "Iteration 5603, loss = 4.42766468\n",
      "Iteration 5604, loss = 4.68079488\n",
      "Iteration 5605, loss = 4.17469253\n",
      "Iteration 5606, loss = 3.77907616\n",
      "Iteration 5607, loss = 3.51585247\n",
      "Iteration 5608, loss = 4.28637544\n",
      "Iteration 5609, loss = 4.33270862\n",
      "Iteration 5610, loss = 3.80260133\n",
      "Iteration 5611, loss = 5.40912740\n",
      "Iteration 5612, loss = 4.54305441\n",
      "Iteration 5613, loss = 4.03505088\n",
      "Iteration 5614, loss = 4.76248942\n",
      "Iteration 5615, loss = 4.40731021\n",
      "Iteration 5616, loss = 4.26385451\n",
      "Iteration 5617, loss = 5.46830287\n",
      "Iteration 5618, loss = 5.23064190\n",
      "Iteration 5619, loss = 4.04411378\n",
      "Iteration 5620, loss = 4.14330863\n",
      "Iteration 5621, loss = 6.09908078\n",
      "Iteration 5622, loss = 5.35589940\n",
      "Iteration 5623, loss = 5.19329756\n",
      "Iteration 5624, loss = 4.46552444\n",
      "Iteration 5625, loss = 5.58847314\n",
      "Iteration 5626, loss = 5.47971406\n",
      "Iteration 5627, loss = 4.25759152\n",
      "Iteration 5628, loss = 5.27361116\n",
      "Iteration 5629, loss = 5.71810410\n",
      "Iteration 5630, loss = 3.99448811\n",
      "Iteration 5631, loss = 3.99684242\n",
      "Iteration 5632, loss = 3.89779907\n",
      "Iteration 5633, loss = 7.38098098\n",
      "Iteration 5634, loss = 6.34619140\n",
      "Iteration 5635, loss = 6.71993708\n",
      "Iteration 5636, loss = 6.30339286\n",
      "Iteration 5637, loss = 4.38551526\n",
      "Iteration 5638, loss = 3.74338830\n",
      "Iteration 5639, loss = 4.78845094\n",
      "Iteration 5640, loss = 3.98272639\n",
      "Iteration 5641, loss = 4.15824132\n",
      "Iteration 5642, loss = 4.62449692\n",
      "Iteration 5643, loss = 5.28611201\n",
      "Iteration 5644, loss = 4.09954426\n",
      "Iteration 5645, loss = 4.93408092\n",
      "Iteration 5646, loss = 4.48046617\n",
      "Iteration 5647, loss = 4.58975093\n",
      "Iteration 5648, loss = 4.03517222\n",
      "Iteration 5649, loss = 4.35828926\n",
      "Iteration 5650, loss = 9.75145581\n",
      "Iteration 5651, loss = 14.10160999\n",
      "Iteration 5652, loss = 13.45654615\n",
      "Iteration 5653, loss = 10.41216608\n",
      "Iteration 5654, loss = 6.98033352\n",
      "Iteration 5655, loss = 5.97627188\n",
      "Iteration 5656, loss = 4.67861885\n",
      "Iteration 5657, loss = 4.18766754\n",
      "Iteration 5658, loss = 3.42081339\n",
      "Iteration 5659, loss = 3.74942094\n",
      "Iteration 5660, loss = 3.72657763\n",
      "Iteration 5661, loss = 3.67936404\n",
      "Iteration 5662, loss = 4.54703129\n",
      "Iteration 5663, loss = 4.28572413\n",
      "Iteration 5664, loss = 4.14665604\n",
      "Iteration 5665, loss = 4.25724783\n",
      "Iteration 5666, loss = 4.59668588\n",
      "Iteration 5667, loss = 3.75406864\n",
      "Iteration 5668, loss = 3.33646541\n",
      "Iteration 5669, loss = 3.69732971\n",
      "Iteration 5670, loss = 4.42465041\n",
      "Iteration 5671, loss = 3.99224860\n",
      "Iteration 5672, loss = 3.69762002\n",
      "Iteration 5673, loss = 3.36218444\n",
      "Iteration 5674, loss = 3.76888626\n",
      "Iteration 5675, loss = 3.79605906\n",
      "Iteration 5676, loss = 4.79289148\n",
      "Iteration 5677, loss = 5.79333694\n",
      "Iteration 5678, loss = 5.04160778\n",
      "Iteration 5679, loss = 5.26751948\n",
      "Iteration 5680, loss = 4.39584957\n",
      "Iteration 5681, loss = 4.20785064\n",
      "Iteration 5682, loss = 5.07293944\n",
      "Iteration 5683, loss = 3.94026490\n",
      "Iteration 5684, loss = 3.89444244\n",
      "Iteration 5685, loss = 4.15911073\n",
      "Iteration 5686, loss = 4.38679763\n",
      "Iteration 5687, loss = 4.56677572\n",
      "Iteration 5688, loss = 4.49247173\n",
      "Iteration 5689, loss = 4.75355977\n",
      "Iteration 5690, loss = 3.73978621\n",
      "Iteration 5691, loss = 3.44658404\n",
      "Iteration 5692, loss = 4.23263037\n",
      "Iteration 5693, loss = 4.00414508\n",
      "Iteration 5694, loss = 3.88377464\n",
      "Iteration 5695, loss = 3.51980335\n",
      "Iteration 5696, loss = 3.26416790\n",
      "Iteration 5697, loss = 3.54710450\n",
      "Iteration 5698, loss = 3.58988021\n",
      "Iteration 5699, loss = 4.90538966\n",
      "Iteration 5700, loss = 4.15740661\n",
      "Iteration 5701, loss = 4.32777278\n",
      "Iteration 5702, loss = 5.31948390\n",
      "Iteration 5703, loss = 5.33057705\n",
      "Iteration 5704, loss = 4.58831825\n",
      "Iteration 5705, loss = 3.88386990\n",
      "Iteration 5706, loss = 3.80177308\n",
      "Iteration 5707, loss = 4.72573201\n",
      "Iteration 5708, loss = 4.35542068\n",
      "Iteration 5709, loss = 4.44131678\n",
      "Iteration 5710, loss = 6.15780365\n",
      "Iteration 5711, loss = 5.39620618\n",
      "Iteration 5712, loss = 4.36516463\n",
      "Iteration 5713, loss = 3.67661346\n",
      "Iteration 5714, loss = 3.69000584\n",
      "Iteration 5715, loss = 3.61032859\n",
      "Iteration 5716, loss = 3.26251985\n",
      "Iteration 5717, loss = 4.11985351\n",
      "Iteration 5718, loss = 7.57835113\n",
      "Iteration 5719, loss = 7.53804053\n",
      "Iteration 5720, loss = 7.70501671\n",
      "Iteration 5721, loss = 6.36428267\n",
      "Iteration 5722, loss = 6.36870587\n",
      "Iteration 5723, loss = 4.37346485\n",
      "Iteration 5724, loss = 4.31627960\n",
      "Iteration 5725, loss = 4.80404089\n",
      "Iteration 5726, loss = 4.32858251\n",
      "Iteration 5727, loss = 4.63541420\n",
      "Iteration 5728, loss = 4.59362516\n",
      "Iteration 5729, loss = 3.90394191\n",
      "Iteration 5730, loss = 3.80753877\n",
      "Iteration 5731, loss = 3.76278378\n",
      "Iteration 5732, loss = 3.75323333\n",
      "Iteration 5733, loss = 3.48454525\n",
      "Iteration 5734, loss = 3.86549219\n",
      "Iteration 5735, loss = 4.41086127\n",
      "Iteration 5736, loss = 3.66434407\n",
      "Iteration 5737, loss = 3.35581174\n",
      "Iteration 5738, loss = 3.58660886\n",
      "Iteration 5739, loss = 3.32190242\n",
      "Iteration 5740, loss = 5.31665375\n",
      "Iteration 5741, loss = 4.22396207\n",
      "Iteration 5742, loss = 3.67221864\n",
      "Iteration 5743, loss = 3.70619770\n",
      "Iteration 5744, loss = 4.45802077\n",
      "Iteration 5745, loss = 4.38523996\n",
      "Iteration 5746, loss = 4.42107485\n",
      "Iteration 5747, loss = 4.80760649\n",
      "Iteration 5748, loss = 3.95151199\n",
      "Iteration 5749, loss = 4.27837184\n",
      "Iteration 5750, loss = 4.99557160\n",
      "Iteration 5751, loss = 5.11105631\n",
      "Iteration 5752, loss = 4.75147317\n",
      "Iteration 5753, loss = 4.37222367\n",
      "Iteration 5754, loss = 4.48710393\n",
      "Iteration 5755, loss = 6.77076699\n",
      "Iteration 5756, loss = 5.95025873\n",
      "Iteration 5757, loss = 4.41937081\n",
      "Iteration 5758, loss = 4.36608489\n",
      "Iteration 5759, loss = 6.16548632\n",
      "Iteration 5760, loss = 5.22042238\n",
      "Iteration 5761, loss = 4.20860751\n",
      "Iteration 5762, loss = 3.13971408\n",
      "Iteration 5763, loss = 3.70047724\n",
      "Iteration 5764, loss = 5.13802710\n",
      "Iteration 5765, loss = 4.17077033\n",
      "Iteration 5766, loss = 3.27139079\n",
      "Iteration 5767, loss = 3.50564470\n",
      "Iteration 5768, loss = 3.60746077\n",
      "Iteration 5769, loss = 4.38273076\n",
      "Iteration 5770, loss = 3.38961871\n",
      "Iteration 5771, loss = 4.31965282\n",
      "Iteration 5772, loss = 4.66807516\n",
      "Iteration 5773, loss = 3.79225691\n",
      "Iteration 5774, loss = 3.57729038\n",
      "Iteration 5775, loss = 3.79097010\n",
      "Iteration 5776, loss = 4.01442748\n",
      "Iteration 5777, loss = 4.28929590\n",
      "Iteration 5778, loss = 3.83879629\n",
      "Iteration 5779, loss = 4.44752464\n",
      "Iteration 5780, loss = 3.54331716\n",
      "Iteration 5781, loss = 4.08843026\n",
      "Iteration 5782, loss = 3.99781569\n",
      "Iteration 5783, loss = 4.41926638\n",
      "Iteration 5784, loss = 5.18248625\n",
      "Iteration 5785, loss = 4.36244192\n",
      "Iteration 5786, loss = 3.61538444\n",
      "Iteration 5787, loss = 3.99562996\n",
      "Iteration 5788, loss = 3.92379396\n",
      "Iteration 5789, loss = 3.74067410\n",
      "Iteration 5790, loss = 3.73213684\n",
      "Iteration 5791, loss = 4.14635003\n",
      "Iteration 5792, loss = 4.12333845\n",
      "Iteration 5793, loss = 3.72442557\n",
      "Iteration 5794, loss = 3.93294055\n",
      "Iteration 5795, loss = 3.61265508\n",
      "Iteration 5796, loss = 3.71120389\n",
      "Iteration 5797, loss = 4.68035831\n",
      "Iteration 5798, loss = 4.58549519\n",
      "Iteration 5799, loss = 3.44211193\n",
      "Iteration 5800, loss = 3.56423443\n",
      "Iteration 5801, loss = 3.77540435\n",
      "Iteration 5802, loss = 3.83284023\n",
      "Iteration 5803, loss = 3.48293705\n",
      "Iteration 5804, loss = 3.93642705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5805, loss = 3.82780164\n",
      "Iteration 5806, loss = 3.38005329\n",
      "Iteration 5807, loss = 3.46583706\n",
      "Iteration 5808, loss = 3.40744388\n",
      "Iteration 5809, loss = 3.45657953\n",
      "Iteration 5810, loss = 3.71322293\n",
      "Iteration 5811, loss = 3.67770940\n",
      "Iteration 5812, loss = 3.77544748\n",
      "Iteration 5813, loss = 4.29224246\n",
      "Iteration 5814, loss = 6.72162795\n",
      "Iteration 5815, loss = 8.04383923\n",
      "Iteration 5816, loss = 7.02689912\n",
      "Iteration 5817, loss = 4.69682662\n",
      "Iteration 5818, loss = 3.99327502\n",
      "Iteration 5819, loss = 4.16010151\n",
      "Iteration 5820, loss = 5.49004182\n",
      "Iteration 5821, loss = 4.69236350\n",
      "Iteration 5822, loss = 4.98689964\n",
      "Iteration 5823, loss = 4.66100754\n",
      "Iteration 5824, loss = 5.15943197\n",
      "Iteration 5825, loss = 4.52778601\n",
      "Iteration 5826, loss = 3.79701982\n",
      "Iteration 5827, loss = 4.05711288\n",
      "Iteration 5828, loss = 4.68906104\n",
      "Iteration 5829, loss = 4.51224932\n",
      "Iteration 5830, loss = 4.41690709\n",
      "Iteration 5831, loss = 4.98305107\n",
      "Iteration 5832, loss = 4.75832312\n",
      "Iteration 5833, loss = 4.48593252\n",
      "Iteration 5834, loss = 4.10583246\n",
      "Iteration 5835, loss = 4.01040761\n",
      "Iteration 5836, loss = 3.66161300\n",
      "Iteration 5837, loss = 3.48895989\n",
      "Iteration 5838, loss = 3.53910008\n",
      "Iteration 5839, loss = 3.39169840\n",
      "Iteration 5840, loss = 3.44866457\n",
      "Iteration 5841, loss = 3.60216444\n",
      "Iteration 5842, loss = 4.10410021\n",
      "Iteration 5843, loss = 4.33480240\n",
      "Iteration 5844, loss = 3.69249508\n",
      "Iteration 5845, loss = 3.39751514\n",
      "Iteration 5846, loss = 3.99203463\n",
      "Iteration 5847, loss = 3.59692278\n",
      "Iteration 5848, loss = 4.23418100\n",
      "Iteration 5849, loss = 4.26924310\n",
      "Iteration 5850, loss = 4.45281038\n",
      "Iteration 5851, loss = 3.99696817\n",
      "Iteration 5852, loss = 3.66924280\n",
      "Iteration 5853, loss = 3.47189389\n",
      "Iteration 5854, loss = 3.66824415\n",
      "Iteration 5855, loss = 4.29046499\n",
      "Iteration 5856, loss = 4.01254160\n",
      "Iteration 5857, loss = 3.62498201\n",
      "Iteration 5858, loss = 4.12353592\n",
      "Iteration 5859, loss = 3.38451239\n",
      "Iteration 5860, loss = 4.03263957\n",
      "Iteration 5861, loss = 4.63440510\n",
      "Iteration 5862, loss = 4.26737109\n",
      "Iteration 5863, loss = 5.83406543\n",
      "Iteration 5864, loss = 5.58393806\n",
      "Iteration 5865, loss = 4.25439737\n",
      "Iteration 5866, loss = 3.63773984\n",
      "Iteration 5867, loss = 3.60099775\n",
      "Iteration 5868, loss = 3.90010626\n",
      "Iteration 5869, loss = 3.64677150\n",
      "Iteration 5870, loss = 3.48681015\n",
      "Iteration 5871, loss = 3.56406039\n",
      "Iteration 5872, loss = 3.40347586\n",
      "Iteration 5873, loss = 3.64407094\n",
      "Iteration 5874, loss = 4.49576423\n",
      "Iteration 5875, loss = 4.86763372\n",
      "Iteration 5876, loss = 4.89437726\n",
      "Iteration 5877, loss = 4.96989719\n",
      "Iteration 5878, loss = 4.61320399\n",
      "Iteration 5879, loss = 3.97216427\n",
      "Iteration 5880, loss = 4.23016634\n",
      "Iteration 5881, loss = 4.04357187\n",
      "Iteration 5882, loss = 4.69523807\n",
      "Iteration 5883, loss = 3.69390080\n",
      "Iteration 5884, loss = 3.94545002\n",
      "Iteration 5885, loss = 3.83455719\n",
      "Iteration 5886, loss = 3.88005681\n",
      "Iteration 5887, loss = 4.22789543\n",
      "Iteration 5888, loss = 4.95854167\n",
      "Iteration 5889, loss = 4.81465053\n",
      "Iteration 5890, loss = 3.67524936\n",
      "Iteration 5891, loss = 3.52706028\n",
      "Iteration 5892, loss = 6.24840493\n",
      "Iteration 5893, loss = 5.58489842\n",
      "Iteration 5894, loss = 5.11059536\n",
      "Iteration 5895, loss = 5.34495657\n",
      "Iteration 5896, loss = 4.23368634\n",
      "Iteration 5897, loss = 3.71451179\n",
      "Iteration 5898, loss = 3.77477889\n",
      "Iteration 5899, loss = 3.84670994\n",
      "Iteration 5900, loss = 3.59540637\n",
      "Iteration 5901, loss = 3.61744443\n",
      "Iteration 5902, loss = 3.75850502\n",
      "Iteration 5903, loss = 3.83719124\n",
      "Iteration 5904, loss = 4.04834151\n",
      "Iteration 5905, loss = 4.58988677\n",
      "Iteration 5906, loss = 3.57523403\n",
      "Iteration 5907, loss = 3.29142886\n",
      "Iteration 5908, loss = 3.43629426\n",
      "Iteration 5909, loss = 3.23012283\n",
      "Iteration 5910, loss = 3.49037865\n",
      "Iteration 5911, loss = 3.56082462\n",
      "Iteration 5912, loss = 4.74806054\n",
      "Iteration 5913, loss = 6.04101630\n",
      "Iteration 5914, loss = 5.78443947\n",
      "Iteration 5915, loss = 5.13386983\n",
      "Iteration 5916, loss = 3.95314013\n",
      "Iteration 5917, loss = 5.70036950\n",
      "Iteration 5918, loss = 6.25695130\n",
      "Iteration 5919, loss = 5.51130234\n",
      "Iteration 5920, loss = 4.12655875\n",
      "Iteration 5921, loss = 3.86662266\n",
      "Iteration 5922, loss = 3.48954785\n",
      "Iteration 5923, loss = 3.57483561\n",
      "Iteration 5924, loss = 3.26426677\n",
      "Iteration 5925, loss = 3.91681038\n",
      "Iteration 5926, loss = 3.54607263\n",
      "Iteration 5927, loss = 4.31074586\n",
      "Iteration 5928, loss = 4.88838820\n",
      "Iteration 5929, loss = 5.05184298\n",
      "Iteration 5930, loss = 4.46626598\n",
      "Iteration 5931, loss = 4.28012608\n",
      "Iteration 5932, loss = 3.98448808\n",
      "Iteration 5933, loss = 4.77348009\n",
      "Iteration 5934, loss = 4.85449967\n",
      "Iteration 5935, loss = 4.73860654\n",
      "Iteration 5936, loss = 4.11667342\n",
      "Iteration 5937, loss = 3.73794690\n",
      "Iteration 5938, loss = 3.93001311\n",
      "Iteration 5939, loss = 4.04760138\n",
      "Iteration 5940, loss = 3.51769872\n",
      "Iteration 5941, loss = 4.49183667\n",
      "Iteration 5942, loss = 4.84907113\n",
      "Iteration 5943, loss = 7.71184438\n",
      "Iteration 5944, loss = 6.35662988\n",
      "Iteration 5945, loss = 4.21602374\n",
      "Iteration 5946, loss = 4.52038656\n",
      "Iteration 5947, loss = 4.67212518\n",
      "Iteration 5948, loss = 5.64389527\n",
      "Iteration 5949, loss = 6.37823723\n",
      "Iteration 5950, loss = 6.80267282\n",
      "Iteration 5951, loss = 6.93573715\n",
      "Iteration 5952, loss = 4.73523260\n",
      "Iteration 5953, loss = 3.79694769\n",
      "Iteration 5954, loss = 3.70839791\n",
      "Iteration 5955, loss = 4.73938716\n",
      "Iteration 5956, loss = 4.25893419\n",
      "Iteration 5957, loss = 4.52038559\n",
      "Iteration 5958, loss = 4.70045215\n",
      "Iteration 5959, loss = 4.42072281\n",
      "Iteration 5960, loss = 3.40713169\n",
      "Iteration 5961, loss = 3.33609695\n",
      "Iteration 5962, loss = 3.27546523\n",
      "Iteration 5963, loss = 3.35529100\n",
      "Iteration 5964, loss = 3.80946713\n",
      "Iteration 5965, loss = 5.43417907\n",
      "Iteration 5966, loss = 5.04734560\n",
      "Iteration 5967, loss = 4.59151842\n",
      "Iteration 5968, loss = 4.58927708\n",
      "Iteration 5969, loss = 4.85632187\n",
      "Iteration 5970, loss = 3.85918403\n",
      "Iteration 5971, loss = 4.18918143\n",
      "Iteration 5972, loss = 4.75431519\n",
      "Iteration 5973, loss = 5.03412032\n",
      "Iteration 5974, loss = 5.28816653\n",
      "Iteration 5975, loss = 4.03423437\n",
      "Iteration 5976, loss = 4.22594403\n",
      "Iteration 5977, loss = 4.14077016\n",
      "Iteration 5978, loss = 3.68713927\n",
      "Iteration 5979, loss = 6.24646934\n",
      "Iteration 5980, loss = 6.82714791\n",
      "Iteration 5981, loss = 6.67195708\n",
      "Iteration 5982, loss = 4.93961860\n",
      "Iteration 5983, loss = 6.41264491\n",
      "Iteration 5984, loss = 5.27328416\n",
      "Iteration 5985, loss = 5.43376314\n",
      "Iteration 5986, loss = 4.50631719\n",
      "Iteration 5987, loss = 4.89458939\n",
      "Iteration 5988, loss = 6.32967428\n",
      "Iteration 5989, loss = 4.07824623\n",
      "Iteration 5990, loss = 4.04631643\n",
      "Iteration 5991, loss = 4.00591605\n",
      "Iteration 5992, loss = 4.44100721\n",
      "Iteration 5993, loss = 3.85135204\n",
      "Iteration 5994, loss = 3.62650624\n",
      "Iteration 5995, loss = 3.98288012\n",
      "Iteration 5996, loss = 3.95540733\n",
      "Iteration 5997, loss = 3.64491524\n",
      "Iteration 5998, loss = 3.78641551\n",
      "Iteration 5999, loss = 3.81074272\n",
      "Iteration 6000, loss = 3.77076932\n",
      "Iteration 6001, loss = 3.89900783\n",
      "Iteration 6002, loss = 3.35242029\n",
      "Iteration 6003, loss = 3.84226444\n",
      "Iteration 6004, loss = 4.03885853\n",
      "Iteration 6005, loss = 4.51506370\n",
      "Iteration 6006, loss = 4.35488087\n",
      "Iteration 6007, loss = 5.65705295\n",
      "Iteration 6008, loss = 5.86225057\n",
      "Iteration 6009, loss = 4.67558322\n",
      "Iteration 6010, loss = 4.47173827\n",
      "Iteration 6011, loss = 4.09664414\n",
      "Iteration 6012, loss = 5.04777038\n",
      "Iteration 6013, loss = 3.97027062\n",
      "Iteration 6014, loss = 3.91775655\n",
      "Iteration 6015, loss = 3.37303515\n",
      "Iteration 6016, loss = 3.73560482\n",
      "Iteration 6017, loss = 3.72838276\n",
      "Iteration 6018, loss = 3.59176885\n",
      "Iteration 6019, loss = 3.51066308\n",
      "Iteration 6020, loss = 7.43690618\n",
      "Iteration 6021, loss = 9.26831802\n",
      "Iteration 6022, loss = 7.71648842\n",
      "Iteration 6023, loss = 5.17313395\n",
      "Iteration 6024, loss = 4.74895012\n",
      "Iteration 6025, loss = 4.48110086\n",
      "Iteration 6026, loss = 5.07977609\n",
      "Iteration 6027, loss = 5.56058569\n",
      "Iteration 6028, loss = 4.67725891\n",
      "Iteration 6029, loss = 5.26089095\n",
      "Iteration 6030, loss = 4.10509878\n",
      "Iteration 6031, loss = 3.87645979\n",
      "Iteration 6032, loss = 4.68287294\n",
      "Iteration 6033, loss = 5.32570376\n",
      "Iteration 6034, loss = 5.52655580\n",
      "Iteration 6035, loss = 4.72777618\n",
      "Iteration 6036, loss = 4.43288600\n",
      "Iteration 6037, loss = 3.47819232\n",
      "Iteration 6038, loss = 3.25540007\n",
      "Iteration 6039, loss = 3.41603330\n",
      "Iteration 6040, loss = 3.37903383\n",
      "Iteration 6041, loss = 3.49352943\n",
      "Iteration 6042, loss = 3.79682636\n",
      "Iteration 6043, loss = 3.44316490\n",
      "Iteration 6044, loss = 4.16096878\n",
      "Iteration 6045, loss = 3.84653007\n",
      "Iteration 6046, loss = 4.00893749\n",
      "Iteration 6047, loss = 3.56620805\n",
      "Iteration 6048, loss = 4.27639108\n",
      "Iteration 6049, loss = 5.43851200\n",
      "Iteration 6050, loss = 5.58381955\n",
      "Iteration 6051, loss = 3.75526852\n",
      "Iteration 6052, loss = 4.37391791\n",
      "Iteration 6053, loss = 3.83739123\n",
      "Iteration 6054, loss = 4.01758320\n",
      "Iteration 6055, loss = 3.79404004\n",
      "Iteration 6056, loss = 4.03339085\n",
      "Iteration 6057, loss = 4.32374150\n",
      "Iteration 6058, loss = 5.91854478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6059, loss = 5.47508665\n",
      "Iteration 6060, loss = 5.01433300\n",
      "Iteration 6061, loss = 4.09712447\n",
      "Iteration 6062, loss = 4.29821941\n",
      "Iteration 6063, loss = 4.25377764\n",
      "Iteration 6064, loss = 3.41552884\n",
      "Iteration 6065, loss = 3.19089801\n",
      "Iteration 6066, loss = 3.34471322\n",
      "Iteration 6067, loss = 3.56568381\n",
      "Iteration 6068, loss = 3.53409455\n",
      "Iteration 6069, loss = 3.42808643\n",
      "Iteration 6070, loss = 3.47312617\n",
      "Iteration 6071, loss = 5.93691272\n",
      "Iteration 6072, loss = 4.76359871\n",
      "Iteration 6073, loss = 3.68733897\n",
      "Iteration 6074, loss = 3.81551373\n",
      "Iteration 6075, loss = 3.42150177\n",
      "Iteration 6076, loss = 3.80259008\n",
      "Iteration 6077, loss = 4.02394932\n",
      "Iteration 6078, loss = 3.61815409\n",
      "Iteration 6079, loss = 3.19835334\n",
      "Iteration 6080, loss = 5.16832568\n",
      "Iteration 6081, loss = 4.81870001\n",
      "Iteration 6082, loss = 4.77120605\n",
      "Iteration 6083, loss = 4.33766630\n",
      "Iteration 6084, loss = 4.50413096\n",
      "Iteration 6085, loss = 3.23810901\n",
      "Iteration 6086, loss = 3.77202612\n",
      "Iteration 6087, loss = 3.70964683\n",
      "Iteration 6088, loss = 3.62565418\n",
      "Iteration 6089, loss = 3.91954988\n",
      "Iteration 6090, loss = 3.86298561\n",
      "Iteration 6091, loss = 3.97281627\n",
      "Iteration 6092, loss = 3.52163333\n",
      "Iteration 6093, loss = 3.32362801\n",
      "Iteration 6094, loss = 4.96324071\n",
      "Iteration 6095, loss = 4.81272874\n",
      "Iteration 6096, loss = 3.79293889\n",
      "Iteration 6097, loss = 3.90882666\n",
      "Iteration 6098, loss = 4.26259340\n",
      "Iteration 6099, loss = 4.81542182\n",
      "Iteration 6100, loss = 3.77175976\n",
      "Iteration 6101, loss = 5.29730636\n",
      "Iteration 6102, loss = 5.32061312\n",
      "Iteration 6103, loss = 4.81250526\n",
      "Iteration 6104, loss = 4.40639189\n",
      "Iteration 6105, loss = 3.58586948\n",
      "Iteration 6106, loss = 3.20714685\n",
      "Iteration 6107, loss = 3.38155324\n",
      "Iteration 6108, loss = 3.31467461\n",
      "Iteration 6109, loss = 3.63005040\n",
      "Iteration 6110, loss = 3.61780281\n",
      "Iteration 6111, loss = 4.25015992\n",
      "Iteration 6112, loss = 3.98475470\n",
      "Iteration 6113, loss = 3.38099903\n",
      "Iteration 6114, loss = 3.93124171\n",
      "Iteration 6115, loss = 3.59608981\n",
      "Iteration 6116, loss = 4.13114486\n",
      "Iteration 6117, loss = 3.97280003\n",
      "Iteration 6118, loss = 4.91206143\n",
      "Iteration 6119, loss = 4.64282700\n",
      "Iteration 6120, loss = 4.35068755\n",
      "Iteration 6121, loss = 4.09830821\n",
      "Iteration 6122, loss = 4.29069853\n",
      "Iteration 6123, loss = 4.85852190\n",
      "Iteration 6124, loss = 5.11432278\n",
      "Iteration 6125, loss = 4.92967398\n",
      "Iteration 6126, loss = 6.14330831\n",
      "Iteration 6127, loss = 7.46638647\n",
      "Iteration 6128, loss = 7.46296762\n",
      "Iteration 6129, loss = 4.98084003\n",
      "Iteration 6130, loss = 6.73544173\n",
      "Iteration 6131, loss = 4.12587329\n",
      "Iteration 6132, loss = 4.04130639\n",
      "Iteration 6133, loss = 3.71194531\n",
      "Iteration 6134, loss = 3.38063825\n",
      "Iteration 6135, loss = 3.25289093\n",
      "Iteration 6136, loss = 3.28100543\n",
      "Iteration 6137, loss = 3.29646435\n",
      "Iteration 6138, loss = 3.33619200\n",
      "Iteration 6139, loss = 3.23912275\n",
      "Iteration 6140, loss = 3.52305254\n",
      "Iteration 6141, loss = 3.51425691\n",
      "Iteration 6142, loss = 3.29330083\n",
      "Iteration 6143, loss = 3.68023373\n",
      "Iteration 6144, loss = 3.87268776\n",
      "Iteration 6145, loss = 3.79204202\n",
      "Iteration 6146, loss = 4.44862020\n",
      "Iteration 6147, loss = 4.65483493\n",
      "Iteration 6148, loss = 5.03606513\n",
      "Iteration 6149, loss = 5.36070395\n",
      "Iteration 6150, loss = 4.66121782\n",
      "Iteration 6151, loss = 5.47218343\n",
      "Iteration 6152, loss = 5.21256093\n",
      "Iteration 6153, loss = 4.46323937\n",
      "Iteration 6154, loss = 4.00990697\n",
      "Iteration 6155, loss = 4.20320975\n",
      "Iteration 6156, loss = 4.25633499\n",
      "Iteration 6157, loss = 4.25159049\n",
      "Iteration 6158, loss = 4.15623285\n",
      "Iteration 6159, loss = 4.33532902\n",
      "Iteration 6160, loss = 3.86275194\n",
      "Iteration 6161, loss = 4.61188872\n",
      "Iteration 6162, loss = 3.39753875\n",
      "Iteration 6163, loss = 3.42989499\n",
      "Iteration 6164, loss = 3.62329155\n",
      "Iteration 6165, loss = 3.30219737\n",
      "Iteration 6166, loss = 3.90093045\n",
      "Iteration 6167, loss = 3.97114672\n",
      "Iteration 6168, loss = 6.08813997\n",
      "Iteration 6169, loss = 5.07600331\n",
      "Iteration 6170, loss = 3.55151222\n",
      "Iteration 6171, loss = 4.59336735\n",
      "Iteration 6172, loss = 3.99806385\n",
      "Iteration 6173, loss = 3.45156700\n",
      "Iteration 6174, loss = 3.67764200\n",
      "Iteration 6175, loss = 3.84064224\n",
      "Iteration 6176, loss = 3.76974932\n",
      "Iteration 6177, loss = 3.80394364\n",
      "Iteration 6178, loss = 3.39130340\n",
      "Iteration 6179, loss = 4.20833239\n",
      "Iteration 6180, loss = 3.81656284\n",
      "Iteration 6181, loss = 3.88940307\n",
      "Iteration 6182, loss = 4.20935559\n",
      "Iteration 6183, loss = 3.15352674\n",
      "Iteration 6184, loss = 3.26595948\n",
      "Iteration 6185, loss = 3.21431548\n",
      "Iteration 6186, loss = 3.54677506\n",
      "Iteration 6187, loss = 5.47523144\n",
      "Iteration 6188, loss = 6.04276267\n",
      "Iteration 6189, loss = 5.30918063\n",
      "Iteration 6190, loss = 4.70803076\n",
      "Iteration 6191, loss = 4.09806209\n",
      "Iteration 6192, loss = 3.68422389\n",
      "Iteration 6193, loss = 3.72218225\n",
      "Iteration 6194, loss = 3.41243157\n",
      "Iteration 6195, loss = 3.85376569\n",
      "Iteration 6196, loss = 3.85877944\n",
      "Iteration 6197, loss = 3.64935122\n",
      "Iteration 6198, loss = 3.48179366\n",
      "Iteration 6199, loss = 3.80602757\n",
      "Iteration 6200, loss = 3.65279320\n",
      "Iteration 6201, loss = 3.48739830\n",
      "Iteration 6202, loss = 3.36719996\n",
      "Iteration 6203, loss = 3.44680334\n",
      "Iteration 6204, loss = 3.44593958\n",
      "Iteration 6205, loss = 3.14225700\n",
      "Iteration 6206, loss = 3.13319686\n",
      "Iteration 6207, loss = 3.49743478\n",
      "Iteration 6208, loss = 3.24975226\n",
      "Iteration 6209, loss = 3.50962441\n",
      "Iteration 6210, loss = 3.73140686\n",
      "Iteration 6211, loss = 4.31822838\n",
      "Iteration 6212, loss = 4.05816526\n",
      "Iteration 6213, loss = 4.47979789\n",
      "Iteration 6214, loss = 3.38607251\n",
      "Iteration 6215, loss = 3.51411473\n",
      "Iteration 6216, loss = 4.92508178\n",
      "Iteration 6217, loss = 3.64078621\n",
      "Iteration 6218, loss = 4.04631208\n",
      "Iteration 6219, loss = 5.73845481\n",
      "Iteration 6220, loss = 5.92333051\n",
      "Iteration 6221, loss = 7.26050507\n",
      "Iteration 6222, loss = 6.86202370\n",
      "Iteration 6223, loss = 6.18767449\n",
      "Iteration 6224, loss = 4.56331741\n",
      "Iteration 6225, loss = 4.23300111\n",
      "Iteration 6226, loss = 4.32290617\n",
      "Iteration 6227, loss = 3.77298507\n",
      "Iteration 6228, loss = 3.94781211\n",
      "Iteration 6229, loss = 4.21794918\n",
      "Iteration 6230, loss = 4.81777413\n",
      "Iteration 6231, loss = 3.67590394\n",
      "Iteration 6232, loss = 3.63289983\n",
      "Iteration 6233, loss = 3.41614480\n",
      "Iteration 6234, loss = 3.95924003\n",
      "Iteration 6235, loss = 3.67959990\n",
      "Iteration 6236, loss = 3.56853969\n",
      "Iteration 6237, loss = 3.46952622\n",
      "Iteration 6238, loss = 3.48293578\n",
      "Iteration 6239, loss = 3.21755507\n",
      "Iteration 6240, loss = 3.49983017\n",
      "Iteration 6241, loss = 5.59260569\n",
      "Iteration 6242, loss = 4.21183591\n",
      "Iteration 6243, loss = 4.63845595\n",
      "Iteration 6244, loss = 5.43343533\n",
      "Iteration 6245, loss = 5.01465432\n",
      "Iteration 6246, loss = 3.93420693\n",
      "Iteration 6247, loss = 3.45022855\n",
      "Iteration 6248, loss = 3.45872634\n",
      "Iteration 6249, loss = 3.46701832\n",
      "Iteration 6250, loss = 3.67588624\n",
      "Iteration 6251, loss = 5.37336687\n",
      "Iteration 6252, loss = 7.09305864\n",
      "Iteration 6253, loss = 6.15325825\n",
      "Iteration 6254, loss = 5.66322297\n",
      "Iteration 6255, loss = 5.29320647\n",
      "Iteration 6256, loss = 4.71087018\n",
      "Iteration 6257, loss = 6.39463545\n",
      "Iteration 6258, loss = 4.63941148\n",
      "Iteration 6259, loss = 4.20105287\n",
      "Iteration 6260, loss = 3.62802483\n",
      "Iteration 6261, loss = 3.16960131\n",
      "Iteration 6262, loss = 3.89841491\n",
      "Iteration 6263, loss = 4.23483609\n",
      "Iteration 6264, loss = 5.39218418\n",
      "Iteration 6265, loss = 4.59219020\n",
      "Iteration 6266, loss = 4.87346348\n",
      "Iteration 6267, loss = 3.88902436\n",
      "Iteration 6268, loss = 5.17367291\n",
      "Iteration 6269, loss = 4.02034408\n",
      "Iteration 6270, loss = 4.89237361\n",
      "Iteration 6271, loss = 3.94133192\n",
      "Iteration 6272, loss = 3.50202669\n",
      "Iteration 6273, loss = 3.69132651\n",
      "Iteration 6274, loss = 3.56206073\n",
      "Iteration 6275, loss = 4.10382301\n",
      "Iteration 6276, loss = 3.77745119\n",
      "Iteration 6277, loss = 4.11693355\n",
      "Iteration 6278, loss = 4.17918963\n",
      "Iteration 6279, loss = 3.64980387\n",
      "Iteration 6280, loss = 3.60573943\n",
      "Iteration 6281, loss = 4.13109248\n",
      "Iteration 6282, loss = 3.90350477\n",
      "Iteration 6283, loss = 3.90931764\n",
      "Iteration 6284, loss = 3.61934897\n",
      "Iteration 6285, loss = 3.34253249\n",
      "Iteration 6286, loss = 3.25274208\n",
      "Iteration 6287, loss = 3.98463233\n",
      "Iteration 6288, loss = 3.51196283\n",
      "Iteration 6289, loss = 3.64966269\n",
      "Iteration 6290, loss = 4.52839990\n",
      "Iteration 6291, loss = 4.23134248\n",
      "Iteration 6292, loss = 3.91513612\n",
      "Iteration 6293, loss = 3.70703663\n",
      "Iteration 6294, loss = 4.76883963\n",
      "Iteration 6295, loss = 4.04656520\n",
      "Iteration 6296, loss = 4.33066853\n",
      "Iteration 6297, loss = 3.70147823\n",
      "Iteration 6298, loss = 3.39721361\n",
      "Iteration 6299, loss = 3.63275453\n",
      "Iteration 6300, loss = 3.48023024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6301, loss = 4.54422390\n",
      "Iteration 6302, loss = 3.30364877\n",
      "Iteration 6303, loss = 4.12342259\n",
      "Iteration 6304, loss = 3.89442517\n",
      "Iteration 6305, loss = 5.62776381\n",
      "Iteration 6306, loss = 6.46298806\n",
      "Iteration 6307, loss = 4.29362090\n",
      "Iteration 6308, loss = 3.86289270\n",
      "Iteration 6309, loss = 3.65261730\n",
      "Iteration 6310, loss = 4.04831844\n",
      "Iteration 6311, loss = 4.02745277\n",
      "Iteration 6312, loss = 7.24654315\n",
      "Iteration 6313, loss = 5.48321284\n",
      "Iteration 6314, loss = 4.17394526\n",
      "Iteration 6315, loss = 3.43047385\n",
      "Iteration 6316, loss = 3.59668485\n",
      "Iteration 6317, loss = 4.42818246\n",
      "Iteration 6318, loss = 4.60820247\n",
      "Iteration 6319, loss = 4.14215098\n",
      "Iteration 6320, loss = 4.71293202\n",
      "Iteration 6321, loss = 4.49803142\n",
      "Iteration 6322, loss = 3.36619376\n",
      "Iteration 6323, loss = 3.42697770\n",
      "Iteration 6324, loss = 3.74547379\n",
      "Iteration 6325, loss = 4.02024420\n",
      "Iteration 6326, loss = 3.13675699\n",
      "Iteration 6327, loss = 3.50725284\n",
      "Iteration 6328, loss = 3.45617084\n",
      "Iteration 6329, loss = 3.50604955\n",
      "Iteration 6330, loss = 4.40119367\n",
      "Iteration 6331, loss = 3.83976534\n",
      "Iteration 6332, loss = 4.15374403\n",
      "Iteration 6333, loss = 4.14382022\n",
      "Iteration 6334, loss = 3.61472982\n",
      "Iteration 6335, loss = 3.31234685\n",
      "Iteration 6336, loss = 4.73599569\n",
      "Iteration 6337, loss = 4.18451908\n",
      "Iteration 6338, loss = 3.89720545\n",
      "Iteration 6339, loss = 4.57605984\n",
      "Iteration 6340, loss = 5.29927183\n",
      "Iteration 6341, loss = 4.70665975\n",
      "Iteration 6342, loss = 4.09851922\n",
      "Iteration 6343, loss = 3.54514004\n",
      "Iteration 6344, loss = 3.26008286\n",
      "Iteration 6345, loss = 3.86618818\n",
      "Iteration 6346, loss = 4.40970377\n",
      "Iteration 6347, loss = 4.89601300\n",
      "Iteration 6348, loss = 6.76135628\n",
      "Iteration 6349, loss = 7.13756612\n",
      "Iteration 6350, loss = 4.98127849\n",
      "Iteration 6351, loss = 3.65579129\n",
      "Iteration 6352, loss = 3.35950234\n",
      "Iteration 6353, loss = 3.26611962\n",
      "Iteration 6354, loss = 3.32389996\n",
      "Iteration 6355, loss = 3.82142912\n",
      "Iteration 6356, loss = 3.38299614\n",
      "Iteration 6357, loss = 3.32624174\n",
      "Iteration 6358, loss = 5.49247157\n",
      "Iteration 6359, loss = 3.67764471\n",
      "Iteration 6360, loss = 3.79187428\n",
      "Iteration 6361, loss = 3.54315921\n",
      "Iteration 6362, loss = 3.93013161\n",
      "Iteration 6363, loss = 3.49689270\n",
      "Iteration 6364, loss = 4.87910277\n",
      "Iteration 6365, loss = 4.53801976\n",
      "Iteration 6366, loss = 4.76733205\n",
      "Iteration 6367, loss = 3.96581638\n",
      "Iteration 6368, loss = 3.55638625\n",
      "Iteration 6369, loss = 3.78723090\n",
      "Iteration 6370, loss = 5.23826360\n",
      "Iteration 6371, loss = 3.81440986\n",
      "Iteration 6372, loss = 3.84979010\n",
      "Iteration 6373, loss = 3.81842636\n",
      "Iteration 6374, loss = 3.89980652\n",
      "Iteration 6375, loss = 3.63063637\n",
      "Iteration 6376, loss = 3.39653400\n",
      "Iteration 6377, loss = 3.20133995\n",
      "Iteration 6378, loss = 4.35032700\n",
      "Iteration 6379, loss = 3.88383924\n",
      "Iteration 6380, loss = 3.58190711\n",
      "Iteration 6381, loss = 3.55753411\n",
      "Iteration 6382, loss = 3.39953161\n",
      "Iteration 6383, loss = 3.53985680\n",
      "Iteration 6384, loss = 3.95288089\n",
      "Iteration 6385, loss = 3.71845355\n",
      "Iteration 6386, loss = 3.32234171\n",
      "Iteration 6387, loss = 3.40026359\n",
      "Iteration 6388, loss = 3.42455108\n",
      "Iteration 6389, loss = 3.42658359\n",
      "Iteration 6390, loss = 3.19828123\n",
      "Iteration 6391, loss = 3.55192987\n",
      "Iteration 6392, loss = 7.01956037\n",
      "Iteration 6393, loss = 5.06291775\n",
      "Iteration 6394, loss = 4.43560766\n",
      "Iteration 6395, loss = 4.62444585\n",
      "Iteration 6396, loss = 3.70378572\n",
      "Iteration 6397, loss = 3.91007163\n",
      "Iteration 6398, loss = 3.18653258\n",
      "Iteration 6399, loss = 3.53477630\n",
      "Iteration 6400, loss = 3.19452189\n",
      "Iteration 6401, loss = 4.79990393\n",
      "Iteration 6402, loss = 4.47759640\n",
      "Iteration 6403, loss = 4.14007563\n",
      "Iteration 6404, loss = 4.08066321\n",
      "Iteration 6405, loss = 6.78506130\n",
      "Iteration 6406, loss = 5.35565961\n",
      "Iteration 6407, loss = 5.04984200\n",
      "Iteration 6408, loss = 4.93553639\n",
      "Iteration 6409, loss = 5.85272315\n",
      "Iteration 6410, loss = 4.25200381\n",
      "Iteration 6411, loss = 3.59939390\n",
      "Iteration 6412, loss = 4.37271645\n",
      "Iteration 6413, loss = 4.34791194\n",
      "Iteration 6414, loss = 4.14841202\n",
      "Iteration 6415, loss = 4.51780177\n",
      "Iteration 6416, loss = 4.69234191\n",
      "Iteration 6417, loss = 4.92658120\n",
      "Iteration 6418, loss = 4.57810846\n",
      "Iteration 6419, loss = 4.65255435\n",
      "Iteration 6420, loss = 4.48842208\n",
      "Iteration 6421, loss = 3.66259473\n",
      "Iteration 6422, loss = 4.63852822\n",
      "Iteration 6423, loss = 4.17967703\n",
      "Iteration 6424, loss = 5.43237963\n",
      "Iteration 6425, loss = 4.35775767\n",
      "Iteration 6426, loss = 4.79203074\n",
      "Iteration 6427, loss = 4.13827710\n",
      "Iteration 6428, loss = 4.67993381\n",
      "Iteration 6429, loss = 4.24285996\n",
      "Iteration 6430, loss = 4.83676464\n",
      "Iteration 6431, loss = 4.16698399\n",
      "Iteration 6432, loss = 3.94618536\n",
      "Iteration 6433, loss = 3.58210992\n",
      "Iteration 6434, loss = 4.60774684\n",
      "Iteration 6435, loss = 4.13466903\n",
      "Iteration 6436, loss = 4.49094741\n",
      "Iteration 6437, loss = 5.91156734\n",
      "Iteration 6438, loss = 4.86104496\n",
      "Iteration 6439, loss = 3.56258632\n",
      "Iteration 6440, loss = 3.44747883\n",
      "Iteration 6441, loss = 3.24811634\n",
      "Iteration 6442, loss = 3.09018645\n",
      "Iteration 6443, loss = 3.28404479\n",
      "Iteration 6444, loss = 3.24716858\n",
      "Iteration 6445, loss = 3.63577062\n",
      "Iteration 6446, loss = 3.45651310\n",
      "Iteration 6447, loss = 6.11969997\n",
      "Iteration 6448, loss = 7.46876520\n",
      "Iteration 6449, loss = 8.44697574\n",
      "Iteration 6450, loss = 6.16020028\n",
      "Iteration 6451, loss = 5.01904098\n",
      "Iteration 6452, loss = 4.55619354\n",
      "Iteration 6453, loss = 3.92232877\n",
      "Iteration 6454, loss = 4.02373764\n",
      "Iteration 6455, loss = 3.62938876\n",
      "Iteration 6456, loss = 3.54284928\n",
      "Iteration 6457, loss = 3.41295718\n",
      "Iteration 6458, loss = 3.63441327\n",
      "Iteration 6459, loss = 3.79376540\n",
      "Iteration 6460, loss = 3.81625220\n",
      "Iteration 6461, loss = 4.28324252\n",
      "Iteration 6462, loss = 4.20183898\n",
      "Iteration 6463, loss = 5.07396346\n",
      "Iteration 6464, loss = 4.61071384\n",
      "Iteration 6465, loss = 4.57081812\n",
      "Iteration 6466, loss = 4.13756941\n",
      "Iteration 6467, loss = 3.75102745\n",
      "Iteration 6468, loss = 4.49248036\n",
      "Iteration 6469, loss = 4.71862181\n",
      "Iteration 6470, loss = 4.62853883\n",
      "Iteration 6471, loss = 3.89074365\n",
      "Iteration 6472, loss = 3.98476073\n",
      "Iteration 6473, loss = 3.46934524\n",
      "Iteration 6474, loss = 3.58590847\n",
      "Iteration 6475, loss = 4.14037639\n",
      "Iteration 6476, loss = 4.16792888\n",
      "Iteration 6477, loss = 3.35896319\n",
      "Iteration 6478, loss = 3.42080821\n",
      "Iteration 6479, loss = 3.68365218\n",
      "Iteration 6480, loss = 3.68468970\n",
      "Iteration 6481, loss = 3.68469197\n",
      "Iteration 6482, loss = 4.11586452\n",
      "Iteration 6483, loss = 3.45149779\n",
      "Iteration 6484, loss = 3.22325755\n",
      "Iteration 6485, loss = 3.06499622\n",
      "Iteration 6486, loss = 3.13001650\n",
      "Iteration 6487, loss = 3.23614474\n",
      "Iteration 6488, loss = 3.36346094\n",
      "Iteration 6489, loss = 3.66218308\n",
      "Iteration 6490, loss = 4.11406287\n",
      "Iteration 6491, loss = 4.26950554\n",
      "Iteration 6492, loss = 6.09704482\n",
      "Iteration 6493, loss = 5.49283685\n",
      "Iteration 6494, loss = 3.93646672\n",
      "Iteration 6495, loss = 3.96891151\n",
      "Iteration 6496, loss = 3.69607354\n",
      "Iteration 6497, loss = 3.96697862\n",
      "Iteration 6498, loss = 3.79057108\n",
      "Iteration 6499, loss = 3.46937028\n",
      "Iteration 6500, loss = 3.29287323\n",
      "Iteration 6501, loss = 3.03608598\n",
      "Iteration 6502, loss = 3.45405869\n",
      "Iteration 6503, loss = 3.52062766\n",
      "Iteration 6504, loss = 3.37048363\n",
      "Iteration 6505, loss = 3.40239571\n",
      "Iteration 6506, loss = 3.54003017\n",
      "Iteration 6507, loss = 3.10563032\n",
      "Iteration 6508, loss = 3.47522549\n",
      "Iteration 6509, loss = 3.59640275\n",
      "Iteration 6510, loss = 3.46806033\n",
      "Iteration 6511, loss = 3.72388594\n",
      "Iteration 6512, loss = 4.11568267\n",
      "Iteration 6513, loss = 3.71260922\n",
      "Iteration 6514, loss = 3.52290410\n",
      "Iteration 6515, loss = 3.12153639\n",
      "Iteration 6516, loss = 3.12534881\n",
      "Iteration 6517, loss = 3.24003552\n",
      "Iteration 6518, loss = 3.15660138\n",
      "Iteration 6519, loss = 3.15464958\n",
      "Iteration 6520, loss = 3.50386149\n",
      "Iteration 6521, loss = 3.43161397\n",
      "Iteration 6522, loss = 6.19748125\n",
      "Iteration 6523, loss = 5.04518247\n",
      "Iteration 6524, loss = 3.59263986\n",
      "Iteration 6525, loss = 3.62737662\n",
      "Iteration 6526, loss = 3.82773048\n",
      "Iteration 6527, loss = 3.61736390\n",
      "Iteration 6528, loss = 4.00053028\n",
      "Iteration 6529, loss = 4.78920574\n",
      "Iteration 6530, loss = 4.18504186\n",
      "Iteration 6531, loss = 4.15413726\n",
      "Iteration 6532, loss = 4.14734019\n",
      "Iteration 6533, loss = 4.01229540\n",
      "Iteration 6534, loss = 4.89101113\n",
      "Iteration 6535, loss = 3.84656927\n",
      "Iteration 6536, loss = 3.82596564\n",
      "Iteration 6537, loss = 3.39260757\n",
      "Iteration 6538, loss = 3.88357455\n",
      "Iteration 6539, loss = 3.79221558\n",
      "Iteration 6540, loss = 5.07734764\n",
      "Iteration 6541, loss = 4.00495230\n",
      "Iteration 6542, loss = 4.12688212\n",
      "Iteration 6543, loss = 4.02492192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6544, loss = 4.26632215\n",
      "Iteration 6545, loss = 6.98067164\n",
      "Iteration 6546, loss = 6.30647486\n",
      "Iteration 6547, loss = 4.74296396\n",
      "Iteration 6548, loss = 4.77121562\n",
      "Iteration 6549, loss = 5.44020317\n",
      "Iteration 6550, loss = 11.23368311\n",
      "Iteration 6551, loss = 7.14043950\n",
      "Iteration 6552, loss = 6.34937369\n",
      "Iteration 6553, loss = 5.32757852\n",
      "Iteration 6554, loss = 4.61129971\n",
      "Iteration 6555, loss = 4.89068159\n",
      "Iteration 6556, loss = 4.12397537\n",
      "Iteration 6557, loss = 4.50846202\n",
      "Iteration 6558, loss = 4.40257984\n",
      "Iteration 6559, loss = 3.92526486\n",
      "Iteration 6560, loss = 4.78234301\n",
      "Iteration 6561, loss = 4.27809350\n",
      "Iteration 6562, loss = 4.13849718\n",
      "Iteration 6563, loss = 7.75465935\n",
      "Iteration 6564, loss = 5.90871385\n",
      "Iteration 6565, loss = 6.55686052\n",
      "Iteration 6566, loss = 6.81742730\n",
      "Iteration 6567, loss = 5.88465150\n",
      "Iteration 6568, loss = 4.33858037\n",
      "Iteration 6569, loss = 4.42794320\n",
      "Iteration 6570, loss = 5.39875877\n",
      "Iteration 6571, loss = 4.68498591\n",
      "Iteration 6572, loss = 5.81148701\n",
      "Iteration 6573, loss = 4.16795205\n",
      "Iteration 6574, loss = 4.12259800\n",
      "Iteration 6575, loss = 3.49408697\n",
      "Iteration 6576, loss = 3.29836686\n",
      "Iteration 6577, loss = 3.19072907\n",
      "Iteration 6578, loss = 3.75167187\n",
      "Iteration 6579, loss = 4.49025232\n",
      "Iteration 6580, loss = 5.09548239\n",
      "Iteration 6581, loss = 3.36953927\n",
      "Iteration 6582, loss = 7.30031592\n",
      "Iteration 6583, loss = 7.11042229\n",
      "Iteration 6584, loss = 4.18383404\n",
      "Iteration 6585, loss = 4.76923296\n",
      "Iteration 6586, loss = 3.49619318\n",
      "Iteration 6587, loss = 3.98014494\n",
      "Iteration 6588, loss = 3.87491636\n",
      "Iteration 6589, loss = 3.60508151\n",
      "Iteration 6590, loss = 4.13839515\n",
      "Iteration 6591, loss = 3.46857666\n",
      "Iteration 6592, loss = 5.55993052\n",
      "Iteration 6593, loss = 5.91499248\n",
      "Iteration 6594, loss = 7.38958436\n",
      "Iteration 6595, loss = 4.33820847\n",
      "Iteration 6596, loss = 4.37048905\n",
      "Iteration 6597, loss = 3.86537986\n",
      "Iteration 6598, loss = 3.77030857\n",
      "Iteration 6599, loss = 3.61172599\n",
      "Iteration 6600, loss = 3.18577118\n",
      "Iteration 6601, loss = 3.30609018\n",
      "Iteration 6602, loss = 3.48927778\n",
      "Iteration 6603, loss = 3.71005629\n",
      "Iteration 6604, loss = 4.07390191\n",
      "Iteration 6605, loss = 3.85267761\n",
      "Iteration 6606, loss = 3.74258089\n",
      "Iteration 6607, loss = 3.12296138\n",
      "Iteration 6608, loss = 3.22661687\n",
      "Iteration 6609, loss = 3.21182554\n",
      "Iteration 6610, loss = 3.26316083\n",
      "Iteration 6611, loss = 3.61247009\n",
      "Iteration 6612, loss = 4.01947855\n",
      "Iteration 6613, loss = 3.67453786\n",
      "Iteration 6614, loss = 4.19173795\n",
      "Iteration 6615, loss = 4.50281633\n",
      "Iteration 6616, loss = 3.33307790\n",
      "Iteration 6617, loss = 4.07304298\n",
      "Iteration 6618, loss = 4.56184390\n",
      "Iteration 6619, loss = 4.25110517\n",
      "Iteration 6620, loss = 4.59549715\n",
      "Iteration 6621, loss = 4.33615479\n",
      "Iteration 6622, loss = 5.02650606\n",
      "Iteration 6623, loss = 4.13963610\n",
      "Iteration 6624, loss = 3.86662787\n",
      "Iteration 6625, loss = 3.48379372\n",
      "Iteration 6626, loss = 3.22915000\n",
      "Iteration 6627, loss = 3.62877363\n",
      "Iteration 6628, loss = 3.53256420\n",
      "Iteration 6629, loss = 3.38134395\n",
      "Iteration 6630, loss = 3.33016626\n",
      "Iteration 6631, loss = 3.74677957\n",
      "Iteration 6632, loss = 3.49100078\n",
      "Iteration 6633, loss = 3.25704605\n",
      "Iteration 6634, loss = 3.11488366\n",
      "Iteration 6635, loss = 3.44431132\n",
      "Iteration 6636, loss = 4.40781445\n",
      "Iteration 6637, loss = 4.10879110\n",
      "Iteration 6638, loss = 3.47146726\n",
      "Iteration 6639, loss = 3.53882276\n",
      "Iteration 6640, loss = 3.64577220\n",
      "Iteration 6641, loss = 4.52734797\n",
      "Iteration 6642, loss = 3.94713554\n",
      "Iteration 6643, loss = 3.74178429\n",
      "Iteration 6644, loss = 3.55424368\n",
      "Iteration 6645, loss = 3.59649991\n",
      "Iteration 6646, loss = 3.38951930\n",
      "Iteration 6647, loss = 3.44796769\n",
      "Iteration 6648, loss = 3.72828054\n",
      "Iteration 6649, loss = 3.65840641\n",
      "Iteration 6650, loss = 3.94134903\n",
      "Iteration 6651, loss = 5.51582892\n",
      "Iteration 6652, loss = 4.24850337\n",
      "Iteration 6653, loss = 4.45232177\n",
      "Iteration 6654, loss = 3.85954081\n",
      "Iteration 6655, loss = 3.58575008\n",
      "Iteration 6656, loss = 4.02675507\n",
      "Iteration 6657, loss = 3.77596422\n",
      "Iteration 6658, loss = 4.19312372\n",
      "Iteration 6659, loss = 5.36239831\n",
      "Iteration 6660, loss = 4.17575690\n",
      "Iteration 6661, loss = 3.18920122\n",
      "Iteration 6662, loss = 4.22100017\n",
      "Iteration 6663, loss = 3.38831475\n",
      "Iteration 6664, loss = 3.48039643\n",
      "Iteration 6665, loss = 3.71167411\n",
      "Iteration 6666, loss = 3.45586201\n",
      "Iteration 6667, loss = 3.16408899\n",
      "Iteration 6668, loss = 3.29362549\n",
      "Iteration 6669, loss = 4.55902192\n",
      "Iteration 6670, loss = 4.07758041\n",
      "Iteration 6671, loss = 6.56719245\n",
      "Iteration 6672, loss = 5.02729960\n",
      "Iteration 6673, loss = 3.58932843\n",
      "Iteration 6674, loss = 3.73968172\n",
      "Iteration 6675, loss = 4.29078303\n",
      "Iteration 6676, loss = 3.34723181\n",
      "Iteration 6677, loss = 3.58094436\n",
      "Iteration 6678, loss = 4.45050515\n",
      "Iteration 6679, loss = 5.22405896\n",
      "Iteration 6680, loss = 5.61265218\n",
      "Iteration 6681, loss = 4.05566818\n",
      "Iteration 6682, loss = 4.15328011\n",
      "Iteration 6683, loss = 4.29840386\n",
      "Iteration 6684, loss = 3.69374173\n",
      "Iteration 6685, loss = 3.62504160\n",
      "Iteration 6686, loss = 3.58589844\n",
      "Iteration 6687, loss = 4.12948925\n",
      "Iteration 6688, loss = 5.16533699\n",
      "Iteration 6689, loss = 3.96016464\n",
      "Iteration 6690, loss = 3.81221616\n",
      "Iteration 6691, loss = 3.78300670\n",
      "Iteration 6692, loss = 4.12705053\n",
      "Iteration 6693, loss = 3.67436916\n",
      "Iteration 6694, loss = 3.14227497\n",
      "Iteration 6695, loss = 3.54501734\n",
      "Iteration 6696, loss = 3.35823952\n",
      "Iteration 6697, loss = 3.39412268\n",
      "Iteration 6698, loss = 3.18422471\n",
      "Iteration 6699, loss = 3.27863805\n",
      "Iteration 6700, loss = 3.10461302\n",
      "Iteration 6701, loss = 3.18432368\n",
      "Iteration 6702, loss = 3.16158190\n",
      "Iteration 6703, loss = 3.68446078\n",
      "Iteration 6704, loss = 3.40732857\n",
      "Iteration 6705, loss = 3.68532372\n",
      "Iteration 6706, loss = 3.25416302\n",
      "Iteration 6707, loss = 3.37923513\n",
      "Iteration 6708, loss = 3.47903013\n",
      "Iteration 6709, loss = 3.49827700\n",
      "Iteration 6710, loss = 3.67171898\n",
      "Iteration 6711, loss = 3.46768317\n",
      "Iteration 6712, loss = 3.29780646\n",
      "Iteration 6713, loss = 3.15918081\n",
      "Iteration 6714, loss = 3.48059665\n",
      "Iteration 6715, loss = 3.70670450\n",
      "Iteration 6716, loss = 3.37000800\n",
      "Iteration 6717, loss = 3.29096073\n",
      "Iteration 6718, loss = 3.53362364\n",
      "Iteration 6719, loss = 3.42615316\n",
      "Iteration 6720, loss = 3.44350619\n",
      "Iteration 6721, loss = 3.08315830\n",
      "Iteration 6722, loss = 3.71084846\n",
      "Iteration 6723, loss = 4.33956682\n",
      "Iteration 6724, loss = 4.61933795\n",
      "Iteration 6725, loss = 6.12650604\n",
      "Iteration 6726, loss = 6.12101747\n",
      "Iteration 6727, loss = 4.02544687\n",
      "Iteration 6728, loss = 3.35097407\n",
      "Iteration 6729, loss = 3.56622109\n",
      "Iteration 6730, loss = 5.10910783\n",
      "Iteration 6731, loss = 3.69447031\n",
      "Iteration 6732, loss = 3.61074854\n",
      "Iteration 6733, loss = 3.38999642\n",
      "Iteration 6734, loss = 3.76619313\n",
      "Iteration 6735, loss = 3.37648552\n",
      "Iteration 6736, loss = 3.78320359\n",
      "Iteration 6737, loss = 3.40951033\n",
      "Iteration 6738, loss = 4.12707283\n",
      "Iteration 6739, loss = 4.48153896\n",
      "Iteration 6740, loss = 6.42171436\n",
      "Iteration 6741, loss = 8.14981746\n",
      "Iteration 6742, loss = 5.50836469\n",
      "Iteration 6743, loss = 5.12563686\n",
      "Iteration 6744, loss = 5.36691934\n",
      "Iteration 6745, loss = 4.78528262\n",
      "Iteration 6746, loss = 5.19240496\n",
      "Iteration 6747, loss = 3.90120689\n",
      "Iteration 6748, loss = 4.39726692\n",
      "Iteration 6749, loss = 4.56509962\n",
      "Iteration 6750, loss = 4.64316006\n",
      "Iteration 6751, loss = 5.02141739\n",
      "Iteration 6752, loss = 4.15995007\n",
      "Iteration 6753, loss = 4.10959094\n",
      "Iteration 6754, loss = 5.76562261\n",
      "Iteration 6755, loss = 3.64569514\n",
      "Iteration 6756, loss = 4.24088855\n",
      "Iteration 6757, loss = 3.85671769\n",
      "Iteration 6758, loss = 5.38566227\n",
      "Iteration 6759, loss = 3.61079778\n",
      "Iteration 6760, loss = 4.72285137\n",
      "Iteration 6761, loss = 4.21385725\n",
      "Iteration 6762, loss = 4.24709449\n",
      "Iteration 6763, loss = 4.56728157\n",
      "Iteration 6764, loss = 5.92201936\n",
      "Iteration 6765, loss = 4.96980048\n",
      "Iteration 6766, loss = 4.59396740\n",
      "Iteration 6767, loss = 4.90006230\n",
      "Iteration 6768, loss = 4.00069484\n",
      "Iteration 6769, loss = 3.52103796\n",
      "Iteration 6770, loss = 4.26402463\n",
      "Iteration 6771, loss = 4.47256171\n",
      "Iteration 6772, loss = 3.37231139\n",
      "Iteration 6773, loss = 3.46999802\n",
      "Iteration 6774, loss = 3.19700935\n",
      "Iteration 6775, loss = 3.10900435\n",
      "Iteration 6776, loss = 3.18610323\n",
      "Iteration 6777, loss = 3.25805854\n",
      "Iteration 6778, loss = 3.06849962\n",
      "Iteration 6779, loss = 3.54684539\n",
      "Iteration 6780, loss = 3.83171635\n",
      "Iteration 6781, loss = 4.21656700\n",
      "Iteration 6782, loss = 4.47821670\n",
      "Iteration 6783, loss = 5.03726913\n",
      "Iteration 6784, loss = 4.42412260\n",
      "Iteration 6785, loss = 3.29389853\n",
      "Iteration 6786, loss = 3.41049596\n",
      "Iteration 6787, loss = 3.49092312\n",
      "Iteration 6788, loss = 5.21181135\n",
      "Iteration 6789, loss = 4.25808212\n",
      "Iteration 6790, loss = 4.60380755\n",
      "Iteration 6791, loss = 4.03274603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6792, loss = 4.01460965\n",
      "Iteration 6793, loss = 4.96744770\n",
      "Iteration 6794, loss = 4.82396234\n",
      "Iteration 6795, loss = 3.84331209\n",
      "Iteration 6796, loss = 3.40178025\n",
      "Iteration 6797, loss = 3.19274995\n",
      "Iteration 6798, loss = 3.33808460\n",
      "Iteration 6799, loss = 4.15892279\n",
      "Iteration 6800, loss = 4.36257039\n",
      "Iteration 6801, loss = 4.11492010\n",
      "Iteration 6802, loss = 4.65056403\n",
      "Iteration 6803, loss = 3.53607215\n",
      "Iteration 6804, loss = 4.72693630\n",
      "Iteration 6805, loss = 5.08248638\n",
      "Iteration 6806, loss = 4.34847667\n",
      "Iteration 6807, loss = 4.21791036\n",
      "Iteration 6808, loss = 3.98442076\n",
      "Iteration 6809, loss = 3.56117023\n",
      "Iteration 6810, loss = 3.94450518\n",
      "Iteration 6811, loss = 5.95287440\n",
      "Iteration 6812, loss = 3.91950802\n",
      "Iteration 6813, loss = 4.41399827\n",
      "Iteration 6814, loss = 4.96090326\n",
      "Iteration 6815, loss = 4.87954122\n",
      "Iteration 6816, loss = 3.89081907\n",
      "Iteration 6817, loss = 4.63984985\n",
      "Iteration 6818, loss = 3.73793083\n",
      "Iteration 6819, loss = 3.96129593\n",
      "Iteration 6820, loss = 3.59901707\n",
      "Iteration 6821, loss = 3.84438263\n",
      "Iteration 6822, loss = 3.33024709\n",
      "Iteration 6823, loss = 3.06298081\n",
      "Iteration 6824, loss = 3.40158523\n",
      "Iteration 6825, loss = 3.90227450\n",
      "Iteration 6826, loss = 3.67182102\n",
      "Iteration 6827, loss = 3.45248993\n",
      "Iteration 6828, loss = 3.42814890\n",
      "Iteration 6829, loss = 4.23511981\n",
      "Iteration 6830, loss = 5.16171227\n",
      "Iteration 6831, loss = 3.84635082\n",
      "Iteration 6832, loss = 6.08474763\n",
      "Iteration 6833, loss = 6.81440372\n",
      "Iteration 6834, loss = 10.88455558\n",
      "Iteration 6835, loss = 8.16754594\n",
      "Iteration 6836, loss = 5.28992923\n",
      "Iteration 6837, loss = 5.03098493\n",
      "Iteration 6838, loss = 6.03411190\n",
      "Iteration 6839, loss = 5.21223172\n",
      "Iteration 6840, loss = 3.93975547\n",
      "Iteration 6841, loss = 4.98964513\n",
      "Iteration 6842, loss = 4.06593768\n",
      "Iteration 6843, loss = 3.97854919\n",
      "Iteration 6844, loss = 5.19454608\n",
      "Iteration 6845, loss = 4.30572377\n",
      "Iteration 6846, loss = 3.16353947\n",
      "Iteration 6847, loss = 3.90862264\n",
      "Iteration 6848, loss = 3.72848326\n",
      "Iteration 6849, loss = 3.34230701\n",
      "Iteration 6850, loss = 4.00579635\n",
      "Iteration 6851, loss = 3.97025409\n",
      "Iteration 6852, loss = 3.57948887\n",
      "Iteration 6853, loss = 3.37194609\n",
      "Iteration 6854, loss = 3.05324773\n",
      "Iteration 6855, loss = 3.13583530\n",
      "Iteration 6856, loss = 3.39316486\n",
      "Iteration 6857, loss = 3.23375538\n",
      "Iteration 6858, loss = 4.28790386\n",
      "Iteration 6859, loss = 3.54849983\n",
      "Iteration 6860, loss = 3.45187594\n",
      "Iteration 6861, loss = 3.08958358\n",
      "Iteration 6862, loss = 3.32366070\n",
      "Iteration 6863, loss = 3.35879682\n",
      "Iteration 6864, loss = 3.06757724\n",
      "Iteration 6865, loss = 3.36162076\n",
      "Iteration 6866, loss = 3.99989704\n",
      "Iteration 6867, loss = 3.44238857\n",
      "Iteration 6868, loss = 4.33939066\n",
      "Iteration 6869, loss = 3.80091267\n",
      "Iteration 6870, loss = 3.66368692\n",
      "Iteration 6871, loss = 3.45170589\n",
      "Iteration 6872, loss = 3.71475950\n",
      "Iteration 6873, loss = 3.46376034\n",
      "Iteration 6874, loss = 3.34814163\n",
      "Iteration 6875, loss = 3.27969103\n",
      "Iteration 6876, loss = 3.16848606\n",
      "Iteration 6877, loss = 3.15237866\n",
      "Iteration 6878, loss = 3.46310156\n",
      "Iteration 6879, loss = 3.52136442\n",
      "Iteration 6880, loss = 3.40737469\n",
      "Iteration 6881, loss = 4.15104325\n",
      "Iteration 6882, loss = 3.83077220\n",
      "Iteration 6883, loss = 3.41126409\n",
      "Iteration 6884, loss = 3.18235416\n",
      "Iteration 6885, loss = 3.10030946\n",
      "Iteration 6886, loss = 3.06231434\n",
      "Iteration 6887, loss = 3.19101146\n",
      "Iteration 6888, loss = 2.99146276\n",
      "Iteration 6889, loss = 3.25751471\n",
      "Iteration 6890, loss = 3.29461071\n",
      "Iteration 6891, loss = 2.97090071\n",
      "Iteration 6892, loss = 3.05857800\n",
      "Iteration 6893, loss = 3.02130348\n",
      "Iteration 6894, loss = 3.01210335\n",
      "Iteration 6895, loss = 3.08047949\n",
      "Iteration 6896, loss = 3.09373401\n",
      "Iteration 6897, loss = 3.46722062\n",
      "Iteration 6898, loss = 3.40461398\n",
      "Iteration 6899, loss = 3.12584889\n",
      "Iteration 6900, loss = 3.17999908\n",
      "Iteration 6901, loss = 3.08179895\n",
      "Iteration 6902, loss = 3.63249812\n",
      "Iteration 6903, loss = 3.60611780\n",
      "Iteration 6904, loss = 3.03312858\n",
      "Iteration 6905, loss = 3.32386039\n",
      "Iteration 6906, loss = 3.74285413\n",
      "Iteration 6907, loss = 5.97163727\n",
      "Iteration 6908, loss = 5.58925280\n",
      "Iteration 6909, loss = 4.56266245\n",
      "Iteration 6910, loss = 4.41411098\n",
      "Iteration 6911, loss = 4.62898287\n",
      "Iteration 6912, loss = 4.51037626\n",
      "Iteration 6913, loss = 4.19689773\n",
      "Iteration 6914, loss = 3.33633264\n",
      "Iteration 6915, loss = 3.44118526\n",
      "Iteration 6916, loss = 3.71998010\n",
      "Iteration 6917, loss = 3.55199575\n",
      "Iteration 6918, loss = 3.29725421\n",
      "Iteration 6919, loss = 3.13430238\n",
      "Iteration 6920, loss = 3.24238009\n",
      "Iteration 6921, loss = 3.21416318\n",
      "Iteration 6922, loss = 3.75143625\n",
      "Iteration 6923, loss = 3.80714396\n",
      "Iteration 6924, loss = 3.95877392\n",
      "Iteration 6925, loss = 4.00814983\n",
      "Iteration 6926, loss = 3.16169464\n",
      "Iteration 6927, loss = 2.94224550\n",
      "Iteration 6928, loss = 3.18698067\n",
      "Iteration 6929, loss = 5.27200383\n",
      "Iteration 6930, loss = 5.59556255\n",
      "Iteration 6931, loss = 5.87852685\n",
      "Iteration 6932, loss = 5.66090862\n",
      "Iteration 6933, loss = 6.15822123\n",
      "Iteration 6934, loss = 5.23977781\n",
      "Iteration 6935, loss = 3.93523201\n",
      "Iteration 6936, loss = 4.64869740\n",
      "Iteration 6937, loss = 4.70086645\n",
      "Iteration 6938, loss = 4.27430860\n",
      "Iteration 6939, loss = 3.25301418\n",
      "Iteration 6940, loss = 3.93631679\n",
      "Iteration 6941, loss = 4.27181151\n",
      "Iteration 6942, loss = 4.12296525\n",
      "Iteration 6943, loss = 4.12367827\n",
      "Iteration 6944, loss = 3.76155645\n",
      "Iteration 6945, loss = 3.61472579\n",
      "Iteration 6946, loss = 4.40369141\n",
      "Iteration 6947, loss = 3.63076619\n",
      "Iteration 6948, loss = 3.66635382\n",
      "Iteration 6949, loss = 4.88726712\n",
      "Iteration 6950, loss = 3.78521386\n",
      "Iteration 6951, loss = 3.51619562\n",
      "Iteration 6952, loss = 3.53946995\n",
      "Iteration 6953, loss = 3.57519669\n",
      "Iteration 6954, loss = 3.21477688\n",
      "Iteration 6955, loss = 3.49354485\n",
      "Iteration 6956, loss = 3.83943885\n",
      "Iteration 6957, loss = 3.15626184\n",
      "Iteration 6958, loss = 3.45313596\n",
      "Iteration 6959, loss = 3.67100416\n",
      "Iteration 6960, loss = 4.08865551\n",
      "Iteration 6961, loss = 3.50757389\n",
      "Iteration 6962, loss = 3.04523033\n",
      "Iteration 6963, loss = 5.71704403\n",
      "Iteration 6964, loss = 4.61685795\n",
      "Iteration 6965, loss = 5.03099134\n",
      "Iteration 6966, loss = 3.77440504\n",
      "Iteration 6967, loss = 3.52486167\n",
      "Iteration 6968, loss = 3.47225775\n",
      "Iteration 6969, loss = 3.09750240\n",
      "Iteration 6970, loss = 3.28419290\n",
      "Iteration 6971, loss = 3.15673035\n",
      "Iteration 6972, loss = 4.36106481\n",
      "Iteration 6973, loss = 4.77485024\n",
      "Iteration 6974, loss = 5.81070609\n",
      "Iteration 6975, loss = 5.43538320\n",
      "Iteration 6976, loss = 4.11645912\n",
      "Iteration 6977, loss = 3.20520040\n",
      "Iteration 6978, loss = 3.48419980\n",
      "Iteration 6979, loss = 3.59164386\n",
      "Iteration 6980, loss = 3.25907288\n",
      "Iteration 6981, loss = 2.99199458\n",
      "Iteration 6982, loss = 3.12631191\n",
      "Iteration 6983, loss = 3.62232443\n",
      "Iteration 6984, loss = 3.68907653\n",
      "Iteration 6985, loss = 3.03481979\n",
      "Iteration 6986, loss = 3.37124541\n",
      "Iteration 6987, loss = 4.05010165\n",
      "Iteration 6988, loss = 4.25313631\n",
      "Iteration 6989, loss = 4.16720228\n",
      "Iteration 6990, loss = 8.41751443\n",
      "Iteration 6991, loss = 7.39839761\n",
      "Iteration 6992, loss = 4.74687889\n",
      "Iteration 6993, loss = 4.70634805\n",
      "Iteration 6994, loss = 3.40569674\n",
      "Iteration 6995, loss = 4.18014731\n",
      "Iteration 6996, loss = 3.46098131\n",
      "Iteration 6997, loss = 3.62481701\n",
      "Iteration 6998, loss = 3.40328230\n",
      "Iteration 6999, loss = 3.29167362\n",
      "Iteration 7000, loss = 3.45379091\n",
      "Iteration 7001, loss = 3.96995523\n",
      "Iteration 7002, loss = 4.28515876\n",
      "Iteration 7003, loss = 4.29930640\n",
      "Iteration 7004, loss = 3.73479337\n",
      "Iteration 7005, loss = 3.94282185\n",
      "Iteration 7006, loss = 4.54135099\n",
      "Iteration 7007, loss = 4.30766377\n",
      "Iteration 7008, loss = 3.58741295\n",
      "Iteration 7009, loss = 4.13313152\n",
      "Iteration 7010, loss = 3.84075951\n",
      "Iteration 7011, loss = 3.76572210\n",
      "Iteration 7012, loss = 5.20989809\n",
      "Iteration 7013, loss = 3.74334322\n",
      "Iteration 7014, loss = 3.42186434\n",
      "Iteration 7015, loss = 3.71919073\n",
      "Iteration 7016, loss = 4.44964576\n",
      "Iteration 7017, loss = 3.92400427\n",
      "Iteration 7018, loss = 3.80075811\n",
      "Iteration 7019, loss = 3.46616409\n",
      "Iteration 7020, loss = 3.34428586\n",
      "Iteration 7021, loss = 3.89777930\n",
      "Iteration 7022, loss = 3.93225955\n",
      "Iteration 7023, loss = 3.76488684\n",
      "Iteration 7024, loss = 3.45849137\n",
      "Iteration 7025, loss = 3.68337515\n",
      "Iteration 7026, loss = 3.68068568\n",
      "Iteration 7027, loss = 3.94388865\n",
      "Iteration 7028, loss = 3.79040449\n",
      "Iteration 7029, loss = 3.59502501\n",
      "Iteration 7030, loss = 4.79951323\n",
      "Iteration 7031, loss = 4.19017004\n",
      "Iteration 7032, loss = 5.17372474\n",
      "Iteration 7033, loss = 5.44876653\n",
      "Iteration 7034, loss = 3.89419132\n",
      "Iteration 7035, loss = 5.20991442\n",
      "Iteration 7036, loss = 3.79704528\n",
      "Iteration 7037, loss = 3.16822801\n",
      "Iteration 7038, loss = 3.09716971\n",
      "Iteration 7039, loss = 3.07404851\n",
      "Iteration 7040, loss = 3.12270970\n",
      "Iteration 7041, loss = 3.15828926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7042, loss = 3.35401081\n",
      "Iteration 7043, loss = 3.74235999\n",
      "Iteration 7044, loss = 4.26474949\n",
      "Iteration 7045, loss = 3.43899661\n",
      "Iteration 7046, loss = 3.35742580\n",
      "Iteration 7047, loss = 3.87378908\n",
      "Iteration 7048, loss = 9.05775964\n",
      "Iteration 7049, loss = 5.21717967\n",
      "Iteration 7050, loss = 5.01735924\n",
      "Iteration 7051, loss = 4.41043988\n",
      "Iteration 7052, loss = 4.11343124\n",
      "Iteration 7053, loss = 3.63040868\n",
      "Iteration 7054, loss = 3.90642714\n",
      "Iteration 7055, loss = 3.81213508\n",
      "Iteration 7056, loss = 3.13738519\n",
      "Iteration 7057, loss = 3.23317156\n",
      "Iteration 7058, loss = 3.12643253\n",
      "Iteration 7059, loss = 3.04279676\n",
      "Iteration 7060, loss = 3.59665088\n",
      "Iteration 7061, loss = 3.20006550\n",
      "Iteration 7062, loss = 3.18009644\n",
      "Iteration 7063, loss = 3.12421171\n",
      "Iteration 7064, loss = 3.79706147\n",
      "Iteration 7065, loss = 3.47540026\n",
      "Iteration 7066, loss = 3.47360753\n",
      "Iteration 7067, loss = 3.38826326\n",
      "Iteration 7068, loss = 3.47035285\n",
      "Iteration 7069, loss = 3.15394241\n",
      "Iteration 7070, loss = 5.27322555\n",
      "Iteration 7071, loss = 5.17538958\n",
      "Iteration 7072, loss = 5.26444843\n",
      "Iteration 7073, loss = 4.59194056\n",
      "Iteration 7074, loss = 3.95068103\n",
      "Iteration 7075, loss = 3.63802747\n",
      "Iteration 7076, loss = 3.67476154\n",
      "Iteration 7077, loss = 3.44702637\n",
      "Iteration 7078, loss = 3.22674380\n",
      "Iteration 7079, loss = 3.38653594\n",
      "Iteration 7080, loss = 4.92086349\n",
      "Iteration 7081, loss = 4.63143946\n",
      "Iteration 7082, loss = 5.58612290\n",
      "Iteration 7083, loss = 6.12808312\n",
      "Iteration 7084, loss = 7.44238302\n",
      "Iteration 7085, loss = 3.68678914\n",
      "Iteration 7086, loss = 3.48882971\n",
      "Iteration 7087, loss = 3.87740632\n",
      "Iteration 7088, loss = 4.98387247\n",
      "Iteration 7089, loss = 4.45750173\n",
      "Iteration 7090, loss = 3.70424003\n",
      "Iteration 7091, loss = 3.62218171\n",
      "Iteration 7092, loss = 3.60644768\n",
      "Iteration 7093, loss = 3.48474876\n",
      "Iteration 7094, loss = 3.16573866\n",
      "Iteration 7095, loss = 2.96133741\n",
      "Iteration 7096, loss = 2.98776230\n",
      "Iteration 7097, loss = 3.26081928\n",
      "Iteration 7098, loss = 3.36268833\n",
      "Iteration 7099, loss = 3.40162605\n",
      "Iteration 7100, loss = 3.22931551\n",
      "Iteration 7101, loss = 3.71530071\n",
      "Iteration 7102, loss = 3.09137649\n",
      "Iteration 7103, loss = 5.12400574\n",
      "Iteration 7104, loss = 3.94645246\n",
      "Iteration 7105, loss = 3.87052804\n",
      "Iteration 7106, loss = 4.63979962\n",
      "Iteration 7107, loss = 4.56643591\n",
      "Iteration 7108, loss = 4.85122539\n",
      "Iteration 7109, loss = 4.90563175\n",
      "Iteration 7110, loss = 4.28342959\n",
      "Iteration 7111, loss = 3.69646795\n",
      "Iteration 7112, loss = 3.98642248\n",
      "Iteration 7113, loss = 4.12196247\n",
      "Iteration 7114, loss = 3.98343991\n",
      "Iteration 7115, loss = 3.55158368\n",
      "Iteration 7116, loss = 3.49449785\n",
      "Iteration 7117, loss = 4.00672663\n",
      "Iteration 7118, loss = 3.34387790\n",
      "Iteration 7119, loss = 2.94284343\n",
      "Iteration 7120, loss = 3.12009328\n",
      "Iteration 7121, loss = 3.07653655\n",
      "Iteration 7122, loss = 3.15473202\n",
      "Iteration 7123, loss = 2.90093196\n",
      "Iteration 7124, loss = 2.95093638\n",
      "Iteration 7125, loss = 3.00057329\n",
      "Iteration 7126, loss = 2.98934446\n",
      "Iteration 7127, loss = 2.96635952\n",
      "Iteration 7128, loss = 3.26290417\n",
      "Iteration 7129, loss = 3.62480739\n",
      "Iteration 7130, loss = 3.61000498\n",
      "Iteration 7131, loss = 5.16420784\n",
      "Iteration 7132, loss = 4.60740521\n",
      "Iteration 7133, loss = 3.82359837\n",
      "Iteration 7134, loss = 3.96767856\n",
      "Iteration 7135, loss = 4.31900942\n",
      "Iteration 7136, loss = 3.96929347\n",
      "Iteration 7137, loss = 4.96243195\n",
      "Iteration 7138, loss = 3.93910897\n",
      "Iteration 7139, loss = 3.74412903\n",
      "Iteration 7140, loss = 4.59916069\n",
      "Iteration 7141, loss = 4.27152528\n",
      "Iteration 7142, loss = 3.56944913\n",
      "Iteration 7143, loss = 3.93753697\n",
      "Iteration 7144, loss = 3.12825385\n",
      "Iteration 7145, loss = 2.97108243\n",
      "Iteration 7146, loss = 3.13783886\n",
      "Iteration 7147, loss = 3.31384781\n",
      "Iteration 7148, loss = 3.57245924\n",
      "Iteration 7149, loss = 3.43811195\n",
      "Iteration 7150, loss = 3.25929832\n",
      "Iteration 7151, loss = 3.36470678\n",
      "Iteration 7152, loss = 3.42902771\n",
      "Iteration 7153, loss = 3.28179761\n",
      "Iteration 7154, loss = 3.32283508\n",
      "Iteration 7155, loss = 3.10262555\n",
      "Iteration 7156, loss = 2.94956110\n",
      "Iteration 7157, loss = 3.28888717\n",
      "Iteration 7158, loss = 2.95328775\n",
      "Iteration 7159, loss = 3.19990883\n",
      "Iteration 7160, loss = 3.51979376\n",
      "Iteration 7161, loss = 2.89759842\n",
      "Iteration 7162, loss = 3.03267750\n",
      "Iteration 7163, loss = 3.76294237\n",
      "Iteration 7164, loss = 4.26089579\n",
      "Iteration 7165, loss = 3.22357689\n",
      "Iteration 7166, loss = 3.35909434\n",
      "Iteration 7167, loss = 2.96264101\n",
      "Iteration 7168, loss = 2.83006916\n",
      "Iteration 7169, loss = 3.47738911\n",
      "Iteration 7170, loss = 3.21877377\n",
      "Iteration 7171, loss = 3.27747642\n",
      "Iteration 7172, loss = 2.99523456\n",
      "Iteration 7173, loss = 3.43657765\n",
      "Iteration 7174, loss = 3.22345573\n",
      "Iteration 7175, loss = 3.25387090\n",
      "Iteration 7176, loss = 3.82861192\n",
      "Iteration 7177, loss = 4.94300844\n",
      "Iteration 7178, loss = 4.64817930\n",
      "Iteration 7179, loss = 3.67699249\n",
      "Iteration 7180, loss = 3.93768323\n",
      "Iteration 7181, loss = 3.32728096\n",
      "Iteration 7182, loss = 3.30494063\n",
      "Iteration 7183, loss = 3.08073906\n",
      "Iteration 7184, loss = 2.94995314\n",
      "Iteration 7185, loss = 4.41015473\n",
      "Iteration 7186, loss = 3.91869792\n",
      "Iteration 7187, loss = 3.33348441\n",
      "Iteration 7188, loss = 3.11761439\n",
      "Iteration 7189, loss = 3.08010112\n",
      "Iteration 7190, loss = 3.28329061\n",
      "Iteration 7191, loss = 3.61859441\n",
      "Iteration 7192, loss = 5.74466824\n",
      "Iteration 7193, loss = 4.78366858\n",
      "Iteration 7194, loss = 2.98035428\n",
      "Iteration 7195, loss = 3.46870875\n",
      "Iteration 7196, loss = 3.54749202\n",
      "Iteration 7197, loss = 3.05784305\n",
      "Iteration 7198, loss = 2.89080924\n",
      "Iteration 7199, loss = 5.19429403\n",
      "Iteration 7200, loss = 7.75815298\n",
      "Iteration 7201, loss = 6.75187308\n",
      "Iteration 7202, loss = 5.75204662\n",
      "Iteration 7203, loss = 5.09266940\n",
      "Iteration 7204, loss = 4.36382338\n",
      "Iteration 7205, loss = 3.66697996\n",
      "Iteration 7206, loss = 3.98302404\n",
      "Iteration 7207, loss = 4.05492311\n",
      "Iteration 7208, loss = 3.18388826\n",
      "Iteration 7209, loss = 3.41040285\n",
      "Iteration 7210, loss = 3.06091443\n",
      "Iteration 7211, loss = 3.16808052\n",
      "Iteration 7212, loss = 3.36873326\n",
      "Iteration 7213, loss = 3.67188549\n",
      "Iteration 7214, loss = 3.48226673\n",
      "Iteration 7215, loss = 3.87301788\n",
      "Iteration 7216, loss = 3.84947994\n",
      "Iteration 7217, loss = 3.49901127\n",
      "Iteration 7218, loss = 4.20067260\n",
      "Iteration 7219, loss = 3.26178633\n",
      "Iteration 7220, loss = 3.22054042\n",
      "Iteration 7221, loss = 3.24893002\n",
      "Iteration 7222, loss = 3.25826174\n",
      "Iteration 7223, loss = 3.65580676\n",
      "Iteration 7224, loss = 3.90954135\n",
      "Iteration 7225, loss = 3.69982624\n",
      "Iteration 7226, loss = 5.65866144\n",
      "Iteration 7227, loss = 3.71479275\n",
      "Iteration 7228, loss = 3.22649894\n",
      "Iteration 7229, loss = 3.81760079\n",
      "Iteration 7230, loss = 3.00397333\n",
      "Iteration 7231, loss = 3.08024202\n",
      "Iteration 7232, loss = 3.55230475\n",
      "Iteration 7233, loss = 3.33966421\n",
      "Iteration 7234, loss = 2.96724638\n",
      "Iteration 7235, loss = 3.43408295\n",
      "Iteration 7236, loss = 4.22065785\n",
      "Iteration 7237, loss = 3.52667540\n",
      "Iteration 7238, loss = 3.26613262\n",
      "Iteration 7239, loss = 4.08265972\n",
      "Iteration 7240, loss = 3.67966288\n",
      "Iteration 7241, loss = 3.02310630\n",
      "Iteration 7242, loss = 3.89294541\n",
      "Iteration 7243, loss = 4.46098743\n",
      "Iteration 7244, loss = 4.21500400\n",
      "Iteration 7245, loss = 4.35895102\n",
      "Iteration 7246, loss = 3.38640754\n",
      "Iteration 7247, loss = 2.99811875\n",
      "Iteration 7248, loss = 3.44747340\n",
      "Iteration 7249, loss = 3.76199517\n",
      "Iteration 7250, loss = 2.97373872\n",
      "Iteration 7251, loss = 3.19823590\n",
      "Iteration 7252, loss = 3.37368204\n",
      "Iteration 7253, loss = 2.91463974\n",
      "Iteration 7254, loss = 3.16091545\n",
      "Iteration 7255, loss = 3.15815157\n",
      "Iteration 7256, loss = 3.54441728\n",
      "Iteration 7257, loss = 3.35082325\n",
      "Iteration 7258, loss = 3.59091382\n",
      "Iteration 7259, loss = 4.15048584\n",
      "Iteration 7260, loss = 3.67648853\n",
      "Iteration 7261, loss = 3.72106104\n",
      "Iteration 7262, loss = 3.78458435\n",
      "Iteration 7263, loss = 4.00159420\n",
      "Iteration 7264, loss = 2.96868692\n",
      "Iteration 7265, loss = 3.03659657\n",
      "Iteration 7266, loss = 3.46507135\n",
      "Iteration 7267, loss = 3.46699787\n",
      "Iteration 7268, loss = 3.91861465\n",
      "Iteration 7269, loss = 3.55970022\n",
      "Iteration 7270, loss = 2.92346305\n",
      "Iteration 7271, loss = 2.85580263\n",
      "Iteration 7272, loss = 3.13701606\n",
      "Iteration 7273, loss = 4.91258815\n",
      "Iteration 7274, loss = 3.90094953\n",
      "Iteration 7275, loss = 3.58159964\n",
      "Iteration 7276, loss = 3.52841693\n",
      "Iteration 7277, loss = 3.42941011\n",
      "Iteration 7278, loss = 3.21591219\n",
      "Iteration 7279, loss = 3.55371772\n",
      "Iteration 7280, loss = 3.64920246\n",
      "Iteration 7281, loss = 4.05675722\n",
      "Iteration 7282, loss = 5.26515024\n",
      "Iteration 7283, loss = 4.61711126\n",
      "Iteration 7284, loss = 3.83036232\n",
      "Iteration 7285, loss = 6.04499228\n",
      "Iteration 7286, loss = 4.72802600\n",
      "Iteration 7287, loss = 3.78460851\n",
      "Iteration 7288, loss = 4.43523839\n",
      "Iteration 7289, loss = 3.99479129\n",
      "Iteration 7290, loss = 3.31096573\n",
      "Iteration 7291, loss = 2.84922460\n",
      "Iteration 7292, loss = 2.90601330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7293, loss = 2.95247596\n",
      "Iteration 7294, loss = 2.76746727\n",
      "Iteration 7295, loss = 3.37274714\n",
      "Iteration 7296, loss = 3.72696930\n",
      "Iteration 7297, loss = 4.30247804\n",
      "Iteration 7298, loss = 3.86211086\n",
      "Iteration 7299, loss = 4.79248098\n",
      "Iteration 7300, loss = 4.49741767\n",
      "Iteration 7301, loss = 3.96014802\n",
      "Iteration 7302, loss = 6.72046854\n",
      "Iteration 7303, loss = 5.87240883\n",
      "Iteration 7304, loss = 6.04089535\n",
      "Iteration 7305, loss = 3.86411641\n",
      "Iteration 7306, loss = 6.05605222\n",
      "Iteration 7307, loss = 3.81438072\n",
      "Iteration 7308, loss = 3.22626874\n",
      "Iteration 7309, loss = 3.48584409\n",
      "Iteration 7310, loss = 3.71424383\n",
      "Iteration 7311, loss = 3.12545771\n",
      "Iteration 7312, loss = 2.84000113\n",
      "Iteration 7313, loss = 3.08330970\n",
      "Iteration 7314, loss = 3.05359844\n",
      "Iteration 7315, loss = 3.09818114\n",
      "Iteration 7316, loss = 4.19716941\n",
      "Iteration 7317, loss = 4.04803741\n",
      "Iteration 7318, loss = 4.31192338\n",
      "Iteration 7319, loss = 2.98760871\n",
      "Iteration 7320, loss = 3.19675845\n",
      "Iteration 7321, loss = 2.79630844\n",
      "Iteration 7322, loss = 3.17492104\n",
      "Iteration 7323, loss = 3.46511626\n",
      "Iteration 7324, loss = 3.22870237\n",
      "Iteration 7325, loss = 3.31291756\n",
      "Iteration 7326, loss = 3.26816928\n",
      "Iteration 7327, loss = 3.05556838\n",
      "Iteration 7328, loss = 2.89123126\n",
      "Iteration 7329, loss = 2.91333228\n",
      "Iteration 7330, loss = 3.16849747\n",
      "Iteration 7331, loss = 3.20544232\n",
      "Iteration 7332, loss = 3.57702317\n",
      "Iteration 7333, loss = 3.01041877\n",
      "Iteration 7334, loss = 3.62688082\n",
      "Iteration 7335, loss = 3.11861060\n",
      "Iteration 7336, loss = 2.96322107\n",
      "Iteration 7337, loss = 3.52012519\n",
      "Iteration 7338, loss = 4.07431076\n",
      "Iteration 7339, loss = 3.55146021\n",
      "Iteration 7340, loss = 3.99346564\n",
      "Iteration 7341, loss = 4.04508719\n",
      "Iteration 7342, loss = 5.19205927\n",
      "Iteration 7343, loss = 3.67332019\n",
      "Iteration 7344, loss = 4.69167217\n",
      "Iteration 7345, loss = 9.70966954\n",
      "Iteration 7346, loss = 5.93022244\n",
      "Iteration 7347, loss = 5.13899959\n",
      "Iteration 7348, loss = 5.57774144\n",
      "Iteration 7349, loss = 4.58902120\n",
      "Iteration 7350, loss = 4.32787690\n",
      "Iteration 7351, loss = 3.69998353\n",
      "Iteration 7352, loss = 3.42808046\n",
      "Iteration 7353, loss = 3.08858508\n",
      "Iteration 7354, loss = 2.83983422\n",
      "Iteration 7355, loss = 2.78530154\n",
      "Iteration 7356, loss = 2.90322835\n",
      "Iteration 7357, loss = 3.06692349\n",
      "Iteration 7358, loss = 3.74749210\n",
      "Iteration 7359, loss = 3.20513131\n",
      "Iteration 7360, loss = 3.19584851\n",
      "Iteration 7361, loss = 2.88080247\n",
      "Iteration 7362, loss = 2.82949631\n",
      "Iteration 7363, loss = 2.80242145\n",
      "Iteration 7364, loss = 3.04171016\n",
      "Iteration 7365, loss = 3.63025470\n",
      "Iteration 7366, loss = 4.02777441\n",
      "Iteration 7367, loss = 4.56288992\n",
      "Iteration 7368, loss = 4.64505280\n",
      "Iteration 7369, loss = 3.64382628\n",
      "Iteration 7370, loss = 4.64186753\n",
      "Iteration 7371, loss = 3.53912488\n",
      "Iteration 7372, loss = 4.43807924\n",
      "Iteration 7373, loss = 3.52614192\n",
      "Iteration 7374, loss = 4.17246633\n",
      "Iteration 7375, loss = 4.31632923\n",
      "Iteration 7376, loss = 4.31653344\n",
      "Iteration 7377, loss = 3.98861248\n",
      "Iteration 7378, loss = 3.55430129\n",
      "Iteration 7379, loss = 3.44903691\n",
      "Iteration 7380, loss = 4.46935396\n",
      "Iteration 7381, loss = 4.15276192\n",
      "Iteration 7382, loss = 4.18199316\n",
      "Iteration 7383, loss = 4.29232301\n",
      "Iteration 7384, loss = 3.67509967\n",
      "Iteration 7385, loss = 3.48546087\n",
      "Iteration 7386, loss = 3.18981989\n",
      "Iteration 7387, loss = 2.79101970\n",
      "Iteration 7388, loss = 3.04500113\n",
      "Iteration 7389, loss = 3.06759433\n",
      "Iteration 7390, loss = 3.21989646\n",
      "Iteration 7391, loss = 3.06540311\n",
      "Iteration 7392, loss = 3.01970685\n",
      "Iteration 7393, loss = 3.43771112\n",
      "Iteration 7394, loss = 3.78810448\n",
      "Iteration 7395, loss = 3.89425704\n",
      "Iteration 7396, loss = 4.25027382\n",
      "Iteration 7397, loss = 3.18407226\n",
      "Iteration 7398, loss = 3.14953131\n",
      "Iteration 7399, loss = 3.10056260\n",
      "Iteration 7400, loss = 3.48033544\n",
      "Iteration 7401, loss = 3.52442589\n",
      "Iteration 7402, loss = 3.13459609\n",
      "Iteration 7403, loss = 4.37385176\n",
      "Iteration 7404, loss = 5.22264477\n",
      "Iteration 7405, loss = 5.49992285\n",
      "Iteration 7406, loss = 4.82597132\n",
      "Iteration 7407, loss = 3.70321552\n",
      "Iteration 7408, loss = 4.37432673\n",
      "Iteration 7409, loss = 3.23785775\n",
      "Iteration 7410, loss = 4.18419449\n",
      "Iteration 7411, loss = 3.69517931\n",
      "Iteration 7412, loss = 3.69952569\n",
      "Iteration 7413, loss = 3.61308484\n",
      "Iteration 7414, loss = 3.35385622\n",
      "Iteration 7415, loss = 4.23439129\n",
      "Iteration 7416, loss = 3.45103965\n",
      "Iteration 7417, loss = 2.83943369\n",
      "Iteration 7418, loss = 3.33614201\n",
      "Iteration 7419, loss = 3.61706211\n",
      "Iteration 7420, loss = 3.29044118\n",
      "Iteration 7421, loss = 3.13526011\n",
      "Iteration 7422, loss = 2.90705427\n",
      "Iteration 7423, loss = 2.90968217\n",
      "Iteration 7424, loss = 2.86368782\n",
      "Iteration 7425, loss = 3.67799843\n",
      "Iteration 7426, loss = 2.91921457\n",
      "Iteration 7427, loss = 3.74975486\n",
      "Iteration 7428, loss = 3.01248506\n",
      "Iteration 7429, loss = 3.67094677\n",
      "Iteration 7430, loss = 4.16257176\n",
      "Iteration 7431, loss = 3.43093896\n",
      "Iteration 7432, loss = 3.24803280\n",
      "Iteration 7433, loss = 3.12234452\n",
      "Iteration 7434, loss = 3.40105080\n",
      "Iteration 7435, loss = 3.12376159\n",
      "Iteration 7436, loss = 2.89372646\n",
      "Iteration 7437, loss = 3.07270349\n",
      "Iteration 7438, loss = 2.99377023\n",
      "Iteration 7439, loss = 2.74618548\n",
      "Iteration 7440, loss = 2.94241818\n",
      "Iteration 7441, loss = 3.56570305\n",
      "Iteration 7442, loss = 3.16981856\n",
      "Iteration 7443, loss = 2.92865663\n",
      "Iteration 7444, loss = 3.00909608\n",
      "Iteration 7445, loss = 2.71416135\n",
      "Iteration 7446, loss = 3.63626593\n",
      "Iteration 7447, loss = 4.19511469\n",
      "Iteration 7448, loss = 4.43790337\n",
      "Iteration 7449, loss = 5.74060563\n",
      "Iteration 7450, loss = 3.73053345\n",
      "Iteration 7451, loss = 3.95328611\n",
      "Iteration 7452, loss = 3.87823202\n",
      "Iteration 7453, loss = 2.78387081\n",
      "Iteration 7454, loss = 4.18450187\n",
      "Iteration 7455, loss = 3.97858662\n",
      "Iteration 7456, loss = 2.78953646\n",
      "Iteration 7457, loss = 3.00889721\n",
      "Iteration 7458, loss = 2.92586202\n",
      "Iteration 7459, loss = 2.88986747\n",
      "Iteration 7460, loss = 2.70073850\n",
      "Iteration 7461, loss = 2.71690484\n",
      "Iteration 7462, loss = 2.65802741\n",
      "Iteration 7463, loss = 2.69331772\n",
      "Iteration 7464, loss = 2.85565888\n",
      "Iteration 7465, loss = 2.74576360\n",
      "Iteration 7466, loss = 3.05440482\n",
      "Iteration 7467, loss = 2.79561770\n",
      "Iteration 7468, loss = 3.05742539\n",
      "Iteration 7469, loss = 3.65650675\n",
      "Iteration 7470, loss = 2.69083501\n",
      "Iteration 7471, loss = 2.61694812\n",
      "Iteration 7472, loss = 2.87684729\n",
      "Iteration 7473, loss = 3.10063630\n",
      "Iteration 7474, loss = 3.09550346\n",
      "Iteration 7475, loss = 3.36154388\n",
      "Iteration 7476, loss = 2.54881684\n",
      "Iteration 7477, loss = 2.94116682\n",
      "Iteration 7478, loss = 2.88162103\n",
      "Iteration 7479, loss = 3.33199606\n",
      "Iteration 7480, loss = 3.22462750\n",
      "Iteration 7481, loss = 3.18822599\n",
      "Iteration 7482, loss = 3.24851731\n",
      "Iteration 7483, loss = 4.57763222\n",
      "Iteration 7484, loss = 3.51511777\n",
      "Iteration 7485, loss = 3.75463277\n",
      "Iteration 7486, loss = 2.91398029\n",
      "Iteration 7487, loss = 3.06129578\n",
      "Iteration 7488, loss = 3.09885495\n",
      "Iteration 7489, loss = 3.25072722\n",
      "Iteration 7490, loss = 2.77451116\n",
      "Iteration 7491, loss = 2.77586512\n",
      "Iteration 7492, loss = 3.04759400\n",
      "Iteration 7493, loss = 2.67396643\n",
      "Iteration 7494, loss = 4.42584108\n",
      "Iteration 7495, loss = 3.51816861\n",
      "Iteration 7496, loss = 2.56693080\n",
      "Iteration 7497, loss = 2.63703154\n",
      "Iteration 7498, loss = 2.88931123\n",
      "Iteration 7499, loss = 2.83111870\n",
      "Iteration 7500, loss = 4.98009265\n",
      "Iteration 7501, loss = 5.30325110\n",
      "Iteration 7502, loss = 5.55164416\n",
      "Iteration 7503, loss = 5.53952568\n",
      "Iteration 7504, loss = 4.44872180\n",
      "Iteration 7505, loss = 4.23896120\n",
      "Iteration 7506, loss = 3.65430745\n",
      "Iteration 7507, loss = 3.32092526\n",
      "Iteration 7508, loss = 3.17407724\n",
      "Iteration 7509, loss = 3.05036318\n",
      "Iteration 7510, loss = 2.92467260\n",
      "Iteration 7511, loss = 2.91504515\n",
      "Iteration 7512, loss = 2.86072993\n",
      "Iteration 7513, loss = 2.57874795\n",
      "Iteration 7514, loss = 3.68495210\n",
      "Iteration 7515, loss = 2.99576821\n",
      "Iteration 7516, loss = 3.48391943\n",
      "Iteration 7517, loss = 3.09806597\n",
      "Iteration 7518, loss = 2.82863119\n",
      "Iteration 7519, loss = 2.79531524\n",
      "Iteration 7520, loss = 2.93130941\n",
      "Iteration 7521, loss = 2.75827563\n",
      "Iteration 7522, loss = 3.37860640\n",
      "Iteration 7523, loss = 4.01822360\n",
      "Iteration 7524, loss = 2.88843886\n",
      "Iteration 7525, loss = 3.74777087\n",
      "Iteration 7526, loss = 5.07429603\n",
      "Iteration 7527, loss = 4.63463981\n",
      "Iteration 7528, loss = 3.86147184\n",
      "Iteration 7529, loss = 3.99700549\n",
      "Iteration 7530, loss = 3.24116441\n",
      "Iteration 7531, loss = 3.24328507\n",
      "Iteration 7532, loss = 3.25972887\n",
      "Iteration 7533, loss = 3.35883429\n",
      "Iteration 7534, loss = 2.85173741\n",
      "Iteration 7535, loss = 2.73341461\n",
      "Iteration 7536, loss = 3.00984872\n",
      "Iteration 7537, loss = 5.85446726\n",
      "Iteration 7538, loss = 5.49431801\n",
      "Iteration 7539, loss = 4.24160132\n",
      "Iteration 7540, loss = 3.27990199\n",
      "Iteration 7541, loss = 4.20150000\n",
      "Iteration 7542, loss = 4.22683833\n",
      "Iteration 7543, loss = 3.32410525\n",
      "Iteration 7544, loss = 2.88432622\n",
      "Iteration 7545, loss = 3.07579668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7546, loss = 3.04797422\n",
      "Iteration 7547, loss = 2.62210226\n",
      "Iteration 7548, loss = 2.74712644\n",
      "Iteration 7549, loss = 2.57076646\n",
      "Iteration 7550, loss = 4.87316949\n",
      "Iteration 7551, loss = 4.05829703\n",
      "Iteration 7552, loss = 3.33134658\n",
      "Iteration 7553, loss = 2.96001651\n",
      "Iteration 7554, loss = 3.19420527\n",
      "Iteration 7555, loss = 3.28413364\n",
      "Iteration 7556, loss = 4.31865177\n",
      "Iteration 7557, loss = 3.34188613\n",
      "Iteration 7558, loss = 3.73608724\n",
      "Iteration 7559, loss = 2.79946768\n",
      "Iteration 7560, loss = 3.57686167\n",
      "Iteration 7561, loss = 3.09785442\n",
      "Iteration 7562, loss = 3.46351580\n",
      "Iteration 7563, loss = 3.65888483\n",
      "Iteration 7564, loss = 3.50634822\n",
      "Iteration 7565, loss = 3.06035788\n",
      "Iteration 7566, loss = 3.92740884\n",
      "Iteration 7567, loss = 3.42682838\n",
      "Iteration 7568, loss = 2.75271752\n",
      "Iteration 7569, loss = 2.89431622\n",
      "Iteration 7570, loss = 2.88356869\n",
      "Iteration 7571, loss = 2.62366051\n",
      "Iteration 7572, loss = 2.54551582\n",
      "Iteration 7573, loss = 3.66034837\n",
      "Iteration 7574, loss = 3.23310562\n",
      "Iteration 7575, loss = 3.13458438\n",
      "Iteration 7576, loss = 3.21558197\n",
      "Iteration 7577, loss = 3.81911728\n",
      "Iteration 7578, loss = 2.97129526\n",
      "Iteration 7579, loss = 3.11207250\n",
      "Iteration 7580, loss = 3.65194258\n",
      "Iteration 7581, loss = 2.68267329\n",
      "Iteration 7582, loss = 2.88780177\n",
      "Iteration 7583, loss = 3.19832752\n",
      "Iteration 7584, loss = 2.70661611\n",
      "Iteration 7585, loss = 4.48852896\n",
      "Iteration 7586, loss = 4.24748828\n",
      "Iteration 7587, loss = 3.14961821\n",
      "Iteration 7588, loss = 2.72308799\n",
      "Iteration 7589, loss = 3.85352055\n",
      "Iteration 7590, loss = 2.99820838\n",
      "Iteration 7591, loss = 4.92132789\n",
      "Iteration 7592, loss = 4.79089165\n",
      "Iteration 7593, loss = 5.50934472\n",
      "Iteration 7594, loss = 5.00903080\n",
      "Iteration 7595, loss = 3.16796391\n",
      "Iteration 7596, loss = 3.23340183\n",
      "Iteration 7597, loss = 6.36676625\n",
      "Iteration 7598, loss = 4.41101756\n",
      "Iteration 7599, loss = 4.41207786\n",
      "Iteration 7600, loss = 5.02505007\n",
      "Iteration 7601, loss = 5.24149377\n",
      "Iteration 7602, loss = 3.02729546\n",
      "Iteration 7603, loss = 3.28038732\n",
      "Iteration 7604, loss = 3.10672173\n",
      "Iteration 7605, loss = 2.91490221\n",
      "Iteration 7606, loss = 2.54967750\n",
      "Iteration 7607, loss = 3.11804078\n",
      "Iteration 7608, loss = 3.19047166\n",
      "Iteration 7609, loss = 3.97090670\n",
      "Iteration 7610, loss = 4.08821226\n",
      "Iteration 7611, loss = 3.34961281\n",
      "Iteration 7612, loss = 3.68406109\n",
      "Iteration 7613, loss = 3.18721170\n",
      "Iteration 7614, loss = 3.07321596\n",
      "Iteration 7615, loss = 2.80863441\n",
      "Iteration 7616, loss = 3.33914880\n",
      "Iteration 7617, loss = 2.63078224\n",
      "Iteration 7618, loss = 2.57434612\n",
      "Iteration 7619, loss = 2.76944008\n",
      "Iteration 7620, loss = 2.78131563\n",
      "Iteration 7621, loss = 2.83178045\n",
      "Iteration 7622, loss = 3.20993842\n",
      "Iteration 7623, loss = 3.37382375\n",
      "Iteration 7624, loss = 5.01138599\n",
      "Iteration 7625, loss = 3.63005209\n",
      "Iteration 7626, loss = 2.76519581\n",
      "Iteration 7627, loss = 2.71674639\n",
      "Iteration 7628, loss = 2.91865059\n",
      "Iteration 7629, loss = 3.54314037\n",
      "Iteration 7630, loss = 4.16710194\n",
      "Iteration 7631, loss = 3.02240994\n",
      "Iteration 7632, loss = 2.73598535\n",
      "Iteration 7633, loss = 3.86899543\n",
      "Iteration 7634, loss = 3.56865807\n",
      "Iteration 7635, loss = 3.01143515\n",
      "Iteration 7636, loss = 2.77034876\n",
      "Iteration 7637, loss = 2.53838718\n",
      "Iteration 7638, loss = 2.69833345\n",
      "Iteration 7639, loss = 2.83046543\n",
      "Iteration 7640, loss = 2.85938159\n",
      "Iteration 7641, loss = 3.40398057\n",
      "Iteration 7642, loss = 3.86500304\n",
      "Iteration 7643, loss = 4.01563771\n",
      "Iteration 7644, loss = 3.89459166\n",
      "Iteration 7645, loss = 3.53277749\n",
      "Iteration 7646, loss = 3.29577674\n",
      "Iteration 7647, loss = 2.85611728\n",
      "Iteration 7648, loss = 5.63606639\n",
      "Iteration 7649, loss = 4.40542987\n",
      "Iteration 7650, loss = 3.77995920\n",
      "Iteration 7651, loss = 2.92107856\n",
      "Iteration 7652, loss = 4.09267375\n",
      "Iteration 7653, loss = 3.30498354\n",
      "Iteration 7654, loss = 3.87975849\n",
      "Iteration 7655, loss = 3.84126134\n",
      "Iteration 7656, loss = 3.90417544\n",
      "Iteration 7657, loss = 3.22653191\n",
      "Iteration 7658, loss = 3.64099132\n",
      "Iteration 7659, loss = 3.70300738\n",
      "Iteration 7660, loss = 3.06508888\n",
      "Iteration 7661, loss = 3.16137290\n",
      "Iteration 7662, loss = 3.12240561\n",
      "Iteration 7663, loss = 3.31272946\n",
      "Iteration 7664, loss = 3.83763247\n",
      "Iteration 7665, loss = 3.18461697\n",
      "Iteration 7666, loss = 2.60756461\n",
      "Iteration 7667, loss = 3.00135667\n",
      "Iteration 7668, loss = 2.56062861\n",
      "Iteration 7669, loss = 2.54037003\n",
      "Iteration 7670, loss = 2.63843940\n",
      "Iteration 7671, loss = 3.00481116\n",
      "Iteration 7672, loss = 2.86450731\n",
      "Iteration 7673, loss = 2.64184411\n",
      "Iteration 7674, loss = 3.36109889\n",
      "Iteration 7675, loss = 3.60729783\n",
      "Iteration 7676, loss = 3.66196930\n",
      "Iteration 7677, loss = 3.50629067\n",
      "Iteration 7678, loss = 3.46738875\n",
      "Iteration 7679, loss = 3.32198293\n",
      "Iteration 7680, loss = 3.30473130\n",
      "Iteration 7681, loss = 3.78866954\n",
      "Iteration 7682, loss = 3.08124128\n",
      "Iteration 7683, loss = 3.16030890\n",
      "Iteration 7684, loss = 3.27238736\n",
      "Iteration 7685, loss = 3.00661238\n",
      "Iteration 7686, loss = 2.60003353\n",
      "Iteration 7687, loss = 4.48166912\n",
      "Iteration 7688, loss = 4.27458124\n",
      "Iteration 7689, loss = 4.37531949\n",
      "Iteration 7690, loss = 3.71847371\n",
      "Iteration 7691, loss = 3.23696936\n",
      "Iteration 7692, loss = 3.64128890\n",
      "Iteration 7693, loss = 3.03528218\n",
      "Iteration 7694, loss = 2.47334919\n",
      "Iteration 7695, loss = 2.33890714\n",
      "Iteration 7696, loss = 3.05942758\n",
      "Iteration 7697, loss = 2.61670018\n",
      "Iteration 7698, loss = 2.61478321\n",
      "Iteration 7699, loss = 2.40679020\n",
      "Iteration 7700, loss = 2.81175610\n",
      "Iteration 7701, loss = 2.91034357\n",
      "Iteration 7702, loss = 2.37180197\n",
      "Iteration 7703, loss = 2.51478862\n",
      "Iteration 7704, loss = 2.69332692\n",
      "Iteration 7705, loss = 2.76292163\n",
      "Iteration 7706, loss = 2.43680400\n",
      "Iteration 7707, loss = 2.81553263\n",
      "Iteration 7708, loss = 2.63562431\n",
      "Iteration 7709, loss = 2.44552766\n",
      "Iteration 7710, loss = 2.34803232\n",
      "Iteration 7711, loss = 4.83702311\n",
      "Iteration 7712, loss = 3.64501768\n",
      "Iteration 7713, loss = 3.99159217\n",
      "Iteration 7714, loss = 3.44566827\n",
      "Iteration 7715, loss = 2.97583267\n",
      "Iteration 7716, loss = 4.73811541\n",
      "Iteration 7717, loss = 8.17263691\n",
      "Iteration 7718, loss = 8.32108023\n",
      "Iteration 7719, loss = 8.14878511\n",
      "Iteration 7720, loss = 4.71223106\n",
      "Iteration 7721, loss = 5.02456678\n",
      "Iteration 7722, loss = 4.69507280\n",
      "Iteration 7723, loss = 5.84867681\n",
      "Iteration 7724, loss = 3.79185248\n",
      "Iteration 7725, loss = 5.58120996\n",
      "Iteration 7726, loss = 4.75864481\n",
      "Iteration 7727, loss = 3.76422881\n",
      "Iteration 7728, loss = 3.90414647\n",
      "Iteration 7729, loss = 7.44843619\n",
      "Iteration 7730, loss = 6.30318651\n",
      "Iteration 7731, loss = 4.82662262\n",
      "Iteration 7732, loss = 4.04817716\n",
      "Iteration 7733, loss = 4.66399900\n",
      "Iteration 7734, loss = 2.95322880\n",
      "Iteration 7735, loss = 2.97170661\n",
      "Iteration 7736, loss = 2.81771620\n",
      "Iteration 7737, loss = 2.91302627\n",
      "Iteration 7738, loss = 3.24754063\n",
      "Iteration 7739, loss = 3.44932875\n",
      "Iteration 7740, loss = 3.35844882\n",
      "Iteration 7741, loss = 2.49974438\n",
      "Iteration 7742, loss = 3.39174852\n",
      "Iteration 7743, loss = 2.91353883\n",
      "Iteration 7744, loss = 3.53254517\n",
      "Iteration 7745, loss = 3.01044481\n",
      "Iteration 7746, loss = 2.82298715\n",
      "Iteration 7747, loss = 2.49005466\n",
      "Iteration 7748, loss = 2.72600331\n",
      "Iteration 7749, loss = 2.41778991\n",
      "Iteration 7750, loss = 2.33415354\n",
      "Iteration 7751, loss = 2.51184622\n",
      "Iteration 7752, loss = 2.73088210\n",
      "Iteration 7753, loss = 2.94341910\n",
      "Iteration 7754, loss = 2.70216530\n",
      "Iteration 7755, loss = 3.89812627\n",
      "Iteration 7756, loss = 5.37329515\n",
      "Iteration 7757, loss = 4.54290700\n",
      "Iteration 7758, loss = 3.87380904\n",
      "Iteration 7759, loss = 3.26593227\n",
      "Iteration 7760, loss = 3.18329290\n",
      "Iteration 7761, loss = 4.05241598\n",
      "Iteration 7762, loss = 4.02934964\n",
      "Iteration 7763, loss = 2.92793465\n",
      "Iteration 7764, loss = 3.21399721\n",
      "Iteration 7765, loss = 2.96123575\n",
      "Iteration 7766, loss = 7.27440472\n",
      "Iteration 7767, loss = 7.77644692\n",
      "Iteration 7768, loss = 5.09515051\n",
      "Iteration 7769, loss = 5.22901479\n",
      "Iteration 7770, loss = 4.58720929\n",
      "Iteration 7771, loss = 4.17854957\n",
      "Iteration 7772, loss = 5.32569334\n",
      "Iteration 7773, loss = 4.59698670\n",
      "Iteration 7774, loss = 3.19019925\n",
      "Iteration 7775, loss = 2.86276601\n",
      "Iteration 7776, loss = 2.43108195\n",
      "Iteration 7777, loss = 2.29222191\n",
      "Iteration 7778, loss = 2.31780193\n",
      "Iteration 7779, loss = 2.44271669\n",
      "Iteration 7780, loss = 2.49758536\n",
      "Iteration 7781, loss = 2.61827174\n",
      "Iteration 7782, loss = 2.96484202\n",
      "Iteration 7783, loss = 2.63303921\n",
      "Iteration 7784, loss = 2.91608672\n",
      "Iteration 7785, loss = 3.00859215\n",
      "Iteration 7786, loss = 2.73925236\n",
      "Iteration 7787, loss = 3.24104872\n",
      "Iteration 7788, loss = 2.52398349\n",
      "Iteration 7789, loss = 2.53821747\n",
      "Iteration 7790, loss = 2.46256842\n",
      "Iteration 7791, loss = 2.77790421\n",
      "Iteration 7792, loss = 2.40498745\n",
      "Iteration 7793, loss = 2.28145596\n",
      "Iteration 7794, loss = 2.35968752\n",
      "Iteration 7795, loss = 2.45897274\n",
      "Iteration 7796, loss = 2.36787663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7797, loss = 2.54755046\n",
      "Iteration 7798, loss = 2.58142148\n",
      "Iteration 7799, loss = 2.73741798\n",
      "Iteration 7800, loss = 2.44236576\n",
      "Iteration 7801, loss = 2.46395318\n",
      "Iteration 7802, loss = 2.38032155\n",
      "Iteration 7803, loss = 2.84753977\n",
      "Iteration 7804, loss = 2.58701069\n",
      "Iteration 7805, loss = 2.45512662\n",
      "Iteration 7806, loss = 2.27130077\n",
      "Iteration 7807, loss = 2.45804116\n",
      "Iteration 7808, loss = 2.56563585\n",
      "Iteration 7809, loss = 2.35369954\n",
      "Iteration 7810, loss = 2.40815149\n",
      "Iteration 7811, loss = 2.28051948\n",
      "Iteration 7812, loss = 2.48511327\n",
      "Iteration 7813, loss = 2.34807183\n",
      "Iteration 7814, loss = 2.36032129\n",
      "Iteration 7815, loss = 3.10773566\n",
      "Iteration 7816, loss = 2.73358999\n",
      "Iteration 7817, loss = 3.36741840\n",
      "Iteration 7818, loss = 4.61856667\n",
      "Iteration 7819, loss = 3.28088340\n",
      "Iteration 7820, loss = 3.21946587\n",
      "Iteration 7821, loss = 2.87215213\n",
      "Iteration 7822, loss = 2.93287952\n",
      "Iteration 7823, loss = 2.65954070\n",
      "Iteration 7824, loss = 2.73225529\n",
      "Iteration 7825, loss = 2.76069883\n",
      "Iteration 7826, loss = 2.55593859\n",
      "Iteration 7827, loss = 2.48078345\n",
      "Iteration 7828, loss = 2.99437125\n",
      "Iteration 7829, loss = 3.28746230\n",
      "Iteration 7830, loss = 3.03776197\n",
      "Iteration 7831, loss = 2.37104456\n",
      "Iteration 7832, loss = 2.41878712\n",
      "Iteration 7833, loss = 4.80832898\n",
      "Iteration 7834, loss = 3.50837554\n",
      "Iteration 7835, loss = 3.35834916\n",
      "Iteration 7836, loss = 3.15538736\n",
      "Iteration 7837, loss = 2.42881348\n",
      "Iteration 7838, loss = 2.66559282\n",
      "Iteration 7839, loss = 2.77060517\n",
      "Iteration 7840, loss = 2.69812885\n",
      "Iteration 7841, loss = 2.40759304\n",
      "Iteration 7842, loss = 2.34699212\n",
      "Iteration 7843, loss = 2.76809203\n",
      "Iteration 7844, loss = 3.37509606\n",
      "Iteration 7845, loss = 4.28336146\n",
      "Iteration 7846, loss = 3.77167226\n",
      "Iteration 7847, loss = 3.17393711\n",
      "Iteration 7848, loss = 3.21881442\n",
      "Iteration 7849, loss = 3.65564393\n",
      "Iteration 7850, loss = 3.00638280\n",
      "Iteration 7851, loss = 3.17473854\n",
      "Iteration 7852, loss = 2.76869312\n",
      "Iteration 7853, loss = 2.38551456\n",
      "Iteration 7854, loss = 2.20167175\n",
      "Iteration 7855, loss = 2.30411177\n",
      "Iteration 7856, loss = 2.40935610\n",
      "Iteration 7857, loss = 3.19062119\n",
      "Iteration 7858, loss = 3.20984979\n",
      "Iteration 7859, loss = 3.12195159\n",
      "Iteration 7860, loss = 2.50566634\n",
      "Iteration 7861, loss = 2.45209941\n",
      "Iteration 7862, loss = 2.25090994\n",
      "Iteration 7863, loss = 2.88591373\n",
      "Iteration 7864, loss = 4.02483534\n",
      "Iteration 7865, loss = 4.08041298\n",
      "Iteration 7866, loss = 4.99103660\n",
      "Iteration 7867, loss = 3.81319687\n",
      "Iteration 7868, loss = 3.49145653\n",
      "Iteration 7869, loss = 2.71851665\n",
      "Iteration 7870, loss = 2.47956112\n",
      "Iteration 7871, loss = 2.47892210\n",
      "Iteration 7872, loss = 3.23035263\n",
      "Iteration 7873, loss = 3.96379211\n",
      "Iteration 7874, loss = 3.26871899\n",
      "Iteration 7875, loss = 3.23445705\n",
      "Iteration 7876, loss = 2.52822754\n",
      "Iteration 7877, loss = 2.59708557\n",
      "Iteration 7878, loss = 3.93604133\n",
      "Iteration 7879, loss = 8.29935083\n",
      "Iteration 7880, loss = 7.36721683\n",
      "Iteration 7881, loss = 4.98124461\n",
      "Iteration 7882, loss = 2.89344837\n",
      "Iteration 7883, loss = 3.23974571\n",
      "Iteration 7884, loss = 3.39792193\n",
      "Iteration 7885, loss = 4.09700176\n",
      "Iteration 7886, loss = 3.71687143\n",
      "Iteration 7887, loss = 3.04104236\n",
      "Iteration 7888, loss = 2.77172848\n",
      "Iteration 7889, loss = 3.13917393\n",
      "Iteration 7890, loss = 3.32411873\n",
      "Iteration 7891, loss = 2.98203856\n",
      "Iteration 7892, loss = 2.84314545\n",
      "Iteration 7893, loss = 3.21380073\n",
      "Iteration 7894, loss = 3.05241267\n",
      "Iteration 7895, loss = 3.02929363\n",
      "Iteration 7896, loss = 2.93093159\n",
      "Iteration 7897, loss = 2.70474540\n",
      "Iteration 7898, loss = 2.66395302\n",
      "Iteration 7899, loss = 2.76255000\n",
      "Iteration 7900, loss = 2.09872524\n",
      "Iteration 7901, loss = 2.61718342\n",
      "Iteration 7902, loss = 2.86551940\n",
      "Iteration 7903, loss = 3.23163403\n",
      "Iteration 7904, loss = 2.80056540\n",
      "Iteration 7905, loss = 2.89441343\n",
      "Iteration 7906, loss = 3.50511660\n",
      "Iteration 7907, loss = 2.68106347\n",
      "Iteration 7908, loss = 2.90892820\n",
      "Iteration 7909, loss = 2.31972158\n",
      "Iteration 7910, loss = 2.26682483\n",
      "Iteration 7911, loss = 2.33795458\n",
      "Iteration 7912, loss = 2.30653291\n",
      "Iteration 7913, loss = 2.50723326\n",
      "Iteration 7914, loss = 2.51681481\n",
      "Iteration 7915, loss = 2.47710140\n",
      "Iteration 7916, loss = 2.18920179\n",
      "Iteration 7917, loss = 2.82003737\n",
      "Iteration 7918, loss = 2.39233220\n",
      "Iteration 7919, loss = 2.78102993\n",
      "Iteration 7920, loss = 2.27780492\n",
      "Iteration 7921, loss = 2.86668818\n",
      "Iteration 7922, loss = 2.73163907\n",
      "Iteration 7923, loss = 2.38773155\n",
      "Iteration 7924, loss = 2.34961001\n",
      "Iteration 7925, loss = 2.64492809\n",
      "Iteration 7926, loss = 2.57749507\n",
      "Iteration 7927, loss = 2.36271207\n",
      "Iteration 7928, loss = 2.31634572\n",
      "Iteration 7929, loss = 2.52999410\n",
      "Iteration 7930, loss = 2.18124436\n",
      "Iteration 7931, loss = 2.19001996\n",
      "Iteration 7932, loss = 2.17122746\n",
      "Iteration 7933, loss = 2.49082755\n",
      "Iteration 7934, loss = 2.69257692\n",
      "Iteration 7935, loss = 4.21327960\n",
      "Iteration 7936, loss = 4.37633749\n",
      "Iteration 7937, loss = 3.58339341\n",
      "Iteration 7938, loss = 3.20288947\n",
      "Iteration 7939, loss = 3.53818058\n",
      "Iteration 7940, loss = 3.90269772\n",
      "Iteration 7941, loss = 4.47202424\n",
      "Iteration 7942, loss = 3.51976578\n",
      "Iteration 7943, loss = 3.84999844\n",
      "Iteration 7944, loss = 2.55771752\n",
      "Iteration 7945, loss = 2.32112962\n",
      "Iteration 7946, loss = 2.51256930\n",
      "Iteration 7947, loss = 2.50455591\n",
      "Iteration 7948, loss = 2.55060410\n",
      "Iteration 7949, loss = 2.55454794\n",
      "Iteration 7950, loss = 2.58383464\n",
      "Iteration 7951, loss = 2.96392797\n",
      "Iteration 7952, loss = 6.84292588\n",
      "Iteration 7953, loss = 4.80966313\n",
      "Iteration 7954, loss = 3.99579651\n",
      "Iteration 7955, loss = 3.68883464\n",
      "Iteration 7956, loss = 3.20320574\n",
      "Iteration 7957, loss = 2.95623514\n",
      "Iteration 7958, loss = 2.38188384\n",
      "Iteration 7959, loss = 2.46445183\n",
      "Iteration 7960, loss = 2.89589456\n",
      "Iteration 7961, loss = 2.98249279\n",
      "Iteration 7962, loss = 2.61201950\n",
      "Iteration 7963, loss = 2.24071523\n",
      "Iteration 7964, loss = 2.21377643\n",
      "Iteration 7965, loss = 2.58219271\n",
      "Iteration 7966, loss = 2.38321828\n",
      "Iteration 7967, loss = 2.18321772\n",
      "Iteration 7968, loss = 2.39817527\n",
      "Iteration 7969, loss = 2.53747409\n",
      "Iteration 7970, loss = 3.80704886\n",
      "Iteration 7971, loss = 3.36068736\n",
      "Iteration 7972, loss = 2.77321973\n",
      "Iteration 7973, loss = 2.62311941\n",
      "Iteration 7974, loss = 2.28994878\n",
      "Iteration 7975, loss = 2.17480687\n",
      "Iteration 7976, loss = 2.17847138\n",
      "Iteration 7977, loss = 2.29506364\n",
      "Iteration 7978, loss = 2.27780304\n",
      "Iteration 7979, loss = 2.37853860\n",
      "Iteration 7980, loss = 2.04845320\n",
      "Iteration 7981, loss = 2.24965279\n",
      "Iteration 7982, loss = 2.44434582\n",
      "Iteration 7983, loss = 2.23746144\n",
      "Iteration 7984, loss = 2.73673413\n",
      "Iteration 7985, loss = 3.06801256\n",
      "Iteration 7986, loss = 2.89503672\n",
      "Iteration 7987, loss = 3.70783365\n",
      "Iteration 7988, loss = 3.41658704\n",
      "Iteration 7989, loss = 3.25081231\n",
      "Iteration 7990, loss = 2.39216873\n",
      "Iteration 7991, loss = 5.23557631\n",
      "Iteration 7992, loss = 4.74076648\n",
      "Iteration 7993, loss = 3.96117975\n",
      "Iteration 7994, loss = 5.95426809\n",
      "Iteration 7995, loss = 4.09808465\n",
      "Iteration 7996, loss = 2.82393453\n",
      "Iteration 7997, loss = 2.94585602\n",
      "Iteration 7998, loss = 3.31029792\n",
      "Iteration 7999, loss = 3.79518366\n",
      "Iteration 8000, loss = 3.06369165\n",
      "Iteration 8001, loss = 2.71339662\n",
      "Iteration 8002, loss = 3.51940721\n",
      "Iteration 8003, loss = 2.58785371\n",
      "Iteration 8004, loss = 3.47629629\n",
      "Iteration 8005, loss = 2.89558500\n",
      "Iteration 8006, loss = 2.32476472\n",
      "Iteration 8007, loss = 2.52319228\n",
      "Iteration 8008, loss = 3.41034960\n",
      "Iteration 8009, loss = 3.47924800\n",
      "Iteration 8010, loss = 2.52703744\n",
      "Iteration 8011, loss = 4.68880860\n",
      "Iteration 8012, loss = 3.14397709\n",
      "Iteration 8013, loss = 2.24941126\n",
      "Iteration 8014, loss = 3.38019283\n",
      "Iteration 8015, loss = 3.49571597\n",
      "Iteration 8016, loss = 2.94228369\n",
      "Iteration 8017, loss = 2.85905747\n",
      "Iteration 8018, loss = 2.51667848\n",
      "Iteration 8019, loss = 2.78439107\n",
      "Iteration 8020, loss = 3.66572042\n",
      "Iteration 8021, loss = 2.97978990\n",
      "Iteration 8022, loss = 2.48071764\n",
      "Iteration 8023, loss = 2.22755254\n",
      "Iteration 8024, loss = 2.79967772\n",
      "Iteration 8025, loss = 3.32595801\n",
      "Iteration 8026, loss = 4.46645268\n",
      "Iteration 8027, loss = 3.03067159\n",
      "Iteration 8028, loss = 2.54242497\n",
      "Iteration 8029, loss = 2.09767165\n",
      "Iteration 8030, loss = 2.18777278\n",
      "Iteration 8031, loss = 2.23250555\n",
      "Iteration 8032, loss = 2.16455804\n",
      "Iteration 8033, loss = 2.04545974\n",
      "Iteration 8034, loss = 2.06201855\n",
      "Iteration 8035, loss = 2.06059891\n",
      "Iteration 8036, loss = 4.35497310\n",
      "Iteration 8037, loss = 2.73597681\n",
      "Iteration 8038, loss = 4.52503477\n",
      "Iteration 8039, loss = 2.75986545\n",
      "Iteration 8040, loss = 2.48667548\n",
      "Iteration 8041, loss = 1.98911445\n",
      "Iteration 8042, loss = 2.03444240\n",
      "Iteration 8043, loss = 2.05605982\n",
      "Iteration 8044, loss = 1.91959206\n",
      "Iteration 8045, loss = 2.01215696\n",
      "Iteration 8046, loss = 2.33157860\n",
      "Iteration 8047, loss = 3.26933916\n",
      "Iteration 8048, loss = 3.35611154\n",
      "Iteration 8049, loss = 2.76486393\n",
      "Iteration 8050, loss = 2.84566265\n",
      "Iteration 8051, loss = 2.51989605\n",
      "Iteration 8052, loss = 3.18223819\n",
      "Iteration 8053, loss = 2.96467017\n",
      "Iteration 8054, loss = 2.31775278\n",
      "Iteration 8055, loss = 2.57056990\n",
      "Iteration 8056, loss = 2.29441288\n",
      "Iteration 8057, loss = 2.51259272\n",
      "Iteration 8058, loss = 2.64146203\n",
      "Iteration 8059, loss = 2.26623044\n",
      "Iteration 8060, loss = 2.64892506\n",
      "Iteration 8061, loss = 2.61980600\n",
      "Iteration 8062, loss = 3.94515027\n",
      "Iteration 8063, loss = 3.34860526\n",
      "Iteration 8064, loss = 2.95976802\n",
      "Iteration 8065, loss = 2.61565024\n",
      "Iteration 8066, loss = 2.49183777\n",
      "Iteration 8067, loss = 2.09674975\n",
      "Iteration 8068, loss = 2.72994352\n",
      "Iteration 8069, loss = 2.08614046\n",
      "Iteration 8070, loss = 2.14851019\n",
      "Iteration 8071, loss = 2.04259616\n",
      "Iteration 8072, loss = 2.26252822\n",
      "Iteration 8073, loss = 2.59411665\n",
      "Iteration 8074, loss = 2.85318833\n",
      "Iteration 8075, loss = 2.42969783\n",
      "Iteration 8076, loss = 2.20434547\n",
      "Iteration 8077, loss = 2.80866639\n",
      "Iteration 8078, loss = 2.64402367\n",
      "Iteration 8079, loss = 2.84234584\n",
      "Iteration 8080, loss = 2.29250210\n",
      "Iteration 8081, loss = 3.04472542\n",
      "Iteration 8082, loss = 3.31119173\n",
      "Iteration 8083, loss = 3.09223593\n",
      "Iteration 8084, loss = 2.49268811\n",
      "Iteration 8085, loss = 2.27603935\n",
      "Iteration 8086, loss = 2.28338958\n",
      "Iteration 8087, loss = 2.28131129\n",
      "Iteration 8088, loss = 1.91350554\n",
      "Iteration 8089, loss = 2.15587239\n",
      "Iteration 8090, loss = 1.99839575\n",
      "Iteration 8091, loss = 1.96927780\n",
      "Iteration 8092, loss = 2.29205865\n",
      "Iteration 8093, loss = 2.75763112\n",
      "Iteration 8094, loss = 2.75492630\n",
      "Iteration 8095, loss = 2.98130069\n",
      "Iteration 8096, loss = 2.74366946\n",
      "Iteration 8097, loss = 2.66769491\n",
      "Iteration 8098, loss = 2.56595533\n",
      "Iteration 8099, loss = 2.73702283\n",
      "Iteration 8100, loss = 4.01086482\n",
      "Iteration 8101, loss = 3.55249014\n",
      "Iteration 8102, loss = 2.67153518\n",
      "Iteration 8103, loss = 3.06342638\n",
      "Iteration 8104, loss = 2.48298098\n",
      "Iteration 8105, loss = 4.15405152\n",
      "Iteration 8106, loss = 3.65383000\n",
      "Iteration 8107, loss = 3.41567516\n",
      "Iteration 8108, loss = 2.68562095\n",
      "Iteration 8109, loss = 2.25756550\n",
      "Iteration 8110, loss = 2.26295692\n",
      "Iteration 8111, loss = 2.35868488\n",
      "Iteration 8112, loss = 3.23011525\n",
      "Iteration 8113, loss = 3.04106138\n",
      "Iteration 8114, loss = 2.96592961\n",
      "Iteration 8115, loss = 2.91340413\n",
      "Iteration 8116, loss = 2.59593612\n",
      "Iteration 8117, loss = 2.62725131\n",
      "Iteration 8118, loss = 3.71908216\n",
      "Iteration 8119, loss = 2.92068180\n",
      "Iteration 8120, loss = 3.05760020\n",
      "Iteration 8121, loss = 2.02892214\n",
      "Iteration 8122, loss = 1.94596126\n",
      "Iteration 8123, loss = 2.63474669\n",
      "Iteration 8124, loss = 2.27386098\n",
      "Iteration 8125, loss = 3.01399089\n",
      "Iteration 8126, loss = 2.97331367\n",
      "Iteration 8127, loss = 2.26314529\n",
      "Iteration 8128, loss = 2.30080657\n",
      "Iteration 8129, loss = 2.35982505\n",
      "Iteration 8130, loss = 2.22599295\n",
      "Iteration 8131, loss = 2.97127747\n",
      "Iteration 8132, loss = 2.57507675\n",
      "Iteration 8133, loss = 2.24100170\n",
      "Iteration 8134, loss = 3.68463542\n",
      "Iteration 8135, loss = 3.19635023\n",
      "Iteration 8136, loss = 2.23850754\n",
      "Iteration 8137, loss = 3.37041823\n",
      "Iteration 8138, loss = 2.63215356\n",
      "Iteration 8139, loss = 4.09480106\n",
      "Iteration 8140, loss = 3.78313122\n",
      "Iteration 8141, loss = 3.71048198\n",
      "Iteration 8142, loss = 2.07763732\n",
      "Iteration 8143, loss = 2.18337472\n",
      "Iteration 8144, loss = 2.32912005\n",
      "Iteration 8145, loss = 2.26698187\n",
      "Iteration 8146, loss = 1.95271978\n",
      "Iteration 8147, loss = 2.31002408\n",
      "Iteration 8148, loss = 2.84302284\n",
      "Iteration 8149, loss = 3.49204945\n",
      "Iteration 8150, loss = 2.44763964\n",
      "Iteration 8151, loss = 3.60565895\n",
      "Iteration 8152, loss = 2.99344725\n",
      "Iteration 8153, loss = 2.52980992\n",
      "Iteration 8154, loss = 1.93817895\n",
      "Iteration 8155, loss = 2.41383749\n",
      "Iteration 8156, loss = 2.49125900\n",
      "Iteration 8157, loss = 2.73911586\n",
      "Iteration 8158, loss = 2.57951338\n",
      "Iteration 8159, loss = 2.54713076\n",
      "Iteration 8160, loss = 3.10526654\n",
      "Iteration 8161, loss = 2.47957852\n",
      "Iteration 8162, loss = 2.76865704\n",
      "Iteration 8163, loss = 3.45649175\n",
      "Iteration 8164, loss = 2.92060866\n",
      "Iteration 8165, loss = 3.28812411\n",
      "Iteration 8166, loss = 2.75858740\n",
      "Iteration 8167, loss = 2.26653598\n",
      "Iteration 8168, loss = 2.66849776\n",
      "Iteration 8169, loss = 3.17260787\n",
      "Iteration 8170, loss = 2.26644841\n",
      "Iteration 8171, loss = 2.43514922\n",
      "Iteration 8172, loss = 2.68793934\n",
      "Iteration 8173, loss = 2.35983433\n",
      "Iteration 8174, loss = 3.49796327\n",
      "Iteration 8175, loss = 3.18058924\n",
      "Iteration 8176, loss = 4.56553596\n",
      "Iteration 8177, loss = 4.01624165\n",
      "Iteration 8178, loss = 3.31378432\n",
      "Iteration 8179, loss = 3.55804241\n",
      "Iteration 8180, loss = 2.90878253\n",
      "Iteration 8181, loss = 2.70155032\n",
      "Iteration 8182, loss = 2.40527360\n",
      "Iteration 8183, loss = 2.38734639\n",
      "Iteration 8184, loss = 2.29350402\n",
      "Iteration 8185, loss = 2.07033227\n",
      "Iteration 8186, loss = 2.40648198\n",
      "Iteration 8187, loss = 2.34982932\n",
      "Iteration 8188, loss = 2.18584911\n",
      "Iteration 8189, loss = 2.13845650\n",
      "Iteration 8190, loss = 2.27914217\n",
      "Iteration 8191, loss = 1.90482266\n",
      "Iteration 8192, loss = 2.47432189\n",
      "Iteration 8193, loss = 2.12600675\n",
      "Iteration 8194, loss = 2.14606493\n",
      "Iteration 8195, loss = 1.85153003\n",
      "Iteration 8196, loss = 1.85598686\n",
      "Iteration 8197, loss = 1.81737194\n",
      "Iteration 8198, loss = 2.07276346\n",
      "Iteration 8199, loss = 1.97533493\n",
      "Iteration 8200, loss = 2.06446610\n",
      "Iteration 8201, loss = 1.95605924\n",
      "Iteration 8202, loss = 2.86540930\n",
      "Iteration 8203, loss = 2.80704998\n",
      "Iteration 8204, loss = 2.48884274\n",
      "Iteration 8205, loss = 1.94616392\n",
      "Iteration 8206, loss = 2.23126637\n",
      "Iteration 8207, loss = 1.97159424\n",
      "Iteration 8208, loss = 1.77618455\n",
      "Iteration 8209, loss = 1.82111953\n",
      "Iteration 8210, loss = 1.89229968\n",
      "Iteration 8211, loss = 2.24580800\n",
      "Iteration 8212, loss = 1.91499355\n",
      "Iteration 8213, loss = 1.92397254\n",
      "Iteration 8214, loss = 1.79719354\n",
      "Iteration 8215, loss = 2.61905103\n",
      "Iteration 8216, loss = 3.77132726\n",
      "Iteration 8217, loss = 3.38548309\n",
      "Iteration 8218, loss = 3.17519034\n",
      "Iteration 8219, loss = 4.85034195\n",
      "Iteration 8220, loss = 3.53028296\n",
      "Iteration 8221, loss = 3.36315421\n",
      "Iteration 8222, loss = 2.16882145\n",
      "Iteration 8223, loss = 3.87085096\n",
      "Iteration 8224, loss = 3.29756457\n",
      "Iteration 8225, loss = 3.10766562\n",
      "Iteration 8226, loss = 2.73750090\n",
      "Iteration 8227, loss = 3.08958846\n",
      "Iteration 8228, loss = 2.78127032\n",
      "Iteration 8229, loss = 3.85926523\n",
      "Iteration 8230, loss = 3.93742783\n",
      "Iteration 8231, loss = 3.03186443\n",
      "Iteration 8232, loss = 2.71387055\n",
      "Iteration 8233, loss = 3.49560333\n",
      "Iteration 8234, loss = 2.88595129\n",
      "Iteration 8235, loss = 2.67696712\n",
      "Iteration 8236, loss = 2.42662739\n",
      "Iteration 8237, loss = 2.20966768\n",
      "Iteration 8238, loss = 3.88928397\n",
      "Iteration 8239, loss = 2.83418042\n",
      "Iteration 8240, loss = 2.70961255\n",
      "Iteration 8241, loss = 2.90283860\n",
      "Iteration 8242, loss = 3.39346531\n",
      "Iteration 8243, loss = 2.68236048\n",
      "Iteration 8244, loss = 2.46211540\n",
      "Iteration 8245, loss = 2.55212811\n",
      "Iteration 8246, loss = 3.12114543\n",
      "Iteration 8247, loss = 2.32290896\n",
      "Iteration 8248, loss = 4.58968336\n",
      "Iteration 8249, loss = 2.93386895\n",
      "Iteration 8250, loss = 2.83978922\n",
      "Iteration 8251, loss = 2.83024476\n",
      "Iteration 8252, loss = 2.37694518\n",
      "Iteration 8253, loss = 2.19093475\n",
      "Iteration 8254, loss = 3.27771644\n",
      "Iteration 8255, loss = 2.71147018\n",
      "Iteration 8256, loss = 2.82076745\n",
      "Iteration 8257, loss = 2.25149392\n",
      "Iteration 8258, loss = 2.52694447\n",
      "Iteration 8259, loss = 2.09290856\n",
      "Iteration 8260, loss = 2.02594838\n",
      "Iteration 8261, loss = 2.00264643\n",
      "Iteration 8262, loss = 3.41823267\n",
      "Iteration 8263, loss = 3.44577403\n",
      "Iteration 8264, loss = 3.80843203\n",
      "Iteration 8265, loss = 2.62729011\n",
      "Iteration 8266, loss = 2.54075576\n",
      "Iteration 8267, loss = 2.28009599\n",
      "Iteration 8268, loss = 2.23092344\n",
      "Iteration 8269, loss = 2.36101718\n",
      "Iteration 8270, loss = 2.70725822\n",
      "Iteration 8271, loss = 1.98755665\n",
      "Iteration 8272, loss = 2.01542020\n",
      "Iteration 8273, loss = 2.78666493\n",
      "Iteration 8274, loss = 2.78033438\n",
      "Iteration 8275, loss = 2.06779758\n",
      "Iteration 8276, loss = 5.11813362\n",
      "Iteration 8277, loss = 2.86418620\n",
      "Iteration 8278, loss = 2.82916879\n",
      "Iteration 8279, loss = 3.55880566\n",
      "Iteration 8280, loss = 2.76332719\n",
      "Iteration 8281, loss = 2.08040538\n",
      "Iteration 8282, loss = 2.34940204\n",
      "Iteration 8283, loss = 3.15812034\n",
      "Iteration 8284, loss = 3.47970543\n",
      "Iteration 8285, loss = 3.18698492\n",
      "Iteration 8286, loss = 2.63179590\n",
      "Iteration 8287, loss = 3.58465229\n",
      "Iteration 8288, loss = 3.44821093\n",
      "Iteration 8289, loss = 2.38950398\n",
      "Iteration 8290, loss = 1.96454255\n",
      "Iteration 8291, loss = 2.07428924\n",
      "Iteration 8292, loss = 2.79425084\n",
      "Iteration 8293, loss = 2.85247980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8294, loss = 2.22563160\n",
      "Iteration 8295, loss = 2.10583506\n",
      "Iteration 8296, loss = 1.94387647\n",
      "Iteration 8297, loss = 2.24104172\n",
      "Iteration 8298, loss = 1.89348454\n",
      "Iteration 8299, loss = 1.99708423\n",
      "Iteration 8300, loss = 2.26954397\n",
      "Iteration 8301, loss = 2.20884857\n",
      "Iteration 8302, loss = 3.47554695\n",
      "Iteration 8303, loss = 2.26491054\n",
      "Iteration 8304, loss = 2.08765967\n",
      "Iteration 8305, loss = 2.05264642\n",
      "Iteration 8306, loss = 2.87510912\n",
      "Iteration 8307, loss = 2.56688065\n",
      "Iteration 8308, loss = 2.16727982\n",
      "Iteration 8309, loss = 3.41546896\n",
      "Iteration 8310, loss = 2.73157833\n",
      "Iteration 8311, loss = 2.91848331\n",
      "Iteration 8312, loss = 2.18886519\n",
      "Iteration 8313, loss = 3.27505835\n",
      "Iteration 8314, loss = 6.03928379\n",
      "Iteration 8315, loss = 6.41102435\n",
      "Iteration 8316, loss = 4.23731793\n",
      "Iteration 8317, loss = 3.69964225\n",
      "Iteration 8318, loss = 2.60353664\n",
      "Iteration 8319, loss = 2.36658849\n",
      "Iteration 8320, loss = 3.71907208\n",
      "Iteration 8321, loss = 2.86122499\n",
      "Iteration 8322, loss = 3.11660747\n",
      "Iteration 8323, loss = 2.03032611\n",
      "Iteration 8324, loss = 2.18574542\n",
      "Iteration 8325, loss = 2.24469208\n",
      "Iteration 8326, loss = 2.28375234\n",
      "Iteration 8327, loss = 2.07249260\n",
      "Iteration 8328, loss = 2.16471220\n",
      "Iteration 8329, loss = 1.94263401\n",
      "Iteration 8330, loss = 2.77822554\n",
      "Iteration 8331, loss = 1.91788949\n",
      "Iteration 8332, loss = 2.32705986\n",
      "Iteration 8333, loss = 2.09022683\n",
      "Iteration 8334, loss = 2.03143189\n",
      "Iteration 8335, loss = 3.52426738\n",
      "Iteration 8336, loss = 2.23218381\n",
      "Iteration 8337, loss = 2.21232211\n",
      "Iteration 8338, loss = 1.86570142\n",
      "Iteration 8339, loss = 2.17021070\n",
      "Iteration 8340, loss = 2.39582654\n",
      "Iteration 8341, loss = 1.87883589\n",
      "Iteration 8342, loss = 1.98952304\n",
      "Iteration 8343, loss = 1.92917521\n",
      "Iteration 8344, loss = 2.57587781\n",
      "Iteration 8345, loss = 1.92561563\n",
      "Iteration 8346, loss = 2.21756351\n",
      "Iteration 8347, loss = 2.47159656\n",
      "Iteration 8348, loss = 2.54136621\n",
      "Iteration 8349, loss = 2.60437711\n",
      "Iteration 8350, loss = 2.45918055\n",
      "Iteration 8351, loss = 2.37061879\n",
      "Iteration 8352, loss = 2.65084234\n",
      "Iteration 8353, loss = 2.63987012\n",
      "Iteration 8354, loss = 2.05217863\n",
      "Iteration 8355, loss = 1.89423760\n",
      "Iteration 8356, loss = 1.81193418\n",
      "Iteration 8357, loss = 1.77890698\n",
      "Iteration 8358, loss = 1.91675725\n",
      "Iteration 8359, loss = 2.01043985\n",
      "Iteration 8360, loss = 2.17357400\n",
      "Iteration 8361, loss = 1.88242799\n",
      "Iteration 8362, loss = 4.26044460\n",
      "Iteration 8363, loss = 5.43570132\n",
      "Iteration 8364, loss = 4.75623955\n",
      "Iteration 8365, loss = 3.41233597\n",
      "Iteration 8366, loss = 3.12655107\n",
      "Iteration 8367, loss = 3.71295607\n",
      "Iteration 8368, loss = 2.49404322\n",
      "Iteration 8369, loss = 2.43976770\n",
      "Iteration 8370, loss = 2.14074773\n",
      "Iteration 8371, loss = 2.54666212\n",
      "Iteration 8372, loss = 3.38373929\n",
      "Iteration 8373, loss = 3.07717073\n",
      "Iteration 8374, loss = 2.34529953\n",
      "Iteration 8375, loss = 2.81975138\n",
      "Iteration 8376, loss = 2.03574308\n",
      "Iteration 8377, loss = 2.82963918\n",
      "Iteration 8378, loss = 3.48405281\n",
      "Iteration 8379, loss = 5.02782976\n",
      "Iteration 8380, loss = 4.26537857\n",
      "Iteration 8381, loss = 3.45049011\n",
      "Iteration 8382, loss = 2.74440038\n",
      "Iteration 8383, loss = 4.48470809\n",
      "Iteration 8384, loss = 4.21629620\n",
      "Iteration 8385, loss = 4.40084584\n",
      "Iteration 8386, loss = 4.86032653\n",
      "Iteration 8387, loss = 3.44643379\n",
      "Iteration 8388, loss = 3.17253739\n",
      "Iteration 8389, loss = 2.20124954\n",
      "Iteration 8390, loss = 2.42900960\n",
      "Iteration 8391, loss = 2.00332482\n",
      "Iteration 8392, loss = 1.85462766\n",
      "Iteration 8393, loss = 1.78777979\n",
      "Iteration 8394, loss = 1.74622586\n",
      "Iteration 8395, loss = 2.16164278\n",
      "Iteration 8396, loss = 1.93711342\n",
      "Iteration 8397, loss = 1.93016908\n",
      "Iteration 8398, loss = 2.43652588\n",
      "Iteration 8399, loss = 1.91980986\n",
      "Iteration 8400, loss = 2.24110499\n",
      "Iteration 8401, loss = 2.16883780\n",
      "Iteration 8402, loss = 2.48989774\n",
      "Iteration 8403, loss = 3.21118278\n",
      "Iteration 8404, loss = 4.46306323\n",
      "Iteration 8405, loss = 3.71639355\n",
      "Iteration 8406, loss = 3.16552705\n",
      "Iteration 8407, loss = 2.91162872\n",
      "Iteration 8408, loss = 3.43418202\n",
      "Iteration 8409, loss = 3.46212644\n",
      "Iteration 8410, loss = 2.79908819\n",
      "Iteration 8411, loss = 2.73078000\n",
      "Iteration 8412, loss = 2.53742191\n",
      "Iteration 8413, loss = 2.29591314\n",
      "Iteration 8414, loss = 2.21499977\n",
      "Iteration 8415, loss = 1.98417070\n",
      "Iteration 8416, loss = 2.11968194\n",
      "Iteration 8417, loss = 2.47203769\n",
      "Iteration 8418, loss = 2.02401549\n",
      "Iteration 8419, loss = 1.95502954\n",
      "Iteration 8420, loss = 2.19712457\n",
      "Iteration 8421, loss = 1.63837690\n",
      "Iteration 8422, loss = 1.88147842\n",
      "Iteration 8423, loss = 1.78336387\n",
      "Iteration 8424, loss = 1.74087870\n",
      "Iteration 8425, loss = 1.66343761\n",
      "Iteration 8426, loss = 1.95077858\n",
      "Iteration 8427, loss = 1.83957818\n",
      "Iteration 8428, loss = 1.80772212\n",
      "Iteration 8429, loss = 1.76964278\n",
      "Iteration 8430, loss = 2.01253800\n",
      "Iteration 8431, loss = 1.98343900\n",
      "Iteration 8432, loss = 2.50999187\n",
      "Iteration 8433, loss = 1.92927042\n",
      "Iteration 8434, loss = 2.10828362\n",
      "Iteration 8435, loss = 2.21250733\n",
      "Iteration 8436, loss = 1.94380101\n",
      "Iteration 8437, loss = 2.08824772\n",
      "Iteration 8438, loss = 1.75660651\n",
      "Iteration 8439, loss = 1.80399231\n",
      "Iteration 8440, loss = 1.82943402\n",
      "Iteration 8441, loss = 2.31576691\n",
      "Iteration 8442, loss = 2.67428794\n",
      "Iteration 8443, loss = 2.98679061\n",
      "Iteration 8444, loss = 3.41707701\n",
      "Iteration 8445, loss = 4.78080764\n",
      "Iteration 8446, loss = 4.21438793\n",
      "Iteration 8447, loss = 5.11807860\n",
      "Iteration 8448, loss = 11.48282704\n",
      "Iteration 8449, loss = 6.13300366\n",
      "Iteration 8450, loss = 9.84869023\n",
      "Iteration 8451, loss = 9.01291104\n",
      "Iteration 8452, loss = 7.09431623\n",
      "Iteration 8453, loss = 7.17214180\n",
      "Iteration 8454, loss = 4.52336811\n",
      "Iteration 8455, loss = 4.07303604\n",
      "Iteration 8456, loss = 3.76205742\n",
      "Iteration 8457, loss = 2.68041500\n",
      "Iteration 8458, loss = 2.35987618\n",
      "Iteration 8459, loss = 2.50073556\n",
      "Iteration 8460, loss = 2.31600545\n",
      "Iteration 8461, loss = 2.35331295\n",
      "Iteration 8462, loss = 2.32198564\n",
      "Iteration 8463, loss = 2.55224376\n",
      "Iteration 8464, loss = 3.55785936\n",
      "Iteration 8465, loss = 2.87549934\n",
      "Iteration 8466, loss = 2.72186017\n",
      "Iteration 8467, loss = 2.26226363\n",
      "Iteration 8468, loss = 3.25562837\n",
      "Iteration 8469, loss = 2.91107733\n",
      "Iteration 8470, loss = 2.46363916\n",
      "Iteration 8471, loss = 3.02913791\n",
      "Iteration 8472, loss = 2.90972158\n",
      "Iteration 8473, loss = 2.64920996\n",
      "Iteration 8474, loss = 2.67288100\n",
      "Iteration 8475, loss = 2.85378055\n",
      "Iteration 8476, loss = 2.38580742\n",
      "Iteration 8477, loss = 2.31868568\n",
      "Iteration 8478, loss = 2.19882848\n",
      "Iteration 8479, loss = 2.13741618\n",
      "Iteration 8480, loss = 2.64847890\n",
      "Iteration 8481, loss = 2.55940615\n",
      "Iteration 8482, loss = 2.96104496\n",
      "Iteration 8483, loss = 2.91218081\n",
      "Iteration 8484, loss = 2.63712040\n",
      "Iteration 8485, loss = 2.72815941\n",
      "Iteration 8486, loss = 2.41859055\n",
      "Iteration 8487, loss = 2.51581450\n",
      "Iteration 8488, loss = 2.67025962\n",
      "Iteration 8489, loss = 2.12803260\n",
      "Iteration 8490, loss = 3.07619422\n",
      "Iteration 8491, loss = 3.26351289\n",
      "Iteration 8492, loss = 3.08713978\n",
      "Iteration 8493, loss = 3.28758437\n",
      "Iteration 8494, loss = 2.98701175\n",
      "Iteration 8495, loss = 2.65015891\n",
      "Iteration 8496, loss = 2.22311499\n",
      "Iteration 8497, loss = 2.96259428\n",
      "Iteration 8498, loss = 2.71831272\n",
      "Iteration 8499, loss = 2.52609228\n",
      "Iteration 8500, loss = 2.12474841\n",
      "Iteration 8501, loss = 2.26763530\n",
      "Iteration 8502, loss = 2.25089923\n",
      "Iteration 8503, loss = 2.63482710\n",
      "Iteration 8504, loss = 2.70750575\n",
      "Iteration 8505, loss = 2.42373788\n",
      "Iteration 8506, loss = 2.24788055\n",
      "Iteration 8507, loss = 2.13214102\n",
      "Iteration 8508, loss = 2.10437065\n",
      "Iteration 8509, loss = 2.41651780\n",
      "Iteration 8510, loss = 2.28613511\n",
      "Iteration 8511, loss = 2.22483277\n",
      "Iteration 8512, loss = 3.01837154\n",
      "Iteration 8513, loss = 4.28104974\n",
      "Iteration 8514, loss = 3.06917785\n",
      "Iteration 8515, loss = 3.68293586\n",
      "Iteration 8516, loss = 3.53994768\n",
      "Iteration 8517, loss = 2.64299671\n",
      "Iteration 8518, loss = 2.29246762\n",
      "Iteration 8519, loss = 2.23745052\n",
      "Iteration 8520, loss = 2.23513866\n",
      "Iteration 8521, loss = 2.36919825\n",
      "Iteration 8522, loss = 2.05666395\n",
      "Iteration 8523, loss = 2.37728906\n",
      "Iteration 8524, loss = 2.26294763\n",
      "Iteration 8525, loss = 2.45364319\n",
      "Iteration 8526, loss = 3.19459419\n",
      "Iteration 8527, loss = 2.61986392\n",
      "Iteration 8528, loss = 2.67609442\n",
      "Iteration 8529, loss = 2.38508050\n",
      "Iteration 8530, loss = 4.35366095\n",
      "Iteration 8531, loss = 3.77692492\n",
      "Iteration 8532, loss = 3.45403234\n",
      "Iteration 8533, loss = 2.33534479\n",
      "Iteration 8534, loss = 2.38115042\n",
      "Iteration 8535, loss = 2.97330283\n",
      "Iteration 8536, loss = 2.33985454\n",
      "Iteration 8537, loss = 2.35222202\n",
      "Iteration 8538, loss = 2.49546937\n",
      "Iteration 8539, loss = 2.57663458\n",
      "Iteration 8540, loss = 2.25886626\n",
      "Iteration 8541, loss = 2.76282868\n",
      "Iteration 8542, loss = 2.26779636\n",
      "Iteration 8543, loss = 2.39185233\n",
      "Iteration 8544, loss = 2.24692183\n",
      "Iteration 8545, loss = 2.58400607\n",
      "Iteration 8546, loss = 2.10598735\n",
      "Iteration 8547, loss = 2.15168368\n",
      "Iteration 8548, loss = 2.00115975\n",
      "Iteration 8549, loss = 1.93123679\n",
      "Iteration 8550, loss = 2.03854999\n",
      "Iteration 8551, loss = 2.13874490\n",
      "Iteration 8552, loss = 2.20059726\n",
      "Iteration 8553, loss = 2.07904882\n",
      "Iteration 8554, loss = 2.03328868\n",
      "Iteration 8555, loss = 1.86244691\n",
      "Iteration 8556, loss = 2.25570312\n",
      "Iteration 8557, loss = 2.21733255\n",
      "Iteration 8558, loss = 2.04840681\n",
      "Iteration 8559, loss = 2.03899959\n",
      "Iteration 8560, loss = 2.35602191\n",
      "Iteration 8561, loss = 2.48851837\n",
      "Iteration 8562, loss = 2.45747546\n",
      "Iteration 8563, loss = 2.57378309\n",
      "Iteration 8564, loss = 4.03942160\n",
      "Iteration 8565, loss = 3.17960568\n",
      "Iteration 8566, loss = 3.18277160\n",
      "Iteration 8567, loss = 3.76665026\n",
      "Iteration 8568, loss = 2.86521615\n",
      "Iteration 8569, loss = 2.86465597\n",
      "Iteration 8570, loss = 2.08244845\n",
      "Iteration 8571, loss = 2.29584191\n",
      "Iteration 8572, loss = 2.14045618\n",
      "Iteration 8573, loss = 2.40375135\n",
      "Iteration 8574, loss = 2.56256449\n",
      "Iteration 8575, loss = 2.72962097\n",
      "Iteration 8576, loss = 3.02852157\n",
      "Iteration 8577, loss = 4.12301581\n",
      "Iteration 8578, loss = 3.38569883\n",
      "Iteration 8579, loss = 2.76631661\n",
      "Iteration 8580, loss = 2.32543070\n",
      "Iteration 8581, loss = 2.34771917\n",
      "Iteration 8582, loss = 1.99985273\n",
      "Iteration 8583, loss = 1.96513140\n",
      "Iteration 8584, loss = 2.11244046\n",
      "Iteration 8585, loss = 2.19877805\n",
      "Iteration 8586, loss = 2.83620957\n",
      "Iteration 8587, loss = 2.30120425\n",
      "Iteration 8588, loss = 2.14870857\n",
      "Iteration 8589, loss = 2.48129743\n",
      "Iteration 8590, loss = 2.29908833\n",
      "Iteration 8591, loss = 2.51677257\n",
      "Iteration 8592, loss = 2.28437756\n",
      "Iteration 8593, loss = 2.71046559\n",
      "Iteration 8594, loss = 2.91052533\n",
      "Iteration 8595, loss = 2.20925297\n",
      "Iteration 8596, loss = 2.83072119\n",
      "Iteration 8597, loss = 3.60015537\n",
      "Iteration 8598, loss = 2.85578340\n",
      "Iteration 8599, loss = 2.40218871\n",
      "Iteration 8600, loss = 2.28517175\n",
      "Iteration 8601, loss = 2.92418280\n",
      "Iteration 8602, loss = 2.61341580\n",
      "Iteration 8603, loss = 3.13058272\n",
      "Iteration 8604, loss = 3.43489920\n",
      "Iteration 8605, loss = 2.88117220\n",
      "Iteration 8606, loss = 2.62502844\n",
      "Iteration 8607, loss = 2.93611245\n",
      "Iteration 8608, loss = 5.73905140\n",
      "Iteration 8609, loss = 5.72849466\n",
      "Iteration 8610, loss = 2.31452251\n",
      "Iteration 8611, loss = 2.12594389\n",
      "Iteration 8612, loss = 2.45381450\n",
      "Iteration 8613, loss = 2.25197361\n",
      "Iteration 8614, loss = 4.83874859\n",
      "Iteration 8615, loss = 4.36074194\n",
      "Iteration 8616, loss = 2.87620081\n",
      "Iteration 8617, loss = 5.98974621\n",
      "Iteration 8618, loss = 4.32513962\n",
      "Iteration 8619, loss = 3.99618554\n",
      "Iteration 8620, loss = 3.65960896\n",
      "Iteration 8621, loss = 2.62441006\n",
      "Iteration 8622, loss = 2.11351928\n",
      "Iteration 8623, loss = 1.92718907\n",
      "Iteration 8624, loss = 2.19295125\n",
      "Iteration 8625, loss = 2.13938487\n",
      "Iteration 8626, loss = 2.30458509\n",
      "Iteration 8627, loss = 1.86791853\n",
      "Iteration 8628, loss = 2.50154292\n",
      "Iteration 8629, loss = 2.63075146\n",
      "Iteration 8630, loss = 2.24684724\n",
      "Iteration 8631, loss = 2.47907285\n",
      "Iteration 8632, loss = 2.30264215\n",
      "Iteration 8633, loss = 2.50531251\n",
      "Iteration 8634, loss = 2.11546837\n",
      "Iteration 8635, loss = 2.05698058\n",
      "Iteration 8636, loss = 2.02168702\n",
      "Iteration 8637, loss = 2.30887553\n",
      "Iteration 8638, loss = 2.42269042\n",
      "Iteration 8639, loss = 3.29596394\n",
      "Iteration 8640, loss = 2.91821026\n",
      "Iteration 8641, loss = 3.20027886\n",
      "Iteration 8642, loss = 4.64238005\n",
      "Iteration 8643, loss = 3.25121053\n",
      "Iteration 8644, loss = 4.06120802\n",
      "Iteration 8645, loss = 3.00458596\n",
      "Iteration 8646, loss = 2.33585101\n",
      "Iteration 8647, loss = 2.17773743\n",
      "Iteration 8648, loss = 1.98432320\n",
      "Iteration 8649, loss = 2.14368326\n",
      "Iteration 8650, loss = 2.43089777\n",
      "Iteration 8651, loss = 2.34823466\n",
      "Iteration 8652, loss = 2.44461289\n",
      "Iteration 8653, loss = 2.37932760\n",
      "Iteration 8654, loss = 1.98513145\n",
      "Iteration 8655, loss = 2.38639269\n",
      "Iteration 8656, loss = 2.18216093\n",
      "Iteration 8657, loss = 2.13587849\n",
      "Iteration 8658, loss = 2.78478739\n",
      "Iteration 8659, loss = 3.49093446\n",
      "Iteration 8660, loss = 3.88538289\n",
      "Iteration 8661, loss = 7.87581898\n",
      "Iteration 8662, loss = 4.10443419\n",
      "Iteration 8663, loss = 6.28637851\n",
      "Iteration 8664, loss = 5.44278131\n",
      "Iteration 8665, loss = 3.76152149\n",
      "Iteration 8666, loss = 3.78024319\n",
      "Iteration 8667, loss = 3.59479587\n",
      "Iteration 8668, loss = 2.94890407\n",
      "Iteration 8669, loss = 2.89061474\n",
      "Iteration 8670, loss = 2.28053746\n",
      "Iteration 8671, loss = 2.45190621\n",
      "Iteration 8672, loss = 2.16927497\n",
      "Iteration 8673, loss = 2.22709290\n",
      "Iteration 8674, loss = 1.98919662\n",
      "Iteration 8675, loss = 1.93441504\n",
      "Iteration 8676, loss = 1.86835458\n",
      "Iteration 8677, loss = 2.15567447\n",
      "Iteration 8678, loss = 2.40727358\n",
      "Iteration 8679, loss = 2.43764581\n",
      "Iteration 8680, loss = 3.64743657\n",
      "Iteration 8681, loss = 3.07891673\n",
      "Iteration 8682, loss = 2.76505914\n",
      "Iteration 8683, loss = 2.30579195\n",
      "Iteration 8684, loss = 1.91898294\n",
      "Iteration 8685, loss = 2.65364317\n",
      "Iteration 8686, loss = 3.68397565\n",
      "Iteration 8687, loss = 3.19145140\n",
      "Iteration 8688, loss = 3.09752925\n",
      "Iteration 8689, loss = 2.39819537\n",
      "Iteration 8690, loss = 2.61546413\n",
      "Iteration 8691, loss = 2.48603824\n",
      "Iteration 8692, loss = 5.11063136\n",
      "Iteration 8693, loss = 5.06373715\n",
      "Iteration 8694, loss = 3.49895120\n",
      "Iteration 8695, loss = 3.11259309\n",
      "Iteration 8696, loss = 3.10819439\n",
      "Iteration 8697, loss = 4.15796631\n",
      "Iteration 8698, loss = 2.81611477\n",
      "Iteration 8699, loss = 2.28514783\n",
      "Iteration 8700, loss = 3.41375011\n",
      "Iteration 8701, loss = 2.47686918\n",
      "Iteration 8702, loss = 2.55702224\n",
      "Iteration 8703, loss = 2.43982424\n",
      "Iteration 8704, loss = 1.99763194\n",
      "Iteration 8705, loss = 2.27527882\n",
      "Iteration 8706, loss = 2.14457470\n",
      "Iteration 8707, loss = 1.90924148\n",
      "Iteration 8708, loss = 2.36299636\n",
      "Iteration 8709, loss = 3.91231687\n",
      "Iteration 8710, loss = 2.88372858\n",
      "Iteration 8711, loss = 3.15484195\n",
      "Iteration 8712, loss = 2.51648962\n",
      "Iteration 8713, loss = 2.24380995\n",
      "Iteration 8714, loss = 2.35287843\n",
      "Iteration 8715, loss = 2.12964481\n",
      "Iteration 8716, loss = 1.92568728\n",
      "Iteration 8717, loss = 2.63167348\n",
      "Iteration 8718, loss = 2.38484688\n",
      "Iteration 8719, loss = 2.34321931\n",
      "Iteration 8720, loss = 2.59287621\n",
      "Iteration 8721, loss = 2.47665210\n",
      "Iteration 8722, loss = 2.51223012\n",
      "Iteration 8723, loss = 1.97400371\n",
      "Iteration 8724, loss = 1.89727195\n",
      "Iteration 8725, loss = 1.83469751\n",
      "Iteration 8726, loss = 1.89549153\n",
      "Iteration 8727, loss = 1.76646181\n",
      "Iteration 8728, loss = 1.91514210\n",
      "Iteration 8729, loss = 1.89632658\n",
      "Iteration 8730, loss = 4.45237343\n",
      "Iteration 8731, loss = 3.64811992\n",
      "Iteration 8732, loss = 3.77216053\n",
      "Iteration 8733, loss = 2.42383592\n",
      "Iteration 8734, loss = 2.27712648\n",
      "Iteration 8735, loss = 2.62607608\n",
      "Iteration 8736, loss = 2.25347527\n",
      "Iteration 8737, loss = 2.00629630\n",
      "Iteration 8738, loss = 2.58768812\n",
      "Iteration 8739, loss = 2.33101142\n",
      "Iteration 8740, loss = 3.10233698\n",
      "Iteration 8741, loss = 2.29888925\n",
      "Iteration 8742, loss = 4.04404036\n",
      "Iteration 8743, loss = 3.98642827\n",
      "Iteration 8744, loss = 3.36414535\n",
      "Iteration 8745, loss = 2.85801189\n",
      "Iteration 8746, loss = 2.50515031\n",
      "Iteration 8747, loss = 2.37012838\n",
      "Iteration 8748, loss = 2.20667821\n",
      "Iteration 8749, loss = 2.14102512\n",
      "Iteration 8750, loss = 2.90901329\n",
      "Iteration 8751, loss = 2.65357511\n",
      "Iteration 8752, loss = 3.22386152\n",
      "Iteration 8753, loss = 3.79368176\n",
      "Iteration 8754, loss = 3.09430480\n",
      "Iteration 8755, loss = 5.28393168\n",
      "Iteration 8756, loss = 3.45641104\n",
      "Iteration 8757, loss = 2.15201760\n",
      "Iteration 8758, loss = 2.97709037\n",
      "Iteration 8759, loss = 2.66456478\n",
      "Iteration 8760, loss = 2.60936208\n",
      "Iteration 8761, loss = 3.14554769\n",
      "Iteration 8762, loss = 2.89164045\n",
      "Iteration 8763, loss = 2.93697752\n",
      "Iteration 8764, loss = 3.09982852\n",
      "Iteration 8765, loss = 2.79835044\n",
      "Iteration 8766, loss = 2.53861068\n",
      "Iteration 8767, loss = 4.19964050\n",
      "Iteration 8768, loss = 3.19743185\n",
      "Iteration 8769, loss = 2.02569866\n",
      "Iteration 8770, loss = 2.09647896\n",
      "Iteration 8771, loss = 2.26434231\n",
      "Iteration 8772, loss = 2.66297305\n",
      "Iteration 8773, loss = 2.38664831\n",
      "Iteration 8774, loss = 2.04390989\n",
      "Iteration 8775, loss = 2.04897847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8776, loss = 2.24275240\n",
      "Iteration 8777, loss = 2.07911437\n",
      "Iteration 8778, loss = 2.28422781\n",
      "Iteration 8779, loss = 1.96220937\n",
      "Iteration 8780, loss = 2.27396360\n",
      "Iteration 8781, loss = 2.10992445\n",
      "Iteration 8782, loss = 2.17103750\n",
      "Iteration 8783, loss = 2.54050192\n",
      "Iteration 8784, loss = 2.00619601\n",
      "Iteration 8785, loss = 2.02142518\n",
      "Iteration 8786, loss = 2.06122173\n",
      "Iteration 8787, loss = 2.18651641\n",
      "Iteration 8788, loss = 2.79697254\n",
      "Iteration 8789, loss = 3.42862579\n",
      "Iteration 8790, loss = 2.77561110\n",
      "Iteration 8791, loss = 4.74048745\n",
      "Iteration 8792, loss = 3.30291590\n",
      "Iteration 8793, loss = 4.01069625\n",
      "Iteration 8794, loss = 3.54281191\n",
      "Iteration 8795, loss = 2.91464530\n",
      "Iteration 8796, loss = 2.85675213\n",
      "Iteration 8797, loss = 2.62935892\n",
      "Iteration 8798, loss = 2.26310115\n",
      "Iteration 8799, loss = 2.31111910\n",
      "Iteration 8800, loss = 2.13939615\n",
      "Iteration 8801, loss = 2.15982243\n",
      "Iteration 8802, loss = 1.90057290\n",
      "Iteration 8803, loss = 1.78274931\n",
      "Iteration 8804, loss = 2.69698978\n",
      "Iteration 8805, loss = 3.61214401\n",
      "Iteration 8806, loss = 2.51964663\n",
      "Iteration 8807, loss = 1.93933162\n",
      "Iteration 8808, loss = 2.62373060\n",
      "Iteration 8809, loss = 1.84134282\n",
      "Iteration 8810, loss = 2.55707443\n",
      "Iteration 8811, loss = 3.02717623\n",
      "Iteration 8812, loss = 2.46675327\n",
      "Iteration 8813, loss = 2.76190446\n",
      "Iteration 8814, loss = 2.51299229\n",
      "Iteration 8815, loss = 2.08827021\n",
      "Iteration 8816, loss = 2.55422149\n",
      "Iteration 8817, loss = 3.16962299\n",
      "Iteration 8818, loss = 2.22333338\n",
      "Iteration 8819, loss = 2.91701101\n",
      "Iteration 8820, loss = 5.70782962\n",
      "Iteration 8821, loss = 3.66293291\n",
      "Iteration 8822, loss = 2.74775833\n",
      "Iteration 8823, loss = 2.82004337\n",
      "Iteration 8824, loss = 3.30144635\n",
      "Iteration 8825, loss = 2.64310660\n",
      "Iteration 8826, loss = 2.67199745\n",
      "Iteration 8827, loss = 2.50726901\n",
      "Iteration 8828, loss = 2.14009680\n",
      "Iteration 8829, loss = 2.90909451\n",
      "Iteration 8830, loss = 2.41624477\n",
      "Iteration 8831, loss = 1.85127933\n",
      "Iteration 8832, loss = 1.92325732\n",
      "Iteration 8833, loss = 1.87251863\n",
      "Iteration 8834, loss = 2.00148831\n",
      "Iteration 8835, loss = 1.96938318\n",
      "Iteration 8836, loss = 2.60142667\n",
      "Iteration 8837, loss = 4.06993958\n",
      "Iteration 8838, loss = 3.07764136\n",
      "Iteration 8839, loss = 2.92858970\n",
      "Iteration 8840, loss = 2.92852254\n",
      "Iteration 8841, loss = 3.08677602\n",
      "Iteration 8842, loss = 2.78373721\n",
      "Iteration 8843, loss = 2.30049799\n",
      "Iteration 8844, loss = 4.70762878\n",
      "Iteration 8845, loss = 4.10750503\n",
      "Iteration 8846, loss = 3.11004583\n",
      "Iteration 8847, loss = 2.35048460\n",
      "Iteration 8848, loss = 2.13020063\n",
      "Iteration 8849, loss = 2.86706570\n",
      "Iteration 8850, loss = 2.16108650\n",
      "Iteration 8851, loss = 1.96840624\n",
      "Iteration 8852, loss = 1.74785553\n",
      "Iteration 8853, loss = 1.82589692\n",
      "Iteration 8854, loss = 1.74550525\n",
      "Iteration 8855, loss = 1.90120074\n",
      "Iteration 8856, loss = 2.29206986\n",
      "Iteration 8857, loss = 2.01407395\n",
      "Iteration 8858, loss = 1.89489544\n",
      "Iteration 8859, loss = 2.45432404\n",
      "Iteration 8860, loss = 1.97039336\n",
      "Iteration 8861, loss = 2.01698547\n",
      "Iteration 8862, loss = 2.55705442\n",
      "Iteration 8863, loss = 2.26082714\n",
      "Iteration 8864, loss = 1.76144312\n",
      "Iteration 8865, loss = 2.26399247\n",
      "Iteration 8866, loss = 2.93811105\n",
      "Iteration 8867, loss = 2.10737143\n",
      "Iteration 8868, loss = 2.32832124\n",
      "Iteration 8869, loss = 1.97685837\n",
      "Iteration 8870, loss = 2.58175012\n",
      "Iteration 8871, loss = 3.05296184\n",
      "Iteration 8872, loss = 2.64300493\n",
      "Iteration 8873, loss = 2.45628492\n",
      "Iteration 8874, loss = 1.80257317\n",
      "Iteration 8875, loss = 2.02012429\n",
      "Iteration 8876, loss = 2.24671717\n",
      "Iteration 8877, loss = 2.20100519\n",
      "Iteration 8878, loss = 2.15869972\n",
      "Iteration 8879, loss = 1.91534747\n",
      "Iteration 8880, loss = 1.72771816\n",
      "Iteration 8881, loss = 2.09237583\n",
      "Iteration 8882, loss = 1.93807107\n",
      "Iteration 8883, loss = 2.13938798\n",
      "Iteration 8884, loss = 1.99724197\n",
      "Iteration 8885, loss = 1.91851668\n",
      "Iteration 8886, loss = 1.86406805\n",
      "Iteration 8887, loss = 2.24091746\n",
      "Iteration 8888, loss = 1.89863510\n",
      "Iteration 8889, loss = 4.51954844\n",
      "Iteration 8890, loss = 3.13653802\n",
      "Iteration 8891, loss = 2.19452944\n",
      "Iteration 8892, loss = 2.38555804\n",
      "Iteration 8893, loss = 2.11902487\n",
      "Iteration 8894, loss = 2.12503876\n",
      "Iteration 8895, loss = 2.12218929\n",
      "Iteration 8896, loss = 1.98788444\n",
      "Iteration 8897, loss = 1.83937393\n",
      "Iteration 8898, loss = 2.25148864\n",
      "Iteration 8899, loss = 2.23982760\n",
      "Iteration 8900, loss = 2.98487814\n",
      "Iteration 8901, loss = 3.14864734\n",
      "Iteration 8902, loss = 2.40817906\n",
      "Iteration 8903, loss = 1.92018656\n",
      "Iteration 8904, loss = 2.10660238\n",
      "Iteration 8905, loss = 2.46960473\n",
      "Iteration 8906, loss = 2.08571229\n",
      "Iteration 8907, loss = 2.77897227\n",
      "Iteration 8908, loss = 2.53128598\n",
      "Iteration 8909, loss = 4.26429601\n",
      "Iteration 8910, loss = 2.97527075\n",
      "Iteration 8911, loss = 4.58232688\n",
      "Iteration 8912, loss = 3.87519062\n",
      "Iteration 8913, loss = 3.29004364\n",
      "Iteration 8914, loss = 3.27293725\n",
      "Iteration 8915, loss = 1.97461830\n",
      "Iteration 8916, loss = 2.52433352\n",
      "Iteration 8917, loss = 2.04000579\n",
      "Iteration 8918, loss = 2.19272297\n",
      "Iteration 8919, loss = 1.68333189\n",
      "Iteration 8920, loss = 1.91463939\n",
      "Iteration 8921, loss = 2.77728788\n",
      "Iteration 8922, loss = 2.17566646\n",
      "Iteration 8923, loss = 2.49382325\n",
      "Iteration 8924, loss = 2.44795911\n",
      "Iteration 8925, loss = 2.84771280\n",
      "Iteration 8926, loss = 1.96043913\n",
      "Iteration 8927, loss = 2.11339652\n",
      "Iteration 8928, loss = 2.10047271\n",
      "Iteration 8929, loss = 1.91741252\n",
      "Iteration 8930, loss = 1.82296272\n",
      "Iteration 8931, loss = 2.48176162\n",
      "Iteration 8932, loss = 3.31243275\n",
      "Iteration 8933, loss = 2.60338695\n",
      "Iteration 8934, loss = 3.99033981\n",
      "Iteration 8935, loss = 3.35176624\n",
      "Iteration 8936, loss = 2.94371048\n",
      "Iteration 8937, loss = 2.37218053\n",
      "Iteration 8938, loss = 3.31435734\n",
      "Iteration 8939, loss = 4.58092522\n",
      "Iteration 8940, loss = 4.79380555\n",
      "Iteration 8941, loss = 5.94237592\n",
      "Iteration 8942, loss = 2.81606107\n",
      "Iteration 8943, loss = 2.19926950\n",
      "Iteration 8944, loss = 2.66415187\n",
      "Iteration 8945, loss = 2.88285711\n",
      "Iteration 8946, loss = 2.28487144\n",
      "Iteration 8947, loss = 2.03846935\n",
      "Iteration 8948, loss = 1.76379031\n",
      "Iteration 8949, loss = 1.76889092\n",
      "Iteration 8950, loss = 2.68925296\n",
      "Iteration 8951, loss = 2.35444878\n",
      "Iteration 8952, loss = 1.93641475\n",
      "Iteration 8953, loss = 1.76290887\n",
      "Iteration 8954, loss = 2.08598065\n",
      "Iteration 8955, loss = 2.93369788\n",
      "Iteration 8956, loss = 2.97438583\n",
      "Iteration 8957, loss = 2.75724542\n",
      "Iteration 8958, loss = 2.10843904\n",
      "Iteration 8959, loss = 1.91021768\n",
      "Iteration 8960, loss = 2.01915065\n",
      "Iteration 8961, loss = 1.86064813\n",
      "Iteration 8962, loss = 1.96895589\n",
      "Iteration 8963, loss = 2.19395959\n",
      "Iteration 8964, loss = 1.84144437\n",
      "Iteration 8965, loss = 2.13037503\n",
      "Iteration 8966, loss = 2.11022224\n",
      "Iteration 8967, loss = 1.97280693\n",
      "Iteration 8968, loss = 1.77128728\n",
      "Iteration 8969, loss = 1.67294069\n",
      "Iteration 8970, loss = 1.96523007\n",
      "Iteration 8971, loss = 1.73559597\n",
      "Iteration 8972, loss = 1.88407585\n",
      "Iteration 8973, loss = 1.93459479\n",
      "Iteration 8974, loss = 1.86272818\n",
      "Iteration 8975, loss = 1.65111804\n",
      "Iteration 8976, loss = 1.68016364\n",
      "Iteration 8977, loss = 1.74305360\n",
      "Iteration 8978, loss = 1.71731991\n",
      "Iteration 8979, loss = 1.75156975\n",
      "Iteration 8980, loss = 1.88089595\n",
      "Iteration 8981, loss = 2.18263438\n",
      "Iteration 8982, loss = 2.61835722\n",
      "Iteration 8983, loss = 2.16610981\n",
      "Iteration 8984, loss = 1.67791001\n",
      "Iteration 8985, loss = 2.78149049\n",
      "Iteration 8986, loss = 3.52343867\n",
      "Iteration 8987, loss = 3.22510893\n",
      "Iteration 8988, loss = 2.58172081\n",
      "Iteration 8989, loss = 2.01043801\n",
      "Iteration 8990, loss = 1.99017539\n",
      "Iteration 8991, loss = 2.28827320\n",
      "Iteration 8992, loss = 2.02325895\n",
      "Iteration 8993, loss = 2.57479067\n",
      "Iteration 8994, loss = 2.18939935\n",
      "Iteration 8995, loss = 2.22082940\n",
      "Iteration 8996, loss = 2.08003463\n",
      "Iteration 8997, loss = 1.94151018\n",
      "Iteration 8998, loss = 1.96230822\n",
      "Iteration 8999, loss = 1.87966539\n",
      "Iteration 9000, loss = 2.23564872\n",
      "Iteration 9001, loss = 1.77884235\n",
      "Iteration 9002, loss = 1.64219574\n",
      "Iteration 9003, loss = 2.06486110\n",
      "Iteration 9004, loss = 4.27025477\n",
      "Iteration 9005, loss = 4.72472908\n",
      "Iteration 9006, loss = 5.17354096\n",
      "Iteration 9007, loss = 2.00479371\n",
      "Iteration 9008, loss = 1.63153862\n",
      "Iteration 9009, loss = 2.11497542\n",
      "Iteration 9010, loss = 1.87705414\n",
      "Iteration 9011, loss = 1.77274312\n",
      "Iteration 9012, loss = 1.93927954\n",
      "Iteration 9013, loss = 1.93141882\n",
      "Iteration 9014, loss = 1.74658453\n",
      "Iteration 9015, loss = 1.70548867\n",
      "Iteration 9016, loss = 2.01187419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9017, loss = 1.96692893\n",
      "Iteration 9018, loss = 2.42506754\n",
      "Iteration 9019, loss = 2.89926110\n",
      "Iteration 9020, loss = 3.22174672\n",
      "Iteration 9021, loss = 2.95842190\n",
      "Iteration 9022, loss = 4.17816138\n",
      "Iteration 9023, loss = 2.94659606\n",
      "Iteration 9024, loss = 2.71617944\n",
      "Iteration 9025, loss = 2.59255722\n",
      "Iteration 9026, loss = 2.38319007\n",
      "Iteration 9027, loss = 2.53766085\n",
      "Iteration 9028, loss = 2.38312395\n",
      "Iteration 9029, loss = 1.89137208\n",
      "Iteration 9030, loss = 2.07092376\n",
      "Iteration 9031, loss = 3.07857422\n",
      "Iteration 9032, loss = 2.99172168\n",
      "Iteration 9033, loss = 2.39086142\n",
      "Iteration 9034, loss = 1.62716421\n",
      "Iteration 9035, loss = 1.68148429\n",
      "Iteration 9036, loss = 2.26775780\n",
      "Iteration 9037, loss = 1.82713037\n",
      "Iteration 9038, loss = 1.85019101\n",
      "Iteration 9039, loss = 1.73973659\n",
      "Iteration 9040, loss = 2.65731774\n",
      "Iteration 9041, loss = 2.49363505\n",
      "Iteration 9042, loss = 2.40443169\n",
      "Iteration 9043, loss = 3.02603767\n",
      "Iteration 9044, loss = 2.59610754\n",
      "Iteration 9045, loss = 2.68220549\n",
      "Iteration 9046, loss = 2.81194431\n",
      "Iteration 9047, loss = 1.87269906\n",
      "Iteration 9048, loss = 1.70539588\n",
      "Iteration 9049, loss = 1.88588213\n",
      "Iteration 9050, loss = 2.22444734\n",
      "Iteration 9051, loss = 2.25947074\n",
      "Iteration 9052, loss = 2.03997586\n",
      "Iteration 9053, loss = 1.71191206\n",
      "Iteration 9054, loss = 2.00434002\n",
      "Iteration 9055, loss = 2.53607401\n",
      "Iteration 9056, loss = 2.76042816\n",
      "Iteration 9057, loss = 7.52124123\n",
      "Iteration 9058, loss = 5.95268704\n",
      "Iteration 9059, loss = 5.65326760\n",
      "Iteration 9060, loss = 2.89848472\n",
      "Iteration 9061, loss = 2.77829387\n",
      "Iteration 9062, loss = 2.91469008\n",
      "Iteration 9063, loss = 2.20325476\n",
      "Iteration 9064, loss = 1.87258518\n",
      "Iteration 9065, loss = 2.08650431\n",
      "Iteration 9066, loss = 2.29736177\n",
      "Iteration 9067, loss = 2.30121837\n",
      "Iteration 9068, loss = 2.12687991\n",
      "Iteration 9069, loss = 2.36857077\n",
      "Iteration 9070, loss = 2.04598461\n",
      "Iteration 9071, loss = 1.85770806\n",
      "Iteration 9072, loss = 1.69052431\n",
      "Iteration 9073, loss = 3.08331908\n",
      "Iteration 9074, loss = 5.46877390\n",
      "Iteration 9075, loss = 3.66139258\n",
      "Iteration 9076, loss = 2.37971897\n",
      "Iteration 9077, loss = 1.84015159\n",
      "Iteration 9078, loss = 1.95953841\n",
      "Iteration 9079, loss = 2.61041695\n",
      "Iteration 9080, loss = 2.67966943\n",
      "Iteration 9081, loss = 2.58672430\n",
      "Iteration 9082, loss = 3.51781239\n",
      "Iteration 9083, loss = 3.46900707\n",
      "Iteration 9084, loss = 2.69225548\n",
      "Iteration 9085, loss = 3.27948314\n",
      "Iteration 9086, loss = 3.15740670\n",
      "Iteration 9087, loss = 4.53506126\n",
      "Iteration 9088, loss = 3.95642765\n",
      "Iteration 9089, loss = 3.35012279\n",
      "Iteration 9090, loss = 3.84797917\n",
      "Iteration 9091, loss = 4.19438523\n",
      "Iteration 9092, loss = 3.49422369\n",
      "Iteration 9093, loss = 4.20205406\n",
      "Iteration 9094, loss = 2.86014647\n",
      "Iteration 9095, loss = 3.22565908\n",
      "Iteration 9096, loss = 3.36216745\n",
      "Iteration 9097, loss = 4.96214215\n",
      "Iteration 9098, loss = 5.68908808\n",
      "Iteration 9099, loss = 3.39442035\n",
      "Iteration 9100, loss = 2.73346033\n",
      "Iteration 9101, loss = 4.91599340\n",
      "Iteration 9102, loss = 4.72327032\n",
      "Iteration 9103, loss = 4.15318203\n",
      "Iteration 9104, loss = 3.13309790\n",
      "Iteration 9105, loss = 3.13430023\n",
      "Iteration 9106, loss = 2.16545295\n",
      "Iteration 9107, loss = 2.00350378\n",
      "Iteration 9108, loss = 1.77162580\n",
      "Iteration 9109, loss = 1.95312724\n",
      "Iteration 9110, loss = 2.15934466\n",
      "Iteration 9111, loss = 2.39192286\n",
      "Iteration 9112, loss = 1.79113373\n",
      "Iteration 9113, loss = 1.71180677\n",
      "Iteration 9114, loss = 1.69574061\n",
      "Iteration 9115, loss = 1.95173775\n",
      "Iteration 9116, loss = 1.91386541\n",
      "Iteration 9117, loss = 2.70437939\n",
      "Iteration 9118, loss = 3.45521796\n",
      "Iteration 9119, loss = 2.43602286\n",
      "Iteration 9120, loss = 1.95260600\n",
      "Iteration 9121, loss = 2.36587754\n",
      "Iteration 9122, loss = 3.50511113\n",
      "Iteration 9123, loss = 2.32590386\n",
      "Iteration 9124, loss = 2.76168531\n",
      "Iteration 9125, loss = 2.20746690\n",
      "Iteration 9126, loss = 1.95901022\n",
      "Iteration 9127, loss = 1.96547514\n",
      "Iteration 9128, loss = 2.37369660\n",
      "Iteration 9129, loss = 2.12911934\n",
      "Iteration 9130, loss = 2.27034399\n",
      "Iteration 9131, loss = 2.31903752\n",
      "Iteration 9132, loss = 4.49198100\n",
      "Iteration 9133, loss = 4.73736347\n",
      "Iteration 9134, loss = 3.16933878\n",
      "Iteration 9135, loss = 3.00480495\n",
      "Iteration 9136, loss = 2.88313074\n",
      "Iteration 9137, loss = 2.95145870\n",
      "Iteration 9138, loss = 2.42878492\n",
      "Iteration 9139, loss = 3.14193138\n",
      "Iteration 9140, loss = 2.17182814\n",
      "Iteration 9141, loss = 1.47611818\n",
      "Iteration 9142, loss = 1.70583039\n",
      "Iteration 9143, loss = 1.52304275\n",
      "Iteration 9144, loss = 1.68211266\n",
      "Iteration 9145, loss = 2.11908941\n",
      "Iteration 9146, loss = 2.44705252\n",
      "Iteration 9147, loss = 3.14428969\n",
      "Iteration 9148, loss = 2.53038217\n",
      "Iteration 9149, loss = 1.83959629\n",
      "Iteration 9150, loss = 1.67575300\n",
      "Iteration 9151, loss = 1.81123065\n",
      "Iteration 9152, loss = 2.10556316\n",
      "Iteration 9153, loss = 2.09571198\n",
      "Iteration 9154, loss = 1.58033411\n",
      "Iteration 9155, loss = 2.06955348\n",
      "Iteration 9156, loss = 1.91090766\n",
      "Iteration 9157, loss = 1.78749167\n",
      "Iteration 9158, loss = 1.96670424\n",
      "Iteration 9159, loss = 1.81065809\n",
      "Iteration 9160, loss = 2.03174098\n",
      "Iteration 9161, loss = 2.34982990\n",
      "Iteration 9162, loss = 2.73861907\n",
      "Iteration 9163, loss = 2.26717610\n",
      "Iteration 9164, loss = 2.97639543\n",
      "Iteration 9165, loss = 2.84591924\n",
      "Iteration 9166, loss = 2.58214339\n",
      "Iteration 9167, loss = 1.95240778\n",
      "Iteration 9168, loss = 1.98164862\n",
      "Iteration 9169, loss = 2.23486706\n",
      "Iteration 9170, loss = 2.92788583\n",
      "Iteration 9171, loss = 5.04345285\n",
      "Iteration 9172, loss = 3.42318441\n",
      "Iteration 9173, loss = 4.15039280\n",
      "Iteration 9174, loss = 2.60984438\n",
      "Iteration 9175, loss = 2.27873479\n",
      "Iteration 9176, loss = 2.03098877\n",
      "Iteration 9177, loss = 2.52413155\n",
      "Iteration 9178, loss = 3.67056980\n",
      "Iteration 9179, loss = 2.42662232\n",
      "Iteration 9180, loss = 2.15697495\n",
      "Iteration 9181, loss = 2.40119373\n",
      "Iteration 9182, loss = 4.21285131\n",
      "Iteration 9183, loss = 3.41935159\n",
      "Iteration 9184, loss = 2.73871287\n",
      "Iteration 9185, loss = 2.38736735\n",
      "Iteration 9186, loss = 2.10588236\n",
      "Iteration 9187, loss = 1.98751182\n",
      "Iteration 9188, loss = 2.39305661\n",
      "Iteration 9189, loss = 2.18577109\n",
      "Iteration 9190, loss = 1.82411638\n",
      "Iteration 9191, loss = 1.69401533\n",
      "Iteration 9192, loss = 1.79679140\n",
      "Iteration 9193, loss = 4.58654877\n",
      "Iteration 9194, loss = 3.75729586\n",
      "Iteration 9195, loss = 3.32487058\n",
      "Iteration 9196, loss = 3.16879665\n",
      "Iteration 9197, loss = 7.37189697\n",
      "Iteration 9198, loss = 5.72170225\n",
      "Iteration 9199, loss = 3.25916900\n",
      "Iteration 9200, loss = 3.54729509\n",
      "Iteration 9201, loss = 3.46890018\n",
      "Iteration 9202, loss = 3.57036059\n",
      "Iteration 9203, loss = 2.79877755\n",
      "Iteration 9204, loss = 2.29547882\n",
      "Iteration 9205, loss = 2.14056704\n",
      "Iteration 9206, loss = 2.26997703\n",
      "Iteration 9207, loss = 2.80021167\n",
      "Iteration 9208, loss = 2.23553117\n",
      "Iteration 9209, loss = 2.00901739\n",
      "Iteration 9210, loss = 3.56262894\n",
      "Iteration 9211, loss = 4.20334710\n",
      "Iteration 9212, loss = 3.05672660\n",
      "Iteration 9213, loss = 2.57351832\n",
      "Iteration 9214, loss = 4.35134287\n",
      "Iteration 9215, loss = 3.64915687\n",
      "Iteration 9216, loss = 2.50073363\n",
      "Iteration 9217, loss = 3.00837821\n",
      "Iteration 9218, loss = 2.29455144\n",
      "Iteration 9219, loss = 2.71837745\n",
      "Iteration 9220, loss = 2.31970268\n",
      "Iteration 9221, loss = 2.05110255\n",
      "Iteration 9222, loss = 2.05692655\n",
      "Iteration 9223, loss = 2.00927223\n",
      "Iteration 9224, loss = 1.95865554\n",
      "Iteration 9225, loss = 1.93245065\n",
      "Iteration 9226, loss = 2.35770378\n",
      "Iteration 9227, loss = 1.79103885\n",
      "Iteration 9228, loss = 1.80680050\n",
      "Iteration 9229, loss = 1.85454888\n",
      "Iteration 9230, loss = 2.25047059\n",
      "Iteration 9231, loss = 2.02049144\n",
      "Iteration 9232, loss = 1.71474648\n",
      "Iteration 9233, loss = 1.70848493\n",
      "Iteration 9234, loss = 1.89664936\n",
      "Iteration 9235, loss = 1.72404838\n",
      "Iteration 9236, loss = 2.28351137\n",
      "Iteration 9237, loss = 2.14214165\n",
      "Iteration 9238, loss = 2.13524938\n",
      "Iteration 9239, loss = 2.12775396\n",
      "Iteration 9240, loss = 3.97597704\n",
      "Iteration 9241, loss = 2.82720236\n",
      "Iteration 9242, loss = 5.06675399\n",
      "Iteration 9243, loss = 5.09453435\n",
      "Iteration 9244, loss = 2.83462284\n",
      "Iteration 9245, loss = 4.15042969\n",
      "Iteration 9246, loss = 3.09008627\n",
      "Iteration 9247, loss = 2.29254216\n",
      "Iteration 9248, loss = 4.67329964\n",
      "Iteration 9249, loss = 3.84372347\n",
      "Iteration 9250, loss = 3.31765974\n",
      "Iteration 9251, loss = 2.95757256\n",
      "Iteration 9252, loss = 4.54858951\n",
      "Iteration 9253, loss = 2.42927372\n",
      "Iteration 9254, loss = 1.96510855\n",
      "Iteration 9255, loss = 1.89449138\n",
      "Iteration 9256, loss = 2.13999126\n",
      "Iteration 9257, loss = 4.53917537\n",
      "Iteration 9258, loss = 3.87291582\n",
      "Iteration 9259, loss = 4.03387432\n",
      "Iteration 9260, loss = 2.14525015\n",
      "Iteration 9261, loss = 1.90830380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9262, loss = 1.65317933\n",
      "Iteration 9263, loss = 1.53730773\n",
      "Iteration 9264, loss = 1.55969737\n",
      "Iteration 9265, loss = 1.76376708\n",
      "Iteration 9266, loss = 1.49329232\n",
      "Iteration 9267, loss = 1.81420694\n",
      "Iteration 9268, loss = 1.69853696\n",
      "Iteration 9269, loss = 1.51891453\n",
      "Iteration 9270, loss = 1.58758328\n",
      "Iteration 9271, loss = 1.48592520\n",
      "Iteration 9272, loss = 1.53806347\n",
      "Iteration 9273, loss = 1.63215118\n",
      "Iteration 9274, loss = 1.54788151\n",
      "Iteration 9275, loss = 2.50521203\n",
      "Iteration 9276, loss = 2.17157119\n",
      "Iteration 9277, loss = 4.99535608\n",
      "Iteration 9278, loss = 3.38006492\n",
      "Iteration 9279, loss = 2.61989160\n",
      "Iteration 9280, loss = 3.98255102\n",
      "Iteration 9281, loss = 3.69281024\n",
      "Iteration 9282, loss = 2.18837056\n",
      "Iteration 9283, loss = 2.39790954\n",
      "Iteration 9284, loss = 1.72992391\n",
      "Iteration 9285, loss = 1.83433283\n",
      "Iteration 9286, loss = 1.86799552\n",
      "Iteration 9287, loss = 1.54161204\n",
      "Iteration 9288, loss = 1.62645086\n",
      "Iteration 9289, loss = 1.69325811\n",
      "Iteration 9290, loss = 2.31015466\n",
      "Iteration 9291, loss = 2.17154537\n",
      "Iteration 9292, loss = 2.55270308\n",
      "Iteration 9293, loss = 2.15728246\n",
      "Iteration 9294, loss = 2.03022285\n",
      "Iteration 9295, loss = 2.35632344\n",
      "Iteration 9296, loss = 1.82711243\n",
      "Iteration 9297, loss = 2.27399673\n",
      "Iteration 9298, loss = 1.97746805\n",
      "Iteration 9299, loss = 1.77263154\n",
      "Iteration 9300, loss = 2.08005104\n",
      "Iteration 9301, loss = 1.98134784\n",
      "Iteration 9302, loss = 1.90968103\n",
      "Iteration 9303, loss = 1.44594535\n",
      "Iteration 9304, loss = 2.78064344\n",
      "Iteration 9305, loss = 3.28544641\n",
      "Iteration 9306, loss = 2.16062764\n",
      "Iteration 9307, loss = 1.72838435\n",
      "Iteration 9308, loss = 2.81383399\n",
      "Iteration 9309, loss = 1.91813661\n",
      "Iteration 9310, loss = 2.51175631\n",
      "Iteration 9311, loss = 3.03603339\n",
      "Iteration 9312, loss = 2.77005710\n",
      "Iteration 9313, loss = 2.72499192\n",
      "Iteration 9314, loss = 1.92544147\n",
      "Iteration 9315, loss = 1.66660825\n",
      "Iteration 9316, loss = 1.61811186\n",
      "Iteration 9317, loss = 1.92110375\n",
      "Iteration 9318, loss = 2.03771760\n",
      "Iteration 9319, loss = 2.08807204\n",
      "Iteration 9320, loss = 1.89333823\n",
      "Iteration 9321, loss = 1.92680549\n",
      "Iteration 9322, loss = 2.51985989\n",
      "Iteration 9323, loss = 1.98966296\n",
      "Iteration 9324, loss = 3.01085895\n",
      "Iteration 9325, loss = 2.57107603\n",
      "Iteration 9326, loss = 2.28596739\n",
      "Iteration 9327, loss = 1.71658346\n",
      "Iteration 9328, loss = 2.20908217\n",
      "Iteration 9329, loss = 1.66391264\n",
      "Iteration 9330, loss = 1.69277928\n",
      "Iteration 9331, loss = 2.37675983\n",
      "Iteration 9332, loss = 2.62263810\n",
      "Iteration 9333, loss = 2.02458481\n",
      "Iteration 9334, loss = 1.66830303\n",
      "Iteration 9335, loss = 1.86766568\n",
      "Iteration 9336, loss = 1.90228933\n",
      "Iteration 9337, loss = 1.51298737\n",
      "Iteration 9338, loss = 1.54827467\n",
      "Iteration 9339, loss = 1.86906937\n",
      "Iteration 9340, loss = 2.45408445\n",
      "Iteration 9341, loss = 1.98288786\n",
      "Iteration 9342, loss = 2.16702615\n",
      "Iteration 9343, loss = 1.56126548\n",
      "Iteration 9344, loss = 1.96104760\n",
      "Iteration 9345, loss = 1.68310930\n",
      "Iteration 9346, loss = 1.84754248\n",
      "Iteration 9347, loss = 2.17342827\n",
      "Iteration 9348, loss = 1.69572074\n",
      "Iteration 9349, loss = 1.72118645\n",
      "Iteration 9350, loss = 1.70212518\n",
      "Iteration 9351, loss = 1.51837566\n",
      "Iteration 9352, loss = 1.86352770\n",
      "Iteration 9353, loss = 1.73348221\n",
      "Iteration 9354, loss = 1.59056054\n",
      "Iteration 9355, loss = 1.44574804\n",
      "Iteration 9356, loss = 1.42560560\n",
      "Iteration 9357, loss = 1.42715905\n",
      "Iteration 9358, loss = 1.69294601\n",
      "Iteration 9359, loss = 1.80055510\n",
      "Iteration 9360, loss = 1.76123330\n",
      "Iteration 9361, loss = 1.72074542\n",
      "Iteration 9362, loss = 1.58599552\n",
      "Iteration 9363, loss = 1.48604483\n",
      "Iteration 9364, loss = 1.44820002\n",
      "Iteration 9365, loss = 2.08030705\n",
      "Iteration 9366, loss = 1.66231069\n",
      "Iteration 9367, loss = 1.62841091\n",
      "Iteration 9368, loss = 1.50794349\n",
      "Iteration 9369, loss = 1.62296688\n",
      "Iteration 9370, loss = 1.79897206\n",
      "Iteration 9371, loss = 1.91912883\n",
      "Iteration 9372, loss = 2.06409961\n",
      "Iteration 9373, loss = 1.50465771\n",
      "Iteration 9374, loss = 1.41576717\n",
      "Iteration 9375, loss = 1.81075043\n",
      "Iteration 9376, loss = 1.97525342\n",
      "Iteration 9377, loss = 1.54261736\n",
      "Iteration 9378, loss = 1.55377283\n",
      "Iteration 9379, loss = 1.96114387\n",
      "Iteration 9380, loss = 1.80614761\n",
      "Iteration 9381, loss = 1.65400589\n",
      "Iteration 9382, loss = 1.59331122\n",
      "Iteration 9383, loss = 1.45250179\n",
      "Iteration 9384, loss = 1.40001313\n",
      "Iteration 9385, loss = 1.51254546\n",
      "Iteration 9386, loss = 1.23858660\n",
      "Iteration 9387, loss = 2.38322372\n",
      "Iteration 9388, loss = 2.33743296\n",
      "Iteration 9389, loss = 1.75367624\n",
      "Iteration 9390, loss = 1.73266255\n",
      "Iteration 9391, loss = 1.77525056\n",
      "Iteration 9392, loss = 2.66168574\n",
      "Iteration 9393, loss = 1.67408592\n",
      "Iteration 9394, loss = 1.52338068\n",
      "Iteration 9395, loss = 1.44390137\n",
      "Iteration 9396, loss = 1.41461744\n",
      "Iteration 9397, loss = 1.44626771\n",
      "Iteration 9398, loss = 1.65098418\n",
      "Iteration 9399, loss = 2.02208693\n",
      "Iteration 9400, loss = 1.49743828\n",
      "Iteration 9401, loss = 1.66203656\n",
      "Iteration 9402, loss = 1.61182046\n",
      "Iteration 9403, loss = 1.55562346\n",
      "Iteration 9404, loss = 2.07326827\n",
      "Iteration 9405, loss = 1.63310138\n",
      "Iteration 9406, loss = 1.48074979\n",
      "Iteration 9407, loss = 1.47241764\n",
      "Iteration 9408, loss = 1.83569361\n",
      "Iteration 9409, loss = 1.89300777\n",
      "Iteration 9410, loss = 1.83411441\n",
      "Iteration 9411, loss = 1.66361689\n",
      "Iteration 9412, loss = 1.87605798\n",
      "Iteration 9413, loss = 1.80757006\n",
      "Iteration 9414, loss = 1.68106586\n",
      "Iteration 9415, loss = 1.83503622\n",
      "Iteration 9416, loss = 1.47157747\n",
      "Iteration 9417, loss = 1.53876792\n",
      "Iteration 9418, loss = 2.03571777\n",
      "Iteration 9419, loss = 1.98919722\n",
      "Iteration 9420, loss = 1.89849551\n",
      "Iteration 9421, loss = 1.86487056\n",
      "Iteration 9422, loss = 1.89302918\n",
      "Iteration 9423, loss = 1.51260545\n",
      "Iteration 9424, loss = 1.48998183\n",
      "Iteration 9425, loss = 1.44337286\n",
      "Iteration 9426, loss = 1.78098370\n",
      "Iteration 9427, loss = 1.65014830\n",
      "Iteration 9428, loss = 1.82127250\n",
      "Iteration 9429, loss = 1.37823596\n",
      "Iteration 9430, loss = 1.37381631\n",
      "Iteration 9431, loss = 1.34456517\n",
      "Iteration 9432, loss = 2.75127535\n",
      "Iteration 9433, loss = 1.60377267\n",
      "Iteration 9434, loss = 2.67258400\n",
      "Iteration 9435, loss = 3.03065518\n",
      "Iteration 9436, loss = 3.54443970\n",
      "Iteration 9437, loss = 3.38242202\n",
      "Iteration 9438, loss = 1.78548786\n",
      "Iteration 9439, loss = 1.73897703\n",
      "Iteration 9440, loss = 1.42467314\n",
      "Iteration 9441, loss = 1.69106608\n",
      "Iteration 9442, loss = 1.41899856\n",
      "Iteration 9443, loss = 1.48495881\n",
      "Iteration 9444, loss = 1.67048536\n",
      "Iteration 9445, loss = 1.60204319\n",
      "Iteration 9446, loss = 1.82292721\n",
      "Iteration 9447, loss = 1.54566489\n",
      "Iteration 9448, loss = 2.37055449\n",
      "Iteration 9449, loss = 1.55934515\n",
      "Iteration 9450, loss = 1.50645622\n",
      "Iteration 9451, loss = 1.48519722\n",
      "Iteration 9452, loss = 1.30238906\n",
      "Iteration 9453, loss = 1.52137897\n",
      "Iteration 9454, loss = 1.84947937\n",
      "Iteration 9455, loss = 1.68273370\n",
      "Iteration 9456, loss = 1.44617233\n",
      "Iteration 9457, loss = 1.38323877\n",
      "Iteration 9458, loss = 1.32893007\n",
      "Iteration 9459, loss = 1.43532536\n",
      "Iteration 9460, loss = 1.38137109\n",
      "Iteration 9461, loss = 1.29325149\n",
      "Iteration 9462, loss = 3.41856017\n",
      "Iteration 9463, loss = 2.10743505\n",
      "Iteration 9464, loss = 2.27631193\n",
      "Iteration 9465, loss = 2.78650635\n",
      "Iteration 9466, loss = 4.09505788\n",
      "Iteration 9467, loss = 3.92602259\n",
      "Iteration 9468, loss = 2.86176799\n",
      "Iteration 9469, loss = 2.47380403\n",
      "Iteration 9470, loss = 2.15977535\n",
      "Iteration 9471, loss = 2.60522894\n",
      "Iteration 9472, loss = 3.00069287\n",
      "Iteration 9473, loss = 1.94936987\n",
      "Iteration 9474, loss = 2.07269749\n",
      "Iteration 9475, loss = 3.25926016\n",
      "Iteration 9476, loss = 2.75081640\n",
      "Iteration 9477, loss = 2.39218584\n",
      "Iteration 9478, loss = 1.79781476\n",
      "Iteration 9479, loss = 1.54097962\n",
      "Iteration 9480, loss = 1.53674495\n",
      "Iteration 9481, loss = 2.47754472\n",
      "Iteration 9482, loss = 3.71644139\n",
      "Iteration 9483, loss = 2.54794323\n",
      "Iteration 9484, loss = 1.99787859\n",
      "Iteration 9485, loss = 2.90935320\n",
      "Iteration 9486, loss = 2.44624630\n",
      "Iteration 9487, loss = 2.19332721\n",
      "Iteration 9488, loss = 2.01421440\n",
      "Iteration 9489, loss = 1.80936361\n",
      "Iteration 9490, loss = 2.00403360\n",
      "Iteration 9491, loss = 1.87794453\n",
      "Iteration 9492, loss = 1.80385606\n",
      "Iteration 9493, loss = 1.87159777\n",
      "Iteration 9494, loss = 2.29445560\n",
      "Iteration 9495, loss = 1.50566827\n",
      "Iteration 9496, loss = 1.42066936\n",
      "Iteration 9497, loss = 1.62018217\n",
      "Iteration 9498, loss = 1.62751783\n",
      "Iteration 9499, loss = 1.72205668\n",
      "Iteration 9500, loss = 1.88071660\n",
      "Iteration 9501, loss = 2.43828669\n",
      "Iteration 9502, loss = 2.24950720\n",
      "Iteration 9503, loss = 3.16156584\n",
      "Iteration 9504, loss = 2.31060709\n",
      "Iteration 9505, loss = 2.17457240\n",
      "Iteration 9506, loss = 1.50895460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9507, loss = 2.11967836\n",
      "Iteration 9508, loss = 2.63904444\n",
      "Iteration 9509, loss = 3.32812522\n",
      "Iteration 9510, loss = 3.11202095\n",
      "Iteration 9511, loss = 2.05050986\n",
      "Iteration 9512, loss = 1.49493022\n",
      "Iteration 9513, loss = 1.62663655\n",
      "Iteration 9514, loss = 1.64505923\n",
      "Iteration 9515, loss = 1.68410023\n",
      "Iteration 9516, loss = 1.82513882\n",
      "Iteration 9517, loss = 3.35909220\n",
      "Iteration 9518, loss = 5.13445828\n",
      "Iteration 9519, loss = 5.54146009\n",
      "Iteration 9520, loss = 4.91980440\n",
      "Iteration 9521, loss = 4.21579899\n",
      "Iteration 9522, loss = 2.73888892\n",
      "Iteration 9523, loss = 2.89908386\n",
      "Iteration 9524, loss = 3.41093231\n",
      "Iteration 9525, loss = 2.28153023\n",
      "Iteration 9526, loss = 1.56842136\n",
      "Iteration 9527, loss = 1.35616049\n",
      "Iteration 9528, loss = 1.40369867\n",
      "Iteration 9529, loss = 1.39649210\n",
      "Iteration 9530, loss = 1.41029773\n",
      "Iteration 9531, loss = 1.80245179\n",
      "Iteration 9532, loss = 2.37252382\n",
      "Iteration 9533, loss = 1.84596733\n",
      "Iteration 9534, loss = 1.95070708\n",
      "Iteration 9535, loss = 1.68463676\n",
      "Iteration 9536, loss = 1.38310444\n",
      "Iteration 9537, loss = 1.53553503\n",
      "Iteration 9538, loss = 1.56409016\n",
      "Iteration 9539, loss = 1.71142062\n",
      "Iteration 9540, loss = 2.06992455\n",
      "Iteration 9541, loss = 1.87807238\n",
      "Iteration 9542, loss = 1.36973284\n",
      "Iteration 9543, loss = 1.36930455\n",
      "Iteration 9544, loss = 1.52872308\n",
      "Iteration 9545, loss = 1.43240180\n",
      "Iteration 9546, loss = 1.29902431\n",
      "Iteration 9547, loss = 1.62974418\n",
      "Iteration 9548, loss = 1.98902489\n",
      "Iteration 9549, loss = 1.67802378\n",
      "Iteration 9550, loss = 1.51003345\n",
      "Iteration 9551, loss = 1.70853718\n",
      "Iteration 9552, loss = 2.05536836\n",
      "Iteration 9553, loss = 1.95982536\n",
      "Iteration 9554, loss = 1.87083297\n",
      "Iteration 9555, loss = 1.61362313\n",
      "Iteration 9556, loss = 1.72504529\n",
      "Iteration 9557, loss = 2.13469422\n",
      "Iteration 9558, loss = 7.47166202\n",
      "Iteration 9559, loss = 12.68889375\n",
      "Iteration 9560, loss = 8.74285784\n",
      "Iteration 9561, loss = 5.23176948\n",
      "Iteration 9562, loss = 5.48270453\n",
      "Iteration 9563, loss = 5.27741416\n",
      "Iteration 9564, loss = 5.76143052\n",
      "Iteration 9565, loss = 4.63572656\n",
      "Iteration 9566, loss = 4.69212858\n",
      "Iteration 9567, loss = 6.30878234\n",
      "Iteration 9568, loss = 5.67473747\n",
      "Iteration 9569, loss = 4.49076669\n",
      "Iteration 9570, loss = 4.59702390\n",
      "Iteration 9571, loss = 4.85703542\n",
      "Iteration 9572, loss = 5.26522873\n",
      "Iteration 9573, loss = 4.84546174\n",
      "Iteration 9574, loss = 4.72795493\n",
      "Iteration 9575, loss = 4.53849838\n",
      "Iteration 9576, loss = 4.20122814\n",
      "Iteration 9577, loss = 4.16536411\n",
      "Iteration 9578, loss = 4.04479417\n",
      "Iteration 9579, loss = 4.21932319\n",
      "Iteration 9580, loss = 4.12941278\n",
      "Iteration 9581, loss = 4.32714521\n",
      "Iteration 9582, loss = 3.96382484\n",
      "Iteration 9583, loss = 3.97113562\n",
      "Iteration 9584, loss = 3.86147068\n",
      "Iteration 9585, loss = 3.95470506\n",
      "Iteration 9586, loss = 4.89865087\n",
      "Iteration 9587, loss = 4.87557508\n",
      "Iteration 9588, loss = 4.26939551\n",
      "Iteration 9589, loss = 4.34426735\n",
      "Iteration 9590, loss = 4.67664943\n",
      "Iteration 9591, loss = 4.35955820\n",
      "Iteration 9592, loss = 4.50301072\n",
      "Iteration 9593, loss = 4.74064303\n",
      "Iteration 9594, loss = 4.51123972\n",
      "Iteration 9595, loss = 4.28923506\n",
      "Iteration 9596, loss = 3.84081790\n",
      "Iteration 9597, loss = 4.03446435\n",
      "Iteration 9598, loss = 4.46646155\n",
      "Iteration 9599, loss = 4.06911259\n",
      "Iteration 9600, loss = 4.08272950\n",
      "Iteration 9601, loss = 4.62069452\n",
      "Iteration 9602, loss = 4.60624182\n",
      "Iteration 9603, loss = 4.68609079\n",
      "Iteration 9604, loss = 4.32759648\n",
      "Iteration 9605, loss = 4.65234473\n",
      "Iteration 9606, loss = 3.80064618\n",
      "Iteration 9607, loss = 3.82318555\n",
      "Iteration 9608, loss = 3.65090483\n",
      "Iteration 9609, loss = 3.53934638\n",
      "Iteration 9610, loss = 3.37808073\n",
      "Iteration 9611, loss = 3.46763431\n",
      "Iteration 9612, loss = 3.52227920\n",
      "Iteration 9613, loss = 4.58593490\n",
      "Iteration 9614, loss = 4.28632283\n",
      "Iteration 9615, loss = 4.36660126\n",
      "Iteration 9616, loss = 5.30992892\n",
      "Iteration 9617, loss = 4.08941949\n",
      "Iteration 9618, loss = 4.74726701\n",
      "Iteration 9619, loss = 4.05665952\n",
      "Iteration 9620, loss = 4.77105807\n",
      "Iteration 9621, loss = 3.80009151\n",
      "Iteration 9622, loss = 3.94842376\n",
      "Iteration 9623, loss = 3.93940425\n",
      "Iteration 9624, loss = 4.42804730\n",
      "Iteration 9625, loss = 3.84150524\n",
      "Iteration 9626, loss = 4.12748371\n",
      "Iteration 9627, loss = 3.51528662\n",
      "Iteration 9628, loss = 3.38961670\n",
      "Iteration 9629, loss = 3.34845180\n",
      "Iteration 9630, loss = 3.50777222\n",
      "Iteration 9631, loss = 3.37606324\n",
      "Iteration 9632, loss = 3.55071106\n",
      "Iteration 9633, loss = 3.47663025\n",
      "Iteration 9634, loss = 3.43549389\n",
      "Iteration 9635, loss = 3.52726327\n",
      "Iteration 9636, loss = 3.24628541\n",
      "Iteration 9637, loss = 3.23063957\n",
      "Iteration 9638, loss = 3.14602155\n",
      "Iteration 9639, loss = 3.19881803\n",
      "Iteration 9640, loss = 3.25419231\n",
      "Iteration 9641, loss = 3.28490639\n",
      "Iteration 9642, loss = 3.54205464\n",
      "Iteration 9643, loss = 4.33077691\n",
      "Iteration 9644, loss = 3.27682589\n",
      "Iteration 9645, loss = 3.14009784\n",
      "Iteration 9646, loss = 3.17003530\n",
      "Iteration 9647, loss = 3.16739434\n",
      "Iteration 9648, loss = 2.98597349\n",
      "Iteration 9649, loss = 3.78329743\n",
      "Iteration 9650, loss = 3.45388664\n",
      "Iteration 9651, loss = 3.20195680\n",
      "Iteration 9652, loss = 3.23343017\n",
      "Iteration 9653, loss = 3.26747102\n",
      "Iteration 9654, loss = 3.32960973\n",
      "Iteration 9655, loss = 3.15697386\n",
      "Iteration 9656, loss = 3.29045427\n",
      "Iteration 9657, loss = 3.06825695\n",
      "Iteration 9658, loss = 3.13291347\n",
      "Iteration 9659, loss = 4.38572754\n",
      "Iteration 9660, loss = 3.73200998\n",
      "Iteration 9661, loss = 3.88657337\n",
      "Iteration 9662, loss = 4.37558621\n",
      "Iteration 9663, loss = 3.21481934\n",
      "Iteration 9664, loss = 3.02899077\n",
      "Iteration 9665, loss = 3.22147448\n",
      "Iteration 9666, loss = 3.04753974\n",
      "Iteration 9667, loss = 3.87189087\n",
      "Iteration 9668, loss = 3.25114569\n",
      "Iteration 9669, loss = 3.40743361\n",
      "Iteration 9670, loss = 3.61947439\n",
      "Iteration 9671, loss = 4.61478398\n",
      "Iteration 9672, loss = 16.73497022\n",
      "Iteration 9673, loss = 10.17970672\n",
      "Iteration 9674, loss = 19.12651334\n",
      "Iteration 9675, loss = 9.52492532\n",
      "Iteration 9676, loss = 7.59157256\n",
      "Iteration 9677, loss = 6.26796801\n",
      "Iteration 9678, loss = 5.32740716\n",
      "Iteration 9679, loss = 4.71403278\n",
      "Iteration 9680, loss = 4.45708657\n",
      "Iteration 9681, loss = 4.24508817\n",
      "Iteration 9682, loss = 3.93051832\n",
      "Iteration 9683, loss = 4.55725356\n",
      "Iteration 9684, loss = 4.28425577\n",
      "Iteration 9685, loss = 3.32656011\n",
      "Iteration 9686, loss = 3.53346132\n",
      "Iteration 9687, loss = 3.13405194\n",
      "Iteration 9688, loss = 2.32648631\n",
      "Iteration 9689, loss = 2.88267837\n",
      "Iteration 9690, loss = 2.33307824\n",
      "Iteration 9691, loss = 1.99663313\n",
      "Iteration 9692, loss = 2.07406215\n",
      "Iteration 9693, loss = 2.17647792\n",
      "Iteration 9694, loss = 1.82856695\n",
      "Iteration 9695, loss = 2.19915809\n",
      "Iteration 9696, loss = 1.91913465\n",
      "Iteration 9697, loss = 2.09162459\n",
      "Iteration 9698, loss = 2.24077280\n",
      "Iteration 9699, loss = 2.14820474\n",
      "Iteration 9700, loss = 4.62808710\n",
      "Iteration 9701, loss = 4.41398736\n",
      "Iteration 9702, loss = 3.97101239\n",
      "Iteration 9703, loss = 3.39724335\n",
      "Iteration 9704, loss = 2.80289782\n",
      "Iteration 9705, loss = 2.34580442\n",
      "Iteration 9706, loss = 1.82079707\n",
      "Iteration 9707, loss = 1.88651547\n",
      "Iteration 9708, loss = 1.87818986\n",
      "Iteration 9709, loss = 1.91081415\n",
      "Iteration 9710, loss = 1.64402330\n",
      "Iteration 9711, loss = 1.68657903\n",
      "Iteration 9712, loss = 1.96260620\n",
      "Iteration 9713, loss = 3.04237923\n",
      "Iteration 9714, loss = 4.04767483\n",
      "Iteration 9715, loss = 2.87150259\n",
      "Iteration 9716, loss = 3.23905602\n",
      "Iteration 9717, loss = 3.15359974\n",
      "Iteration 9718, loss = 1.76715204\n",
      "Iteration 9719, loss = 1.90276800\n",
      "Iteration 9720, loss = 1.79478386\n",
      "Iteration 9721, loss = 2.63034349\n",
      "Iteration 9722, loss = 2.30907891\n",
      "Iteration 9723, loss = 1.64798566\n",
      "Iteration 9724, loss = 1.98594495\n",
      "Iteration 9725, loss = 1.73062116\n",
      "Iteration 9726, loss = 1.60682758\n",
      "Iteration 9727, loss = 1.57449011\n",
      "Iteration 9728, loss = 1.60725318\n",
      "Iteration 9729, loss = 1.52347928\n",
      "Iteration 9730, loss = 1.53493445\n",
      "Iteration 9731, loss = 2.13708249\n",
      "Iteration 9732, loss = 1.82776458\n",
      "Iteration 9733, loss = 2.22324864\n",
      "Iteration 9734, loss = 2.77556083\n",
      "Iteration 9735, loss = 2.32029931\n",
      "Iteration 9736, loss = 3.48956667\n",
      "Iteration 9737, loss = 4.59541571\n",
      "Iteration 9738, loss = 4.32515054\n",
      "Iteration 9739, loss = 3.33124050\n",
      "Iteration 9740, loss = 3.24980318\n",
      "Iteration 9741, loss = 1.87483547\n",
      "Iteration 9742, loss = 1.86928205\n",
      "Iteration 9743, loss = 1.83116748\n",
      "Iteration 9744, loss = 2.38008438\n",
      "Iteration 9745, loss = 3.49530694\n",
      "Iteration 9746, loss = 3.28588726\n",
      "Iteration 9747, loss = 2.39930775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9748, loss = 2.28337880\n",
      "Iteration 9749, loss = 2.00336719\n",
      "Iteration 9750, loss = 2.13061294\n",
      "Iteration 9751, loss = 1.71718445\n",
      "Iteration 9752, loss = 1.69900441\n",
      "Iteration 9753, loss = 1.72327660\n",
      "Iteration 9754, loss = 2.02739956\n",
      "Iteration 9755, loss = 1.83025917\n",
      "Iteration 9756, loss = 2.23998561\n",
      "Iteration 9757, loss = 1.76145988\n",
      "Iteration 9758, loss = 1.48952483\n",
      "Iteration 9759, loss = 1.65612926\n",
      "Iteration 9760, loss = 1.37469862\n",
      "Iteration 9761, loss = 2.03970306\n",
      "Iteration 9762, loss = 1.79483220\n",
      "Iteration 9763, loss = 1.63995701\n",
      "Iteration 9764, loss = 1.73841246\n",
      "Iteration 9765, loss = 2.24411268\n",
      "Iteration 9766, loss = 2.12663958\n",
      "Iteration 9767, loss = 1.64781162\n",
      "Iteration 9768, loss = 1.75993415\n",
      "Iteration 9769, loss = 1.49103536\n",
      "Iteration 9770, loss = 1.45466830\n",
      "Iteration 9771, loss = 1.52336075\n",
      "Iteration 9772, loss = 1.98743821\n",
      "Iteration 9773, loss = 2.09306686\n",
      "Iteration 9774, loss = 2.28423074\n",
      "Iteration 9775, loss = 2.48450694\n",
      "Iteration 9776, loss = 1.61246272\n",
      "Iteration 9777, loss = 2.06625797\n",
      "Iteration 9778, loss = 2.82028203\n",
      "Iteration 9779, loss = 3.46947240\n",
      "Iteration 9780, loss = 3.39183546\n",
      "Iteration 9781, loss = 4.66895156\n",
      "Iteration 9782, loss = 3.90104442\n",
      "Iteration 9783, loss = 3.61000473\n",
      "Iteration 9784, loss = 2.70373052\n",
      "Iteration 9785, loss = 2.89342445\n",
      "Iteration 9786, loss = 4.40486507\n",
      "Iteration 9787, loss = 2.37037763\n",
      "Iteration 9788, loss = 3.27736907\n",
      "Iteration 9789, loss = 2.42682181\n",
      "Iteration 9790, loss = 2.65812103\n",
      "Iteration 9791, loss = 2.27189873\n",
      "Iteration 9792, loss = 1.85130692\n",
      "Iteration 9793, loss = 1.84676214\n",
      "Iteration 9794, loss = 1.59196687\n",
      "Iteration 9795, loss = 1.52337779\n",
      "Iteration 9796, loss = 1.67346964\n",
      "Iteration 9797, loss = 1.51440346\n",
      "Iteration 9798, loss = 1.59977567\n",
      "Iteration 9799, loss = 1.80583321\n",
      "Iteration 9800, loss = 1.84681768\n",
      "Iteration 9801, loss = 1.62805830\n",
      "Iteration 9802, loss = 1.50139336\n",
      "Iteration 9803, loss = 1.48933297\n",
      "Iteration 9804, loss = 1.83213555\n",
      "Iteration 9805, loss = 1.57697635\n",
      "Iteration 9806, loss = 1.82802233\n",
      "Iteration 9807, loss = 1.88228048\n",
      "Iteration 9808, loss = 1.55481721\n",
      "Iteration 9809, loss = 1.95579268\n",
      "Iteration 9810, loss = 2.24564793\n",
      "Iteration 9811, loss = 3.60362689\n",
      "Iteration 9812, loss = 2.33742359\n",
      "Iteration 9813, loss = 2.13369536\n",
      "Iteration 9814, loss = 1.78428374\n",
      "Iteration 9815, loss = 2.12064706\n",
      "Iteration 9816, loss = 1.80263149\n",
      "Iteration 9817, loss = 2.04482008\n",
      "Iteration 9818, loss = 1.90826377\n",
      "Iteration 9819, loss = 1.73311358\n",
      "Iteration 9820, loss = 2.70607553\n",
      "Iteration 9821, loss = 2.66857723\n",
      "Iteration 9822, loss = 2.42634340\n",
      "Iteration 9823, loss = 1.79544719\n",
      "Iteration 9824, loss = 2.58308086\n",
      "Iteration 9825, loss = 2.54575158\n",
      "Iteration 9826, loss = 2.31328766\n",
      "Iteration 9827, loss = 1.95516232\n",
      "Iteration 9828, loss = 1.65748391\n",
      "Iteration 9829, loss = 3.08984115\n",
      "Iteration 9830, loss = 2.54039902\n",
      "Iteration 9831, loss = 2.29614926\n",
      "Iteration 9832, loss = 2.16003752\n",
      "Iteration 9833, loss = 1.83933528\n",
      "Iteration 9834, loss = 2.06810058\n",
      "Iteration 9835, loss = 1.87178237\n",
      "Iteration 9836, loss = 1.91565323\n",
      "Iteration 9837, loss = 2.96836456\n",
      "Iteration 9838, loss = 2.81032256\n",
      "Iteration 9839, loss = 2.22117010\n",
      "Iteration 9840, loss = 1.82154923\n",
      "Iteration 9841, loss = 2.07610557\n",
      "Iteration 9842, loss = 2.21389278\n",
      "Iteration 9843, loss = 1.68575238\n",
      "Iteration 9844, loss = 1.42250852\n",
      "Iteration 9845, loss = 1.59723388\n",
      "Iteration 9846, loss = 1.76678197\n",
      "Iteration 9847, loss = 1.49374760\n",
      "Iteration 9848, loss = 1.49910702\n",
      "Iteration 9849, loss = 1.73009942\n",
      "Iteration 9850, loss = 1.72721874\n",
      "Iteration 9851, loss = 1.79042321\n",
      "Iteration 9852, loss = 2.72460509\n",
      "Iteration 9853, loss = 1.83765770\n",
      "Iteration 9854, loss = 2.05442067\n",
      "Iteration 9855, loss = 1.59016245\n",
      "Iteration 9856, loss = 1.49627680\n",
      "Iteration 9857, loss = 1.49713007\n",
      "Iteration 9858, loss = 1.42155813\n",
      "Iteration 9859, loss = 1.44135149\n",
      "Iteration 9860, loss = 1.37227853\n",
      "Iteration 9861, loss = 2.47705625\n",
      "Iteration 9862, loss = 2.55462403\n",
      "Iteration 9863, loss = 2.11457086\n",
      "Iteration 9864, loss = 1.99348774\n",
      "Iteration 9865, loss = 2.05016076\n",
      "Iteration 9866, loss = 2.06210315\n",
      "Iteration 9867, loss = 1.94583417\n",
      "Iteration 9868, loss = 1.60714208\n",
      "Iteration 9869, loss = 3.32891982\n",
      "Iteration 9870, loss = 3.38634600\n",
      "Iteration 9871, loss = 4.86459611\n",
      "Iteration 9872, loss = 4.79444094\n",
      "Iteration 9873, loss = 4.60012065\n",
      "Iteration 9874, loss = 5.03085544\n",
      "Iteration 9875, loss = 3.49629833\n",
      "Iteration 9876, loss = 2.43973224\n",
      "Iteration 9877, loss = 2.43654394\n",
      "Iteration 9878, loss = 2.00251997\n",
      "Iteration 9879, loss = 1.97564960\n",
      "Iteration 9880, loss = 1.96823724\n",
      "Iteration 9881, loss = 1.55975696\n",
      "Iteration 9882, loss = 1.67144687\n",
      "Iteration 9883, loss = 1.68605280\n",
      "Iteration 9884, loss = 1.94581944\n",
      "Iteration 9885, loss = 1.82156818\n",
      "Iteration 9886, loss = 1.74900668\n",
      "Iteration 9887, loss = 1.47796010\n",
      "Iteration 9888, loss = 1.77603313\n",
      "Iteration 9889, loss = 1.60870893\n",
      "Iteration 9890, loss = 1.74234363\n",
      "Iteration 9891, loss = 1.50357513\n",
      "Iteration 9892, loss = 1.50849065\n",
      "Iteration 9893, loss = 1.66577451\n",
      "Iteration 9894, loss = 1.76507368\n",
      "Iteration 9895, loss = 2.00467708\n",
      "Iteration 9896, loss = 1.61996557\n",
      "Iteration 9897, loss = 1.56148892\n",
      "Iteration 9898, loss = 2.06749696\n",
      "Iteration 9899, loss = 2.16624025\n",
      "Iteration 9900, loss = 3.08838244\n",
      "Iteration 9901, loss = 2.08066152\n",
      "Iteration 9902, loss = 1.81647311\n",
      "Iteration 9903, loss = 1.99112039\n",
      "Iteration 9904, loss = 2.95332282\n",
      "Iteration 9905, loss = 3.09397287\n",
      "Iteration 9906, loss = 2.37384256\n",
      "Iteration 9907, loss = 2.19107369\n",
      "Iteration 9908, loss = 3.15393543\n",
      "Iteration 9909, loss = 3.17625689\n",
      "Iteration 9910, loss = 1.84835385\n",
      "Iteration 9911, loss = 1.58766822\n",
      "Iteration 9912, loss = 1.55225747\n",
      "Iteration 9913, loss = 1.73516651\n",
      "Iteration 9914, loss = 2.53706788\n",
      "Iteration 9915, loss = 1.89528445\n",
      "Iteration 9916, loss = 1.83870470\n",
      "Iteration 9917, loss = 1.71979233\n",
      "Iteration 9918, loss = 1.58922062\n",
      "Iteration 9919, loss = 1.47131914\n",
      "Iteration 9920, loss = 1.43136654\n",
      "Iteration 9921, loss = 2.35828019\n",
      "Iteration 9922, loss = 1.61266048\n",
      "Iteration 9923, loss = 1.76792775\n",
      "Iteration 9924, loss = 1.73731342\n",
      "Iteration 9925, loss = 1.77268485\n",
      "Iteration 9926, loss = 1.71383176\n",
      "Iteration 9927, loss = 1.48922076\n",
      "Iteration 9928, loss = 1.44274953\n",
      "Iteration 9929, loss = 1.52905045\n",
      "Iteration 9930, loss = 1.83431358\n",
      "Iteration 9931, loss = 1.70581759\n",
      "Iteration 9932, loss = 1.84988456\n",
      "Iteration 9933, loss = 2.49106895\n",
      "Iteration 9934, loss = 2.36487704\n",
      "Iteration 9935, loss = 1.78120606\n",
      "Iteration 9936, loss = 1.73830515\n",
      "Iteration 9937, loss = 1.83819964\n",
      "Iteration 9938, loss = 1.89088031\n",
      "Iteration 9939, loss = 2.14424263\n",
      "Iteration 9940, loss = 2.82586773\n",
      "Iteration 9941, loss = 3.22844936\n",
      "Iteration 9942, loss = 3.57967300\n",
      "Iteration 9943, loss = 3.04675238\n",
      "Iteration 9944, loss = 2.00682355\n",
      "Iteration 9945, loss = 2.02158528\n",
      "Iteration 9946, loss = 1.49652842\n",
      "Iteration 9947, loss = 1.63336075\n",
      "Iteration 9948, loss = 1.51760056\n",
      "Iteration 9949, loss = 1.46835496\n",
      "Iteration 9950, loss = 1.61329813\n",
      "Iteration 9951, loss = 1.96963895\n",
      "Iteration 9952, loss = 1.96147400\n",
      "Iteration 9953, loss = 2.22229976\n",
      "Iteration 9954, loss = 1.71004207\n",
      "Iteration 9955, loss = 1.85969369\n",
      "Iteration 9956, loss = 1.92171252\n",
      "Iteration 9957, loss = 1.65644613\n",
      "Iteration 9958, loss = 2.58140071\n",
      "Iteration 9959, loss = 1.92282222\n",
      "Iteration 9960, loss = 1.92307730\n",
      "Iteration 9961, loss = 2.72743953\n",
      "Iteration 9962, loss = 1.80507429\n",
      "Iteration 9963, loss = 2.30738209\n",
      "Iteration 9964, loss = 2.27651773\n",
      "Iteration 9965, loss = 1.70397422\n",
      "Iteration 9966, loss = 1.83472090\n",
      "Iteration 9967, loss = 1.99286010\n",
      "Iteration 9968, loss = 1.84512481\n",
      "Iteration 9969, loss = 1.92153574\n",
      "Iteration 9970, loss = 1.91604530\n",
      "Iteration 9971, loss = 2.31581430\n",
      "Iteration 9972, loss = 1.83133101\n",
      "Iteration 9973, loss = 1.78870281\n",
      "Iteration 9974, loss = 1.79182365\n",
      "Iteration 9975, loss = 1.60522109\n",
      "Iteration 9976, loss = 1.68283838\n",
      "Iteration 9977, loss = 1.61222182\n",
      "Iteration 9978, loss = 1.91935137\n",
      "Iteration 9979, loss = 1.70465848\n",
      "Iteration 9980, loss = 2.16478457\n",
      "Iteration 9981, loss = 2.34174911\n",
      "Iteration 9982, loss = 1.72017079\n",
      "Iteration 9983, loss = 1.68413881\n",
      "Iteration 9984, loss = 1.80479038\n",
      "Iteration 9985, loss = 1.50809677\n",
      "Iteration 9986, loss = 1.64494776\n",
      "Iteration 9987, loss = 1.55554817\n",
      "Iteration 9988, loss = 1.58546541\n",
      "Iteration 9989, loss = 1.70794511\n",
      "Iteration 9990, loss = 1.63338631\n",
      "Iteration 9991, loss = 1.53411943\n",
      "Iteration 9992, loss = 2.10888167\n",
      "Iteration 9993, loss = 2.61508925\n",
      "Iteration 9994, loss = 2.54964302\n",
      "Iteration 9995, loss = 1.98303042\n",
      "Iteration 9996, loss = 2.16482323\n",
      "Iteration 9997, loss = 4.02763780\n",
      "Iteration 9998, loss = 3.37645182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9999, loss = 4.57013589\n",
      "Iteration 10000, loss = 2.81089393\n",
      "Iteration 10001, loss = 2.70860093\n",
      "Iteration 10002, loss = 2.15454097\n",
      "Iteration 10003, loss = 2.13897348\n",
      "Iteration 10004, loss = 2.15051741\n",
      "Iteration 10005, loss = 2.25090282\n",
      "Iteration 10006, loss = 1.94885228\n",
      "Iteration 10007, loss = 1.74265499\n",
      "Iteration 10008, loss = 2.37987177\n",
      "Iteration 10009, loss = 2.26882696\n",
      "Iteration 10010, loss = 2.25525667\n",
      "Iteration 10011, loss = 1.96421500\n",
      "Iteration 10012, loss = 2.03908073\n",
      "Iteration 10013, loss = 1.95271984\n",
      "Iteration 10014, loss = 1.78204757\n",
      "Iteration 10015, loss = 1.95380573\n",
      "Iteration 10016, loss = 3.53891497\n",
      "Iteration 10017, loss = 3.45035789\n",
      "Iteration 10018, loss = 3.91442724\n",
      "Iteration 10019, loss = 3.96506312\n",
      "Iteration 10020, loss = 2.81851935\n",
      "Iteration 10021, loss = 3.44235890\n",
      "Iteration 10022, loss = 2.21804217\n",
      "Iteration 10023, loss = 2.31638801\n",
      "Iteration 10024, loss = 1.71501064\n",
      "Iteration 10025, loss = 1.75107965\n",
      "Iteration 10026, loss = 1.64135185\n",
      "Iteration 10027, loss = 2.01113818\n",
      "Iteration 10028, loss = 2.16125013\n",
      "Iteration 10029, loss = 2.67539058\n",
      "Iteration 10030, loss = 3.21504019\n",
      "Iteration 10031, loss = 2.05996560\n",
      "Iteration 10032, loss = 1.59724632\n",
      "Iteration 10033, loss = 1.80001853\n",
      "Iteration 10034, loss = 1.63777585\n",
      "Iteration 10035, loss = 1.67329983\n",
      "Iteration 10036, loss = 1.91028560\n",
      "Iteration 10037, loss = 1.45859286\n",
      "Iteration 10038, loss = 1.49785492\n",
      "Iteration 10039, loss = 1.49232371\n",
      "Iteration 10040, loss = 1.38589393\n",
      "Iteration 10041, loss = 1.30578014\n",
      "Iteration 10042, loss = 1.53676683\n",
      "Iteration 10043, loss = 1.92747417\n",
      "Iteration 10044, loss = 1.69608361\n",
      "Iteration 10045, loss = 1.86785336\n",
      "Iteration 10046, loss = 1.51360582\n",
      "Iteration 10047, loss = 1.66112635\n",
      "Iteration 10048, loss = 1.49552544\n",
      "Iteration 10049, loss = 1.50901815\n",
      "Iteration 10050, loss = 1.42947627\n",
      "Iteration 10051, loss = 1.54625086\n",
      "Iteration 10052, loss = 1.43833989\n",
      "Iteration 10053, loss = 1.43540060\n",
      "Iteration 10054, loss = 2.29339956\n",
      "Iteration 10055, loss = 2.24180630\n",
      "Iteration 10056, loss = 1.54064393\n",
      "Iteration 10057, loss = 1.68930395\n",
      "Iteration 10058, loss = 1.90073843\n",
      "Iteration 10059, loss = 1.67576340\n",
      "Iteration 10060, loss = 1.50077273\n",
      "Iteration 10061, loss = 1.99091081\n",
      "Iteration 10062, loss = 1.57376689\n",
      "Iteration 10063, loss = 1.83192332\n",
      "Iteration 10064, loss = 1.61064738\n",
      "Iteration 10065, loss = 2.25257497\n",
      "Iteration 10066, loss = 2.00589225\n",
      "Iteration 10067, loss = 1.60409187\n",
      "Iteration 10068, loss = 1.69501716\n",
      "Iteration 10069, loss = 1.50356476\n",
      "Iteration 10070, loss = 1.38255209\n",
      "Iteration 10071, loss = 1.69056141\n",
      "Iteration 10072, loss = 1.52822660\n",
      "Iteration 10073, loss = 2.38354850\n",
      "Iteration 10074, loss = 2.39641284\n",
      "Iteration 10075, loss = 2.82629944\n",
      "Iteration 10076, loss = 1.96871859\n",
      "Iteration 10077, loss = 1.67939416\n",
      "Iteration 10078, loss = 2.51778049\n",
      "Iteration 10079, loss = 1.78595911\n",
      "Iteration 10080, loss = 2.03742135\n",
      "Iteration 10081, loss = 1.89165411\n",
      "Iteration 10082, loss = 1.97903359\n",
      "Iteration 10083, loss = 1.97897709\n",
      "Iteration 10084, loss = 2.03160644\n",
      "Iteration 10085, loss = 1.92629170\n",
      "Iteration 10086, loss = 1.72517419\n",
      "Iteration 10087, loss = 2.14010377\n",
      "Iteration 10088, loss = 2.04342370\n",
      "Iteration 10089, loss = 1.81504577\n",
      "Iteration 10090, loss = 2.04708200\n",
      "Iteration 10091, loss = 2.41738588\n",
      "Iteration 10092, loss = 3.49943665\n",
      "Iteration 10093, loss = 2.22854404\n",
      "Iteration 10094, loss = 1.88336046\n",
      "Iteration 10095, loss = 2.96986983\n",
      "Iteration 10096, loss = 3.86053398\n",
      "Iteration 10097, loss = 6.06511167\n",
      "Iteration 10098, loss = 4.14715568\n",
      "Iteration 10099, loss = 3.22025621\n",
      "Iteration 10100, loss = 2.29668871\n",
      "Iteration 10101, loss = 1.90521000\n",
      "Iteration 10102, loss = 2.00532375\n",
      "Iteration 10103, loss = 1.85793969\n",
      "Iteration 10104, loss = 1.53033236\n",
      "Iteration 10105, loss = 1.71577478\n",
      "Iteration 10106, loss = 1.50482768\n",
      "Iteration 10107, loss = 1.43258021\n",
      "Iteration 10108, loss = 1.32124663\n",
      "Iteration 10109, loss = 1.33467511\n",
      "Iteration 10110, loss = 1.32782317\n",
      "Iteration 10111, loss = 1.58367405\n",
      "Iteration 10112, loss = 1.67086900\n",
      "Iteration 10113, loss = 1.44396767\n",
      "Iteration 10114, loss = 1.70481002\n",
      "Iteration 10115, loss = 1.40920791\n",
      "Iteration 10116, loss = 1.56962606\n",
      "Iteration 10117, loss = 2.55448865\n",
      "Iteration 10118, loss = 3.33888919\n",
      "Iteration 10119, loss = 2.50009054\n",
      "Iteration 10120, loss = 1.52796044\n",
      "Iteration 10121, loss = 2.15464362\n",
      "Iteration 10122, loss = 2.30076284\n",
      "Iteration 10123, loss = 3.05422705\n",
      "Iteration 10124, loss = 2.14630718\n",
      "Iteration 10125, loss = 1.78900291\n",
      "Iteration 10126, loss = 2.05774298\n",
      "Iteration 10127, loss = 2.92165274\n",
      "Iteration 10128, loss = 2.80535442\n",
      "Iteration 10129, loss = 3.57430449\n",
      "Iteration 10130, loss = 2.71579576\n",
      "Iteration 10131, loss = 3.30496012\n",
      "Iteration 10132, loss = 3.81108778\n",
      "Iteration 10133, loss = 4.13875554\n",
      "Iteration 10134, loss = 3.19453920\n",
      "Iteration 10135, loss = 2.86160168\n",
      "Iteration 10136, loss = 3.58443274\n",
      "Iteration 10137, loss = 6.04354066\n",
      "Iteration 10138, loss = 12.15334623\n",
      "Iteration 10139, loss = 7.98328091\n",
      "Iteration 10140, loss = 5.69309616\n",
      "Iteration 10141, loss = 6.75601948\n",
      "Iteration 10142, loss = 3.83736245\n",
      "Iteration 10143, loss = 4.82033944\n",
      "Iteration 10144, loss = 2.49360152\n",
      "Iteration 10145, loss = 2.10279919\n",
      "Iteration 10146, loss = 1.86622545\n",
      "Iteration 10147, loss = 1.63106611\n",
      "Iteration 10148, loss = 1.53990594\n",
      "Iteration 10149, loss = 1.57626128\n",
      "Iteration 10150, loss = 1.91243409\n",
      "Iteration 10151, loss = 2.43609281\n",
      "Iteration 10152, loss = 2.42732204\n",
      "Iteration 10153, loss = 1.67537520\n",
      "Iteration 10154, loss = 2.13778193\n",
      "Iteration 10155, loss = 2.52413352\n",
      "Iteration 10156, loss = 2.35551997\n",
      "Iteration 10157, loss = 1.93338026\n",
      "Iteration 10158, loss = 2.02483828\n",
      "Iteration 10159, loss = 2.36204494\n",
      "Iteration 10160, loss = 2.47390147\n",
      "Iteration 10161, loss = 1.63628211\n",
      "Iteration 10162, loss = 1.95705976\n",
      "Iteration 10163, loss = 2.30363904\n",
      "Iteration 10164, loss = 2.39275016\n",
      "Iteration 10165, loss = 2.33328954\n",
      "Iteration 10166, loss = 2.71525311\n",
      "Iteration 10167, loss = 3.73531700\n",
      "Iteration 10168, loss = 4.20796081\n",
      "Iteration 10169, loss = 2.89236924\n",
      "Iteration 10170, loss = 2.46742041\n",
      "Iteration 10171, loss = 3.15759168\n",
      "Iteration 10172, loss = 3.67586034\n",
      "Iteration 10173, loss = 4.68954504\n",
      "Iteration 10174, loss = 2.16671572\n",
      "Iteration 10175, loss = 2.31860639\n",
      "Iteration 10176, loss = 2.82051830\n",
      "Iteration 10177, loss = 2.00405798\n",
      "Iteration 10178, loss = 1.61720373\n",
      "Iteration 10179, loss = 2.32874804\n",
      "Iteration 10180, loss = 1.50333131\n",
      "Iteration 10181, loss = 2.09965901\n",
      "Iteration 10182, loss = 1.76431028\n",
      "Iteration 10183, loss = 2.28777956\n",
      "Iteration 10184, loss = 2.86009294\n",
      "Iteration 10185, loss = 1.79535578\n",
      "Iteration 10186, loss = 2.62750036\n",
      "Iteration 10187, loss = 2.58096268\n",
      "Iteration 10188, loss = 2.69644295\n",
      "Iteration 10189, loss = 3.37429535\n",
      "Iteration 10190, loss = 3.26717433\n",
      "Iteration 10191, loss = 2.83494367\n",
      "Iteration 10192, loss = 3.56695035\n",
      "Iteration 10193, loss = 2.10633863\n",
      "Iteration 10194, loss = 1.78729931\n",
      "Iteration 10195, loss = 1.48639165\n",
      "Iteration 10196, loss = 1.94987787\n",
      "Iteration 10197, loss = 2.34559305\n",
      "Iteration 10198, loss = 2.21626528\n",
      "Iteration 10199, loss = 1.82848916\n",
      "Iteration 10200, loss = 1.39381103\n",
      "Iteration 10201, loss = 1.54809875\n",
      "Iteration 10202, loss = 1.18817813\n",
      "Iteration 10203, loss = 1.44726603\n",
      "Iteration 10204, loss = 1.91359956\n",
      "Iteration 10205, loss = 1.55119560\n",
      "Iteration 10206, loss = 1.57511986\n",
      "Iteration 10207, loss = 1.72218412\n",
      "Iteration 10208, loss = 1.34665383\n",
      "Iteration 10209, loss = 1.50108399\n",
      "Iteration 10210, loss = 1.38073834\n",
      "Iteration 10211, loss = 1.38477594\n",
      "Iteration 10212, loss = 1.54947725\n",
      "Iteration 10213, loss = 1.49791764\n",
      "Iteration 10214, loss = 1.31389868\n",
      "Iteration 10215, loss = 1.35160446\n",
      "Iteration 10216, loss = 1.55538396\n",
      "Iteration 10217, loss = 1.62218080\n",
      "Iteration 10218, loss = 1.32184267\n",
      "Iteration 10219, loss = 1.24369094\n",
      "Iteration 10220, loss = 1.25427790\n",
      "Iteration 10221, loss = 1.32215287\n",
      "Iteration 10222, loss = 1.40618362\n",
      "Iteration 10223, loss = 1.45398756\n",
      "Iteration 10224, loss = 1.38812490\n",
      "Iteration 10225, loss = 1.32677663\n",
      "Iteration 10226, loss = 1.64214563\n",
      "Iteration 10227, loss = 1.34243062\n",
      "Iteration 10228, loss = 1.40779345\n",
      "Iteration 10229, loss = 1.49299902\n",
      "Iteration 10230, loss = 1.22210819\n",
      "Iteration 10231, loss = 1.36116380\n",
      "Iteration 10232, loss = 1.49321451\n",
      "Iteration 10233, loss = 1.55732053\n",
      "Iteration 10234, loss = 2.08515412\n",
      "Iteration 10235, loss = 2.91217175\n",
      "Iteration 10236, loss = 3.29036519\n",
      "Iteration 10237, loss = 6.88300200\n",
      "Iteration 10238, loss = 4.11067285\n",
      "Iteration 10239, loss = 3.28244020\n",
      "Iteration 10240, loss = 1.72966956\n",
      "Iteration 10241, loss = 1.48385126\n",
      "Iteration 10242, loss = 1.46518546\n",
      "Iteration 10243, loss = 1.33445704\n",
      "Iteration 10244, loss = 1.59126736\n",
      "Iteration 10245, loss = 1.33700677\n",
      "Iteration 10246, loss = 1.41492380\n",
      "Iteration 10247, loss = 1.35341719\n",
      "Iteration 10248, loss = 1.27829168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10249, loss = 1.34709296\n",
      "Iteration 10250, loss = 1.25310498\n",
      "Iteration 10251, loss = 1.40893206\n",
      "Iteration 10252, loss = 1.31404439\n",
      "Iteration 10253, loss = 2.06339702\n",
      "Iteration 10254, loss = 2.56707696\n",
      "Iteration 10255, loss = 1.86661831\n",
      "Iteration 10256, loss = 2.24744116\n",
      "Iteration 10257, loss = 1.86146766\n",
      "Iteration 10258, loss = 2.15375767\n",
      "Iteration 10259, loss = 1.52331460\n",
      "Iteration 10260, loss = 1.53527220\n",
      "Iteration 10261, loss = 1.36445257\n",
      "Iteration 10262, loss = 1.48909048\n",
      "Iteration 10263, loss = 1.33283902\n",
      "Iteration 10264, loss = 1.30512867\n",
      "Iteration 10265, loss = 1.28658842\n",
      "Iteration 10266, loss = 1.30932237\n",
      "Iteration 10267, loss = 1.34927782\n",
      "Iteration 10268, loss = 1.29625606\n",
      "Iteration 10269, loss = 1.20438677\n",
      "Iteration 10270, loss = 1.37868300\n",
      "Iteration 10271, loss = 1.83186248\n",
      "Iteration 10272, loss = 1.97214972\n",
      "Iteration 10273, loss = 1.29815201\n",
      "Iteration 10274, loss = 1.23438152\n",
      "Iteration 10275, loss = 1.37159961\n",
      "Iteration 10276, loss = 1.71910105\n",
      "Iteration 10277, loss = 1.54086982\n",
      "Iteration 10278, loss = 1.83756689\n",
      "Iteration 10279, loss = 1.69070220\n",
      "Iteration 10280, loss = 1.85207649\n",
      "Iteration 10281, loss = 1.52639605\n",
      "Iteration 10282, loss = 2.35438706\n",
      "Iteration 10283, loss = 3.36116141\n",
      "Iteration 10284, loss = 3.20235160\n",
      "Iteration 10285, loss = 5.65455329\n",
      "Iteration 10286, loss = 4.78930207\n",
      "Iteration 10287, loss = 4.57841721\n",
      "Iteration 10288, loss = 5.73801423\n",
      "Iteration 10289, loss = 6.97315438\n",
      "Iteration 10290, loss = 3.47016844\n",
      "Iteration 10291, loss = 2.32210390\n",
      "Iteration 10292, loss = 2.37309628\n",
      "Iteration 10293, loss = 1.80944710\n",
      "Iteration 10294, loss = 1.98643121\n",
      "Iteration 10295, loss = 1.68341558\n",
      "Iteration 10296, loss = 1.60897611\n",
      "Iteration 10297, loss = 1.31291710\n",
      "Iteration 10298, loss = 1.35810818\n",
      "Iteration 10299, loss = 2.67288551\n",
      "Iteration 10300, loss = 1.98294109\n",
      "Iteration 10301, loss = 3.64463513\n",
      "Iteration 10302, loss = 2.73832063\n",
      "Iteration 10303, loss = 2.26809853\n",
      "Iteration 10304, loss = 2.73335570\n",
      "Iteration 10305, loss = 2.64191930\n",
      "Iteration 10306, loss = 2.85500897\n",
      "Iteration 10307, loss = 3.03644957\n",
      "Iteration 10308, loss = 2.72759108\n",
      "Iteration 10309, loss = 2.25936632\n",
      "Iteration 10310, loss = 2.31883720\n",
      "Iteration 10311, loss = 2.25822831\n",
      "Iteration 10312, loss = 2.35375287\n",
      "Iteration 10313, loss = 2.46969059\n",
      "Iteration 10314, loss = 4.12660392\n",
      "Iteration 10315, loss = 2.89390388\n",
      "Iteration 10316, loss = 2.05691189\n",
      "Iteration 10317, loss = 2.25178832\n",
      "Iteration 10318, loss = 2.47596615\n",
      "Iteration 10319, loss = 1.65418143\n",
      "Iteration 10320, loss = 1.80972939\n",
      "Iteration 10321, loss = 1.62844235\n",
      "Iteration 10322, loss = 2.70151951\n",
      "Iteration 10323, loss = 2.26810736\n",
      "Iteration 10324, loss = 1.80925773\n",
      "Iteration 10325, loss = 1.71932696\n",
      "Iteration 10326, loss = 2.31169357\n",
      "Iteration 10327, loss = 2.05702519\n",
      "Iteration 10328, loss = 1.76646822\n",
      "Iteration 10329, loss = 2.31569415\n",
      "Iteration 10330, loss = 1.98824588\n",
      "Iteration 10331, loss = 1.83641914\n",
      "Iteration 10332, loss = 1.38549584\n",
      "Iteration 10333, loss = 1.47267663\n",
      "Iteration 10334, loss = 1.96983817\n",
      "Iteration 10335, loss = 1.82650901\n",
      "Iteration 10336, loss = 1.74577373\n",
      "Iteration 10337, loss = 1.62708684\n",
      "Iteration 10338, loss = 1.55910985\n",
      "Iteration 10339, loss = 1.83755735\n",
      "Iteration 10340, loss = 2.30753041\n",
      "Iteration 10341, loss = 1.43742394\n",
      "Iteration 10342, loss = 1.39481377\n",
      "Iteration 10343, loss = 1.26709341\n",
      "Iteration 10344, loss = 1.26432239\n",
      "Iteration 10345, loss = 1.24040058\n",
      "Iteration 10346, loss = 1.30755234\n",
      "Iteration 10347, loss = 1.28894457\n",
      "Iteration 10348, loss = 1.38905467\n",
      "Iteration 10349, loss = 1.34683627\n",
      "Iteration 10350, loss = 1.33166369\n",
      "Iteration 10351, loss = 1.83320498\n",
      "Iteration 10352, loss = 1.38949557\n",
      "Iteration 10353, loss = 1.49680841\n",
      "Iteration 10354, loss = 1.30895806\n",
      "Iteration 10355, loss = 1.24736345\n",
      "Iteration 10356, loss = 1.35997919\n",
      "Iteration 10357, loss = 1.29659524\n",
      "Iteration 10358, loss = 1.31922978\n",
      "Iteration 10359, loss = 1.26387038\n",
      "Iteration 10360, loss = 1.39865490\n",
      "Iteration 10361, loss = 3.89161509\n",
      "Iteration 10362, loss = 1.76402608\n",
      "Iteration 10363, loss = 1.72055470\n",
      "Iteration 10364, loss = 1.72938768\n",
      "Iteration 10365, loss = 1.49713512\n",
      "Iteration 10366, loss = 1.34110980\n",
      "Iteration 10367, loss = 1.41815978\n",
      "Iteration 10368, loss = 1.22259642\n",
      "Iteration 10369, loss = 1.51070158\n",
      "Iteration 10370, loss = 1.33177615\n",
      "Iteration 10371, loss = 1.32111800\n",
      "Iteration 10372, loss = 1.51044584\n",
      "Iteration 10373, loss = 1.45657191\n",
      "Iteration 10374, loss = 1.92493312\n",
      "Iteration 10375, loss = 3.20500501\n",
      "Iteration 10376, loss = 4.77624973\n",
      "Iteration 10377, loss = 2.62718778\n",
      "Iteration 10378, loss = 1.87792362\n",
      "Iteration 10379, loss = 1.60873932\n",
      "Iteration 10380, loss = 2.04895046\n",
      "Iteration 10381, loss = 2.01658928\n",
      "Iteration 10382, loss = 2.83165771\n",
      "Iteration 10383, loss = 1.97444783\n",
      "Iteration 10384, loss = 1.92438734\n",
      "Iteration 10385, loss = 1.48658899\n",
      "Iteration 10386, loss = 1.34329235\n",
      "Iteration 10387, loss = 1.18474722\n",
      "Iteration 10388, loss = 1.26858616\n",
      "Iteration 10389, loss = 1.92840635\n",
      "Iteration 10390, loss = 2.77265906\n",
      "Iteration 10391, loss = 2.53897973\n",
      "Iteration 10392, loss = 2.26252525\n",
      "Iteration 10393, loss = 1.84245490\n",
      "Iteration 10394, loss = 2.03931795\n",
      "Iteration 10395, loss = 2.28148479\n",
      "Iteration 10396, loss = 1.53498528\n",
      "Iteration 10397, loss = 1.48867028\n",
      "Iteration 10398, loss = 1.31193327\n",
      "Iteration 10399, loss = 1.19697290\n",
      "Iteration 10400, loss = 1.37629990\n",
      "Iteration 10401, loss = 1.35133708\n",
      "Iteration 10402, loss = 1.62947720\n",
      "Iteration 10403, loss = 1.49710326\n",
      "Iteration 10404, loss = 1.64812043\n",
      "Iteration 10405, loss = 1.85063824\n",
      "Iteration 10406, loss = 1.88557645\n",
      "Iteration 10407, loss = 1.36966715\n",
      "Iteration 10408, loss = 1.99315715\n",
      "Iteration 10409, loss = 3.15800313\n",
      "Iteration 10410, loss = 2.17470198\n",
      "Iteration 10411, loss = 1.98837675\n",
      "Iteration 10412, loss = 1.96728874\n",
      "Iteration 10413, loss = 1.44784176\n",
      "Iteration 10414, loss = 1.56573606\n",
      "Iteration 10415, loss = 1.32295333\n",
      "Iteration 10416, loss = 1.33676297\n",
      "Iteration 10417, loss = 1.52813591\n",
      "Iteration 10418, loss = 1.69528317\n",
      "Iteration 10419, loss = 1.66227018\n",
      "Iteration 10420, loss = 1.82190244\n",
      "Iteration 10421, loss = 1.61846472\n",
      "Iteration 10422, loss = 1.64535787\n",
      "Iteration 10423, loss = 1.57124051\n",
      "Iteration 10424, loss = 1.35487754\n",
      "Iteration 10425, loss = 1.17300868\n",
      "Iteration 10426, loss = 1.34401057\n",
      "Iteration 10427, loss = 1.20800104\n",
      "Iteration 10428, loss = 1.49205634\n",
      "Iteration 10429, loss = 1.71268097\n",
      "Iteration 10430, loss = 1.40467116\n",
      "Iteration 10431, loss = 1.37864799\n",
      "Iteration 10432, loss = 1.44166245\n",
      "Iteration 10433, loss = 2.25383225\n",
      "Iteration 10434, loss = 2.97126464\n",
      "Iteration 10435, loss = 3.08108654\n",
      "Iteration 10436, loss = 3.14478810\n",
      "Iteration 10437, loss = 2.40736590\n",
      "Iteration 10438, loss = 1.67734472\n",
      "Iteration 10439, loss = 1.96414252\n",
      "Iteration 10440, loss = 1.61098373\n",
      "Iteration 10441, loss = 2.08098280\n",
      "Iteration 10442, loss = 1.51496151\n",
      "Iteration 10443, loss = 1.50268829\n",
      "Iteration 10444, loss = 1.29135746\n",
      "Iteration 10445, loss = 1.35075053\n",
      "Iteration 10446, loss = 1.34835065\n",
      "Iteration 10447, loss = 1.49441755\n",
      "Iteration 10448, loss = 1.34586729\n",
      "Iteration 10449, loss = 1.27861105\n",
      "Iteration 10450, loss = 1.22246963\n",
      "Iteration 10451, loss = 1.38197656\n",
      "Iteration 10452, loss = 1.28397195\n",
      "Iteration 10453, loss = 1.32767611\n",
      "Iteration 10454, loss = 1.22995429\n",
      "Iteration 10455, loss = 2.13049143\n",
      "Iteration 10456, loss = 1.85034203\n",
      "Iteration 10457, loss = 1.94429469\n",
      "Iteration 10458, loss = 1.61691168\n",
      "Iteration 10459, loss = 3.67448978\n",
      "Iteration 10460, loss = 3.21899523\n",
      "Iteration 10461, loss = 2.12393887\n",
      "Iteration 10462, loss = 1.87677000\n",
      "Iteration 10463, loss = 1.68165217\n",
      "Iteration 10464, loss = 1.67254422\n",
      "Iteration 10465, loss = 1.36116446\n",
      "Iteration 10466, loss = 1.52757072\n",
      "Iteration 10467, loss = 1.21918808\n",
      "Iteration 10468, loss = 1.38441218\n",
      "Iteration 10469, loss = 1.71865152\n",
      "Iteration 10470, loss = 1.34890762\n",
      "Iteration 10471, loss = 1.28724583\n",
      "Iteration 10472, loss = 1.41258089\n",
      "Iteration 10473, loss = 1.40222625\n",
      "Iteration 10474, loss = 1.40646035\n",
      "Iteration 10475, loss = 1.28471979\n",
      "Iteration 10476, loss = 1.17304749\n",
      "Iteration 10477, loss = 1.36947744\n",
      "Iteration 10478, loss = 1.96951439\n",
      "Iteration 10479, loss = 2.25951374\n",
      "Iteration 10480, loss = 1.53752307\n",
      "Iteration 10481, loss = 1.41952925\n",
      "Iteration 10482, loss = 1.36113055\n",
      "Iteration 10483, loss = 1.30763050\n",
      "Iteration 10484, loss = 1.99200648\n",
      "Iteration 10485, loss = 2.60288215\n",
      "Iteration 10486, loss = 2.92283923\n",
      "Iteration 10487, loss = 2.53635972\n",
      "Iteration 10488, loss = 1.80723619\n",
      "Iteration 10489, loss = 1.86133976\n",
      "Iteration 10490, loss = 1.75414949\n",
      "Iteration 10491, loss = 1.28838760\n",
      "Iteration 10492, loss = 1.64604377\n",
      "Iteration 10493, loss = 1.28736012\n",
      "Iteration 10494, loss = 1.83046539\n",
      "Iteration 10495, loss = 1.40977649\n",
      "Iteration 10496, loss = 1.40434694\n",
      "Iteration 10497, loss = 1.25973834\n",
      "Iteration 10498, loss = 1.40327737\n",
      "Iteration 10499, loss = 1.33909677\n",
      "Iteration 10500, loss = 1.16725850\n",
      "Iteration 10501, loss = 1.51332240\n",
      "Iteration 10502, loss = 2.14089734\n",
      "Iteration 10503, loss = 1.73957908\n",
      "Iteration 10504, loss = 1.52200104\n",
      "Iteration 10505, loss = 1.34794832\n",
      "Iteration 10506, loss = 2.91179705\n",
      "Iteration 10507, loss = 2.05263924\n",
      "Iteration 10508, loss = 1.81249949\n",
      "Iteration 10509, loss = 1.83391727\n",
      "Iteration 10510, loss = 1.50726108\n",
      "Iteration 10511, loss = 1.32615322\n",
      "Iteration 10512, loss = 1.58733241\n",
      "Iteration 10513, loss = 1.77832549\n",
      "Iteration 10514, loss = 1.41694498\n",
      "Iteration 10515, loss = 1.26462078\n",
      "Iteration 10516, loss = 1.54013328\n",
      "Iteration 10517, loss = 1.48185659\n",
      "Iteration 10518, loss = 1.44082494\n",
      "Iteration 10519, loss = 1.46603820\n",
      "Iteration 10520, loss = 1.44619764\n",
      "Iteration 10521, loss = 1.68711059\n",
      "Iteration 10522, loss = 1.84967097\n",
      "Iteration 10523, loss = 1.57153982\n",
      "Iteration 10524, loss = 1.28065463\n",
      "Iteration 10525, loss = 1.24420749\n",
      "Iteration 10526, loss = 1.39585128\n",
      "Iteration 10527, loss = 1.68547194\n",
      "Iteration 10528, loss = 2.46856005\n",
      "Iteration 10529, loss = 1.81761728\n",
      "Iteration 10530, loss = 1.84238416\n",
      "Iteration 10531, loss = 1.47056624\n",
      "Iteration 10532, loss = 1.82352262\n",
      "Iteration 10533, loss = 2.39806719\n",
      "Iteration 10534, loss = 1.68954478\n",
      "Iteration 10535, loss = 1.51285129\n",
      "Iteration 10536, loss = 2.12203998\n",
      "Iteration 10537, loss = 2.06665753\n",
      "Iteration 10538, loss = 2.50973529\n",
      "Iteration 10539, loss = 1.85538313\n",
      "Iteration 10540, loss = 1.56353897\n",
      "Iteration 10541, loss = 1.77635848\n",
      "Iteration 10542, loss = 1.45356732\n",
      "Iteration 10543, loss = 1.56716642\n",
      "Iteration 10544, loss = 1.74815936\n",
      "Iteration 10545, loss = 2.05098155\n",
      "Iteration 10546, loss = 2.32923590\n",
      "Iteration 10547, loss = 2.14167417\n",
      "Iteration 10548, loss = 1.96189355\n",
      "Iteration 10549, loss = 1.54358852\n",
      "Iteration 10550, loss = 1.31932377\n",
      "Iteration 10551, loss = 1.30030753\n",
      "Iteration 10552, loss = 1.26143929\n",
      "Iteration 10553, loss = 1.33726509\n",
      "Iteration 10554, loss = 1.66752869\n",
      "Iteration 10555, loss = 1.32660027\n",
      "Iteration 10556, loss = 1.34506211\n",
      "Iteration 10557, loss = 1.82370000\n",
      "Iteration 10558, loss = 3.23392046\n",
      "Iteration 10559, loss = 2.85942879\n",
      "Iteration 10560, loss = 2.06312360\n",
      "Iteration 10561, loss = 2.53447103\n",
      "Iteration 10562, loss = 4.43371649\n",
      "Iteration 10563, loss = 7.83142755\n",
      "Iteration 10564, loss = 3.42269439\n",
      "Iteration 10565, loss = 2.32654427\n",
      "Iteration 10566, loss = 2.10810267\n",
      "Iteration 10567, loss = 2.27690638\n",
      "Iteration 10568, loss = 1.91453645\n",
      "Iteration 10569, loss = 2.04289614\n",
      "Iteration 10570, loss = 1.37643360\n",
      "Iteration 10571, loss = 1.34423179\n",
      "Iteration 10572, loss = 1.28359093\n",
      "Iteration 10573, loss = 1.45896690\n",
      "Iteration 10574, loss = 1.74810076\n",
      "Iteration 10575, loss = 3.19763110\n",
      "Iteration 10576, loss = 3.36801673\n",
      "Iteration 10577, loss = 1.86772179\n",
      "Iteration 10578, loss = 3.22569947\n",
      "Iteration 10579, loss = 7.66678112\n",
      "Iteration 10580, loss = 7.97500766\n",
      "Iteration 10581, loss = 3.94868949\n",
      "Iteration 10582, loss = 3.09987307\n",
      "Iteration 10583, loss = 2.59859551\n",
      "Iteration 10584, loss = 1.75506423\n",
      "Iteration 10585, loss = 1.55677870\n",
      "Iteration 10586, loss = 1.76428553\n",
      "Iteration 10587, loss = 1.79431882\n",
      "Iteration 10588, loss = 1.52090982\n",
      "Iteration 10589, loss = 1.94877246\n",
      "Iteration 10590, loss = 1.49787072\n",
      "Iteration 10591, loss = 1.45359614\n",
      "Iteration 10592, loss = 1.58883776\n",
      "Iteration 10593, loss = 1.50857941\n",
      "Iteration 10594, loss = 1.28832044\n",
      "Iteration 10595, loss = 1.43816908\n",
      "Iteration 10596, loss = 1.48871338\n",
      "Iteration 10597, loss = 1.61748527\n",
      "Iteration 10598, loss = 2.05965829\n",
      "Iteration 10599, loss = 1.33212787\n",
      "Iteration 10600, loss = 1.40198227\n",
      "Iteration 10601, loss = 1.40945355\n",
      "Iteration 10602, loss = 1.25315912\n",
      "Iteration 10603, loss = 1.27873160\n",
      "Iteration 10604, loss = 1.35284639\n",
      "Iteration 10605, loss = 1.83943295\n",
      "Iteration 10606, loss = 2.44002092\n",
      "Iteration 10607, loss = 2.66952115\n",
      "Iteration 10608, loss = 1.39797207\n",
      "Iteration 10609, loss = 2.12221251\n",
      "Iteration 10610, loss = 1.81619677\n",
      "Iteration 10611, loss = 2.67696377\n",
      "Iteration 10612, loss = 1.40871259\n",
      "Iteration 10613, loss = 1.52015450\n",
      "Iteration 10614, loss = 1.48702260\n",
      "Iteration 10615, loss = 1.30387953\n",
      "Iteration 10616, loss = 1.35256277\n",
      "Iteration 10617, loss = 1.29742151\n",
      "Iteration 10618, loss = 1.39342828\n",
      "Iteration 10619, loss = 1.33828477\n",
      "Iteration 10620, loss = 1.23671656\n",
      "Iteration 10621, loss = 1.32181348\n",
      "Iteration 10622, loss = 1.24051552\n",
      "Iteration 10623, loss = 1.24869039\n",
      "Iteration 10624, loss = 1.19617889\n",
      "Iteration 10625, loss = 1.22302431\n",
      "Iteration 10626, loss = 1.21692693\n",
      "Iteration 10627, loss = 1.28623946\n",
      "Iteration 10628, loss = 1.28579191\n",
      "Iteration 10629, loss = 1.21861623\n",
      "Iteration 10630, loss = 1.27306116\n",
      "Iteration 10631, loss = 1.48238087\n",
      "Iteration 10632, loss = 3.39997423\n",
      "Iteration 10633, loss = 2.71244123\n",
      "Iteration 10634, loss = 2.64540456\n",
      "Iteration 10635, loss = 2.37475281\n",
      "Iteration 10636, loss = 1.87574685\n",
      "Iteration 10637, loss = 1.67946652\n",
      "Iteration 10638, loss = 1.23829328\n",
      "Iteration 10639, loss = 1.34476405\n",
      "Iteration 10640, loss = 1.48737812\n",
      "Iteration 10641, loss = 2.19116134\n",
      "Iteration 10642, loss = 2.12088908\n",
      "Iteration 10643, loss = 1.42054346\n",
      "Iteration 10644, loss = 1.53938856\n",
      "Iteration 10645, loss = 1.88366995\n",
      "Iteration 10646, loss = 2.90587391\n",
      "Iteration 10647, loss = 3.96416616\n",
      "Iteration 10648, loss = 3.86877625\n",
      "Iteration 10649, loss = 3.30930495\n",
      "Iteration 10650, loss = 3.87160270\n",
      "Iteration 10651, loss = 4.60060552\n",
      "Iteration 10652, loss = 4.20238068\n",
      "Iteration 10653, loss = 2.59927610\n",
      "Iteration 10654, loss = 3.34931995\n",
      "Iteration 10655, loss = 3.22280752\n",
      "Iteration 10656, loss = 2.88669803\n",
      "Iteration 10657, loss = 2.07964774\n",
      "Iteration 10658, loss = 1.56305494\n",
      "Iteration 10659, loss = 1.75450426\n",
      "Iteration 10660, loss = 2.10299380\n",
      "Iteration 10661, loss = 2.04477701\n",
      "Iteration 10662, loss = 1.40273443\n",
      "Iteration 10663, loss = 1.28198582\n",
      "Iteration 10664, loss = 1.44836799\n",
      "Iteration 10665, loss = 1.37440216\n",
      "Iteration 10666, loss = 1.52325380\n",
      "Iteration 10667, loss = 2.00123225\n",
      "Iteration 10668, loss = 1.64952025\n",
      "Iteration 10669, loss = 1.55640087\n",
      "Iteration 10670, loss = 1.43327046\n",
      "Iteration 10671, loss = 1.25693085\n",
      "Iteration 10672, loss = 1.37261489\n",
      "Iteration 10673, loss = 1.21773372\n",
      "Iteration 10674, loss = 1.21021080\n",
      "Iteration 10675, loss = 1.66611112\n",
      "Iteration 10676, loss = 2.48547771\n",
      "Iteration 10677, loss = 1.83316302\n",
      "Iteration 10678, loss = 1.42651687\n",
      "Iteration 10679, loss = 1.34449879\n",
      "Iteration 10680, loss = 1.84764957\n",
      "Iteration 10681, loss = 1.72131712\n",
      "Iteration 10682, loss = 1.40515456\n",
      "Iteration 10683, loss = 1.23465547\n",
      "Iteration 10684, loss = 1.16123368\n",
      "Iteration 10685, loss = 1.38036887\n",
      "Iteration 10686, loss = 1.60171955\n",
      "Iteration 10687, loss = 1.52654359\n",
      "Iteration 10688, loss = 1.55569049\n",
      "Iteration 10689, loss = 2.37862252\n",
      "Iteration 10690, loss = 1.52653133\n",
      "Iteration 10691, loss = 1.38826173\n",
      "Iteration 10692, loss = 1.60257918\n",
      "Iteration 10693, loss = 2.02938834\n",
      "Iteration 10694, loss = 1.51788323\n",
      "Iteration 10695, loss = 1.75137948\n",
      "Iteration 10696, loss = 2.21128670\n",
      "Iteration 10697, loss = 2.73078690\n",
      "Iteration 10698, loss = 2.21256124\n",
      "Iteration 10699, loss = 2.35367743\n",
      "Iteration 10700, loss = 1.67528296\n",
      "Iteration 10701, loss = 1.46483918\n",
      "Iteration 10702, loss = 1.33215341\n",
      "Iteration 10703, loss = 1.28979982\n",
      "Iteration 10704, loss = 1.27989312\n",
      "Iteration 10705, loss = 1.92620981\n",
      "Iteration 10706, loss = 1.91902268\n",
      "Iteration 10707, loss = 2.73988707\n",
      "Iteration 10708, loss = 4.01226154\n",
      "Iteration 10709, loss = 1.97522672\n",
      "Iteration 10710, loss = 4.23859961\n",
      "Iteration 10711, loss = 3.89009622\n",
      "Iteration 10712, loss = 4.80856436\n",
      "Iteration 10713, loss = 2.71280327\n",
      "Iteration 10714, loss = 3.74728655\n",
      "Iteration 10715, loss = 2.41902025\n",
      "Iteration 10716, loss = 3.51243533\n",
      "Iteration 10717, loss = 1.82465099\n",
      "Iteration 10718, loss = 1.86207029\n",
      "Iteration 10719, loss = 2.55861058\n",
      "Iteration 10720, loss = 2.36630280\n",
      "Iteration 10721, loss = 2.05194580\n",
      "Iteration 10722, loss = 2.63327735\n",
      "Iteration 10723, loss = 2.34302374\n",
      "Iteration 10724, loss = 2.99913634\n",
      "Iteration 10725, loss = 1.84629017\n",
      "Iteration 10726, loss = 1.97693512\n",
      "Iteration 10727, loss = 2.99929395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10728, loss = 2.29090422\n",
      "Iteration 10729, loss = 1.51866687\n",
      "Iteration 10730, loss = 1.59198609\n",
      "Iteration 10731, loss = 1.47020368\n",
      "Iteration 10732, loss = 1.29624103\n",
      "Iteration 10733, loss = 1.37806586\n",
      "Iteration 10734, loss = 1.30083575\n",
      "Iteration 10735, loss = 1.24237632\n",
      "Iteration 10736, loss = 1.51273857\n",
      "Iteration 10737, loss = 1.42910575\n",
      "Iteration 10738, loss = 1.81857290\n",
      "Iteration 10739, loss = 1.34208923\n",
      "Iteration 10740, loss = 2.49339822\n",
      "Iteration 10741, loss = 2.21295821\n",
      "Iteration 10742, loss = 4.10059846\n",
      "Iteration 10743, loss = 4.11365616\n",
      "Iteration 10744, loss = 2.85429026\n",
      "Iteration 10745, loss = 2.91136641\n",
      "Iteration 10746, loss = 1.44981292\n",
      "Iteration 10747, loss = 1.82850578\n",
      "Iteration 10748, loss = 2.19538475\n",
      "Iteration 10749, loss = 2.67506753\n",
      "Iteration 10750, loss = 2.00885538\n",
      "Iteration 10751, loss = 2.32822124\n",
      "Iteration 10752, loss = 1.47389543\n",
      "Iteration 10753, loss = 1.37403719\n",
      "Iteration 10754, loss = 1.15458310\n",
      "Iteration 10755, loss = 1.25315094\n",
      "Iteration 10756, loss = 1.23540920\n",
      "Iteration 10757, loss = 1.23063068\n",
      "Iteration 10758, loss = 1.18567064\n",
      "Iteration 10759, loss = 1.24609521\n",
      "Iteration 10760, loss = 1.22075552\n",
      "Iteration 10761, loss = 1.13807318\n",
      "Iteration 10762, loss = 1.19958396\n",
      "Iteration 10763, loss = 2.05129943\n",
      "Iteration 10764, loss = 1.46468489\n",
      "Iteration 10765, loss = 2.49820540\n",
      "Iteration 10766, loss = 1.98366011\n",
      "Iteration 10767, loss = 1.91211012\n",
      "Iteration 10768, loss = 1.66480607\n",
      "Iteration 10769, loss = 1.63826751\n",
      "Iteration 10770, loss = 1.36141127\n",
      "Iteration 10771, loss = 1.42406818\n",
      "Iteration 10772, loss = 1.45469822\n",
      "Iteration 10773, loss = 1.92102129\n",
      "Iteration 10774, loss = 2.45229970\n",
      "Iteration 10775, loss = 2.11384010\n",
      "Iteration 10776, loss = 1.33550306\n",
      "Iteration 10777, loss = 1.22459684\n",
      "Iteration 10778, loss = 1.13249680\n",
      "Iteration 10779, loss = 1.22602596\n",
      "Iteration 10780, loss = 1.84768982\n",
      "Iteration 10781, loss = 1.54040694\n",
      "Iteration 10782, loss = 1.34462022\n",
      "Iteration 10783, loss = 1.10093455\n",
      "Iteration 10784, loss = 1.32717004\n",
      "Iteration 10785, loss = 2.26424024\n",
      "Iteration 10786, loss = 5.65250462\n",
      "Iteration 10787, loss = 5.87652599\n",
      "Iteration 10788, loss = 4.76667107\n",
      "Iteration 10789, loss = 3.68142769\n",
      "Iteration 10790, loss = 3.87919192\n",
      "Iteration 10791, loss = 2.04300357\n",
      "Iteration 10792, loss = 1.57394432\n",
      "Iteration 10793, loss = 1.49718309\n",
      "Iteration 10794, loss = 1.31083871\n",
      "Iteration 10795, loss = 1.39929893\n",
      "Iteration 10796, loss = 1.17631165\n",
      "Iteration 10797, loss = 1.62295821\n",
      "Iteration 10798, loss = 1.64439064\n",
      "Iteration 10799, loss = 1.77626258\n",
      "Iteration 10800, loss = 1.87566924\n",
      "Iteration 10801, loss = 1.41327000\n",
      "Iteration 10802, loss = 1.27922547\n",
      "Iteration 10803, loss = 1.63831986\n",
      "Iteration 10804, loss = 1.26933322\n",
      "Iteration 10805, loss = 1.55085989\n",
      "Iteration 10806, loss = 1.73315824\n",
      "Iteration 10807, loss = 1.51556506\n",
      "Iteration 10808, loss = 1.41059761\n",
      "Iteration 10809, loss = 1.33867327\n",
      "Iteration 10810, loss = 1.50191531\n",
      "Iteration 10811, loss = 1.16528703\n",
      "Iteration 10812, loss = 1.22765943\n",
      "Iteration 10813, loss = 1.23339314\n",
      "Iteration 10814, loss = 1.16912774\n",
      "Iteration 10815, loss = 1.13992035\n",
      "Iteration 10816, loss = 1.65507480\n",
      "Iteration 10817, loss = 1.59773142\n",
      "Iteration 10818, loss = 1.56693001\n",
      "Iteration 10819, loss = 1.40331814\n",
      "Iteration 10820, loss = 1.11346111\n",
      "Iteration 10821, loss = 1.26684278\n",
      "Iteration 10822, loss = 1.26567845\n",
      "Iteration 10823, loss = 1.86569065\n",
      "Iteration 10824, loss = 1.53844984\n",
      "Iteration 10825, loss = 1.33189591\n",
      "Iteration 10826, loss = 1.37454242\n",
      "Iteration 10827, loss = 1.15768733\n",
      "Iteration 10828, loss = 1.64942665\n",
      "Iteration 10829, loss = 1.38126971\n",
      "Iteration 10830, loss = 1.15679666\n",
      "Iteration 10831, loss = 1.46681880\n",
      "Iteration 10832, loss = 1.44195987\n",
      "Iteration 10833, loss = 1.52556964\n",
      "Iteration 10834, loss = 1.21028161\n",
      "Iteration 10835, loss = 1.21600055\n",
      "Iteration 10836, loss = 1.36384719\n",
      "Iteration 10837, loss = 1.51718352\n",
      "Iteration 10838, loss = 1.44615501\n",
      "Iteration 10839, loss = 2.47458463\n",
      "Iteration 10840, loss = 1.61522554\n",
      "Iteration 10841, loss = 1.42899370\n",
      "Iteration 10842, loss = 1.90424590\n",
      "Iteration 10843, loss = 2.63709532\n",
      "Iteration 10844, loss = 2.01229558\n",
      "Iteration 10845, loss = 2.67856890\n",
      "Iteration 10846, loss = 3.87374584\n",
      "Iteration 10847, loss = 2.15488708\n",
      "Iteration 10848, loss = 2.28575643\n",
      "Iteration 10849, loss = 4.41437585\n",
      "Iteration 10850, loss = 3.08055151\n",
      "Iteration 10851, loss = 2.19371639\n",
      "Iteration 10852, loss = 1.75026009\n",
      "Iteration 10853, loss = 2.63058702\n",
      "Iteration 10854, loss = 3.16757410\n",
      "Iteration 10855, loss = 1.66430206\n",
      "Iteration 10856, loss = 1.39589999\n",
      "Iteration 10857, loss = 1.49090208\n",
      "Iteration 10858, loss = 1.34201059\n",
      "Iteration 10859, loss = 3.20630160\n",
      "Iteration 10860, loss = 5.33014442\n",
      "Iteration 10861, loss = 4.86018025\n",
      "Iteration 10862, loss = 5.18417347\n",
      "Iteration 10863, loss = 5.73498968\n",
      "Iteration 10864, loss = 7.22991295\n",
      "Iteration 10865, loss = 5.72608895\n",
      "Iteration 10866, loss = 4.11726128\n",
      "Iteration 10867, loss = 6.15716926\n",
      "Iteration 10868, loss = 3.82569930\n",
      "Iteration 10869, loss = 5.45328236\n",
      "Iteration 10870, loss = 3.95401493\n",
      "Iteration 10871, loss = 3.88234180\n",
      "Iteration 10872, loss = 1.97622370\n",
      "Iteration 10873, loss = 2.11444739\n",
      "Iteration 10874, loss = 1.45509481\n",
      "Iteration 10875, loss = 1.36988764\n",
      "Iteration 10876, loss = 2.15560824\n",
      "Iteration 10877, loss = 2.68975075\n",
      "Iteration 10878, loss = 2.24236723\n",
      "Iteration 10879, loss = 3.70276113\n",
      "Iteration 10880, loss = 1.95348543\n",
      "Iteration 10881, loss = 1.50182660\n",
      "Iteration 10882, loss = 1.66768167\n",
      "Iteration 10883, loss = 1.65830445\n",
      "Iteration 10884, loss = 1.82802219\n",
      "Iteration 10885, loss = 1.91285465\n",
      "Iteration 10886, loss = 1.43219932\n",
      "Iteration 10887, loss = 1.79886660\n",
      "Iteration 10888, loss = 1.53737219\n",
      "Iteration 10889, loss = 1.68012084\n",
      "Iteration 10890, loss = 1.47823582\n",
      "Iteration 10891, loss = 2.13602112\n",
      "Iteration 10892, loss = 1.69338475\n",
      "Iteration 10893, loss = 1.63368887\n",
      "Iteration 10894, loss = 1.51900018\n",
      "Iteration 10895, loss = 1.28132640\n",
      "Iteration 10896, loss = 1.47697859\n",
      "Iteration 10897, loss = 1.20583717\n",
      "Iteration 10898, loss = 1.09388948\n",
      "Iteration 10899, loss = 1.33198205\n",
      "Iteration 10900, loss = 1.32200175\n",
      "Iteration 10901, loss = 1.18583742\n",
      "Iteration 10902, loss = 1.11807782\n",
      "Iteration 10903, loss = 1.08529716\n",
      "Iteration 10904, loss = 1.17756330\n",
      "Iteration 10905, loss = 1.20674879\n",
      "Iteration 10906, loss = 1.46864748\n",
      "Iteration 10907, loss = 1.24913267\n",
      "Iteration 10908, loss = 1.41412958\n",
      "Iteration 10909, loss = 1.12664146\n",
      "Iteration 10910, loss = 1.14465083\n",
      "Iteration 10911, loss = 1.10487462\n",
      "Iteration 10912, loss = 1.12287802\n",
      "Iteration 10913, loss = 1.65812122\n",
      "Iteration 10914, loss = 2.20861663\n",
      "Iteration 10915, loss = 2.09030667\n",
      "Iteration 10916, loss = 1.87592048\n",
      "Iteration 10917, loss = 1.45756387\n",
      "Iteration 10918, loss = 1.47693721\n",
      "Iteration 10919, loss = 2.58963075\n",
      "Iteration 10920, loss = 2.10339543\n",
      "Iteration 10921, loss = 2.40441277\n",
      "Iteration 10922, loss = 1.52788548\n",
      "Iteration 10923, loss = 1.71973661\n",
      "Iteration 10924, loss = 1.66354419\n",
      "Iteration 10925, loss = 1.68164270\n",
      "Iteration 10926, loss = 1.56220003\n",
      "Iteration 10927, loss = 1.22298057\n",
      "Iteration 10928, loss = 1.28415030\n",
      "Iteration 10929, loss = 1.17690237\n",
      "Iteration 10930, loss = 1.28892546\n",
      "Iteration 10931, loss = 1.25818725\n",
      "Iteration 10932, loss = 1.55080058\n",
      "Iteration 10933, loss = 1.27895470\n",
      "Iteration 10934, loss = 1.24376198\n",
      "Iteration 10935, loss = 1.24724768\n",
      "Iteration 10936, loss = 1.95801593\n",
      "Iteration 10937, loss = 1.91843774\n",
      "Iteration 10938, loss = 1.27276802\n",
      "Iteration 10939, loss = 1.52040424\n",
      "Iteration 10940, loss = 1.33927556\n",
      "Iteration 10941, loss = 1.65419630\n",
      "Iteration 10942, loss = 1.41386870\n",
      "Iteration 10943, loss = 1.67582017\n",
      "Iteration 10944, loss = 1.22023160\n",
      "Iteration 10945, loss = 1.17963314\n",
      "Iteration 10946, loss = 1.27115822\n",
      "Iteration 10947, loss = 1.24171960\n",
      "Iteration 10948, loss = 1.46487827\n",
      "Iteration 10949, loss = 1.49521658\n",
      "Iteration 10950, loss = 1.60911907\n",
      "Iteration 10951, loss = 2.17730429\n",
      "Iteration 10952, loss = 1.34324196\n",
      "Iteration 10953, loss = 1.15760549\n",
      "Iteration 10954, loss = 1.42476414\n",
      "Iteration 10955, loss = 1.23331257\n",
      "Iteration 10956, loss = 1.22763437\n",
      "Iteration 10957, loss = 1.47320639\n",
      "Iteration 10958, loss = 1.25863858\n",
      "Iteration 10959, loss = 1.59380596\n",
      "Iteration 10960, loss = 1.65930135\n",
      "Iteration 10961, loss = 1.40788458\n",
      "Iteration 10962, loss = 1.60069697\n",
      "Iteration 10963, loss = 1.12086815\n",
      "Iteration 10964, loss = 1.03555648\n",
      "Iteration 10965, loss = 1.11187394\n",
      "Iteration 10966, loss = 1.03005809\n",
      "Iteration 10967, loss = 1.06039875\n",
      "Iteration 10968, loss = 1.07799308\n",
      "Iteration 10969, loss = 1.05839617\n",
      "Iteration 10970, loss = 1.08137058\n",
      "Iteration 10971, loss = 1.16933519\n",
      "Iteration 10972, loss = 1.12487336\n",
      "Iteration 10973, loss = 1.10731194\n",
      "Iteration 10974, loss = 1.10018410\n",
      "Iteration 10975, loss = 1.20906248\n",
      "Iteration 10976, loss = 1.10209267\n",
      "Iteration 10977, loss = 1.26689066\n",
      "Iteration 10978, loss = 1.13486081\n",
      "Iteration 10979, loss = 1.16631936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10980, loss = 1.50456485\n",
      "Iteration 10981, loss = 1.31719686\n",
      "Iteration 10982, loss = 1.40365473\n",
      "Iteration 10983, loss = 1.71646069\n",
      "Iteration 10984, loss = 1.86202578\n",
      "Iteration 10985, loss = 1.55561750\n",
      "Iteration 10986, loss = 1.26684153\n",
      "Iteration 10987, loss = 1.15839646\n",
      "Iteration 10988, loss = 1.07631902\n",
      "Iteration 10989, loss = 1.07400793\n",
      "Iteration 10990, loss = 1.11939500\n",
      "Iteration 10991, loss = 1.24524083\n",
      "Iteration 10992, loss = 1.41724023\n",
      "Iteration 10993, loss = 1.18529714\n",
      "Iteration 10994, loss = 1.46119196\n",
      "Iteration 10995, loss = 1.28996655\n",
      "Iteration 10996, loss = 1.17441348\n",
      "Iteration 10997, loss = 1.33111591\n",
      "Iteration 10998, loss = 1.38824966\n",
      "Iteration 10999, loss = 1.14244874\n",
      "Iteration 11000, loss = 1.20919501\n",
      "Iteration 11001, loss = 1.15531775\n",
      "Iteration 11002, loss = 1.24890412\n",
      "Iteration 11003, loss = 1.36729786\n",
      "Iteration 11004, loss = 1.13108586\n",
      "Iteration 11005, loss = 1.32096081\n",
      "Iteration 11006, loss = 1.28642074\n",
      "Iteration 11007, loss = 1.26596200\n",
      "Iteration 11008, loss = 1.46961570\n",
      "Iteration 11009, loss = 1.45905104\n",
      "Iteration 11010, loss = 1.37352311\n",
      "Iteration 11011, loss = 1.17241103\n",
      "Iteration 11012, loss = 1.15182825\n",
      "Iteration 11013, loss = 1.13591465\n",
      "Iteration 11014, loss = 1.34022682\n",
      "Iteration 11015, loss = 1.50065186\n",
      "Iteration 11016, loss = 1.15699698\n",
      "Iteration 11017, loss = 1.45276478\n",
      "Iteration 11018, loss = 1.21789287\n",
      "Iteration 11019, loss = 1.25165805\n",
      "Iteration 11020, loss = 1.23882927\n",
      "Iteration 11021, loss = 1.38763889\n",
      "Iteration 11022, loss = 1.66081876\n",
      "Iteration 11023, loss = 1.30191188\n",
      "Iteration 11024, loss = 1.22490516\n",
      "Iteration 11025, loss = 1.32946162\n",
      "Iteration 11026, loss = 1.69359282\n",
      "Iteration 11027, loss = 3.64351380\n",
      "Iteration 11028, loss = 2.53598584\n",
      "Iteration 11029, loss = 2.24795585\n",
      "Iteration 11030, loss = 2.64125965\n",
      "Iteration 11031, loss = 1.48424333\n",
      "Iteration 11032, loss = 1.54256467\n",
      "Iteration 11033, loss = 1.92722626\n",
      "Iteration 11034, loss = 2.72475852\n",
      "Iteration 11035, loss = 2.94492630\n",
      "Iteration 11036, loss = 2.39481392\n",
      "Iteration 11037, loss = 2.66879533\n",
      "Iteration 11038, loss = 3.53420495\n",
      "Iteration 11039, loss = 3.77100344\n",
      "Iteration 11040, loss = 2.65565862\n",
      "Iteration 11041, loss = 2.73239193\n",
      "Iteration 11042, loss = 1.98555152\n",
      "Iteration 11043, loss = 1.77974732\n",
      "Iteration 11044, loss = 4.10991864\n",
      "Iteration 11045, loss = 5.16551987\n",
      "Iteration 11046, loss = 2.45969400\n",
      "Iteration 11047, loss = 1.82051669\n",
      "Iteration 11048, loss = 1.66961777\n",
      "Iteration 11049, loss = 1.36264421\n",
      "Iteration 11050, loss = 1.53478311\n",
      "Iteration 11051, loss = 1.64702234\n",
      "Iteration 11052, loss = 2.52755112\n",
      "Iteration 11053, loss = 1.63256291\n",
      "Iteration 11054, loss = 1.75508760\n",
      "Iteration 11055, loss = 1.56300120\n",
      "Iteration 11056, loss = 1.15056706\n",
      "Iteration 11057, loss = 1.82977050\n",
      "Iteration 11058, loss = 1.28426527\n",
      "Iteration 11059, loss = 1.36214147\n",
      "Iteration 11060, loss = 1.32998494\n",
      "Iteration 11061, loss = 1.46265181\n",
      "Iteration 11062, loss = 1.12623296\n",
      "Iteration 11063, loss = 1.47127991\n",
      "Iteration 11064, loss = 1.42271734\n",
      "Iteration 11065, loss = 1.39352375\n",
      "Iteration 11066, loss = 1.84387008\n",
      "Iteration 11067, loss = 1.32043270\n",
      "Iteration 11068, loss = 1.46672254\n",
      "Iteration 11069, loss = 1.37856904\n",
      "Iteration 11070, loss = 1.87246682\n",
      "Iteration 11071, loss = 1.68114682\n",
      "Iteration 11072, loss = 1.35331495\n",
      "Iteration 11073, loss = 1.21265794\n",
      "Iteration 11074, loss = 1.17683335\n",
      "Iteration 11075, loss = 1.28191907\n",
      "Iteration 11076, loss = 1.37333019\n",
      "Iteration 11077, loss = 1.40180393\n",
      "Iteration 11078, loss = 1.28258867\n",
      "Iteration 11079, loss = 1.52886479\n",
      "Iteration 11080, loss = 1.30118370\n",
      "Iteration 11081, loss = 1.12648178\n",
      "Iteration 11082, loss = 1.15489311\n",
      "Iteration 11083, loss = 1.12443144\n",
      "Iteration 11084, loss = 1.30988556\n",
      "Iteration 11085, loss = 1.60248289\n",
      "Iteration 11086, loss = 1.43701786\n",
      "Iteration 11087, loss = 1.30845343\n",
      "Iteration 11088, loss = 2.07175536\n",
      "Iteration 11089, loss = 3.88865627\n",
      "Iteration 11090, loss = 1.49903548\n",
      "Iteration 11091, loss = 1.58600387\n",
      "Iteration 11092, loss = 1.71911198\n",
      "Iteration 11093, loss = 1.65628426\n",
      "Iteration 11094, loss = 1.57724791\n",
      "Iteration 11095, loss = 2.41502205\n",
      "Iteration 11096, loss = 1.81759746\n",
      "Iteration 11097, loss = 1.45819555\n",
      "Iteration 11098, loss = 1.64713794\n",
      "Iteration 11099, loss = 1.52749354\n",
      "Iteration 11100, loss = 1.32794068\n",
      "Iteration 11101, loss = 1.62519096\n",
      "Iteration 11102, loss = 1.15465100\n",
      "Iteration 11103, loss = 1.10118901\n",
      "Iteration 11104, loss = 1.13689944\n",
      "Iteration 11105, loss = 1.24500949\n",
      "Iteration 11106, loss = 1.21960160\n",
      "Iteration 11107, loss = 1.11815556\n",
      "Iteration 11108, loss = 1.48828932\n",
      "Iteration 11109, loss = 2.33940759\n",
      "Iteration 11110, loss = 3.34321615\n",
      "Iteration 11111, loss = 3.30975203\n",
      "Iteration 11112, loss = 3.14330049\n",
      "Iteration 11113, loss = 2.69519350\n",
      "Iteration 11114, loss = 4.16798000\n",
      "Iteration 11115, loss = 14.18904025\n",
      "Iteration 11116, loss = 5.96517003\n",
      "Iteration 11117, loss = 7.43041136\n",
      "Iteration 11118, loss = 6.02003978\n",
      "Iteration 11119, loss = 2.60014881\n",
      "Iteration 11120, loss = 3.56843706\n",
      "Iteration 11121, loss = 2.34065199\n",
      "Iteration 11122, loss = 2.19422844\n",
      "Iteration 11123, loss = 1.96053637\n",
      "Iteration 11124, loss = 1.85332569\n",
      "Iteration 11125, loss = 1.51074130\n",
      "Iteration 11126, loss = 1.55975048\n",
      "Iteration 11127, loss = 1.83890014\n",
      "Iteration 11128, loss = 1.65109267\n",
      "Iteration 11129, loss = 1.84804236\n",
      "Iteration 11130, loss = 1.61323973\n",
      "Iteration 11131, loss = 1.36082627\n",
      "Iteration 11132, loss = 1.59328910\n",
      "Iteration 11133, loss = 2.20352357\n",
      "Iteration 11134, loss = 1.50159415\n",
      "Iteration 11135, loss = 2.09121290\n",
      "Iteration 11136, loss = 1.73237929\n",
      "Iteration 11137, loss = 1.68317165\n",
      "Iteration 11138, loss = 1.47214811\n",
      "Iteration 11139, loss = 1.38095803\n",
      "Iteration 11140, loss = 2.62628647\n",
      "Iteration 11141, loss = 3.76114150\n",
      "Iteration 11142, loss = 2.33129668\n",
      "Iteration 11143, loss = 2.08223441\n",
      "Iteration 11144, loss = 1.63197760\n",
      "Iteration 11145, loss = 1.49701847\n",
      "Iteration 11146, loss = 1.40531244\n",
      "Iteration 11147, loss = 1.40466627\n",
      "Iteration 11148, loss = 1.22944144\n",
      "Iteration 11149, loss = 1.28708647\n",
      "Iteration 11150, loss = 1.14464771\n",
      "Iteration 11151, loss = 1.23534329\n",
      "Iteration 11152, loss = 1.14804362\n",
      "Iteration 11153, loss = 1.60461132\n",
      "Iteration 11154, loss = 1.88588736\n",
      "Iteration 11155, loss = 1.44548983\n",
      "Iteration 11156, loss = 1.37030080\n",
      "Iteration 11157, loss = 1.27746038\n",
      "Iteration 11158, loss = 1.21094341\n",
      "Iteration 11159, loss = 1.13890413\n",
      "Iteration 11160, loss = 1.70656979\n",
      "Iteration 11161, loss = 1.41199683\n",
      "Iteration 11162, loss = 1.36894785\n",
      "Iteration 11163, loss = 2.06378872\n",
      "Iteration 11164, loss = 3.27650288\n",
      "Iteration 11165, loss = 6.33311177\n",
      "Iteration 11166, loss = 4.66677034\n",
      "Iteration 11167, loss = 2.96131858\n",
      "Iteration 11168, loss = 3.33592831\n",
      "Iteration 11169, loss = 4.26758982\n",
      "Iteration 11170, loss = 2.75192667\n",
      "Iteration 11171, loss = 2.64532192\n",
      "Iteration 11172, loss = 4.91030108\n",
      "Iteration 11173, loss = 4.79695641\n",
      "Iteration 11174, loss = 4.50867089\n",
      "Iteration 11175, loss = 4.39803541\n",
      "Iteration 11176, loss = 4.04537002\n",
      "Iteration 11177, loss = 3.72731302\n",
      "Iteration 11178, loss = 2.97587528\n",
      "Iteration 11179, loss = 4.17377887\n",
      "Iteration 11180, loss = 2.53842078\n",
      "Iteration 11181, loss = 1.76809956\n",
      "Iteration 11182, loss = 1.41090704\n",
      "Iteration 11183, loss = 1.36722549\n",
      "Iteration 11184, loss = 2.04650658\n",
      "Iteration 11185, loss = 1.39588446\n",
      "Iteration 11186, loss = 1.65125238\n",
      "Iteration 11187, loss = 1.24151738\n",
      "Iteration 11188, loss = 1.59007315\n",
      "Iteration 11189, loss = 1.65395377\n",
      "Iteration 11190, loss = 1.77845254\n",
      "Iteration 11191, loss = 1.32764280\n",
      "Iteration 11192, loss = 1.38383803\n",
      "Iteration 11193, loss = 1.34851086\n",
      "Iteration 11194, loss = 1.17521530\n",
      "Iteration 11195, loss = 1.10706706\n",
      "Iteration 11196, loss = 1.24209384\n",
      "Iteration 11197, loss = 1.14595587\n",
      "Iteration 11198, loss = 1.08925452\n",
      "Iteration 11199, loss = 1.21075546\n",
      "Iteration 11200, loss = 1.91238940\n",
      "Iteration 11201, loss = 1.43611324\n",
      "Iteration 11202, loss = 1.73939105\n",
      "Iteration 11203, loss = 1.57306509\n",
      "Iteration 11204, loss = 1.53376838\n",
      "Iteration 11205, loss = 1.65016951\n",
      "Iteration 11206, loss = 1.37761345\n",
      "Iteration 11207, loss = 1.42745405\n",
      "Iteration 11208, loss = 1.39579549\n",
      "Iteration 11209, loss = 1.40345757\n",
      "Iteration 11210, loss = 1.46488570\n",
      "Iteration 11211, loss = 1.69984535\n",
      "Iteration 11212, loss = 1.96339556\n",
      "Iteration 11213, loss = 1.65038129\n",
      "Iteration 11214, loss = 1.66138400\n",
      "Iteration 11215, loss = 1.24949350\n",
      "Iteration 11216, loss = 1.74532308\n",
      "Iteration 11217, loss = 1.47945478\n",
      "Iteration 11218, loss = 2.01932862\n",
      "Iteration 11219, loss = 1.72541049\n",
      "Iteration 11220, loss = 1.66429677\n",
      "Iteration 11221, loss = 1.21979005\n",
      "Iteration 11222, loss = 1.14337786\n",
      "Iteration 11223, loss = 1.19403913\n",
      "Iteration 11224, loss = 1.09695828\n",
      "Iteration 11225, loss = 1.06761230\n",
      "Iteration 11226, loss = 1.07108296\n",
      "Iteration 11227, loss = 1.27662653\n",
      "Iteration 11228, loss = 1.42325938\n",
      "Iteration 11229, loss = 1.17248616\n",
      "Iteration 11230, loss = 1.24841536\n",
      "Iteration 11231, loss = 1.18339643\n",
      "Iteration 11232, loss = 1.20680427\n",
      "Iteration 11233, loss = 1.11923412\n",
      "Iteration 11234, loss = 1.26917513\n",
      "Iteration 11235, loss = 1.36937542\n",
      "Iteration 11236, loss = 1.13277914\n",
      "Iteration 11237, loss = 1.05717689\n",
      "Iteration 11238, loss = 1.18899024\n",
      "Iteration 11239, loss = 1.09810016\n",
      "Iteration 11240, loss = 1.07842997\n",
      "Iteration 11241, loss = 1.09602810\n",
      "Iteration 11242, loss = 1.10295183\n",
      "Iteration 11243, loss = 1.04109478\n",
      "Iteration 11244, loss = 1.15539831\n",
      "Iteration 11245, loss = 1.19547489\n",
      "Iteration 11246, loss = 1.51793188\n",
      "Iteration 11247, loss = 1.43016471\n",
      "Iteration 11248, loss = 1.19716373\n",
      "Iteration 11249, loss = 1.57005020\n",
      "Iteration 11250, loss = 1.77238184\n",
      "Iteration 11251, loss = 1.91836701\n",
      "Iteration 11252, loss = 1.52506028\n",
      "Iteration 11253, loss = 1.46344047\n",
      "Iteration 11254, loss = 1.08426402\n",
      "Iteration 11255, loss = 1.12024999\n",
      "Iteration 11256, loss = 1.29195668\n",
      "Iteration 11257, loss = 1.33772688\n",
      "Iteration 11258, loss = 1.23628856\n",
      "Iteration 11259, loss = 1.13839708\n",
      "Iteration 11260, loss = 1.81582498\n",
      "Iteration 11261, loss = 1.70126479\n",
      "Iteration 11262, loss = 2.46606328\n",
      "Iteration 11263, loss = 3.62649164\n",
      "Iteration 11264, loss = 1.88527173\n",
      "Iteration 11265, loss = 1.55883825\n",
      "Iteration 11266, loss = 1.99295115\n",
      "Iteration 11267, loss = 1.41698072\n",
      "Iteration 11268, loss = 2.04834729\n",
      "Iteration 11269, loss = 3.78728702\n",
      "Iteration 11270, loss = 4.57223143\n",
      "Iteration 11271, loss = 3.64157474\n",
      "Iteration 11272, loss = 6.13858316\n",
      "Iteration 11273, loss = 4.14090803\n",
      "Iteration 11274, loss = 1.93756346\n",
      "Iteration 11275, loss = 1.61808986\n",
      "Iteration 11276, loss = 2.75335463\n",
      "Iteration 11277, loss = 2.74450373\n",
      "Iteration 11278, loss = 3.31437763\n",
      "Iteration 11279, loss = 2.86873279\n",
      "Iteration 11280, loss = 2.23330977\n",
      "Iteration 11281, loss = 1.60356639\n",
      "Iteration 11282, loss = 1.18769960\n",
      "Iteration 11283, loss = 1.33640889\n",
      "Iteration 11284, loss = 1.17917875\n",
      "Iteration 11285, loss = 1.27032516\n",
      "Iteration 11286, loss = 1.28895676\n",
      "Iteration 11287, loss = 1.63728009\n",
      "Iteration 11288, loss = 1.35258789\n",
      "Iteration 11289, loss = 1.33751754\n",
      "Iteration 11290, loss = 2.25215830\n",
      "Iteration 11291, loss = 3.44894355\n",
      "Iteration 11292, loss = 2.05020810\n",
      "Iteration 11293, loss = 1.36206526\n",
      "Iteration 11294, loss = 1.45024455\n",
      "Iteration 11295, loss = 1.45857263\n",
      "Iteration 11296, loss = 2.01249843\n",
      "Iteration 11297, loss = 1.14743080\n",
      "Iteration 11298, loss = 1.12572910\n",
      "Iteration 11299, loss = 1.07920146\n",
      "Iteration 11300, loss = 1.04487270\n",
      "Iteration 11301, loss = 1.03871245\n",
      "Iteration 11302, loss = 1.30459304\n",
      "Iteration 11303, loss = 1.17846823\n",
      "Iteration 11304, loss = 1.55884246\n",
      "Iteration 11305, loss = 1.61149738\n",
      "Iteration 11306, loss = 1.61641236\n",
      "Iteration 11307, loss = 1.39887241\n",
      "Iteration 11308, loss = 2.16740868\n",
      "Iteration 11309, loss = 1.62378473\n",
      "Iteration 11310, loss = 2.35398743\n",
      "Iteration 11311, loss = 2.19478107\n",
      "Iteration 11312, loss = 1.37146472\n",
      "Iteration 11313, loss = 1.33354555\n",
      "Iteration 11314, loss = 1.49826630\n",
      "Iteration 11315, loss = 1.25172814\n",
      "Iteration 11316, loss = 1.26498062\n",
      "Iteration 11317, loss = 1.15362604\n",
      "Iteration 11318, loss = 1.27838669\n",
      "Iteration 11319, loss = 1.14544466\n",
      "Iteration 11320, loss = 1.20910711\n",
      "Iteration 11321, loss = 1.11695207\n",
      "Iteration 11322, loss = 1.05522179\n",
      "Iteration 11323, loss = 0.99486945\n",
      "Iteration 11324, loss = 1.11051357\n",
      "Iteration 11325, loss = 1.51877695\n",
      "Iteration 11326, loss = 1.71612877\n",
      "Iteration 11327, loss = 2.20710433\n",
      "Iteration 11328, loss = 1.35680594\n",
      "Iteration 11329, loss = 1.43423347\n",
      "Iteration 11330, loss = 2.09236802\n",
      "Iteration 11331, loss = 2.74638300\n",
      "Iteration 11332, loss = 1.83281364\n",
      "Iteration 11333, loss = 2.19211060\n",
      "Iteration 11334, loss = 1.62002350\n",
      "Iteration 11335, loss = 1.09324165\n",
      "Iteration 11336, loss = 1.33338643\n",
      "Iteration 11337, loss = 1.48812541\n",
      "Iteration 11338, loss = 1.63895223\n",
      "Iteration 11339, loss = 1.16218128\n",
      "Iteration 11340, loss = 1.23906222\n",
      "Iteration 11341, loss = 1.18549806\n",
      "Iteration 11342, loss = 1.25940267\n",
      "Iteration 11343, loss = 1.34271830\n",
      "Iteration 11344, loss = 1.21077601\n",
      "Iteration 11345, loss = 1.07095468\n",
      "Iteration 11346, loss = 1.15801090\n",
      "Iteration 11347, loss = 1.67080575\n",
      "Iteration 11348, loss = 2.07859526\n",
      "Iteration 11349, loss = 1.77456143\n",
      "Iteration 11350, loss = 1.40171443\n",
      "Iteration 11351, loss = 1.36611007\n",
      "Iteration 11352, loss = 1.11063917\n",
      "Iteration 11353, loss = 1.22453639\n",
      "Iteration 11354, loss = 1.15974554\n",
      "Iteration 11355, loss = 1.04587464\n",
      "Iteration 11356, loss = 1.05339635\n",
      "Iteration 11357, loss = 1.09674380\n",
      "Iteration 11358, loss = 1.36401714\n",
      "Iteration 11359, loss = 1.11671508\n",
      "Iteration 11360, loss = 1.62785808\n",
      "Iteration 11361, loss = 1.38160595\n",
      "Iteration 11362, loss = 1.37835934\n",
      "Iteration 11363, loss = 1.12662994\n",
      "Iteration 11364, loss = 1.32534457\n",
      "Iteration 11365, loss = 1.03403246\n",
      "Iteration 11366, loss = 1.31866035\n",
      "Iteration 11367, loss = 1.31409915\n",
      "Iteration 11368, loss = 1.11041890\n",
      "Iteration 11369, loss = 1.00558359\n",
      "Iteration 11370, loss = 1.14072515\n",
      "Iteration 11371, loss = 1.15032612\n",
      "Iteration 11372, loss = 1.07467771\n",
      "Iteration 11373, loss = 1.01864130\n",
      "Iteration 11374, loss = 1.45959186\n",
      "Iteration 11375, loss = 1.05222759\n",
      "Iteration 11376, loss = 1.12501551\n",
      "Iteration 11377, loss = 1.59066572\n",
      "Iteration 11378, loss = 3.29817425\n",
      "Iteration 11379, loss = 5.77940782\n",
      "Iteration 11380, loss = 5.09901646\n",
      "Iteration 11381, loss = 3.12743961\n",
      "Iteration 11382, loss = 2.55111976\n",
      "Iteration 11383, loss = 2.38685785\n",
      "Iteration 11384, loss = 1.50514838\n",
      "Iteration 11385, loss = 1.38301098\n",
      "Iteration 11386, loss = 1.32749349\n",
      "Iteration 11387, loss = 2.07348370\n",
      "Iteration 11388, loss = 2.43855881\n",
      "Iteration 11389, loss = 2.55162202\n",
      "Iteration 11390, loss = 2.26705719\n",
      "Iteration 11391, loss = 1.91847303\n",
      "Iteration 11392, loss = 1.87536986\n",
      "Iteration 11393, loss = 2.02936766\n",
      "Iteration 11394, loss = 1.94475762\n",
      "Iteration 11395, loss = 1.47612694\n",
      "Iteration 11396, loss = 1.28322062\n",
      "Iteration 11397, loss = 1.53518163\n",
      "Iteration 11398, loss = 2.88820999\n",
      "Iteration 11399, loss = 2.92739795\n",
      "Iteration 11400, loss = 1.91758877\n",
      "Iteration 11401, loss = 1.78090596\n",
      "Iteration 11402, loss = 1.41989585\n",
      "Iteration 11403, loss = 1.29813078\n",
      "Iteration 11404, loss = 1.64558621\n",
      "Iteration 11405, loss = 1.42510722\n",
      "Iteration 11406, loss = 1.27163262\n",
      "Iteration 11407, loss = 1.15871834\n",
      "Iteration 11408, loss = 1.06950484\n",
      "Iteration 11409, loss = 1.07660266\n",
      "Iteration 11410, loss = 1.12463923\n",
      "Iteration 11411, loss = 1.33315267\n",
      "Iteration 11412, loss = 1.41433872\n",
      "Iteration 11413, loss = 1.27444572\n",
      "Iteration 11414, loss = 1.29825967\n",
      "Iteration 11415, loss = 1.66840261\n",
      "Iteration 11416, loss = 2.03646167\n",
      "Iteration 11417, loss = 2.34174992\n",
      "Iteration 11418, loss = 2.59042980\n",
      "Iteration 11419, loss = 2.26915658\n",
      "Iteration 11420, loss = 1.80906075\n",
      "Iteration 11421, loss = 1.99312000\n",
      "Iteration 11422, loss = 1.61257777\n",
      "Iteration 11423, loss = 1.80332079\n",
      "Iteration 11424, loss = 1.78340361\n",
      "Iteration 11425, loss = 1.66881083\n",
      "Iteration 11426, loss = 1.63972084\n",
      "Iteration 11427, loss = 1.94053175\n",
      "Iteration 11428, loss = 1.81596668\n",
      "Iteration 11429, loss = 1.91221754\n",
      "Iteration 11430, loss = 2.91671352\n",
      "Iteration 11431, loss = 2.93566793\n",
      "Iteration 11432, loss = 3.26667839\n",
      "Iteration 11433, loss = 1.88560745\n",
      "Iteration 11434, loss = 1.72737021\n",
      "Iteration 11435, loss = 1.21186744\n",
      "Iteration 11436, loss = 1.22127909\n",
      "Iteration 11437, loss = 1.51329967\n",
      "Iteration 11438, loss = 1.35055417\n",
      "Iteration 11439, loss = 1.24221524\n",
      "Iteration 11440, loss = 1.08766005\n",
      "Iteration 11441, loss = 1.31978221\n",
      "Iteration 11442, loss = 0.99788917\n",
      "Iteration 11443, loss = 1.13989319\n",
      "Iteration 11444, loss = 1.11122274\n",
      "Iteration 11445, loss = 1.22682817\n",
      "Iteration 11446, loss = 1.12226412\n",
      "Iteration 11447, loss = 1.17033469\n",
      "Iteration 11448, loss = 1.06126439\n",
      "Iteration 11449, loss = 1.07927069\n",
      "Iteration 11450, loss = 1.33085907\n",
      "Iteration 11451, loss = 1.39770473\n",
      "Iteration 11452, loss = 1.38961183\n",
      "Iteration 11453, loss = 1.24807776\n",
      "Iteration 11454, loss = 1.70322458\n",
      "Iteration 11455, loss = 1.71079571\n",
      "Iteration 11456, loss = 1.32886967\n",
      "Iteration 11457, loss = 1.06690156\n",
      "Iteration 11458, loss = 1.03364435\n",
      "Iteration 11459, loss = 1.02971477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11460, loss = 1.06057506\n",
      "Iteration 11461, loss = 1.06130840\n",
      "Iteration 11462, loss = 0.98947598\n",
      "Iteration 11463, loss = 1.14387823\n",
      "Iteration 11464, loss = 1.40107097\n",
      "Iteration 11465, loss = 1.47106607\n",
      "Iteration 11466, loss = 1.65230886\n",
      "Iteration 11467, loss = 1.44429331\n",
      "Iteration 11468, loss = 1.35577114\n",
      "Iteration 11469, loss = 1.80099480\n",
      "Iteration 11470, loss = 3.07204379\n",
      "Iteration 11471, loss = 2.12818078\n",
      "Iteration 11472, loss = 2.49275138\n",
      "Iteration 11473, loss = 2.40526036\n",
      "Iteration 11474, loss = 3.18422233\n",
      "Iteration 11475, loss = 3.33051781\n",
      "Iteration 11476, loss = 2.43299739\n",
      "Iteration 11477, loss = 1.62092012\n",
      "Iteration 11478, loss = 1.66954809\n",
      "Iteration 11479, loss = 1.38167066\n",
      "Iteration 11480, loss = 1.63335964\n",
      "Iteration 11481, loss = 1.59475583\n",
      "Iteration 11482, loss = 1.27377309\n",
      "Iteration 11483, loss = 1.19517798\n",
      "Iteration 11484, loss = 1.13666737\n",
      "Iteration 11485, loss = 1.11824349\n",
      "Iteration 11486, loss = 1.05810330\n",
      "Iteration 11487, loss = 1.02513508\n",
      "Iteration 11488, loss = 0.98185567\n",
      "Iteration 11489, loss = 1.49307386\n",
      "Iteration 11490, loss = 1.65903094\n",
      "Iteration 11491, loss = 1.58387078\n",
      "Iteration 11492, loss = 1.26344049\n",
      "Iteration 11493, loss = 1.11592508\n",
      "Iteration 11494, loss = 1.05777514\n",
      "Iteration 11495, loss = 1.03372008\n",
      "Iteration 11496, loss = 1.10550706\n",
      "Iteration 11497, loss = 1.02464936\n",
      "Iteration 11498, loss = 1.01469517\n",
      "Iteration 11499, loss = 1.08223401\n",
      "Iteration 11500, loss = 1.02453459\n",
      "Iteration 11501, loss = 1.18716102\n",
      "Iteration 11502, loss = 1.45076416\n",
      "Iteration 11503, loss = 1.46926537\n",
      "Iteration 11504, loss = 1.41927262\n",
      "Iteration 11505, loss = 1.35390646\n",
      "Iteration 11506, loss = 1.29147648\n",
      "Iteration 11507, loss = 1.36806734\n",
      "Iteration 11508, loss = 1.16352629\n",
      "Iteration 11509, loss = 1.78148332\n",
      "Iteration 11510, loss = 1.77346185\n",
      "Iteration 11511, loss = 1.47921671\n",
      "Iteration 11512, loss = 1.35409925\n",
      "Iteration 11513, loss = 1.48541551\n",
      "Iteration 11514, loss = 1.46189771\n",
      "Iteration 11515, loss = 1.57797063\n",
      "Iteration 11516, loss = 1.36604767\n",
      "Iteration 11517, loss = 1.13359584\n",
      "Iteration 11518, loss = 1.47132852\n",
      "Iteration 11519, loss = 1.79260249\n",
      "Iteration 11520, loss = 1.93072686\n",
      "Iteration 11521, loss = 2.14287580\n",
      "Iteration 11522, loss = 1.90734894\n",
      "Iteration 11523, loss = 1.32992817\n",
      "Iteration 11524, loss = 1.15196244\n",
      "Iteration 11525, loss = 1.00130860\n",
      "Iteration 11526, loss = 0.98158080\n",
      "Iteration 11527, loss = 1.09745874\n",
      "Iteration 11528, loss = 1.17000362\n",
      "Iteration 11529, loss = 1.19440317\n",
      "Iteration 11530, loss = 1.03261245\n",
      "Iteration 11531, loss = 1.03371783\n",
      "Iteration 11532, loss = 1.14099473\n",
      "Iteration 11533, loss = 1.08390070\n",
      "Iteration 11534, loss = 1.10175368\n",
      "Iteration 11535, loss = 1.12867088\n",
      "Iteration 11536, loss = 1.05893516\n",
      "Iteration 11537, loss = 1.10445448\n",
      "Iteration 11538, loss = 1.22297852\n",
      "Iteration 11539, loss = 1.13497483\n",
      "Iteration 11540, loss = 1.31357081\n",
      "Iteration 11541, loss = 1.08884054\n",
      "Iteration 11542, loss = 1.01368072\n",
      "Iteration 11543, loss = 1.00881640\n",
      "Iteration 11544, loss = 0.96553280\n",
      "Iteration 11545, loss = 1.07410734\n",
      "Iteration 11546, loss = 1.00253503\n",
      "Iteration 11547, loss = 1.00063253\n",
      "Iteration 11548, loss = 1.13585287\n",
      "Iteration 11549, loss = 1.00629513\n",
      "Iteration 11550, loss = 1.14428546\n",
      "Iteration 11551, loss = 1.49474326\n",
      "Iteration 11552, loss = 1.86561948\n",
      "Iteration 11553, loss = 1.84866363\n",
      "Iteration 11554, loss = 1.32231734\n",
      "Iteration 11555, loss = 1.51455032\n",
      "Iteration 11556, loss = 1.58644185\n",
      "Iteration 11557, loss = 1.25705572\n",
      "Iteration 11558, loss = 1.28640000\n",
      "Iteration 11559, loss = 2.03797987\n",
      "Iteration 11560, loss = 1.72232506\n",
      "Iteration 11561, loss = 1.34544485\n",
      "Iteration 11562, loss = 1.31681983\n",
      "Iteration 11563, loss = 1.26055146\n",
      "Iteration 11564, loss = 1.67779069\n",
      "Iteration 11565, loss = 1.11330734\n",
      "Iteration 11566, loss = 1.25519971\n",
      "Iteration 11567, loss = 1.22891780\n",
      "Iteration 11568, loss = 2.00807291\n",
      "Iteration 11569, loss = 1.87520576\n",
      "Iteration 11570, loss = 1.19129956\n",
      "Iteration 11571, loss = 1.26412763\n",
      "Iteration 11572, loss = 1.47625838\n",
      "Iteration 11573, loss = 1.79313855\n",
      "Iteration 11574, loss = 1.64296010\n",
      "Iteration 11575, loss = 1.68231509\n",
      "Iteration 11576, loss = 1.34447373\n",
      "Iteration 11577, loss = 1.68516889\n",
      "Iteration 11578, loss = 2.20432901\n",
      "Iteration 11579, loss = 2.40925351\n",
      "Iteration 11580, loss = 2.05476910\n",
      "Iteration 11581, loss = 2.50698714\n",
      "Iteration 11582, loss = 2.67259025\n",
      "Iteration 11583, loss = 1.79560248\n",
      "Iteration 11584, loss = 1.81510358\n",
      "Iteration 11585, loss = 1.56640268\n",
      "Iteration 11586, loss = 1.82619152\n",
      "Iteration 11587, loss = 1.57130950\n",
      "Iteration 11588, loss = 3.15025193\n",
      "Iteration 11589, loss = 1.85181318\n",
      "Iteration 11590, loss = 1.41642730\n",
      "Iteration 11591, loss = 1.84079126\n",
      "Iteration 11592, loss = 1.90655116\n",
      "Iteration 11593, loss = 1.54326514\n",
      "Iteration 11594, loss = 1.21768778\n",
      "Iteration 11595, loss = 1.45485949\n",
      "Iteration 11596, loss = 1.05743013\n",
      "Iteration 11597, loss = 1.06278606\n",
      "Iteration 11598, loss = 1.22162094\n",
      "Iteration 11599, loss = 1.12287477\n",
      "Iteration 11600, loss = 1.23311044\n",
      "Iteration 11601, loss = 1.55858872\n",
      "Iteration 11602, loss = 1.01420270\n",
      "Iteration 11603, loss = 1.09496194\n",
      "Iteration 11604, loss = 1.20781354\n",
      "Iteration 11605, loss = 1.12941814\n",
      "Iteration 11606, loss = 1.11524033\n",
      "Iteration 11607, loss = 1.01359169\n",
      "Iteration 11608, loss = 1.27981577\n",
      "Iteration 11609, loss = 2.03596501\n",
      "Iteration 11610, loss = 2.77656341\n",
      "Iteration 11611, loss = 2.00448612\n",
      "Iteration 11612, loss = 1.94603029\n",
      "Iteration 11613, loss = 3.24946012\n",
      "Iteration 11614, loss = 2.78332020\n",
      "Iteration 11615, loss = 2.50268280\n",
      "Iteration 11616, loss = 3.08078277\n",
      "Iteration 11617, loss = 1.40608235\n",
      "Iteration 11618, loss = 1.39847555\n",
      "Iteration 11619, loss = 1.47217174\n",
      "Iteration 11620, loss = 1.32462264\n",
      "Iteration 11621, loss = 0.97732999\n",
      "Iteration 11622, loss = 1.24388199\n",
      "Iteration 11623, loss = 1.03643258\n",
      "Iteration 11624, loss = 1.04798278\n",
      "Iteration 11625, loss = 1.04973585\n",
      "Iteration 11626, loss = 1.00822723\n",
      "Iteration 11627, loss = 1.13832855\n",
      "Iteration 11628, loss = 1.17095852\n",
      "Iteration 11629, loss = 1.22633359\n",
      "Iteration 11630, loss = 1.09796852\n",
      "Iteration 11631, loss = 1.90976481\n",
      "Iteration 11632, loss = 1.95689734\n",
      "Iteration 11633, loss = 2.04254228\n",
      "Iteration 11634, loss = 1.53185769\n",
      "Iteration 11635, loss = 1.29643553\n",
      "Iteration 11636, loss = 1.71632073\n",
      "Iteration 11637, loss = 1.11710347\n",
      "Iteration 11638, loss = 1.34348717\n",
      "Iteration 11639, loss = 1.09930409\n",
      "Iteration 11640, loss = 1.10166758\n",
      "Iteration 11641, loss = 1.49012924\n",
      "Iteration 11642, loss = 1.73242311\n",
      "Iteration 11643, loss = 2.08537359\n",
      "Iteration 11644, loss = 2.94405820\n",
      "Iteration 11645, loss = 2.65057037\n",
      "Iteration 11646, loss = 1.50481644\n",
      "Iteration 11647, loss = 1.30274860\n",
      "Iteration 11648, loss = 1.16564119\n",
      "Iteration 11649, loss = 1.48566505\n",
      "Iteration 11650, loss = 1.94707653\n",
      "Iteration 11651, loss = 2.38245982\n",
      "Iteration 11652, loss = 3.24897375\n",
      "Iteration 11653, loss = 3.81800360\n",
      "Iteration 11654, loss = 1.85070799\n",
      "Iteration 11655, loss = 2.61988803\n",
      "Iteration 11656, loss = 1.32110378\n",
      "Iteration 11657, loss = 1.31344120\n",
      "Iteration 11658, loss = 1.20004567\n",
      "Iteration 11659, loss = 1.92456041\n",
      "Iteration 11660, loss = 1.68118098\n",
      "Iteration 11661, loss = 2.59537821\n",
      "Iteration 11662, loss = 2.20258374\n",
      "Iteration 11663, loss = 2.52036180\n",
      "Iteration 11664, loss = 2.04200803\n",
      "Iteration 11665, loss = 1.93266992\n",
      "Iteration 11666, loss = 1.53735826\n",
      "Iteration 11667, loss = 1.41168678\n",
      "Iteration 11668, loss = 1.28248952\n",
      "Iteration 11669, loss = 1.22954687\n",
      "Iteration 11670, loss = 1.31002293\n",
      "Iteration 11671, loss = 1.04729687\n",
      "Iteration 11672, loss = 1.12116910\n",
      "Iteration 11673, loss = 1.49715736\n",
      "Iteration 11674, loss = 1.20143081\n",
      "Iteration 11675, loss = 1.21412874\n",
      "Iteration 11676, loss = 1.65119162\n",
      "Iteration 11677, loss = 1.36312706\n",
      "Iteration 11678, loss = 1.12801423\n",
      "Iteration 11679, loss = 1.09421274\n",
      "Iteration 11680, loss = 1.12007652\n",
      "Iteration 11681, loss = 0.99975435\n",
      "Iteration 11682, loss = 1.00846751\n",
      "Iteration 11683, loss = 0.98076629\n",
      "Iteration 11684, loss = 1.08076254\n",
      "Iteration 11685, loss = 1.11616677\n",
      "Iteration 11686, loss = 1.89038831\n",
      "Iteration 11687, loss = 2.32023006\n",
      "Iteration 11688, loss = 1.67152308\n",
      "Iteration 11689, loss = 1.20141912\n",
      "Iteration 11690, loss = 1.38924155\n",
      "Iteration 11691, loss = 1.13790657\n",
      "Iteration 11692, loss = 1.02050486\n",
      "Iteration 11693, loss = 1.48711058\n",
      "Iteration 11694, loss = 1.28128389\n",
      "Iteration 11695, loss = 1.62932312\n",
      "Iteration 11696, loss = 2.43968819\n",
      "Iteration 11697, loss = 2.77865452\n",
      "Iteration 11698, loss = 3.76327396\n",
      "Iteration 11699, loss = 3.73184154\n",
      "Iteration 11700, loss = 1.78674455\n",
      "Iteration 11701, loss = 1.46888032\n",
      "Iteration 11702, loss = 1.28860626\n",
      "Iteration 11703, loss = 1.71179011\n",
      "Iteration 11704, loss = 1.53525085\n",
      "Iteration 11705, loss = 1.16272055\n",
      "Iteration 11706, loss = 1.32529547\n",
      "Iteration 11707, loss = 1.19910927\n",
      "Iteration 11708, loss = 1.68137898\n",
      "Iteration 11709, loss = 1.41545100\n",
      "Iteration 11710, loss = 1.11409172\n",
      "Iteration 11711, loss = 1.04423386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11712, loss = 1.10463248\n",
      "Iteration 11713, loss = 1.11318885\n",
      "Iteration 11714, loss = 1.09118705\n",
      "Iteration 11715, loss = 0.97832572\n",
      "Iteration 11716, loss = 1.04125672\n",
      "Iteration 11717, loss = 1.56112128\n",
      "Iteration 11718, loss = 3.51229370\n",
      "Iteration 11719, loss = 6.46832356\n",
      "Iteration 11720, loss = 5.99952157\n",
      "Iteration 11721, loss = 6.23950609\n",
      "Iteration 11722, loss = 4.56216012\n",
      "Iteration 11723, loss = 4.51864242\n",
      "Iteration 11724, loss = 4.96675427\n",
      "Iteration 11725, loss = 3.44013014\n",
      "Iteration 11726, loss = 3.49604824\n",
      "Iteration 11727, loss = 1.92804386\n",
      "Iteration 11728, loss = 1.53189652\n",
      "Iteration 11729, loss = 2.46132401\n",
      "Iteration 11730, loss = 5.01948822\n",
      "Iteration 11731, loss = 4.83226479\n",
      "Iteration 11732, loss = 8.65901493\n",
      "Iteration 11733, loss = 8.84780907\n",
      "Iteration 11734, loss = 10.26006495\n",
      "Iteration 11735, loss = 3.89632860\n",
      "Iteration 11736, loss = 4.55537013\n",
      "Iteration 11737, loss = 2.60876062\n",
      "Iteration 11738, loss = 4.03400517\n",
      "Iteration 11739, loss = 4.25878671\n",
      "Iteration 11740, loss = 7.29508183\n",
      "Iteration 11741, loss = 21.52153202\n",
      "Iteration 11742, loss = 23.63455718\n",
      "Iteration 11743, loss = 19.89169926\n",
      "Iteration 11744, loss = 17.55672219\n",
      "Iteration 11745, loss = 15.91101622\n",
      "Iteration 11746, loss = 15.21821975\n",
      "Iteration 11747, loss = 13.99032954\n",
      "Iteration 11748, loss = 13.97626060\n",
      "Iteration 11749, loss = 12.52756864\n",
      "Iteration 11750, loss = 12.18260082\n",
      "Iteration 11751, loss = 11.36969765\n",
      "Iteration 11752, loss = 10.99998067\n",
      "Iteration 11753, loss = 10.60251524\n",
      "Iteration 11754, loss = 10.36406177\n",
      "Iteration 11755, loss = 10.02719354\n",
      "Iteration 11756, loss = 9.22295234\n",
      "Iteration 11757, loss = 9.09500233\n",
      "Iteration 11758, loss = 9.08770636\n",
      "Iteration 11759, loss = 8.28270985\n",
      "Iteration 11760, loss = 9.10838596\n",
      "Iteration 11761, loss = 8.19439575\n",
      "Iteration 11762, loss = 8.80917863\n",
      "Iteration 11763, loss = 8.20928242\n",
      "Iteration 11764, loss = 8.63483557\n",
      "Iteration 11765, loss = 8.22472792\n",
      "Iteration 11766, loss = 7.99571943\n",
      "Iteration 11767, loss = 8.34136611\n",
      "Iteration 11768, loss = 8.73941989\n",
      "Iteration 11769, loss = 8.13432198\n",
      "Iteration 11770, loss = 7.82128375\n",
      "Iteration 11771, loss = 8.17424266\n",
      "Iteration 11772, loss = 8.91573189\n",
      "Iteration 11773, loss = 8.48082449\n",
      "Iteration 11774, loss = 8.99411034\n",
      "Iteration 11775, loss = 8.36352526\n",
      "Iteration 11776, loss = 7.98874085\n",
      "Iteration 11777, loss = 7.98798985\n",
      "Iteration 11778, loss = 7.86335816\n",
      "Iteration 11779, loss = 7.80618158\n",
      "Iteration 11780, loss = 7.63296556\n",
      "Iteration 11781, loss = 7.95552692\n",
      "Iteration 11782, loss = 7.88994794\n",
      "Iteration 11783, loss = 7.27288109\n",
      "Iteration 11784, loss = 7.71580767\n",
      "Iteration 11785, loss = 7.90703567\n",
      "Iteration 11786, loss = 7.37326755\n",
      "Iteration 11787, loss = 7.38452787\n",
      "Iteration 11788, loss = 7.98919479\n",
      "Iteration 11789, loss = 7.76764150\n",
      "Iteration 11790, loss = 7.77120322\n",
      "Iteration 11791, loss = 7.61023853\n",
      "Iteration 11792, loss = 7.72329820\n",
      "Iteration 11793, loss = 7.45478269\n",
      "Iteration 11794, loss = 7.48093623\n",
      "Iteration 11795, loss = 7.28943529\n",
      "Iteration 11796, loss = 6.95922469\n",
      "Iteration 11797, loss = 7.08719221\n",
      "Iteration 11798, loss = 7.22316658\n",
      "Iteration 11799, loss = 7.10152469\n",
      "Iteration 11800, loss = 6.70985173\n",
      "Iteration 11801, loss = 7.03562593\n",
      "Iteration 11802, loss = 6.98843212\n",
      "Iteration 11803, loss = 6.49029072\n",
      "Iteration 11804, loss = 7.16285420\n",
      "Iteration 11805, loss = 7.82473613\n",
      "Iteration 11806, loss = 7.69705842\n",
      "Iteration 11807, loss = 6.48138218\n",
      "Iteration 11808, loss = 6.70012182\n",
      "Iteration 11809, loss = 7.31002943\n",
      "Iteration 11810, loss = 7.92199269\n",
      "Iteration 11811, loss = 7.92069156\n",
      "Iteration 11812, loss = 11.22214716\n",
      "Iteration 11813, loss = 8.91195273\n",
      "Iteration 11814, loss = 8.98134284\n",
      "Iteration 11815, loss = 7.82883885\n",
      "Iteration 11816, loss = 7.68576430\n",
      "Iteration 11817, loss = 7.01317693\n",
      "Iteration 11818, loss = 6.76460505\n",
      "Iteration 11819, loss = 6.94683629\n",
      "Iteration 11820, loss = 7.11051184\n",
      "Iteration 11821, loss = 5.90879506\n",
      "Iteration 11822, loss = 7.70092388\n",
      "Iteration 11823, loss = 7.38704442\n",
      "Iteration 11824, loss = 7.12873629\n",
      "Iteration 11825, loss = 6.94408274\n",
      "Iteration 11826, loss = 6.60843003\n",
      "Iteration 11827, loss = 6.21588232\n",
      "Iteration 11828, loss = 6.90221438\n",
      "Iteration 11829, loss = 8.45201306\n",
      "Iteration 11830, loss = 6.50126811\n",
      "Iteration 11831, loss = 5.97856763\n",
      "Iteration 11832, loss = 6.37949775\n",
      "Iteration 11833, loss = 6.33152247\n",
      "Iteration 11834, loss = 5.85800499\n",
      "Iteration 11835, loss = 5.58415406\n",
      "Iteration 11836, loss = 5.76928940\n",
      "Iteration 11837, loss = 6.23332911\n",
      "Iteration 11838, loss = 5.35461578\n",
      "Iteration 11839, loss = 5.67499653\n",
      "Iteration 11840, loss = 5.51448836\n",
      "Iteration 11841, loss = 5.38879126\n",
      "Iteration 11842, loss = 5.94267958\n",
      "Iteration 11843, loss = 6.16776418\n",
      "Iteration 11844, loss = 6.53291982\n",
      "Iteration 11845, loss = 5.59435871\n",
      "Iteration 11846, loss = 5.43750413\n",
      "Iteration 11847, loss = 5.72922361\n",
      "Iteration 11848, loss = 5.58898962\n",
      "Iteration 11849, loss = 5.48006611\n",
      "Iteration 11850, loss = 6.34654739\n",
      "Iteration 11851, loss = 5.54192683\n",
      "Iteration 11852, loss = 5.48499516\n",
      "Iteration 11853, loss = 5.51372644\n",
      "Iteration 11854, loss = 6.06099960\n",
      "Iteration 11855, loss = 6.48360695\n",
      "Iteration 11856, loss = 6.50017882\n",
      "Iteration 11857, loss = 5.46559806\n",
      "Iteration 11858, loss = 5.05906545\n",
      "Iteration 11859, loss = 7.40467998\n",
      "Iteration 11860, loss = 6.85833798\n",
      "Iteration 11861, loss = 7.18075094\n",
      "Iteration 11862, loss = 6.01617884\n",
      "Iteration 11863, loss = 6.79968408\n",
      "Iteration 11864, loss = 6.20177978\n",
      "Iteration 11865, loss = 6.95276858\n",
      "Iteration 11866, loss = 4.97601812\n",
      "Iteration 11867, loss = 5.56377287\n",
      "Iteration 11868, loss = 5.40682244\n",
      "Iteration 11869, loss = 6.01777915\n",
      "Iteration 11870, loss = 4.91266517\n",
      "Iteration 11871, loss = 5.04715921\n",
      "Iteration 11872, loss = 4.90619906\n",
      "Iteration 11873, loss = 5.34218075\n",
      "Iteration 11874, loss = 4.82629474\n",
      "Iteration 11875, loss = 5.32587116\n",
      "Iteration 11876, loss = 4.94760247\n",
      "Iteration 11877, loss = 5.06435973\n",
      "Iteration 11878, loss = 4.84561794\n",
      "Iteration 11879, loss = 4.80123689\n",
      "Iteration 11880, loss = 5.19303393\n",
      "Iteration 11881, loss = 4.96986489\n",
      "Iteration 11882, loss = 5.10610687\n",
      "Iteration 11883, loss = 4.95486119\n",
      "Iteration 11884, loss = 5.09844806\n",
      "Iteration 11885, loss = 4.77958876\n",
      "Iteration 11886, loss = 4.62852685\n",
      "Iteration 11887, loss = 5.38531169\n",
      "Iteration 11888, loss = 5.30920548\n",
      "Iteration 11889, loss = 4.84436019\n",
      "Iteration 11890, loss = 5.99223617\n",
      "Iteration 11891, loss = 4.32816113\n",
      "Iteration 11892, loss = 5.45941048\n",
      "Iteration 11893, loss = 4.57594838\n",
      "Iteration 11894, loss = 4.71795703\n",
      "Iteration 11895, loss = 4.86688150\n",
      "Iteration 11896, loss = 5.05130381\n",
      "Iteration 11897, loss = 5.27066842\n",
      "Iteration 11898, loss = 4.92700574\n",
      "Iteration 11899, loss = 4.67323108\n",
      "Iteration 11900, loss = 5.01629399\n",
      "Iteration 11901, loss = 5.46599296\n",
      "Iteration 11902, loss = 6.01576904\n",
      "Iteration 11903, loss = 7.11110536\n",
      "Iteration 11904, loss = 5.67164519\n",
      "Iteration 11905, loss = 5.57265852\n",
      "Iteration 11906, loss = 7.65414599\n",
      "Iteration 11907, loss = 10.84835480\n",
      "Iteration 11908, loss = 17.19824232\n",
      "Iteration 11909, loss = 9.75982861\n",
      "Iteration 11910, loss = 6.08521379\n",
      "Iteration 11911, loss = 5.18873699\n",
      "Iteration 11912, loss = 6.02475555\n",
      "Iteration 11913, loss = 7.47271189\n",
      "Iteration 11914, loss = 6.79788180\n",
      "Iteration 11915, loss = 6.35107281\n",
      "Iteration 11916, loss = 5.35266870\n",
      "Iteration 11917, loss = 5.67608518\n",
      "Iteration 11918, loss = 5.11125666\n",
      "Iteration 11919, loss = 5.21588986\n",
      "Iteration 11920, loss = 4.76785911\n",
      "Iteration 11921, loss = 4.46763074\n",
      "Iteration 11922, loss = 4.27378946\n",
      "Iteration 11923, loss = 4.97993438\n",
      "Iteration 11924, loss = 4.80221861\n",
      "Iteration 11925, loss = 4.25461748\n",
      "Iteration 11926, loss = 4.49113660\n",
      "Iteration 11927, loss = 4.58575584\n",
      "Iteration 11928, loss = 4.33767138\n",
      "Iteration 11929, loss = 4.61679714\n",
      "Iteration 11930, loss = 4.54194672\n",
      "Iteration 11931, loss = 5.12048112\n",
      "Iteration 11932, loss = 5.19443574\n",
      "Iteration 11933, loss = 4.91637381\n",
      "Iteration 11934, loss = 4.61691420\n",
      "Iteration 11935, loss = 4.46272004\n",
      "Iteration 11936, loss = 6.09295803\n",
      "Iteration 11937, loss = 5.10912682\n",
      "Iteration 11938, loss = 4.59328275\n",
      "Iteration 11939, loss = 9.40239884\n",
      "Iteration 11940, loss = 10.18532967\n",
      "Iteration 11941, loss = 6.39133825\n",
      "Iteration 11942, loss = 6.37103406\n",
      "Iteration 11943, loss = 4.89806023\n",
      "Iteration 11944, loss = 4.78921358\n",
      "Iteration 11945, loss = 4.23375070\n",
      "Iteration 11946, loss = 4.32799673\n",
      "Iteration 11947, loss = 4.30544127\n",
      "Iteration 11948, loss = 4.20518393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11949, loss = 4.34703269\n",
      "Iteration 11950, loss = 4.05604989\n",
      "Iteration 11951, loss = 4.90960301\n",
      "Iteration 11952, loss = 4.47104098\n",
      "Iteration 11953, loss = 3.98415478\n",
      "Iteration 11954, loss = 5.03463430\n",
      "Iteration 11955, loss = 4.99890257\n",
      "Iteration 11956, loss = 4.18247582\n",
      "Iteration 11957, loss = 4.20332199\n",
      "Iteration 11958, loss = 4.05924569\n",
      "Iteration 11959, loss = 4.12241176\n",
      "Iteration 11960, loss = 4.28041554\n",
      "Iteration 11961, loss = 4.39777486\n",
      "Iteration 11962, loss = 4.96964719\n",
      "Iteration 11963, loss = 4.59809724\n",
      "Iteration 11964, loss = 4.14883771\n",
      "Iteration 11965, loss = 4.10242667\n",
      "Iteration 11966, loss = 4.07195633\n",
      "Iteration 11967, loss = 4.13414870\n",
      "Iteration 11968, loss = 4.64769292\n",
      "Iteration 11969, loss = 4.04679200\n",
      "Iteration 11970, loss = 4.45254206\n",
      "Iteration 11971, loss = 4.96913561\n",
      "Iteration 11972, loss = 4.40401126\n",
      "Iteration 11973, loss = 5.20100748\n",
      "Iteration 11974, loss = 5.38528414\n",
      "Iteration 11975, loss = 4.50334452\n",
      "Iteration 11976, loss = 4.45115343\n",
      "Iteration 11977, loss = 4.99179117\n",
      "Iteration 11978, loss = 4.76655250\n",
      "Iteration 11979, loss = 4.62081394\n",
      "Iteration 11980, loss = 4.55457983\n",
      "Iteration 11981, loss = 4.33569499\n",
      "Iteration 11982, loss = 5.10738351\n",
      "Iteration 11983, loss = 6.61248823\n",
      "Iteration 11984, loss = 6.30740555\n",
      "Iteration 11985, loss = 10.64724875\n",
      "Iteration 11986, loss = 12.15006975\n",
      "Iteration 11987, loss = 9.67178109\n",
      "Iteration 11988, loss = 7.07403586\n",
      "Iteration 11989, loss = 5.59318918\n",
      "Iteration 11990, loss = 5.44919742\n",
      "Iteration 11991, loss = 5.82412476\n",
      "Iteration 11992, loss = 7.72159675\n",
      "Iteration 11993, loss = 5.90193633\n",
      "Iteration 11994, loss = 4.46855963\n",
      "Iteration 11995, loss = 5.55519554\n",
      "Iteration 11996, loss = 5.69145002\n",
      "Iteration 11997, loss = 5.89188364\n",
      "Iteration 11998, loss = 7.25653807\n",
      "Iteration 11999, loss = 5.95253009\n",
      "Iteration 12000, loss = 5.19113085\n",
      "Iteration 12001, loss = 4.95603305\n",
      "Iteration 12002, loss = 4.12371965\n",
      "Iteration 12003, loss = 4.17888844\n",
      "Iteration 12004, loss = 4.24271342\n",
      "Iteration 12005, loss = 4.13797655\n",
      "Iteration 12006, loss = 4.61182560\n",
      "Iteration 12007, loss = 4.24323846\n",
      "Iteration 12008, loss = 4.37214965\n",
      "Iteration 12009, loss = 4.13490727\n",
      "Iteration 12010, loss = 4.89593171\n",
      "Iteration 12011, loss = 4.59636215\n",
      "Iteration 12012, loss = 4.21554489\n",
      "Iteration 12013, loss = 4.27570604\n",
      "Iteration 12014, loss = 4.96129541\n",
      "Iteration 12015, loss = 6.04686231\n",
      "Iteration 12016, loss = 4.92938904\n",
      "Iteration 12017, loss = 4.39421931\n",
      "Iteration 12018, loss = 4.09386520\n",
      "Iteration 12019, loss = 4.08073064\n",
      "Iteration 12020, loss = 4.11409044\n",
      "Iteration 12021, loss = 4.25624555\n",
      "Iteration 12022, loss = 4.62009253\n",
      "Iteration 12023, loss = 4.85259975\n",
      "Iteration 12024, loss = 4.74860218\n",
      "Iteration 12025, loss = 4.33769567\n",
      "Iteration 12026, loss = 4.38229822\n",
      "Iteration 12027, loss = 3.93430211\n",
      "Iteration 12028, loss = 3.76257661\n",
      "Iteration 12029, loss = 4.06659051\n",
      "Iteration 12030, loss = 4.59126978\n",
      "Iteration 12031, loss = 4.15914195\n",
      "Iteration 12032, loss = 3.84346537\n",
      "Iteration 12033, loss = 3.92958426\n",
      "Iteration 12034, loss = 4.09109722\n",
      "Iteration 12035, loss = 6.66086000\n",
      "Iteration 12036, loss = 7.54386377\n",
      "Iteration 12037, loss = 5.67646301\n",
      "Iteration 12038, loss = 4.20493427\n",
      "Iteration 12039, loss = 4.39708661\n",
      "Iteration 12040, loss = 4.63732112\n",
      "Iteration 12041, loss = 5.75746245\n",
      "Iteration 12042, loss = 4.39328174\n",
      "Iteration 12043, loss = 4.55194574\n",
      "Iteration 12044, loss = 4.66441458\n",
      "Iteration 12045, loss = 5.20188166\n",
      "Iteration 12046, loss = 3.90182378\n",
      "Iteration 12047, loss = 5.33100151\n",
      "Iteration 12048, loss = 4.78059300\n",
      "Iteration 12049, loss = 5.51312831\n",
      "Iteration 12050, loss = 4.32547857\n",
      "Iteration 12051, loss = 3.78142544\n",
      "Iteration 12052, loss = 4.00371480\n",
      "Iteration 12053, loss = 3.75587610\n",
      "Iteration 12054, loss = 4.13695045\n",
      "Iteration 12055, loss = 8.71408374\n",
      "Iteration 12056, loss = 7.61423078\n",
      "Iteration 12057, loss = 6.89661001\n",
      "Iteration 12058, loss = 5.33025503\n",
      "Iteration 12059, loss = 4.50294568\n",
      "Iteration 12060, loss = 4.88717844\n",
      "Iteration 12061, loss = 4.18575124\n",
      "Iteration 12062, loss = 4.89060786\n",
      "Iteration 12063, loss = 4.35022788\n",
      "Iteration 12064, loss = 4.48906579\n",
      "Iteration 12065, loss = 4.71567977\n",
      "Iteration 12066, loss = 5.18629081\n",
      "Iteration 12067, loss = 6.11528005\n",
      "Iteration 12068, loss = 5.13567417\n",
      "Iteration 12069, loss = 6.99658176\n",
      "Iteration 12070, loss = 4.47632556\n",
      "Iteration 12071, loss = 5.09608528\n",
      "Iteration 12072, loss = 3.79278553\n",
      "Iteration 12073, loss = 4.16085778\n",
      "Iteration 12074, loss = 3.76338162\n",
      "Iteration 12075, loss = 3.83653446\n",
      "Iteration 12076, loss = 4.17696426\n",
      "Iteration 12077, loss = 4.16907036\n",
      "Iteration 12078, loss = 3.72862728\n",
      "Iteration 12079, loss = 3.77072344\n",
      "Iteration 12080, loss = 3.83685310\n",
      "Iteration 12081, loss = 3.77029351\n",
      "Iteration 12082, loss = 3.74600369\n",
      "Iteration 12083, loss = 3.72316610\n",
      "Iteration 12084, loss = 3.61138912\n",
      "Iteration 12085, loss = 3.67271563\n",
      "Iteration 12086, loss = 3.51483705\n",
      "Iteration 12087, loss = 4.39117043\n",
      "Iteration 12088, loss = 3.78295971\n",
      "Iteration 12089, loss = 3.62553752\n",
      "Iteration 12090, loss = 3.65733858\n",
      "Iteration 12091, loss = 3.62340828\n",
      "Iteration 12092, loss = 3.71917569\n",
      "Iteration 12093, loss = 4.39183041\n",
      "Iteration 12094, loss = 3.57874327\n",
      "Iteration 12095, loss = 3.72053051\n",
      "Iteration 12096, loss = 3.85967976\n",
      "Iteration 12097, loss = 3.73819573\n",
      "Iteration 12098, loss = 4.07638942\n",
      "Iteration 12099, loss = 4.84634476\n",
      "Iteration 12100, loss = 3.79714588\n",
      "Iteration 12101, loss = 3.59130175\n",
      "Iteration 12102, loss = 4.57153619\n",
      "Iteration 12103, loss = 6.32247139\n",
      "Iteration 12104, loss = 4.17409165\n",
      "Iteration 12105, loss = 6.97596606\n",
      "Iteration 12106, loss = 7.47950646\n",
      "Iteration 12107, loss = 5.32997618\n",
      "Iteration 12108, loss = 3.94813232\n",
      "Iteration 12109, loss = 3.78962100\n",
      "Iteration 12110, loss = 3.78150032\n",
      "Iteration 12111, loss = 3.94592392\n",
      "Iteration 12112, loss = 4.17299407\n",
      "Iteration 12113, loss = 4.98794668\n",
      "Iteration 12114, loss = 4.06348027\n",
      "Iteration 12115, loss = 3.91783553\n",
      "Iteration 12116, loss = 3.96834910\n",
      "Iteration 12117, loss = 3.58837292\n",
      "Iteration 12118, loss = 3.52211761\n",
      "Iteration 12119, loss = 3.71202230\n",
      "Iteration 12120, loss = 3.65924376\n",
      "Iteration 12121, loss = 3.82303550\n",
      "Iteration 12122, loss = 5.62615740\n",
      "Iteration 12123, loss = 4.39840953\n",
      "Iteration 12124, loss = 4.00078743\n",
      "Iteration 12125, loss = 3.48781921\n",
      "Iteration 12126, loss = 3.80421901\n",
      "Iteration 12127, loss = 3.46651303\n",
      "Iteration 12128, loss = 3.90699702\n",
      "Iteration 12129, loss = 3.65929034\n",
      "Iteration 12130, loss = 5.18045995\n",
      "Iteration 12131, loss = 13.64612032\n",
      "Iteration 12132, loss = 15.81370708\n",
      "Iteration 12133, loss = 10.75497597\n",
      "Iteration 12134, loss = 6.56660847\n",
      "Iteration 12135, loss = 6.34660777\n",
      "Iteration 12136, loss = 4.12478250\n",
      "Iteration 12137, loss = 6.00197072\n",
      "Iteration 12138, loss = 5.19207242\n",
      "Iteration 12139, loss = 5.03771114\n",
      "Iteration 12140, loss = 4.85985288\n",
      "Iteration 12141, loss = 4.63200329\n",
      "Iteration 12142, loss = 5.08838981\n",
      "Iteration 12143, loss = 5.13504030\n",
      "Iteration 12144, loss = 5.63871157\n",
      "Iteration 12145, loss = 4.54818005\n",
      "Iteration 12146, loss = 3.89451680\n",
      "Iteration 12147, loss = 4.04069299\n",
      "Iteration 12148, loss = 4.53670663\n",
      "Iteration 12149, loss = 3.79849407\n",
      "Iteration 12150, loss = 4.18547558\n",
      "Iteration 12151, loss = 3.85634148\n",
      "Iteration 12152, loss = 4.38765500\n",
      "Iteration 12153, loss = 4.16427030\n",
      "Iteration 12154, loss = 3.83548752\n",
      "Iteration 12155, loss = 3.84255640\n",
      "Iteration 12156, loss = 5.96939060\n",
      "Iteration 12157, loss = 5.21082222\n",
      "Iteration 12158, loss = 4.34378214\n",
      "Iteration 12159, loss = 4.74721742\n",
      "Iteration 12160, loss = 3.83800113\n",
      "Iteration 12161, loss = 3.65613689\n",
      "Iteration 12162, loss = 3.72634773\n",
      "Iteration 12163, loss = 3.45015806\n",
      "Iteration 12164, loss = 3.46812190\n",
      "Iteration 12165, loss = 3.40723322\n",
      "Iteration 12166, loss = 3.41097690\n",
      "Iteration 12167, loss = 3.65591979\n",
      "Iteration 12168, loss = 3.62498144\n",
      "Iteration 12169, loss = 4.17469314\n",
      "Iteration 12170, loss = 4.71935072\n",
      "Iteration 12171, loss = 4.00406046\n",
      "Iteration 12172, loss = 3.67491640\n",
      "Iteration 12173, loss = 4.64722047\n",
      "Iteration 12174, loss = 4.70046759\n",
      "Iteration 12175, loss = 4.54684438\n",
      "Iteration 12176, loss = 4.12958402\n",
      "Iteration 12177, loss = 3.97730648\n",
      "Iteration 12178, loss = 3.65551442\n",
      "Iteration 12179, loss = 3.73627091\n",
      "Iteration 12180, loss = 4.27953032\n",
      "Iteration 12181, loss = 3.44567236\n",
      "Iteration 12182, loss = 3.50589435\n",
      "Iteration 12183, loss = 3.93775355\n",
      "Iteration 12184, loss = 3.74974553\n",
      "Iteration 12185, loss = 3.53069679\n",
      "Iteration 12186, loss = 4.96818068\n",
      "Iteration 12187, loss = 5.22675719\n",
      "Iteration 12188, loss = 4.87787720\n",
      "Iteration 12189, loss = 3.91475264\n",
      "Iteration 12190, loss = 3.89820001\n",
      "Iteration 12191, loss = 3.84994675\n",
      "Iteration 12192, loss = 5.21948087\n",
      "Iteration 12193, loss = 4.21750855\n",
      "Iteration 12194, loss = 3.69979612\n",
      "Iteration 12195, loss = 4.27566820\n",
      "Iteration 12196, loss = 6.11537804\n",
      "Iteration 12197, loss = 4.14028556\n",
      "Iteration 12198, loss = 3.61665688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12199, loss = 3.63664787\n",
      "Iteration 12200, loss = 3.60584686\n",
      "Iteration 12201, loss = 3.71600770\n",
      "Iteration 12202, loss = 4.18117978\n",
      "Iteration 12203, loss = 6.40984564\n",
      "Iteration 12204, loss = 7.68309665\n",
      "Iteration 12205, loss = 7.04234531\n",
      "Iteration 12206, loss = 5.19456587\n",
      "Iteration 12207, loss = 3.68070552\n",
      "Iteration 12208, loss = 3.98106628\n",
      "Iteration 12209, loss = 3.34740479\n",
      "Iteration 12210, loss = 3.73721967\n",
      "Iteration 12211, loss = 3.52140430\n",
      "Iteration 12212, loss = 3.64245093\n",
      "Iteration 12213, loss = 4.11209014\n",
      "Iteration 12214, loss = 3.49178785\n",
      "Iteration 12215, loss = 3.76677692\n",
      "Iteration 12216, loss = 5.84713099\n",
      "Iteration 12217, loss = 6.53464925\n",
      "Iteration 12218, loss = 4.66645152\n",
      "Iteration 12219, loss = 4.52753973\n",
      "Iteration 12220, loss = 4.52824651\n",
      "Iteration 12221, loss = 4.19599986\n",
      "Iteration 12222, loss = 4.44397491\n",
      "Iteration 12223, loss = 4.21049185\n",
      "Iteration 12224, loss = 4.13000425\n",
      "Iteration 12225, loss = 3.56554476\n",
      "Iteration 12226, loss = 3.86610875\n",
      "Iteration 12227, loss = 3.45049466\n",
      "Iteration 12228, loss = 3.28758018\n",
      "Iteration 12229, loss = 3.15286510\n",
      "Iteration 12230, loss = 3.68769958\n",
      "Iteration 12231, loss = 3.48721345\n",
      "Iteration 12232, loss = 3.32082176\n",
      "Iteration 12233, loss = 3.65574998\n",
      "Iteration 12234, loss = 3.99689689\n",
      "Iteration 12235, loss = 3.68921189\n",
      "Iteration 12236, loss = 4.45904850\n",
      "Iteration 12237, loss = 6.08198740\n",
      "Iteration 12238, loss = 6.54317950\n",
      "Iteration 12239, loss = 12.80630628\n",
      "Iteration 12240, loss = 7.33927821\n",
      "Iteration 12241, loss = 6.26346260\n",
      "Iteration 12242, loss = 5.70513021\n",
      "Iteration 12243, loss = 6.04902250\n",
      "Iteration 12244, loss = 6.64304735\n",
      "Iteration 12245, loss = 4.21628965\n",
      "Iteration 12246, loss = 3.69039729\n",
      "Iteration 12247, loss = 4.09545331\n",
      "Iteration 12248, loss = 6.22514577\n",
      "Iteration 12249, loss = 4.19972979\n",
      "Iteration 12250, loss = 3.76598075\n",
      "Iteration 12251, loss = 3.68528345\n",
      "Iteration 12252, loss = 4.53575862\n",
      "Iteration 12253, loss = 4.03309475\n",
      "Iteration 12254, loss = 3.39703117\n",
      "Iteration 12255, loss = 3.38647724\n",
      "Iteration 12256, loss = 3.69475797\n",
      "Iteration 12257, loss = 3.51862219\n",
      "Iteration 12258, loss = 3.46356544\n",
      "Iteration 12259, loss = 3.31242943\n",
      "Iteration 12260, loss = 3.57384637\n",
      "Iteration 12261, loss = 5.30211001\n",
      "Iteration 12262, loss = 6.04500896\n",
      "Iteration 12263, loss = 4.08519774\n",
      "Iteration 12264, loss = 3.74211410\n",
      "Iteration 12265, loss = 3.85922931\n",
      "Iteration 12266, loss = 3.86288650\n",
      "Iteration 12267, loss = 3.97357249\n",
      "Iteration 12268, loss = 3.37433091\n",
      "Iteration 12269, loss = 3.52652422\n",
      "Iteration 12270, loss = 4.96272361\n",
      "Iteration 12271, loss = 4.18535249\n",
      "Iteration 12272, loss = 3.23086119\n",
      "Iteration 12273, loss = 3.51433909\n",
      "Iteration 12274, loss = 4.02748078\n",
      "Iteration 12275, loss = 3.68834599\n",
      "Iteration 12276, loss = 3.37121963\n",
      "Iteration 12277, loss = 3.58360560\n",
      "Iteration 12278, loss = 3.41521005\n",
      "Iteration 12279, loss = 3.53289175\n",
      "Iteration 12280, loss = 3.60459042\n",
      "Iteration 12281, loss = 3.38964281\n",
      "Iteration 12282, loss = 3.61095187\n",
      "Iteration 12283, loss = 3.45816823\n",
      "Iteration 12284, loss = 3.16057136\n",
      "Iteration 12285, loss = 3.77515517\n",
      "Iteration 12286, loss = 3.72147419\n",
      "Iteration 12287, loss = 4.34566884\n",
      "Iteration 12288, loss = 3.75722804\n",
      "Iteration 12289, loss = 4.56463131\n",
      "Iteration 12290, loss = 5.08077329\n",
      "Iteration 12291, loss = 6.57447666\n",
      "Iteration 12292, loss = 4.23074118\n",
      "Iteration 12293, loss = 3.99658040\n",
      "Iteration 12294, loss = 6.10327573\n",
      "Iteration 12295, loss = 4.73538860\n",
      "Iteration 12296, loss = 3.70070814\n",
      "Iteration 12297, loss = 3.65458120\n",
      "Iteration 12298, loss = 3.32676113\n",
      "Iteration 12299, loss = 3.37350800\n",
      "Iteration 12300, loss = 5.04971523\n",
      "Iteration 12301, loss = 4.65649988\n",
      "Iteration 12302, loss = 4.64837751\n",
      "Iteration 12303, loss = 3.46134092\n",
      "Iteration 12304, loss = 3.85545325\n",
      "Iteration 12305, loss = 3.76660770\n",
      "Iteration 12306, loss = 3.37797688\n",
      "Iteration 12307, loss = 3.27770587\n",
      "Iteration 12308, loss = 3.39778796\n",
      "Iteration 12309, loss = 3.20063005\n",
      "Iteration 12310, loss = 3.29934601\n",
      "Iteration 12311, loss = 3.59407002\n",
      "Iteration 12312, loss = 4.96428049\n",
      "Iteration 12313, loss = 3.81085814\n",
      "Iteration 12314, loss = 4.03394177\n",
      "Iteration 12315, loss = 3.80768488\n",
      "Iteration 12316, loss = 3.55019833\n",
      "Iteration 12317, loss = 3.66015557\n",
      "Iteration 12318, loss = 3.41624347\n",
      "Iteration 12319, loss = 3.37740801\n",
      "Iteration 12320, loss = 3.28804904\n",
      "Iteration 12321, loss = 4.37870407\n",
      "Iteration 12322, loss = 3.45657591\n",
      "Iteration 12323, loss = 3.29435589\n",
      "Iteration 12324, loss = 3.23784389\n",
      "Iteration 12325, loss = 3.38607419\n",
      "Iteration 12326, loss = 3.25734621\n",
      "Iteration 12327, loss = 3.21758582\n",
      "Iteration 12328, loss = 3.11526260\n",
      "Iteration 12329, loss = 3.33156805\n",
      "Iteration 12330, loss = 3.15873471\n",
      "Iteration 12331, loss = 3.32298012\n",
      "Iteration 12332, loss = 3.32242289\n",
      "Iteration 12333, loss = 3.35108653\n",
      "Iteration 12334, loss = 3.38044563\n",
      "Iteration 12335, loss = 3.24596505\n",
      "Iteration 12336, loss = 3.25364423\n",
      "Iteration 12337, loss = 3.62257669\n",
      "Iteration 12338, loss = 3.33499617\n",
      "Iteration 12339, loss = 3.99288410\n",
      "Iteration 12340, loss = 3.56113846\n",
      "Iteration 12341, loss = 3.24951499\n",
      "Iteration 12342, loss = 4.11892895\n",
      "Iteration 12343, loss = 3.77591055\n",
      "Iteration 12344, loss = 3.91372489\n",
      "Iteration 12345, loss = 3.72234229\n",
      "Iteration 12346, loss = 4.18394056\n",
      "Iteration 12347, loss = 4.90418467\n",
      "Iteration 12348, loss = 5.07863534\n",
      "Iteration 12349, loss = 3.75864696\n",
      "Iteration 12350, loss = 3.93616262\n",
      "Iteration 12351, loss = 3.45891965\n",
      "Iteration 12352, loss = 3.91502604\n",
      "Iteration 12353, loss = 3.24103744\n",
      "Iteration 12354, loss = 4.02172873\n",
      "Iteration 12355, loss = 3.91942890\n",
      "Iteration 12356, loss = 3.95382492\n",
      "Iteration 12357, loss = 3.20289419\n",
      "Iteration 12358, loss = 3.26924341\n",
      "Iteration 12359, loss = 3.33458564\n",
      "Iteration 12360, loss = 3.30327452\n",
      "Iteration 12361, loss = 3.18894297\n",
      "Iteration 12362, loss = 3.30519823\n",
      "Iteration 12363, loss = 3.74393064\n",
      "Iteration 12364, loss = 3.81369233\n",
      "Iteration 12365, loss = 3.85991569\n",
      "Iteration 12366, loss = 3.54133851\n",
      "Iteration 12367, loss = 3.65935555\n",
      "Iteration 12368, loss = 3.13170506\n",
      "Iteration 12369, loss = 3.46511028\n",
      "Iteration 12370, loss = 4.00822306\n",
      "Iteration 12371, loss = 5.41769953\n",
      "Iteration 12372, loss = 4.22868136\n",
      "Iteration 12373, loss = 3.84772371\n",
      "Iteration 12374, loss = 4.70606985\n",
      "Iteration 12375, loss = 4.53885835\n",
      "Iteration 12376, loss = 4.00581133\n",
      "Iteration 12377, loss = 3.38361645\n",
      "Iteration 12378, loss = 3.37947735\n",
      "Iteration 12379, loss = 3.71735251\n",
      "Iteration 12380, loss = 3.65932099\n",
      "Iteration 12381, loss = 3.36943196\n",
      "Iteration 12382, loss = 3.34918176\n",
      "Iteration 12383, loss = 3.50320505\n",
      "Iteration 12384, loss = 4.49123530\n",
      "Iteration 12385, loss = 4.11793660\n",
      "Iteration 12386, loss = 4.62485016\n",
      "Iteration 12387, loss = 5.79140877\n",
      "Iteration 12388, loss = 4.63083378\n",
      "Iteration 12389, loss = 3.50328614\n",
      "Iteration 12390, loss = 3.79965839\n",
      "Iteration 12391, loss = 3.79852556\n",
      "Iteration 12392, loss = 3.31567007\n",
      "Iteration 12393, loss = 3.25680817\n",
      "Iteration 12394, loss = 3.32272301\n",
      "Iteration 12395, loss = 3.24233959\n",
      "Iteration 12396, loss = 5.31007108\n",
      "Iteration 12397, loss = 4.61958168\n",
      "Iteration 12398, loss = 3.92851883\n",
      "Iteration 12399, loss = 4.60325593\n",
      "Iteration 12400, loss = 3.91465537\n",
      "Iteration 12401, loss = 4.70949056\n",
      "Iteration 12402, loss = 3.83834942\n",
      "Iteration 12403, loss = 4.23583603\n",
      "Iteration 12404, loss = 3.35522136\n",
      "Iteration 12405, loss = 4.73547907\n",
      "Iteration 12406, loss = 3.88867619\n",
      "Iteration 12407, loss = 3.51679876\n",
      "Iteration 12408, loss = 3.21936669\n",
      "Iteration 12409, loss = 3.14340115\n",
      "Iteration 12410, loss = 3.36923176\n",
      "Iteration 12411, loss = 3.13262646\n",
      "Iteration 12412, loss = 4.05735534\n",
      "Iteration 12413, loss = 3.16674059\n",
      "Iteration 12414, loss = 3.17855000\n",
      "Iteration 12415, loss = 3.22379008\n",
      "Iteration 12416, loss = 4.15067459\n",
      "Iteration 12417, loss = 4.12544272\n",
      "Iteration 12418, loss = 3.70864966\n",
      "Iteration 12419, loss = 3.60951940\n",
      "Iteration 12420, loss = 3.73489978\n",
      "Iteration 12421, loss = 3.18781961\n",
      "Iteration 12422, loss = 3.32805314\n",
      "Iteration 12423, loss = 3.26235637\n",
      "Iteration 12424, loss = 3.06469127\n",
      "Iteration 12425, loss = 3.16451812\n",
      "Iteration 12426, loss = 2.96408005\n",
      "Iteration 12427, loss = 4.21258827\n",
      "Iteration 12428, loss = 4.47180365\n",
      "Iteration 12429, loss = 4.96988983\n",
      "Iteration 12430, loss = 4.72551210\n",
      "Iteration 12431, loss = 3.67986627\n",
      "Iteration 12432, loss = 3.17249258\n",
      "Iteration 12433, loss = 3.11668236\n",
      "Iteration 12434, loss = 3.25017155\n",
      "Iteration 12435, loss = 3.00755346\n",
      "Iteration 12436, loss = 3.30583564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12437, loss = 3.18805268\n",
      "Iteration 12438, loss = 3.40261589\n",
      "Iteration 12439, loss = 3.48916287\n",
      "Iteration 12440, loss = 4.03671631\n",
      "Iteration 12441, loss = 4.03216286\n",
      "Iteration 12442, loss = 3.99517681\n",
      "Iteration 12443, loss = 4.20243769\n",
      "Iteration 12444, loss = 3.83962576\n",
      "Iteration 12445, loss = 3.02933479\n",
      "Iteration 12446, loss = 3.32084780\n",
      "Iteration 12447, loss = 3.14762718\n",
      "Iteration 12448, loss = 3.14837644\n",
      "Iteration 12449, loss = 6.09411154\n",
      "Iteration 12450, loss = 9.72870457\n",
      "Iteration 12451, loss = 14.77537529\n",
      "Iteration 12452, loss = 13.03428947\n",
      "Iteration 12453, loss = 9.37295172\n",
      "Iteration 12454, loss = 9.11336286\n",
      "Iteration 12455, loss = 7.83671283\n",
      "Iteration 12456, loss = 7.11496191\n",
      "Iteration 12457, loss = 5.48583901\n",
      "Iteration 12458, loss = 5.03365038\n",
      "Iteration 12459, loss = 5.32849827\n",
      "Iteration 12460, loss = 5.99896620\n",
      "Iteration 12461, loss = 5.13726816\n",
      "Iteration 12462, loss = 4.84509856\n",
      "Iteration 12463, loss = 4.87578846\n",
      "Iteration 12464, loss = 5.89872246\n",
      "Iteration 12465, loss = 6.42792439\n",
      "Iteration 12466, loss = 7.09198793\n",
      "Iteration 12467, loss = 4.15910415\n",
      "Iteration 12468, loss = 4.06928002\n",
      "Iteration 12469, loss = 4.15063257\n",
      "Iteration 12470, loss = 3.79232616\n",
      "Iteration 12471, loss = 3.51623670\n",
      "Iteration 12472, loss = 5.78700262\n",
      "Iteration 12473, loss = 6.14512835\n",
      "Iteration 12474, loss = 3.59070058\n",
      "Iteration 12475, loss = 3.87629046\n",
      "Iteration 12476, loss = 4.95411956\n",
      "Iteration 12477, loss = 3.98989062\n",
      "Iteration 12478, loss = 3.98241143\n",
      "Iteration 12479, loss = 4.71863834\n",
      "Iteration 12480, loss = 7.40322365\n",
      "Iteration 12481, loss = 6.18659472\n",
      "Iteration 12482, loss = 6.45207629\n",
      "Iteration 12483, loss = 4.42891541\n",
      "Iteration 12484, loss = 4.43238793\n",
      "Iteration 12485, loss = 4.34640971\n",
      "Iteration 12486, loss = 6.39310696\n",
      "Iteration 12487, loss = 3.42607922\n",
      "Iteration 12488, loss = 3.77616573\n",
      "Iteration 12489, loss = 4.71763243\n",
      "Iteration 12490, loss = 3.40186328\n",
      "Iteration 12491, loss = 3.73143917\n",
      "Iteration 12492, loss = 3.56445207\n",
      "Iteration 12493, loss = 3.24012019\n",
      "Iteration 12494, loss = 3.23640048\n",
      "Iteration 12495, loss = 3.36652608\n",
      "Iteration 12496, loss = 3.39992377\n",
      "Iteration 12497, loss = 3.47408416\n",
      "Iteration 12498, loss = 3.27940002\n",
      "Iteration 12499, loss = 3.39529109\n",
      "Iteration 12500, loss = 3.96809558\n",
      "Iteration 12501, loss = 3.50518810\n",
      "Iteration 12502, loss = 3.07963239\n",
      "Iteration 12503, loss = 3.21019659\n",
      "Iteration 12504, loss = 3.48442744\n",
      "Iteration 12505, loss = 3.10338363\n",
      "Iteration 12506, loss = 4.39188316\n",
      "Iteration 12507, loss = 3.21517452\n",
      "Iteration 12508, loss = 3.14629698\n",
      "Iteration 12509, loss = 2.98541086\n",
      "Iteration 12510, loss = 3.49458946\n",
      "Iteration 12511, loss = 3.93971749\n",
      "Iteration 12512, loss = 3.06616809\n",
      "Iteration 12513, loss = 3.03017182\n",
      "Iteration 12514, loss = 3.46460957\n",
      "Iteration 12515, loss = 3.27546325\n",
      "Iteration 12516, loss = 3.12178281\n",
      "Iteration 12517, loss = 3.01927351\n",
      "Iteration 12518, loss = 3.14192242\n",
      "Iteration 12519, loss = 3.49607401\n",
      "Iteration 12520, loss = 3.78928615\n",
      "Iteration 12521, loss = 3.78939850\n",
      "Iteration 12522, loss = 4.29281712\n",
      "Iteration 12523, loss = 3.81852557\n",
      "Iteration 12524, loss = 3.50410463\n",
      "Iteration 12525, loss = 3.80935290\n",
      "Iteration 12526, loss = 3.07607285\n",
      "Iteration 12527, loss = 3.67805900\n",
      "Iteration 12528, loss = 2.93605480\n",
      "Iteration 12529, loss = 4.23937116\n",
      "Iteration 12530, loss = 3.80230560\n",
      "Iteration 12531, loss = 3.24392765\n",
      "Iteration 12532, loss = 3.86340437\n",
      "Iteration 12533, loss = 3.08569207\n",
      "Iteration 12534, loss = 3.04480641\n",
      "Iteration 12535, loss = 3.28234115\n",
      "Iteration 12536, loss = 3.00782249\n",
      "Iteration 12537, loss = 3.07536571\n",
      "Iteration 12538, loss = 3.28898470\n",
      "Iteration 12539, loss = 3.09158021\n",
      "Iteration 12540, loss = 2.92984698\n",
      "Iteration 12541, loss = 3.05788024\n",
      "Iteration 12542, loss = 3.13968891\n",
      "Iteration 12543, loss = 3.02753313\n",
      "Iteration 12544, loss = 3.15851531\n",
      "Iteration 12545, loss = 3.35878259\n",
      "Iteration 12546, loss = 3.32555790\n",
      "Iteration 12547, loss = 5.44001576\n",
      "Iteration 12548, loss = 6.42422319\n",
      "Iteration 12549, loss = 4.45974374\n",
      "Iteration 12550, loss = 4.55028370\n",
      "Iteration 12551, loss = 4.09129185\n",
      "Iteration 12552, loss = 6.58942019\n",
      "Iteration 12553, loss = 4.85500252\n",
      "Iteration 12554, loss = 3.77850910\n",
      "Iteration 12555, loss = 3.00927743\n",
      "Iteration 12556, loss = 3.17548067\n",
      "Iteration 12557, loss = 4.44311976\n",
      "Iteration 12558, loss = 4.04125017\n",
      "Iteration 12559, loss = 3.24546225\n",
      "Iteration 12560, loss = 3.10852271\n",
      "Iteration 12561, loss = 3.73747424\n",
      "Iteration 12562, loss = 3.43394062\n",
      "Iteration 12563, loss = 4.27282846\n",
      "Iteration 12564, loss = 2.95496034\n",
      "Iteration 12565, loss = 3.12027290\n",
      "Iteration 12566, loss = 3.66634117\n",
      "Iteration 12567, loss = 3.78327761\n",
      "Iteration 12568, loss = 3.63758414\n",
      "Iteration 12569, loss = 3.29493860\n",
      "Iteration 12570, loss = 3.38467326\n",
      "Iteration 12571, loss = 3.69849007\n",
      "Iteration 12572, loss = 3.08172966\n",
      "Iteration 12573, loss = 3.30446761\n",
      "Iteration 12574, loss = 3.32666917\n",
      "Iteration 12575, loss = 3.09339159\n",
      "Iteration 12576, loss = 5.38610207\n",
      "Iteration 12577, loss = 5.94670296\n",
      "Iteration 12578, loss = 3.52929543\n",
      "Iteration 12579, loss = 3.28597834\n",
      "Iteration 12580, loss = 3.00255733\n",
      "Iteration 12581, loss = 2.98125825\n",
      "Iteration 12582, loss = 2.91779903\n",
      "Iteration 12583, loss = 3.20148868\n",
      "Iteration 12584, loss = 3.11116300\n",
      "Iteration 12585, loss = 3.47460162\n",
      "Iteration 12586, loss = 4.04290852\n",
      "Iteration 12587, loss = 3.02204652\n",
      "Iteration 12588, loss = 2.94661134\n",
      "Iteration 12589, loss = 3.19757323\n",
      "Iteration 12590, loss = 2.97548590\n",
      "Iteration 12591, loss = 3.18837415\n",
      "Iteration 12592, loss = 3.78607948\n",
      "Iteration 12593, loss = 2.97987810\n",
      "Iteration 12594, loss = 2.97587490\n",
      "Iteration 12595, loss = 2.91251097\n",
      "Iteration 12596, loss = 2.93229437\n",
      "Iteration 12597, loss = 2.98222965\n",
      "Iteration 12598, loss = 2.98730397\n",
      "Iteration 12599, loss = 2.86544292\n",
      "Iteration 12600, loss = 2.92392447\n",
      "Iteration 12601, loss = 3.43231805\n",
      "Iteration 12602, loss = 3.75279993\n",
      "Iteration 12603, loss = 3.42716551\n",
      "Iteration 12604, loss = 3.49078554\n",
      "Iteration 12605, loss = 3.57939937\n",
      "Iteration 12606, loss = 3.12754040\n",
      "Iteration 12607, loss = 3.48901995\n",
      "Iteration 12608, loss = 3.58565059\n",
      "Iteration 12609, loss = 4.24802146\n",
      "Iteration 12610, loss = 3.82597389\n",
      "Iteration 12611, loss = 4.39006999\n",
      "Iteration 12612, loss = 9.99494903\n",
      "Iteration 12613, loss = 14.05372178\n",
      "Iteration 12614, loss = 11.91566441\n",
      "Iteration 12615, loss = 6.01833345\n",
      "Iteration 12616, loss = 6.29376737\n",
      "Iteration 12617, loss = 5.76970082\n",
      "Iteration 12618, loss = 4.91118203\n",
      "Iteration 12619, loss = 3.65699586\n",
      "Iteration 12620, loss = 3.70962459\n",
      "Iteration 12621, loss = 3.76524170\n",
      "Iteration 12622, loss = 3.30956225\n",
      "Iteration 12623, loss = 3.71371674\n",
      "Iteration 12624, loss = 4.99336350\n",
      "Iteration 12625, loss = 5.34382907\n",
      "Iteration 12626, loss = 4.51753995\n",
      "Iteration 12627, loss = 3.20771592\n",
      "Iteration 12628, loss = 3.50051382\n",
      "Iteration 12629, loss = 3.41389271\n",
      "Iteration 12630, loss = 3.34874635\n",
      "Iteration 12631, loss = 3.14130474\n",
      "Iteration 12632, loss = 3.14695714\n",
      "Iteration 12633, loss = 3.14494771\n",
      "Iteration 12634, loss = 3.17581180\n",
      "Iteration 12635, loss = 3.28582225\n",
      "Iteration 12636, loss = 3.33259880\n",
      "Iteration 12637, loss = 2.95565150\n",
      "Iteration 12638, loss = 2.89369271\n",
      "Iteration 12639, loss = 2.96837389\n",
      "Iteration 12640, loss = 2.89900917\n",
      "Iteration 12641, loss = 2.86187500\n",
      "Iteration 12642, loss = 2.95745678\n",
      "Iteration 12643, loss = 2.85671809\n",
      "Iteration 12644, loss = 3.15943647\n",
      "Iteration 12645, loss = 3.11913958\n",
      "Iteration 12646, loss = 2.84966870\n",
      "Iteration 12647, loss = 3.17542067\n",
      "Iteration 12648, loss = 3.02977920\n",
      "Iteration 12649, loss = 4.52618130\n",
      "Iteration 12650, loss = 6.67749430\n",
      "Iteration 12651, loss = 5.42485890\n",
      "Iteration 12652, loss = 3.45856213\n",
      "Iteration 12653, loss = 3.11275538\n",
      "Iteration 12654, loss = 3.21018246\n",
      "Iteration 12655, loss = 3.18081575\n",
      "Iteration 12656, loss = 3.68194773\n",
      "Iteration 12657, loss = 2.98509501\n",
      "Iteration 12658, loss = 2.94235594\n",
      "Iteration 12659, loss = 3.56227480\n",
      "Iteration 12660, loss = 2.97529871\n",
      "Iteration 12661, loss = 3.24009492\n",
      "Iteration 12662, loss = 3.07734340\n",
      "Iteration 12663, loss = 3.53811729\n",
      "Iteration 12664, loss = 3.01986521\n",
      "Iteration 12665, loss = 3.30944631\n",
      "Iteration 12666, loss = 3.15242345\n",
      "Iteration 12667, loss = 2.99457292\n",
      "Iteration 12668, loss = 2.79949019\n",
      "Iteration 12669, loss = 2.75962590\n",
      "Iteration 12670, loss = 2.99859454\n",
      "Iteration 12671, loss = 3.07278204\n",
      "Iteration 12672, loss = 4.44985958\n",
      "Iteration 12673, loss = 3.94889638\n",
      "Iteration 12674, loss = 4.52576649\n",
      "Iteration 12675, loss = 8.05641400\n",
      "Iteration 12676, loss = 5.50049420\n",
      "Iteration 12677, loss = 4.03307811\n",
      "Iteration 12678, loss = 2.97500213\n",
      "Iteration 12679, loss = 2.96805156\n",
      "Iteration 12680, loss = 3.58792186\n",
      "Iteration 12681, loss = 2.96960128\n",
      "Iteration 12682, loss = 3.94873679\n",
      "Iteration 12683, loss = 3.09855629\n",
      "Iteration 12684, loss = 3.08033370\n",
      "Iteration 12685, loss = 4.82127345\n",
      "Iteration 12686, loss = 3.69770998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12687, loss = 3.47922014\n",
      "Iteration 12688, loss = 5.92684846\n",
      "Iteration 12689, loss = 3.71034591\n",
      "Iteration 12690, loss = 3.21568648\n",
      "Iteration 12691, loss = 2.78585047\n",
      "Iteration 12692, loss = 2.91549798\n",
      "Iteration 12693, loss = 3.64642177\n",
      "Iteration 12694, loss = 3.41829444\n",
      "Iteration 12695, loss = 3.00126088\n",
      "Iteration 12696, loss = 2.96788962\n",
      "Iteration 12697, loss = 3.47353288\n",
      "Iteration 12698, loss = 3.00760815\n",
      "Iteration 12699, loss = 3.24936017\n",
      "Iteration 12700, loss = 2.71322763\n",
      "Iteration 12701, loss = 3.27245331\n",
      "Iteration 12702, loss = 3.82527739\n",
      "Iteration 12703, loss = 3.17032982\n",
      "Iteration 12704, loss = 2.95748560\n",
      "Iteration 12705, loss = 3.01465910\n",
      "Iteration 12706, loss = 3.05266713\n",
      "Iteration 12707, loss = 4.65229885\n",
      "Iteration 12708, loss = 7.33544115\n",
      "Iteration 12709, loss = 5.34745053\n",
      "Iteration 12710, loss = 5.32185183\n",
      "Iteration 12711, loss = 4.58394537\n",
      "Iteration 12712, loss = 5.33738512\n",
      "Iteration 12713, loss = 3.66135589\n",
      "Iteration 12714, loss = 3.14316060\n",
      "Iteration 12715, loss = 3.64534794\n",
      "Iteration 12716, loss = 2.97272020\n",
      "Iteration 12717, loss = 3.07205832\n",
      "Iteration 12718, loss = 3.16552050\n",
      "Iteration 12719, loss = 2.81509205\n",
      "Iteration 12720, loss = 2.90400267\n",
      "Iteration 12721, loss = 2.92451083\n",
      "Iteration 12722, loss = 2.83869889\n",
      "Iteration 12723, loss = 3.06008643\n",
      "Iteration 12724, loss = 3.44719635\n",
      "Iteration 12725, loss = 4.73518801\n",
      "Iteration 12726, loss = 3.70164178\n",
      "Iteration 12727, loss = 3.19952405\n",
      "Iteration 12728, loss = 3.52047172\n",
      "Iteration 12729, loss = 3.29909873\n",
      "Iteration 12730, loss = 5.29195114\n",
      "Iteration 12731, loss = 3.55060668\n",
      "Iteration 12732, loss = 5.73734850\n",
      "Iteration 12733, loss = 4.48313505\n",
      "Iteration 12734, loss = 4.93285343\n",
      "Iteration 12735, loss = 3.77673023\n",
      "Iteration 12736, loss = 4.17738371\n",
      "Iteration 12737, loss = 3.42912478\n",
      "Iteration 12738, loss = 3.45628524\n",
      "Iteration 12739, loss = 3.11345123\n",
      "Iteration 12740, loss = 3.38511269\n",
      "Iteration 12741, loss = 2.91499124\n",
      "Iteration 12742, loss = 3.19688156\n",
      "Iteration 12743, loss = 3.11190565\n",
      "Iteration 12744, loss = 8.44374649\n",
      "Iteration 12745, loss = 9.62420523\n",
      "Iteration 12746, loss = 6.62051889\n",
      "Iteration 12747, loss = 4.08211591\n",
      "Iteration 12748, loss = 4.29094481\n",
      "Iteration 12749, loss = 3.64135039\n",
      "Iteration 12750, loss = 3.41767066\n",
      "Iteration 12751, loss = 4.57551038\n",
      "Iteration 12752, loss = 6.26381029\n",
      "Iteration 12753, loss = 4.25803601\n",
      "Iteration 12754, loss = 2.92504425\n",
      "Iteration 12755, loss = 3.28986357\n",
      "Iteration 12756, loss = 2.89036240\n",
      "Iteration 12757, loss = 3.15527054\n",
      "Iteration 12758, loss = 2.84880292\n",
      "Iteration 12759, loss = 3.15081461\n",
      "Iteration 12760, loss = 3.24313685\n",
      "Iteration 12761, loss = 3.17044195\n",
      "Iteration 12762, loss = 2.85528209\n",
      "Iteration 12763, loss = 3.36822717\n",
      "Iteration 12764, loss = 2.95828792\n",
      "Iteration 12765, loss = 3.87220810\n",
      "Iteration 12766, loss = 3.33200201\n",
      "Iteration 12767, loss = 2.93545809\n",
      "Iteration 12768, loss = 2.79553796\n",
      "Iteration 12769, loss = 3.00000883\n",
      "Iteration 12770, loss = 3.76135710\n",
      "Iteration 12771, loss = 3.72784485\n",
      "Iteration 12772, loss = 3.31564455\n",
      "Iteration 12773, loss = 2.99751686\n",
      "Iteration 12774, loss = 2.88405003\n",
      "Iteration 12775, loss = 2.77610981\n",
      "Iteration 12776, loss = 2.78196330\n",
      "Iteration 12777, loss = 3.01905420\n",
      "Iteration 12778, loss = 3.35191660\n",
      "Iteration 12779, loss = 2.87807023\n",
      "Iteration 12780, loss = 3.01099773\n",
      "Iteration 12781, loss = 2.92106504\n",
      "Iteration 12782, loss = 2.80589837\n",
      "Iteration 12783, loss = 2.81947865\n",
      "Iteration 12784, loss = 2.82394290\n",
      "Iteration 12785, loss = 2.82410369\n",
      "Iteration 12786, loss = 2.79915002\n",
      "Iteration 12787, loss = 3.18135884\n",
      "Iteration 12788, loss = 2.74547033\n",
      "Iteration 12789, loss = 3.00282120\n",
      "Iteration 12790, loss = 2.83491758\n",
      "Iteration 12791, loss = 2.77722499\n",
      "Iteration 12792, loss = 2.83231079\n",
      "Iteration 12793, loss = 2.71014780\n",
      "Iteration 12794, loss = 2.70661241\n",
      "Iteration 12795, loss = 2.83580919\n",
      "Iteration 12796, loss = 2.93455169\n",
      "Iteration 12797, loss = 2.82891682\n",
      "Iteration 12798, loss = 3.18560541\n",
      "Iteration 12799, loss = 2.99494361\n",
      "Iteration 12800, loss = 2.67633840\n",
      "Iteration 12801, loss = 2.89737458\n",
      "Iteration 12802, loss = 3.46849062\n",
      "Iteration 12803, loss = 4.96335729\n",
      "Iteration 12804, loss = 3.29936485\n",
      "Iteration 12805, loss = 2.96920691\n",
      "Iteration 12806, loss = 3.30797584\n",
      "Iteration 12807, loss = 2.99455206\n",
      "Iteration 12808, loss = 4.16157664\n",
      "Iteration 12809, loss = 5.27997158\n",
      "Iteration 12810, loss = 3.42801080\n",
      "Iteration 12811, loss = 3.84284912\n",
      "Iteration 12812, loss = 3.65881962\n",
      "Iteration 12813, loss = 5.46744701\n",
      "Iteration 12814, loss = 3.72823649\n",
      "Iteration 12815, loss = 3.09752441\n",
      "Iteration 12816, loss = 2.86344921\n",
      "Iteration 12817, loss = 2.77065857\n",
      "Iteration 12818, loss = 3.05820853\n",
      "Iteration 12819, loss = 2.88025659\n",
      "Iteration 12820, loss = 2.84648701\n",
      "Iteration 12821, loss = 3.01424870\n",
      "Iteration 12822, loss = 2.75324357\n",
      "Iteration 12823, loss = 2.96125565\n",
      "Iteration 12824, loss = 3.05986541\n",
      "Iteration 12825, loss = 2.81607137\n",
      "Iteration 12826, loss = 2.67908486\n",
      "Iteration 12827, loss = 2.77155446\n",
      "Iteration 12828, loss = 2.92510041\n",
      "Iteration 12829, loss = 3.10491964\n",
      "Iteration 12830, loss = 3.70117285\n",
      "Iteration 12831, loss = 3.59183674\n",
      "Iteration 12832, loss = 5.10816981\n",
      "Iteration 12833, loss = 4.54893614\n",
      "Iteration 12834, loss = 4.01247597\n",
      "Iteration 12835, loss = 3.90550879\n",
      "Iteration 12836, loss = 3.99883299\n",
      "Iteration 12837, loss = 3.04688559\n",
      "Iteration 12838, loss = 3.02272171\n",
      "Iteration 12839, loss = 2.76362715\n",
      "Iteration 12840, loss = 3.08177877\n",
      "Iteration 12841, loss = 2.95838470\n",
      "Iteration 12842, loss = 2.93155093\n",
      "Iteration 12843, loss = 4.47072074\n",
      "Iteration 12844, loss = 3.11122032\n",
      "Iteration 12845, loss = 3.06612348\n",
      "Iteration 12846, loss = 2.73892894\n",
      "Iteration 12847, loss = 2.96235139\n",
      "Iteration 12848, loss = 2.79931791\n",
      "Iteration 12849, loss = 2.64705215\n",
      "Iteration 12850, loss = 2.82852115\n",
      "Iteration 12851, loss = 2.84307123\n",
      "Iteration 12852, loss = 3.27099289\n",
      "Iteration 12853, loss = 3.04266731\n",
      "Iteration 12854, loss = 3.31270075\n",
      "Iteration 12855, loss = 2.75746962\n",
      "Iteration 12856, loss = 2.62863705\n",
      "Iteration 12857, loss = 3.07890277\n",
      "Iteration 12858, loss = 2.93326947\n",
      "Iteration 12859, loss = 2.75105751\n",
      "Iteration 12860, loss = 3.03966464\n",
      "Iteration 12861, loss = 2.78207970\n",
      "Iteration 12862, loss = 3.43365298\n",
      "Iteration 12863, loss = 2.98903483\n",
      "Iteration 12864, loss = 3.89892852\n",
      "Iteration 12865, loss = 3.25441338\n",
      "Iteration 12866, loss = 2.90349929\n",
      "Iteration 12867, loss = 3.10382928\n",
      "Iteration 12868, loss = 5.37362560\n",
      "Iteration 12869, loss = 3.45669836\n",
      "Iteration 12870, loss = 3.63705621\n",
      "Iteration 12871, loss = 3.07389511\n",
      "Iteration 12872, loss = 2.88502373\n",
      "Iteration 12873, loss = 2.93027309\n",
      "Iteration 12874, loss = 2.96135500\n",
      "Iteration 12875, loss = 4.69957888\n",
      "Iteration 12876, loss = 4.67040302\n",
      "Iteration 12877, loss = 3.80117142\n",
      "Iteration 12878, loss = 3.03349321\n",
      "Iteration 12879, loss = 2.88731776\n",
      "Iteration 12880, loss = 2.76253391\n",
      "Iteration 12881, loss = 2.86371216\n",
      "Iteration 12882, loss = 2.86348736\n",
      "Iteration 12883, loss = 2.81629319\n",
      "Iteration 12884, loss = 2.78162513\n",
      "Iteration 12885, loss = 2.66319749\n",
      "Iteration 12886, loss = 2.79732959\n",
      "Iteration 12887, loss = 2.86580728\n",
      "Iteration 12888, loss = 2.82993234\n",
      "Iteration 12889, loss = 2.97989141\n",
      "Iteration 12890, loss = 2.73503024\n",
      "Iteration 12891, loss = 2.87008985\n",
      "Iteration 12892, loss = 2.96959866\n",
      "Iteration 12893, loss = 2.94725073\n",
      "Iteration 12894, loss = 2.77469721\n",
      "Iteration 12895, loss = 2.85319890\n",
      "Iteration 12896, loss = 3.56183833\n",
      "Iteration 12897, loss = 3.19146279\n",
      "Iteration 12898, loss = 2.68354345\n",
      "Iteration 12899, loss = 3.12959284\n",
      "Iteration 12900, loss = 2.90523365\n",
      "Iteration 12901, loss = 3.28060789\n",
      "Iteration 12902, loss = 2.67546097\n",
      "Iteration 12903, loss = 2.74296259\n",
      "Iteration 12904, loss = 3.52956657\n",
      "Iteration 12905, loss = 3.31308785\n",
      "Iteration 12906, loss = 3.18595038\n",
      "Iteration 12907, loss = 2.94632199\n",
      "Iteration 12908, loss = 2.63627408\n",
      "Iteration 12909, loss = 2.67306234\n",
      "Iteration 12910, loss = 2.80009718\n",
      "Iteration 12911, loss = 2.71057400\n",
      "Iteration 12912, loss = 2.71166544\n",
      "Iteration 12913, loss = 2.94987108\n",
      "Iteration 12914, loss = 2.73104833\n",
      "Iteration 12915, loss = 2.67408510\n",
      "Iteration 12916, loss = 3.25679998\n",
      "Iteration 12917, loss = 2.96717796\n",
      "Iteration 12918, loss = 2.77178990\n",
      "Iteration 12919, loss = 2.95353414\n",
      "Iteration 12920, loss = 3.58862984\n",
      "Iteration 12921, loss = 3.40505499\n",
      "Iteration 12922, loss = 3.00831806\n",
      "Iteration 12923, loss = 2.84100598\n",
      "Iteration 12924, loss = 2.92031005\n",
      "Iteration 12925, loss = 4.11063505\n",
      "Iteration 12926, loss = 3.41634106\n",
      "Iteration 12927, loss = 3.23168174\n",
      "Iteration 12928, loss = 3.38820264\n",
      "Iteration 12929, loss = 3.30411832\n",
      "Iteration 12930, loss = 2.73288841\n",
      "Iteration 12931, loss = 3.07999079\n",
      "Iteration 12932, loss = 2.68070508\n",
      "Iteration 12933, loss = 3.32082734\n",
      "Iteration 12934, loss = 3.23164940\n",
      "Iteration 12935, loss = 2.65084023\n",
      "Iteration 12936, loss = 2.50204151\n",
      "Iteration 12937, loss = 2.62737916\n",
      "Iteration 12938, loss = 2.68781060\n",
      "Iteration 12939, loss = 2.57556939\n",
      "Iteration 12940, loss = 2.79006640\n",
      "Iteration 12941, loss = 2.68813339\n",
      "Iteration 12942, loss = 2.52828532\n",
      "Iteration 12943, loss = 2.60195696\n",
      "Iteration 12944, loss = 2.87299921\n",
      "Iteration 12945, loss = 3.89017727\n",
      "Iteration 12946, loss = 5.63259580\n",
      "Iteration 12947, loss = 4.23558843\n",
      "Iteration 12948, loss = 5.33028116\n",
      "Iteration 12949, loss = 7.55910361\n",
      "Iteration 12950, loss = 8.19504956\n",
      "Iteration 12951, loss = 5.18791185\n",
      "Iteration 12952, loss = 3.45529278\n",
      "Iteration 12953, loss = 3.25190745\n",
      "Iteration 12954, loss = 3.24347807\n",
      "Iteration 12955, loss = 2.67747820\n",
      "Iteration 12956, loss = 2.92475964\n",
      "Iteration 12957, loss = 3.42858260\n",
      "Iteration 12958, loss = 2.67075955\n",
      "Iteration 12959, loss = 2.99867244\n",
      "Iteration 12960, loss = 3.07043744\n",
      "Iteration 12961, loss = 2.77723979\n",
      "Iteration 12962, loss = 2.74692900\n",
      "Iteration 12963, loss = 2.62435018\n",
      "Iteration 12964, loss = 2.55967079\n",
      "Iteration 12965, loss = 2.54186899\n",
      "Iteration 12966, loss = 2.62619120\n",
      "Iteration 12967, loss = 2.78139848\n",
      "Iteration 12968, loss = 2.53789550\n",
      "Iteration 12969, loss = 2.66528319\n",
      "Iteration 12970, loss = 3.16305523\n",
      "Iteration 12971, loss = 2.90502363\n",
      "Iteration 12972, loss = 2.98827037\n",
      "Iteration 12973, loss = 2.74658399\n",
      "Iteration 12974, loss = 2.61049457\n",
      "Iteration 12975, loss = 2.50038781\n",
      "Iteration 12976, loss = 2.78192342\n",
      "Iteration 12977, loss = 2.52213019\n",
      "Iteration 12978, loss = 2.67759290\n",
      "Iteration 12979, loss = 3.80862897\n",
      "Iteration 12980, loss = 3.64715612\n",
      "Iteration 12981, loss = 3.74932648\n",
      "Iteration 12982, loss = 3.19894733\n",
      "Iteration 12983, loss = 2.87602830\n",
      "Iteration 12984, loss = 2.66105336\n",
      "Iteration 12985, loss = 2.89621005\n",
      "Iteration 12986, loss = 3.19849303\n",
      "Iteration 12987, loss = 3.45435807\n",
      "Iteration 12988, loss = 3.81795298\n",
      "Iteration 12989, loss = 3.59015757\n",
      "Iteration 12990, loss = 3.28450931\n",
      "Iteration 12991, loss = 3.34079608\n",
      "Iteration 12992, loss = 2.75567352\n",
      "Iteration 12993, loss = 2.94611859\n",
      "Iteration 12994, loss = 2.66000797\n",
      "Iteration 12995, loss = 2.68212721\n",
      "Iteration 12996, loss = 4.34796521\n",
      "Iteration 12997, loss = 4.67794592\n",
      "Iteration 12998, loss = 4.30939746\n",
      "Iteration 12999, loss = 4.58185097\n",
      "Iteration 13000, loss = 3.84063592\n",
      "Iteration 13001, loss = 2.85011980\n",
      "Iteration 13002, loss = 4.23251036\n",
      "Iteration 13003, loss = 3.37279615\n",
      "Iteration 13004, loss = 3.50508944\n",
      "Iteration 13005, loss = 4.21385131\n",
      "Iteration 13006, loss = 4.97499511\n",
      "Iteration 13007, loss = 6.98638389\n",
      "Iteration 13008, loss = 4.87874398\n",
      "Iteration 13009, loss = 6.86335708\n",
      "Iteration 13010, loss = 9.08993726\n",
      "Iteration 13011, loss = 9.70810920\n",
      "Iteration 13012, loss = 8.36618098\n",
      "Iteration 13013, loss = 7.17190399\n",
      "Iteration 13014, loss = 6.18605711\n",
      "Iteration 13015, loss = 3.93678893\n",
      "Iteration 13016, loss = 3.63429570\n",
      "Iteration 13017, loss = 4.01096454\n",
      "Iteration 13018, loss = 5.05771067\n",
      "Iteration 13019, loss = 5.60774649\n",
      "Iteration 13020, loss = 4.98603991\n",
      "Iteration 13021, loss = 3.21060641\n",
      "Iteration 13022, loss = 2.81074576\n",
      "Iteration 13023, loss = 3.24484343\n",
      "Iteration 13024, loss = 3.08103000\n",
      "Iteration 13025, loss = 3.10824267\n",
      "Iteration 13026, loss = 3.14428847\n",
      "Iteration 13027, loss = 2.78309426\n",
      "Iteration 13028, loss = 2.88896715\n",
      "Iteration 13029, loss = 2.90247263\n",
      "Iteration 13030, loss = 2.79507938\n",
      "Iteration 13031, loss = 2.84192922\n",
      "Iteration 13032, loss = 2.71050031\n",
      "Iteration 13033, loss = 4.56108154\n",
      "Iteration 13034, loss = 5.85339509\n",
      "Iteration 13035, loss = 4.05802050\n",
      "Iteration 13036, loss = 3.28959884\n",
      "Iteration 13037, loss = 3.00910000\n",
      "Iteration 13038, loss = 2.90082647\n",
      "Iteration 13039, loss = 2.83337115\n",
      "Iteration 13040, loss = 3.75725264\n",
      "Iteration 13041, loss = 4.01269342\n",
      "Iteration 13042, loss = 4.08675678\n",
      "Iteration 13043, loss = 2.51109183\n",
      "Iteration 13044, loss = 2.62847818\n",
      "Iteration 13045, loss = 2.69837733\n",
      "Iteration 13046, loss = 2.88618114\n",
      "Iteration 13047, loss = 2.55261609\n",
      "Iteration 13048, loss = 3.17092608\n",
      "Iteration 13049, loss = 2.93749676\n",
      "Iteration 13050, loss = 3.02694782\n",
      "Iteration 13051, loss = 2.83757120\n",
      "Iteration 13052, loss = 3.30529600\n",
      "Iteration 13053, loss = 2.75082568\n",
      "Iteration 13054, loss = 2.46997665\n",
      "Iteration 13055, loss = 2.60602656\n",
      "Iteration 13056, loss = 2.82076798\n",
      "Iteration 13057, loss = 2.85041753\n",
      "Iteration 13058, loss = 2.89850958\n",
      "Iteration 13059, loss = 3.07785687\n",
      "Iteration 13060, loss = 2.89383558\n",
      "Iteration 13061, loss = 2.47745117\n",
      "Iteration 13062, loss = 2.63688830\n",
      "Iteration 13063, loss = 2.92359923\n",
      "Iteration 13064, loss = 2.61789795\n",
      "Iteration 13065, loss = 2.55962189\n",
      "Iteration 13066, loss = 2.47385482\n",
      "Iteration 13067, loss = 2.70396121\n",
      "Iteration 13068, loss = 3.21347366\n",
      "Iteration 13069, loss = 3.40116012\n",
      "Iteration 13070, loss = 2.94257787\n",
      "Iteration 13071, loss = 3.72640938\n",
      "Iteration 13072, loss = 3.00689593\n",
      "Iteration 13073, loss = 2.77499671\n",
      "Iteration 13074, loss = 2.67706347\n",
      "Iteration 13075, loss = 3.06537768\n",
      "Iteration 13076, loss = 2.55500936\n",
      "Iteration 13077, loss = 2.87860419\n",
      "Iteration 13078, loss = 2.74189434\n",
      "Iteration 13079, loss = 4.06565470\n",
      "Iteration 13080, loss = 2.69112763\n",
      "Iteration 13081, loss = 3.45555780\n",
      "Iteration 13082, loss = 3.01338980\n",
      "Iteration 13083, loss = 2.72995836\n",
      "Iteration 13084, loss = 2.46713999\n",
      "Iteration 13085, loss = 2.54593334\n",
      "Iteration 13086, loss = 2.54048938\n",
      "Iteration 13087, loss = 2.83112319\n",
      "Iteration 13088, loss = 2.56832126\n",
      "Iteration 13089, loss = 2.62542073\n",
      "Iteration 13090, loss = 2.51502201\n",
      "Iteration 13091, loss = 2.81525622\n",
      "Iteration 13092, loss = 2.77409345\n",
      "Iteration 13093, loss = 3.37451652\n",
      "Iteration 13094, loss = 4.11238834\n",
      "Iteration 13095, loss = 3.09903216\n",
      "Iteration 13096, loss = 2.63495686\n",
      "Iteration 13097, loss = 4.62264231\n",
      "Iteration 13098, loss = 4.60438254\n",
      "Iteration 13099, loss = 3.24356593\n",
      "Iteration 13100, loss = 3.38476357\n",
      "Iteration 13101, loss = 3.19880697\n",
      "Iteration 13102, loss = 2.71984700\n",
      "Iteration 13103, loss = 2.69871275\n",
      "Iteration 13104, loss = 3.05902518\n",
      "Iteration 13105, loss = 2.45427814\n",
      "Iteration 13106, loss = 2.68084323\n",
      "Iteration 13107, loss = 2.76802020\n",
      "Iteration 13108, loss = 2.91125029\n",
      "Iteration 13109, loss = 2.80064677\n",
      "Iteration 13110, loss = 2.77665190\n",
      "Iteration 13111, loss = 2.53237974\n",
      "Iteration 13112, loss = 3.33312513\n",
      "Iteration 13113, loss = 2.69220274\n",
      "Iteration 13114, loss = 3.11433825\n",
      "Iteration 13115, loss = 2.55638004\n",
      "Iteration 13116, loss = 2.52793364\n",
      "Iteration 13117, loss = 2.46019022\n",
      "Iteration 13118, loss = 2.36732303\n",
      "Iteration 13119, loss = 2.44854661\n",
      "Iteration 13120, loss = 2.59434386\n",
      "Iteration 13121, loss = 4.35924274\n",
      "Iteration 13122, loss = 4.22620110\n",
      "Iteration 13123, loss = 3.83515883\n",
      "Iteration 13124, loss = 7.62366381\n",
      "Iteration 13125, loss = 5.95983316\n",
      "Iteration 13126, loss = 4.30143233\n",
      "Iteration 13127, loss = 4.39352829\n",
      "Iteration 13128, loss = 3.36761018\n",
      "Iteration 13129, loss = 2.83079887\n",
      "Iteration 13130, loss = 3.62534698\n",
      "Iteration 13131, loss = 3.02467871\n",
      "Iteration 13132, loss = 2.61083188\n",
      "Iteration 13133, loss = 2.86383431\n",
      "Iteration 13134, loss = 2.64756373\n",
      "Iteration 13135, loss = 2.57915018\n",
      "Iteration 13136, loss = 2.66186500\n",
      "Iteration 13137, loss = 2.66953709\n",
      "Iteration 13138, loss = 2.72203299\n",
      "Iteration 13139, loss = 2.70964735\n",
      "Iteration 13140, loss = 2.67719030\n",
      "Iteration 13141, loss = 3.01979857\n",
      "Iteration 13142, loss = 2.60964311\n",
      "Iteration 13143, loss = 2.73481231\n",
      "Iteration 13144, loss = 2.48034693\n",
      "Iteration 13145, loss = 3.06512171\n",
      "Iteration 13146, loss = 2.52276231\n",
      "Iteration 13147, loss = 2.62785146\n",
      "Iteration 13148, loss = 2.51335278\n",
      "Iteration 13149, loss = 2.64358422\n",
      "Iteration 13150, loss = 2.89530578\n",
      "Iteration 13151, loss = 2.95471757\n",
      "Iteration 13152, loss = 2.46182927\n",
      "Iteration 13153, loss = 2.48658141\n",
      "Iteration 13154, loss = 2.47112932\n",
      "Iteration 13155, loss = 2.39325617\n",
      "Iteration 13156, loss = 3.02711596\n",
      "Iteration 13157, loss = 2.99378829\n",
      "Iteration 13158, loss = 2.47496247\n",
      "Iteration 13159, loss = 2.45258506\n",
      "Iteration 13160, loss = 2.45147138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13161, loss = 2.52022703\n",
      "Iteration 13162, loss = 2.52489841\n",
      "Iteration 13163, loss = 2.48711811\n",
      "Iteration 13164, loss = 2.76099778\n",
      "Iteration 13165, loss = 2.65487961\n",
      "Iteration 13166, loss = 2.59261617\n",
      "Iteration 13167, loss = 2.24908997\n",
      "Iteration 13168, loss = 3.01952792\n",
      "Iteration 13169, loss = 4.85598300\n",
      "Iteration 13170, loss = 7.68418396\n",
      "Iteration 13171, loss = 9.06852290\n",
      "Iteration 13172, loss = 15.05922127\n",
      "Iteration 13173, loss = 10.37041529\n",
      "Iteration 13174, loss = 10.65030913\n",
      "Iteration 13175, loss = 11.26617011\n",
      "Iteration 13176, loss = 8.25620982\n",
      "Iteration 13177, loss = 7.98938955\n",
      "Iteration 13178, loss = 4.01836960\n",
      "Iteration 13179, loss = 4.34451842\n",
      "Iteration 13180, loss = 3.63364329\n",
      "Iteration 13181, loss = 2.78778948\n",
      "Iteration 13182, loss = 2.91990573\n",
      "Iteration 13183, loss = 2.81883954\n",
      "Iteration 13184, loss = 2.53032476\n",
      "Iteration 13185, loss = 2.69789619\n",
      "Iteration 13186, loss = 2.78949540\n",
      "Iteration 13187, loss = 2.87846067\n",
      "Iteration 13188, loss = 2.80363467\n",
      "Iteration 13189, loss = 2.64808121\n",
      "Iteration 13190, loss = 2.51628810\n",
      "Iteration 13191, loss = 2.71522613\n",
      "Iteration 13192, loss = 2.80187806\n",
      "Iteration 13193, loss = 2.63166674\n",
      "Iteration 13194, loss = 2.71488242\n",
      "Iteration 13195, loss = 2.44846735\n",
      "Iteration 13196, loss = 2.70710186\n",
      "Iteration 13197, loss = 2.47372534\n",
      "Iteration 13198, loss = 2.69261742\n",
      "Iteration 13199, loss = 4.60565268\n",
      "Iteration 13200, loss = 3.79727196\n",
      "Iteration 13201, loss = 2.94182155\n",
      "Iteration 13202, loss = 3.13987179\n",
      "Iteration 13203, loss = 2.99674901\n",
      "Iteration 13204, loss = 2.60740946\n",
      "Iteration 13205, loss = 2.67891169\n",
      "Iteration 13206, loss = 2.45754745\n",
      "Iteration 13207, loss = 2.57478228\n",
      "Iteration 13208, loss = 3.29978467\n",
      "Iteration 13209, loss = 3.60060309\n",
      "Iteration 13210, loss = 2.79374173\n",
      "Iteration 13211, loss = 3.01681399\n",
      "Iteration 13212, loss = 3.24669053\n",
      "Iteration 13213, loss = 3.89857262\n",
      "Iteration 13214, loss = 3.08796850\n",
      "Iteration 13215, loss = 3.18645108\n",
      "Iteration 13216, loss = 2.61573005\n",
      "Iteration 13217, loss = 2.67695970\n",
      "Iteration 13218, loss = 2.48270144\n",
      "Iteration 13219, loss = 2.52856951\n",
      "Iteration 13220, loss = 2.38962282\n",
      "Iteration 13221, loss = 2.55599778\n",
      "Iteration 13222, loss = 2.60819694\n",
      "Iteration 13223, loss = 2.53752973\n",
      "Iteration 13224, loss = 2.53396909\n",
      "Iteration 13225, loss = 3.39877774\n",
      "Iteration 13226, loss = 2.90281815\n",
      "Iteration 13227, loss = 2.43352078\n",
      "Iteration 13228, loss = 2.52845757\n",
      "Iteration 13229, loss = 2.71950002\n",
      "Iteration 13230, loss = 2.48683753\n",
      "Iteration 13231, loss = 3.51283524\n",
      "Iteration 13232, loss = 2.62002248\n",
      "Iteration 13233, loss = 2.90758743\n",
      "Iteration 13234, loss = 5.71089046\n",
      "Iteration 13235, loss = 5.57105889\n",
      "Iteration 13236, loss = 3.35508505\n",
      "Iteration 13237, loss = 2.55458606\n",
      "Iteration 13238, loss = 2.68031908\n",
      "Iteration 13239, loss = 2.53279689\n",
      "Iteration 13240, loss = 2.61050722\n",
      "Iteration 13241, loss = 2.59784540\n",
      "Iteration 13242, loss = 4.88266415\n",
      "Iteration 13243, loss = 4.09409387\n",
      "Iteration 13244, loss = 2.74169928\n",
      "Iteration 13245, loss = 2.50688230\n",
      "Iteration 13246, loss = 4.29060595\n",
      "Iteration 13247, loss = 3.91808511\n",
      "Iteration 13248, loss = 3.66421196\n",
      "Iteration 13249, loss = 2.57170975\n",
      "Iteration 13250, loss = 2.75384139\n",
      "Iteration 13251, loss = 2.54198013\n",
      "Iteration 13252, loss = 3.62815414\n",
      "Iteration 13253, loss = 4.52527000\n",
      "Iteration 13254, loss = 3.33768536\n",
      "Iteration 13255, loss = 3.27735005\n",
      "Iteration 13256, loss = 3.08829327\n",
      "Iteration 13257, loss = 2.56300050\n",
      "Iteration 13258, loss = 2.86888815\n",
      "Iteration 13259, loss = 2.45953937\n",
      "Iteration 13260, loss = 2.45470033\n",
      "Iteration 13261, loss = 2.40795906\n",
      "Iteration 13262, loss = 2.73406269\n",
      "Iteration 13263, loss = 2.85188322\n",
      "Iteration 13264, loss = 3.67194824\n",
      "Iteration 13265, loss = 3.04935614\n",
      "Iteration 13266, loss = 2.72843683\n",
      "Iteration 13267, loss = 2.86284523\n",
      "Iteration 13268, loss = 2.99399913\n",
      "Iteration 13269, loss = 2.86791495\n",
      "Iteration 13270, loss = 2.78149746\n",
      "Iteration 13271, loss = 2.69709116\n",
      "Iteration 13272, loss = 3.00124184\n",
      "Iteration 13273, loss = 3.38472291\n",
      "Iteration 13274, loss = 2.70573823\n",
      "Iteration 13275, loss = 2.80941363\n",
      "Iteration 13276, loss = 2.99859378\n",
      "Iteration 13277, loss = 3.03996170\n",
      "Iteration 13278, loss = 3.79256516\n",
      "Iteration 13279, loss = 3.65511919\n",
      "Iteration 13280, loss = 4.23209658\n",
      "Iteration 13281, loss = 5.96883696\n",
      "Iteration 13282, loss = 4.54482406\n",
      "Iteration 13283, loss = 4.05439876\n",
      "Iteration 13284, loss = 3.12911873\n",
      "Iteration 13285, loss = 3.95842256\n",
      "Iteration 13286, loss = 3.33239276\n",
      "Iteration 13287, loss = 2.47371546\n",
      "Iteration 13288, loss = 2.43944972\n",
      "Iteration 13289, loss = 2.85369872\n",
      "Iteration 13290, loss = 2.77658965\n",
      "Iteration 13291, loss = 4.31447138\n",
      "Iteration 13292, loss = 4.13507174\n",
      "Iteration 13293, loss = 4.03072889\n",
      "Iteration 13294, loss = 3.41068686\n",
      "Iteration 13295, loss = 2.70715816\n",
      "Iteration 13296, loss = 2.81961149\n",
      "Iteration 13297, loss = 2.42216118\n",
      "Iteration 13298, loss = 2.31525752\n",
      "Iteration 13299, loss = 2.37197987\n",
      "Iteration 13300, loss = 2.29171675\n",
      "Iteration 13301, loss = 2.73615295\n",
      "Iteration 13302, loss = 2.47188236\n",
      "Iteration 13303, loss = 2.65662379\n",
      "Iteration 13304, loss = 2.24601267\n",
      "Iteration 13305, loss = 2.59306769\n",
      "Iteration 13306, loss = 2.56374427\n",
      "Iteration 13307, loss = 2.86253184\n",
      "Iteration 13308, loss = 2.48741344\n",
      "Iteration 13309, loss = 3.61748593\n",
      "Iteration 13310, loss = 2.99856362\n",
      "Iteration 13311, loss = 2.35552363\n",
      "Iteration 13312, loss = 2.56814340\n",
      "Iteration 13313, loss = 2.39063049\n",
      "Iteration 13314, loss = 2.46524782\n",
      "Iteration 13315, loss = 3.42439148\n",
      "Iteration 13316, loss = 3.77294421\n",
      "Iteration 13317, loss = 2.68580858\n",
      "Iteration 13318, loss = 3.68016031\n",
      "Iteration 13319, loss = 3.08004265\n",
      "Iteration 13320, loss = 4.93494342\n",
      "Iteration 13321, loss = 2.76525755\n",
      "Iteration 13322, loss = 2.69133220\n",
      "Iteration 13323, loss = 3.00079401\n",
      "Iteration 13324, loss = 3.50891380\n",
      "Iteration 13325, loss = 2.88154303\n",
      "Iteration 13326, loss = 3.23417761\n",
      "Iteration 13327, loss = 2.79687517\n",
      "Iteration 13328, loss = 3.26484692\n",
      "Iteration 13329, loss = 2.63305021\n",
      "Iteration 13330, loss = 2.57003911\n",
      "Iteration 13331, loss = 2.65857386\n",
      "Iteration 13332, loss = 2.40820791\n",
      "Iteration 13333, loss = 2.62797572\n",
      "Iteration 13334, loss = 2.63186521\n",
      "Iteration 13335, loss = 2.24736320\n",
      "Iteration 13336, loss = 3.23523244\n",
      "Iteration 13337, loss = 5.47870665\n",
      "Iteration 13338, loss = 4.41725749\n",
      "Iteration 13339, loss = 2.66268347\n",
      "Iteration 13340, loss = 2.27024047\n",
      "Iteration 13341, loss = 2.27659859\n",
      "Iteration 13342, loss = 2.52817343\n",
      "Iteration 13343, loss = 2.50736034\n",
      "Iteration 13344, loss = 2.47477135\n",
      "Iteration 13345, loss = 2.63849494\n",
      "Iteration 13346, loss = 2.35004540\n",
      "Iteration 13347, loss = 2.31173945\n",
      "Iteration 13348, loss = 2.48819134\n",
      "Iteration 13349, loss = 2.26643126\n",
      "Iteration 13350, loss = 2.58627118\n",
      "Iteration 13351, loss = 2.42671702\n",
      "Iteration 13352, loss = 2.56860277\n",
      "Iteration 13353, loss = 2.59566037\n",
      "Iteration 13354, loss = 3.94769276\n",
      "Iteration 13355, loss = 7.70426817\n",
      "Iteration 13356, loss = 11.07179192\n",
      "Iteration 13357, loss = 7.22837732\n",
      "Iteration 13358, loss = 6.66317728\n",
      "Iteration 13359, loss = 6.03690689\n",
      "Iteration 13360, loss = 4.93087720\n",
      "Iteration 13361, loss = 4.22385779\n",
      "Iteration 13362, loss = 3.79264263\n",
      "Iteration 13363, loss = 3.29331426\n",
      "Iteration 13364, loss = 3.34861094\n",
      "Iteration 13365, loss = 3.37994460\n",
      "Iteration 13366, loss = 3.04192180\n",
      "Iteration 13367, loss = 3.05679361\n",
      "Iteration 13368, loss = 3.63376596\n",
      "Iteration 13369, loss = 4.87271763\n",
      "Iteration 13370, loss = 6.50477025\n",
      "Iteration 13371, loss = 3.25840828\n",
      "Iteration 13372, loss = 2.37483415\n",
      "Iteration 13373, loss = 3.52066499\n",
      "Iteration 13374, loss = 2.69912912\n",
      "Iteration 13375, loss = 2.57619115\n",
      "Iteration 13376, loss = 2.39576964\n",
      "Iteration 13377, loss = 2.42376629\n",
      "Iteration 13378, loss = 2.44106435\n",
      "Iteration 13379, loss = 2.36252733\n",
      "Iteration 13380, loss = 2.33487804\n",
      "Iteration 13381, loss = 2.25879636\n",
      "Iteration 13382, loss = 2.31019327\n",
      "Iteration 13383, loss = 2.79225340\n",
      "Iteration 13384, loss = 2.23950854\n",
      "Iteration 13385, loss = 2.32994198\n",
      "Iteration 13386, loss = 2.62182071\n",
      "Iteration 13387, loss = 2.20789691\n",
      "Iteration 13388, loss = 4.32384586\n",
      "Iteration 13389, loss = 2.95692295\n",
      "Iteration 13390, loss = 2.62143621\n",
      "Iteration 13391, loss = 2.78008662\n",
      "Iteration 13392, loss = 2.96558106\n",
      "Iteration 13393, loss = 2.67745071\n",
      "Iteration 13394, loss = 2.28620979\n",
      "Iteration 13395, loss = 2.36839243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13396, loss = 2.18729013\n",
      "Iteration 13397, loss = 2.20063368\n",
      "Iteration 13398, loss = 2.28266071\n",
      "Iteration 13399, loss = 2.37870775\n",
      "Iteration 13400, loss = 2.35203247\n",
      "Iteration 13401, loss = 2.29986382\n",
      "Iteration 13402, loss = 2.31264257\n",
      "Iteration 13403, loss = 2.16097380\n",
      "Iteration 13404, loss = 2.19007273\n",
      "Iteration 13405, loss = 2.30127888\n",
      "Iteration 13406, loss = 2.86283747\n",
      "Iteration 13407, loss = 2.15202254\n",
      "Iteration 13408, loss = 2.31981414\n",
      "Iteration 13409, loss = 3.13483617\n",
      "Iteration 13410, loss = 2.66742900\n",
      "Iteration 13411, loss = 2.78723690\n",
      "Iteration 13412, loss = 2.38892573\n",
      "Iteration 13413, loss = 3.03659747\n",
      "Iteration 13414, loss = 2.76995905\n",
      "Iteration 13415, loss = 3.76020238\n",
      "Iteration 13416, loss = 7.77538641\n",
      "Iteration 13417, loss = 10.75357040\n",
      "Iteration 13418, loss = 4.36324931\n",
      "Iteration 13419, loss = 4.16579103\n",
      "Iteration 13420, loss = 4.60722791\n",
      "Iteration 13421, loss = 3.52427955\n",
      "Iteration 13422, loss = 3.59367599\n",
      "Iteration 13423, loss = 3.45432176\n",
      "Iteration 13424, loss = 2.43986321\n",
      "Iteration 13425, loss = 2.77282650\n",
      "Iteration 13426, loss = 3.09568698\n",
      "Iteration 13427, loss = 6.92635481\n",
      "Iteration 13428, loss = 4.52273604\n",
      "Iteration 13429, loss = 2.91532067\n",
      "Iteration 13430, loss = 3.79277311\n",
      "Iteration 13431, loss = 4.26495770\n",
      "Iteration 13432, loss = 3.44609073\n",
      "Iteration 13433, loss = 2.67304175\n",
      "Iteration 13434, loss = 2.63908740\n",
      "Iteration 13435, loss = 2.69677370\n",
      "Iteration 13436, loss = 2.36899908\n",
      "Iteration 13437, loss = 2.83698434\n",
      "Iteration 13438, loss = 2.53556720\n",
      "Iteration 13439, loss = 2.27694172\n",
      "Iteration 13440, loss = 2.27305357\n",
      "Iteration 13441, loss = 2.60697695\n",
      "Iteration 13442, loss = 2.85081002\n",
      "Iteration 13443, loss = 2.87182091\n",
      "Iteration 13444, loss = 4.05870008\n",
      "Iteration 13445, loss = 4.41527578\n",
      "Iteration 13446, loss = 4.23135186\n",
      "Iteration 13447, loss = 4.78501347\n",
      "Iteration 13448, loss = 3.27936596\n",
      "Iteration 13449, loss = 2.95318412\n",
      "Iteration 13450, loss = 2.47021956\n",
      "Iteration 13451, loss = 2.59425027\n",
      "Iteration 13452, loss = 3.28133159\n",
      "Iteration 13453, loss = 3.51001247\n",
      "Iteration 13454, loss = 3.59195059\n",
      "Iteration 13455, loss = 2.74753515\n",
      "Iteration 13456, loss = 2.65328879\n",
      "Iteration 13457, loss = 2.75131757\n",
      "Iteration 13458, loss = 4.20605200\n",
      "Iteration 13459, loss = 4.05931217\n",
      "Iteration 13460, loss = 3.43298373\n",
      "Iteration 13461, loss = 3.21407834\n",
      "Iteration 13462, loss = 2.67438053\n",
      "Iteration 13463, loss = 2.62579176\n",
      "Iteration 13464, loss = 2.60729060\n",
      "Iteration 13465, loss = 2.20737331\n",
      "Iteration 13466, loss = 2.37669355\n",
      "Iteration 13467, loss = 2.46715559\n",
      "Iteration 13468, loss = 2.47867984\n",
      "Iteration 13469, loss = 2.45539380\n",
      "Iteration 13470, loss = 2.10663584\n",
      "Iteration 13471, loss = 2.44758379\n",
      "Iteration 13472, loss = 2.52969972\n",
      "Iteration 13473, loss = 2.82886743\n",
      "Iteration 13474, loss = 2.83040875\n",
      "Iteration 13475, loss = 4.17714436\n",
      "Iteration 13476, loss = 2.76052998\n",
      "Iteration 13477, loss = 2.50614071\n",
      "Iteration 13478, loss = 2.50931999\n",
      "Iteration 13479, loss = 2.30168752\n",
      "Iteration 13480, loss = 2.44540789\n",
      "Iteration 13481, loss = 2.28030306\n",
      "Iteration 13482, loss = 2.36237056\n",
      "Iteration 13483, loss = 2.24657345\n",
      "Iteration 13484, loss = 2.29870701\n",
      "Iteration 13485, loss = 2.19872179\n",
      "Iteration 13486, loss = 2.24394086\n",
      "Iteration 13487, loss = 2.54004341\n",
      "Iteration 13488, loss = 3.35157958\n",
      "Iteration 13489, loss = 2.72539975\n",
      "Iteration 13490, loss = 3.17064122\n",
      "Iteration 13491, loss = 2.98645164\n",
      "Iteration 13492, loss = 2.53859296\n",
      "Iteration 13493, loss = 3.31510784\n",
      "Iteration 13494, loss = 4.26413114\n",
      "Iteration 13495, loss = 3.29152879\n",
      "Iteration 13496, loss = 2.84714276\n",
      "Iteration 13497, loss = 2.67767557\n",
      "Iteration 13498, loss = 3.46590955\n",
      "Iteration 13499, loss = 3.75401099\n",
      "Iteration 13500, loss = 3.06949080\n",
      "Iteration 13501, loss = 2.21363155\n",
      "Iteration 13502, loss = 2.34227452\n",
      "Iteration 13503, loss = 2.17218197\n",
      "Iteration 13504, loss = 2.25298045\n",
      "Iteration 13505, loss = 2.26953011\n",
      "Iteration 13506, loss = 2.21545495\n",
      "Iteration 13507, loss = 2.29675520\n",
      "Iteration 13508, loss = 2.28144482\n",
      "Iteration 13509, loss = 2.46971390\n",
      "Iteration 13510, loss = 2.20022828\n",
      "Iteration 13511, loss = 2.23736008\n",
      "Iteration 13512, loss = 2.14008493\n",
      "Iteration 13513, loss = 2.04949991\n",
      "Iteration 13514, loss = 2.12210072\n",
      "Iteration 13515, loss = 2.42866118\n",
      "Iteration 13516, loss = 2.42321399\n",
      "Iteration 13517, loss = 3.81767818\n",
      "Iteration 13518, loss = 4.51245355\n",
      "Iteration 13519, loss = 3.84159366\n",
      "Iteration 13520, loss = 2.88965360\n",
      "Iteration 13521, loss = 2.44161694\n",
      "Iteration 13522, loss = 3.39719080\n",
      "Iteration 13523, loss = 2.31905187\n",
      "Iteration 13524, loss = 2.99536715\n",
      "Iteration 13525, loss = 2.74423573\n",
      "Iteration 13526, loss = 3.94820821\n",
      "Iteration 13527, loss = 3.81863926\n",
      "Iteration 13528, loss = 4.96448180\n",
      "Iteration 13529, loss = 7.19403663\n",
      "Iteration 13530, loss = 7.60956740\n",
      "Iteration 13531, loss = 4.20938923\n",
      "Iteration 13532, loss = 5.11910315\n",
      "Iteration 13533, loss = 4.03035113\n",
      "Iteration 13534, loss = 3.81205201\n",
      "Iteration 13535, loss = 2.83455268\n",
      "Iteration 13536, loss = 2.88962345\n",
      "Iteration 13537, loss = 2.66452630\n",
      "Iteration 13538, loss = 2.15925648\n",
      "Iteration 13539, loss = 2.42842505\n",
      "Iteration 13540, loss = 2.58327613\n",
      "Iteration 13541, loss = 2.75837993\n",
      "Iteration 13542, loss = 2.47490169\n",
      "Iteration 13543, loss = 3.14515316\n",
      "Iteration 13544, loss = 3.41558624\n",
      "Iteration 13545, loss = 2.22124000\n",
      "Iteration 13546, loss = 2.70099500\n",
      "Iteration 13547, loss = 2.78618186\n",
      "Iteration 13548, loss = 2.18673908\n",
      "Iteration 13549, loss = 2.92336743\n",
      "Iteration 13550, loss = 2.61783832\n",
      "Iteration 13551, loss = 2.55059348\n",
      "Iteration 13552, loss = 2.87968024\n",
      "Iteration 13553, loss = 2.64353128\n",
      "Iteration 13554, loss = 2.60943316\n",
      "Iteration 13555, loss = 2.51752814\n",
      "Iteration 13556, loss = 2.69338316\n",
      "Iteration 13557, loss = 2.52821566\n",
      "Iteration 13558, loss = 3.30413642\n",
      "Iteration 13559, loss = 3.25424493\n",
      "Iteration 13560, loss = 2.52517825\n",
      "Iteration 13561, loss = 2.24186001\n",
      "Iteration 13562, loss = 2.45833390\n",
      "Iteration 13563, loss = 2.06991032\n",
      "Iteration 13564, loss = 2.27939356\n",
      "Iteration 13565, loss = 2.13122697\n",
      "Iteration 13566, loss = 2.31148560\n",
      "Iteration 13567, loss = 2.35030732\n",
      "Iteration 13568, loss = 2.56409344\n",
      "Iteration 13569, loss = 2.69743943\n",
      "Iteration 13570, loss = 3.80623597\n",
      "Iteration 13571, loss = 2.77462698\n",
      "Iteration 13572, loss = 2.23279608\n",
      "Iteration 13573, loss = 2.71554353\n",
      "Iteration 13574, loss = 2.47321171\n",
      "Iteration 13575, loss = 2.56826576\n",
      "Iteration 13576, loss = 2.48233362\n",
      "Iteration 13577, loss = 3.34047007\n",
      "Iteration 13578, loss = 3.02711154\n",
      "Iteration 13579, loss = 2.79229523\n",
      "Iteration 13580, loss = 2.54570251\n",
      "Iteration 13581, loss = 2.30015275\n",
      "Iteration 13582, loss = 2.19774069\n",
      "Iteration 13583, loss = 2.26669075\n",
      "Iteration 13584, loss = 2.57733674\n",
      "Iteration 13585, loss = 2.34792588\n",
      "Iteration 13586, loss = 2.55344090\n",
      "Iteration 13587, loss = 2.56860717\n",
      "Iteration 13588, loss = 2.76413648\n",
      "Iteration 13589, loss = 2.26265493\n",
      "Iteration 13590, loss = 2.26903885\n",
      "Iteration 13591, loss = 2.17154408\n",
      "Iteration 13592, loss = 2.17884484\n",
      "Iteration 13593, loss = 2.24166436\n",
      "Iteration 13594, loss = 2.34011106\n",
      "Iteration 13595, loss = 2.12807946\n",
      "Iteration 13596, loss = 2.03645130\n",
      "Iteration 13597, loss = 2.12641928\n",
      "Iteration 13598, loss = 2.39023447\n",
      "Iteration 13599, loss = 2.12063601\n",
      "Iteration 13600, loss = 2.67595247\n",
      "Iteration 13601, loss = 2.54268578\n",
      "Iteration 13602, loss = 2.26366803\n",
      "Iteration 13603, loss = 2.39509187\n",
      "Iteration 13604, loss = 2.44582299\n",
      "Iteration 13605, loss = 2.31140466\n",
      "Iteration 13606, loss = 4.27072188\n",
      "Iteration 13607, loss = 3.84679632\n",
      "Iteration 13608, loss = 2.75098885\n",
      "Iteration 13609, loss = 2.47090491\n",
      "Iteration 13610, loss = 3.25564662\n",
      "Iteration 13611, loss = 4.40878436\n",
      "Iteration 13612, loss = 6.76086294\n",
      "Iteration 13613, loss = 4.63254666\n",
      "Iteration 13614, loss = 5.63752667\n",
      "Iteration 13615, loss = 4.00834056\n",
      "Iteration 13616, loss = 2.94482117\n",
      "Iteration 13617, loss = 2.57314562\n",
      "Iteration 13618, loss = 3.66020297\n",
      "Iteration 13619, loss = 3.56810989\n",
      "Iteration 13620, loss = 4.20880118\n",
      "Iteration 13621, loss = 4.11416776\n",
      "Iteration 13622, loss = 2.52085469\n",
      "Iteration 13623, loss = 2.52277991\n",
      "Iteration 13624, loss = 2.50593745\n",
      "Iteration 13625, loss = 5.37076904\n",
      "Iteration 13626, loss = 5.28943578\n",
      "Iteration 13627, loss = 4.44923848\n",
      "Iteration 13628, loss = 3.24747052\n",
      "Iteration 13629, loss = 3.85928308\n",
      "Iteration 13630, loss = 2.41869671\n",
      "Iteration 13631, loss = 2.60742688\n",
      "Iteration 13632, loss = 2.21829908\n",
      "Iteration 13633, loss = 2.63952232\n",
      "Iteration 13634, loss = 2.45999222\n",
      "Iteration 13635, loss = 3.38365016\n",
      "Iteration 13636, loss = 2.77775021\n",
      "Iteration 13637, loss = 2.37761971\n",
      "Iteration 13638, loss = 2.11278848\n",
      "Iteration 13639, loss = 2.50729761\n",
      "Iteration 13640, loss = 2.07642592\n",
      "Iteration 13641, loss = 2.54077127\n",
      "Iteration 13642, loss = 2.75920425\n",
      "Iteration 13643, loss = 3.12902952\n",
      "Iteration 13644, loss = 2.24304624\n",
      "Iteration 13645, loss = 2.11198542\n",
      "Iteration 13646, loss = 2.46926257\n",
      "Iteration 13647, loss = 2.27477088\n",
      "Iteration 13648, loss = 2.22566365\n",
      "Iteration 13649, loss = 2.04526046\n",
      "Iteration 13650, loss = 2.34929725\n",
      "Iteration 13651, loss = 2.02522274\n",
      "Iteration 13652, loss = 2.30347742\n",
      "Iteration 13653, loss = 2.18336187\n",
      "Iteration 13654, loss = 2.15758759\n",
      "Iteration 13655, loss = 2.55819975\n",
      "Iteration 13656, loss = 2.26236351\n",
      "Iteration 13657, loss = 3.52500666\n",
      "Iteration 13658, loss = 2.65316073\n",
      "Iteration 13659, loss = 2.88414834\n",
      "Iteration 13660, loss = 2.45265706\n",
      "Iteration 13661, loss = 3.54649584\n",
      "Iteration 13662, loss = 2.29154951\n",
      "Iteration 13663, loss = 2.42897499\n",
      "Iteration 13664, loss = 2.12978289\n",
      "Iteration 13665, loss = 2.23119084\n",
      "Iteration 13666, loss = 2.15265706\n",
      "Iteration 13667, loss = 2.10527785\n",
      "Iteration 13668, loss = 2.39556902\n",
      "Iteration 13669, loss = 2.56944498\n",
      "Iteration 13670, loss = 2.73408336\n",
      "Iteration 13671, loss = 2.03778212\n",
      "Iteration 13672, loss = 2.63123312\n",
      "Iteration 13673, loss = 2.88080774\n",
      "Iteration 13674, loss = 2.52268135\n",
      "Iteration 13675, loss = 2.78324799\n",
      "Iteration 13676, loss = 2.67778370\n",
      "Iteration 13677, loss = 2.04588333\n",
      "Iteration 13678, loss = 2.02117196\n",
      "Iteration 13679, loss = 1.99152505\n",
      "Iteration 13680, loss = 2.42134440\n",
      "Iteration 13681, loss = 2.60250454\n",
      "Iteration 13682, loss = 2.64682770\n",
      "Iteration 13683, loss = 2.20883643\n",
      "Iteration 13684, loss = 2.43179371\n",
      "Iteration 13685, loss = 2.61090142\n",
      "Iteration 13686, loss = 2.53798180\n",
      "Iteration 13687, loss = 2.75797558\n",
      "Iteration 13688, loss = 4.97748423\n",
      "Iteration 13689, loss = 12.52380783\n",
      "Iteration 13690, loss = 17.99312639\n",
      "Iteration 13691, loss = 12.63210535\n",
      "Iteration 13692, loss = 10.63288257\n",
      "Iteration 13693, loss = 9.30519849\n",
      "Iteration 13694, loss = 7.95509771\n",
      "Iteration 13695, loss = 4.46572312\n",
      "Iteration 13696, loss = 5.18945646\n",
      "Iteration 13697, loss = 4.05132711\n",
      "Iteration 13698, loss = 4.05416520\n",
      "Iteration 13699, loss = 3.96997973\n",
      "Iteration 13700, loss = 3.03317417\n",
      "Iteration 13701, loss = 3.23350429\n",
      "Iteration 13702, loss = 3.64826406\n",
      "Iteration 13703, loss = 5.04198724\n",
      "Iteration 13704, loss = 3.32583052\n",
      "Iteration 13705, loss = 2.35433819\n",
      "Iteration 13706, loss = 2.39867506\n",
      "Iteration 13707, loss = 2.97088189\n",
      "Iteration 13708, loss = 2.59257419\n",
      "Iteration 13709, loss = 2.48492205\n",
      "Iteration 13710, loss = 2.22877610\n",
      "Iteration 13711, loss = 2.48618157\n",
      "Iteration 13712, loss = 2.46737947\n",
      "Iteration 13713, loss = 2.45941408\n",
      "Iteration 13714, loss = 2.76671325\n",
      "Iteration 13715, loss = 3.19559480\n",
      "Iteration 13716, loss = 2.78359657\n",
      "Iteration 13717, loss = 2.38439401\n",
      "Iteration 13718, loss = 2.66927982\n",
      "Iteration 13719, loss = 3.10755014\n",
      "Iteration 13720, loss = 2.70666321\n",
      "Iteration 13721, loss = 2.60240771\n",
      "Iteration 13722, loss = 2.60393954\n",
      "Iteration 13723, loss = 2.23191945\n",
      "Iteration 13724, loss = 2.40712858\n",
      "Iteration 13725, loss = 2.12142233\n",
      "Iteration 13726, loss = 2.45916267\n",
      "Iteration 13727, loss = 2.13512950\n",
      "Iteration 13728, loss = 2.16686936\n",
      "Iteration 13729, loss = 2.17377440\n",
      "Iteration 13730, loss = 2.13913478\n",
      "Iteration 13731, loss = 2.32861100\n",
      "Iteration 13732, loss = 2.99833739\n",
      "Iteration 13733, loss = 3.24952853\n",
      "Iteration 13734, loss = 4.27863381\n",
      "Iteration 13735, loss = 4.59036739\n",
      "Iteration 13736, loss = 3.70966642\n",
      "Iteration 13737, loss = 7.37477395\n",
      "Iteration 13738, loss = 7.73612894\n",
      "Iteration 13739, loss = 5.67361607\n",
      "Iteration 13740, loss = 3.67409290\n",
      "Iteration 13741, loss = 3.05440183\n",
      "Iteration 13742, loss = 3.54305019\n",
      "Iteration 13743, loss = 6.41042630\n",
      "Iteration 13744, loss = 3.74008891\n",
      "Iteration 13745, loss = 2.91878218\n",
      "Iteration 13746, loss = 2.56719189\n",
      "Iteration 13747, loss = 2.87704835\n",
      "Iteration 13748, loss = 2.42280589\n",
      "Iteration 13749, loss = 2.46953790\n",
      "Iteration 13750, loss = 2.71352033\n",
      "Iteration 13751, loss = 2.24361506\n",
      "Iteration 13752, loss = 2.87936235\n",
      "Iteration 13753, loss = 2.52520756\n",
      "Iteration 13754, loss = 2.87815390\n",
      "Iteration 13755, loss = 2.81215023\n",
      "Iteration 13756, loss = 2.48009114\n",
      "Iteration 13757, loss = 2.50391067\n",
      "Iteration 13758, loss = 2.16370816\n",
      "Iteration 13759, loss = 2.43626294\n",
      "Iteration 13760, loss = 2.42986547\n",
      "Iteration 13761, loss = 2.47074801\n",
      "Iteration 13762, loss = 2.39063430\n",
      "Iteration 13763, loss = 2.40434304\n",
      "Iteration 13764, loss = 2.42957744\n",
      "Iteration 13765, loss = 2.07417059\n",
      "Iteration 13766, loss = 2.43943905\n",
      "Iteration 13767, loss = 2.65699413\n",
      "Iteration 13768, loss = 2.29735308\n",
      "Iteration 13769, loss = 2.48745889\n",
      "Iteration 13770, loss = 2.12496027\n",
      "Iteration 13771, loss = 2.03585539\n",
      "Iteration 13772, loss = 2.00750770\n",
      "Iteration 13773, loss = 2.06424039\n",
      "Iteration 13774, loss = 2.05673365\n",
      "Iteration 13775, loss = 2.03552836\n",
      "Iteration 13776, loss = 2.14263162\n",
      "Iteration 13777, loss = 2.24836246\n",
      "Iteration 13778, loss = 2.10558295\n",
      "Iteration 13779, loss = 2.26709108\n",
      "Iteration 13780, loss = 2.09457772\n",
      "Iteration 13781, loss = 2.84160308\n",
      "Iteration 13782, loss = 2.86057872\n",
      "Iteration 13783, loss = 2.12581800\n",
      "Iteration 13784, loss = 2.22549425\n",
      "Iteration 13785, loss = 2.01252475\n",
      "Iteration 13786, loss = 2.19186628\n",
      "Iteration 13787, loss = 2.18894485\n",
      "Iteration 13788, loss = 2.59194485\n",
      "Iteration 13789, loss = 2.64216161\n",
      "Iteration 13790, loss = 2.18847073\n",
      "Iteration 13791, loss = 2.16213998\n",
      "Iteration 13792, loss = 2.29381583\n",
      "Iteration 13793, loss = 2.13597341\n",
      "Iteration 13794, loss = 2.14057555\n",
      "Iteration 13795, loss = 2.18914842\n",
      "Iteration 13796, loss = 2.25855422\n",
      "Iteration 13797, loss = 2.13594255\n",
      "Iteration 13798, loss = 2.38455924\n",
      "Iteration 13799, loss = 2.38847371\n",
      "Iteration 13800, loss = 2.93193811\n",
      "Iteration 13801, loss = 2.86737113\n",
      "Iteration 13802, loss = 2.86179678\n",
      "Iteration 13803, loss = 3.82128943\n",
      "Iteration 13804, loss = 4.35072391\n",
      "Iteration 13805, loss = 3.00315489\n",
      "Iteration 13806, loss = 2.60814965\n",
      "Iteration 13807, loss = 2.95427670\n",
      "Iteration 13808, loss = 3.53744834\n",
      "Iteration 13809, loss = 2.19306122\n",
      "Iteration 13810, loss = 2.70208923\n",
      "Iteration 13811, loss = 1.91045582\n",
      "Iteration 13812, loss = 3.12262832\n",
      "Iteration 13813, loss = 2.85125633\n",
      "Iteration 13814, loss = 2.79360133\n",
      "Iteration 13815, loss = 1.99885622\n",
      "Iteration 13816, loss = 2.49347628\n",
      "Iteration 13817, loss = 2.11130477\n",
      "Iteration 13818, loss = 2.28444573\n",
      "Iteration 13819, loss = 2.20238750\n",
      "Iteration 13820, loss = 2.11405348\n",
      "Iteration 13821, loss = 2.40672834\n",
      "Iteration 13822, loss = 2.24532233\n",
      "Iteration 13823, loss = 2.63275633\n",
      "Iteration 13824, loss = 2.74468339\n",
      "Iteration 13825, loss = 2.79474797\n",
      "Iteration 13826, loss = 2.13776817\n",
      "Iteration 13827, loss = 2.27714769\n",
      "Iteration 13828, loss = 2.22786052\n",
      "Iteration 13829, loss = 3.00988239\n",
      "Iteration 13830, loss = 2.76124392\n",
      "Iteration 13831, loss = 3.47396986\n",
      "Iteration 13832, loss = 3.61055277\n",
      "Iteration 13833, loss = 3.26880341\n",
      "Iteration 13834, loss = 2.51785695\n",
      "Iteration 13835, loss = 3.44793264\n",
      "Iteration 13836, loss = 3.17794097\n",
      "Iteration 13837, loss = 2.77271787\n",
      "Iteration 13838, loss = 2.14074281\n",
      "Iteration 13839, loss = 2.27827321\n",
      "Iteration 13840, loss = 2.15961667\n",
      "Iteration 13841, loss = 2.71680853\n",
      "Iteration 13842, loss = 3.09610592\n",
      "Iteration 13843, loss = 2.29825571\n",
      "Iteration 13844, loss = 2.46302379\n",
      "Iteration 13845, loss = 2.10440278\n",
      "Iteration 13846, loss = 1.99730040\n",
      "Iteration 13847, loss = 2.11056296\n",
      "Iteration 13848, loss = 2.69009577\n",
      "Iteration 13849, loss = 2.20440820\n",
      "Iteration 13850, loss = 3.40633116\n",
      "Iteration 13851, loss = 2.85060176\n",
      "Iteration 13852, loss = 2.72844884\n",
      "Iteration 13853, loss = 4.48197011\n",
      "Iteration 13854, loss = 3.03459407\n",
      "Iteration 13855, loss = 3.14094549\n",
      "Iteration 13856, loss = 2.74796807\n",
      "Iteration 13857, loss = 3.75672317\n",
      "Iteration 13858, loss = 2.50063972\n",
      "Iteration 13859, loss = 2.53954335\n",
      "Iteration 13860, loss = 2.29800613\n",
      "Iteration 13861, loss = 2.20342266\n",
      "Iteration 13862, loss = 2.18427582\n",
      "Iteration 13863, loss = 2.23890120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13864, loss = 2.26997438\n",
      "Iteration 13865, loss = 2.34493098\n",
      "Iteration 13866, loss = 2.30824664\n",
      "Iteration 13867, loss = 2.24844753\n",
      "Iteration 13868, loss = 2.15577540\n",
      "Iteration 13869, loss = 2.00995208\n",
      "Iteration 13870, loss = 1.99628276\n",
      "Iteration 13871, loss = 2.42517159\n",
      "Iteration 13872, loss = 3.14598431\n",
      "Iteration 13873, loss = 4.43307689\n",
      "Iteration 13874, loss = 2.14865406\n",
      "Iteration 13875, loss = 2.47116428\n",
      "Iteration 13876, loss = 2.43910633\n",
      "Iteration 13877, loss = 2.22284877\n",
      "Iteration 13878, loss = 2.21407557\n",
      "Iteration 13879, loss = 2.11038330\n",
      "Iteration 13880, loss = 1.96898713\n",
      "Iteration 13881, loss = 2.32983439\n",
      "Iteration 13882, loss = 2.52416840\n",
      "Iteration 13883, loss = 2.77510141\n",
      "Iteration 13884, loss = 2.10151887\n",
      "Iteration 13885, loss = 2.15202519\n",
      "Iteration 13886, loss = 2.14829869\n",
      "Iteration 13887, loss = 2.60697784\n",
      "Iteration 13888, loss = 2.54768116\n",
      "Iteration 13889, loss = 2.49797522\n",
      "Iteration 13890, loss = 2.28767170\n",
      "Iteration 13891, loss = 2.41142865\n",
      "Iteration 13892, loss = 2.05970136\n",
      "Iteration 13893, loss = 2.10318667\n",
      "Iteration 13894, loss = 2.17827236\n",
      "Iteration 13895, loss = 2.50362558\n",
      "Iteration 13896, loss = 2.41776115\n",
      "Iteration 13897, loss = 2.59201809\n",
      "Iteration 13898, loss = 2.20857918\n",
      "Iteration 13899, loss = 2.14775125\n",
      "Iteration 13900, loss = 2.07101837\n",
      "Iteration 13901, loss = 2.16052802\n",
      "Iteration 13902, loss = 2.00701108\n",
      "Iteration 13903, loss = 2.35597680\n",
      "Iteration 13904, loss = 2.28521133\n",
      "Iteration 13905, loss = 2.05678472\n",
      "Iteration 13906, loss = 2.04276557\n",
      "Iteration 13907, loss = 2.11325585\n",
      "Iteration 13908, loss = 2.16270793\n",
      "Iteration 13909, loss = 1.94732421\n",
      "Iteration 13910, loss = 2.03511622\n",
      "Iteration 13911, loss = 2.04702262\n",
      "Iteration 13912, loss = 2.12945195\n",
      "Iteration 13913, loss = 2.12942763\n",
      "Iteration 13914, loss = 2.02033750\n",
      "Iteration 13915, loss = 2.01409544\n",
      "Iteration 13916, loss = 2.09262665\n",
      "Iteration 13917, loss = 1.90627034\n",
      "Iteration 13918, loss = 2.28219192\n",
      "Iteration 13919, loss = 1.94280098\n",
      "Iteration 13920, loss = 2.71641773\n",
      "Iteration 13921, loss = 4.10441756\n",
      "Iteration 13922, loss = 7.99858232\n",
      "Iteration 13923, loss = 4.18113229\n",
      "Iteration 13924, loss = 3.60506829\n",
      "Iteration 13925, loss = 2.78135014\n",
      "Iteration 13926, loss = 3.55948611\n",
      "Iteration 13927, loss = 3.60996990\n",
      "Iteration 13928, loss = 2.97213344\n",
      "Iteration 13929, loss = 3.50870388\n",
      "Iteration 13930, loss = 3.24069126\n",
      "Iteration 13931, loss = 3.45906945\n",
      "Iteration 13932, loss = 5.75385805\n",
      "Iteration 13933, loss = 4.45776712\n",
      "Iteration 13934, loss = 3.28916442\n",
      "Iteration 13935, loss = 3.70332581\n",
      "Iteration 13936, loss = 3.85748630\n",
      "Iteration 13937, loss = 3.71256402\n",
      "Iteration 13938, loss = 2.26595412\n",
      "Iteration 13939, loss = 2.75301313\n",
      "Iteration 13940, loss = 2.47801752\n",
      "Iteration 13941, loss = 2.21645581\n",
      "Iteration 13942, loss = 2.04077771\n",
      "Iteration 13943, loss = 3.68353068\n",
      "Iteration 13944, loss = 3.89833218\n",
      "Iteration 13945, loss = 4.43643792\n",
      "Iteration 13946, loss = 6.06910072\n",
      "Iteration 13947, loss = 5.23576468\n",
      "Iteration 13948, loss = 3.62122584\n",
      "Iteration 13949, loss = 2.95740901\n",
      "Iteration 13950, loss = 3.44286082\n",
      "Iteration 13951, loss = 3.17929390\n",
      "Iteration 13952, loss = 3.53059738\n",
      "Iteration 13953, loss = 2.89857769\n",
      "Iteration 13954, loss = 3.98520712\n",
      "Iteration 13955, loss = 3.94749952\n",
      "Iteration 13956, loss = 3.74918826\n",
      "Iteration 13957, loss = 6.39146243\n",
      "Iteration 13958, loss = 5.20334730\n",
      "Iteration 13959, loss = 5.45540565\n",
      "Iteration 13960, loss = 5.44408974\n",
      "Iteration 13961, loss = 3.21869863\n",
      "Iteration 13962, loss = 2.47694128\n",
      "Iteration 13963, loss = 2.23158365\n",
      "Iteration 13964, loss = 2.56138462\n",
      "Iteration 13965, loss = 3.63505308\n",
      "Iteration 13966, loss = 5.68807309\n",
      "Iteration 13967, loss = 3.72590951\n",
      "Iteration 13968, loss = 3.43805849\n",
      "Iteration 13969, loss = 2.54674454\n",
      "Iteration 13970, loss = 3.96779843\n",
      "Iteration 13971, loss = 7.03642011\n",
      "Iteration 13972, loss = 12.33500721\n",
      "Iteration 13973, loss = 10.72365500\n",
      "Iteration 13974, loss = 8.47230104\n",
      "Iteration 13975, loss = 6.02074471\n",
      "Iteration 13976, loss = 4.27406292\n",
      "Iteration 13977, loss = 3.15705732\n",
      "Iteration 13978, loss = 2.87972964\n",
      "Iteration 13979, loss = 2.86366530\n",
      "Iteration 13980, loss = 2.57867762\n",
      "Iteration 13981, loss = 3.11067921\n",
      "Iteration 13982, loss = 2.67752422\n",
      "Iteration 13983, loss = 2.68176958\n",
      "Iteration 13984, loss = 4.08205924\n",
      "Iteration 13985, loss = 3.35873960\n",
      "Iteration 13986, loss = 2.65636908\n",
      "Iteration 13987, loss = 2.75259993\n",
      "Iteration 13988, loss = 2.60128735\n",
      "Iteration 13989, loss = 3.28367007\n",
      "Iteration 13990, loss = 6.83900684\n",
      "Iteration 13991, loss = 7.45241041\n",
      "Iteration 13992, loss = 4.32769406\n",
      "Iteration 13993, loss = 3.07220602\n",
      "Iteration 13994, loss = 3.23533138\n",
      "Iteration 13995, loss = 3.01754083\n",
      "Iteration 13996, loss = 2.83597926\n",
      "Iteration 13997, loss = 2.05132276\n",
      "Iteration 13998, loss = 2.07902309\n",
      "Iteration 13999, loss = 2.05115259\n",
      "Iteration 14000, loss = 1.98515889\n",
      "Iteration 14001, loss = 1.92341851\n",
      "Iteration 14002, loss = 2.04360531\n",
      "Iteration 14003, loss = 2.06114860\n",
      "Iteration 14004, loss = 2.31741475\n",
      "Iteration 14005, loss = 3.09813127\n",
      "Iteration 14006, loss = 7.67421676\n",
      "Iteration 14007, loss = 6.29873127\n",
      "Iteration 14008, loss = 3.64798200\n",
      "Iteration 14009, loss = 3.82071618\n",
      "Iteration 14010, loss = 2.72527819\n",
      "Iteration 14011, loss = 4.09638276\n",
      "Iteration 14012, loss = 3.21413662\n",
      "Iteration 14013, loss = 3.89352180\n",
      "Iteration 14014, loss = 3.78770978\n",
      "Iteration 14015, loss = 2.24799555\n",
      "Iteration 14016, loss = 2.95699233\n",
      "Iteration 14017, loss = 2.19742107\n",
      "Iteration 14018, loss = 2.43101242\n",
      "Iteration 14019, loss = 2.51725767\n",
      "Iteration 14020, loss = 2.52727582\n",
      "Iteration 14021, loss = 2.69295618\n",
      "Iteration 14022, loss = 2.93449271\n",
      "Iteration 14023, loss = 4.30851864\n",
      "Iteration 14024, loss = 3.34921835\n",
      "Iteration 14025, loss = 2.53227673\n",
      "Iteration 14026, loss = 2.01460095\n",
      "Iteration 14027, loss = 2.27743135\n",
      "Iteration 14028, loss = 2.27095735\n",
      "Iteration 14029, loss = 2.57463486\n",
      "Iteration 14030, loss = 2.39653075\n",
      "Iteration 14031, loss = 2.43462724\n",
      "Iteration 14032, loss = 2.68414514\n",
      "Iteration 14033, loss = 2.01180539\n",
      "Iteration 14034, loss = 2.06557584\n",
      "Iteration 14035, loss = 2.25099572\n",
      "Iteration 14036, loss = 2.28362052\n",
      "Iteration 14037, loss = 1.99082337\n",
      "Iteration 14038, loss = 2.60741623\n",
      "Iteration 14039, loss = 2.33072788\n",
      "Iteration 14040, loss = 2.08628993\n",
      "Iteration 14041, loss = 2.12426609\n",
      "Iteration 14042, loss = 2.25723826\n",
      "Iteration 14043, loss = 1.87466155\n",
      "Iteration 14044, loss = 2.07411782\n",
      "Iteration 14045, loss = 2.34108202\n",
      "Iteration 14046, loss = 2.00141362\n",
      "Iteration 14047, loss = 2.68304596\n",
      "Iteration 14048, loss = 3.55217929\n",
      "Iteration 14049, loss = 5.02481938\n",
      "Iteration 14050, loss = 3.60601089\n",
      "Iteration 14051, loss = 4.16878444\n",
      "Iteration 14052, loss = 6.73609011\n",
      "Iteration 14053, loss = 6.73851749\n",
      "Iteration 14054, loss = 4.17177919\n",
      "Iteration 14055, loss = 3.48803712\n",
      "Iteration 14056, loss = 3.62128955\n",
      "Iteration 14057, loss = 2.21355259\n",
      "Iteration 14058, loss = 3.85373691\n",
      "Iteration 14059, loss = 3.64172603\n",
      "Iteration 14060, loss = 2.46881318\n",
      "Iteration 14061, loss = 2.27535976\n",
      "Iteration 14062, loss = 2.16379417\n",
      "Iteration 14063, loss = 2.80285716\n",
      "Iteration 14064, loss = 2.18267265\n",
      "Iteration 14065, loss = 2.04494947\n",
      "Iteration 14066, loss = 2.18496582\n",
      "Iteration 14067, loss = 2.12656636\n",
      "Iteration 14068, loss = 2.52074809\n",
      "Iteration 14069, loss = 2.45167993\n",
      "Iteration 14070, loss = 2.10658631\n",
      "Iteration 14071, loss = 1.92984386\n",
      "Iteration 14072, loss = 1.98769675\n",
      "Iteration 14073, loss = 2.02016612\n",
      "Iteration 14074, loss = 2.03201930\n",
      "Iteration 14075, loss = 2.03638826\n",
      "Iteration 14076, loss = 2.34499600\n",
      "Iteration 14077, loss = 3.87859971\n",
      "Iteration 14078, loss = 2.59180493\n",
      "Iteration 14079, loss = 3.44478128\n",
      "Iteration 14080, loss = 2.25075601\n",
      "Iteration 14081, loss = 2.10077969\n",
      "Iteration 14082, loss = 2.07320760\n",
      "Iteration 14083, loss = 2.00804598\n",
      "Iteration 14084, loss = 1.91976374\n",
      "Iteration 14085, loss = 1.98377650\n",
      "Iteration 14086, loss = 1.99658066\n",
      "Iteration 14087, loss = 2.08201023\n",
      "Iteration 14088, loss = 2.43878856\n",
      "Iteration 14089, loss = 2.51145866\n",
      "Iteration 14090, loss = 2.32798926\n",
      "Iteration 14091, loss = 2.53339747\n",
      "Iteration 14092, loss = 3.39002578\n",
      "Iteration 14093, loss = 3.64659013\n",
      "Iteration 14094, loss = 3.16165652\n",
      "Iteration 14095, loss = 4.12689077\n",
      "Iteration 14096, loss = 2.52666037\n",
      "Iteration 14097, loss = 2.27065553\n",
      "Iteration 14098, loss = 1.97124725\n",
      "Iteration 14099, loss = 1.98316947\n",
      "Iteration 14100, loss = 2.00402317\n",
      "Iteration 14101, loss = 2.14808615\n",
      "Iteration 14102, loss = 2.59077856\n",
      "Iteration 14103, loss = 2.24718605\n",
      "Iteration 14104, loss = 2.68323765\n",
      "Iteration 14105, loss = 2.01710574\n",
      "Iteration 14106, loss = 2.14079486\n",
      "Iteration 14107, loss = 2.07200214\n",
      "Iteration 14108, loss = 2.02860480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14109, loss = 2.40885768\n",
      "Iteration 14110, loss = 2.21148669\n",
      "Iteration 14111, loss = 1.92984661\n",
      "Iteration 14112, loss = 2.59247028\n",
      "Iteration 14113, loss = 2.48278096\n",
      "Iteration 14114, loss = 2.01641371\n",
      "Iteration 14115, loss = 2.21515065\n",
      "Iteration 14116, loss = 2.40923040\n",
      "Iteration 14117, loss = 2.13662414\n",
      "Iteration 14118, loss = 1.95588778\n",
      "Iteration 14119, loss = 1.95657436\n",
      "Iteration 14120, loss = 2.26955670\n",
      "Iteration 14121, loss = 2.47121697\n",
      "Iteration 14122, loss = 3.43241270\n",
      "Iteration 14123, loss = 2.70531927\n",
      "Iteration 14124, loss = 1.95500829\n",
      "Iteration 14125, loss = 2.60766467\n",
      "Iteration 14126, loss = 2.84738721\n",
      "Iteration 14127, loss = 2.73687941\n",
      "Iteration 14128, loss = 3.26407739\n",
      "Iteration 14129, loss = 3.71939225\n",
      "Iteration 14130, loss = 3.47736098\n",
      "Iteration 14131, loss = 3.93187668\n",
      "Iteration 14132, loss = 2.95556322\n",
      "Iteration 14133, loss = 3.04236987\n",
      "Iteration 14134, loss = 2.90579950\n",
      "Iteration 14135, loss = 3.86905836\n",
      "Iteration 14136, loss = 2.55715548\n",
      "Iteration 14137, loss = 3.04325437\n",
      "Iteration 14138, loss = 2.90413324\n",
      "Iteration 14139, loss = 3.27022718\n",
      "Iteration 14140, loss = 2.69186272\n",
      "Iteration 14141, loss = 3.23636191\n",
      "Iteration 14142, loss = 3.39568971\n",
      "Iteration 14143, loss = 3.40149124\n",
      "Iteration 14144, loss = 4.18704853\n",
      "Iteration 14145, loss = 2.79955897\n",
      "Iteration 14146, loss = 2.43888541\n",
      "Iteration 14147, loss = 2.77269572\n",
      "Iteration 14148, loss = 3.36436764\n",
      "Iteration 14149, loss = 3.03522473\n",
      "Iteration 14150, loss = 2.82027602\n",
      "Iteration 14151, loss = 3.04878898\n",
      "Iteration 14152, loss = 2.11019934\n",
      "Iteration 14153, loss = 2.02115133\n",
      "Iteration 14154, loss = 1.94077284\n",
      "Iteration 14155, loss = 2.07587175\n",
      "Iteration 14156, loss = 2.04602846\n",
      "Iteration 14157, loss = 1.98270661\n",
      "Iteration 14158, loss = 2.00612394\n",
      "Iteration 14159, loss = 1.92395945\n",
      "Iteration 14160, loss = 1.94832585\n",
      "Iteration 14161, loss = 3.15412151\n",
      "Iteration 14162, loss = 2.49998996\n",
      "Iteration 14163, loss = 2.27262325\n",
      "Iteration 14164, loss = 2.13806924\n",
      "Iteration 14165, loss = 2.08315742\n",
      "Iteration 14166, loss = 2.22767285\n",
      "Iteration 14167, loss = 1.98511971\n",
      "Iteration 14168, loss = 2.23334822\n",
      "Iteration 14169, loss = 3.74076752\n",
      "Iteration 14170, loss = 9.12761482\n",
      "Iteration 14171, loss = 8.45405679\n",
      "Iteration 14172, loss = 4.90134962\n",
      "Iteration 14173, loss = 4.60995966\n",
      "Iteration 14174, loss = 2.82237208\n",
      "Iteration 14175, loss = 3.21181539\n",
      "Iteration 14176, loss = 2.44314403\n",
      "Iteration 14177, loss = 2.15267564\n",
      "Iteration 14178, loss = 2.26324234\n",
      "Iteration 14179, loss = 2.08887719\n",
      "Iteration 14180, loss = 2.02266410\n",
      "Iteration 14181, loss = 1.96307681\n",
      "Iteration 14182, loss = 1.98899934\n",
      "Iteration 14183, loss = 1.99264022\n",
      "Iteration 14184, loss = 2.57524068\n",
      "Iteration 14185, loss = 2.52661999\n",
      "Iteration 14186, loss = 2.38103245\n",
      "Iteration 14187, loss = 2.06442208\n",
      "Iteration 14188, loss = 2.03078964\n",
      "Iteration 14189, loss = 2.04386458\n",
      "Iteration 14190, loss = 1.91990334\n",
      "Iteration 14191, loss = 1.86295161\n",
      "Iteration 14192, loss = 1.96057230\n",
      "Iteration 14193, loss = 1.93271933\n",
      "Iteration 14194, loss = 2.24077600\n",
      "Iteration 14195, loss = 2.27212110\n",
      "Iteration 14196, loss = 2.02263824\n",
      "Iteration 14197, loss = 1.95409843\n",
      "Iteration 14198, loss = 2.06676087\n",
      "Iteration 14199, loss = 2.51779680\n",
      "Iteration 14200, loss = 2.37718086\n",
      "Iteration 14201, loss = 1.93604254\n",
      "Iteration 14202, loss = 2.01690928\n",
      "Iteration 14203, loss = 3.10020605\n",
      "Iteration 14204, loss = 3.05257600\n",
      "Iteration 14205, loss = 2.94339495\n",
      "Iteration 14206, loss = 2.59799313\n",
      "Iteration 14207, loss = 2.38283684\n",
      "Iteration 14208, loss = 2.29348763\n",
      "Iteration 14209, loss = 2.16330977\n",
      "Iteration 14210, loss = 1.96540369\n",
      "Iteration 14211, loss = 2.08076123\n",
      "Iteration 14212, loss = 2.00019798\n",
      "Iteration 14213, loss = 2.19241740\n",
      "Iteration 14214, loss = 3.20493990\n",
      "Iteration 14215, loss = 3.53630329\n",
      "Iteration 14216, loss = 2.35538971\n",
      "Iteration 14217, loss = 2.03340431\n",
      "Iteration 14218, loss = 1.96239481\n",
      "Iteration 14219, loss = 1.85951241\n",
      "Iteration 14220, loss = 1.99678384\n",
      "Iteration 14221, loss = 2.14631821\n",
      "Iteration 14222, loss = 2.21629400\n",
      "Iteration 14223, loss = 3.43657949\n",
      "Iteration 14224, loss = 3.99853506\n",
      "Iteration 14225, loss = 2.33681542\n",
      "Iteration 14226, loss = 2.36493506\n",
      "Iteration 14227, loss = 1.99988017\n",
      "Iteration 14228, loss = 2.18129496\n",
      "Iteration 14229, loss = 1.96084535\n",
      "Iteration 14230, loss = 2.00900172\n",
      "Iteration 14231, loss = 2.10344365\n",
      "Iteration 14232, loss = 1.93555284\n",
      "Iteration 14233, loss = 2.88504530\n",
      "Iteration 14234, loss = 2.62175585\n",
      "Iteration 14235, loss = 3.77934877\n",
      "Iteration 14236, loss = 2.92078957\n",
      "Iteration 14237, loss = 2.64027470\n",
      "Iteration 14238, loss = 2.33047868\n",
      "Iteration 14239, loss = 2.47451533\n",
      "Iteration 14240, loss = 2.36207199\n",
      "Iteration 14241, loss = 2.52451430\n",
      "Iteration 14242, loss = 2.36768566\n",
      "Iteration 14243, loss = 3.03783404\n",
      "Iteration 14244, loss = 2.58814235\n",
      "Iteration 14245, loss = 2.39346487\n",
      "Iteration 14246, loss = 2.17215758\n",
      "Iteration 14247, loss = 2.04169131\n",
      "Iteration 14248, loss = 1.96646045\n",
      "Iteration 14249, loss = 2.06868886\n",
      "Iteration 14250, loss = 2.29039101\n",
      "Iteration 14251, loss = 2.04007441\n",
      "Iteration 14252, loss = 2.27928687\n",
      "Iteration 14253, loss = 2.18109539\n",
      "Iteration 14254, loss = 1.84447277\n",
      "Iteration 14255, loss = 2.21388872\n",
      "Iteration 14256, loss = 2.51508001\n",
      "Iteration 14257, loss = 2.18921511\n",
      "Iteration 14258, loss = 2.32584674\n",
      "Iteration 14259, loss = 2.42620617\n",
      "Iteration 14260, loss = 3.08023798\n",
      "Iteration 14261, loss = 2.49668354\n",
      "Iteration 14262, loss = 2.42381803\n",
      "Iteration 14263, loss = 2.26337815\n",
      "Iteration 14264, loss = 1.94834218\n",
      "Iteration 14265, loss = 2.22439405\n",
      "Iteration 14266, loss = 3.21378059\n",
      "Iteration 14267, loss = 3.72104074\n",
      "Iteration 14268, loss = 3.52976598\n",
      "Iteration 14269, loss = 4.32134923\n",
      "Iteration 14270, loss = 5.94588643\n",
      "Iteration 14271, loss = 10.31084582\n",
      "Iteration 14272, loss = 7.79667130\n",
      "Iteration 14273, loss = 7.36186462\n",
      "Iteration 14274, loss = 5.79328532\n",
      "Iteration 14275, loss = 4.83703209\n",
      "Iteration 14276, loss = 4.15173539\n",
      "Iteration 14277, loss = 3.35449594\n",
      "Iteration 14278, loss = 3.82079107\n",
      "Iteration 14279, loss = 2.88095617\n",
      "Iteration 14280, loss = 2.45107000\n",
      "Iteration 14281, loss = 2.57292135\n",
      "Iteration 14282, loss = 2.28263719\n",
      "Iteration 14283, loss = 2.05351102\n",
      "Iteration 14284, loss = 1.88102603\n",
      "Iteration 14285, loss = 1.90296600\n",
      "Iteration 14286, loss = 2.37895830\n",
      "Iteration 14287, loss = 3.91723423\n",
      "Iteration 14288, loss = 2.50759969\n",
      "Iteration 14289, loss = 3.42969327\n",
      "Iteration 14290, loss = 3.99228436\n",
      "Iteration 14291, loss = 5.98436017\n",
      "Iteration 14292, loss = 4.23606314\n",
      "Iteration 14293, loss = 5.82291776\n",
      "Iteration 14294, loss = 10.76896783\n",
      "Iteration 14295, loss = 14.95386426\n",
      "Iteration 14296, loss = 16.77652770\n",
      "Iteration 14297, loss = 14.35211245\n",
      "Iteration 14298, loss = 13.71002499\n",
      "Iteration 14299, loss = 11.70791111\n",
      "Iteration 14300, loss = 11.63067838\n",
      "Iteration 14301, loss = 10.00932074\n",
      "Iteration 14302, loss = 9.20830458\n",
      "Iteration 14303, loss = 8.28793725\n",
      "Iteration 14304, loss = 6.98109233\n",
      "Iteration 14305, loss = 6.54520762\n",
      "Iteration 14306, loss = 7.03862228\n",
      "Iteration 14307, loss = 7.48721225\n",
      "Iteration 14308, loss = 6.93178655\n",
      "Iteration 14309, loss = 7.42188256\n",
      "Iteration 14310, loss = 11.18348926\n",
      "Iteration 14311, loss = 8.90840549\n",
      "Iteration 14312, loss = 10.09892918\n",
      "Iteration 14313, loss = 9.81490482\n",
      "Iteration 14314, loss = 8.44011992\n",
      "Iteration 14315, loss = 7.93107485\n",
      "Iteration 14316, loss = 6.52448717\n",
      "Iteration 14317, loss = 6.03368569\n",
      "Iteration 14318, loss = 5.53557041\n",
      "Iteration 14319, loss = 5.61473675\n",
      "Iteration 14320, loss = 5.78043928\n",
      "Iteration 14321, loss = 6.17116156\n",
      "Iteration 14322, loss = 5.50374064\n",
      "Iteration 14323, loss = 5.41161695\n",
      "Iteration 14324, loss = 5.46189054\n",
      "Iteration 14325, loss = 5.13900748\n",
      "Iteration 14326, loss = 5.09077808\n",
      "Iteration 14327, loss = 5.17250837\n",
      "Iteration 14328, loss = 5.05664617\n",
      "Iteration 14329, loss = 5.08849166\n",
      "Iteration 14330, loss = 5.10418027\n",
      "Iteration 14331, loss = 5.24252153\n",
      "Iteration 14332, loss = 6.10141657\n",
      "Iteration 14333, loss = 5.87081781\n",
      "Iteration 14334, loss = 5.63939321\n",
      "Iteration 14335, loss = 6.65079034\n",
      "Iteration 14336, loss = 5.32452805\n",
      "Iteration 14337, loss = 6.21577746\n",
      "Iteration 14338, loss = 5.03717525\n",
      "Iteration 14339, loss = 4.94272505\n",
      "Iteration 14340, loss = 5.00173152\n",
      "Iteration 14341, loss = 4.98990114\n",
      "Iteration 14342, loss = 5.11600771\n",
      "Iteration 14343, loss = 5.09735242\n",
      "Iteration 14344, loss = 5.01567568\n",
      "Iteration 14345, loss = 4.89544291\n",
      "Iteration 14346, loss = 5.15005534\n",
      "Iteration 14347, loss = 4.85350645\n",
      "Iteration 14348, loss = 4.90639862\n",
      "Iteration 14349, loss = 4.80121126\n",
      "Iteration 14350, loss = 4.96802447\n",
      "Iteration 14351, loss = 5.22427672\n",
      "Iteration 14352, loss = 5.67300847\n",
      "Iteration 14353, loss = 5.16722699\n",
      "Iteration 14354, loss = 6.95475011\n",
      "Iteration 14355, loss = 6.72922374\n",
      "Iteration 14356, loss = 5.08151990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14357, loss = 4.90806148\n",
      "Iteration 14358, loss = 5.50644278\n",
      "Iteration 14359, loss = 6.08363928\n",
      "Iteration 14360, loss = 5.28848316\n",
      "Iteration 14361, loss = 5.29490494\n",
      "Iteration 14362, loss = 4.88782982\n",
      "Iteration 14363, loss = 5.78526640\n",
      "Iteration 14364, loss = 4.95894765\n",
      "Iteration 14365, loss = 5.28360624\n",
      "Iteration 14366, loss = 5.37575161\n",
      "Iteration 14367, loss = 4.68080043\n",
      "Iteration 14368, loss = 6.84757245\n",
      "Iteration 14369, loss = 6.83285518\n",
      "Iteration 14370, loss = 6.23211112\n",
      "Iteration 14371, loss = 5.72220837\n",
      "Iteration 14372, loss = 5.57748998\n",
      "Iteration 14373, loss = 4.81437789\n",
      "Iteration 14374, loss = 4.63426807\n",
      "Iteration 14375, loss = 4.97937313\n",
      "Iteration 14376, loss = 5.07077040\n",
      "Iteration 14377, loss = 4.66660954\n",
      "Iteration 14378, loss = 4.71861921\n",
      "Iteration 14379, loss = 4.58924376\n",
      "Iteration 14380, loss = 4.89772467\n",
      "Iteration 14381, loss = 4.73615022\n",
      "Iteration 14382, loss = 5.09462348\n",
      "Iteration 14383, loss = 4.93782258\n",
      "Iteration 14384, loss = 4.87959974\n",
      "Iteration 14385, loss = 4.62467453\n",
      "Iteration 14386, loss = 4.76134337\n",
      "Iteration 14387, loss = 4.64277770\n",
      "Iteration 14388, loss = 4.74873624\n",
      "Iteration 14389, loss = 4.51077415\n",
      "Iteration 14390, loss = 4.73118565\n",
      "Iteration 14391, loss = 5.01610811\n",
      "Iteration 14392, loss = 4.74308381\n",
      "Iteration 14393, loss = 5.81501699\n",
      "Iteration 14394, loss = 5.37049638\n",
      "Iteration 14395, loss = 4.81423997\n",
      "Iteration 14396, loss = 6.20092220\n",
      "Iteration 14397, loss = 5.21629055\n",
      "Iteration 14398, loss = 5.21412591\n",
      "Iteration 14399, loss = 5.52722524\n",
      "Iteration 14400, loss = 5.39350705\n",
      "Iteration 14401, loss = 5.29864990\n",
      "Iteration 14402, loss = 4.92011651\n",
      "Iteration 14403, loss = 5.45530719\n",
      "Iteration 14404, loss = 5.54687611\n",
      "Iteration 14405, loss = 7.19790616\n",
      "Iteration 14406, loss = 7.13948133\n",
      "Iteration 14407, loss = 5.89431528\n",
      "Iteration 14408, loss = 5.15683906\n",
      "Iteration 14409, loss = 4.62184267\n",
      "Iteration 14410, loss = 4.35285385\n",
      "Iteration 14411, loss = 4.57151090\n",
      "Iteration 14412, loss = 4.54658186\n",
      "Iteration 14413, loss = 4.98342423\n",
      "Iteration 14414, loss = 7.30515855\n",
      "Iteration 14415, loss = 9.79199077\n",
      "Iteration 14416, loss = 7.38722713\n",
      "Iteration 14417, loss = 5.04141166\n",
      "Iteration 14418, loss = 4.97288697\n",
      "Iteration 14419, loss = 4.96455060\n",
      "Iteration 14420, loss = 4.46334218\n",
      "Iteration 14421, loss = 5.52701334\n",
      "Iteration 14422, loss = 4.82519057\n",
      "Iteration 14423, loss = 4.43405088\n",
      "Iteration 14424, loss = 4.50070619\n",
      "Iteration 14425, loss = 4.37572068\n",
      "Iteration 14426, loss = 4.78261154\n",
      "Iteration 14427, loss = 4.53407555\n",
      "Iteration 14428, loss = 4.37282301\n",
      "Iteration 14429, loss = 4.44387763\n",
      "Iteration 14430, loss = 4.42639707\n",
      "Iteration 14431, loss = 4.44256104\n",
      "Iteration 14432, loss = 4.37909846\n",
      "Iteration 14433, loss = 4.47258077\n",
      "Iteration 14434, loss = 4.73390293\n",
      "Iteration 14435, loss = 5.13069914\n",
      "Iteration 14436, loss = 4.94835238\n",
      "Iteration 14437, loss = 4.64439166\n",
      "Iteration 14438, loss = 4.53586922\n",
      "Iteration 14439, loss = 4.79797817\n",
      "Iteration 14440, loss = 4.92027488\n",
      "Iteration 14441, loss = 4.54531000\n",
      "Iteration 14442, loss = 4.51416577\n",
      "Iteration 14443, loss = 4.48399817\n",
      "Iteration 14444, loss = 4.41005611\n",
      "Iteration 14445, loss = 4.43722723\n",
      "Iteration 14446, loss = 4.34573759\n",
      "Iteration 14447, loss = 4.28439363\n",
      "Iteration 14448, loss = 4.77497577\n",
      "Iteration 14449, loss = 4.74067624\n",
      "Iteration 14450, loss = 4.88288502\n",
      "Iteration 14451, loss = 4.73025297\n",
      "Iteration 14452, loss = 4.66287058\n",
      "Iteration 14453, loss = 4.37167557\n",
      "Iteration 14454, loss = 4.42469599\n",
      "Iteration 14455, loss = 4.40327022\n",
      "Iteration 14456, loss = 4.42746651\n",
      "Iteration 14457, loss = 4.35070381\n",
      "Iteration 14458, loss = 4.88443681\n",
      "Iteration 14459, loss = 4.36201958\n",
      "Iteration 14460, loss = 4.59418270\n",
      "Iteration 14461, loss = 4.20034078\n",
      "Iteration 14462, loss = 4.41069632\n",
      "Iteration 14463, loss = 4.48600475\n",
      "Iteration 14464, loss = 4.57243264\n",
      "Iteration 14465, loss = 4.65320432\n",
      "Iteration 14466, loss = 4.48327591\n",
      "Iteration 14467, loss = 4.29728284\n",
      "Iteration 14468, loss = 4.85502616\n",
      "Iteration 14469, loss = 4.54747708\n",
      "Iteration 14470, loss = 5.36668453\n",
      "Iteration 14471, loss = 4.46110603\n",
      "Iteration 14472, loss = 4.50036918\n",
      "Iteration 14473, loss = 4.60541498\n",
      "Iteration 14474, loss = 5.30201857\n",
      "Iteration 14475, loss = 5.31768648\n",
      "Iteration 14476, loss = 4.67444688\n",
      "Iteration 14477, loss = 4.62399438\n",
      "Iteration 14478, loss = 4.32071189\n",
      "Iteration 14479, loss = 4.39629087\n",
      "Iteration 14480, loss = 4.22262028\n",
      "Iteration 14481, loss = 4.20759171\n",
      "Iteration 14482, loss = 4.46399480\n",
      "Iteration 14483, loss = 4.74737922\n",
      "Iteration 14484, loss = 4.71208649\n",
      "Iteration 14485, loss = 4.74172620\n",
      "Iteration 14486, loss = 4.38578255\n",
      "Iteration 14487, loss = 4.22993620\n",
      "Iteration 14488, loss = 4.48281994\n",
      "Iteration 14489, loss = 4.26090440\n",
      "Iteration 14490, loss = 4.22943956\n",
      "Iteration 14491, loss = 4.22858074\n",
      "Iteration 14492, loss = 4.19390567\n",
      "Iteration 14493, loss = 4.26705311\n",
      "Iteration 14494, loss = 4.28212466\n",
      "Iteration 14495, loss = 5.65209779\n",
      "Iteration 14496, loss = 4.46868728\n",
      "Iteration 14497, loss = 4.16600143\n",
      "Iteration 14498, loss = 4.40211347\n",
      "Iteration 14499, loss = 4.14590739\n",
      "Iteration 14500, loss = 4.68009469\n",
      "Iteration 14501, loss = 5.42926384\n",
      "Iteration 14502, loss = 6.12390708\n",
      "Iteration 14503, loss = 5.64783839\n",
      "Iteration 14504, loss = 4.64064365\n",
      "Iteration 14505, loss = 5.20090974\n",
      "Iteration 14506, loss = 4.43681694\n",
      "Iteration 14507, loss = 4.86897299\n",
      "Iteration 14508, loss = 5.12630969\n",
      "Iteration 14509, loss = 4.24322194\n",
      "Iteration 14510, loss = 4.81935614\n",
      "Iteration 14511, loss = 4.46033143\n",
      "Iteration 14512, loss = 4.30788939\n",
      "Iteration 14513, loss = 4.31268503\n",
      "Iteration 14514, loss = 4.36301020\n",
      "Iteration 14515, loss = 4.65273507\n",
      "Iteration 14516, loss = 4.84303090\n",
      "Iteration 14517, loss = 5.07986835\n",
      "Iteration 14518, loss = 7.14399953\n",
      "Iteration 14519, loss = 5.68546045\n",
      "Iteration 14520, loss = 6.15148900\n",
      "Iteration 14521, loss = 7.36185485\n",
      "Iteration 14522, loss = 5.34576210\n",
      "Iteration 14523, loss = 4.47615648\n",
      "Iteration 14524, loss = 4.29806632\n",
      "Iteration 14525, loss = 6.96886541\n",
      "Iteration 14526, loss = 7.72461863\n",
      "Iteration 14527, loss = 6.04143364\n",
      "Iteration 14528, loss = 4.45491199\n",
      "Iteration 14529, loss = 4.47582240\n",
      "Iteration 14530, loss = 4.63776685\n",
      "Iteration 14531, loss = 4.22580045\n",
      "Iteration 14532, loss = 4.52803602\n",
      "Iteration 14533, loss = 4.32813079\n",
      "Iteration 14534, loss = 4.11139141\n",
      "Iteration 14535, loss = 4.09827085\n",
      "Iteration 14536, loss = 4.25139318\n",
      "Iteration 14537, loss = 4.73946127\n",
      "Iteration 14538, loss = 4.79196712\n",
      "Iteration 14539, loss = 4.98781869\n",
      "Iteration 14540, loss = 5.82361335\n",
      "Iteration 14541, loss = 4.17082741\n",
      "Iteration 14542, loss = 4.33896400\n",
      "Iteration 14543, loss = 4.05873220\n",
      "Iteration 14544, loss = 4.26294657\n",
      "Iteration 14545, loss = 4.11668745\n",
      "Iteration 14546, loss = 4.12300542\n",
      "Iteration 14547, loss = 4.21924040\n",
      "Iteration 14548, loss = 4.81591489\n",
      "Iteration 14549, loss = 4.82704206\n",
      "Iteration 14550, loss = 4.22880359\n",
      "Iteration 14551, loss = 4.14315633\n",
      "Iteration 14552, loss = 4.09889845\n",
      "Iteration 14553, loss = 4.26201507\n",
      "Iteration 14554, loss = 4.21339105\n",
      "Iteration 14555, loss = 4.36062666\n",
      "Iteration 14556, loss = 4.71437411\n",
      "Iteration 14557, loss = 4.09410984\n",
      "Iteration 14558, loss = 4.29911491\n",
      "Iteration 14559, loss = 4.48440864\n",
      "Iteration 14560, loss = 5.56831846\n",
      "Iteration 14561, loss = 6.97409276\n",
      "Iteration 14562, loss = 5.63668562\n",
      "Iteration 14563, loss = 6.48435449\n",
      "Iteration 14564, loss = 5.55478354\n",
      "Iteration 14565, loss = 5.82629119\n",
      "Iteration 14566, loss = 4.08342071\n",
      "Iteration 14567, loss = 5.46170991\n",
      "Iteration 14568, loss = 4.22458981\n",
      "Iteration 14569, loss = 4.10369514\n",
      "Iteration 14570, loss = 4.80372587\n",
      "Iteration 14571, loss = 4.35239549\n",
      "Iteration 14572, loss = 3.99928823\n",
      "Iteration 14573, loss = 4.08570245\n",
      "Iteration 14574, loss = 4.27755047\n",
      "Iteration 14575, loss = 4.19863025\n",
      "Iteration 14576, loss = 4.20585147\n",
      "Iteration 14577, loss = 4.63256571\n",
      "Iteration 14578, loss = 4.01751809\n",
      "Iteration 14579, loss = 4.23486052\n",
      "Iteration 14580, loss = 4.07282009\n",
      "Iteration 14581, loss = 4.26577866\n",
      "Iteration 14582, loss = 4.36846446\n",
      "Iteration 14583, loss = 4.17101724\n",
      "Iteration 14584, loss = 5.40483517\n",
      "Iteration 14585, loss = 6.46486201\n",
      "Iteration 14586, loss = 5.15918739\n",
      "Iteration 14587, loss = 5.76137419\n",
      "Iteration 14588, loss = 4.56118839\n",
      "Iteration 14589, loss = 4.03872172\n",
      "Iteration 14590, loss = 3.93545018\n",
      "Iteration 14591, loss = 5.15139303\n",
      "Iteration 14592, loss = 4.34103934\n",
      "Iteration 14593, loss = 4.15401869\n",
      "Iteration 14594, loss = 4.91694361\n",
      "Iteration 14595, loss = 8.73033069\n",
      "Iteration 14596, loss = 9.85363919\n",
      "Iteration 14597, loss = 6.72171413\n",
      "Iteration 14598, loss = 6.72379552\n",
      "Iteration 14599, loss = 7.29381386\n",
      "Iteration 14600, loss = 5.82909593\n",
      "Iteration 14601, loss = 5.83503646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14602, loss = 4.09550888\n",
      "Iteration 14603, loss = 4.38188041\n",
      "Iteration 14604, loss = 4.13052128\n",
      "Iteration 14605, loss = 4.67200500\n",
      "Iteration 14606, loss = 4.38233446\n",
      "Iteration 14607, loss = 4.08364104\n",
      "Iteration 14608, loss = 4.15094385\n",
      "Iteration 14609, loss = 4.54713669\n",
      "Iteration 14610, loss = 4.14763617\n",
      "Iteration 14611, loss = 4.17288852\n",
      "Iteration 14612, loss = 4.59429420\n",
      "Iteration 14613, loss = 4.03763378\n",
      "Iteration 14614, loss = 4.64034484\n",
      "Iteration 14615, loss = 4.04419508\n",
      "Iteration 14616, loss = 4.52822783\n",
      "Iteration 14617, loss = 4.40811152\n",
      "Iteration 14618, loss = 3.99531782\n",
      "Iteration 14619, loss = 4.70588799\n",
      "Iteration 14620, loss = 4.27463416\n",
      "Iteration 14621, loss = 4.02390917\n",
      "Iteration 14622, loss = 4.14879716\n",
      "Iteration 14623, loss = 4.06322734\n",
      "Iteration 14624, loss = 4.27909184\n",
      "Iteration 14625, loss = 4.58979471\n",
      "Iteration 14626, loss = 4.00086212\n",
      "Iteration 14627, loss = 4.07015924\n",
      "Iteration 14628, loss = 4.12663400\n",
      "Iteration 14629, loss = 4.09733568\n",
      "Iteration 14630, loss = 4.03121977\n",
      "Iteration 14631, loss = 4.02087646\n",
      "Iteration 14632, loss = 4.13451046\n",
      "Iteration 14633, loss = 3.98149823\n",
      "Iteration 14634, loss = 4.08769800\n",
      "Iteration 14635, loss = 4.02518470\n",
      "Iteration 14636, loss = 4.05223076\n",
      "Iteration 14637, loss = 4.04561684\n",
      "Iteration 14638, loss = 4.09110108\n",
      "Iteration 14639, loss = 4.24196470\n",
      "Iteration 14640, loss = 5.33309160\n",
      "Iteration 14641, loss = 4.02935096\n",
      "Iteration 14642, loss = 4.09836599\n",
      "Iteration 14643, loss = 4.62828003\n",
      "Iteration 14644, loss = 4.29720875\n",
      "Iteration 14645, loss = 4.12124221\n",
      "Iteration 14646, loss = 4.03630105\n",
      "Iteration 14647, loss = 4.03111144\n",
      "Iteration 14648, loss = 3.99306593\n",
      "Iteration 14649, loss = 4.27734494\n",
      "Iteration 14650, loss = 4.28351125\n",
      "Iteration 14651, loss = 4.10846151\n",
      "Iteration 14652, loss = 4.34062442\n",
      "Iteration 14653, loss = 4.26054413\n",
      "Iteration 14654, loss = 4.05181172\n",
      "Iteration 14655, loss = 3.99294308\n",
      "Iteration 14656, loss = 4.13765653\n",
      "Iteration 14657, loss = 3.96154956\n",
      "Iteration 14658, loss = 4.70427944\n",
      "Iteration 14659, loss = 7.51703403\n",
      "Iteration 14660, loss = 7.16007322\n",
      "Iteration 14661, loss = 4.99350440\n",
      "Iteration 14662, loss = 5.20052115\n",
      "Iteration 14663, loss = 4.07106302\n",
      "Iteration 14664, loss = 4.13086602\n",
      "Iteration 14665, loss = 4.46260015\n",
      "Iteration 14666, loss = 4.45411609\n",
      "Iteration 14667, loss = 3.99155488\n",
      "Iteration 14668, loss = 4.07217652\n",
      "Iteration 14669, loss = 3.96878067\n",
      "Iteration 14670, loss = 3.98917854\n",
      "Iteration 14671, loss = 4.17531598\n",
      "Iteration 14672, loss = 3.87651745\n",
      "Iteration 14673, loss = 3.90174561\n",
      "Iteration 14674, loss = 4.08894901\n",
      "Iteration 14675, loss = 4.27242827\n",
      "Iteration 14676, loss = 4.42559735\n",
      "Iteration 14677, loss = 4.41625237\n",
      "Iteration 14678, loss = 5.53459967\n",
      "Iteration 14679, loss = 9.31729823\n",
      "Iteration 14680, loss = 10.83814913\n",
      "Iteration 14681, loss = 5.97647665\n",
      "Iteration 14682, loss = 7.65673859\n",
      "Iteration 14683, loss = 5.77686987\n",
      "Iteration 14684, loss = 4.24087327\n",
      "Iteration 14685, loss = 4.86040861\n",
      "Iteration 14686, loss = 4.13413565\n",
      "Iteration 14687, loss = 4.24250036\n",
      "Iteration 14688, loss = 4.26473942\n",
      "Iteration 14689, loss = 4.16953216\n",
      "Iteration 14690, loss = 4.30857557\n",
      "Iteration 14691, loss = 3.97510198\n",
      "Iteration 14692, loss = 4.39806803\n",
      "Iteration 14693, loss = 4.08173652\n",
      "Iteration 14694, loss = 3.95759992\n",
      "Iteration 14695, loss = 4.55324422\n",
      "Iteration 14696, loss = 6.37195452\n",
      "Iteration 14697, loss = 4.91357897\n",
      "Iteration 14698, loss = 4.92117786\n",
      "Iteration 14699, loss = 5.15077539\n",
      "Iteration 14700, loss = 4.95227255\n",
      "Iteration 14701, loss = 5.91099040\n",
      "Iteration 14702, loss = 4.51391699\n",
      "Iteration 14703, loss = 7.07827586\n",
      "Iteration 14704, loss = 9.81730166\n",
      "Iteration 14705, loss = 6.90788046\n",
      "Iteration 14706, loss = 6.25998650\n",
      "Iteration 14707, loss = 5.62772958\n",
      "Iteration 14708, loss = 4.59603956\n",
      "Iteration 14709, loss = 4.36451192\n",
      "Iteration 14710, loss = 4.64149459\n",
      "Iteration 14711, loss = 4.84049039\n",
      "Iteration 14712, loss = 3.92980313\n",
      "Iteration 14713, loss = 4.01301322\n",
      "Iteration 14714, loss = 3.90620917\n",
      "Iteration 14715, loss = 3.87167947\n",
      "Iteration 14716, loss = 4.15972419\n",
      "Iteration 14717, loss = 3.90047406\n",
      "Iteration 14718, loss = 3.85352396\n",
      "Iteration 14719, loss = 4.26755040\n",
      "Iteration 14720, loss = 3.93444508\n",
      "Iteration 14721, loss = 4.14307575\n",
      "Iteration 14722, loss = 4.19015016\n",
      "Iteration 14723, loss = 4.68498840\n",
      "Iteration 14724, loss = 5.63114216\n",
      "Iteration 14725, loss = 5.82031698\n",
      "Iteration 14726, loss = 6.96088323\n",
      "Iteration 14727, loss = 7.60806761\n",
      "Iteration 14728, loss = 6.23925719\n",
      "Iteration 14729, loss = 6.26465970\n",
      "Iteration 14730, loss = 5.30246623\n",
      "Iteration 14731, loss = 4.52746981\n",
      "Iteration 14732, loss = 4.77586035\n",
      "Iteration 14733, loss = 4.22392051\n",
      "Iteration 14734, loss = 4.38841036\n",
      "Iteration 14735, loss = 4.66144751\n",
      "Iteration 14736, loss = 4.86178588\n",
      "Iteration 14737, loss = 4.65828864\n",
      "Iteration 14738, loss = 3.82186706\n",
      "Iteration 14739, loss = 3.85302888\n",
      "Iteration 14740, loss = 4.26025725\n",
      "Iteration 14741, loss = 4.18299721\n",
      "Iteration 14742, loss = 4.19076114\n",
      "Iteration 14743, loss = 3.90166510\n",
      "Iteration 14744, loss = 4.15621697\n",
      "Iteration 14745, loss = 4.07126199\n",
      "Iteration 14746, loss = 4.06523600\n",
      "Iteration 14747, loss = 4.61047492\n",
      "Iteration 14748, loss = 4.08339288\n",
      "Iteration 14749, loss = 4.03175003\n",
      "Iteration 14750, loss = 3.80297807\n",
      "Iteration 14751, loss = 3.92990458\n",
      "Iteration 14752, loss = 3.92434949\n",
      "Iteration 14753, loss = 3.98976377\n",
      "Iteration 14754, loss = 5.09817874\n",
      "Iteration 14755, loss = 5.21657262\n",
      "Iteration 14756, loss = 4.21167870\n",
      "Iteration 14757, loss = 4.07777343\n",
      "Iteration 14758, loss = 3.96518785\n",
      "Iteration 14759, loss = 4.13039575\n",
      "Iteration 14760, loss = 4.15128300\n",
      "Iteration 14761, loss = 4.43013318\n",
      "Iteration 14762, loss = 4.47093151\n",
      "Iteration 14763, loss = 3.92285750\n",
      "Iteration 14764, loss = 4.56087913\n",
      "Iteration 14765, loss = 4.41400048\n",
      "Iteration 14766, loss = 3.96483201\n",
      "Iteration 14767, loss = 4.00939448\n",
      "Iteration 14768, loss = 4.11624720\n",
      "Iteration 14769, loss = 4.04491137\n",
      "Iteration 14770, loss = 4.17431688\n",
      "Iteration 14771, loss = 4.96921659\n",
      "Iteration 14772, loss = 5.95162889\n",
      "Iteration 14773, loss = 4.21576987\n",
      "Iteration 14774, loss = 3.75790081\n",
      "Iteration 14775, loss = 4.15559222\n",
      "Iteration 14776, loss = 4.01204762\n",
      "Iteration 14777, loss = 4.48193006\n",
      "Iteration 14778, loss = 4.71138753\n",
      "Iteration 14779, loss = 4.18754809\n",
      "Iteration 14780, loss = 3.94775480\n",
      "Iteration 14781, loss = 3.93021689\n",
      "Iteration 14782, loss = 3.79122849\n",
      "Iteration 14783, loss = 3.80110587\n",
      "Iteration 14784, loss = 3.86921770\n",
      "Iteration 14785, loss = 3.84694966\n",
      "Iteration 14786, loss = 4.08747630\n",
      "Iteration 14787, loss = 3.81798527\n",
      "Iteration 14788, loss = 3.83043979\n",
      "Iteration 14789, loss = 3.96546408\n",
      "Iteration 14790, loss = 4.52062383\n",
      "Iteration 14791, loss = 4.12035752\n",
      "Iteration 14792, loss = 4.41069020\n",
      "Iteration 14793, loss = 4.87818854\n",
      "Iteration 14794, loss = 4.25965219\n",
      "Iteration 14795, loss = 4.35431739\n",
      "Iteration 14796, loss = 4.31169069\n",
      "Iteration 14797, loss = 4.06047323\n",
      "Iteration 14798, loss = 4.39204918\n",
      "Iteration 14799, loss = 4.09590111\n",
      "Iteration 14800, loss = 3.95809848\n",
      "Iteration 14801, loss = 3.97157204\n",
      "Iteration 14802, loss = 3.95292162\n",
      "Iteration 14803, loss = 4.11161384\n",
      "Iteration 14804, loss = 4.20927975\n",
      "Iteration 14805, loss = 4.59101192\n",
      "Iteration 14806, loss = 4.24159208\n",
      "Iteration 14807, loss = 4.44241035\n",
      "Iteration 14808, loss = 4.32669805\n",
      "Iteration 14809, loss = 3.74670941\n",
      "Iteration 14810, loss = 4.30969905\n",
      "Iteration 14811, loss = 4.10081950\n",
      "Iteration 14812, loss = 3.85498836\n",
      "Iteration 14813, loss = 4.12725161\n",
      "Iteration 14814, loss = 4.22922845\n",
      "Iteration 14815, loss = 4.01402916\n",
      "Iteration 14816, loss = 4.22610103\n",
      "Iteration 14817, loss = 4.21973679\n",
      "Iteration 14818, loss = 4.39626337\n",
      "Iteration 14819, loss = 4.00462059\n",
      "Iteration 14820, loss = 4.13409839\n",
      "Iteration 14821, loss = 3.94662739\n",
      "Iteration 14822, loss = 3.93484865\n",
      "Iteration 14823, loss = 3.87146995\n",
      "Iteration 14824, loss = 3.82820957\n",
      "Iteration 14825, loss = 4.56113481\n",
      "Iteration 14826, loss = 4.23778333\n",
      "Iteration 14827, loss = 4.22223611\n",
      "Iteration 14828, loss = 3.88736933\n",
      "Iteration 14829, loss = 3.85909661\n",
      "Iteration 14830, loss = 3.90861757\n",
      "Iteration 14831, loss = 3.80603601\n",
      "Iteration 14832, loss = 3.86687943\n",
      "Iteration 14833, loss = 4.04065883\n",
      "Iteration 14834, loss = 3.82518533\n",
      "Iteration 14835, loss = 4.00519392\n",
      "Iteration 14836, loss = 4.24748970\n",
      "Iteration 14837, loss = 5.16774120\n",
      "Iteration 14838, loss = 5.84043612\n",
      "Iteration 14839, loss = 6.89605944\n",
      "Iteration 14840, loss = 5.36214069\n",
      "Iteration 14841, loss = 3.93956171\n",
      "Iteration 14842, loss = 4.34977533\n",
      "Iteration 14843, loss = 4.19588118\n",
      "Iteration 14844, loss = 5.02180386\n",
      "Iteration 14845, loss = 5.72228432\n",
      "Iteration 14846, loss = 7.67521798\n",
      "Iteration 14847, loss = 5.03042151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14848, loss = 4.32418124\n",
      "Iteration 14849, loss = 4.34368291\n",
      "Iteration 14850, loss = 3.71017964\n",
      "Iteration 14851, loss = 3.91387317\n",
      "Iteration 14852, loss = 4.59653339\n",
      "Iteration 14853, loss = 4.18972905\n",
      "Iteration 14854, loss = 3.80593369\n",
      "Iteration 14855, loss = 4.29420948\n",
      "Iteration 14856, loss = 4.06321729\n",
      "Iteration 14857, loss = 3.93767555\n",
      "Iteration 14858, loss = 3.79873052\n",
      "Iteration 14859, loss = 4.00619859\n",
      "Iteration 14860, loss = 4.08306288\n",
      "Iteration 14861, loss = 3.92948740\n",
      "Iteration 14862, loss = 3.95584480\n",
      "Iteration 14863, loss = 3.81733728\n",
      "Iteration 14864, loss = 3.85155088\n",
      "Iteration 14865, loss = 3.72371964\n",
      "Iteration 14866, loss = 3.83178501\n",
      "Iteration 14867, loss = 3.93862713\n",
      "Iteration 14868, loss = 3.86673381\n",
      "Iteration 14869, loss = 4.11920300\n",
      "Iteration 14870, loss = 3.97862866\n",
      "Iteration 14871, loss = 3.88419571\n",
      "Iteration 14872, loss = 3.81196189\n",
      "Iteration 14873, loss = 3.79974942\n",
      "Iteration 14874, loss = 3.82296244\n",
      "Iteration 14875, loss = 4.05374919\n",
      "Iteration 14876, loss = 4.38918869\n",
      "Iteration 14877, loss = 5.19036886\n",
      "Iteration 14878, loss = 5.13097317\n",
      "Iteration 14879, loss = 4.68634573\n",
      "Iteration 14880, loss = 5.59665168\n",
      "Iteration 14881, loss = 4.42704366\n",
      "Iteration 14882, loss = 4.47739224\n",
      "Iteration 14883, loss = 4.58045325\n",
      "Iteration 14884, loss = 4.65143499\n",
      "Iteration 14885, loss = 5.65015541\n",
      "Iteration 14886, loss = 5.86614288\n",
      "Iteration 14887, loss = 4.89951422\n",
      "Iteration 14888, loss = 4.09264425\n",
      "Iteration 14889, loss = 4.04053862\n",
      "Iteration 14890, loss = 4.78774956\n",
      "Iteration 14891, loss = 4.20932044\n",
      "Iteration 14892, loss = 4.68720856\n",
      "Iteration 14893, loss = 4.13656419\n",
      "Iteration 14894, loss = 4.18749956\n",
      "Iteration 14895, loss = 3.69324411\n",
      "Iteration 14896, loss = 5.10675350\n",
      "Iteration 14897, loss = 3.86651501\n",
      "Iteration 14898, loss = 3.95036281\n",
      "Iteration 14899, loss = 4.35230288\n",
      "Iteration 14900, loss = 4.38030650\n",
      "Iteration 14901, loss = 4.03185309\n",
      "Iteration 14902, loss = 4.16813871\n",
      "Iteration 14903, loss = 4.15838292\n",
      "Iteration 14904, loss = 4.18421750\n",
      "Iteration 14905, loss = 3.86071240\n",
      "Iteration 14906, loss = 3.78550139\n",
      "Iteration 14907, loss = 3.83074539\n",
      "Iteration 14908, loss = 3.81327141\n",
      "Iteration 14909, loss = 4.05221504\n",
      "Iteration 14910, loss = 3.82645727\n",
      "Iteration 14911, loss = 3.73405908\n",
      "Iteration 14912, loss = 3.90538048\n",
      "Iteration 14913, loss = 3.66934645\n",
      "Iteration 14914, loss = 4.08340050\n",
      "Iteration 14915, loss = 4.27458822\n",
      "Iteration 14916, loss = 3.97002199\n",
      "Iteration 14917, loss = 3.85126156\n",
      "Iteration 14918, loss = 4.41124638\n",
      "Iteration 14919, loss = 5.79145794\n",
      "Iteration 14920, loss = 4.91170338\n",
      "Iteration 14921, loss = 4.07987741\n",
      "Iteration 14922, loss = 4.36619070\n",
      "Iteration 14923, loss = 3.95170922\n",
      "Iteration 14924, loss = 4.00710011\n",
      "Iteration 14925, loss = 3.78025254\n",
      "Iteration 14926, loss = 3.90844290\n",
      "Iteration 14927, loss = 5.41490932\n",
      "Iteration 14928, loss = 5.59381480\n",
      "Iteration 14929, loss = 4.79206917\n",
      "Iteration 14930, loss = 5.12148335\n",
      "Iteration 14931, loss = 3.99171322\n",
      "Iteration 14932, loss = 3.84110719\n",
      "Iteration 14933, loss = 4.10030425\n",
      "Iteration 14934, loss = 3.77673429\n",
      "Iteration 14935, loss = 3.79567513\n",
      "Iteration 14936, loss = 3.77020458\n",
      "Iteration 14937, loss = 3.95531736\n",
      "Iteration 14938, loss = 3.76008300\n",
      "Iteration 14939, loss = 3.72556160\n",
      "Iteration 14940, loss = 3.85746517\n",
      "Iteration 14941, loss = 3.88212242\n",
      "Iteration 14942, loss = 3.76744140\n",
      "Iteration 14943, loss = 3.98162084\n",
      "Iteration 14944, loss = 4.56235633\n",
      "Iteration 14945, loss = 4.34467392\n",
      "Iteration 14946, loss = 4.91404544\n",
      "Iteration 14947, loss = 4.81070355\n",
      "Iteration 14948, loss = 4.25810496\n",
      "Iteration 14949, loss = 4.16606782\n",
      "Iteration 14950, loss = 4.77956180\n",
      "Iteration 14951, loss = 4.94866781\n",
      "Iteration 14952, loss = 6.16321572\n",
      "Iteration 14953, loss = 5.66765967\n",
      "Iteration 14954, loss = 4.38834201\n",
      "Iteration 14955, loss = 4.20098557\n",
      "Iteration 14956, loss = 3.59866686\n",
      "Iteration 14957, loss = 4.17199287\n",
      "Iteration 14958, loss = 3.99958971\n",
      "Iteration 14959, loss = 4.52395680\n",
      "Iteration 14960, loss = 4.10198200\n",
      "Iteration 14961, loss = 4.14904218\n",
      "Iteration 14962, loss = 4.15160754\n",
      "Iteration 14963, loss = 3.83495734\n",
      "Iteration 14964, loss = 3.74758424\n",
      "Iteration 14965, loss = 3.98613070\n",
      "Iteration 14966, loss = 3.96668866\n",
      "Iteration 14967, loss = 3.75916191\n",
      "Iteration 14968, loss = 3.77432877\n",
      "Iteration 14969, loss = 5.10506561\n",
      "Iteration 14970, loss = 5.45435714\n",
      "Iteration 14971, loss = 4.59597076\n",
      "Iteration 14972, loss = 4.16697166\n",
      "Iteration 14973, loss = 5.07381546\n",
      "Iteration 14974, loss = 3.90033517\n",
      "Iteration 14975, loss = 4.17986892\n",
      "Iteration 14976, loss = 3.69010204\n",
      "Iteration 14977, loss = 3.85159195\n",
      "Iteration 14978, loss = 3.82573797\n",
      "Iteration 14979, loss = 3.69844153\n",
      "Iteration 14980, loss = 4.04234898\n",
      "Iteration 14981, loss = 4.24158542\n",
      "Iteration 14982, loss = 4.52418644\n",
      "Iteration 14983, loss = 4.16210165\n",
      "Iteration 14984, loss = 3.66096541\n",
      "Iteration 14985, loss = 4.24858868\n",
      "Iteration 14986, loss = 3.77226249\n",
      "Iteration 14987, loss = 3.81236424\n",
      "Iteration 14988, loss = 3.84651052\n",
      "Iteration 14989, loss = 3.79949722\n",
      "Iteration 14990, loss = 3.91243923\n",
      "Iteration 14991, loss = 4.46902776\n",
      "Iteration 14992, loss = 4.09969972\n",
      "Iteration 14993, loss = 4.03194143\n",
      "Iteration 14994, loss = 4.08933442\n",
      "Iteration 14995, loss = 3.74554544\n",
      "Iteration 14996, loss = 3.82607024\n",
      "Iteration 14997, loss = 3.80221009\n",
      "Iteration 14998, loss = 3.69088817\n",
      "Iteration 14999, loss = 3.81691214\n",
      "Iteration 15000, loss = 3.92982340\n",
      "Iteration 15001, loss = 3.80481738\n",
      "Iteration 15002, loss = 3.74747810\n",
      "Iteration 15003, loss = 4.24144596\n",
      "Iteration 15004, loss = 4.06985356\n",
      "Iteration 15005, loss = 3.85287086\n",
      "Iteration 15006, loss = 3.69081019\n",
      "Iteration 15007, loss = 3.90865013\n",
      "Iteration 15008, loss = 4.09095398\n",
      "Iteration 15009, loss = 3.78048147\n",
      "Iteration 15010, loss = 3.95833675\n",
      "Iteration 15011, loss = 5.07630554\n",
      "Iteration 15012, loss = 6.03639151\n",
      "Iteration 15013, loss = 5.02624969\n",
      "Iteration 15014, loss = 4.02182386\n",
      "Iteration 15015, loss = 3.80196592\n",
      "Iteration 15016, loss = 3.90015595\n",
      "Iteration 15017, loss = 3.74998416\n",
      "Iteration 15018, loss = 3.99413140\n",
      "Iteration 15019, loss = 3.71256121\n",
      "Iteration 15020, loss = 4.25479259\n",
      "Iteration 15021, loss = 6.47241365\n",
      "Iteration 15022, loss = 4.85514788\n",
      "Iteration 15023, loss = 4.81248484\n",
      "Iteration 15024, loss = 5.91629353\n",
      "Iteration 15025, loss = 5.13130499\n",
      "Iteration 15026, loss = 7.08559230\n",
      "Iteration 15027, loss = 5.20229651\n",
      "Iteration 15028, loss = 6.11065306\n",
      "Iteration 15029, loss = 4.07461362\n",
      "Iteration 15030, loss = 4.54881496\n",
      "Iteration 15031, loss = 4.12554242\n",
      "Iteration 15032, loss = 3.97291526\n",
      "Iteration 15033, loss = 3.68539471\n",
      "Iteration 15034, loss = 3.75473029\n",
      "Iteration 15035, loss = 3.82616577\n",
      "Iteration 15036, loss = 3.94789439\n",
      "Iteration 15037, loss = 3.97191089\n",
      "Iteration 15038, loss = 3.74622820\n",
      "Iteration 15039, loss = 3.76999447\n",
      "Iteration 15040, loss = 3.81712905\n",
      "Iteration 15041, loss = 3.75553854\n",
      "Iteration 15042, loss = 3.68363798\n",
      "Iteration 15043, loss = 3.77161542\n",
      "Iteration 15044, loss = 3.84144020\n",
      "Iteration 15045, loss = 4.23321446\n",
      "Iteration 15046, loss = 3.77828497\n",
      "Iteration 15047, loss = 3.90560887\n",
      "Iteration 15048, loss = 3.99527425\n",
      "Iteration 15049, loss = 3.81784210\n",
      "Iteration 15050, loss = 3.70002716\n",
      "Iteration 15051, loss = 3.68795995\n",
      "Iteration 15052, loss = 3.96821152\n",
      "Iteration 15053, loss = 3.79772203\n",
      "Iteration 15054, loss = 3.79033343\n",
      "Iteration 15055, loss = 3.87649809\n",
      "Iteration 15056, loss = 4.13851974\n",
      "Iteration 15057, loss = 3.70732892\n",
      "Iteration 15058, loss = 3.73084174\n",
      "Iteration 15059, loss = 3.78448798\n",
      "Iteration 15060, loss = 3.89283292\n",
      "Iteration 15061, loss = 3.78225445\n",
      "Iteration 15062, loss = 4.19466383\n",
      "Iteration 15063, loss = 3.74302111\n",
      "Iteration 15064, loss = 3.72992345\n",
      "Iteration 15065, loss = 3.69048320\n",
      "Iteration 15066, loss = 3.72110013\n",
      "Iteration 15067, loss = 3.91313746\n",
      "Iteration 15068, loss = 3.74888433\n",
      "Iteration 15069, loss = 4.00843409\n",
      "Iteration 15070, loss = 4.00395967\n",
      "Iteration 15071, loss = 3.81452611\n",
      "Iteration 15072, loss = 3.71640666\n",
      "Iteration 15073, loss = 3.77113770\n",
      "Iteration 15074, loss = 3.67438567\n",
      "Iteration 15075, loss = 3.63839931\n",
      "Iteration 15076, loss = 3.66440699\n",
      "Iteration 15077, loss = 3.69674146\n",
      "Iteration 15078, loss = 3.74343939\n",
      "Iteration 15079, loss = 3.55877329\n",
      "Iteration 15080, loss = 3.79576816\n",
      "Iteration 15081, loss = 3.94159844\n",
      "Iteration 15082, loss = 3.64832594\n",
      "Iteration 15083, loss = 3.65153453\n",
      "Iteration 15084, loss = 3.77396986\n",
      "Iteration 15085, loss = 3.98008198\n",
      "Iteration 15086, loss = 4.06752178\n",
      "Iteration 15087, loss = 3.83496257\n",
      "Iteration 15088, loss = 4.02659965\n",
      "Iteration 15089, loss = 3.79912542\n",
      "Iteration 15090, loss = 4.42662613\n",
      "Iteration 15091, loss = 3.83522990\n",
      "Iteration 15092, loss = 3.72069237\n",
      "Iteration 15093, loss = 3.70383426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15094, loss = 4.01239411\n",
      "Iteration 15095, loss = 6.16560191\n",
      "Iteration 15096, loss = 4.16347488\n",
      "Iteration 15097, loss = 3.86328160\n",
      "Iteration 15098, loss = 3.69525479\n",
      "Iteration 15099, loss = 3.67123476\n",
      "Iteration 15100, loss = 6.32573640\n",
      "Iteration 15101, loss = 9.15194778\n",
      "Iteration 15102, loss = 5.60608764\n",
      "Iteration 15103, loss = 5.36735707\n",
      "Iteration 15104, loss = 3.92481458\n",
      "Iteration 15105, loss = 4.47814997\n",
      "Iteration 15106, loss = 3.87038201\n",
      "Iteration 15107, loss = 3.86141759\n",
      "Iteration 15108, loss = 3.87861080\n",
      "Iteration 15109, loss = 3.97653006\n",
      "Iteration 15110, loss = 3.98405307\n",
      "Iteration 15111, loss = 4.68356165\n",
      "Iteration 15112, loss = 6.14363536\n",
      "Iteration 15113, loss = 7.54807854\n",
      "Iteration 15114, loss = 6.03638553\n",
      "Iteration 15115, loss = 4.26871148\n",
      "Iteration 15116, loss = 5.15555843\n",
      "Iteration 15117, loss = 7.87595797\n",
      "Iteration 15118, loss = 8.66472767\n",
      "Iteration 15119, loss = 4.79277830\n",
      "Iteration 15120, loss = 5.71565532\n",
      "Iteration 15121, loss = 5.43220950\n",
      "Iteration 15122, loss = 5.71315907\n",
      "Iteration 15123, loss = 4.73521804\n",
      "Iteration 15124, loss = 5.03446664\n",
      "Iteration 15125, loss = 4.23095657\n",
      "Iteration 15126, loss = 4.08879044\n",
      "Iteration 15127, loss = 3.86832841\n",
      "Iteration 15128, loss = 3.99409774\n",
      "Iteration 15129, loss = 3.69990586\n",
      "Iteration 15130, loss = 3.69042274\n",
      "Iteration 15131, loss = 3.83719876\n",
      "Iteration 15132, loss = 3.96010767\n",
      "Iteration 15133, loss = 4.49091498\n",
      "Iteration 15134, loss = 3.65534133\n",
      "Iteration 15135, loss = 3.46593911\n",
      "Iteration 15136, loss = 4.29114475\n",
      "Iteration 15137, loss = 3.99266350\n",
      "Iteration 15138, loss = 4.27484508\n",
      "Iteration 15139, loss = 4.97423335\n",
      "Iteration 15140, loss = 4.23285829\n",
      "Iteration 15141, loss = 3.94399189\n",
      "Iteration 15142, loss = 3.71550854\n",
      "Iteration 15143, loss = 4.00825282\n",
      "Iteration 15144, loss = 3.96692226\n",
      "Iteration 15145, loss = 3.97053637\n",
      "Iteration 15146, loss = 4.82065673\n",
      "Iteration 15147, loss = 3.97890494\n",
      "Iteration 15148, loss = 3.74572424\n",
      "Iteration 15149, loss = 3.90371226\n",
      "Iteration 15150, loss = 3.85572792\n",
      "Iteration 15151, loss = 3.93050053\n",
      "Iteration 15152, loss = 3.73264092\n",
      "Iteration 15153, loss = 3.66521297\n",
      "Iteration 15154, loss = 3.80138642\n",
      "Iteration 15155, loss = 4.03088857\n",
      "Iteration 15156, loss = 6.17497023\n",
      "Iteration 15157, loss = 5.12012473\n",
      "Iteration 15158, loss = 4.15450092\n",
      "Iteration 15159, loss = 4.11963546\n",
      "Iteration 15160, loss = 4.72600203\n",
      "Iteration 15161, loss = 3.62277051\n",
      "Iteration 15162, loss = 4.27896966\n",
      "Iteration 15163, loss = 3.56356292\n",
      "Iteration 15164, loss = 3.73679664\n",
      "Iteration 15165, loss = 4.67150397\n",
      "Iteration 15166, loss = 3.95421622\n",
      "Iteration 15167, loss = 4.62302241\n",
      "Iteration 15168, loss = 3.87174462\n",
      "Iteration 15169, loss = 4.46927604\n",
      "Iteration 15170, loss = 3.83206637\n",
      "Iteration 15171, loss = 3.60838156\n",
      "Iteration 15172, loss = 3.61493864\n",
      "Iteration 15173, loss = 5.12069042\n",
      "Iteration 15174, loss = 4.75052361\n",
      "Iteration 15175, loss = 4.13042524\n",
      "Iteration 15176, loss = 3.63766786\n",
      "Iteration 15177, loss = 3.69458305\n",
      "Iteration 15178, loss = 3.82480821\n",
      "Iteration 15179, loss = 3.56282972\n",
      "Iteration 15180, loss = 4.11665179\n",
      "Iteration 15181, loss = 4.19656663\n",
      "Iteration 15182, loss = 4.00689727\n",
      "Iteration 15183, loss = 3.74138785\n",
      "Iteration 15184, loss = 3.77747873\n",
      "Iteration 15185, loss = 3.83535745\n",
      "Iteration 15186, loss = 3.62963264\n",
      "Iteration 15187, loss = 3.87634882\n",
      "Iteration 15188, loss = 3.82329155\n",
      "Iteration 15189, loss = 3.63287280\n",
      "Iteration 15190, loss = 3.71214228\n",
      "Iteration 15191, loss = 3.54950775\n",
      "Iteration 15192, loss = 3.82761565\n",
      "Iteration 15193, loss = 4.90672084\n",
      "Iteration 15194, loss = 4.62278395\n",
      "Iteration 15195, loss = 5.25034007\n",
      "Iteration 15196, loss = 3.88869074\n",
      "Iteration 15197, loss = 4.59615837\n",
      "Iteration 15198, loss = 4.53394531\n",
      "Iteration 15199, loss = 3.57336145\n",
      "Iteration 15200, loss = 4.10163228\n",
      "Iteration 15201, loss = 3.62710164\n",
      "Iteration 15202, loss = 3.80173448\n",
      "Iteration 15203, loss = 3.73713487\n",
      "Iteration 15204, loss = 3.95589547\n",
      "Iteration 15205, loss = 3.86047680\n",
      "Iteration 15206, loss = 3.67595709\n",
      "Iteration 15207, loss = 3.57854802\n",
      "Iteration 15208, loss = 3.78948983\n",
      "Iteration 15209, loss = 3.76188564\n",
      "Iteration 15210, loss = 3.94452652\n",
      "Iteration 15211, loss = 3.69175175\n",
      "Iteration 15212, loss = 4.01067432\n",
      "Iteration 15213, loss = 3.86612454\n",
      "Iteration 15214, loss = 4.15438627\n",
      "Iteration 15215, loss = 3.57156183\n",
      "Iteration 15216, loss = 3.77241850\n",
      "Iteration 15217, loss = 4.28405056\n",
      "Iteration 15218, loss = 4.67027363\n",
      "Iteration 15219, loss = 4.94648260\n",
      "Iteration 15220, loss = 6.60683741\n",
      "Iteration 15221, loss = 6.40516478\n",
      "Iteration 15222, loss = 4.95424063\n",
      "Iteration 15223, loss = 5.50738654\n",
      "Iteration 15224, loss = 5.90310728\n",
      "Iteration 15225, loss = 5.11817695\n",
      "Iteration 15226, loss = 4.15158118\n",
      "Iteration 15227, loss = 6.14588620\n",
      "Iteration 15228, loss = 5.73910620\n",
      "Iteration 15229, loss = 4.23581842\n",
      "Iteration 15230, loss = 3.80830306\n",
      "Iteration 15231, loss = 5.33013347\n",
      "Iteration 15232, loss = 4.65963482\n",
      "Iteration 15233, loss = 4.42028743\n",
      "Iteration 15234, loss = 3.92683452\n",
      "Iteration 15235, loss = 4.22925955\n",
      "Iteration 15236, loss = 3.59508029\n",
      "Iteration 15237, loss = 3.74796165\n",
      "Iteration 15238, loss = 3.66953506\n",
      "Iteration 15239, loss = 4.13473930\n",
      "Iteration 15240, loss = 3.64962479\n",
      "Iteration 15241, loss = 3.65089293\n",
      "Iteration 15242, loss = 3.64284303\n",
      "Iteration 15243, loss = 3.59752486\n",
      "Iteration 15244, loss = 3.72683687\n",
      "Iteration 15245, loss = 3.61511560\n",
      "Iteration 15246, loss = 4.78988474\n",
      "Iteration 15247, loss = 4.41926998\n",
      "Iteration 15248, loss = 5.06116400\n",
      "Iteration 15249, loss = 5.34910049\n",
      "Iteration 15250, loss = 4.31297456\n",
      "Iteration 15251, loss = 4.27217497\n",
      "Iteration 15252, loss = 3.73584286\n",
      "Iteration 15253, loss = 3.93473757\n",
      "Iteration 15254, loss = 4.44188088\n",
      "Iteration 15255, loss = 3.92314717\n",
      "Iteration 15256, loss = 4.09629833\n",
      "Iteration 15257, loss = 3.62342948\n",
      "Iteration 15258, loss = 3.61804296\n",
      "Iteration 15259, loss = 3.54867588\n",
      "Iteration 15260, loss = 4.62896722\n",
      "Iteration 15261, loss = 5.86113985\n",
      "Iteration 15262, loss = 4.44834148\n",
      "Iteration 15263, loss = 4.62914942\n",
      "Iteration 15264, loss = 5.08077150\n",
      "Iteration 15265, loss = 4.47935428\n",
      "Iteration 15266, loss = 5.12861552\n",
      "Iteration 15267, loss = 4.30120153\n",
      "Iteration 15268, loss = 4.77179453\n",
      "Iteration 15269, loss = 5.03746338\n",
      "Iteration 15270, loss = 3.83916526\n",
      "Iteration 15271, loss = 4.62702948\n",
      "Iteration 15272, loss = 4.48544985\n",
      "Iteration 15273, loss = 4.90582206\n",
      "Iteration 15274, loss = 3.89544343\n",
      "Iteration 15275, loss = 3.46464399\n",
      "Iteration 15276, loss = 3.77252434\n",
      "Iteration 15277, loss = 5.11448806\n",
      "Iteration 15278, loss = 5.13280978\n",
      "Iteration 15279, loss = 4.08154032\n",
      "Iteration 15280, loss = 3.81514659\n",
      "Iteration 15281, loss = 3.61656707\n",
      "Iteration 15282, loss = 4.69973918\n",
      "Iteration 15283, loss = 6.14138431\n",
      "Iteration 15284, loss = 3.94940789\n",
      "Iteration 15285, loss = 4.38062576\n",
      "Iteration 15286, loss = 4.80356064\n",
      "Iteration 15287, loss = 3.86945394\n",
      "Iteration 15288, loss = 3.72375807\n",
      "Iteration 15289, loss = 3.56393734\n",
      "Iteration 15290, loss = 3.88029051\n",
      "Iteration 15291, loss = 3.58049222\n",
      "Iteration 15292, loss = 3.68058473\n",
      "Iteration 15293, loss = 3.60719628\n",
      "Iteration 15294, loss = 3.83449289\n",
      "Iteration 15295, loss = 3.63456232\n",
      "Iteration 15296, loss = 3.46822709\n",
      "Iteration 15297, loss = 3.72583656\n",
      "Iteration 15298, loss = 3.49257275\n",
      "Iteration 15299, loss = 3.58078670\n",
      "Iteration 15300, loss = 3.57929329\n",
      "Iteration 15301, loss = 3.57218372\n",
      "Iteration 15302, loss = 3.66277624\n",
      "Iteration 15303, loss = 3.97750435\n",
      "Iteration 15304, loss = 3.78596717\n",
      "Iteration 15305, loss = 3.69352986\n",
      "Iteration 15306, loss = 3.63123151\n",
      "Iteration 15307, loss = 3.91519731\n",
      "Iteration 15308, loss = 4.26722476\n",
      "Iteration 15309, loss = 4.12094714\n",
      "Iteration 15310, loss = 5.12137549\n",
      "Iteration 15311, loss = 4.15773645\n",
      "Iteration 15312, loss = 4.22223327\n",
      "Iteration 15313, loss = 3.81552408\n",
      "Iteration 15314, loss = 3.48464800\n",
      "Iteration 15315, loss = 3.96871936\n",
      "Iteration 15316, loss = 3.78484402\n",
      "Iteration 15317, loss = 3.61474535\n",
      "Iteration 15318, loss = 3.60972408\n",
      "Iteration 15319, loss = 4.30319100\n",
      "Iteration 15320, loss = 4.48871079\n",
      "Iteration 15321, loss = 3.57845871\n",
      "Iteration 15322, loss = 3.63612485\n",
      "Iteration 15323, loss = 3.57123214\n",
      "Iteration 15324, loss = 3.56104210\n",
      "Iteration 15325, loss = 4.09243200\n",
      "Iteration 15326, loss = 3.87663000\n",
      "Iteration 15327, loss = 4.20468617\n",
      "Iteration 15328, loss = 7.79373508\n",
      "Iteration 15329, loss = 8.45275944\n",
      "Iteration 15330, loss = 4.58532374\n",
      "Iteration 15331, loss = 4.82732979\n",
      "Iteration 15332, loss = 3.64648370\n",
      "Iteration 15333, loss = 3.89496291\n",
      "Iteration 15334, loss = 4.17723018\n",
      "Iteration 15335, loss = 6.33194122\n",
      "Iteration 15336, loss = 5.07996034\n",
      "Iteration 15337, loss = 3.75323571\n",
      "Iteration 15338, loss = 3.64599349\n",
      "Iteration 15339, loss = 3.81779556\n",
      "Iteration 15340, loss = 3.91111580\n",
      "Iteration 15341, loss = 3.86853447\n",
      "Iteration 15342, loss = 3.63634168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15343, loss = 3.55105866\n",
      "Iteration 15344, loss = 3.75041434\n",
      "Iteration 15345, loss = 3.64028096\n",
      "Iteration 15346, loss = 5.74475186\n",
      "Iteration 15347, loss = 5.76149123\n",
      "Iteration 15348, loss = 5.31595030\n",
      "Iteration 15349, loss = 5.60394232\n",
      "Iteration 15350, loss = 4.89823264\n",
      "Iteration 15351, loss = 3.55023715\n",
      "Iteration 15352, loss = 4.32558864\n",
      "Iteration 15353, loss = 3.95670856\n",
      "Iteration 15354, loss = 4.17557709\n",
      "Iteration 15355, loss = 4.05070164\n",
      "Iteration 15356, loss = 3.75613817\n",
      "Iteration 15357, loss = 4.03106677\n",
      "Iteration 15358, loss = 3.93813367\n",
      "Iteration 15359, loss = 3.75764355\n",
      "Iteration 15360, loss = 3.55425900\n",
      "Iteration 15361, loss = 3.39980318\n",
      "Iteration 15362, loss = 3.93943347\n",
      "Iteration 15363, loss = 3.94173517\n",
      "Iteration 15364, loss = 3.41656407\n",
      "Iteration 15365, loss = 3.82377783\n",
      "Iteration 15366, loss = 4.51871795\n",
      "Iteration 15367, loss = 4.83495563\n",
      "Iteration 15368, loss = 3.55835977\n",
      "Iteration 15369, loss = 4.31246153\n",
      "Iteration 15370, loss = 4.08728841\n",
      "Iteration 15371, loss = 3.83943150\n",
      "Iteration 15372, loss = 3.57544226\n",
      "Iteration 15373, loss = 4.38459806\n",
      "Iteration 15374, loss = 4.48212595\n",
      "Iteration 15375, loss = 3.64253337\n",
      "Iteration 15376, loss = 3.69859230\n",
      "Iteration 15377, loss = 5.06280634\n",
      "Iteration 15378, loss = 4.71481087\n",
      "Iteration 15379, loss = 3.91217998\n",
      "Iteration 15380, loss = 4.17151119\n",
      "Iteration 15381, loss = 4.68731841\n",
      "Iteration 15382, loss = 3.88225298\n",
      "Iteration 15383, loss = 3.51234750\n",
      "Iteration 15384, loss = 3.83272128\n",
      "Iteration 15385, loss = 3.68620223\n",
      "Iteration 15386, loss = 4.10145942\n",
      "Iteration 15387, loss = 3.50516499\n",
      "Iteration 15388, loss = 3.95440762\n",
      "Iteration 15389, loss = 3.67914263\n",
      "Iteration 15390, loss = 3.49492951\n",
      "Iteration 15391, loss = 3.69106524\n",
      "Iteration 15392, loss = 3.48599827\n",
      "Iteration 15393, loss = 3.72738506\n",
      "Iteration 15394, loss = 4.30789704\n",
      "Iteration 15395, loss = 4.15764039\n",
      "Iteration 15396, loss = 4.97924240\n",
      "Iteration 15397, loss = 4.44509195\n",
      "Iteration 15398, loss = 4.00896306\n",
      "Iteration 15399, loss = 3.81447589\n",
      "Iteration 15400, loss = 4.01348780\n",
      "Iteration 15401, loss = 4.59174118\n",
      "Iteration 15402, loss = 3.41536675\n",
      "Iteration 15403, loss = 4.00033819\n",
      "Iteration 15404, loss = 3.97404143\n",
      "Iteration 15405, loss = 3.61574800\n",
      "Iteration 15406, loss = 3.53213033\n",
      "Iteration 15407, loss = 3.53497180\n",
      "Iteration 15408, loss = 3.56667925\n",
      "Iteration 15409, loss = 3.83412584\n",
      "Iteration 15410, loss = 3.39884263\n",
      "Iteration 15411, loss = 3.52764399\n",
      "Iteration 15412, loss = 3.42637255\n",
      "Iteration 15413, loss = 3.57001122\n",
      "Iteration 15414, loss = 3.46616498\n",
      "Iteration 15415, loss = 4.30790915\n",
      "Iteration 15416, loss = 3.51372962\n",
      "Iteration 15417, loss = 3.67411211\n",
      "Iteration 15418, loss = 4.29015133\n",
      "Iteration 15419, loss = 3.66863159\n",
      "Iteration 15420, loss = 3.90278770\n",
      "Iteration 15421, loss = 3.72722112\n",
      "Iteration 15422, loss = 3.67918218\n",
      "Iteration 15423, loss = 3.40663272\n",
      "Iteration 15424, loss = 3.57249158\n",
      "Iteration 15425, loss = 3.77098699\n",
      "Iteration 15426, loss = 3.84831180\n",
      "Iteration 15427, loss = 3.40478689\n",
      "Iteration 15428, loss = 3.43412815\n",
      "Iteration 15429, loss = 3.73979534\n",
      "Iteration 15430, loss = 3.40499373\n",
      "Iteration 15431, loss = 3.48376028\n",
      "Iteration 15432, loss = 4.92477379\n",
      "Iteration 15433, loss = 4.70031719\n",
      "Iteration 15434, loss = 4.18956528\n",
      "Iteration 15435, loss = 4.46985282\n",
      "Iteration 15436, loss = 4.34674799\n",
      "Iteration 15437, loss = 4.61207571\n",
      "Iteration 15438, loss = 7.87439937\n",
      "Iteration 15439, loss = 9.74742646\n",
      "Iteration 15440, loss = 9.31845860\n",
      "Iteration 15441, loss = 5.06643812\n",
      "Iteration 15442, loss = 4.69618674\n",
      "Iteration 15443, loss = 4.48142600\n",
      "Iteration 15444, loss = 4.09387710\n",
      "Iteration 15445, loss = 3.82294585\n",
      "Iteration 15446, loss = 5.34817038\n",
      "Iteration 15447, loss = 3.64127612\n",
      "Iteration 15448, loss = 3.48953418\n",
      "Iteration 15449, loss = 3.55351798\n",
      "Iteration 15450, loss = 3.52336486\n",
      "Iteration 15451, loss = 3.52150148\n",
      "Iteration 15452, loss = 3.60615173\n",
      "Iteration 15453, loss = 3.56103147\n",
      "Iteration 15454, loss = 3.67699384\n",
      "Iteration 15455, loss = 3.96806700\n",
      "Iteration 15456, loss = 3.82964463\n",
      "Iteration 15457, loss = 4.26398780\n",
      "Iteration 15458, loss = 4.11060614\n",
      "Iteration 15459, loss = 3.60747321\n",
      "Iteration 15460, loss = 3.36472119\n",
      "Iteration 15461, loss = 4.87407691\n",
      "Iteration 15462, loss = 5.33263923\n",
      "Iteration 15463, loss = 4.17136920\n",
      "Iteration 15464, loss = 4.11817931\n",
      "Iteration 15465, loss = 4.20230492\n",
      "Iteration 15466, loss = 3.99290736\n",
      "Iteration 15467, loss = 4.88826389\n",
      "Iteration 15468, loss = 7.08210641\n",
      "Iteration 15469, loss = 4.43102389\n",
      "Iteration 15470, loss = 4.44282987\n",
      "Iteration 15471, loss = 4.01818637\n",
      "Iteration 15472, loss = 3.94151354\n",
      "Iteration 15473, loss = 3.64665732\n",
      "Iteration 15474, loss = 3.37112371\n",
      "Iteration 15475, loss = 3.71056576\n",
      "Iteration 15476, loss = 3.99298090\n",
      "Iteration 15477, loss = 3.45029876\n",
      "Iteration 15478, loss = 3.52203801\n",
      "Iteration 15479, loss = 3.25362726\n",
      "Iteration 15480, loss = 3.66302146\n",
      "Iteration 15481, loss = 3.44334025\n",
      "Iteration 15482, loss = 3.49023137\n",
      "Iteration 15483, loss = 3.82516172\n",
      "Iteration 15484, loss = 3.55552124\n",
      "Iteration 15485, loss = 3.50990739\n",
      "Iteration 15486, loss = 3.41756162\n",
      "Iteration 15487, loss = 3.39678572\n",
      "Iteration 15488, loss = 3.38880646\n",
      "Iteration 15489, loss = 3.35833350\n",
      "Iteration 15490, loss = 3.37422406\n",
      "Iteration 15491, loss = 3.42854726\n",
      "Iteration 15492, loss = 3.68968503\n",
      "Iteration 15493, loss = 3.75220282\n",
      "Iteration 15494, loss = 3.37209561\n",
      "Iteration 15495, loss = 3.59031568\n",
      "Iteration 15496, loss = 3.45551899\n",
      "Iteration 15497, loss = 3.70393239\n",
      "Iteration 15498, loss = 3.80805713\n",
      "Iteration 15499, loss = 3.79944873\n",
      "Iteration 15500, loss = 3.44506254\n",
      "Iteration 15501, loss = 3.41477123\n",
      "Iteration 15502, loss = 3.42868778\n",
      "Iteration 15503, loss = 3.35989970\n",
      "Iteration 15504, loss = 3.59684216\n",
      "Iteration 15505, loss = 3.54887658\n",
      "Iteration 15506, loss = 3.48572641\n",
      "Iteration 15507, loss = 3.99603287\n",
      "Iteration 15508, loss = 3.92151375\n",
      "Iteration 15509, loss = 3.45526754\n",
      "Iteration 15510, loss = 3.63768142\n",
      "Iteration 15511, loss = 3.86603384\n",
      "Iteration 15512, loss = 3.70874037\n",
      "Iteration 15513, loss = 4.06389654\n",
      "Iteration 15514, loss = 3.52457883\n",
      "Iteration 15515, loss = 3.45525587\n",
      "Iteration 15516, loss = 3.33109572\n",
      "Iteration 15517, loss = 4.18284806\n",
      "Iteration 15518, loss = 4.06377764\n",
      "Iteration 15519, loss = 5.26228991\n",
      "Iteration 15520, loss = 4.76984073\n",
      "Iteration 15521, loss = 3.78811119\n",
      "Iteration 15522, loss = 3.35812363\n",
      "Iteration 15523, loss = 3.25721175\n",
      "Iteration 15524, loss = 3.77925338\n",
      "Iteration 15525, loss = 3.67433308\n",
      "Iteration 15526, loss = 3.25227827\n",
      "Iteration 15527, loss = 3.43637399\n",
      "Iteration 15528, loss = 3.89819151\n",
      "Iteration 15529, loss = 3.31406832\n",
      "Iteration 15530, loss = 3.37538207\n",
      "Iteration 15531, loss = 4.11033482\n",
      "Iteration 15532, loss = 4.08122076\n",
      "Iteration 15533, loss = 5.04452085\n",
      "Iteration 15534, loss = 3.61854132\n",
      "Iteration 15535, loss = 3.99542039\n",
      "Iteration 15536, loss = 3.85864140\n",
      "Iteration 15537, loss = 3.50626440\n",
      "Iteration 15538, loss = 3.67180054\n",
      "Iteration 15539, loss = 3.44983430\n",
      "Iteration 15540, loss = 3.39142179\n",
      "Iteration 15541, loss = 3.30857295\n",
      "Iteration 15542, loss = 3.43436166\n",
      "Iteration 15543, loss = 3.41745379\n",
      "Iteration 15544, loss = 3.43043995\n",
      "Iteration 15545, loss = 3.95567692\n",
      "Iteration 15546, loss = 3.52766040\n",
      "Iteration 15547, loss = 3.88223029\n",
      "Iteration 15548, loss = 3.52579051\n",
      "Iteration 15549, loss = 3.73167753\n",
      "Iteration 15550, loss = 3.41666629\n",
      "Iteration 15551, loss = 3.24957894\n",
      "Iteration 15552, loss = 3.56769079\n",
      "Iteration 15553, loss = 3.89685258\n",
      "Iteration 15554, loss = 3.63343911\n",
      "Iteration 15555, loss = 3.92126923\n",
      "Iteration 15556, loss = 3.44440750\n",
      "Iteration 15557, loss = 3.42656873\n",
      "Iteration 15558, loss = 4.20841483\n",
      "Iteration 15559, loss = 4.47480206\n",
      "Iteration 15560, loss = 3.56102915\n",
      "Iteration 15561, loss = 3.52965197\n",
      "Iteration 15562, loss = 3.31318835\n",
      "Iteration 15563, loss = 3.26174643\n",
      "Iteration 15564, loss = 3.63974299\n",
      "Iteration 15565, loss = 3.44115264\n",
      "Iteration 15566, loss = 3.99444697\n",
      "Iteration 15567, loss = 3.32246844\n",
      "Iteration 15568, loss = 3.28916924\n",
      "Iteration 15569, loss = 3.28400386\n",
      "Iteration 15570, loss = 3.29450813\n",
      "Iteration 15571, loss = 3.72486589\n",
      "Iteration 15572, loss = 4.61990684\n",
      "Iteration 15573, loss = 5.67998424\n",
      "Iteration 15574, loss = 3.67622867\n",
      "Iteration 15575, loss = 3.65804041\n",
      "Iteration 15576, loss = 3.33074354\n",
      "Iteration 15577, loss = 3.50481711\n",
      "Iteration 15578, loss = 4.40632184\n",
      "Iteration 15579, loss = 3.91240575\n",
      "Iteration 15580, loss = 4.84436103\n",
      "Iteration 15581, loss = 7.60191962\n",
      "Iteration 15582, loss = 8.09882020\n",
      "Iteration 15583, loss = 5.14018842\n",
      "Iteration 15584, loss = 4.28173333\n",
      "Iteration 15585, loss = 4.45043566\n",
      "Iteration 15586, loss = 4.67461732\n",
      "Iteration 15587, loss = 4.12379396\n",
      "Iteration 15588, loss = 3.81174814\n",
      "Iteration 15589, loss = 3.85027007\n",
      "Iteration 15590, loss = 3.29384480\n",
      "Iteration 15591, loss = 3.35752717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15592, loss = 3.28984332\n",
      "Iteration 15593, loss = 3.63855152\n",
      "Iteration 15594, loss = 3.74821128\n",
      "Iteration 15595, loss = 3.65101007\n",
      "Iteration 15596, loss = 3.44847052\n",
      "Iteration 15597, loss = 3.54001111\n",
      "Iteration 15598, loss = 3.42609602\n",
      "Iteration 15599, loss = 3.35980640\n",
      "Iteration 15600, loss = 3.27136010\n",
      "Iteration 15601, loss = 3.28046842\n",
      "Iteration 15602, loss = 3.23385326\n",
      "Iteration 15603, loss = 4.56090772\n",
      "Iteration 15604, loss = 8.07145003\n",
      "Iteration 15605, loss = 5.76089056\n",
      "Iteration 15606, loss = 4.29445896\n",
      "Iteration 15607, loss = 4.03884165\n",
      "Iteration 15608, loss = 3.97284584\n",
      "Iteration 15609, loss = 4.91801127\n",
      "Iteration 15610, loss = 3.90188101\n",
      "Iteration 15611, loss = 3.90160675\n",
      "Iteration 15612, loss = 3.47871319\n",
      "Iteration 15613, loss = 3.32746773\n",
      "Iteration 15614, loss = 3.34610083\n",
      "Iteration 15615, loss = 3.44950327\n",
      "Iteration 15616, loss = 4.26579294\n",
      "Iteration 15617, loss = 3.59768873\n",
      "Iteration 15618, loss = 3.25361461\n",
      "Iteration 15619, loss = 3.24013203\n",
      "Iteration 15620, loss = 3.32997753\n",
      "Iteration 15621, loss = 3.61606381\n",
      "Iteration 15622, loss = 3.20796450\n",
      "Iteration 15623, loss = 3.23453082\n",
      "Iteration 15624, loss = 3.29862037\n",
      "Iteration 15625, loss = 3.23794157\n",
      "Iteration 15626, loss = 3.50703449\n",
      "Iteration 15627, loss = 4.24902978\n",
      "Iteration 15628, loss = 3.77613355\n",
      "Iteration 15629, loss = 4.61336410\n",
      "Iteration 15630, loss = 5.25580992\n",
      "Iteration 15631, loss = 3.87370585\n",
      "Iteration 15632, loss = 3.48154330\n",
      "Iteration 15633, loss = 4.17000054\n",
      "Iteration 15634, loss = 4.23254889\n",
      "Iteration 15635, loss = 5.71986037\n",
      "Iteration 15636, loss = 3.41250981\n",
      "Iteration 15637, loss = 3.42778508\n",
      "Iteration 15638, loss = 3.36459310\n",
      "Iteration 15639, loss = 3.19913878\n",
      "Iteration 15640, loss = 3.38606542\n",
      "Iteration 15641, loss = 3.37728041\n",
      "Iteration 15642, loss = 3.51657472\n",
      "Iteration 15643, loss = 3.38925218\n",
      "Iteration 15644, loss = 3.62864640\n",
      "Iteration 15645, loss = 4.05282878\n",
      "Iteration 15646, loss = 3.96531505\n",
      "Iteration 15647, loss = 6.46534074\n",
      "Iteration 15648, loss = 6.09483218\n",
      "Iteration 15649, loss = 5.22313825\n",
      "Iteration 15650, loss = 3.69083538\n",
      "Iteration 15651, loss = 3.45764228\n",
      "Iteration 15652, loss = 4.11329531\n",
      "Iteration 15653, loss = 4.09434073\n",
      "Iteration 15654, loss = 5.74143426\n",
      "Iteration 15655, loss = 5.13172337\n",
      "Iteration 15656, loss = 4.41121804\n",
      "Iteration 15657, loss = 3.29838069\n",
      "Iteration 15658, loss = 3.09498137\n",
      "Iteration 15659, loss = 3.16877115\n",
      "Iteration 15660, loss = 3.24044766\n",
      "Iteration 15661, loss = 3.14668508\n",
      "Iteration 15662, loss = 3.14158816\n",
      "Iteration 15663, loss = 3.13214195\n",
      "Iteration 15664, loss = 3.19824848\n",
      "Iteration 15665, loss = 3.24090964\n",
      "Iteration 15666, loss = 3.13167537\n",
      "Iteration 15667, loss = 3.19262486\n",
      "Iteration 15668, loss = 3.19287189\n",
      "Iteration 15669, loss = 3.34954841\n",
      "Iteration 15670, loss = 4.27672548\n",
      "Iteration 15671, loss = 4.96239672\n",
      "Iteration 15672, loss = 3.62285341\n",
      "Iteration 15673, loss = 3.32530967\n",
      "Iteration 15674, loss = 3.21753929\n",
      "Iteration 15675, loss = 3.14075633\n",
      "Iteration 15676, loss = 3.27269559\n",
      "Iteration 15677, loss = 3.53037709\n",
      "Iteration 15678, loss = 4.88665840\n",
      "Iteration 15679, loss = 4.74309649\n",
      "Iteration 15680, loss = 3.83943010\n",
      "Iteration 15681, loss = 3.33994600\n",
      "Iteration 15682, loss = 3.20356765\n",
      "Iteration 15683, loss = 3.33518750\n",
      "Iteration 15684, loss = 3.12644355\n",
      "Iteration 15685, loss = 3.24092536\n",
      "Iteration 15686, loss = 3.16987906\n",
      "Iteration 15687, loss = 3.18084394\n",
      "Iteration 15688, loss = 3.23142259\n",
      "Iteration 15689, loss = 3.17273486\n",
      "Iteration 15690, loss = 3.50478451\n",
      "Iteration 15691, loss = 3.86954627\n",
      "Iteration 15692, loss = 3.36948527\n",
      "Iteration 15693, loss = 3.70261333\n",
      "Iteration 15694, loss = 4.43580988\n",
      "Iteration 15695, loss = 5.05434540\n",
      "Iteration 15696, loss = 3.44836057\n",
      "Iteration 15697, loss = 3.38287183\n",
      "Iteration 15698, loss = 3.54759527\n",
      "Iteration 15699, loss = 3.27902404\n",
      "Iteration 15700, loss = 3.55404618\n",
      "Iteration 15701, loss = 3.67845898\n",
      "Iteration 15702, loss = 3.75308414\n",
      "Iteration 15703, loss = 3.17897929\n",
      "Iteration 15704, loss = 3.21334769\n",
      "Iteration 15705, loss = 3.28580551\n",
      "Iteration 15706, loss = 3.46510880\n",
      "Iteration 15707, loss = 3.30787462\n",
      "Iteration 15708, loss = 3.52501359\n",
      "Iteration 15709, loss = 3.49330777\n",
      "Iteration 15710, loss = 3.13314763\n",
      "Iteration 15711, loss = 4.20116268\n",
      "Iteration 15712, loss = 4.82766951\n",
      "Iteration 15713, loss = 4.06515454\n",
      "Iteration 15714, loss = 3.44663217\n",
      "Iteration 15715, loss = 3.38825425\n",
      "Iteration 15716, loss = 3.21658918\n",
      "Iteration 15717, loss = 3.26716071\n",
      "Iteration 15718, loss = 3.20826729\n",
      "Iteration 15719, loss = 3.18767428\n",
      "Iteration 15720, loss = 3.22796722\n",
      "Iteration 15721, loss = 3.15863754\n",
      "Iteration 15722, loss = 4.20252714\n",
      "Iteration 15723, loss = 3.20776994\n",
      "Iteration 15724, loss = 3.21381779\n",
      "Iteration 15725, loss = 3.16592922\n",
      "Iteration 15726, loss = 3.21533486\n",
      "Iteration 15727, loss = 3.29787169\n",
      "Iteration 15728, loss = 3.24774646\n",
      "Iteration 15729, loss = 3.15309805\n",
      "Iteration 15730, loss = 3.01588954\n",
      "Iteration 15731, loss = 3.32141502\n",
      "Iteration 15732, loss = 3.08937145\n",
      "Iteration 15733, loss = 3.66288119\n",
      "Iteration 15734, loss = 3.97331364\n",
      "Iteration 15735, loss = 4.14403005\n",
      "Iteration 15736, loss = 3.39971105\n",
      "Iteration 15737, loss = 3.38168858\n",
      "Iteration 15738, loss = 3.10857007\n",
      "Iteration 15739, loss = 3.28590346\n",
      "Iteration 15740, loss = 3.80398112\n",
      "Iteration 15741, loss = 3.40531158\n",
      "Iteration 15742, loss = 3.43626116\n",
      "Iteration 15743, loss = 3.44220507\n",
      "Iteration 15744, loss = 3.45435135\n",
      "Iteration 15745, loss = 3.39762389\n",
      "Iteration 15746, loss = 3.55421683\n",
      "Iteration 15747, loss = 3.36829285\n",
      "Iteration 15748, loss = 3.38125665\n",
      "Iteration 15749, loss = 3.45686845\n",
      "Iteration 15750, loss = 3.24453261\n",
      "Iteration 15751, loss = 3.44653503\n",
      "Iteration 15752, loss = 3.35374331\n",
      "Iteration 15753, loss = 3.07777154\n",
      "Iteration 15754, loss = 3.42165127\n",
      "Iteration 15755, loss = 3.32165169\n",
      "Iteration 15756, loss = 3.06318502\n",
      "Iteration 15757, loss = 3.03678066\n",
      "Iteration 15758, loss = 3.37409889\n",
      "Iteration 15759, loss = 2.99564649\n",
      "Iteration 15760, loss = 3.66571637\n",
      "Iteration 15761, loss = 2.93445164\n",
      "Iteration 15762, loss = 3.41835981\n",
      "Iteration 15763, loss = 3.02346579\n",
      "Iteration 15764, loss = 3.20800002\n",
      "Iteration 15765, loss = 3.44965806\n",
      "Iteration 15766, loss = 3.39706003\n",
      "Iteration 15767, loss = 3.38330782\n",
      "Iteration 15768, loss = 3.59535503\n",
      "Iteration 15769, loss = 3.10946110\n",
      "Iteration 15770, loss = 3.46675775\n",
      "Iteration 15771, loss = 3.08544833\n",
      "Iteration 15772, loss = 3.06162680\n",
      "Iteration 15773, loss = 3.54739256\n",
      "Iteration 15774, loss = 3.25146482\n",
      "Iteration 15775, loss = 3.11322911\n",
      "Iteration 15776, loss = 3.72353898\n",
      "Iteration 15777, loss = 4.34861938\n",
      "Iteration 15778, loss = 4.13791740\n",
      "Iteration 15779, loss = 3.45235108\n",
      "Iteration 15780, loss = 3.05124506\n",
      "Iteration 15781, loss = 3.21333106\n",
      "Iteration 15782, loss = 3.41265687\n",
      "Iteration 15783, loss = 4.66460609\n",
      "Iteration 15784, loss = 3.30591539\n",
      "Iteration 15785, loss = 3.30400990\n",
      "Iteration 15786, loss = 3.36073639\n",
      "Iteration 15787, loss = 3.38580653\n",
      "Iteration 15788, loss = 3.07537192\n",
      "Iteration 15789, loss = 3.02312690\n",
      "Iteration 15790, loss = 3.27826433\n",
      "Iteration 15791, loss = 3.07093602\n",
      "Iteration 15792, loss = 3.40572147\n",
      "Iteration 15793, loss = 3.48300111\n",
      "Iteration 15794, loss = 3.01399183\n",
      "Iteration 15795, loss = 3.88782722\n",
      "Iteration 15796, loss = 3.36484867\n",
      "Iteration 15797, loss = 3.47703870\n",
      "Iteration 15798, loss = 3.08674196\n",
      "Iteration 15799, loss = 3.89714192\n",
      "Iteration 15800, loss = 3.62280770\n",
      "Iteration 15801, loss = 3.21009444\n",
      "Iteration 15802, loss = 2.98525500\n",
      "Iteration 15803, loss = 2.99427755\n",
      "Iteration 15804, loss = 3.19108648\n",
      "Iteration 15805, loss = 4.04364093\n",
      "Iteration 15806, loss = 3.05930132\n",
      "Iteration 15807, loss = 3.25050454\n",
      "Iteration 15808, loss = 3.51195784\n",
      "Iteration 15809, loss = 3.68923072\n",
      "Iteration 15810, loss = 3.73931272\n",
      "Iteration 15811, loss = 3.21473606\n",
      "Iteration 15812, loss = 3.30744382\n",
      "Iteration 15813, loss = 3.17926070\n",
      "Iteration 15814, loss = 3.11776195\n",
      "Iteration 15815, loss = 3.33837961\n",
      "Iteration 15816, loss = 4.00245065\n",
      "Iteration 15817, loss = 3.36289480\n",
      "Iteration 15818, loss = 3.26080475\n",
      "Iteration 15819, loss = 3.20511679\n",
      "Iteration 15820, loss = 3.11469408\n",
      "Iteration 15821, loss = 3.00560135\n",
      "Iteration 15822, loss = 3.01903716\n",
      "Iteration 15823, loss = 3.69448694\n",
      "Iteration 15824, loss = 3.51894936\n",
      "Iteration 15825, loss = 3.31928825\n",
      "Iteration 15826, loss = 3.01209559\n",
      "Iteration 15827, loss = 3.26470305\n",
      "Iteration 15828, loss = 3.00878385\n",
      "Iteration 15829, loss = 2.93208843\n",
      "Iteration 15830, loss = 3.09450449\n",
      "Iteration 15831, loss = 3.17011255\n",
      "Iteration 15832, loss = 3.19991126\n",
      "Iteration 15833, loss = 3.15792927\n",
      "Iteration 15834, loss = 6.53049118\n",
      "Iteration 15835, loss = 3.96523018\n",
      "Iteration 15836, loss = 3.61192045\n",
      "Iteration 15837, loss = 3.53144810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15838, loss = 3.83496126\n",
      "Iteration 15839, loss = 3.32731144\n",
      "Iteration 15840, loss = 3.89899218\n",
      "Iteration 15841, loss = 3.69757814\n",
      "Iteration 15842, loss = 2.95455538\n",
      "Iteration 15843, loss = 3.21022255\n",
      "Iteration 15844, loss = 2.97401616\n",
      "Iteration 15845, loss = 3.18390780\n",
      "Iteration 15846, loss = 3.10885509\n",
      "Iteration 15847, loss = 4.87985255\n",
      "Iteration 15848, loss = 3.76061127\n",
      "Iteration 15849, loss = 3.35584325\n",
      "Iteration 15850, loss = 3.83037055\n",
      "Iteration 15851, loss = 3.83499857\n",
      "Iteration 15852, loss = 5.60284124\n",
      "Iteration 15853, loss = 3.48309100\n",
      "Iteration 15854, loss = 4.05738070\n",
      "Iteration 15855, loss = 3.08433894\n",
      "Iteration 15856, loss = 3.25262473\n",
      "Iteration 15857, loss = 2.90814288\n",
      "Iteration 15858, loss = 2.89839175\n",
      "Iteration 15859, loss = 3.26245044\n",
      "Iteration 15860, loss = 3.39961530\n",
      "Iteration 15861, loss = 3.02059383\n",
      "Iteration 15862, loss = 2.99278504\n",
      "Iteration 15863, loss = 3.21881989\n",
      "Iteration 15864, loss = 2.85139099\n",
      "Iteration 15865, loss = 2.83954670\n",
      "Iteration 15866, loss = 3.37593638\n",
      "Iteration 15867, loss = 3.15050207\n",
      "Iteration 15868, loss = 3.21109115\n",
      "Iteration 15869, loss = 3.07716279\n",
      "Iteration 15870, loss = 2.99344908\n",
      "Iteration 15871, loss = 3.22891222\n",
      "Iteration 15872, loss = 3.60440559\n",
      "Iteration 15873, loss = 3.31269589\n",
      "Iteration 15874, loss = 3.56357601\n",
      "Iteration 15875, loss = 3.33457410\n",
      "Iteration 15876, loss = 4.39742027\n",
      "Iteration 15877, loss = 3.37388223\n",
      "Iteration 15878, loss = 3.11466746\n",
      "Iteration 15879, loss = 3.10857622\n",
      "Iteration 15880, loss = 3.05376204\n",
      "Iteration 15881, loss = 3.13823985\n",
      "Iteration 15882, loss = 2.87608353\n",
      "Iteration 15883, loss = 3.03316536\n",
      "Iteration 15884, loss = 4.82776819\n",
      "Iteration 15885, loss = 4.18163403\n",
      "Iteration 15886, loss = 3.53506181\n",
      "Iteration 15887, loss = 3.21106485\n",
      "Iteration 15888, loss = 3.14708069\n",
      "Iteration 15889, loss = 3.30073504\n",
      "Iteration 15890, loss = 4.99455697\n",
      "Iteration 15891, loss = 5.41234819\n",
      "Iteration 15892, loss = 11.42961730\n",
      "Iteration 15893, loss = 7.34736876\n",
      "Iteration 15894, loss = 5.48742955\n",
      "Iteration 15895, loss = 4.36294356\n",
      "Iteration 15896, loss = 3.16933561\n",
      "Iteration 15897, loss = 3.64054910\n",
      "Iteration 15898, loss = 3.29214619\n",
      "Iteration 15899, loss = 2.96750712\n",
      "Iteration 15900, loss = 3.02349107\n",
      "Iteration 15901, loss = 2.90200844\n",
      "Iteration 15902, loss = 2.99868588\n",
      "Iteration 15903, loss = 2.98541716\n",
      "Iteration 15904, loss = 3.54772158\n",
      "Iteration 15905, loss = 3.87464960\n",
      "Iteration 15906, loss = 3.02486161\n",
      "Iteration 15907, loss = 2.96122145\n",
      "Iteration 15908, loss = 3.06831383\n",
      "Iteration 15909, loss = 3.12277724\n",
      "Iteration 15910, loss = 3.51842920\n",
      "Iteration 15911, loss = 3.23457867\n",
      "Iteration 15912, loss = 3.76886385\n",
      "Iteration 15913, loss = 3.18711264\n",
      "Iteration 15914, loss = 3.92219520\n",
      "Iteration 15915, loss = 2.97125570\n",
      "Iteration 15916, loss = 3.04495579\n",
      "Iteration 15917, loss = 3.08982084\n",
      "Iteration 15918, loss = 3.94075295\n",
      "Iteration 15919, loss = 2.90633797\n",
      "Iteration 15920, loss = 3.00709705\n",
      "Iteration 15921, loss = 2.95503028\n",
      "Iteration 15922, loss = 2.92347476\n",
      "Iteration 15923, loss = 3.10959619\n",
      "Iteration 15924, loss = 2.83732807\n",
      "Iteration 15925, loss = 3.43108736\n",
      "Iteration 15926, loss = 3.47732388\n",
      "Iteration 15927, loss = 3.34756615\n",
      "Iteration 15928, loss = 2.96921189\n",
      "Iteration 15929, loss = 3.37878621\n",
      "Iteration 15930, loss = 2.98831754\n",
      "Iteration 15931, loss = 3.58951582\n",
      "Iteration 15932, loss = 3.07537133\n",
      "Iteration 15933, loss = 4.50356814\n",
      "Iteration 15934, loss = 3.42124509\n",
      "Iteration 15935, loss = 4.16858342\n",
      "Iteration 15936, loss = 4.27681922\n",
      "Iteration 15937, loss = 3.17010759\n",
      "Iteration 15938, loss = 3.02699217\n",
      "Iteration 15939, loss = 3.06017373\n",
      "Iteration 15940, loss = 4.72136827\n",
      "Iteration 15941, loss = 4.48383198\n",
      "Iteration 15942, loss = 4.34984684\n",
      "Iteration 15943, loss = 4.23090051\n",
      "Iteration 15944, loss = 3.24613566\n",
      "Iteration 15945, loss = 3.15351014\n",
      "Iteration 15946, loss = 2.86350113\n",
      "Iteration 15947, loss = 3.01845663\n",
      "Iteration 15948, loss = 3.64436153\n",
      "Iteration 15949, loss = 2.72138526\n",
      "Iteration 15950, loss = 4.90546215\n",
      "Iteration 15951, loss = 5.98910493\n",
      "Iteration 15952, loss = 9.50249518\n",
      "Iteration 15953, loss = 7.75297807\n",
      "Iteration 15954, loss = 7.57839402\n",
      "Iteration 15955, loss = 5.21531456\n",
      "Iteration 15956, loss = 5.77049702\n",
      "Iteration 15957, loss = 6.86266716\n",
      "Iteration 15958, loss = 8.88360624\n",
      "Iteration 15959, loss = 6.92990874\n",
      "Iteration 15960, loss = 6.26577477\n",
      "Iteration 15961, loss = 3.95125257\n",
      "Iteration 15962, loss = 4.04240360\n",
      "Iteration 15963, loss = 3.64669152\n",
      "Iteration 15964, loss = 3.49244107\n",
      "Iteration 15965, loss = 3.32175252\n",
      "Iteration 15966, loss = 2.94248408\n",
      "Iteration 15967, loss = 3.20869617\n",
      "Iteration 15968, loss = 3.20519535\n",
      "Iteration 15969, loss = 2.80847388\n",
      "Iteration 15970, loss = 3.07584444\n",
      "Iteration 15971, loss = 2.93543312\n",
      "Iteration 15972, loss = 5.23084649\n",
      "Iteration 15973, loss = 4.12307637\n",
      "Iteration 15974, loss = 3.05863308\n",
      "Iteration 15975, loss = 2.94596925\n",
      "Iteration 15976, loss = 3.20766566\n",
      "Iteration 15977, loss = 3.06570552\n",
      "Iteration 15978, loss = 2.80974268\n",
      "Iteration 15979, loss = 3.67182726\n",
      "Iteration 15980, loss = 3.97060945\n",
      "Iteration 15981, loss = 2.99722134\n",
      "Iteration 15982, loss = 3.36384598\n",
      "Iteration 15983, loss = 3.23531446\n",
      "Iteration 15984, loss = 3.31654851\n",
      "Iteration 15985, loss = 2.93144507\n",
      "Iteration 15986, loss = 3.10785099\n",
      "Iteration 15987, loss = 3.27589680\n",
      "Iteration 15988, loss = 3.52815381\n",
      "Iteration 15989, loss = 2.96727180\n",
      "Iteration 15990, loss = 2.87080169\n",
      "Iteration 15991, loss = 3.14527711\n",
      "Iteration 15992, loss = 3.06400995\n",
      "Iteration 15993, loss = 3.79452913\n",
      "Iteration 15994, loss = 2.95738208\n",
      "Iteration 15995, loss = 3.18859818\n",
      "Iteration 15996, loss = 3.92623575\n",
      "Iteration 15997, loss = 3.48865815\n",
      "Iteration 15998, loss = 3.17643863\n",
      "Iteration 15999, loss = 3.01340507\n",
      "Iteration 16000, loss = 2.99058139\n",
      "Iteration 16001, loss = 4.19785701\n",
      "Iteration 16002, loss = 3.71751681\n",
      "Iteration 16003, loss = 5.04917938\n",
      "Iteration 16004, loss = 3.47318843\n",
      "Iteration 16005, loss = 2.82333981\n",
      "Iteration 16006, loss = 3.43383227\n",
      "Iteration 16007, loss = 3.37072618\n",
      "Iteration 16008, loss = 3.36308043\n",
      "Iteration 16009, loss = 3.18745950\n",
      "Iteration 16010, loss = 4.00669573\n",
      "Iteration 16011, loss = 5.69175624\n",
      "Iteration 16012, loss = 6.02623278\n",
      "Iteration 16013, loss = 3.63600620\n",
      "Iteration 16014, loss = 4.29233174\n",
      "Iteration 16015, loss = 4.42155256\n",
      "Iteration 16016, loss = 3.04277804\n",
      "Iteration 16017, loss = 4.56285899\n",
      "Iteration 16018, loss = 3.10770066\n",
      "Iteration 16019, loss = 3.06591115\n",
      "Iteration 16020, loss = 4.96874655\n",
      "Iteration 16021, loss = 3.82547659\n",
      "Iteration 16022, loss = 2.86821619\n",
      "Iteration 16023, loss = 2.96590088\n",
      "Iteration 16024, loss = 2.72026988\n",
      "Iteration 16025, loss = 3.38641878\n",
      "Iteration 16026, loss = 3.11481657\n",
      "Iteration 16027, loss = 3.16435775\n",
      "Iteration 16028, loss = 2.89431654\n",
      "Iteration 16029, loss = 2.67822394\n",
      "Iteration 16030, loss = 2.98104570\n",
      "Iteration 16031, loss = 3.10801985\n",
      "Iteration 16032, loss = 4.81558408\n",
      "Iteration 16033, loss = 5.62739111\n",
      "Iteration 16034, loss = 6.15176924\n",
      "Iteration 16035, loss = 5.15548850\n",
      "Iteration 16036, loss = 3.16464494\n",
      "Iteration 16037, loss = 3.02467379\n",
      "Iteration 16038, loss = 3.39370524\n",
      "Iteration 16039, loss = 2.94126949\n",
      "Iteration 16040, loss = 3.38386749\n",
      "Iteration 16041, loss = 3.12802066\n",
      "Iteration 16042, loss = 2.70439038\n",
      "Iteration 16043, loss = 2.83548919\n",
      "Iteration 16044, loss = 2.96974835\n",
      "Iteration 16045, loss = 3.08622205\n",
      "Iteration 16046, loss = 3.21474087\n",
      "Iteration 16047, loss = 3.73681550\n",
      "Iteration 16048, loss = 4.65617750\n",
      "Iteration 16049, loss = 3.87315751\n",
      "Iteration 16050, loss = 3.41083481\n",
      "Iteration 16051, loss = 3.41115256\n",
      "Iteration 16052, loss = 3.38807553\n",
      "Iteration 16053, loss = 2.91892584\n",
      "Iteration 16054, loss = 2.83333386\n",
      "Iteration 16055, loss = 3.05226002\n",
      "Iteration 16056, loss = 6.34505796\n",
      "Iteration 16057, loss = 4.34755480\n",
      "Iteration 16058, loss = 3.94629512\n",
      "Iteration 16059, loss = 4.12146708\n",
      "Iteration 16060, loss = 3.81377791\n",
      "Iteration 16061, loss = 2.69905678\n",
      "Iteration 16062, loss = 3.17780214\n",
      "Iteration 16063, loss = 3.10204585\n",
      "Iteration 16064, loss = 3.07915238\n",
      "Iteration 16065, loss = 2.95066978\n",
      "Iteration 16066, loss = 3.34184366\n",
      "Iteration 16067, loss = 6.15276080\n",
      "Iteration 16068, loss = 7.54375717\n",
      "Iteration 16069, loss = 4.19047187\n",
      "Iteration 16070, loss = 3.01157793\n",
      "Iteration 16071, loss = 4.19690033\n",
      "Iteration 16072, loss = 3.60652955\n",
      "Iteration 16073, loss = 4.20362828\n",
      "Iteration 16074, loss = 2.95797442\n",
      "Iteration 16075, loss = 2.71660446\n",
      "Iteration 16076, loss = 2.77685267\n",
      "Iteration 16077, loss = 2.70339844\n",
      "Iteration 16078, loss = 2.93253149\n",
      "Iteration 16079, loss = 3.39793334\n",
      "Iteration 16080, loss = 4.81513399\n",
      "Iteration 16081, loss = 4.63244161\n",
      "Iteration 16082, loss = 2.96697434\n",
      "Iteration 16083, loss = 2.99377648\n",
      "Iteration 16084, loss = 3.33916322\n",
      "Iteration 16085, loss = 3.42391821\n",
      "Iteration 16086, loss = 2.88230883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16087, loss = 2.83352911\n",
      "Iteration 16088, loss = 3.22183987\n",
      "Iteration 16089, loss = 2.85277823\n",
      "Iteration 16090, loss = 3.82941350\n",
      "Iteration 16091, loss = 3.50193436\n",
      "Iteration 16092, loss = 2.98741768\n",
      "Iteration 16093, loss = 2.85623064\n",
      "Iteration 16094, loss = 2.78604371\n",
      "Iteration 16095, loss = 2.86032672\n",
      "Iteration 16096, loss = 3.76310368\n",
      "Iteration 16097, loss = 2.83323992\n",
      "Iteration 16098, loss = 2.60508232\n",
      "Iteration 16099, loss = 2.98428263\n",
      "Iteration 16100, loss = 2.88643935\n",
      "Iteration 16101, loss = 2.95512649\n",
      "Iteration 16102, loss = 2.85515115\n",
      "Iteration 16103, loss = 3.14421262\n",
      "Iteration 16104, loss = 2.82902689\n",
      "Iteration 16105, loss = 3.78685113\n",
      "Iteration 16106, loss = 2.97537316\n",
      "Iteration 16107, loss = 2.75205104\n",
      "Iteration 16108, loss = 2.89996597\n",
      "Iteration 16109, loss = 2.71245267\n",
      "Iteration 16110, loss = 4.64646905\n",
      "Iteration 16111, loss = 3.06044795\n",
      "Iteration 16112, loss = 3.94657703\n",
      "Iteration 16113, loss = 2.83825236\n",
      "Iteration 16114, loss = 4.00614062\n",
      "Iteration 16115, loss = 3.14584215\n",
      "Iteration 16116, loss = 3.10859100\n",
      "Iteration 16117, loss = 3.83752323\n",
      "Iteration 16118, loss = 5.31931772\n",
      "Iteration 16119, loss = 3.99002276\n",
      "Iteration 16120, loss = 3.01421093\n",
      "Iteration 16121, loss = 3.06885926\n",
      "Iteration 16122, loss = 3.39200500\n",
      "Iteration 16123, loss = 3.05778385\n",
      "Iteration 16124, loss = 3.30822271\n",
      "Iteration 16125, loss = 3.20038866\n",
      "Iteration 16126, loss = 4.32655561\n",
      "Iteration 16127, loss = 3.75952703\n",
      "Iteration 16128, loss = 2.85784411\n",
      "Iteration 16129, loss = 2.99925769\n",
      "Iteration 16130, loss = 2.73534708\n",
      "Iteration 16131, loss = 2.71682837\n",
      "Iteration 16132, loss = 4.90521496\n",
      "Iteration 16133, loss = 4.28096845\n",
      "Iteration 16134, loss = 4.25152646\n",
      "Iteration 16135, loss = 5.32572579\n",
      "Iteration 16136, loss = 3.80891834\n",
      "Iteration 16137, loss = 4.32620887\n",
      "Iteration 16138, loss = 2.99593090\n",
      "Iteration 16139, loss = 3.80492270\n",
      "Iteration 16140, loss = 3.94883628\n",
      "Iteration 16141, loss = 3.22727782\n",
      "Iteration 16142, loss = 3.42925519\n",
      "Iteration 16143, loss = 2.98223579\n",
      "Iteration 16144, loss = 3.72105892\n",
      "Iteration 16145, loss = 2.61093293\n",
      "Iteration 16146, loss = 3.19174196\n",
      "Iteration 16147, loss = 2.82057733\n",
      "Iteration 16148, loss = 2.99184792\n",
      "Iteration 16149, loss = 2.95033131\n",
      "Iteration 16150, loss = 3.18288161\n",
      "Iteration 16151, loss = 2.88283799\n",
      "Iteration 16152, loss = 2.67376112\n",
      "Iteration 16153, loss = 2.75173806\n",
      "Iteration 16154, loss = 2.92219099\n",
      "Iteration 16155, loss = 2.93009679\n",
      "Iteration 16156, loss = 3.14607877\n",
      "Iteration 16157, loss = 3.11129775\n",
      "Iteration 16158, loss = 2.80743870\n",
      "Iteration 16159, loss = 2.66392214\n",
      "Iteration 16160, loss = 2.71286115\n",
      "Iteration 16161, loss = 2.79619846\n",
      "Iteration 16162, loss = 2.55418234\n",
      "Iteration 16163, loss = 2.52674211\n",
      "Iteration 16164, loss = 2.56259514\n",
      "Iteration 16165, loss = 2.74659593\n",
      "Iteration 16166, loss = 2.89781738\n",
      "Iteration 16167, loss = 2.77322426\n",
      "Iteration 16168, loss = 2.59108783\n",
      "Iteration 16169, loss = 2.59941644\n",
      "Iteration 16170, loss = 2.61819911\n",
      "Iteration 16171, loss = 2.62000097\n",
      "Iteration 16172, loss = 2.64935161\n",
      "Iteration 16173, loss = 2.62451219\n",
      "Iteration 16174, loss = 2.63856835\n",
      "Iteration 16175, loss = 2.65404897\n",
      "Iteration 16176, loss = 2.94461093\n",
      "Iteration 16177, loss = 3.12220294\n",
      "Iteration 16178, loss = 3.23692359\n",
      "Iteration 16179, loss = 2.83894995\n",
      "Iteration 16180, loss = 2.76264089\n",
      "Iteration 16181, loss = 2.95162193\n",
      "Iteration 16182, loss = 3.08267126\n",
      "Iteration 16183, loss = 3.17935307\n",
      "Iteration 16184, loss = 3.03543787\n",
      "Iteration 16185, loss = 5.22957654\n",
      "Iteration 16186, loss = 4.33964839\n",
      "Iteration 16187, loss = 2.79127996\n",
      "Iteration 16188, loss = 3.31820868\n",
      "Iteration 16189, loss = 3.30383050\n",
      "Iteration 16190, loss = 3.11925987\n",
      "Iteration 16191, loss = 3.29055673\n",
      "Iteration 16192, loss = 3.32922555\n",
      "Iteration 16193, loss = 2.87325931\n",
      "Iteration 16194, loss = 2.72841900\n",
      "Iteration 16195, loss = 2.56796716\n",
      "Iteration 16196, loss = 2.79664431\n",
      "Iteration 16197, loss = 2.95695190\n",
      "Iteration 16198, loss = 2.95106854\n",
      "Iteration 16199, loss = 5.21352429\n",
      "Iteration 16200, loss = 5.37791157\n",
      "Iteration 16201, loss = 4.39630413\n",
      "Iteration 16202, loss = 3.29751166\n",
      "Iteration 16203, loss = 2.92221806\n",
      "Iteration 16204, loss = 3.61008146\n",
      "Iteration 16205, loss = 2.90780126\n",
      "Iteration 16206, loss = 3.25468865\n",
      "Iteration 16207, loss = 3.05931383\n",
      "Iteration 16208, loss = 3.17311840\n",
      "Iteration 16209, loss = 2.69135071\n",
      "Iteration 16210, loss = 2.82048523\n",
      "Iteration 16211, loss = 2.92004262\n",
      "Iteration 16212, loss = 3.52769190\n",
      "Iteration 16213, loss = 2.87261658\n",
      "Iteration 16214, loss = 3.23989833\n",
      "Iteration 16215, loss = 2.82588405\n",
      "Iteration 16216, loss = 2.62648581\n",
      "Iteration 16217, loss = 2.95730572\n",
      "Iteration 16218, loss = 2.72808542\n",
      "Iteration 16219, loss = 2.52524639\n",
      "Iteration 16220, loss = 2.54988798\n",
      "Iteration 16221, loss = 2.78891965\n",
      "Iteration 16222, loss = 2.68538374\n",
      "Iteration 16223, loss = 2.58287891\n",
      "Iteration 16224, loss = 2.60124720\n",
      "Iteration 16225, loss = 2.63114891\n",
      "Iteration 16226, loss = 2.94409233\n",
      "Iteration 16227, loss = 2.57341282\n",
      "Iteration 16228, loss = 2.76946563\n",
      "Iteration 16229, loss = 2.62943788\n",
      "Iteration 16230, loss = 2.53118372\n",
      "Iteration 16231, loss = 2.92350408\n",
      "Iteration 16232, loss = 3.47678355\n",
      "Iteration 16233, loss = 3.53058150\n",
      "Iteration 16234, loss = 3.79426598\n",
      "Iteration 16235, loss = 3.12185784\n",
      "Iteration 16236, loss = 3.79596645\n",
      "Iteration 16237, loss = 3.25806773\n",
      "Iteration 16238, loss = 4.24749878\n",
      "Iteration 16239, loss = 3.05165267\n",
      "Iteration 16240, loss = 2.92954672\n",
      "Iteration 16241, loss = 3.19571989\n",
      "Iteration 16242, loss = 3.05575508\n",
      "Iteration 16243, loss = 2.60236438\n",
      "Iteration 16244, loss = 2.54665727\n",
      "Iteration 16245, loss = 2.74343685\n",
      "Iteration 16246, loss = 3.01368668\n",
      "Iteration 16247, loss = 2.94244564\n",
      "Iteration 16248, loss = 2.76088750\n",
      "Iteration 16249, loss = 2.76701341\n",
      "Iteration 16250, loss = 3.10981586\n",
      "Iteration 16251, loss = 2.80329993\n",
      "Iteration 16252, loss = 2.99177583\n",
      "Iteration 16253, loss = 2.99640947\n",
      "Iteration 16254, loss = 3.06633699\n",
      "Iteration 16255, loss = 2.54811536\n",
      "Iteration 16256, loss = 2.71006036\n",
      "Iteration 16257, loss = 2.95422428\n",
      "Iteration 16258, loss = 3.97407252\n",
      "Iteration 16259, loss = 4.14397947\n",
      "Iteration 16260, loss = 3.29053788\n",
      "Iteration 16261, loss = 3.53974621\n",
      "Iteration 16262, loss = 2.63547879\n",
      "Iteration 16263, loss = 2.91703979\n",
      "Iteration 16264, loss = 2.66891723\n",
      "Iteration 16265, loss = 3.55073782\n",
      "Iteration 16266, loss = 2.84280397\n",
      "Iteration 16267, loss = 3.05202136\n",
      "Iteration 16268, loss = 3.64616126\n",
      "Iteration 16269, loss = 2.88928600\n",
      "Iteration 16270, loss = 2.89339030\n",
      "Iteration 16271, loss = 2.87316499\n",
      "Iteration 16272, loss = 3.88893198\n",
      "Iteration 16273, loss = 3.64069449\n",
      "Iteration 16274, loss = 3.49101234\n",
      "Iteration 16275, loss = 3.27166506\n",
      "Iteration 16276, loss = 2.80346362\n",
      "Iteration 16277, loss = 2.45821316\n",
      "Iteration 16278, loss = 2.62481622\n",
      "Iteration 16279, loss = 2.48551087\n",
      "Iteration 16280, loss = 2.55700631\n",
      "Iteration 16281, loss = 2.71738309\n",
      "Iteration 16282, loss = 3.57482488\n",
      "Iteration 16283, loss = 4.45302732\n",
      "Iteration 16284, loss = 4.63216055\n",
      "Iteration 16285, loss = 4.48436047\n",
      "Iteration 16286, loss = 3.59766173\n",
      "Iteration 16287, loss = 3.81799147\n",
      "Iteration 16288, loss = 3.35501216\n",
      "Iteration 16289, loss = 3.75532050\n",
      "Iteration 16290, loss = 3.53114104\n",
      "Iteration 16291, loss = 2.66441681\n",
      "Iteration 16292, loss = 2.59758469\n",
      "Iteration 16293, loss = 3.25842136\n",
      "Iteration 16294, loss = 3.23236910\n",
      "Iteration 16295, loss = 4.23799421\n",
      "Iteration 16296, loss = 3.42711310\n",
      "Iteration 16297, loss = 3.04708662\n",
      "Iteration 16298, loss = 2.50481633\n",
      "Iteration 16299, loss = 2.76901685\n",
      "Iteration 16300, loss = 4.91362422\n",
      "Iteration 16301, loss = 6.55384531\n",
      "Iteration 16302, loss = 4.01196486\n",
      "Iteration 16303, loss = 3.23956393\n",
      "Iteration 16304, loss = 2.60133035\n",
      "Iteration 16305, loss = 2.46524340\n",
      "Iteration 16306, loss = 2.83106603\n",
      "Iteration 16307, loss = 2.70690880\n",
      "Iteration 16308, loss = 2.97256305\n",
      "Iteration 16309, loss = 3.08088019\n",
      "Iteration 16310, loss = 2.74333512\n",
      "Iteration 16311, loss = 2.53650023\n",
      "Iteration 16312, loss = 2.61227413\n",
      "Iteration 16313, loss = 3.25946481\n",
      "Iteration 16314, loss = 2.83896396\n",
      "Iteration 16315, loss = 2.48282321\n",
      "Iteration 16316, loss = 4.03822643\n",
      "Iteration 16317, loss = 3.22127449\n",
      "Iteration 16318, loss = 4.16061011\n",
      "Iteration 16319, loss = 3.67679646\n",
      "Iteration 16320, loss = 3.74131074\n",
      "Iteration 16321, loss = 2.91481334\n",
      "Iteration 16322, loss = 2.91375119\n",
      "Iteration 16323, loss = 3.46302326\n",
      "Iteration 16324, loss = 4.06436372\n",
      "Iteration 16325, loss = 4.39706239\n",
      "Iteration 16326, loss = 3.21082436\n",
      "Iteration 16327, loss = 5.38266856\n",
      "Iteration 16328, loss = 5.40906756\n",
      "Iteration 16329, loss = 4.26549269\n",
      "Iteration 16330, loss = 3.05222357\n",
      "Iteration 16331, loss = 2.60193347\n",
      "Iteration 16332, loss = 2.45535607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16333, loss = 2.72313084\n",
      "Iteration 16334, loss = 2.60141830\n",
      "Iteration 16335, loss = 2.65048907\n",
      "Iteration 16336, loss = 2.46099310\n",
      "Iteration 16337, loss = 2.44896855\n",
      "Iteration 16338, loss = 2.51360241\n",
      "Iteration 16339, loss = 2.58805840\n",
      "Iteration 16340, loss = 2.58733037\n",
      "Iteration 16341, loss = 2.38734635\n",
      "Iteration 16342, loss = 2.45626709\n",
      "Iteration 16343, loss = 2.52554073\n",
      "Iteration 16344, loss = 2.56390818\n",
      "Iteration 16345, loss = 2.93502307\n",
      "Iteration 16346, loss = 3.21286623\n",
      "Iteration 16347, loss = 2.72993874\n",
      "Iteration 16348, loss = 3.39860801\n",
      "Iteration 16349, loss = 2.85697120\n",
      "Iteration 16350, loss = 2.72911171\n",
      "Iteration 16351, loss = 2.54107923\n",
      "Iteration 16352, loss = 2.55465826\n",
      "Iteration 16353, loss = 2.48269074\n",
      "Iteration 16354, loss = 2.37238521\n",
      "Iteration 16355, loss = 2.41970872\n",
      "Iteration 16356, loss = 2.62561909\n",
      "Iteration 16357, loss = 4.58405187\n",
      "Iteration 16358, loss = 5.97985816\n",
      "Iteration 16359, loss = 4.62380965\n",
      "Iteration 16360, loss = 3.86463490\n",
      "Iteration 16361, loss = 5.33034139\n",
      "Iteration 16362, loss = 6.37663528\n",
      "Iteration 16363, loss = 4.16971108\n",
      "Iteration 16364, loss = 2.95170165\n",
      "Iteration 16365, loss = 2.83287058\n",
      "Iteration 16366, loss = 2.97928556\n",
      "Iteration 16367, loss = 2.59637421\n",
      "Iteration 16368, loss = 2.66187409\n",
      "Iteration 16369, loss = 2.79169539\n",
      "Iteration 16370, loss = 2.53154113\n",
      "Iteration 16371, loss = 2.67112792\n",
      "Iteration 16372, loss = 4.35917572\n",
      "Iteration 16373, loss = 3.30611369\n",
      "Iteration 16374, loss = 2.55239683\n",
      "Iteration 16375, loss = 2.39755605\n",
      "Iteration 16376, loss = 2.46391095\n",
      "Iteration 16377, loss = 2.32634987\n",
      "Iteration 16378, loss = 2.49439161\n",
      "Iteration 16379, loss = 2.71503189\n",
      "Iteration 16380, loss = 3.12761515\n",
      "Iteration 16381, loss = 2.57236673\n",
      "Iteration 16382, loss = 2.61099987\n",
      "Iteration 16383, loss = 3.64157457\n",
      "Iteration 16384, loss = 5.98284748\n",
      "Iteration 16385, loss = 6.56182008\n",
      "Iteration 16386, loss = 4.48825586\n",
      "Iteration 16387, loss = 4.15006129\n",
      "Iteration 16388, loss = 2.95187771\n",
      "Iteration 16389, loss = 2.93103717\n",
      "Iteration 16390, loss = 3.49549408\n",
      "Iteration 16391, loss = 4.00544412\n",
      "Iteration 16392, loss = 2.44508081\n",
      "Iteration 16393, loss = 2.58182288\n",
      "Iteration 16394, loss = 2.63731721\n",
      "Iteration 16395, loss = 2.41181436\n",
      "Iteration 16396, loss = 2.72639488\n",
      "Iteration 16397, loss = 3.21964115\n",
      "Iteration 16398, loss = 2.83610367\n",
      "Iteration 16399, loss = 2.37441539\n",
      "Iteration 16400, loss = 2.60703591\n",
      "Iteration 16401, loss = 3.03407512\n",
      "Iteration 16402, loss = 2.85991759\n",
      "Iteration 16403, loss = 2.89189268\n",
      "Iteration 16404, loss = 3.49791287\n",
      "Iteration 16405, loss = 3.33419273\n",
      "Iteration 16406, loss = 3.35495865\n",
      "Iteration 16407, loss = 2.92198742\n",
      "Iteration 16408, loss = 3.25978772\n",
      "Iteration 16409, loss = 2.73574035\n",
      "Iteration 16410, loss = 3.03977323\n",
      "Iteration 16411, loss = 4.10134676\n",
      "Iteration 16412, loss = 2.84395361\n",
      "Iteration 16413, loss = 2.73948933\n",
      "Iteration 16414, loss = 2.56135062\n",
      "Iteration 16415, loss = 2.59107843\n",
      "Iteration 16416, loss = 2.71817538\n",
      "Iteration 16417, loss = 2.57763380\n",
      "Iteration 16418, loss = 2.50590048\n",
      "Iteration 16419, loss = 2.42937580\n",
      "Iteration 16420, loss = 2.37808989\n",
      "Iteration 16421, loss = 2.42488761\n",
      "Iteration 16422, loss = 2.69113411\n",
      "Iteration 16423, loss = 3.29706057\n",
      "Iteration 16424, loss = 4.02444057\n",
      "Iteration 16425, loss = 2.63771584\n",
      "Iteration 16426, loss = 3.02431457\n",
      "Iteration 16427, loss = 2.83286690\n",
      "Iteration 16428, loss = 2.42792381\n",
      "Iteration 16429, loss = 2.64297872\n",
      "Iteration 16430, loss = 3.94039546\n",
      "Iteration 16431, loss = 3.49488424\n",
      "Iteration 16432, loss = 2.62016212\n",
      "Iteration 16433, loss = 2.37615768\n",
      "Iteration 16434, loss = 2.32907012\n",
      "Iteration 16435, loss = 2.49562525\n",
      "Iteration 16436, loss = 2.45022642\n",
      "Iteration 16437, loss = 2.91339668\n",
      "Iteration 16438, loss = 2.62488963\n",
      "Iteration 16439, loss = 2.84141390\n",
      "Iteration 16440, loss = 3.44331605\n",
      "Iteration 16441, loss = 3.47031781\n",
      "Iteration 16442, loss = 2.61954903\n",
      "Iteration 16443, loss = 2.55190112\n",
      "Iteration 16444, loss = 2.59073796\n",
      "Iteration 16445, loss = 3.72625535\n",
      "Iteration 16446, loss = 4.47960692\n",
      "Iteration 16447, loss = 2.61653789\n",
      "Iteration 16448, loss = 2.33010502\n",
      "Iteration 16449, loss = 2.55778879\n",
      "Iteration 16450, loss = 2.89754078\n",
      "Iteration 16451, loss = 4.23335215\n",
      "Iteration 16452, loss = 3.57244023\n",
      "Iteration 16453, loss = 5.38936730\n",
      "Iteration 16454, loss = 4.43187746\n",
      "Iteration 16455, loss = 3.63437278\n",
      "Iteration 16456, loss = 2.47875153\n",
      "Iteration 16457, loss = 3.16149691\n",
      "Iteration 16458, loss = 3.27509924\n",
      "Iteration 16459, loss = 3.46040392\n",
      "Iteration 16460, loss = 3.27472531\n",
      "Iteration 16461, loss = 2.44788366\n",
      "Iteration 16462, loss = 2.76284901\n",
      "Iteration 16463, loss = 3.14056540\n",
      "Iteration 16464, loss = 2.52312398\n",
      "Iteration 16465, loss = 2.33797074\n",
      "Iteration 16466, loss = 2.53242958\n",
      "Iteration 16467, loss = 2.48517798\n",
      "Iteration 16468, loss = 3.26865688\n",
      "Iteration 16469, loss = 2.55234456\n",
      "Iteration 16470, loss = 2.68886551\n",
      "Iteration 16471, loss = 2.40499006\n",
      "Iteration 16472, loss = 2.67504633\n",
      "Iteration 16473, loss = 2.51748649\n",
      "Iteration 16474, loss = 2.45737216\n",
      "Iteration 16475, loss = 2.49607013\n",
      "Iteration 16476, loss = 2.41333404\n",
      "Iteration 16477, loss = 2.43352670\n",
      "Iteration 16478, loss = 2.40750092\n",
      "Iteration 16479, loss = 2.33709855\n",
      "Iteration 16480, loss = 2.28043418\n",
      "Iteration 16481, loss = 2.30243239\n",
      "Iteration 16482, loss = 2.77060582\n",
      "Iteration 16483, loss = 2.99111633\n",
      "Iteration 16484, loss = 3.86471997\n",
      "Iteration 16485, loss = 2.79057282\n",
      "Iteration 16486, loss = 2.67844164\n",
      "Iteration 16487, loss = 2.70624852\n",
      "Iteration 16488, loss = 3.00354162\n",
      "Iteration 16489, loss = 4.30807821\n",
      "Iteration 16490, loss = 6.21634369\n",
      "Iteration 16491, loss = 4.21845257\n",
      "Iteration 16492, loss = 2.99879270\n",
      "Iteration 16493, loss = 2.45205623\n",
      "Iteration 16494, loss = 2.44999653\n",
      "Iteration 16495, loss = 2.44314100\n",
      "Iteration 16496, loss = 2.69782908\n",
      "Iteration 16497, loss = 2.46534403\n",
      "Iteration 16498, loss = 2.40497429\n",
      "Iteration 16499, loss = 2.71660250\n",
      "Iteration 16500, loss = 2.56356786\n",
      "Iteration 16501, loss = 2.53313996\n",
      "Iteration 16502, loss = 2.61259416\n",
      "Iteration 16503, loss = 2.65544835\n",
      "Iteration 16504, loss = 2.44152256\n",
      "Iteration 16505, loss = 2.28436294\n",
      "Iteration 16506, loss = 2.30456709\n",
      "Iteration 16507, loss = 2.26204331\n",
      "Iteration 16508, loss = 2.65507251\n",
      "Iteration 16509, loss = 2.43461396\n",
      "Iteration 16510, loss = 2.30086925\n",
      "Iteration 16511, loss = 2.42042588\n",
      "Iteration 16512, loss = 2.37688004\n",
      "Iteration 16513, loss = 2.44118172\n",
      "Iteration 16514, loss = 2.68781950\n",
      "Iteration 16515, loss = 2.49463247\n",
      "Iteration 16516, loss = 2.52275007\n",
      "Iteration 16517, loss = 3.69598608\n",
      "Iteration 16518, loss = 2.98019200\n",
      "Iteration 16519, loss = 3.07199300\n",
      "Iteration 16520, loss = 3.15760871\n",
      "Iteration 16521, loss = 2.86249290\n",
      "Iteration 16522, loss = 3.54910943\n",
      "Iteration 16523, loss = 2.70183336\n",
      "Iteration 16524, loss = 2.49381246\n",
      "Iteration 16525, loss = 2.93479039\n",
      "Iteration 16526, loss = 3.91480227\n",
      "Iteration 16527, loss = 3.02400829\n",
      "Iteration 16528, loss = 2.95768292\n",
      "Iteration 16529, loss = 2.88900715\n",
      "Iteration 16530, loss = 4.16930422\n",
      "Iteration 16531, loss = 3.35157164\n",
      "Iteration 16532, loss = 3.85572998\n",
      "Iteration 16533, loss = 3.84669854\n",
      "Iteration 16534, loss = 4.17466824\n",
      "Iteration 16535, loss = 2.55898894\n",
      "Iteration 16536, loss = 2.28475716\n",
      "Iteration 16537, loss = 2.31480691\n",
      "Iteration 16538, loss = 2.99998057\n",
      "Iteration 16539, loss = 2.78692614\n",
      "Iteration 16540, loss = 3.11940130\n",
      "Iteration 16541, loss = 3.52270874\n",
      "Iteration 16542, loss = 2.50933257\n",
      "Iteration 16543, loss = 2.46446681\n",
      "Iteration 16544, loss = 2.42629479\n",
      "Iteration 16545, loss = 3.39952926\n",
      "Iteration 16546, loss = 3.19356732\n",
      "Iteration 16547, loss = 3.27133734\n",
      "Iteration 16548, loss = 3.54997284\n",
      "Iteration 16549, loss = 2.44328623\n",
      "Iteration 16550, loss = 2.40729490\n",
      "Iteration 16551, loss = 2.37795782\n",
      "Iteration 16552, loss = 2.70685333\n",
      "Iteration 16553, loss = 2.72795512\n",
      "Iteration 16554, loss = 2.59210392\n",
      "Iteration 16555, loss = 3.79059591\n",
      "Iteration 16556, loss = 4.63410682\n",
      "Iteration 16557, loss = 6.86824924\n",
      "Iteration 16558, loss = 3.77503032\n",
      "Iteration 16559, loss = 3.03400213\n",
      "Iteration 16560, loss = 3.02246790\n",
      "Iteration 16561, loss = 2.51604860\n",
      "Iteration 16562, loss = 2.43475044\n",
      "Iteration 16563, loss = 2.68557235\n",
      "Iteration 16564, loss = 2.76709793\n",
      "Iteration 16565, loss = 2.70528994\n",
      "Iteration 16566, loss = 2.75373217\n",
      "Iteration 16567, loss = 2.84784079\n",
      "Iteration 16568, loss = 2.62644068\n",
      "Iteration 16569, loss = 3.35950756\n",
      "Iteration 16570, loss = 2.66062662\n",
      "Iteration 16571, loss = 2.61243999\n",
      "Iteration 16572, loss = 2.53597099\n",
      "Iteration 16573, loss = 3.06239283\n",
      "Iteration 16574, loss = 3.10018536\n",
      "Iteration 16575, loss = 3.42961972\n",
      "Iteration 16576, loss = 2.47240754\n",
      "Iteration 16577, loss = 2.59379400\n",
      "Iteration 16578, loss = 2.30049756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16579, loss = 2.32546989\n",
      "Iteration 16580, loss = 2.21697266\n",
      "Iteration 16581, loss = 2.50243299\n",
      "Iteration 16582, loss = 3.01201277\n",
      "Iteration 16583, loss = 3.45481851\n",
      "Iteration 16584, loss = 2.75463019\n",
      "Iteration 16585, loss = 3.19540175\n",
      "Iteration 16586, loss = 2.75049562\n",
      "Iteration 16587, loss = 2.81256123\n",
      "Iteration 16588, loss = 2.65299421\n",
      "Iteration 16589, loss = 3.57974375\n",
      "Iteration 16590, loss = 3.32111747\n",
      "Iteration 16591, loss = 3.96517997\n",
      "Iteration 16592, loss = 3.12383983\n",
      "Iteration 16593, loss = 4.01704643\n",
      "Iteration 16594, loss = 7.59958353\n",
      "Iteration 16595, loss = 8.44164126\n",
      "Iteration 16596, loss = 3.80670631\n",
      "Iteration 16597, loss = 3.27631707\n",
      "Iteration 16598, loss = 3.54473538\n",
      "Iteration 16599, loss = 3.60280522\n",
      "Iteration 16600, loss = 6.24471746\n",
      "Iteration 16601, loss = 7.40908095\n",
      "Iteration 16602, loss = 6.66806960\n",
      "Iteration 16603, loss = 4.04067521\n",
      "Iteration 16604, loss = 4.75926547\n",
      "Iteration 16605, loss = 3.15022290\n",
      "Iteration 16606, loss = 2.85191930\n",
      "Iteration 16607, loss = 2.55055612\n",
      "Iteration 16608, loss = 2.98269183\n",
      "Iteration 16609, loss = 2.77477079\n",
      "Iteration 16610, loss = 2.57115429\n",
      "Iteration 16611, loss = 2.65309485\n",
      "Iteration 16612, loss = 2.67531835\n",
      "Iteration 16613, loss = 3.29786752\n",
      "Iteration 16614, loss = 2.42401615\n",
      "Iteration 16615, loss = 2.52678887\n",
      "Iteration 16616, loss = 2.93859423\n",
      "Iteration 16617, loss = 2.27635018\n",
      "Iteration 16618, loss = 2.33020345\n",
      "Iteration 16619, loss = 2.42470049\n",
      "Iteration 16620, loss = 2.61442485\n",
      "Iteration 16621, loss = 3.04909001\n",
      "Iteration 16622, loss = 2.62775235\n",
      "Iteration 16623, loss = 2.30903946\n",
      "Iteration 16624, loss = 2.49951493\n",
      "Iteration 16625, loss = 3.14158019\n",
      "Iteration 16626, loss = 3.12735642\n",
      "Iteration 16627, loss = 3.69194977\n",
      "Iteration 16628, loss = 2.46418810\n",
      "Iteration 16629, loss = 3.63869611\n",
      "Iteration 16630, loss = 4.96345025\n",
      "Iteration 16631, loss = 4.00015609\n",
      "Iteration 16632, loss = 5.09127446\n",
      "Iteration 16633, loss = 3.45974711\n",
      "Iteration 16634, loss = 4.06823685\n",
      "Iteration 16635, loss = 3.79916934\n",
      "Iteration 16636, loss = 3.57122523\n",
      "Iteration 16637, loss = 5.39671992\n",
      "Iteration 16638, loss = 10.51717211\n",
      "Iteration 16639, loss = 4.13768304\n",
      "Iteration 16640, loss = 4.21021933\n",
      "Iteration 16641, loss = 3.38438072\n",
      "Iteration 16642, loss = 3.34163506\n",
      "Iteration 16643, loss = 2.58912358\n",
      "Iteration 16644, loss = 2.48308270\n",
      "Iteration 16645, loss = 2.90873650\n",
      "Iteration 16646, loss = 2.42554674\n",
      "Iteration 16647, loss = 2.28414201\n",
      "Iteration 16648, loss = 3.03001220\n",
      "Iteration 16649, loss = 2.73784562\n",
      "Iteration 16650, loss = 2.33532058\n",
      "Iteration 16651, loss = 2.26814548\n",
      "Iteration 16652, loss = 2.42721831\n",
      "Iteration 16653, loss = 2.51129913\n",
      "Iteration 16654, loss = 2.30860718\n",
      "Iteration 16655, loss = 2.37817649\n",
      "Iteration 16656, loss = 2.28560158\n",
      "Iteration 16657, loss = 2.77185889\n",
      "Iteration 16658, loss = 2.34621431\n",
      "Iteration 16659, loss = 2.32695055\n",
      "Iteration 16660, loss = 2.18159105\n",
      "Iteration 16661, loss = 2.40486883\n",
      "Iteration 16662, loss = 2.52742083\n",
      "Iteration 16663, loss = 2.46796113\n",
      "Iteration 16664, loss = 2.27240250\n",
      "Iteration 16665, loss = 2.24646116\n",
      "Iteration 16666, loss = 2.30287077\n",
      "Iteration 16667, loss = 2.25831531\n",
      "Iteration 16668, loss = 2.46089219\n",
      "Iteration 16669, loss = 3.17265351\n",
      "Iteration 16670, loss = 7.18201479\n",
      "Iteration 16671, loss = 12.22094126\n",
      "Iteration 16672, loss = 10.45153844\n",
      "Iteration 16673, loss = 7.45215171\n",
      "Iteration 16674, loss = 5.24842253\n",
      "Iteration 16675, loss = 3.62903563\n",
      "Iteration 16676, loss = 3.54677845\n",
      "Iteration 16677, loss = 3.47071954\n",
      "Iteration 16678, loss = 2.99206558\n",
      "Iteration 16679, loss = 2.73176406\n",
      "Iteration 16680, loss = 2.75525763\n",
      "Iteration 16681, loss = 2.66343241\n",
      "Iteration 16682, loss = 4.35297834\n",
      "Iteration 16683, loss = 2.99827121\n",
      "Iteration 16684, loss = 2.80023681\n",
      "Iteration 16685, loss = 3.07828411\n",
      "Iteration 16686, loss = 4.22022656\n",
      "Iteration 16687, loss = 2.90347457\n",
      "Iteration 16688, loss = 2.79916407\n",
      "Iteration 16689, loss = 3.19569307\n",
      "Iteration 16690, loss = 2.30986654\n",
      "Iteration 16691, loss = 2.43911424\n",
      "Iteration 16692, loss = 2.24051022\n",
      "Iteration 16693, loss = 2.50594904\n",
      "Iteration 16694, loss = 2.18644535\n",
      "Iteration 16695, loss = 2.23432969\n",
      "Iteration 16696, loss = 3.08931360\n",
      "Iteration 16697, loss = 2.06587001\n",
      "Iteration 16698, loss = 2.51106944\n",
      "Iteration 16699, loss = 2.25740942\n",
      "Iteration 16700, loss = 2.81511519\n",
      "Iteration 16701, loss = 4.93453093\n",
      "Iteration 16702, loss = 7.59152523\n",
      "Iteration 16703, loss = 4.11094734\n",
      "Iteration 16704, loss = 2.64206063\n",
      "Iteration 16705, loss = 3.13212213\n",
      "Iteration 16706, loss = 2.31788066\n",
      "Iteration 16707, loss = 2.81210132\n",
      "Iteration 16708, loss = 2.74343435\n",
      "Iteration 16709, loss = 2.79915158\n",
      "Iteration 16710, loss = 2.64801585\n",
      "Iteration 16711, loss = 2.58806624\n",
      "Iteration 16712, loss = 2.34410193\n",
      "Iteration 16713, loss = 2.29526897\n",
      "Iteration 16714, loss = 2.14007174\n",
      "Iteration 16715, loss = 2.23023207\n",
      "Iteration 16716, loss = 2.29219795\n",
      "Iteration 16717, loss = 2.42375691\n",
      "Iteration 16718, loss = 2.51129081\n",
      "Iteration 16719, loss = 2.25510331\n",
      "Iteration 16720, loss = 2.42994353\n",
      "Iteration 16721, loss = 2.76186101\n",
      "Iteration 16722, loss = 3.05224637\n",
      "Iteration 16723, loss = 2.53338662\n",
      "Iteration 16724, loss = 2.84601050\n",
      "Iteration 16725, loss = 2.31524198\n",
      "Iteration 16726, loss = 3.25331294\n",
      "Iteration 16727, loss = 2.79203132\n",
      "Iteration 16728, loss = 2.51449967\n",
      "Iteration 16729, loss = 2.26898594\n",
      "Iteration 16730, loss = 2.17641609\n",
      "Iteration 16731, loss = 2.37800899\n",
      "Iteration 16732, loss = 2.34252686\n",
      "Iteration 16733, loss = 3.49771066\n",
      "Iteration 16734, loss = 4.24425055\n",
      "Iteration 16735, loss = 3.52407958\n",
      "Iteration 16736, loss = 3.88901059\n",
      "Iteration 16737, loss = 3.91501922\n",
      "Iteration 16738, loss = 3.58246015\n",
      "Iteration 16739, loss = 3.42803507\n",
      "Iteration 16740, loss = 3.21979890\n",
      "Iteration 16741, loss = 3.27174425\n",
      "Iteration 16742, loss = 2.52388152\n",
      "Iteration 16743, loss = 2.82371198\n",
      "Iteration 16744, loss = 2.30097076\n",
      "Iteration 16745, loss = 2.60013552\n",
      "Iteration 16746, loss = 2.23565520\n",
      "Iteration 16747, loss = 2.35025410\n",
      "Iteration 16748, loss = 2.41300390\n",
      "Iteration 16749, loss = 2.39215333\n",
      "Iteration 16750, loss = 2.13999975\n",
      "Iteration 16751, loss = 2.79635837\n",
      "Iteration 16752, loss = 2.59936265\n",
      "Iteration 16753, loss = 2.73599084\n",
      "Iteration 16754, loss = 2.17272819\n",
      "Iteration 16755, loss = 2.17751979\n",
      "Iteration 16756, loss = 2.30552506\n",
      "Iteration 16757, loss = 3.29599696\n",
      "Iteration 16758, loss = 3.29582791\n",
      "Iteration 16759, loss = 2.45572586\n",
      "Iteration 16760, loss = 2.68515675\n",
      "Iteration 16761, loss = 2.14332772\n",
      "Iteration 16762, loss = 2.41171980\n",
      "Iteration 16763, loss = 2.29113553\n",
      "Iteration 16764, loss = 2.86760269\n",
      "Iteration 16765, loss = 2.11611132\n",
      "Iteration 16766, loss = 2.83165284\n",
      "Iteration 16767, loss = 2.48682623\n",
      "Iteration 16768, loss = 2.83124657\n",
      "Iteration 16769, loss = 4.14177784\n",
      "Iteration 16770, loss = 5.11241025\n",
      "Iteration 16771, loss = 5.97545603\n",
      "Iteration 16772, loss = 3.97764762\n",
      "Iteration 16773, loss = 3.74156367\n",
      "Iteration 16774, loss = 3.24037923\n",
      "Iteration 16775, loss = 2.30984400\n",
      "Iteration 16776, loss = 3.03890328\n",
      "Iteration 16777, loss = 3.02887113\n",
      "Iteration 16778, loss = 2.92579665\n",
      "Iteration 16779, loss = 2.74749714\n",
      "Iteration 16780, loss = 2.27976614\n",
      "Iteration 16781, loss = 3.34116845\n",
      "Iteration 16782, loss = 3.62599519\n",
      "Iteration 16783, loss = 3.40920302\n",
      "Iteration 16784, loss = 4.08137171\n",
      "Iteration 16785, loss = 2.63675284\n",
      "Iteration 16786, loss = 2.32084472\n",
      "Iteration 16787, loss = 2.29395314\n",
      "Iteration 16788, loss = 2.37719560\n",
      "Iteration 16789, loss = 2.24956805\n",
      "Iteration 16790, loss = 2.28094785\n",
      "Iteration 16791, loss = 2.06875146\n",
      "Iteration 16792, loss = 3.47917179\n",
      "Iteration 16793, loss = 4.59806459\n",
      "Iteration 16794, loss = 2.68174951\n",
      "Iteration 16795, loss = 2.74119318\n",
      "Iteration 16796, loss = 2.24709685\n",
      "Iteration 16797, loss = 2.23179194\n",
      "Iteration 16798, loss = 2.60888459\n",
      "Iteration 16799, loss = 2.51333609\n",
      "Iteration 16800, loss = 2.52252258\n",
      "Iteration 16801, loss = 2.41809488\n",
      "Iteration 16802, loss = 2.91953418\n",
      "Iteration 16803, loss = 3.95467020\n",
      "Iteration 16804, loss = 5.82440712\n",
      "Iteration 16805, loss = 7.08006630\n",
      "Iteration 16806, loss = 9.16575611\n",
      "Iteration 16807, loss = 6.60634003\n",
      "Iteration 16808, loss = 5.78649411\n",
      "Iteration 16809, loss = 5.38765253\n",
      "Iteration 16810, loss = 3.61705920\n",
      "Iteration 16811, loss = 3.60515049\n",
      "Iteration 16812, loss = 3.84321710\n",
      "Iteration 16813, loss = 3.95382888\n",
      "Iteration 16814, loss = 4.95950691\n",
      "Iteration 16815, loss = 6.18271815\n",
      "Iteration 16816, loss = 3.99891068\n",
      "Iteration 16817, loss = 2.83979734\n",
      "Iteration 16818, loss = 2.89230937\n",
      "Iteration 16819, loss = 2.72165940\n",
      "Iteration 16820, loss = 2.43845037\n",
      "Iteration 16821, loss = 2.34451533\n",
      "Iteration 16822, loss = 2.21998067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16823, loss = 2.71479056\n",
      "Iteration 16824, loss = 2.28551051\n",
      "Iteration 16825, loss = 2.16699821\n",
      "Iteration 16826, loss = 2.30113831\n",
      "Iteration 16827, loss = 2.38161371\n",
      "Iteration 16828, loss = 2.97044033\n",
      "Iteration 16829, loss = 3.30589411\n",
      "Iteration 16830, loss = 3.68778273\n",
      "Iteration 16831, loss = 4.09978138\n",
      "Iteration 16832, loss = 4.10750673\n",
      "Iteration 16833, loss = 3.45475188\n",
      "Iteration 16834, loss = 2.76276782\n",
      "Iteration 16835, loss = 3.27522763\n",
      "Iteration 16836, loss = 7.13051062\n",
      "Iteration 16837, loss = 3.96837300\n",
      "Iteration 16838, loss = 4.26669656\n",
      "Iteration 16839, loss = 7.62453097\n",
      "Iteration 16840, loss = 5.97588687\n",
      "Iteration 16841, loss = 4.02370021\n",
      "Iteration 16842, loss = 3.24713450\n",
      "Iteration 16843, loss = 3.40251737\n",
      "Iteration 16844, loss = 3.84559587\n",
      "Iteration 16845, loss = 2.85482200\n",
      "Iteration 16846, loss = 2.37423743\n",
      "Iteration 16847, loss = 2.18716980\n",
      "Iteration 16848, loss = 2.15414924\n",
      "Iteration 16849, loss = 2.23309179\n",
      "Iteration 16850, loss = 2.22212503\n",
      "Iteration 16851, loss = 2.15466030\n",
      "Iteration 16852, loss = 2.16301235\n",
      "Iteration 16853, loss = 2.22164539\n",
      "Iteration 16854, loss = 2.51245472\n",
      "Iteration 16855, loss = 2.20867354\n",
      "Iteration 16856, loss = 2.50274705\n",
      "Iteration 16857, loss = 2.38149325\n",
      "Iteration 16858, loss = 2.38688226\n",
      "Iteration 16859, loss = 3.01288395\n",
      "Iteration 16860, loss = 2.99225109\n",
      "Iteration 16861, loss = 2.47685385\n",
      "Iteration 16862, loss = 2.19473791\n",
      "Iteration 16863, loss = 2.35181500\n",
      "Iteration 16864, loss = 2.55845772\n",
      "Iteration 16865, loss = 2.12522230\n",
      "Iteration 16866, loss = 2.32763975\n",
      "Iteration 16867, loss = 2.33283583\n",
      "Iteration 16868, loss = 2.26441746\n",
      "Iteration 16869, loss = 2.16877746\n",
      "Iteration 16870, loss = 4.32495369\n",
      "Iteration 16871, loss = 4.40843793\n",
      "Iteration 16872, loss = 3.35967907\n",
      "Iteration 16873, loss = 2.29603374\n",
      "Iteration 16874, loss = 2.50620968\n",
      "Iteration 16875, loss = 2.47584069\n",
      "Iteration 16876, loss = 2.20647831\n",
      "Iteration 16877, loss = 2.58734041\n",
      "Iteration 16878, loss = 2.04909877\n",
      "Iteration 16879, loss = 2.72594218\n",
      "Iteration 16880, loss = 2.52569729\n",
      "Iteration 16881, loss = 3.93538283\n",
      "Iteration 16882, loss = 3.03939926\n",
      "Iteration 16883, loss = 2.30457479\n",
      "Iteration 16884, loss = 2.71420819\n",
      "Iteration 16885, loss = 2.58234282\n",
      "Iteration 16886, loss = 2.15804550\n",
      "Iteration 16887, loss = 2.16159134\n",
      "Iteration 16888, loss = 2.22652740\n",
      "Iteration 16889, loss = 2.98023426\n",
      "Iteration 16890, loss = 2.19681342\n",
      "Iteration 16891, loss = 2.03474508\n",
      "Iteration 16892, loss = 2.44349351\n",
      "Iteration 16893, loss = 2.02401051\n",
      "Iteration 16894, loss = 2.35516435\n",
      "Iteration 16895, loss = 2.31055214\n",
      "Iteration 16896, loss = 2.59651152\n",
      "Iteration 16897, loss = 2.09619701\n",
      "Iteration 16898, loss = 2.40959414\n",
      "Iteration 16899, loss = 3.49540646\n",
      "Iteration 16900, loss = 2.58786478\n",
      "Iteration 16901, loss = 2.15246117\n",
      "Iteration 16902, loss = 2.21529001\n",
      "Iteration 16903, loss = 2.19770012\n",
      "Iteration 16904, loss = 2.42798317\n",
      "Iteration 16905, loss = 2.55351426\n",
      "Iteration 16906, loss = 2.29377052\n",
      "Iteration 16907, loss = 2.44441661\n",
      "Iteration 16908, loss = 2.25105820\n",
      "Iteration 16909, loss = 2.14285549\n",
      "Iteration 16910, loss = 2.36503060\n",
      "Iteration 16911, loss = 2.70311052\n",
      "Iteration 16912, loss = 3.42635093\n",
      "Iteration 16913, loss = 2.34371414\n",
      "Iteration 16914, loss = 2.69254618\n",
      "Iteration 16915, loss = 2.18743944\n",
      "Iteration 16916, loss = 2.05129652\n",
      "Iteration 16917, loss = 2.11925346\n",
      "Iteration 16918, loss = 2.19582786\n",
      "Iteration 16919, loss = 2.41458412\n",
      "Iteration 16920, loss = 2.21001545\n",
      "Iteration 16921, loss = 2.10782145\n",
      "Iteration 16922, loss = 2.39392187\n",
      "Iteration 16923, loss = 2.18120371\n",
      "Iteration 16924, loss = 2.49738557\n",
      "Iteration 16925, loss = 2.43358338\n",
      "Iteration 16926, loss = 2.14704875\n",
      "Iteration 16927, loss = 2.19081440\n",
      "Iteration 16928, loss = 2.65076906\n",
      "Iteration 16929, loss = 2.67898524\n",
      "Iteration 16930, loss = 2.65342826\n",
      "Iteration 16931, loss = 2.91770120\n",
      "Iteration 16932, loss = 2.13944497\n",
      "Iteration 16933, loss = 2.37697196\n",
      "Iteration 16934, loss = 2.50699734\n",
      "Iteration 16935, loss = 2.61743013\n",
      "Iteration 16936, loss = 2.21190734\n",
      "Iteration 16937, loss = 2.08356435\n",
      "Iteration 16938, loss = 2.15811441\n",
      "Iteration 16939, loss = 2.22514590\n",
      "Iteration 16940, loss = 3.16367229\n",
      "Iteration 16941, loss = 5.53820599\n",
      "Iteration 16942, loss = 2.88537897\n",
      "Iteration 16943, loss = 2.58429796\n",
      "Iteration 16944, loss = 2.10819035\n",
      "Iteration 16945, loss = 2.36692422\n",
      "Iteration 16946, loss = 2.30891479\n",
      "Iteration 16947, loss = 2.14819064\n",
      "Iteration 16948, loss = 2.17868472\n",
      "Iteration 16949, loss = 2.18226255\n",
      "Iteration 16950, loss = 2.23635917\n",
      "Iteration 16951, loss = 2.11049726\n",
      "Iteration 16952, loss = 2.67453222\n",
      "Iteration 16953, loss = 2.11318892\n",
      "Iteration 16954, loss = 2.32805641\n",
      "Iteration 16955, loss = 2.73942561\n",
      "Iteration 16956, loss = 2.41671022\n",
      "Iteration 16957, loss = 2.32199537\n",
      "Iteration 16958, loss = 2.12041826\n",
      "Iteration 16959, loss = 2.17984730\n",
      "Iteration 16960, loss = 2.36964966\n",
      "Iteration 16961, loss = 2.19194969\n",
      "Iteration 16962, loss = 2.82367880\n",
      "Iteration 16963, loss = 2.16526765\n",
      "Iteration 16964, loss = 2.08462009\n",
      "Iteration 16965, loss = 2.34132218\n",
      "Iteration 16966, loss = 2.16739233\n",
      "Iteration 16967, loss = 2.47608914\n",
      "Iteration 16968, loss = 2.62490417\n",
      "Iteration 16969, loss = 2.02229216\n",
      "Iteration 16970, loss = 2.55810355\n",
      "Iteration 16971, loss = 2.22930701\n",
      "Iteration 16972, loss = 2.05014816\n",
      "Iteration 16973, loss = 2.06623619\n",
      "Iteration 16974, loss = 2.08565828\n",
      "Iteration 16975, loss = 2.31764587\n",
      "Iteration 16976, loss = 2.28403472\n",
      "Iteration 16977, loss = 2.34892285\n",
      "Iteration 16978, loss = 2.85530254\n",
      "Iteration 16979, loss = 2.25923321\n",
      "Iteration 16980, loss = 2.04317130\n",
      "Iteration 16981, loss = 2.05613558\n",
      "Iteration 16982, loss = 2.59873868\n",
      "Iteration 16983, loss = 2.29268820\n",
      "Iteration 16984, loss = 2.41856005\n",
      "Iteration 16985, loss = 3.36310361\n",
      "Iteration 16986, loss = 2.31982993\n",
      "Iteration 16987, loss = 2.49605100\n",
      "Iteration 16988, loss = 2.78046494\n",
      "Iteration 16989, loss = 2.63085348\n",
      "Iteration 16990, loss = 2.75187383\n",
      "Iteration 16991, loss = 2.73159562\n",
      "Iteration 16992, loss = 2.99505253\n",
      "Iteration 16993, loss = 2.25543594\n",
      "Iteration 16994, loss = 2.18918188\n",
      "Iteration 16995, loss = 2.14613313\n",
      "Iteration 16996, loss = 2.19337377\n",
      "Iteration 16997, loss = 2.10525710\n",
      "Iteration 16998, loss = 2.27662147\n",
      "Iteration 16999, loss = 2.46782907\n",
      "Iteration 17000, loss = 2.64629212\n",
      "Iteration 17001, loss = 2.57455416\n",
      "Iteration 17002, loss = 2.76754453\n",
      "Iteration 17003, loss = 2.35791341\n",
      "Iteration 17004, loss = 2.68831758\n",
      "Iteration 17005, loss = 2.52657626\n",
      "Iteration 17006, loss = 2.27492376\n",
      "Iteration 17007, loss = 2.26458681\n",
      "Iteration 17008, loss = 2.19800254\n",
      "Iteration 17009, loss = 2.57592380\n",
      "Iteration 17010, loss = 5.34404935\n",
      "Iteration 17011, loss = 8.08134221\n",
      "Iteration 17012, loss = 5.12403540\n",
      "Iteration 17013, loss = 3.31354550\n",
      "Iteration 17014, loss = 2.97766458\n",
      "Iteration 17015, loss = 2.67063024\n",
      "Iteration 17016, loss = 4.63300207\n",
      "Iteration 17017, loss = 13.65530497\n",
      "Iteration 17018, loss = 16.50946123\n",
      "Iteration 17019, loss = 16.71031747\n",
      "Iteration 17020, loss = 12.93731326\n",
      "Iteration 17021, loss = 12.54490600\n",
      "Iteration 17022, loss = 11.98413788\n",
      "Iteration 17023, loss = 10.90691367\n",
      "Iteration 17024, loss = 10.59161568\n",
      "Iteration 17025, loss = 10.06389514\n",
      "Iteration 17026, loss = 9.87721159\n",
      "Iteration 17027, loss = 9.69103934\n",
      "Iteration 17028, loss = 9.29172761\n",
      "Iteration 17029, loss = 8.84756062\n",
      "Iteration 17030, loss = 8.54622573\n",
      "Iteration 17031, loss = 8.51654148\n",
      "Iteration 17032, loss = 8.40484173\n",
      "Iteration 17033, loss = 7.81968884\n",
      "Iteration 17034, loss = 7.92038945\n",
      "Iteration 17035, loss = 7.46187168\n",
      "Iteration 17036, loss = 7.09488843\n",
      "Iteration 17037, loss = 6.91138105\n",
      "Iteration 17038, loss = 6.62684136\n",
      "Iteration 17039, loss = 6.56940260\n",
      "Iteration 17040, loss = 6.66897060\n",
      "Iteration 17041, loss = 6.16437048\n",
      "Iteration 17042, loss = 5.57684134\n",
      "Iteration 17043, loss = 5.16470586\n",
      "Iteration 17044, loss = 4.81857162\n",
      "Iteration 17045, loss = 4.74493624\n",
      "Iteration 17046, loss = 4.56042798\n",
      "Iteration 17047, loss = 4.37743604\n",
      "Iteration 17048, loss = 4.29500831\n",
      "Iteration 17049, loss = 4.34106676\n",
      "Iteration 17050, loss = 4.21950359\n",
      "Iteration 17051, loss = 4.33976641\n",
      "Iteration 17052, loss = 4.54800687\n",
      "Iteration 17053, loss = 4.17459589\n",
      "Iteration 17054, loss = 4.89830402\n",
      "Iteration 17055, loss = 4.25643201\n",
      "Iteration 17056, loss = 4.14785647\n",
      "Iteration 17057, loss = 4.34821446\n",
      "Iteration 17058, loss = 4.30667570\n",
      "Iteration 17059, loss = 4.16460899\n",
      "Iteration 17060, loss = 4.32527742\n",
      "Iteration 17061, loss = 4.15186795\n",
      "Iteration 17062, loss = 4.08023858\n",
      "Iteration 17063, loss = 4.46638072\n",
      "Iteration 17064, loss = 4.51286776\n",
      "Iteration 17065, loss = 4.62626973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17066, loss = 4.83544237\n",
      "Iteration 17067, loss = 4.40110135\n",
      "Iteration 17068, loss = 4.37476824\n",
      "Iteration 17069, loss = 4.23433385\n",
      "Iteration 17070, loss = 3.99008533\n",
      "Iteration 17071, loss = 4.08001434\n",
      "Iteration 17072, loss = 4.03769881\n",
      "Iteration 17073, loss = 4.00748118\n",
      "Iteration 17074, loss = 4.18990026\n",
      "Iteration 17075, loss = 3.97726090\n",
      "Iteration 17076, loss = 3.89511587\n",
      "Iteration 17077, loss = 4.01824413\n",
      "Iteration 17078, loss = 4.05691550\n",
      "Iteration 17079, loss = 3.84397184\n",
      "Iteration 17080, loss = 3.93970839\n",
      "Iteration 17081, loss = 3.73624295\n",
      "Iteration 17082, loss = 3.99103483\n",
      "Iteration 17083, loss = 3.87401359\n",
      "Iteration 17084, loss = 3.75792455\n",
      "Iteration 17085, loss = 3.68498198\n",
      "Iteration 17086, loss = 3.71103594\n",
      "Iteration 17087, loss = 3.77791055\n",
      "Iteration 17088, loss = 3.88442260\n",
      "Iteration 17089, loss = 3.54525888\n",
      "Iteration 17090, loss = 3.79436536\n",
      "Iteration 17091, loss = 3.62072574\n",
      "Iteration 17092, loss = 3.72400626\n",
      "Iteration 17093, loss = 3.91089603\n",
      "Iteration 17094, loss = 4.08570433\n",
      "Iteration 17095, loss = 3.62064578\n",
      "Iteration 17096, loss = 3.81638207\n",
      "Iteration 17097, loss = 3.66766563\n",
      "Iteration 17098, loss = 3.65546751\n",
      "Iteration 17099, loss = 3.65316905\n",
      "Iteration 17100, loss = 3.85960028\n",
      "Iteration 17101, loss = 3.54642226\n",
      "Iteration 17102, loss = 3.41513968\n",
      "Iteration 17103, loss = 4.30046134\n",
      "Iteration 17104, loss = 3.01880041\n",
      "Iteration 17105, loss = 2.60575796\n",
      "Iteration 17106, loss = 2.46738565\n",
      "Iteration 17107, loss = 2.46477888\n",
      "Iteration 17108, loss = 2.16722947\n",
      "Iteration 17109, loss = 2.45623329\n",
      "Iteration 17110, loss = 3.51489036\n",
      "Iteration 17111, loss = 3.44239648\n",
      "Iteration 17112, loss = 2.47009971\n",
      "Iteration 17113, loss = 2.36568479\n",
      "Iteration 17114, loss = 4.38850677\n",
      "Iteration 17115, loss = 2.90560057\n",
      "Iteration 17116, loss = 2.36161752\n",
      "Iteration 17117, loss = 2.38335511\n",
      "Iteration 17118, loss = 2.63537587\n",
      "Iteration 17119, loss = 2.46825886\n",
      "Iteration 17120, loss = 2.40733510\n",
      "Iteration 17121, loss = 2.18852942\n",
      "Iteration 17122, loss = 2.14946797\n",
      "Iteration 17123, loss = 2.20125447\n",
      "Iteration 17124, loss = 2.08817924\n",
      "Iteration 17125, loss = 2.05433882\n",
      "Iteration 17126, loss = 2.06046220\n",
      "Iteration 17127, loss = 2.25308810\n",
      "Iteration 17128, loss = 2.68791192\n",
      "Iteration 17129, loss = 3.02798587\n",
      "Iteration 17130, loss = 2.31954926\n",
      "Iteration 17131, loss = 2.44187251\n",
      "Iteration 17132, loss = 2.21340008\n",
      "Iteration 17133, loss = 2.48306542\n",
      "Iteration 17134, loss = 2.82036244\n",
      "Iteration 17135, loss = 2.54447898\n",
      "Iteration 17136, loss = 2.56002607\n",
      "Iteration 17137, loss = 2.36447980\n",
      "Iteration 17138, loss = 3.00252334\n",
      "Iteration 17139, loss = 3.11307214\n",
      "Iteration 17140, loss = 4.00604693\n",
      "Iteration 17141, loss = 2.78584438\n",
      "Iteration 17142, loss = 2.68249852\n",
      "Iteration 17143, loss = 3.96637204\n",
      "Iteration 17144, loss = 2.96608845\n",
      "Iteration 17145, loss = 2.47055576\n",
      "Iteration 17146, loss = 2.38287756\n",
      "Iteration 17147, loss = 2.53293624\n",
      "Iteration 17148, loss = 2.27870290\n",
      "Iteration 17149, loss = 3.35565217\n",
      "Iteration 17150, loss = 2.46667630\n",
      "Iteration 17151, loss = 2.96387809\n",
      "Iteration 17152, loss = 2.60315361\n",
      "Iteration 17153, loss = 3.31102463\n",
      "Iteration 17154, loss = 2.67614700\n",
      "Iteration 17155, loss = 2.32818020\n",
      "Iteration 17156, loss = 2.34106912\n",
      "Iteration 17157, loss = 2.22146069\n",
      "Iteration 17158, loss = 2.08357924\n",
      "Iteration 17159, loss = 2.10935528\n",
      "Iteration 17160, loss = 2.92988679\n",
      "Iteration 17161, loss = 2.23221201\n",
      "Iteration 17162, loss = 2.29886122\n",
      "Iteration 17163, loss = 2.26923875\n",
      "Iteration 17164, loss = 2.39955173\n",
      "Iteration 17165, loss = 3.16595242\n",
      "Iteration 17166, loss = 2.68419401\n",
      "Iteration 17167, loss = 2.52565685\n",
      "Iteration 17168, loss = 1.91137006\n",
      "Iteration 17169, loss = 2.29407872\n",
      "Iteration 17170, loss = 2.15793773\n",
      "Iteration 17171, loss = 2.07173136\n",
      "Iteration 17172, loss = 2.12657390\n",
      "Iteration 17173, loss = 3.10594082\n",
      "Iteration 17174, loss = 2.73809576\n",
      "Iteration 17175, loss = 2.99786274\n",
      "Iteration 17176, loss = 3.39831080\n",
      "Iteration 17177, loss = 4.99752607\n",
      "Iteration 17178, loss = 2.75911500\n",
      "Iteration 17179, loss = 2.25985079\n",
      "Iteration 17180, loss = 2.15229221\n",
      "Iteration 17181, loss = 2.11504909\n",
      "Iteration 17182, loss = 2.01814830\n",
      "Iteration 17183, loss = 2.58579361\n",
      "Iteration 17184, loss = 2.04359244\n",
      "Iteration 17185, loss = 2.71047812\n",
      "Iteration 17186, loss = 2.31472463\n",
      "Iteration 17187, loss = 2.39991144\n",
      "Iteration 17188, loss = 2.00528562\n",
      "Iteration 17189, loss = 2.16498870\n",
      "Iteration 17190, loss = 2.05999886\n",
      "Iteration 17191, loss = 2.24646506\n",
      "Iteration 17192, loss = 2.31719537\n",
      "Iteration 17193, loss = 2.18942082\n",
      "Iteration 17194, loss = 2.39235950\n",
      "Iteration 17195, loss = 2.28113414\n",
      "Iteration 17196, loss = 2.27047341\n",
      "Iteration 17197, loss = 2.14565461\n",
      "Iteration 17198, loss = 1.99646830\n",
      "Iteration 17199, loss = 2.14247312\n",
      "Iteration 17200, loss = 2.40095711\n",
      "Iteration 17201, loss = 2.32970667\n",
      "Iteration 17202, loss = 2.46390990\n",
      "Iteration 17203, loss = 2.09928975\n",
      "Iteration 17204, loss = 1.99057282\n",
      "Iteration 17205, loss = 2.92064339\n",
      "Iteration 17206, loss = 3.70800683\n",
      "Iteration 17207, loss = 3.43575033\n",
      "Iteration 17208, loss = 3.69979327\n",
      "Iteration 17209, loss = 3.02005961\n",
      "Iteration 17210, loss = 2.18013874\n",
      "Iteration 17211, loss = 2.68938652\n",
      "Iteration 17212, loss = 3.37910100\n",
      "Iteration 17213, loss = 4.04662309\n",
      "Iteration 17214, loss = 2.39122770\n",
      "Iteration 17215, loss = 2.07003650\n",
      "Iteration 17216, loss = 1.99640642\n",
      "Iteration 17217, loss = 2.51346885\n",
      "Iteration 17218, loss = 2.60295394\n",
      "Iteration 17219, loss = 2.17903163\n",
      "Iteration 17220, loss = 2.40781579\n",
      "Iteration 17221, loss = 2.34410479\n",
      "Iteration 17222, loss = 2.39651315\n",
      "Iteration 17223, loss = 2.33354836\n",
      "Iteration 17224, loss = 2.05737776\n",
      "Iteration 17225, loss = 2.16780463\n",
      "Iteration 17226, loss = 3.90921039\n",
      "Iteration 17227, loss = 8.81266208\n",
      "Iteration 17228, loss = 3.59830702\n",
      "Iteration 17229, loss = 5.15349387\n",
      "Iteration 17230, loss = 4.09995087\n",
      "Iteration 17231, loss = 4.21190154\n",
      "Iteration 17232, loss = 2.21779931\n",
      "Iteration 17233, loss = 3.12168412\n",
      "Iteration 17234, loss = 2.72061688\n",
      "Iteration 17235, loss = 2.06899874\n",
      "Iteration 17236, loss = 2.60076071\n",
      "Iteration 17237, loss = 4.52318020\n",
      "Iteration 17238, loss = 3.78204789\n",
      "Iteration 17239, loss = 4.19333355\n",
      "Iteration 17240, loss = 5.88273229\n",
      "Iteration 17241, loss = 4.52586693\n",
      "Iteration 17242, loss = 3.83611232\n",
      "Iteration 17243, loss = 2.35811647\n",
      "Iteration 17244, loss = 2.24903345\n",
      "Iteration 17245, loss = 2.16546280\n",
      "Iteration 17246, loss = 2.23653577\n",
      "Iteration 17247, loss = 2.46726824\n",
      "Iteration 17248, loss = 2.93727477\n",
      "Iteration 17249, loss = 2.24989984\n",
      "Iteration 17250, loss = 2.07756284\n",
      "Iteration 17251, loss = 1.88667603\n",
      "Iteration 17252, loss = 2.04794847\n",
      "Iteration 17253, loss = 2.02993373\n",
      "Iteration 17254, loss = 2.07777846\n",
      "Iteration 17255, loss = 2.17217023\n",
      "Iteration 17256, loss = 2.75716490\n",
      "Iteration 17257, loss = 2.11289702\n",
      "Iteration 17258, loss = 1.96183294\n",
      "Iteration 17259, loss = 1.86571439\n",
      "Iteration 17260, loss = 2.21966096\n",
      "Iteration 17261, loss = 1.95573190\n",
      "Iteration 17262, loss = 1.97074695\n",
      "Iteration 17263, loss = 2.10689086\n",
      "Iteration 17264, loss = 2.02726266\n",
      "Iteration 17265, loss = 2.12436654\n",
      "Iteration 17266, loss = 1.99862668\n",
      "Iteration 17267, loss = 2.14269476\n",
      "Iteration 17268, loss = 2.64913506\n",
      "Iteration 17269, loss = 1.93433684\n",
      "Iteration 17270, loss = 2.06145756\n",
      "Iteration 17271, loss = 1.96019570\n",
      "Iteration 17272, loss = 1.90754059\n",
      "Iteration 17273, loss = 1.95804219\n",
      "Iteration 17274, loss = 2.16602268\n",
      "Iteration 17275, loss = 2.11712362\n",
      "Iteration 17276, loss = 2.37455438\n",
      "Iteration 17277, loss = 2.20588261\n",
      "Iteration 17278, loss = 2.14589286\n",
      "Iteration 17279, loss = 2.20328096\n",
      "Iteration 17280, loss = 2.04665380\n",
      "Iteration 17281, loss = 2.23606879\n",
      "Iteration 17282, loss = 1.97205830\n",
      "Iteration 17283, loss = 2.14329640\n",
      "Iteration 17284, loss = 2.47312439\n",
      "Iteration 17285, loss = 2.02947640\n",
      "Iteration 17286, loss = 1.91965961\n",
      "Iteration 17287, loss = 2.00506995\n",
      "Iteration 17288, loss = 2.02938185\n",
      "Iteration 17289, loss = 1.99243874\n",
      "Iteration 17290, loss = 2.25523301\n",
      "Iteration 17291, loss = 4.12488558\n",
      "Iteration 17292, loss = 3.64338331\n",
      "Iteration 17293, loss = 3.67871379\n",
      "Iteration 17294, loss = 2.57267436\n",
      "Iteration 17295, loss = 2.61239342\n",
      "Iteration 17296, loss = 2.59795149\n",
      "Iteration 17297, loss = 5.25259213\n",
      "Iteration 17298, loss = 3.42498650\n",
      "Iteration 17299, loss = 4.01800905\n",
      "Iteration 17300, loss = 2.92959595\n",
      "Iteration 17301, loss = 2.23656989\n",
      "Iteration 17302, loss = 2.32866507\n",
      "Iteration 17303, loss = 2.54515420\n",
      "Iteration 17304, loss = 3.31408376\n",
      "Iteration 17305, loss = 3.41531425\n",
      "Iteration 17306, loss = 2.24422554\n",
      "Iteration 17307, loss = 3.52699966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17308, loss = 1.80349040\n",
      "Iteration 17309, loss = 1.84548939\n",
      "Iteration 17310, loss = 2.76152919\n",
      "Iteration 17311, loss = 2.88148315\n",
      "Iteration 17312, loss = 2.57302803\n",
      "Iteration 17313, loss = 2.03164317\n",
      "Iteration 17314, loss = 2.20673871\n",
      "Iteration 17315, loss = 1.89133947\n",
      "Iteration 17316, loss = 2.02765548\n",
      "Iteration 17317, loss = 2.53412817\n",
      "Iteration 17318, loss = 2.10386058\n",
      "Iteration 17319, loss = 1.93384232\n",
      "Iteration 17320, loss = 1.98265865\n",
      "Iteration 17321, loss = 1.94353342\n",
      "Iteration 17322, loss = 2.19897592\n",
      "Iteration 17323, loss = 1.83753849\n",
      "Iteration 17324, loss = 2.11193798\n",
      "Iteration 17325, loss = 1.88191637\n",
      "Iteration 17326, loss = 2.02754361\n",
      "Iteration 17327, loss = 1.99543203\n",
      "Iteration 17328, loss = 2.26876205\n",
      "Iteration 17329, loss = 2.14844089\n",
      "Iteration 17330, loss = 1.92412869\n",
      "Iteration 17331, loss = 1.89215976\n",
      "Iteration 17332, loss = 1.90891499\n",
      "Iteration 17333, loss = 1.95063326\n",
      "Iteration 17334, loss = 3.63070618\n",
      "Iteration 17335, loss = 3.63440281\n",
      "Iteration 17336, loss = 2.68755007\n",
      "Iteration 17337, loss = 3.30403234\n",
      "Iteration 17338, loss = 2.97651771\n",
      "Iteration 17339, loss = 3.93097014\n",
      "Iteration 17340, loss = 3.39459694\n",
      "Iteration 17341, loss = 2.89102510\n",
      "Iteration 17342, loss = 4.06492302\n",
      "Iteration 17343, loss = 2.70108708\n",
      "Iteration 17344, loss = 3.52684106\n",
      "Iteration 17345, loss = 2.61803313\n",
      "Iteration 17346, loss = 6.24735291\n",
      "Iteration 17347, loss = 3.27178614\n",
      "Iteration 17348, loss = 3.64772161\n",
      "Iteration 17349, loss = 2.29915836\n",
      "Iteration 17350, loss = 2.33599418\n",
      "Iteration 17351, loss = 2.39539482\n",
      "Iteration 17352, loss = 2.33968107\n",
      "Iteration 17353, loss = 2.23933705\n",
      "Iteration 17354, loss = 2.39036938\n",
      "Iteration 17355, loss = 2.14515577\n",
      "Iteration 17356, loss = 2.56600749\n",
      "Iteration 17357, loss = 3.10441700\n",
      "Iteration 17358, loss = 2.22173577\n",
      "Iteration 17359, loss = 2.02311625\n",
      "Iteration 17360, loss = 2.09317913\n",
      "Iteration 17361, loss = 1.93128185\n",
      "Iteration 17362, loss = 2.20923062\n",
      "Iteration 17363, loss = 3.56631534\n",
      "Iteration 17364, loss = 2.62961403\n",
      "Iteration 17365, loss = 2.83897938\n",
      "Iteration 17366, loss = 2.97496050\n",
      "Iteration 17367, loss = 3.18955159\n",
      "Iteration 17368, loss = 3.78175132\n",
      "Iteration 17369, loss = 2.78726372\n",
      "Iteration 17370, loss = 2.78070219\n",
      "Iteration 17371, loss = 2.22884389\n",
      "Iteration 17372, loss = 2.11989043\n",
      "Iteration 17373, loss = 2.02316346\n",
      "Iteration 17374, loss = 2.17078432\n",
      "Iteration 17375, loss = 2.43466720\n",
      "Iteration 17376, loss = 2.31885931\n",
      "Iteration 17377, loss = 5.20897448\n",
      "Iteration 17378, loss = 3.16725891\n",
      "Iteration 17379, loss = 2.97751825\n",
      "Iteration 17380, loss = 3.18725762\n",
      "Iteration 17381, loss = 4.21459634\n",
      "Iteration 17382, loss = 2.55379892\n",
      "Iteration 17383, loss = 3.00559770\n",
      "Iteration 17384, loss = 2.86375160\n",
      "Iteration 17385, loss = 2.45764596\n",
      "Iteration 17386, loss = 2.08003071\n",
      "Iteration 17387, loss = 2.05891922\n",
      "Iteration 17388, loss = 2.00132551\n",
      "Iteration 17389, loss = 2.01824275\n",
      "Iteration 17390, loss = 2.01955606\n",
      "Iteration 17391, loss = 2.33701688\n",
      "Iteration 17392, loss = 3.88123272\n",
      "Iteration 17393, loss = 7.76924947\n",
      "Iteration 17394, loss = 4.48046280\n",
      "Iteration 17395, loss = 2.91285048\n",
      "Iteration 17396, loss = 2.88794511\n",
      "Iteration 17397, loss = 3.14486622\n",
      "Iteration 17398, loss = 2.11028531\n",
      "Iteration 17399, loss = 2.19767353\n",
      "Iteration 17400, loss = 2.02612420\n",
      "Iteration 17401, loss = 2.33191592\n",
      "Iteration 17402, loss = 2.11982658\n",
      "Iteration 17403, loss = 2.45902150\n",
      "Iteration 17404, loss = 2.26827596\n",
      "Iteration 17405, loss = 2.05994894\n",
      "Iteration 17406, loss = 2.11665629\n",
      "Iteration 17407, loss = 1.86911814\n",
      "Iteration 17408, loss = 2.55949682\n",
      "Iteration 17409, loss = 2.36495689\n",
      "Iteration 17410, loss = 2.03261167\n",
      "Iteration 17411, loss = 1.98778653\n",
      "Iteration 17412, loss = 2.02059031\n",
      "Iteration 17413, loss = 2.18242072\n",
      "Iteration 17414, loss = 1.89317465\n",
      "Iteration 17415, loss = 1.87895440\n",
      "Iteration 17416, loss = 2.15561647\n",
      "Iteration 17417, loss = 1.87757995\n",
      "Iteration 17418, loss = 1.96362835\n",
      "Iteration 17419, loss = 1.85437211\n",
      "Iteration 17420, loss = 1.76509932\n",
      "Iteration 17421, loss = 1.84192065\n",
      "Iteration 17422, loss = 1.80849475\n",
      "Iteration 17423, loss = 1.87966472\n",
      "Iteration 17424, loss = 2.69619768\n",
      "Iteration 17425, loss = 5.44372424\n",
      "Iteration 17426, loss = 4.52985642\n",
      "Iteration 17427, loss = 4.65637526\n",
      "Iteration 17428, loss = 3.98880168\n",
      "Iteration 17429, loss = 3.83880750\n",
      "Iteration 17430, loss = 4.68419635\n",
      "Iteration 17431, loss = 2.23025423\n",
      "Iteration 17432, loss = 2.61987991\n",
      "Iteration 17433, loss = 2.31171872\n",
      "Iteration 17434, loss = 2.01568884\n",
      "Iteration 17435, loss = 2.14584552\n",
      "Iteration 17436, loss = 2.17223588\n",
      "Iteration 17437, loss = 3.10280916\n",
      "Iteration 17438, loss = 2.64729517\n",
      "Iteration 17439, loss = 2.52245452\n",
      "Iteration 17440, loss = 2.52043983\n",
      "Iteration 17441, loss = 2.67774319\n",
      "Iteration 17442, loss = 2.72847782\n",
      "Iteration 17443, loss = 2.11003324\n",
      "Iteration 17444, loss = 1.99564317\n",
      "Iteration 17445, loss = 1.93017801\n",
      "Iteration 17446, loss = 2.20076738\n",
      "Iteration 17447, loss = 2.03901027\n",
      "Iteration 17448, loss = 1.93744609\n",
      "Iteration 17449, loss = 1.84134801\n",
      "Iteration 17450, loss = 1.85471957\n",
      "Iteration 17451, loss = 1.91437935\n",
      "Iteration 17452, loss = 1.89878832\n",
      "Iteration 17453, loss = 1.96993928\n",
      "Iteration 17454, loss = 1.92167995\n",
      "Iteration 17455, loss = 2.30390657\n",
      "Iteration 17456, loss = 1.96656607\n",
      "Iteration 17457, loss = 1.89479054\n",
      "Iteration 17458, loss = 1.91011705\n",
      "Iteration 17459, loss = 1.88563079\n",
      "Iteration 17460, loss = 1.86222422\n",
      "Iteration 17461, loss = 2.06755444\n",
      "Iteration 17462, loss = 1.79241412\n",
      "Iteration 17463, loss = 1.82423933\n",
      "Iteration 17464, loss = 2.02303226\n",
      "Iteration 17465, loss = 2.59522834\n",
      "Iteration 17466, loss = 2.22802172\n",
      "Iteration 17467, loss = 2.91564834\n",
      "Iteration 17468, loss = 2.31728379\n",
      "Iteration 17469, loss = 1.73300847\n",
      "Iteration 17470, loss = 2.07905433\n",
      "Iteration 17471, loss = 2.01381622\n",
      "Iteration 17472, loss = 1.96632540\n",
      "Iteration 17473, loss = 1.83137005\n",
      "Iteration 17474, loss = 1.91295577\n",
      "Iteration 17475, loss = 1.96038774\n",
      "Iteration 17476, loss = 1.87787550\n",
      "Iteration 17477, loss = 2.17126622\n",
      "Iteration 17478, loss = 2.47404875\n",
      "Iteration 17479, loss = 2.31643743\n",
      "Iteration 17480, loss = 1.94780545\n",
      "Iteration 17481, loss = 2.41916041\n",
      "Iteration 17482, loss = 3.10357256\n",
      "Iteration 17483, loss = 2.44557846\n",
      "Iteration 17484, loss = 2.61828693\n",
      "Iteration 17485, loss = 2.29831244\n",
      "Iteration 17486, loss = 2.33254856\n",
      "Iteration 17487, loss = 1.93184963\n",
      "Iteration 17488, loss = 2.59006814\n",
      "Iteration 17489, loss = 2.66238280\n",
      "Iteration 17490, loss = 4.03976597\n",
      "Iteration 17491, loss = 2.13697011\n",
      "Iteration 17492, loss = 2.30667068\n",
      "Iteration 17493, loss = 1.93679858\n",
      "Iteration 17494, loss = 2.01118153\n",
      "Iteration 17495, loss = 2.32425582\n",
      "Iteration 17496, loss = 2.90337423\n",
      "Iteration 17497, loss = 4.92587413\n",
      "Iteration 17498, loss = 2.95304158\n",
      "Iteration 17499, loss = 2.20614706\n",
      "Iteration 17500, loss = 2.23407895\n",
      "Iteration 17501, loss = 1.95356909\n",
      "Iteration 17502, loss = 1.97946269\n",
      "Iteration 17503, loss = 2.24070340\n",
      "Iteration 17504, loss = 3.60604734\n",
      "Iteration 17505, loss = 2.38149119\n",
      "Iteration 17506, loss = 2.29546695\n",
      "Iteration 17507, loss = 1.96691649\n",
      "Iteration 17508, loss = 3.05396742\n",
      "Iteration 17509, loss = 2.50137112\n",
      "Iteration 17510, loss = 2.33862451\n",
      "Iteration 17511, loss = 2.72977926\n",
      "Iteration 17512, loss = 2.08256441\n",
      "Iteration 17513, loss = 1.98944824\n",
      "Iteration 17514, loss = 1.86415616\n",
      "Iteration 17515, loss = 2.10332112\n",
      "Iteration 17516, loss = 2.69995658\n",
      "Iteration 17517, loss = 4.30602721\n",
      "Iteration 17518, loss = 3.11250980\n",
      "Iteration 17519, loss = 3.03500720\n",
      "Iteration 17520, loss = 3.31896966\n",
      "Iteration 17521, loss = 2.40458720\n",
      "Iteration 17522, loss = 2.48437716\n",
      "Iteration 17523, loss = 2.62733476\n",
      "Iteration 17524, loss = 1.85151822\n",
      "Iteration 17525, loss = 2.03255761\n",
      "Iteration 17526, loss = 2.73866891\n",
      "Iteration 17527, loss = 2.37115277\n",
      "Iteration 17528, loss = 2.15194459\n",
      "Iteration 17529, loss = 2.30680131\n",
      "Iteration 17530, loss = 2.56581565\n",
      "Iteration 17531, loss = 2.45965090\n",
      "Iteration 17532, loss = 3.06517495\n",
      "Iteration 17533, loss = 2.94713799\n",
      "Iteration 17534, loss = 2.77443177\n",
      "Iteration 17535, loss = 2.03550121\n",
      "Iteration 17536, loss = 2.07825103\n",
      "Iteration 17537, loss = 2.06204318\n",
      "Iteration 17538, loss = 2.16196783\n",
      "Iteration 17539, loss = 2.04363277\n",
      "Iteration 17540, loss = 1.85005962\n",
      "Iteration 17541, loss = 1.89186968\n",
      "Iteration 17542, loss = 1.98117356\n",
      "Iteration 17543, loss = 1.96160197\n",
      "Iteration 17544, loss = 1.83941612\n",
      "Iteration 17545, loss = 1.80021047\n",
      "Iteration 17546, loss = 1.99598335\n",
      "Iteration 17547, loss = 2.01223794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17548, loss = 1.97554927\n",
      "Iteration 17549, loss = 1.72881659\n",
      "Iteration 17550, loss = 2.27659110\n",
      "Iteration 17551, loss = 4.02405280\n",
      "Iteration 17552, loss = 2.31258952\n",
      "Iteration 17553, loss = 2.06129989\n",
      "Iteration 17554, loss = 2.14461848\n",
      "Iteration 17555, loss = 2.12723140\n",
      "Iteration 17556, loss = 2.04244460\n",
      "Iteration 17557, loss = 1.86846307\n",
      "Iteration 17558, loss = 2.44883971\n",
      "Iteration 17559, loss = 2.11635001\n",
      "Iteration 17560, loss = 1.90595755\n",
      "Iteration 17561, loss = 2.08925247\n",
      "Iteration 17562, loss = 1.98086531\n",
      "Iteration 17563, loss = 1.76935968\n",
      "Iteration 17564, loss = 1.96621943\n",
      "Iteration 17565, loss = 2.41033320\n",
      "Iteration 17566, loss = 4.87130751\n",
      "Iteration 17567, loss = 3.17684684\n",
      "Iteration 17568, loss = 3.08862136\n",
      "Iteration 17569, loss = 4.80983863\n",
      "Iteration 17570, loss = 2.95567768\n",
      "Iteration 17571, loss = 2.50870882\n",
      "Iteration 17572, loss = 2.43431890\n",
      "Iteration 17573, loss = 2.61558192\n",
      "Iteration 17574, loss = 2.21369829\n",
      "Iteration 17575, loss = 1.87251113\n",
      "Iteration 17576, loss = 2.46791385\n",
      "Iteration 17577, loss = 3.38522619\n",
      "Iteration 17578, loss = 2.86415946\n",
      "Iteration 17579, loss = 1.90929635\n",
      "Iteration 17580, loss = 2.75137077\n",
      "Iteration 17581, loss = 2.94675792\n",
      "Iteration 17582, loss = 2.83158819\n",
      "Iteration 17583, loss = 3.22224589\n",
      "Iteration 17584, loss = 2.52571403\n",
      "Iteration 17585, loss = 2.75675695\n",
      "Iteration 17586, loss = 2.29882716\n",
      "Iteration 17587, loss = 1.98909181\n",
      "Iteration 17588, loss = 2.20084831\n",
      "Iteration 17589, loss = 2.33015953\n",
      "Iteration 17590, loss = 2.21063978\n",
      "Iteration 17591, loss = 1.87828602\n",
      "Iteration 17592, loss = 1.85903815\n",
      "Iteration 17593, loss = 1.80358102\n",
      "Iteration 17594, loss = 2.39126124\n",
      "Iteration 17595, loss = 2.03609773\n",
      "Iteration 17596, loss = 2.21819370\n",
      "Iteration 17597, loss = 2.33079019\n",
      "Iteration 17598, loss = 2.43718445\n",
      "Iteration 17599, loss = 2.18300297\n",
      "Iteration 17600, loss = 2.30026408\n",
      "Iteration 17601, loss = 2.61413593\n",
      "Iteration 17602, loss = 2.28682713\n",
      "Iteration 17603, loss = 2.77253229\n",
      "Iteration 17604, loss = 2.08946547\n",
      "Iteration 17605, loss = 2.02002141\n",
      "Iteration 17606, loss = 2.07238531\n",
      "Iteration 17607, loss = 2.36584119\n",
      "Iteration 17608, loss = 2.32559419\n",
      "Iteration 17609, loss = 2.16444680\n",
      "Iteration 17610, loss = 2.32288949\n",
      "Iteration 17611, loss = 2.36185034\n",
      "Iteration 17612, loss = 2.00944077\n",
      "Iteration 17613, loss = 1.88079789\n",
      "Iteration 17614, loss = 1.80562958\n",
      "Iteration 17615, loss = 2.17061314\n",
      "Iteration 17616, loss = 2.22775812\n",
      "Iteration 17617, loss = 5.15239456\n",
      "Iteration 17618, loss = 10.33838251\n",
      "Iteration 17619, loss = 5.56605005\n",
      "Iteration 17620, loss = 4.91520777\n",
      "Iteration 17621, loss = 4.21334416\n",
      "Iteration 17622, loss = 2.99721477\n",
      "Iteration 17623, loss = 3.15793058\n",
      "Iteration 17624, loss = 4.06725572\n",
      "Iteration 17625, loss = 3.98757864\n",
      "Iteration 17626, loss = 3.06631433\n",
      "Iteration 17627, loss = 2.86631622\n",
      "Iteration 17628, loss = 2.45485959\n",
      "Iteration 17629, loss = 2.53240357\n",
      "Iteration 17630, loss = 2.20199303\n",
      "Iteration 17631, loss = 2.04528056\n",
      "Iteration 17632, loss = 2.47089570\n",
      "Iteration 17633, loss = 3.45129280\n",
      "Iteration 17634, loss = 2.58323343\n",
      "Iteration 17635, loss = 2.23665233\n",
      "Iteration 17636, loss = 1.88142026\n",
      "Iteration 17637, loss = 1.99023328\n",
      "Iteration 17638, loss = 1.81371607\n",
      "Iteration 17639, loss = 2.08052580\n",
      "Iteration 17640, loss = 1.98284943\n",
      "Iteration 17641, loss = 2.58070286\n",
      "Iteration 17642, loss = 2.32525357\n",
      "Iteration 17643, loss = 2.96819854\n",
      "Iteration 17644, loss = 2.96679291\n",
      "Iteration 17645, loss = 1.91790311\n",
      "Iteration 17646, loss = 1.93460939\n",
      "Iteration 17647, loss = 2.29611291\n",
      "Iteration 17648, loss = 2.05615985\n",
      "Iteration 17649, loss = 2.33339824\n",
      "Iteration 17650, loss = 2.96302120\n",
      "Iteration 17651, loss = 2.48915664\n",
      "Iteration 17652, loss = 2.78095126\n",
      "Iteration 17653, loss = 2.67572112\n",
      "Iteration 17654, loss = 1.98185859\n",
      "Iteration 17655, loss = 1.98051893\n",
      "Iteration 17656, loss = 2.20463587\n",
      "Iteration 17657, loss = 2.53693432\n",
      "Iteration 17658, loss = 2.22457239\n",
      "Iteration 17659, loss = 2.47808117\n",
      "Iteration 17660, loss = 1.93338303\n",
      "Iteration 17661, loss = 1.79801130\n",
      "Iteration 17662, loss = 2.16548418\n",
      "Iteration 17663, loss = 2.96882226\n",
      "Iteration 17664, loss = 2.55637389\n",
      "Iteration 17665, loss = 2.90985959\n",
      "Iteration 17666, loss = 2.50041065\n",
      "Iteration 17667, loss = 3.45230573\n",
      "Iteration 17668, loss = 2.89965395\n",
      "Iteration 17669, loss = 1.85005654\n",
      "Iteration 17670, loss = 2.06905549\n",
      "Iteration 17671, loss = 1.76057297\n",
      "Iteration 17672, loss = 1.70863870\n",
      "Iteration 17673, loss = 1.78156307\n",
      "Iteration 17674, loss = 1.91262472\n",
      "Iteration 17675, loss = 2.09292755\n",
      "Iteration 17676, loss = 1.87365011\n",
      "Iteration 17677, loss = 2.39984238\n",
      "Iteration 17678, loss = 2.03673740\n",
      "Iteration 17679, loss = 2.40540727\n",
      "Iteration 17680, loss = 2.05777018\n",
      "Iteration 17681, loss = 2.43140407\n",
      "Iteration 17682, loss = 2.34707252\n",
      "Iteration 17683, loss = 3.31564061\n",
      "Iteration 17684, loss = 2.15141396\n",
      "Iteration 17685, loss = 1.89675036\n",
      "Iteration 17686, loss = 1.82536261\n",
      "Iteration 17687, loss = 1.79555778\n",
      "Iteration 17688, loss = 1.88511212\n",
      "Iteration 17689, loss = 2.36936958\n",
      "Iteration 17690, loss = 1.77040701\n",
      "Iteration 17691, loss = 1.67611095\n",
      "Iteration 17692, loss = 1.73818591\n",
      "Iteration 17693, loss = 1.81374238\n",
      "Iteration 17694, loss = 1.76135478\n",
      "Iteration 17695, loss = 1.66738404\n",
      "Iteration 17696, loss = 2.32902051\n",
      "Iteration 17697, loss = 2.57363358\n",
      "Iteration 17698, loss = 2.35787269\n",
      "Iteration 17699, loss = 2.02729604\n",
      "Iteration 17700, loss = 1.74741670\n",
      "Iteration 17701, loss = 1.98544926\n",
      "Iteration 17702, loss = 1.80163075\n",
      "Iteration 17703, loss = 1.77839074\n",
      "Iteration 17704, loss = 1.87313236\n",
      "Iteration 17705, loss = 3.41710120\n",
      "Iteration 17706, loss = 4.02959356\n",
      "Iteration 17707, loss = 3.12013998\n",
      "Iteration 17708, loss = 2.84657003\n",
      "Iteration 17709, loss = 2.78745656\n",
      "Iteration 17710, loss = 2.39470132\n",
      "Iteration 17711, loss = 2.28637305\n",
      "Iteration 17712, loss = 3.25390343\n",
      "Iteration 17713, loss = 3.68562296\n",
      "Iteration 17714, loss = 1.96371377\n",
      "Iteration 17715, loss = 1.74854650\n",
      "Iteration 17716, loss = 2.21293292\n",
      "Iteration 17717, loss = 2.09683243\n",
      "Iteration 17718, loss = 1.84314424\n",
      "Iteration 17719, loss = 2.46575464\n",
      "Iteration 17720, loss = 2.32064669\n",
      "Iteration 17721, loss = 2.11756834\n",
      "Iteration 17722, loss = 1.82408649\n",
      "Iteration 17723, loss = 1.70815234\n",
      "Iteration 17724, loss = 1.74098883\n",
      "Iteration 17725, loss = 1.78449052\n",
      "Iteration 17726, loss = 1.85342592\n",
      "Iteration 17727, loss = 2.98104243\n",
      "Iteration 17728, loss = 2.68376034\n",
      "Iteration 17729, loss = 2.19053043\n",
      "Iteration 17730, loss = 2.95898411\n",
      "Iteration 17731, loss = 2.03827469\n",
      "Iteration 17732, loss = 2.75599721\n",
      "Iteration 17733, loss = 2.68368175\n",
      "Iteration 17734, loss = 2.55866276\n",
      "Iteration 17735, loss = 5.57581281\n",
      "Iteration 17736, loss = 8.08251264\n",
      "Iteration 17737, loss = 4.68989681\n",
      "Iteration 17738, loss = 3.14767515\n",
      "Iteration 17739, loss = 2.60560775\n",
      "Iteration 17740, loss = 2.56630951\n",
      "Iteration 17741, loss = 1.83117209\n",
      "Iteration 17742, loss = 2.54514290\n",
      "Iteration 17743, loss = 2.11365005\n",
      "Iteration 17744, loss = 2.33519188\n",
      "Iteration 17745, loss = 2.53491092\n",
      "Iteration 17746, loss = 2.15873705\n",
      "Iteration 17747, loss = 2.43174627\n",
      "Iteration 17748, loss = 2.21829145\n",
      "Iteration 17749, loss = 2.36781584\n",
      "Iteration 17750, loss = 2.02114837\n",
      "Iteration 17751, loss = 2.57531475\n",
      "Iteration 17752, loss = 5.13738579\n",
      "Iteration 17753, loss = 10.05203264\n",
      "Iteration 17754, loss = 6.60603182\n",
      "Iteration 17755, loss = 4.61221806\n",
      "Iteration 17756, loss = 4.43991799\n",
      "Iteration 17757, loss = 2.50428024\n",
      "Iteration 17758, loss = 2.57604976\n",
      "Iteration 17759, loss = 2.39123716\n",
      "Iteration 17760, loss = 2.02929047\n",
      "Iteration 17761, loss = 2.11734479\n",
      "Iteration 17762, loss = 1.79200461\n",
      "Iteration 17763, loss = 1.90337004\n",
      "Iteration 17764, loss = 1.83270745\n",
      "Iteration 17765, loss = 1.79619004\n",
      "Iteration 17766, loss = 1.74153655\n",
      "Iteration 17767, loss = 2.01861502\n",
      "Iteration 17768, loss = 2.43707747\n",
      "Iteration 17769, loss = 2.02606647\n",
      "Iteration 17770, loss = 1.74347383\n",
      "Iteration 17771, loss = 1.68494026\n",
      "Iteration 17772, loss = 1.75027403\n",
      "Iteration 17773, loss = 1.66784745\n",
      "Iteration 17774, loss = 1.68456135\n",
      "Iteration 17775, loss = 1.77352373\n",
      "Iteration 17776, loss = 2.21972936\n",
      "Iteration 17777, loss = 1.81810943\n",
      "Iteration 17778, loss = 1.76358919\n",
      "Iteration 17779, loss = 2.05342243\n",
      "Iteration 17780, loss = 2.57594115\n",
      "Iteration 17781, loss = 5.20628123\n",
      "Iteration 17782, loss = 3.06486625\n",
      "Iteration 17783, loss = 2.46476147\n",
      "Iteration 17784, loss = 2.32559810\n",
      "Iteration 17785, loss = 2.45009435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17786, loss = 2.18128152\n",
      "Iteration 17787, loss = 2.22049894\n",
      "Iteration 17788, loss = 1.91075850\n",
      "Iteration 17789, loss = 1.78799423\n",
      "Iteration 17790, loss = 1.94591824\n",
      "Iteration 17791, loss = 1.92269047\n",
      "Iteration 17792, loss = 2.54237208\n",
      "Iteration 17793, loss = 1.78668710\n",
      "Iteration 17794, loss = 1.86286528\n",
      "Iteration 17795, loss = 1.74426240\n",
      "Iteration 17796, loss = 1.69496073\n",
      "Iteration 17797, loss = 1.66719055\n",
      "Iteration 17798, loss = 1.91203656\n",
      "Iteration 17799, loss = 3.30658490\n",
      "Iteration 17800, loss = 8.38016053\n",
      "Iteration 17801, loss = 6.15343749\n",
      "Iteration 17802, loss = 3.47288625\n",
      "Iteration 17803, loss = 2.90055600\n",
      "Iteration 17804, loss = 2.56960933\n",
      "Iteration 17805, loss = 2.52769815\n",
      "Iteration 17806, loss = 2.62334963\n",
      "Iteration 17807, loss = 2.23950930\n",
      "Iteration 17808, loss = 1.72601168\n",
      "Iteration 17809, loss = 1.78724424\n",
      "Iteration 17810, loss = 1.86059684\n",
      "Iteration 17811, loss = 1.73144466\n",
      "Iteration 17812, loss = 1.66550193\n",
      "Iteration 17813, loss = 1.63290566\n",
      "Iteration 17814, loss = 1.65516776\n",
      "Iteration 17815, loss = 1.96691594\n",
      "Iteration 17816, loss = 2.31726060\n",
      "Iteration 17817, loss = 4.41425937\n",
      "Iteration 17818, loss = 3.24290775\n",
      "Iteration 17819, loss = 2.23702763\n",
      "Iteration 17820, loss = 1.86650172\n",
      "Iteration 17821, loss = 1.76626814\n",
      "Iteration 17822, loss = 2.32264470\n",
      "Iteration 17823, loss = 1.74082748\n",
      "Iteration 17824, loss = 2.13768693\n",
      "Iteration 17825, loss = 2.05686457\n",
      "Iteration 17826, loss = 2.13202302\n",
      "Iteration 17827, loss = 1.72827525\n",
      "Iteration 17828, loss = 1.71338356\n",
      "Iteration 17829, loss = 2.06552008\n",
      "Iteration 17830, loss = 2.30062526\n",
      "Iteration 17831, loss = 1.82253023\n",
      "Iteration 17832, loss = 1.69099636\n",
      "Iteration 17833, loss = 1.76847450\n",
      "Iteration 17834, loss = 1.78084927\n",
      "Iteration 17835, loss = 1.84764281\n",
      "Iteration 17836, loss = 1.79875446\n",
      "Iteration 17837, loss = 1.85599855\n",
      "Iteration 17838, loss = 2.23782066\n",
      "Iteration 17839, loss = 3.25026937\n",
      "Iteration 17840, loss = 2.06946685\n",
      "Iteration 17841, loss = 1.84488614\n",
      "Iteration 17842, loss = 1.94242554\n",
      "Iteration 17843, loss = 1.83923525\n",
      "Iteration 17844, loss = 2.52050649\n",
      "Iteration 17845, loss = 2.32172919\n",
      "Iteration 17846, loss = 1.81284650\n",
      "Iteration 17847, loss = 1.86802616\n",
      "Iteration 17848, loss = 1.83106460\n",
      "Iteration 17849, loss = 1.77901434\n",
      "Iteration 17850, loss = 2.15303542\n",
      "Iteration 17851, loss = 5.09390667\n",
      "Iteration 17852, loss = 9.60717518\n",
      "Iteration 17853, loss = 6.00519382\n",
      "Iteration 17854, loss = 3.96518428\n",
      "Iteration 17855, loss = 3.35458629\n",
      "Iteration 17856, loss = 2.21206422\n",
      "Iteration 17857, loss = 2.39346131\n",
      "Iteration 17858, loss = 2.18121711\n",
      "Iteration 17859, loss = 1.85703006\n",
      "Iteration 17860, loss = 1.80341286\n",
      "Iteration 17861, loss = 2.12980334\n",
      "Iteration 17862, loss = 1.72639924\n",
      "Iteration 17863, loss = 1.62094954\n",
      "Iteration 17864, loss = 1.73975535\n",
      "Iteration 17865, loss = 2.18611347\n",
      "Iteration 17866, loss = 1.89017847\n",
      "Iteration 17867, loss = 1.72452623\n",
      "Iteration 17868, loss = 1.82856028\n",
      "Iteration 17869, loss = 1.92133474\n",
      "Iteration 17870, loss = 1.75512238\n",
      "Iteration 17871, loss = 1.67914864\n",
      "Iteration 17872, loss = 2.65954101\n",
      "Iteration 17873, loss = 1.65846848\n",
      "Iteration 17874, loss = 1.71780649\n",
      "Iteration 17875, loss = 1.87536787\n",
      "Iteration 17876, loss = 1.79495505\n",
      "Iteration 17877, loss = 1.88342695\n",
      "Iteration 17878, loss = 1.72331548\n",
      "Iteration 17879, loss = 1.67072110\n",
      "Iteration 17880, loss = 2.57825952\n",
      "Iteration 17881, loss = 3.94100140\n",
      "Iteration 17882, loss = 2.84805780\n",
      "Iteration 17883, loss = 2.62070933\n",
      "Iteration 17884, loss = 2.35216984\n",
      "Iteration 17885, loss = 3.16983692\n",
      "Iteration 17886, loss = 1.79343439\n",
      "Iteration 17887, loss = 1.81730911\n",
      "Iteration 17888, loss = 2.28328393\n",
      "Iteration 17889, loss = 1.83865945\n",
      "Iteration 17890, loss = 1.83418751\n",
      "Iteration 17891, loss = 2.08698157\n",
      "Iteration 17892, loss = 2.71519289\n",
      "Iteration 17893, loss = 1.91803373\n",
      "Iteration 17894, loss = 1.69324032\n",
      "Iteration 17895, loss = 1.52682502\n",
      "Iteration 17896, loss = 1.97032412\n",
      "Iteration 17897, loss = 2.25246226\n",
      "Iteration 17898, loss = 2.02441115\n",
      "Iteration 17899, loss = 1.96285077\n",
      "Iteration 17900, loss = 1.85413259\n",
      "Iteration 17901, loss = 1.72157159\n",
      "Iteration 17902, loss = 1.76564428\n",
      "Iteration 17903, loss = 1.75746532\n",
      "Iteration 17904, loss = 1.76477780\n",
      "Iteration 17905, loss = 1.89443149\n",
      "Iteration 17906, loss = 1.67598864\n",
      "Iteration 17907, loss = 1.63748486\n",
      "Iteration 17908, loss = 1.91825897\n",
      "Iteration 17909, loss = 2.70541226\n",
      "Iteration 17910, loss = 3.56190388\n",
      "Iteration 17911, loss = 3.55670500\n",
      "Iteration 17912, loss = 2.89535641\n",
      "Iteration 17913, loss = 2.68124531\n",
      "Iteration 17914, loss = 3.52643584\n",
      "Iteration 17915, loss = 2.67418880\n",
      "Iteration 17916, loss = 2.20559377\n",
      "Iteration 17917, loss = 2.13682652\n",
      "Iteration 17918, loss = 3.17164855\n",
      "Iteration 17919, loss = 3.58163362\n",
      "Iteration 17920, loss = 3.39824802\n",
      "Iteration 17921, loss = 5.00072470\n",
      "Iteration 17922, loss = 8.40568647\n",
      "Iteration 17923, loss = 11.77795146\n",
      "Iteration 17924, loss = 8.56289379\n",
      "Iteration 17925, loss = 6.75244372\n",
      "Iteration 17926, loss = 6.96697009\n",
      "Iteration 17927, loss = 4.40901123\n",
      "Iteration 17928, loss = 3.22019863\n",
      "Iteration 17929, loss = 2.52669574\n",
      "Iteration 17930, loss = 2.67613237\n",
      "Iteration 17931, loss = 2.52348564\n",
      "Iteration 17932, loss = 2.41196034\n",
      "Iteration 17933, loss = 2.05986983\n",
      "Iteration 17934, loss = 2.76569590\n",
      "Iteration 17935, loss = 3.92736994\n",
      "Iteration 17936, loss = 2.37181300\n",
      "Iteration 17937, loss = 1.83136190\n",
      "Iteration 17938, loss = 2.82114809\n",
      "Iteration 17939, loss = 1.73832795\n",
      "Iteration 17940, loss = 1.84640587\n",
      "Iteration 17941, loss = 1.73176385\n",
      "Iteration 17942, loss = 1.90032656\n",
      "Iteration 17943, loss = 2.17186491\n",
      "Iteration 17944, loss = 2.33408792\n",
      "Iteration 17945, loss = 2.67971859\n",
      "Iteration 17946, loss = 3.02339514\n",
      "Iteration 17947, loss = 2.54461742\n",
      "Iteration 17948, loss = 2.35771861\n",
      "Iteration 17949, loss = 1.77455902\n",
      "Iteration 17950, loss = 2.12115422\n",
      "Iteration 17951, loss = 2.18637188\n",
      "Iteration 17952, loss = 1.56027361\n",
      "Iteration 17953, loss = 1.72932984\n",
      "Iteration 17954, loss = 1.59334562\n",
      "Iteration 17955, loss = 2.26575381\n",
      "Iteration 17956, loss = 2.30259487\n",
      "Iteration 17957, loss = 2.20057108\n",
      "Iteration 17958, loss = 2.35269411\n",
      "Iteration 17959, loss = 1.90340958\n",
      "Iteration 17960, loss = 2.00554714\n",
      "Iteration 17961, loss = 1.79149390\n",
      "Iteration 17962, loss = 1.82138592\n",
      "Iteration 17963, loss = 1.97651632\n",
      "Iteration 17964, loss = 2.52369496\n",
      "Iteration 17965, loss = 4.29188057\n",
      "Iteration 17966, loss = 4.65906782\n",
      "Iteration 17967, loss = 5.32992676\n",
      "Iteration 17968, loss = 2.37101916\n",
      "Iteration 17969, loss = 2.10566048\n",
      "Iteration 17970, loss = 2.84567381\n",
      "Iteration 17971, loss = 1.62569214\n",
      "Iteration 17972, loss = 2.42275634\n",
      "Iteration 17973, loss = 3.69896297\n",
      "Iteration 17974, loss = 1.94082462\n",
      "Iteration 17975, loss = 2.35552756\n",
      "Iteration 17976, loss = 2.08882180\n",
      "Iteration 17977, loss = 1.84401150\n",
      "Iteration 17978, loss = 2.21103690\n",
      "Iteration 17979, loss = 4.18173898\n",
      "Iteration 17980, loss = 6.85675077\n",
      "Iteration 17981, loss = 3.77836694\n",
      "Iteration 17982, loss = 2.45569299\n",
      "Iteration 17983, loss = 1.78177600\n",
      "Iteration 17984, loss = 1.96613807\n",
      "Iteration 17985, loss = 1.72720082\n",
      "Iteration 17986, loss = 1.84258710\n",
      "Iteration 17987, loss = 2.08502176\n",
      "Iteration 17988, loss = 3.35883475\n",
      "Iteration 17989, loss = 3.69410693\n",
      "Iteration 17990, loss = 3.22686328\n",
      "Iteration 17991, loss = 3.94431183\n",
      "Iteration 17992, loss = 2.38702250\n",
      "Iteration 17993, loss = 2.16490910\n",
      "Iteration 17994, loss = 2.02841427\n",
      "Iteration 17995, loss = 1.69672212\n",
      "Iteration 17996, loss = 1.70526871\n",
      "Iteration 17997, loss = 1.96299027\n",
      "Iteration 17998, loss = 1.91766325\n",
      "Iteration 17999, loss = 1.71650560\n",
      "Iteration 18000, loss = 2.59705847\n",
      "Iteration 18001, loss = 2.14130061\n",
      "Iteration 18002, loss = 2.34527604\n",
      "Iteration 18003, loss = 1.91045820\n",
      "Iteration 18004, loss = 1.67949501\n",
      "Iteration 18005, loss = 1.86187787\n",
      "Iteration 18006, loss = 1.99747958\n",
      "Iteration 18007, loss = 2.85968915\n",
      "Iteration 18008, loss = 1.98032507\n",
      "Iteration 18009, loss = 1.79864138\n",
      "Iteration 18010, loss = 1.78297949\n",
      "Iteration 18011, loss = 1.59390843\n",
      "Iteration 18012, loss = 1.62872756\n",
      "Iteration 18013, loss = 1.61335892\n",
      "Iteration 18014, loss = 1.59544815\n",
      "Iteration 18015, loss = 1.73769598\n",
      "Iteration 18016, loss = 1.55133943\n",
      "Iteration 18017, loss = 1.63656708\n",
      "Iteration 18018, loss = 1.60591824\n",
      "Iteration 18019, loss = 1.52171860\n",
      "Iteration 18020, loss = 2.73672839\n",
      "Iteration 18021, loss = 4.62969002\n",
      "Iteration 18022, loss = 3.56557530\n",
      "Iteration 18023, loss = 3.82534764\n",
      "Iteration 18024, loss = 2.38096664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18025, loss = 1.80898605\n",
      "Iteration 18026, loss = 2.41116083\n",
      "Iteration 18027, loss = 2.68074123\n",
      "Iteration 18028, loss = 3.07501957\n",
      "Iteration 18029, loss = 2.37374768\n",
      "Iteration 18030, loss = 2.33156367\n",
      "Iteration 18031, loss = 1.94913569\n",
      "Iteration 18032, loss = 2.38339636\n",
      "Iteration 18033, loss = 3.03438014\n",
      "Iteration 18034, loss = 1.96242913\n",
      "Iteration 18035, loss = 1.59621558\n",
      "Iteration 18036, loss = 1.68518571\n",
      "Iteration 18037, loss = 1.61198228\n",
      "Iteration 18038, loss = 1.65899139\n",
      "Iteration 18039, loss = 1.73615804\n",
      "Iteration 18040, loss = 1.59102871\n",
      "Iteration 18041, loss = 1.57938281\n",
      "Iteration 18042, loss = 1.53803850\n",
      "Iteration 18043, loss = 1.52567204\n",
      "Iteration 18044, loss = 1.58529623\n",
      "Iteration 18045, loss = 1.54552620\n",
      "Iteration 18046, loss = 1.83572861\n",
      "Iteration 18047, loss = 1.58076673\n",
      "Iteration 18048, loss = 2.27102479\n",
      "Iteration 18049, loss = 3.80422272\n",
      "Iteration 18050, loss = 2.54261192\n",
      "Iteration 18051, loss = 2.15900183\n",
      "Iteration 18052, loss = 2.47883316\n",
      "Iteration 18053, loss = 4.46988337\n",
      "Iteration 18054, loss = 7.45685410\n",
      "Iteration 18055, loss = 10.77842860\n",
      "Iteration 18056, loss = 9.21788498\n",
      "Iteration 18057, loss = 5.63833252\n",
      "Iteration 18058, loss = 5.47134505\n",
      "Iteration 18059, loss = 3.33483662\n",
      "Iteration 18060, loss = 2.25390094\n",
      "Iteration 18061, loss = 2.33423202\n",
      "Iteration 18062, loss = 2.52271900\n",
      "Iteration 18063, loss = 2.13039751\n",
      "Iteration 18064, loss = 1.82591410\n",
      "Iteration 18065, loss = 2.10112407\n",
      "Iteration 18066, loss = 1.92850620\n",
      "Iteration 18067, loss = 1.86149279\n",
      "Iteration 18068, loss = 1.74661224\n",
      "Iteration 18069, loss = 1.84568933\n",
      "Iteration 18070, loss = 2.66520396\n",
      "Iteration 18071, loss = 2.98551941\n",
      "Iteration 18072, loss = 2.65896573\n",
      "Iteration 18073, loss = 1.99601959\n",
      "Iteration 18074, loss = 3.85508546\n",
      "Iteration 18075, loss = 4.23365522\n",
      "Iteration 18076, loss = 3.83145445\n",
      "Iteration 18077, loss = 3.34468628\n",
      "Iteration 18078, loss = 2.47494974\n",
      "Iteration 18079, loss = 2.08758401\n",
      "Iteration 18080, loss = 1.69762906\n",
      "Iteration 18081, loss = 2.64900771\n",
      "Iteration 18082, loss = 1.73560267\n",
      "Iteration 18083, loss = 1.70872449\n",
      "Iteration 18084, loss = 1.63045256\n",
      "Iteration 18085, loss = 1.55339636\n",
      "Iteration 18086, loss = 1.60673257\n",
      "Iteration 18087, loss = 1.62982866\n",
      "Iteration 18088, loss = 2.94822829\n",
      "Iteration 18089, loss = 2.25148858\n",
      "Iteration 18090, loss = 1.84473314\n",
      "Iteration 18091, loss = 2.42342601\n",
      "Iteration 18092, loss = 2.25382773\n",
      "Iteration 18093, loss = 3.01703134\n",
      "Iteration 18094, loss = 2.16294822\n",
      "Iteration 18095, loss = 2.45636334\n",
      "Iteration 18096, loss = 2.16759648\n",
      "Iteration 18097, loss = 1.81626367\n",
      "Iteration 18098, loss = 1.64970081\n",
      "Iteration 18099, loss = 1.72761961\n",
      "Iteration 18100, loss = 2.91435872\n",
      "Iteration 18101, loss = 2.15125360\n",
      "Iteration 18102, loss = 2.81475796\n",
      "Iteration 18103, loss = 2.11254699\n",
      "Iteration 18104, loss = 2.03432908\n",
      "Iteration 18105, loss = 1.92507975\n",
      "Iteration 18106, loss = 2.14609622\n",
      "Iteration 18107, loss = 1.81579240\n",
      "Iteration 18108, loss = 1.63133978\n",
      "Iteration 18109, loss = 1.64762895\n",
      "Iteration 18110, loss = 1.72621839\n",
      "Iteration 18111, loss = 1.62209699\n",
      "Iteration 18112, loss = 2.81608029\n",
      "Iteration 18113, loss = 2.78211291\n",
      "Iteration 18114, loss = 2.20238553\n",
      "Iteration 18115, loss = 1.62969689\n",
      "Iteration 18116, loss = 1.61313448\n",
      "Iteration 18117, loss = 1.55088048\n",
      "Iteration 18118, loss = 1.84458573\n",
      "Iteration 18119, loss = 1.70802499\n",
      "Iteration 18120, loss = 1.74979880\n",
      "Iteration 18121, loss = 1.73652651\n",
      "Iteration 18122, loss = 1.91858825\n",
      "Iteration 18123, loss = 1.49931761\n",
      "Iteration 18124, loss = 1.86920551\n",
      "Iteration 18125, loss = 1.90569770\n",
      "Iteration 18126, loss = 2.26545491\n",
      "Iteration 18127, loss = 2.16637272\n",
      "Iteration 18128, loss = 2.28168175\n",
      "Iteration 18129, loss = 2.20373675\n",
      "Iteration 18130, loss = 1.97097552\n",
      "Iteration 18131, loss = 1.55287071\n",
      "Iteration 18132, loss = 1.69645401\n",
      "Iteration 18133, loss = 1.79765270\n",
      "Iteration 18134, loss = 2.02235834\n",
      "Iteration 18135, loss = 3.16101920\n",
      "Iteration 18136, loss = 5.32495581\n",
      "Iteration 18137, loss = 5.26709182\n",
      "Iteration 18138, loss = 4.08352574\n",
      "Iteration 18139, loss = 4.71603509\n",
      "Iteration 18140, loss = 3.74500459\n",
      "Iteration 18141, loss = 3.32442197\n",
      "Iteration 18142, loss = 4.19201026\n",
      "Iteration 18143, loss = 2.75427400\n",
      "Iteration 18144, loss = 1.76655176\n",
      "Iteration 18145, loss = 1.59167915\n",
      "Iteration 18146, loss = 1.47328981\n",
      "Iteration 18147, loss = 2.14276370\n",
      "Iteration 18148, loss = 3.15905117\n",
      "Iteration 18149, loss = 2.26086068\n",
      "Iteration 18150, loss = 3.79391158\n",
      "Iteration 18151, loss = 3.21361347\n",
      "Iteration 18152, loss = 1.64050166\n",
      "Iteration 18153, loss = 1.63085719\n",
      "Iteration 18154, loss = 1.70579265\n",
      "Iteration 18155, loss = 1.64306852\n",
      "Iteration 18156, loss = 1.60944817\n",
      "Iteration 18157, loss = 1.86802295\n",
      "Iteration 18158, loss = 1.81930156\n",
      "Iteration 18159, loss = 1.55374312\n",
      "Iteration 18160, loss = 1.59862131\n",
      "Iteration 18161, loss = 1.60537395\n",
      "Iteration 18162, loss = 1.85543377\n",
      "Iteration 18163, loss = 2.11102368\n",
      "Iteration 18164, loss = 1.93566443\n",
      "Iteration 18165, loss = 3.34646159\n",
      "Iteration 18166, loss = 3.07594321\n",
      "Iteration 18167, loss = 4.21023225\n",
      "Iteration 18168, loss = 4.66481385\n",
      "Iteration 18169, loss = 2.65462793\n",
      "Iteration 18170, loss = 4.24737213\n",
      "Iteration 18171, loss = 5.71123917\n",
      "Iteration 18172, loss = 6.40724057\n",
      "Iteration 18173, loss = 8.71812851\n",
      "Iteration 18174, loss = 5.58744379\n",
      "Iteration 18175, loss = 4.07890237\n",
      "Iteration 18176, loss = 3.02187429\n",
      "Iteration 18177, loss = 2.99846319\n",
      "Iteration 18178, loss = 2.32846815\n",
      "Iteration 18179, loss = 2.14328240\n",
      "Iteration 18180, loss = 2.37559664\n",
      "Iteration 18181, loss = 2.12080581\n",
      "Iteration 18182, loss = 1.86897287\n",
      "Iteration 18183, loss = 1.84699295\n",
      "Iteration 18184, loss = 1.76201599\n",
      "Iteration 18185, loss = 1.57438959\n",
      "Iteration 18186, loss = 1.77355873\n",
      "Iteration 18187, loss = 1.68131243\n",
      "Iteration 18188, loss = 1.78604569\n",
      "Iteration 18189, loss = 1.88851702\n",
      "Iteration 18190, loss = 1.72853887\n",
      "Iteration 18191, loss = 1.67161559\n",
      "Iteration 18192, loss = 1.55201153\n",
      "Iteration 18193, loss = 1.51028303\n",
      "Iteration 18194, loss = 1.64953466\n",
      "Iteration 18195, loss = 1.71055063\n",
      "Iteration 18196, loss = 1.69742709\n",
      "Iteration 18197, loss = 1.78517277\n",
      "Iteration 18198, loss = 1.79480603\n",
      "Iteration 18199, loss = 1.55577528\n",
      "Iteration 18200, loss = 1.50124276\n",
      "Iteration 18201, loss = 1.51715804\n",
      "Iteration 18202, loss = 1.70181510\n",
      "Iteration 18203, loss = 1.96181544\n",
      "Iteration 18204, loss = 1.65635545\n",
      "Iteration 18205, loss = 1.97092949\n",
      "Iteration 18206, loss = 1.81757055\n",
      "Iteration 18207, loss = 1.44691145\n",
      "Iteration 18208, loss = 1.54533153\n",
      "Iteration 18209, loss = 2.03950142\n",
      "Iteration 18210, loss = 1.98865824\n",
      "Iteration 18211, loss = 1.87234650\n",
      "Iteration 18212, loss = 1.90938234\n",
      "Iteration 18213, loss = 2.10547368\n",
      "Iteration 18214, loss = 1.82930448\n",
      "Iteration 18215, loss = 1.78594609\n",
      "Iteration 18216, loss = 1.51218178\n",
      "Iteration 18217, loss = 1.47132653\n",
      "Iteration 18218, loss = 1.48356004\n",
      "Iteration 18219, loss = 1.48961767\n",
      "Iteration 18220, loss = 1.59834029\n",
      "Iteration 18221, loss = 1.48068818\n",
      "Iteration 18222, loss = 1.84987661\n",
      "Iteration 18223, loss = 2.28182091\n",
      "Iteration 18224, loss = 1.68189508\n",
      "Iteration 18225, loss = 1.85773104\n",
      "Iteration 18226, loss = 1.72420666\n",
      "Iteration 18227, loss = 1.88734205\n",
      "Iteration 18228, loss = 1.57005456\n",
      "Iteration 18229, loss = 1.64269584\n",
      "Iteration 18230, loss = 2.22212091\n",
      "Iteration 18231, loss = 1.99102424\n",
      "Iteration 18232, loss = 2.32578723\n",
      "Iteration 18233, loss = 2.38456660\n",
      "Iteration 18234, loss = 2.90560681\n",
      "Iteration 18235, loss = 3.16856268\n",
      "Iteration 18236, loss = 5.16810429\n",
      "Iteration 18237, loss = 4.08431420\n",
      "Iteration 18238, loss = 2.73934647\n",
      "Iteration 18239, loss = 2.27612977\n",
      "Iteration 18240, loss = 2.38463823\n",
      "Iteration 18241, loss = 1.71727322\n",
      "Iteration 18242, loss = 1.86087773\n",
      "Iteration 18243, loss = 1.64754734\n",
      "Iteration 18244, loss = 1.52747144\n",
      "Iteration 18245, loss = 1.51464477\n",
      "Iteration 18246, loss = 2.03291880\n",
      "Iteration 18247, loss = 2.64113726\n",
      "Iteration 18248, loss = 5.11250611\n",
      "Iteration 18249, loss = 10.38480717\n",
      "Iteration 18250, loss = 7.95185607\n",
      "Iteration 18251, loss = 6.11993585\n",
      "Iteration 18252, loss = 6.63598154\n",
      "Iteration 18253, loss = 3.05479926\n",
      "Iteration 18254, loss = 2.38604497\n",
      "Iteration 18255, loss = 2.52058446\n",
      "Iteration 18256, loss = 2.20752782\n",
      "Iteration 18257, loss = 2.00693161\n",
      "Iteration 18258, loss = 1.89828570\n",
      "Iteration 18259, loss = 1.63060335\n",
      "Iteration 18260, loss = 1.53503762\n",
      "Iteration 18261, loss = 1.56523916\n",
      "Iteration 18262, loss = 1.61214592\n",
      "Iteration 18263, loss = 1.54789179\n",
      "Iteration 18264, loss = 1.45366119\n",
      "Iteration 18265, loss = 1.46739016\n",
      "Iteration 18266, loss = 1.47940780\n",
      "Iteration 18267, loss = 1.50685263\n",
      "Iteration 18268, loss = 1.62136515\n",
      "Iteration 18269, loss = 1.52428020\n",
      "Iteration 18270, loss = 1.47926446\n",
      "Iteration 18271, loss = 1.46803251\n",
      "Iteration 18272, loss = 1.48949102\n",
      "Iteration 18273, loss = 1.50822088\n",
      "Iteration 18274, loss = 1.70519480\n",
      "Iteration 18275, loss = 1.53604319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18276, loss = 1.46214924\n",
      "Iteration 18277, loss = 1.51451299\n",
      "Iteration 18278, loss = 1.51349710\n",
      "Iteration 18279, loss = 1.45849971\n",
      "Iteration 18280, loss = 1.57334909\n",
      "Iteration 18281, loss = 1.46729637\n",
      "Iteration 18282, loss = 1.50469538\n",
      "Iteration 18283, loss = 1.45706086\n",
      "Iteration 18284, loss = 1.41537321\n",
      "Iteration 18285, loss = 1.44285344\n",
      "Iteration 18286, loss = 1.95903986\n",
      "Iteration 18287, loss = 1.55512513\n",
      "Iteration 18288, loss = 1.42537825\n",
      "Iteration 18289, loss = 1.44307462\n",
      "Iteration 18290, loss = 1.52515317\n",
      "Iteration 18291, loss = 1.52027155\n",
      "Iteration 18292, loss = 1.97261131\n",
      "Iteration 18293, loss = 1.77788154\n",
      "Iteration 18294, loss = 1.52400426\n",
      "Iteration 18295, loss = 1.51579463\n",
      "Iteration 18296, loss = 1.43484478\n",
      "Iteration 18297, loss = 1.65936702\n",
      "Iteration 18298, loss = 1.79011478\n",
      "Iteration 18299, loss = 1.58421526\n",
      "Iteration 18300, loss = 1.53099408\n",
      "Iteration 18301, loss = 1.50939754\n",
      "Iteration 18302, loss = 1.50382801\n",
      "Iteration 18303, loss = 1.52952813\n",
      "Iteration 18304, loss = 1.64652416\n",
      "Iteration 18305, loss = 1.66389200\n",
      "Iteration 18306, loss = 1.46746866\n",
      "Iteration 18307, loss = 1.95417412\n",
      "Iteration 18308, loss = 2.50678660\n",
      "Iteration 18309, loss = 3.51224626\n",
      "Iteration 18310, loss = 3.73730330\n",
      "Iteration 18311, loss = 2.31577354\n",
      "Iteration 18312, loss = 2.30508679\n",
      "Iteration 18313, loss = 1.64479394\n",
      "Iteration 18314, loss = 1.44142735\n",
      "Iteration 18315, loss = 1.49785628\n",
      "Iteration 18316, loss = 1.40081584\n",
      "Iteration 18317, loss = 1.63615386\n",
      "Iteration 18318, loss = 1.63962418\n",
      "Iteration 18319, loss = 2.51636086\n",
      "Iteration 18320, loss = 1.66544047\n",
      "Iteration 18321, loss = 1.85457294\n",
      "Iteration 18322, loss = 1.46682727\n",
      "Iteration 18323, loss = 1.54559154\n",
      "Iteration 18324, loss = 1.45602217\n",
      "Iteration 18325, loss = 1.55206273\n",
      "Iteration 18326, loss = 1.48019854\n",
      "Iteration 18327, loss = 1.46204684\n",
      "Iteration 18328, loss = 1.49517763\n",
      "Iteration 18329, loss = 2.45506015\n",
      "Iteration 18330, loss = 4.34385727\n",
      "Iteration 18331, loss = 8.42463102\n",
      "Iteration 18332, loss = 4.96561310\n",
      "Iteration 18333, loss = 3.60415792\n",
      "Iteration 18334, loss = 8.51014400\n",
      "Iteration 18335, loss = 12.37218059\n",
      "Iteration 18336, loss = 12.44063660\n",
      "Iteration 18337, loss = 7.38803271\n",
      "Iteration 18338, loss = 7.72350439\n",
      "Iteration 18339, loss = 6.77986438\n",
      "Iteration 18340, loss = 5.51388071\n",
      "Iteration 18341, loss = 4.88297827\n",
      "Iteration 18342, loss = 2.51705991\n",
      "Iteration 18343, loss = 3.30517629\n",
      "Iteration 18344, loss = 3.06712680\n",
      "Iteration 18345, loss = 2.72117120\n",
      "Iteration 18346, loss = 2.35224565\n",
      "Iteration 18347, loss = 2.71590066\n",
      "Iteration 18348, loss = 1.93975805\n",
      "Iteration 18349, loss = 1.77773818\n",
      "Iteration 18350, loss = 1.76428483\n",
      "Iteration 18351, loss = 1.83948409\n",
      "Iteration 18352, loss = 1.81193641\n",
      "Iteration 18353, loss = 1.56240404\n",
      "Iteration 18354, loss = 1.58033923\n",
      "Iteration 18355, loss = 1.49529737\n",
      "Iteration 18356, loss = 1.64766569\n",
      "Iteration 18357, loss = 2.00369348\n",
      "Iteration 18358, loss = 1.81373484\n",
      "Iteration 18359, loss = 2.62371600\n",
      "Iteration 18360, loss = 2.36461273\n",
      "Iteration 18361, loss = 1.77244682\n",
      "Iteration 18362, loss = 1.59500843\n",
      "Iteration 18363, loss = 3.07571441\n",
      "Iteration 18364, loss = 6.37022537\n",
      "Iteration 18365, loss = 5.08223480\n",
      "Iteration 18366, loss = 2.46838791\n",
      "Iteration 18367, loss = 3.60611092\n",
      "Iteration 18368, loss = 3.55234710\n",
      "Iteration 18369, loss = 4.41513437\n",
      "Iteration 18370, loss = 3.05199056\n",
      "Iteration 18371, loss = 2.44924564\n",
      "Iteration 18372, loss = 2.22068869\n",
      "Iteration 18373, loss = 1.66238567\n",
      "Iteration 18374, loss = 1.63279030\n",
      "Iteration 18375, loss = 2.12745770\n",
      "Iteration 18376, loss = 3.67224433\n",
      "Iteration 18377, loss = 2.61464427\n",
      "Iteration 18378, loss = 3.01485159\n",
      "Iteration 18379, loss = 1.84226180\n",
      "Iteration 18380, loss = 1.82772868\n",
      "Iteration 18381, loss = 1.54328268\n",
      "Iteration 18382, loss = 1.50345858\n",
      "Iteration 18383, loss = 2.16593145\n",
      "Iteration 18384, loss = 1.63373577\n",
      "Iteration 18385, loss = 1.49905900\n",
      "Iteration 18386, loss = 2.43267624\n",
      "Iteration 18387, loss = 3.00138149\n",
      "Iteration 18388, loss = 2.53278279\n",
      "Iteration 18389, loss = 2.58825939\n",
      "Iteration 18390, loss = 1.56634547\n",
      "Iteration 18391, loss = 1.43834141\n",
      "Iteration 18392, loss = 1.97006849\n",
      "Iteration 18393, loss = 1.62431608\n",
      "Iteration 18394, loss = 2.00804338\n",
      "Iteration 18395, loss = 1.72976570\n",
      "Iteration 18396, loss = 1.71722548\n",
      "Iteration 18397, loss = 1.61902549\n",
      "Iteration 18398, loss = 1.81503628\n",
      "Iteration 18399, loss = 2.25162306\n",
      "Iteration 18400, loss = 1.80122391\n",
      "Iteration 18401, loss = 1.50475149\n",
      "Iteration 18402, loss = 1.72488357\n",
      "Iteration 18403, loss = 1.78345358\n",
      "Iteration 18404, loss = 1.47990330\n",
      "Iteration 18405, loss = 1.38390724\n",
      "Iteration 18406, loss = 1.49379587\n",
      "Iteration 18407, loss = 1.42529491\n",
      "Iteration 18408, loss = 1.45946577\n",
      "Iteration 18409, loss = 1.51935718\n",
      "Iteration 18410, loss = 2.65103501\n",
      "Iteration 18411, loss = 1.53368612\n",
      "Iteration 18412, loss = 1.47908782\n",
      "Iteration 18413, loss = 1.58510819\n",
      "Iteration 18414, loss = 1.82826209\n",
      "Iteration 18415, loss = 1.87242581\n",
      "Iteration 18416, loss = 1.95817133\n",
      "Iteration 18417, loss = 2.37849722\n",
      "Iteration 18418, loss = 1.70590444\n",
      "Iteration 18419, loss = 2.30063352\n",
      "Iteration 18420, loss = 1.99367111\n",
      "Iteration 18421, loss = 1.88481483\n",
      "Iteration 18422, loss = 3.66262957\n",
      "Iteration 18423, loss = 3.03026067\n",
      "Iteration 18424, loss = 1.77242228\n",
      "Iteration 18425, loss = 2.51030835\n",
      "Iteration 18426, loss = 3.09575024\n",
      "Iteration 18427, loss = 4.46568498\n",
      "Iteration 18428, loss = 2.73974859\n",
      "Iteration 18429, loss = 2.08650610\n",
      "Iteration 18430, loss = 2.10906458\n",
      "Iteration 18431, loss = 1.72029957\n",
      "Iteration 18432, loss = 1.97225185\n",
      "Iteration 18433, loss = 1.73602616\n",
      "Iteration 18434, loss = 1.91394274\n",
      "Iteration 18435, loss = 2.19279806\n",
      "Iteration 18436, loss = 1.72642426\n",
      "Iteration 18437, loss = 1.64820427\n",
      "Iteration 18438, loss = 1.62788708\n",
      "Iteration 18439, loss = 1.97454406\n",
      "Iteration 18440, loss = 3.68371817\n",
      "Iteration 18441, loss = 1.95829306\n",
      "Iteration 18442, loss = 1.91417268\n",
      "Iteration 18443, loss = 1.96387004\n",
      "Iteration 18444, loss = 2.07222323\n",
      "Iteration 18445, loss = 1.49173534\n",
      "Iteration 18446, loss = 1.56670626\n",
      "Iteration 18447, loss = 1.58536981\n",
      "Iteration 18448, loss = 1.71758693\n",
      "Iteration 18449, loss = 1.70319615\n",
      "Iteration 18450, loss = 1.40357532\n",
      "Iteration 18451, loss = 1.58686587\n",
      "Iteration 18452, loss = 1.45885346\n",
      "Iteration 18453, loss = 1.41236394\n",
      "Iteration 18454, loss = 1.50416148\n",
      "Iteration 18455, loss = 1.50933747\n",
      "Iteration 18456, loss = 2.04309830\n",
      "Iteration 18457, loss = 1.68523450\n",
      "Iteration 18458, loss = 1.70005682\n",
      "Iteration 18459, loss = 1.92431109\n",
      "Iteration 18460, loss = 1.78915301\n",
      "Iteration 18461, loss = 1.53701399\n",
      "Iteration 18462, loss = 1.43416883\n",
      "Iteration 18463, loss = 1.42339904\n",
      "Iteration 18464, loss = 1.70215733\n",
      "Iteration 18465, loss = 1.89597618\n",
      "Iteration 18466, loss = 2.08477614\n",
      "Iteration 18467, loss = 1.90215085\n",
      "Iteration 18468, loss = 1.98166185\n",
      "Iteration 18469, loss = 2.07152715\n",
      "Iteration 18470, loss = 1.78330148\n",
      "Iteration 18471, loss = 1.35326036\n",
      "Iteration 18472, loss = 2.48998700\n",
      "Iteration 18473, loss = 2.97942512\n",
      "Iteration 18474, loss = 1.95184186\n",
      "Iteration 18475, loss = 1.61422373\n",
      "Iteration 18476, loss = 1.79136219\n",
      "Iteration 18477, loss = 1.67087507\n",
      "Iteration 18478, loss = 2.10367270\n",
      "Iteration 18479, loss = 2.51121257\n",
      "Iteration 18480, loss = 1.68880554\n",
      "Iteration 18481, loss = 2.46647905\n",
      "Iteration 18482, loss = 1.95631178\n",
      "Iteration 18483, loss = 1.75177222\n",
      "Iteration 18484, loss = 1.54943588\n",
      "Iteration 18485, loss = 1.56651257\n",
      "Iteration 18486, loss = 2.30842543\n",
      "Iteration 18487, loss = 3.02408702\n",
      "Iteration 18488, loss = 4.50556716\n",
      "Iteration 18489, loss = 6.42380343\n",
      "Iteration 18490, loss = 3.20776838\n",
      "Iteration 18491, loss = 2.80152412\n",
      "Iteration 18492, loss = 2.78120627\n",
      "Iteration 18493, loss = 3.38233023\n",
      "Iteration 18494, loss = 2.78005443\n",
      "Iteration 18495, loss = 2.68811447\n",
      "Iteration 18496, loss = 2.28995699\n",
      "Iteration 18497, loss = 2.48584723\n",
      "Iteration 18498, loss = 2.36895806\n",
      "Iteration 18499, loss = 1.97216154\n",
      "Iteration 18500, loss = 1.88110397\n",
      "Iteration 18501, loss = 1.99376321\n",
      "Iteration 18502, loss = 1.75698143\n",
      "Iteration 18503, loss = 1.52439913\n",
      "Iteration 18504, loss = 1.54480649\n",
      "Iteration 18505, loss = 1.66241641\n",
      "Iteration 18506, loss = 2.07588164\n",
      "Iteration 18507, loss = 2.18022206\n",
      "Iteration 18508, loss = 1.58863751\n",
      "Iteration 18509, loss = 1.46270892\n",
      "Iteration 18510, loss = 1.36841295\n",
      "Iteration 18511, loss = 1.45773842\n",
      "Iteration 18512, loss = 1.53068650\n",
      "Iteration 18513, loss = 1.53230999\n",
      "Iteration 18514, loss = 1.45356882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18515, loss = 2.06541018\n",
      "Iteration 18516, loss = 1.44608869\n",
      "Iteration 18517, loss = 1.41231628\n",
      "Iteration 18518, loss = 1.58620595\n",
      "Iteration 18519, loss = 1.55261536\n",
      "Iteration 18520, loss = 1.66646358\n",
      "Iteration 18521, loss = 1.39167437\n",
      "Iteration 18522, loss = 1.40040528\n",
      "Iteration 18523, loss = 1.44098847\n",
      "Iteration 18524, loss = 1.45722641\n",
      "Iteration 18525, loss = 1.59276249\n",
      "Iteration 18526, loss = 1.54302600\n",
      "Iteration 18527, loss = 1.92730036\n",
      "Iteration 18528, loss = 1.93486085\n",
      "Iteration 18529, loss = 2.30768400\n",
      "Iteration 18530, loss = 1.70024831\n",
      "Iteration 18531, loss = 2.09512734\n",
      "Iteration 18532, loss = 1.68821706\n",
      "Iteration 18533, loss = 1.43707447\n",
      "Iteration 18534, loss = 1.95741697\n",
      "Iteration 18535, loss = 2.07629728\n",
      "Iteration 18536, loss = 2.37181929\n",
      "Iteration 18537, loss = 1.69638767\n",
      "Iteration 18538, loss = 1.95329361\n",
      "Iteration 18539, loss = 2.13975541\n",
      "Iteration 18540, loss = 2.06306454\n",
      "Iteration 18541, loss = 1.67220818\n",
      "Iteration 18542, loss = 1.50439857\n",
      "Iteration 18543, loss = 1.49867394\n",
      "Iteration 18544, loss = 1.52382801\n",
      "Iteration 18545, loss = 1.49727144\n",
      "Iteration 18546, loss = 1.78568906\n",
      "Iteration 18547, loss = 1.56677447\n",
      "Iteration 18548, loss = 1.66022283\n",
      "Iteration 18549, loss = 3.39459471\n",
      "Iteration 18550, loss = 3.11251385\n",
      "Iteration 18551, loss = 3.58769470\n",
      "Iteration 18552, loss = 2.11195052\n",
      "Iteration 18553, loss = 1.76091269\n",
      "Iteration 18554, loss = 1.58475154\n",
      "Iteration 18555, loss = 1.57573099\n",
      "Iteration 18556, loss = 1.56519593\n",
      "Iteration 18557, loss = 1.46284641\n",
      "Iteration 18558, loss = 1.45014314\n",
      "Iteration 18559, loss = 1.43530177\n",
      "Iteration 18560, loss = 1.66768922\n",
      "Iteration 18561, loss = 1.75675845\n",
      "Iteration 18562, loss = 1.95112681\n",
      "Iteration 18563, loss = 1.51761247\n",
      "Iteration 18564, loss = 1.64288530\n",
      "Iteration 18565, loss = 1.57723288\n",
      "Iteration 18566, loss = 1.54484269\n",
      "Iteration 18567, loss = 2.49364323\n",
      "Iteration 18568, loss = 3.55601571\n",
      "Iteration 18569, loss = 2.26237753\n",
      "Iteration 18570, loss = 2.22535300\n",
      "Iteration 18571, loss = 1.96738036\n",
      "Iteration 18572, loss = 1.81036328\n",
      "Iteration 18573, loss = 2.07570763\n",
      "Iteration 18574, loss = 1.57925131\n",
      "Iteration 18575, loss = 1.67132730\n",
      "Iteration 18576, loss = 1.52682580\n",
      "Iteration 18577, loss = 1.51416594\n",
      "Iteration 18578, loss = 1.49918728\n",
      "Iteration 18579, loss = 1.50208015\n",
      "Iteration 18580, loss = 1.67327315\n",
      "Iteration 18581, loss = 1.42766783\n",
      "Iteration 18582, loss = 1.50813248\n",
      "Iteration 18583, loss = 1.62699871\n",
      "Iteration 18584, loss = 1.51025728\n",
      "Iteration 18585, loss = 1.54332912\n",
      "Iteration 18586, loss = 2.01299854\n",
      "Iteration 18587, loss = 1.98515347\n",
      "Iteration 18588, loss = 1.66904705\n",
      "Iteration 18589, loss = 1.50668841\n",
      "Iteration 18590, loss = 1.43283739\n",
      "Iteration 18591, loss = 2.02639707\n",
      "Iteration 18592, loss = 3.15984519\n",
      "Iteration 18593, loss = 1.67772807\n",
      "Iteration 18594, loss = 1.71087606\n",
      "Iteration 18595, loss = 1.43669203\n",
      "Iteration 18596, loss = 1.38552243\n",
      "Iteration 18597, loss = 1.38768609\n",
      "Iteration 18598, loss = 1.45426923\n",
      "Iteration 18599, loss = 1.59965089\n",
      "Iteration 18600, loss = 1.70954534\n",
      "Iteration 18601, loss = 1.50855353\n",
      "Iteration 18602, loss = 1.54853029\n",
      "Iteration 18603, loss = 1.85304826\n",
      "Iteration 18604, loss = 2.00799052\n",
      "Iteration 18605, loss = 1.46290903\n",
      "Iteration 18606, loss = 2.02365478\n",
      "Iteration 18607, loss = 2.69276113\n",
      "Iteration 18608, loss = 1.83619276\n",
      "Iteration 18609, loss = 2.15681088\n",
      "Iteration 18610, loss = 1.66499248\n",
      "Iteration 18611, loss = 1.70230422\n",
      "Iteration 18612, loss = 1.80611963\n",
      "Iteration 18613, loss = 1.58770698\n",
      "Iteration 18614, loss = 1.55816496\n",
      "Iteration 18615, loss = 1.67896721\n",
      "Iteration 18616, loss = 1.76254184\n",
      "Iteration 18617, loss = 1.40857257\n",
      "Iteration 18618, loss = 1.53651759\n",
      "Iteration 18619, loss = 1.73566715\n",
      "Iteration 18620, loss = 2.27489463\n",
      "Iteration 18621, loss = 2.18798114\n",
      "Iteration 18622, loss = 4.13462823\n",
      "Iteration 18623, loss = 2.31544518\n",
      "Iteration 18624, loss = 1.70197035\n",
      "Iteration 18625, loss = 1.78623260\n",
      "Iteration 18626, loss = 2.02130802\n",
      "Iteration 18627, loss = 1.65272520\n",
      "Iteration 18628, loss = 1.70325926\n",
      "Iteration 18629, loss = 1.73350753\n",
      "Iteration 18630, loss = 1.83189342\n",
      "Iteration 18631, loss = 1.94935849\n",
      "Iteration 18632, loss = 3.91836629\n",
      "Iteration 18633, loss = 2.57636357\n",
      "Iteration 18634, loss = 2.33460691\n",
      "Iteration 18635, loss = 2.33236273\n",
      "Iteration 18636, loss = 2.29283500\n",
      "Iteration 18637, loss = 1.56419934\n",
      "Iteration 18638, loss = 1.69378478\n",
      "Iteration 18639, loss = 1.65910579\n",
      "Iteration 18640, loss = 1.51402961\n",
      "Iteration 18641, loss = 1.82271271\n",
      "Iteration 18642, loss = 2.27793993\n",
      "Iteration 18643, loss = 1.65053742\n",
      "Iteration 18644, loss = 2.81008539\n",
      "Iteration 18645, loss = 2.60130671\n",
      "Iteration 18646, loss = 2.26601680\n",
      "Iteration 18647, loss = 1.72723548\n",
      "Iteration 18648, loss = 1.60695400\n",
      "Iteration 18649, loss = 1.60902748\n",
      "Iteration 18650, loss = 1.85909621\n",
      "Iteration 18651, loss = 3.08119887\n",
      "Iteration 18652, loss = 1.98419759\n",
      "Iteration 18653, loss = 2.38209913\n",
      "Iteration 18654, loss = 2.50949106\n",
      "Iteration 18655, loss = 2.86004902\n",
      "Iteration 18656, loss = 2.18396635\n",
      "Iteration 18657, loss = 2.06937572\n",
      "Iteration 18658, loss = 1.55178327\n",
      "Iteration 18659, loss = 1.56383742\n",
      "Iteration 18660, loss = 1.74962246\n",
      "Iteration 18661, loss = 1.51914853\n",
      "Iteration 18662, loss = 1.92127147\n",
      "Iteration 18663, loss = 1.85575970\n",
      "Iteration 18664, loss = 2.64073979\n",
      "Iteration 18665, loss = 2.05653870\n",
      "Iteration 18666, loss = 3.88929914\n",
      "Iteration 18667, loss = 2.48907505\n",
      "Iteration 18668, loss = 1.80667418\n",
      "Iteration 18669, loss = 1.53357920\n",
      "Iteration 18670, loss = 1.77270795\n",
      "Iteration 18671, loss = 1.58967191\n",
      "Iteration 18672, loss = 1.76480173\n",
      "Iteration 18673, loss = 1.56851414\n",
      "Iteration 18674, loss = 4.62249638\n",
      "Iteration 18675, loss = 7.91936204\n",
      "Iteration 18676, loss = 4.39054006\n",
      "Iteration 18677, loss = 3.42049136\n",
      "Iteration 18678, loss = 3.02999343\n",
      "Iteration 18679, loss = 2.36538596\n",
      "Iteration 18680, loss = 1.61463643\n",
      "Iteration 18681, loss = 2.12659023\n",
      "Iteration 18682, loss = 1.97470468\n",
      "Iteration 18683, loss = 2.09817857\n",
      "Iteration 18684, loss = 2.20853335\n",
      "Iteration 18685, loss = 1.69143378\n",
      "Iteration 18686, loss = 1.42835532\n",
      "Iteration 18687, loss = 1.73513959\n",
      "Iteration 18688, loss = 1.62879149\n",
      "Iteration 18689, loss = 1.71666063\n",
      "Iteration 18690, loss = 2.62009089\n",
      "Iteration 18691, loss = 4.41928485\n",
      "Iteration 18692, loss = 3.19609054\n",
      "Iteration 18693, loss = 2.62810756\n",
      "Iteration 18694, loss = 2.72537634\n",
      "Iteration 18695, loss = 2.36698313\n",
      "Iteration 18696, loss = 2.03929477\n",
      "Iteration 18697, loss = 2.72769549\n",
      "Iteration 18698, loss = 2.03567861\n",
      "Iteration 18699, loss = 2.09083021\n",
      "Iteration 18700, loss = 1.56449149\n",
      "Iteration 18701, loss = 2.00189397\n",
      "Iteration 18702, loss = 2.02119653\n",
      "Iteration 18703, loss = 1.51458647\n",
      "Iteration 18704, loss = 1.45588418\n",
      "Iteration 18705, loss = 1.60590583\n",
      "Iteration 18706, loss = 1.91152397\n",
      "Iteration 18707, loss = 1.60743580\n",
      "Iteration 18708, loss = 1.93892445\n",
      "Iteration 18709, loss = 2.35512659\n",
      "Iteration 18710, loss = 1.51583923\n",
      "Iteration 18711, loss = 1.68088270\n",
      "Iteration 18712, loss = 2.97811211\n",
      "Iteration 18713, loss = 3.49670337\n",
      "Iteration 18714, loss = 2.41140074\n",
      "Iteration 18715, loss = 2.11389205\n",
      "Iteration 18716, loss = 1.79711220\n",
      "Iteration 18717, loss = 1.79238524\n",
      "Iteration 18718, loss = 1.72741612\n",
      "Iteration 18719, loss = 1.84984410\n",
      "Iteration 18720, loss = 2.20906542\n",
      "Iteration 18721, loss = 2.20305339\n",
      "Iteration 18722, loss = 1.48330235\n",
      "Iteration 18723, loss = 2.12761836\n",
      "Iteration 18724, loss = 2.46395833\n",
      "Iteration 18725, loss = 3.14828540\n",
      "Iteration 18726, loss = 4.25102960\n",
      "Iteration 18727, loss = 3.32321172\n",
      "Iteration 18728, loss = 2.73812987\n",
      "Iteration 18729, loss = 1.59493539\n",
      "Iteration 18730, loss = 1.79268325\n",
      "Iteration 18731, loss = 1.50943498\n",
      "Iteration 18732, loss = 1.49376791\n",
      "Iteration 18733, loss = 1.46687499\n",
      "Iteration 18734, loss = 1.43889020\n",
      "Iteration 18735, loss = 1.63535666\n",
      "Iteration 18736, loss = 1.73811942\n",
      "Iteration 18737, loss = 2.36520638\n",
      "Iteration 18738, loss = 1.79804507\n",
      "Iteration 18739, loss = 2.11383117\n",
      "Iteration 18740, loss = 1.57012231\n",
      "Iteration 18741, loss = 1.84858172\n",
      "Iteration 18742, loss = 1.57918982\n",
      "Iteration 18743, loss = 1.52448161\n",
      "Iteration 18744, loss = 1.44606032\n",
      "Iteration 18745, loss = 1.36508655\n",
      "Iteration 18746, loss = 1.37210242\n",
      "Iteration 18747, loss = 1.39555927\n",
      "Iteration 18748, loss = 1.41621388\n",
      "Iteration 18749, loss = 1.45394643\n",
      "Iteration 18750, loss = 2.39119315\n",
      "Iteration 18751, loss = 3.68595341\n",
      "Iteration 18752, loss = 2.58081021\n",
      "Iteration 18753, loss = 2.22313722\n",
      "Iteration 18754, loss = 1.43244168\n",
      "Iteration 18755, loss = 1.69487182\n",
      "Iteration 18756, loss = 1.51768362\n",
      "Iteration 18757, loss = 1.37566682\n",
      "Iteration 18758, loss = 1.59401733\n",
      "Iteration 18759, loss = 1.86644527\n",
      "Iteration 18760, loss = 1.72757238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18761, loss = 1.43941140\n",
      "Iteration 18762, loss = 1.45778224\n",
      "Iteration 18763, loss = 1.41636666\n",
      "Iteration 18764, loss = 1.65580165\n",
      "Iteration 18765, loss = 1.89567405\n",
      "Iteration 18766, loss = 1.64319016\n",
      "Iteration 18767, loss = 1.32448661\n",
      "Iteration 18768, loss = 1.67353446\n",
      "Iteration 18769, loss = 1.92219411\n",
      "Iteration 18770, loss = 1.98370976\n",
      "Iteration 18771, loss = 1.46164806\n",
      "Iteration 18772, loss = 1.42273693\n",
      "Iteration 18773, loss = 1.37532281\n",
      "Iteration 18774, loss = 1.47806392\n",
      "Iteration 18775, loss = 1.68175492\n",
      "Iteration 18776, loss = 2.54334965\n",
      "Iteration 18777, loss = 2.58249359\n",
      "Iteration 18778, loss = 1.82283582\n",
      "Iteration 18779, loss = 1.75387652\n",
      "Iteration 18780, loss = 1.87826295\n",
      "Iteration 18781, loss = 2.74243710\n",
      "Iteration 18782, loss = 3.66396779\n",
      "Iteration 18783, loss = 3.30100603\n",
      "Iteration 18784, loss = 3.95534229\n",
      "Iteration 18785, loss = 3.07029029\n",
      "Iteration 18786, loss = 2.66684614\n",
      "Iteration 18787, loss = 2.88508856\n",
      "Iteration 18788, loss = 1.96315536\n",
      "Iteration 18789, loss = 2.39643411\n",
      "Iteration 18790, loss = 1.85116856\n",
      "Iteration 18791, loss = 1.96761127\n",
      "Iteration 18792, loss = 1.64856141\n",
      "Iteration 18793, loss = 2.08809861\n",
      "Iteration 18794, loss = 1.53744829\n",
      "Iteration 18795, loss = 1.42958066\n",
      "Iteration 18796, loss = 1.35543675\n",
      "Iteration 18797, loss = 1.49835163\n",
      "Iteration 18798, loss = 1.41168426\n",
      "Iteration 18799, loss = 1.43685695\n",
      "Iteration 18800, loss = 1.80774326\n",
      "Iteration 18801, loss = 1.75530183\n",
      "Iteration 18802, loss = 1.55522534\n",
      "Iteration 18803, loss = 1.44088796\n",
      "Iteration 18804, loss = 1.41937012\n",
      "Iteration 18805, loss = 1.38304463\n",
      "Iteration 18806, loss = 1.60894113\n",
      "Iteration 18807, loss = 1.67978183\n",
      "Iteration 18808, loss = 3.56248086\n",
      "Iteration 18809, loss = 5.95086371\n",
      "Iteration 18810, loss = 3.09424270\n",
      "Iteration 18811, loss = 2.34954861\n",
      "Iteration 18812, loss = 1.72825523\n",
      "Iteration 18813, loss = 1.91451001\n",
      "Iteration 18814, loss = 1.43200390\n",
      "Iteration 18815, loss = 2.06309195\n",
      "Iteration 18816, loss = 2.03132335\n",
      "Iteration 18817, loss = 2.05619590\n",
      "Iteration 18818, loss = 2.39733332\n",
      "Iteration 18819, loss = 1.83468756\n",
      "Iteration 18820, loss = 1.78737540\n",
      "Iteration 18821, loss = 2.06870264\n",
      "Iteration 18822, loss = 1.62121912\n",
      "Iteration 18823, loss = 1.49187840\n",
      "Iteration 18824, loss = 1.84921432\n",
      "Iteration 18825, loss = 2.03129133\n",
      "Iteration 18826, loss = 1.73666800\n",
      "Iteration 18827, loss = 1.52668617\n",
      "Iteration 18828, loss = 1.93048563\n",
      "Iteration 18829, loss = 1.72624662\n",
      "Iteration 18830, loss = 2.18155025\n",
      "Iteration 18831, loss = 2.07682880\n",
      "Iteration 18832, loss = 2.16270059\n",
      "Iteration 18833, loss = 1.62009838\n",
      "Iteration 18834, loss = 1.68673260\n",
      "Iteration 18835, loss = 2.66570502\n",
      "Iteration 18836, loss = 4.28198159\n",
      "Iteration 18837, loss = 5.55466426\n",
      "Iteration 18838, loss = 4.96119438\n",
      "Iteration 18839, loss = 8.26450808\n",
      "Iteration 18840, loss = 11.06860676\n",
      "Iteration 18841, loss = 7.06258987\n",
      "Iteration 18842, loss = 4.71339968\n",
      "Iteration 18843, loss = 5.50256337\n",
      "Iteration 18844, loss = 2.94118142\n",
      "Iteration 18845, loss = 3.21111366\n",
      "Iteration 18846, loss = 2.35155647\n",
      "Iteration 18847, loss = 2.02616733\n",
      "Iteration 18848, loss = 1.95952297\n",
      "Iteration 18849, loss = 1.96247538\n",
      "Iteration 18850, loss = 1.66981664\n",
      "Iteration 18851, loss = 2.14972190\n",
      "Iteration 18852, loss = 1.42160829\n",
      "Iteration 18853, loss = 1.59804442\n",
      "Iteration 18854, loss = 1.39444901\n",
      "Iteration 18855, loss = 1.43375890\n",
      "Iteration 18856, loss = 1.41631565\n",
      "Iteration 18857, loss = 1.35096850\n",
      "Iteration 18858, loss = 1.39770746\n",
      "Iteration 18859, loss = 1.61475074\n",
      "Iteration 18860, loss = 1.34714989\n",
      "Iteration 18861, loss = 1.40583201\n",
      "Iteration 18862, loss = 1.44940310\n",
      "Iteration 18863, loss = 1.44831376\n",
      "Iteration 18864, loss = 1.38460875\n",
      "Iteration 18865, loss = 1.45674005\n",
      "Iteration 18866, loss = 1.45806538\n",
      "Iteration 18867, loss = 1.38396364\n",
      "Iteration 18868, loss = 1.60033875\n",
      "Iteration 18869, loss = 1.95795442\n",
      "Iteration 18870, loss = 1.88887603\n",
      "Iteration 18871, loss = 1.58352357\n",
      "Iteration 18872, loss = 1.37263330\n",
      "Iteration 18873, loss = 1.46818898\n",
      "Iteration 18874, loss = 1.57420474\n",
      "Iteration 18875, loss = 1.53228675\n",
      "Iteration 18876, loss = 1.46747696\n",
      "Iteration 18877, loss = 2.04577745\n",
      "Iteration 18878, loss = 1.81665832\n",
      "Iteration 18879, loss = 1.30800038\n",
      "Iteration 18880, loss = 1.48791836\n",
      "Iteration 18881, loss = 1.57160578\n",
      "Iteration 18882, loss = 1.86119384\n",
      "Iteration 18883, loss = 1.47114381\n",
      "Iteration 18884, loss = 1.94076371\n",
      "Iteration 18885, loss = 1.68805502\n",
      "Iteration 18886, loss = 1.96775012\n",
      "Iteration 18887, loss = 1.86879301\n",
      "Iteration 18888, loss = 2.84992691\n",
      "Iteration 18889, loss = 2.06309781\n",
      "Iteration 18890, loss = 1.56693453\n",
      "Iteration 18891, loss = 1.52200045\n",
      "Iteration 18892, loss = 1.87309069\n",
      "Iteration 18893, loss = 1.62748494\n",
      "Iteration 18894, loss = 1.75888882\n",
      "Iteration 18895, loss = 1.94891794\n",
      "Iteration 18896, loss = 3.33353156\n",
      "Iteration 18897, loss = 2.40818843\n",
      "Iteration 18898, loss = 1.58695319\n",
      "Iteration 18899, loss = 1.65074053\n",
      "Iteration 18900, loss = 1.62837795\n",
      "Iteration 18901, loss = 1.56790099\n",
      "Iteration 18902, loss = 2.00900783\n",
      "Iteration 18903, loss = 1.39357808\n",
      "Iteration 18904, loss = 1.36125100\n",
      "Iteration 18905, loss = 1.47882668\n",
      "Iteration 18906, loss = 1.65948551\n",
      "Iteration 18907, loss = 2.59298393\n",
      "Iteration 18908, loss = 1.94762040\n",
      "Iteration 18909, loss = 2.08997196\n",
      "Iteration 18910, loss = 2.15445905\n",
      "Iteration 18911, loss = 2.51239768\n",
      "Iteration 18912, loss = 1.65305735\n",
      "Iteration 18913, loss = 1.84782873\n",
      "Iteration 18914, loss = 1.45309076\n",
      "Iteration 18915, loss = 1.60945580\n",
      "Iteration 18916, loss = 1.29847044\n",
      "Iteration 18917, loss = 1.77163099\n",
      "Iteration 18918, loss = 1.89594524\n",
      "Iteration 18919, loss = 1.74967389\n",
      "Iteration 18920, loss = 1.67057525\n",
      "Iteration 18921, loss = 1.64838112\n",
      "Iteration 18922, loss = 2.20304848\n",
      "Iteration 18923, loss = 1.52645607\n",
      "Iteration 18924, loss = 1.86088750\n",
      "Iteration 18925, loss = 1.31613428\n",
      "Iteration 18926, loss = 1.87491373\n",
      "Iteration 18927, loss = 1.55739646\n",
      "Iteration 18928, loss = 1.58663286\n",
      "Iteration 18929, loss = 1.46994360\n",
      "Iteration 18930, loss = 1.71691445\n",
      "Iteration 18931, loss = 1.46807454\n",
      "Iteration 18932, loss = 1.79930463\n",
      "Iteration 18933, loss = 1.63537096\n",
      "Iteration 18934, loss = 1.74962045\n",
      "Iteration 18935, loss = 1.51156885\n",
      "Iteration 18936, loss = 1.42868396\n",
      "Iteration 18937, loss = 1.35133295\n",
      "Iteration 18938, loss = 1.40210759\n",
      "Iteration 18939, loss = 1.83239075\n",
      "Iteration 18940, loss = 1.41401874\n",
      "Iteration 18941, loss = 1.40219632\n",
      "Iteration 18942, loss = 1.57334012\n",
      "Iteration 18943, loss = 3.51334725\n",
      "Iteration 18944, loss = 2.32925468\n",
      "Iteration 18945, loss = 2.53454451\n",
      "Iteration 18946, loss = 1.63552048\n",
      "Iteration 18947, loss = 1.44522102\n",
      "Iteration 18948, loss = 1.47291013\n",
      "Iteration 18949, loss = 1.43066835\n",
      "Iteration 18950, loss = 1.51016375\n",
      "Iteration 18951, loss = 1.48158031\n",
      "Iteration 18952, loss = 1.50075750\n",
      "Iteration 18953, loss = 1.42128329\n",
      "Iteration 18954, loss = 1.54756010\n",
      "Iteration 18955, loss = 1.32191835\n",
      "Iteration 18956, loss = 1.34875981\n",
      "Iteration 18957, loss = 1.38542413\n",
      "Iteration 18958, loss = 1.46641635\n",
      "Iteration 18959, loss = 1.66205913\n",
      "Iteration 18960, loss = 2.20872090\n",
      "Iteration 18961, loss = 2.49487584\n",
      "Iteration 18962, loss = 3.54562681\n",
      "Iteration 18963, loss = 2.42766436\n",
      "Iteration 18964, loss = 2.85845613\n",
      "Iteration 18965, loss = 2.19220581\n",
      "Iteration 18966, loss = 2.26936112\n",
      "Iteration 18967, loss = 3.12510965\n",
      "Iteration 18968, loss = 1.39057544\n",
      "Iteration 18969, loss = 1.48397412\n",
      "Iteration 18970, loss = 2.05935072\n",
      "Iteration 18971, loss = 2.48682949\n",
      "Iteration 18972, loss = 3.53834206\n",
      "Iteration 18973, loss = 3.49526790\n",
      "Iteration 18974, loss = 2.04475855\n",
      "Iteration 18975, loss = 2.08065134\n",
      "Iteration 18976, loss = 1.85333725\n",
      "Iteration 18977, loss = 1.45896512\n",
      "Iteration 18978, loss = 1.85103041\n",
      "Iteration 18979, loss = 2.11948660\n",
      "Iteration 18980, loss = 3.09803053\n",
      "Iteration 18981, loss = 4.60815445\n",
      "Iteration 18982, loss = 3.11224667\n",
      "Iteration 18983, loss = 2.42362992\n",
      "Iteration 18984, loss = 2.30667281\n",
      "Iteration 18985, loss = 2.06283858\n",
      "Iteration 18986, loss = 2.09677088\n",
      "Iteration 18987, loss = 2.13899082\n",
      "Iteration 18988, loss = 1.89997383\n",
      "Iteration 18989, loss = 1.41214861\n",
      "Iteration 18990, loss = 1.48970521\n",
      "Iteration 18991, loss = 1.44768437\n",
      "Iteration 18992, loss = 1.92042747\n",
      "Iteration 18993, loss = 2.20804780\n",
      "Iteration 18994, loss = 1.94336941\n",
      "Iteration 18995, loss = 3.91772936\n",
      "Iteration 18996, loss = 2.22168294\n",
      "Iteration 18997, loss = 2.06764420\n",
      "Iteration 18998, loss = 2.16890258\n",
      "Iteration 18999, loss = 2.20842532\n",
      "Iteration 19000, loss = 2.65182740\n",
      "Iteration 19001, loss = 2.05757602\n",
      "Iteration 19002, loss = 3.60920584\n",
      "Iteration 19003, loss = 2.43324377\n",
      "Iteration 19004, loss = 2.31070086\n",
      "Iteration 19005, loss = 1.54221672\n",
      "Iteration 19006, loss = 1.47610418\n",
      "Iteration 19007, loss = 1.38327475\n",
      "Iteration 19008, loss = 1.67868139\n",
      "Iteration 19009, loss = 1.66375268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19010, loss = 2.45385155\n",
      "Iteration 19011, loss = 1.94303788\n",
      "Iteration 19012, loss = 2.29736724\n",
      "Iteration 19013, loss = 1.51577889\n",
      "Iteration 19014, loss = 1.63059329\n",
      "Iteration 19015, loss = 1.44733375\n",
      "Iteration 19016, loss = 1.35635509\n",
      "Iteration 19017, loss = 1.53650638\n",
      "Iteration 19018, loss = 2.14927085\n",
      "Iteration 19019, loss = 2.07176740\n",
      "Iteration 19020, loss = 2.34024864\n",
      "Iteration 19021, loss = 1.81743924\n",
      "Iteration 19022, loss = 1.65226130\n",
      "Iteration 19023, loss = 2.52339818\n",
      "Iteration 19024, loss = 1.99809990\n",
      "Iteration 19025, loss = 1.57507875\n",
      "Iteration 19026, loss = 1.63911030\n",
      "Iteration 19027, loss = 1.42388413\n",
      "Iteration 19028, loss = 1.70274656\n",
      "Iteration 19029, loss = 1.41477464\n",
      "Iteration 19030, loss = 1.53349802\n",
      "Iteration 19031, loss = 1.30969764\n",
      "Iteration 19032, loss = 1.35559899\n",
      "Iteration 19033, loss = 1.35952545\n",
      "Iteration 19034, loss = 1.73025912\n",
      "Iteration 19035, loss = 1.32444797\n",
      "Iteration 19036, loss = 1.31515016\n",
      "Iteration 19037, loss = 1.31021809\n",
      "Iteration 19038, loss = 1.30809592\n",
      "Iteration 19039, loss = 1.44453087\n",
      "Iteration 19040, loss = 1.56615438\n",
      "Iteration 19041, loss = 1.40508826\n",
      "Iteration 19042, loss = 2.13895615\n",
      "Iteration 19043, loss = 2.20690769\n",
      "Iteration 19044, loss = 1.61681196\n",
      "Iteration 19045, loss = 1.89732514\n",
      "Iteration 19046, loss = 2.68651143\n",
      "Iteration 19047, loss = 2.10945043\n",
      "Iteration 19048, loss = 2.88189094\n",
      "Iteration 19049, loss = 2.90244048\n",
      "Iteration 19050, loss = 3.04876035\n",
      "Iteration 19051, loss = 1.68591283\n",
      "Iteration 19052, loss = 2.21300762\n",
      "Iteration 19053, loss = 1.65453637\n",
      "Iteration 19054, loss = 1.43880157\n",
      "Iteration 19055, loss = 1.47543387\n",
      "Iteration 19056, loss = 1.45089567\n",
      "Iteration 19057, loss = 1.54213524\n",
      "Iteration 19058, loss = 2.58177486\n",
      "Iteration 19059, loss = 1.95292602\n",
      "Iteration 19060, loss = 1.71054952\n",
      "Iteration 19061, loss = 1.36858878\n",
      "Iteration 19062, loss = 1.43620453\n",
      "Iteration 19063, loss = 1.62994309\n",
      "Iteration 19064, loss = 1.59054180\n",
      "Iteration 19065, loss = 1.43565413\n",
      "Iteration 19066, loss = 1.86028024\n",
      "Iteration 19067, loss = 1.47729332\n",
      "Iteration 19068, loss = 1.64526429\n",
      "Iteration 19069, loss = 1.87848043\n",
      "Iteration 19070, loss = 1.42958059\n",
      "Iteration 19071, loss = 1.44215826\n",
      "Iteration 19072, loss = 1.46900455\n",
      "Iteration 19073, loss = 1.37609739\n",
      "Iteration 19074, loss = 1.38956715\n",
      "Iteration 19075, loss = 1.41447324\n",
      "Iteration 19076, loss = 1.37813344\n",
      "Iteration 19077, loss = 1.80425879\n",
      "Iteration 19078, loss = 3.92542175\n",
      "Iteration 19079, loss = 2.75704540\n",
      "Iteration 19080, loss = 2.33492279\n",
      "Iteration 19081, loss = 2.50991815\n",
      "Iteration 19082, loss = 1.96094489\n",
      "Iteration 19083, loss = 1.97539400\n",
      "Iteration 19084, loss = 1.85802586\n",
      "Iteration 19085, loss = 1.66578499\n",
      "Iteration 19086, loss = 2.01694200\n",
      "Iteration 19087, loss = 1.61850301\n",
      "Iteration 19088, loss = 2.09999404\n",
      "Iteration 19089, loss = 1.83450739\n",
      "Iteration 19090, loss = 2.62722496\n",
      "Iteration 19091, loss = 1.66509293\n",
      "Iteration 19092, loss = 1.54833112\n",
      "Iteration 19093, loss = 1.46604180\n",
      "Iteration 19094, loss = 2.19020757\n",
      "Iteration 19095, loss = 1.59947033\n",
      "Iteration 19096, loss = 1.95162619\n",
      "Iteration 19097, loss = 1.74261412\n",
      "Iteration 19098, loss = 2.16056056\n",
      "Iteration 19099, loss = 1.44590838\n",
      "Iteration 19100, loss = 1.46103750\n",
      "Iteration 19101, loss = 1.36514670\n",
      "Iteration 19102, loss = 1.30134778\n",
      "Iteration 19103, loss = 1.48641714\n",
      "Iteration 19104, loss = 1.51135718\n",
      "Iteration 19105, loss = 1.50144809\n",
      "Iteration 19106, loss = 1.69864263\n",
      "Iteration 19107, loss = 1.55818870\n",
      "Iteration 19108, loss = 1.42740682\n",
      "Iteration 19109, loss = 1.31412865\n",
      "Iteration 19110, loss = 1.36691469\n",
      "Iteration 19111, loss = 1.29458249\n",
      "Iteration 19112, loss = 1.46388105\n",
      "Iteration 19113, loss = 1.37967214\n",
      "Iteration 19114, loss = 1.53833239\n",
      "Iteration 19115, loss = 1.41807640\n",
      "Iteration 19116, loss = 1.34636188\n",
      "Iteration 19117, loss = 1.38240186\n",
      "Iteration 19118, loss = 1.43914923\n",
      "Iteration 19119, loss = 1.45064465\n",
      "Iteration 19120, loss = 1.51425784\n",
      "Iteration 19121, loss = 1.70648994\n",
      "Iteration 19122, loss = 1.62576749\n",
      "Iteration 19123, loss = 2.03764025\n",
      "Iteration 19124, loss = 3.05342518\n",
      "Iteration 19125, loss = 2.35292413\n",
      "Iteration 19126, loss = 1.48648187\n",
      "Iteration 19127, loss = 1.78571716\n",
      "Iteration 19128, loss = 1.73361343\n",
      "Iteration 19129, loss = 1.81860889\n",
      "Iteration 19130, loss = 3.29504935\n",
      "Iteration 19131, loss = 1.85860015\n",
      "Iteration 19132, loss = 1.82590360\n",
      "Iteration 19133, loss = 1.47051028\n",
      "Iteration 19134, loss = 1.85863209\n",
      "Iteration 19135, loss = 1.41665667\n",
      "Iteration 19136, loss = 1.43198612\n",
      "Iteration 19137, loss = 1.63397313\n",
      "Iteration 19138, loss = 1.49038789\n",
      "Iteration 19139, loss = 1.71238305\n",
      "Iteration 19140, loss = 1.81329549\n",
      "Iteration 19141, loss = 2.11247534\n",
      "Iteration 19142, loss = 1.75149399\n",
      "Iteration 19143, loss = 1.52481972\n",
      "Iteration 19144, loss = 1.97432616\n",
      "Iteration 19145, loss = 1.70710268\n",
      "Iteration 19146, loss = 1.72635215\n",
      "Iteration 19147, loss = 1.51708206\n",
      "Iteration 19148, loss = 1.38838954\n",
      "Iteration 19149, loss = 1.37089024\n",
      "Iteration 19150, loss = 1.33214955\n",
      "Iteration 19151, loss = 1.76173929\n",
      "Iteration 19152, loss = 1.42324836\n",
      "Iteration 19153, loss = 1.47923211\n",
      "Iteration 19154, loss = 1.50553605\n",
      "Iteration 19155, loss = 1.41001061\n",
      "Iteration 19156, loss = 1.32193803\n",
      "Iteration 19157, loss = 1.30355849\n",
      "Iteration 19158, loss = 1.27906389\n",
      "Iteration 19159, loss = 1.50963165\n",
      "Iteration 19160, loss = 1.58000077\n",
      "Iteration 19161, loss = 1.37437497\n",
      "Iteration 19162, loss = 1.33495194\n",
      "Iteration 19163, loss = 1.75372898\n",
      "Iteration 19164, loss = 1.46253495\n",
      "Iteration 19165, loss = 1.27776421\n",
      "Iteration 19166, loss = 1.42623567\n",
      "Iteration 19167, loss = 1.28824955\n",
      "Iteration 19168, loss = 1.30073393\n",
      "Iteration 19169, loss = 1.29693973\n",
      "Iteration 19170, loss = 1.36256879\n",
      "Iteration 19171, loss = 1.36086008\n",
      "Iteration 19172, loss = 1.57051779\n",
      "Iteration 19173, loss = 2.63097369\n",
      "Iteration 19174, loss = 1.49449032\n",
      "Iteration 19175, loss = 1.73203753\n",
      "Iteration 19176, loss = 1.59164372\n",
      "Iteration 19177, loss = 2.16505583\n",
      "Iteration 19178, loss = 1.41519425\n",
      "Iteration 19179, loss = 1.64601149\n",
      "Iteration 19180, loss = 1.50906955\n",
      "Iteration 19181, loss = 1.70293899\n",
      "Iteration 19182, loss = 1.76235794\n",
      "Iteration 19183, loss = 1.69803882\n",
      "Iteration 19184, loss = 1.98983617\n",
      "Iteration 19185, loss = 2.00339466\n",
      "Iteration 19186, loss = 1.53579928\n",
      "Iteration 19187, loss = 1.51002038\n",
      "Iteration 19188, loss = 1.40473953\n",
      "Iteration 19189, loss = 1.32976017\n",
      "Iteration 19190, loss = 1.28395838\n",
      "Iteration 19191, loss = 1.38378766\n",
      "Iteration 19192, loss = 1.51080956\n",
      "Iteration 19193, loss = 1.48662319\n",
      "Iteration 19194, loss = 1.58428536\n",
      "Iteration 19195, loss = 2.14840491\n",
      "Iteration 19196, loss = 1.66567558\n",
      "Iteration 19197, loss = 2.91307348\n",
      "Iteration 19198, loss = 5.47298404\n",
      "Iteration 19199, loss = 3.17510319\n",
      "Iteration 19200, loss = 2.04877086\n",
      "Iteration 19201, loss = 1.93560794\n",
      "Iteration 19202, loss = 1.99740204\n",
      "Iteration 19203, loss = 1.99481561\n",
      "Iteration 19204, loss = 2.55549852\n",
      "Iteration 19205, loss = 1.62956257\n",
      "Iteration 19206, loss = 1.54139386\n",
      "Iteration 19207, loss = 1.45147445\n",
      "Iteration 19208, loss = 1.38247319\n",
      "Iteration 19209, loss = 1.40839184\n",
      "Iteration 19210, loss = 1.40579813\n",
      "Iteration 19211, loss = 1.41315374\n",
      "Iteration 19212, loss = 1.57922203\n",
      "Iteration 19213, loss = 1.43671437\n",
      "Iteration 19214, loss = 1.37618658\n",
      "Iteration 19215, loss = 1.48221479\n",
      "Iteration 19216, loss = 1.58111584\n",
      "Iteration 19217, loss = 1.35180799\n",
      "Iteration 19218, loss = 1.44389278\n",
      "Iteration 19219, loss = 1.33213679\n",
      "Iteration 19220, loss = 1.85493536\n",
      "Iteration 19221, loss = 5.72981788\n",
      "Iteration 19222, loss = 10.80847977\n",
      "Iteration 19223, loss = 8.55379272\n",
      "Iteration 19224, loss = 6.96901770\n",
      "Iteration 19225, loss = 6.32514746\n",
      "Iteration 19226, loss = 3.76917448\n",
      "Iteration 19227, loss = 2.67263757\n",
      "Iteration 19228, loss = 3.89815247\n",
      "Iteration 19229, loss = 2.89266234\n",
      "Iteration 19230, loss = 2.71598238\n",
      "Iteration 19231, loss = 2.36435940\n",
      "Iteration 19232, loss = 1.94930990\n",
      "Iteration 19233, loss = 1.60862403\n",
      "Iteration 19234, loss = 1.77213281\n",
      "Iteration 19235, loss = 1.54564500\n",
      "Iteration 19236, loss = 1.59224961\n",
      "Iteration 19237, loss = 1.76530798\n",
      "Iteration 19238, loss = 2.52405265\n",
      "Iteration 19239, loss = 1.79526909\n",
      "Iteration 19240, loss = 2.41682458\n",
      "Iteration 19241, loss = 2.24460272\n",
      "Iteration 19242, loss = 1.46203902\n",
      "Iteration 19243, loss = 1.45494241\n",
      "Iteration 19244, loss = 1.56734215\n",
      "Iteration 19245, loss = 1.51989042\n",
      "Iteration 19246, loss = 1.40787922\n",
      "Iteration 19247, loss = 1.49294700\n",
      "Iteration 19248, loss = 1.35278216\n",
      "Iteration 19249, loss = 1.42550443\n",
      "Iteration 19250, loss = 1.45211526\n",
      "Iteration 19251, loss = 1.54989756\n",
      "Iteration 19252, loss = 1.79260470\n",
      "Iteration 19253, loss = 2.03138012\n",
      "Iteration 19254, loss = 1.85697274\n",
      "Iteration 19255, loss = 1.72018853\n",
      "Iteration 19256, loss = 2.37711488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19257, loss = 1.81531690\n",
      "Iteration 19258, loss = 1.81764222\n",
      "Iteration 19259, loss = 1.42434310\n",
      "Iteration 19260, loss = 1.46603795\n",
      "Iteration 19261, loss = 1.48654157\n",
      "Iteration 19262, loss = 1.63643946\n",
      "Iteration 19263, loss = 1.57762418\n",
      "Iteration 19264, loss = 1.57803450\n",
      "Iteration 19265, loss = 1.67741559\n",
      "Iteration 19266, loss = 1.46318724\n",
      "Iteration 19267, loss = 1.54152782\n",
      "Iteration 19268, loss = 1.51340502\n",
      "Iteration 19269, loss = 1.41633860\n",
      "Iteration 19270, loss = 1.45376616\n",
      "Iteration 19271, loss = 1.43115234\n",
      "Iteration 19272, loss = 2.29245575\n",
      "Iteration 19273, loss = 1.72821129\n",
      "Iteration 19274, loss = 1.98930570\n",
      "Iteration 19275, loss = 1.57134451\n",
      "Iteration 19276, loss = 1.48020758\n",
      "Iteration 19277, loss = 1.37667924\n",
      "Iteration 19278, loss = 1.31432982\n",
      "Iteration 19279, loss = 1.30367093\n",
      "Iteration 19280, loss = 1.30412637\n",
      "Iteration 19281, loss = 1.47322196\n",
      "Iteration 19282, loss = 2.30560202\n",
      "Iteration 19283, loss = 1.96324667\n",
      "Iteration 19284, loss = 2.21667503\n",
      "Iteration 19285, loss = 1.87385179\n",
      "Iteration 19286, loss = 2.63579874\n",
      "Iteration 19287, loss = 1.76767118\n",
      "Iteration 19288, loss = 1.48478038\n",
      "Iteration 19289, loss = 1.37973746\n",
      "Iteration 19290, loss = 1.46895436\n",
      "Iteration 19291, loss = 1.50508791\n",
      "Iteration 19292, loss = 2.24839354\n",
      "Iteration 19293, loss = 1.88434716\n",
      "Iteration 19294, loss = 1.66368401\n",
      "Iteration 19295, loss = 1.37751369\n",
      "Iteration 19296, loss = 1.82968174\n",
      "Iteration 19297, loss = 2.01422442\n",
      "Iteration 19298, loss = 1.45467836\n",
      "Iteration 19299, loss = 1.40898159\n",
      "Iteration 19300, loss = 1.39963092\n",
      "Iteration 19301, loss = 1.47947382\n",
      "Iteration 19302, loss = 1.77856576\n",
      "Iteration 19303, loss = 1.45040768\n",
      "Iteration 19304, loss = 1.36467354\n",
      "Iteration 19305, loss = 1.42214255\n",
      "Iteration 19306, loss = 1.67656399\n",
      "Iteration 19307, loss = 1.67079805\n",
      "Iteration 19308, loss = 1.58335644\n",
      "Iteration 19309, loss = 1.87172442\n",
      "Iteration 19310, loss = 2.63951026\n",
      "Iteration 19311, loss = 3.59500305\n",
      "Iteration 19312, loss = 2.27016956\n",
      "Iteration 19313, loss = 1.99590876\n",
      "Iteration 19314, loss = 1.97744395\n",
      "Iteration 19315, loss = 1.55288103\n",
      "Iteration 19316, loss = 1.46236379\n",
      "Iteration 19317, loss = 1.38422022\n",
      "Iteration 19318, loss = 1.38423577\n",
      "Iteration 19319, loss = 1.44952063\n",
      "Iteration 19320, loss = 1.75997277\n",
      "Iteration 19321, loss = 1.54232636\n",
      "Iteration 19322, loss = 1.74120119\n",
      "Iteration 19323, loss = 1.37318878\n",
      "Iteration 19324, loss = 1.27081051\n",
      "Iteration 19325, loss = 1.61231375\n",
      "Iteration 19326, loss = 2.49791324\n",
      "Iteration 19327, loss = 1.39468469\n",
      "Iteration 19328, loss = 1.66697632\n",
      "Iteration 19329, loss = 1.85346714\n",
      "Iteration 19330, loss = 2.07273714\n",
      "Iteration 19331, loss = 1.63640808\n",
      "Iteration 19332, loss = 2.15055926\n",
      "Iteration 19333, loss = 1.41724828\n",
      "Iteration 19334, loss = 1.50914773\n",
      "Iteration 19335, loss = 1.54565126\n",
      "Iteration 19336, loss = 1.46167290\n",
      "Iteration 19337, loss = 1.41328334\n",
      "Iteration 19338, loss = 1.59284618\n",
      "Iteration 19339, loss = 1.63040968\n",
      "Iteration 19340, loss = 1.36092792\n",
      "Iteration 19341, loss = 1.30675052\n",
      "Iteration 19342, loss = 1.34541671\n",
      "Iteration 19343, loss = 1.33300536\n",
      "Iteration 19344, loss = 1.28186281\n",
      "Iteration 19345, loss = 1.33097105\n",
      "Iteration 19346, loss = 1.32685233\n",
      "Iteration 19347, loss = 1.25865187\n",
      "Iteration 19348, loss = 1.37627727\n",
      "Iteration 19349, loss = 1.89467471\n",
      "Iteration 19350, loss = 2.47934946\n",
      "Iteration 19351, loss = 4.54287166\n",
      "Iteration 19352, loss = 3.40413417\n",
      "Iteration 19353, loss = 2.80260789\n",
      "Iteration 19354, loss = 2.46548269\n",
      "Iteration 19355, loss = 1.63962995\n",
      "Iteration 19356, loss = 1.52112739\n",
      "Iteration 19357, loss = 1.36233659\n",
      "Iteration 19358, loss = 1.42038600\n",
      "Iteration 19359, loss = 1.67589001\n",
      "Iteration 19360, loss = 1.52465058\n",
      "Iteration 19361, loss = 1.62330781\n",
      "Iteration 19362, loss = 1.35771507\n",
      "Iteration 19363, loss = 1.35120070\n",
      "Iteration 19364, loss = 1.35820969\n",
      "Iteration 19365, loss = 1.31828639\n",
      "Iteration 19366, loss = 1.31731597\n",
      "Iteration 19367, loss = 1.36758782\n",
      "Iteration 19368, loss = 1.47546792\n",
      "Iteration 19369, loss = 1.35608809\n",
      "Iteration 19370, loss = 1.85278566\n",
      "Iteration 19371, loss = 1.52992438\n",
      "Iteration 19372, loss = 1.82717756\n",
      "Iteration 19373, loss = 1.39099852\n",
      "Iteration 19374, loss = 1.33428022\n",
      "Iteration 19375, loss = 1.29518371\n",
      "Iteration 19376, loss = 1.70794752\n",
      "Iteration 19377, loss = 1.36226522\n",
      "Iteration 19378, loss = 1.32425040\n",
      "Iteration 19379, loss = 1.30574634\n",
      "Iteration 19380, loss = 1.43650698\n",
      "Iteration 19381, loss = 1.42257666\n",
      "Iteration 19382, loss = 1.54833203\n",
      "Iteration 19383, loss = 1.57285490\n",
      "Iteration 19384, loss = 1.45641959\n",
      "Iteration 19385, loss = 1.49256512\n",
      "Iteration 19386, loss = 1.37590780\n",
      "Iteration 19387, loss = 1.37323430\n",
      "Iteration 19388, loss = 1.45585106\n",
      "Iteration 19389, loss = 1.48898574\n",
      "Iteration 19390, loss = 1.70358143\n",
      "Iteration 19391, loss = 1.71020876\n",
      "Iteration 19392, loss = 3.58480070\n",
      "Iteration 19393, loss = 6.27150256\n",
      "Iteration 19394, loss = 4.72220305\n",
      "Iteration 19395, loss = 4.61460254\n",
      "Iteration 19396, loss = 2.45321029\n",
      "Iteration 19397, loss = 2.40403749\n",
      "Iteration 19398, loss = 2.08758065\n",
      "Iteration 19399, loss = 1.59063794\n",
      "Iteration 19400, loss = 1.57450633\n",
      "Iteration 19401, loss = 1.38139453\n",
      "Iteration 19402, loss = 1.38538217\n",
      "Iteration 19403, loss = 1.30080707\n",
      "Iteration 19404, loss = 1.38714815\n",
      "Iteration 19405, loss = 1.85177885\n",
      "Iteration 19406, loss = 1.47650166\n",
      "Iteration 19407, loss = 1.55041551\n",
      "Iteration 19408, loss = 1.71152110\n",
      "Iteration 19409, loss = 2.10106217\n",
      "Iteration 19410, loss = 1.81088918\n",
      "Iteration 19411, loss = 2.72734280\n",
      "Iteration 19412, loss = 2.42380064\n",
      "Iteration 19413, loss = 2.06152651\n",
      "Iteration 19414, loss = 1.72210795\n",
      "Iteration 19415, loss = 1.32978236\n",
      "Iteration 19416, loss = 1.41865784\n",
      "Iteration 19417, loss = 1.34701843\n",
      "Iteration 19418, loss = 1.82777632\n",
      "Iteration 19419, loss = 1.80509277\n",
      "Iteration 19420, loss = 1.33506683\n",
      "Iteration 19421, loss = 1.46017132\n",
      "Iteration 19422, loss = 1.49174241\n",
      "Iteration 19423, loss = 1.32261173\n",
      "Iteration 19424, loss = 1.29800890\n",
      "Iteration 19425, loss = 1.34600234\n",
      "Iteration 19426, loss = 1.83301919\n",
      "Iteration 19427, loss = 1.56485208\n",
      "Iteration 19428, loss = 1.42411894\n",
      "Iteration 19429, loss = 1.65042618\n",
      "Iteration 19430, loss = 1.40983118\n",
      "Iteration 19431, loss = 1.32393956\n",
      "Iteration 19432, loss = 1.39812670\n",
      "Iteration 19433, loss = 1.40535480\n",
      "Iteration 19434, loss = 1.32224381\n",
      "Iteration 19435, loss = 1.26098792\n",
      "Iteration 19436, loss = 1.33100671\n",
      "Iteration 19437, loss = 1.31055030\n",
      "Iteration 19438, loss = 1.22631896\n",
      "Iteration 19439, loss = 1.28015607\n",
      "Iteration 19440, loss = 1.26663378\n",
      "Iteration 19441, loss = 1.27119332\n",
      "Iteration 19442, loss = 1.25258977\n",
      "Iteration 19443, loss = 1.26863477\n",
      "Iteration 19444, loss = 1.24710219\n",
      "Iteration 19445, loss = 1.29147416\n",
      "Iteration 19446, loss = 1.30720323\n",
      "Iteration 19447, loss = 1.30851773\n",
      "Iteration 19448, loss = 1.78293932\n",
      "Iteration 19449, loss = 1.91248378\n",
      "Iteration 19450, loss = 2.58465386\n",
      "Iteration 19451, loss = 1.95327619\n",
      "Iteration 19452, loss = 3.50130293\n",
      "Iteration 19453, loss = 2.57927721\n",
      "Iteration 19454, loss = 6.94106166\n",
      "Iteration 19455, loss = 4.29277173\n",
      "Iteration 19456, loss = 3.70967867\n",
      "Iteration 19457, loss = 2.17733267\n",
      "Iteration 19458, loss = 2.48734161\n",
      "Iteration 19459, loss = 1.73364917\n",
      "Iteration 19460, loss = 1.66568420\n",
      "Iteration 19461, loss = 1.41042717\n",
      "Iteration 19462, loss = 1.40587956\n",
      "Iteration 19463, loss = 1.77390400\n",
      "Iteration 19464, loss = 1.44392509\n",
      "Iteration 19465, loss = 2.05938283\n",
      "Iteration 19466, loss = 2.15718337\n",
      "Iteration 19467, loss = 1.76883346\n",
      "Iteration 19468, loss = 1.83238105\n",
      "Iteration 19469, loss = 1.41074478\n",
      "Iteration 19470, loss = 1.44831583\n",
      "Iteration 19471, loss = 1.26023450\n",
      "Iteration 19472, loss = 1.41147522\n",
      "Iteration 19473, loss = 1.40641689\n",
      "Iteration 19474, loss = 1.34185369\n",
      "Iteration 19475, loss = 1.52199581\n",
      "Iteration 19476, loss = 1.40483190\n",
      "Iteration 19477, loss = 1.49278815\n",
      "Iteration 19478, loss = 3.37118488\n",
      "Iteration 19479, loss = 7.63781456\n",
      "Iteration 19480, loss = 9.95171061\n",
      "Iteration 19481, loss = 6.19954123\n",
      "Iteration 19482, loss = 4.54529964\n",
      "Iteration 19483, loss = 3.07614845\n",
      "Iteration 19484, loss = 2.69819863\n",
      "Iteration 19485, loss = 2.75663160\n",
      "Iteration 19486, loss = 1.71305265\n",
      "Iteration 19487, loss = 1.52910812\n",
      "Iteration 19488, loss = 1.56451580\n",
      "Iteration 19489, loss = 1.40895060\n",
      "Iteration 19490, loss = 1.33681668\n",
      "Iteration 19491, loss = 1.30451510\n",
      "Iteration 19492, loss = 1.30261536\n",
      "Iteration 19493, loss = 1.33901578\n",
      "Iteration 19494, loss = 1.32548791\n",
      "Iteration 19495, loss = 1.29758007\n",
      "Iteration 19496, loss = 1.34497716\n",
      "Iteration 19497, loss = 1.35661007\n",
      "Iteration 19498, loss = 1.48014100\n",
      "Iteration 19499, loss = 1.32245239\n",
      "Iteration 19500, loss = 1.40596512\n",
      "Iteration 19501, loss = 1.45083213\n",
      "Iteration 19502, loss = 1.43797215\n",
      "Iteration 19503, loss = 1.43026722\n",
      "Iteration 19504, loss = 1.55339419\n",
      "Iteration 19505, loss = 1.59423476\n",
      "Iteration 19506, loss = 1.29557033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19507, loss = 1.44580147\n",
      "Iteration 19508, loss = 1.32089190\n",
      "Iteration 19509, loss = 1.22300123\n",
      "Iteration 19510, loss = 1.24663967\n",
      "Iteration 19511, loss = 1.22241761\n",
      "Iteration 19512, loss = 1.24417459\n",
      "Iteration 19513, loss = 1.38419684\n",
      "Iteration 19514, loss = 1.40363626\n",
      "Iteration 19515, loss = 1.32664005\n",
      "Iteration 19516, loss = 1.30643452\n",
      "Iteration 19517, loss = 1.24473308\n",
      "Iteration 19518, loss = 1.61966664\n",
      "Iteration 19519, loss = 2.28994152\n",
      "Iteration 19520, loss = 2.31519332\n",
      "Iteration 19521, loss = 1.41350506\n",
      "Iteration 19522, loss = 1.38795522\n",
      "Iteration 19523, loss = 1.92785993\n",
      "Iteration 19524, loss = 2.19917314\n",
      "Iteration 19525, loss = 2.62223549\n",
      "Iteration 19526, loss = 2.05240763\n",
      "Iteration 19527, loss = 1.99581537\n",
      "Iteration 19528, loss = 1.53188716\n",
      "Iteration 19529, loss = 1.84224335\n",
      "Iteration 19530, loss = 1.68437310\n",
      "Iteration 19531, loss = 1.97246862\n",
      "Iteration 19532, loss = 3.38219594\n",
      "Iteration 19533, loss = 1.74164120\n",
      "Iteration 19534, loss = 1.83516568\n",
      "Iteration 19535, loss = 2.14567609\n",
      "Iteration 19536, loss = 5.82889316\n",
      "Iteration 19537, loss = 4.87312592\n",
      "Iteration 19538, loss = 4.10291641\n",
      "Iteration 19539, loss = 3.03024643\n",
      "Iteration 19540, loss = 3.15358758\n",
      "Iteration 19541, loss = 1.95490659\n",
      "Iteration 19542, loss = 1.73430538\n",
      "Iteration 19543, loss = 2.09498277\n",
      "Iteration 19544, loss = 1.79613505\n",
      "Iteration 19545, loss = 1.91863193\n",
      "Iteration 19546, loss = 1.97202160\n",
      "Iteration 19547, loss = 1.83685207\n",
      "Iteration 19548, loss = 1.48112077\n",
      "Iteration 19549, loss = 1.36181256\n",
      "Iteration 19550, loss = 1.38488380\n",
      "Iteration 19551, loss = 1.26763754\n",
      "Iteration 19552, loss = 1.27000148\n",
      "Iteration 19553, loss = 1.23817061\n",
      "Iteration 19554, loss = 1.30633242\n",
      "Iteration 19555, loss = 1.27669013\n",
      "Iteration 19556, loss = 1.38883790\n",
      "Iteration 19557, loss = 1.41761886\n",
      "Iteration 19558, loss = 1.30031917\n",
      "Iteration 19559, loss = 1.57483944\n",
      "Iteration 19560, loss = 1.24052784\n",
      "Iteration 19561, loss = 1.39272491\n",
      "Iteration 19562, loss = 1.58170591\n",
      "Iteration 19563, loss = 1.60231171\n",
      "Iteration 19564, loss = 2.47003010\n",
      "Iteration 19565, loss = 1.92372150\n",
      "Iteration 19566, loss = 2.01806792\n",
      "Iteration 19567, loss = 2.06223514\n",
      "Iteration 19568, loss = 2.08484029\n",
      "Iteration 19569, loss = 1.77102766\n",
      "Iteration 19570, loss = 1.85458075\n",
      "Iteration 19571, loss = 1.86816268\n",
      "Iteration 19572, loss = 3.53940661\n",
      "Iteration 19573, loss = 2.31660029\n",
      "Iteration 19574, loss = 2.42586859\n",
      "Iteration 19575, loss = 1.77804531\n",
      "Iteration 19576, loss = 1.75824746\n",
      "Iteration 19577, loss = 1.89608999\n",
      "Iteration 19578, loss = 2.10796792\n",
      "Iteration 19579, loss = 2.44539486\n",
      "Iteration 19580, loss = 2.11839788\n",
      "Iteration 19581, loss = 1.80811061\n",
      "Iteration 19582, loss = 2.17961235\n",
      "Iteration 19583, loss = 1.95924592\n",
      "Iteration 19584, loss = 1.91664589\n",
      "Iteration 19585, loss = 1.50237790\n",
      "Iteration 19586, loss = 1.49244560\n",
      "Iteration 19587, loss = 1.44499798\n",
      "Iteration 19588, loss = 1.49672637\n",
      "Iteration 19589, loss = 1.52426888\n",
      "Iteration 19590, loss = 1.48782522\n",
      "Iteration 19591, loss = 1.47637614\n",
      "Iteration 19592, loss = 1.72684801\n",
      "Iteration 19593, loss = 1.45145820\n",
      "Iteration 19594, loss = 1.27492721\n",
      "Iteration 19595, loss = 1.27132140\n",
      "Iteration 19596, loss = 1.26682947\n",
      "Iteration 19597, loss = 1.31743342\n",
      "Iteration 19598, loss = 1.68099052\n",
      "Iteration 19599, loss = 1.41575349\n",
      "Iteration 19600, loss = 1.53210216\n",
      "Iteration 19601, loss = 1.59131233\n",
      "Iteration 19602, loss = 1.57336549\n",
      "Iteration 19603, loss = 1.23516476\n",
      "Iteration 19604, loss = 1.30324777\n",
      "Iteration 19605, loss = 1.24355054\n",
      "Iteration 19606, loss = 1.26484515\n",
      "Iteration 19607, loss = 1.34552148\n",
      "Iteration 19608, loss = 1.22568179\n",
      "Iteration 19609, loss = 1.26562113\n",
      "Iteration 19610, loss = 1.28932398\n",
      "Iteration 19611, loss = 1.25381749\n",
      "Iteration 19612, loss = 1.29738942\n",
      "Iteration 19613, loss = 1.40031832\n",
      "Iteration 19614, loss = 1.84526288\n",
      "Iteration 19615, loss = 1.29587934\n",
      "Iteration 19616, loss = 1.36897988\n",
      "Iteration 19617, loss = 1.25660345\n",
      "Iteration 19618, loss = 1.21535775\n",
      "Iteration 19619, loss = 1.26653653\n",
      "Iteration 19620, loss = 1.23833817\n",
      "Iteration 19621, loss = 1.24555826\n",
      "Iteration 19622, loss = 1.29525620\n",
      "Iteration 19623, loss = 1.28032135\n",
      "Iteration 19624, loss = 1.23692524\n",
      "Iteration 19625, loss = 1.32509630\n",
      "Iteration 19626, loss = 1.48481841\n",
      "Iteration 19627, loss = 1.53071629\n",
      "Iteration 19628, loss = 1.92151577\n",
      "Iteration 19629, loss = 1.98337344\n",
      "Iteration 19630, loss = 2.64067855\n",
      "Iteration 19631, loss = 1.60387937\n",
      "Iteration 19632, loss = 1.60990260\n",
      "Iteration 19633, loss = 1.33577605\n",
      "Iteration 19634, loss = 1.34109239\n",
      "Iteration 19635, loss = 1.40179872\n",
      "Iteration 19636, loss = 1.39128563\n",
      "Iteration 19637, loss = 1.25248264\n",
      "Iteration 19638, loss = 1.52836395\n",
      "Iteration 19639, loss = 1.46108343\n",
      "Iteration 19640, loss = 1.83672801\n",
      "Iteration 19641, loss = 1.64051351\n",
      "Iteration 19642, loss = 1.75269811\n",
      "Iteration 19643, loss = 1.74944980\n",
      "Iteration 19644, loss = 2.05571320\n",
      "Iteration 19645, loss = 1.75395286\n",
      "Iteration 19646, loss = 1.54728602\n",
      "Iteration 19647, loss = 1.50624743\n",
      "Iteration 19648, loss = 1.57816078\n",
      "Iteration 19649, loss = 1.43660621\n",
      "Iteration 19650, loss = 2.14552138\n",
      "Iteration 19651, loss = 1.36755823\n",
      "Iteration 19652, loss = 1.36470022\n",
      "Iteration 19653, loss = 1.25275176\n",
      "Iteration 19654, loss = 1.37286774\n",
      "Iteration 19655, loss = 1.46174188\n",
      "Iteration 19656, loss = 1.24781597\n",
      "Iteration 19657, loss = 1.45426702\n",
      "Iteration 19658, loss = 1.50776082\n",
      "Iteration 19659, loss = 1.54867673\n",
      "Iteration 19660, loss = 1.56464011\n",
      "Iteration 19661, loss = 3.47451101\n",
      "Iteration 19662, loss = 3.30855666\n",
      "Iteration 19663, loss = 6.18815354\n",
      "Iteration 19664, loss = 3.83673454\n",
      "Iteration 19665, loss = 2.22999800\n",
      "Iteration 19666, loss = 1.93841221\n",
      "Iteration 19667, loss = 2.22878018\n",
      "Iteration 19668, loss = 1.50067906\n",
      "Iteration 19669, loss = 1.51787927\n",
      "Iteration 19670, loss = 1.50714304\n",
      "Iteration 19671, loss = 1.30484590\n",
      "Iteration 19672, loss = 1.64995467\n",
      "Iteration 19673, loss = 1.65099518\n",
      "Iteration 19674, loss = 1.40080402\n",
      "Iteration 19675, loss = 1.59652849\n",
      "Iteration 19676, loss = 1.36977502\n",
      "Iteration 19677, loss = 1.60408957\n",
      "Iteration 19678, loss = 2.24124848\n",
      "Iteration 19679, loss = 1.74139666\n",
      "Iteration 19680, loss = 1.54485247\n",
      "Iteration 19681, loss = 1.35714603\n",
      "Iteration 19682, loss = 1.34746050\n",
      "Iteration 19683, loss = 1.26442380\n",
      "Iteration 19684, loss = 1.30218177\n",
      "Iteration 19685, loss = 1.30192410\n",
      "Iteration 19686, loss = 1.30913986\n",
      "Iteration 19687, loss = 1.42851656\n",
      "Iteration 19688, loss = 1.25433413\n",
      "Iteration 19689, loss = 1.60320135\n",
      "Iteration 19690, loss = 1.46055089\n",
      "Iteration 19691, loss = 1.69727084\n",
      "Iteration 19692, loss = 1.29048346\n",
      "Iteration 19693, loss = 1.25363538\n",
      "Iteration 19694, loss = 1.30462503\n",
      "Iteration 19695, loss = 1.37229349\n",
      "Iteration 19696, loss = 1.36994365\n",
      "Iteration 19697, loss = 1.34269726\n",
      "Iteration 19698, loss = 1.38568449\n",
      "Iteration 19699, loss = 1.32334736\n",
      "Iteration 19700, loss = 1.77193808\n",
      "Iteration 19701, loss = 1.85127541\n",
      "Iteration 19702, loss = 1.43271249\n",
      "Iteration 19703, loss = 1.58322380\n",
      "Iteration 19704, loss = 1.33732196\n",
      "Iteration 19705, loss = 1.37834752\n",
      "Iteration 19706, loss = 1.71111633\n",
      "Iteration 19707, loss = 1.69175609\n",
      "Iteration 19708, loss = 1.68384865\n",
      "Iteration 19709, loss = 1.44938713\n",
      "Iteration 19710, loss = 1.85536777\n",
      "Iteration 19711, loss = 1.64610587\n",
      "Iteration 19712, loss = 1.82168619\n",
      "Iteration 19713, loss = 1.30240880\n",
      "Iteration 19714, loss = 1.37487383\n",
      "Iteration 19715, loss = 1.31546271\n",
      "Iteration 19716, loss = 1.21492649\n",
      "Iteration 19717, loss = 1.22451688\n",
      "Iteration 19718, loss = 1.23085307\n",
      "Iteration 19719, loss = 1.21950088\n",
      "Iteration 19720, loss = 1.25009347\n",
      "Iteration 19721, loss = 1.33228323\n",
      "Iteration 19722, loss = 1.41649739\n",
      "Iteration 19723, loss = 1.51310106\n",
      "Iteration 19724, loss = 1.22878078\n",
      "Iteration 19725, loss = 1.24973234\n",
      "Iteration 19726, loss = 1.23279645\n",
      "Iteration 19727, loss = 1.23566952\n",
      "Iteration 19728, loss = 1.35381013\n",
      "Iteration 19729, loss = 1.40329086\n",
      "Iteration 19730, loss = 1.35240397\n",
      "Iteration 19731, loss = 1.20643791\n",
      "Iteration 19732, loss = 1.18566976\n",
      "Iteration 19733, loss = 1.21443501\n",
      "Iteration 19734, loss = 1.19020751\n",
      "Iteration 19735, loss = 1.26849738\n",
      "Iteration 19736, loss = 1.21594543\n",
      "Iteration 19737, loss = 1.29459203\n",
      "Iteration 19738, loss = 1.25321067\n",
      "Iteration 19739, loss = 1.20649611\n",
      "Iteration 19740, loss = 1.21437279\n",
      "Iteration 19741, loss = 1.34405341\n",
      "Iteration 19742, loss = 1.40913547\n",
      "Iteration 19743, loss = 1.50448744\n",
      "Iteration 19744, loss = 1.69320155\n",
      "Iteration 19745, loss = 1.73617157\n",
      "Iteration 19746, loss = 2.33680086\n",
      "Iteration 19747, loss = 2.63060867\n",
      "Iteration 19748, loss = 2.60081509\n",
      "Iteration 19749, loss = 1.62843456\n",
      "Iteration 19750, loss = 1.66132041\n",
      "Iteration 19751, loss = 1.58865651\n",
      "Iteration 19752, loss = 1.90183160\n",
      "Iteration 19753, loss = 2.17533581\n",
      "Iteration 19754, loss = 2.03855617\n",
      "Iteration 19755, loss = 1.44234842\n",
      "Iteration 19756, loss = 1.36557574\n",
      "Iteration 19757, loss = 1.45654594\n",
      "Iteration 19758, loss = 1.23583162\n",
      "Iteration 19759, loss = 1.35514162\n",
      "Iteration 19760, loss = 1.41296470\n",
      "Iteration 19761, loss = 1.28407917\n",
      "Iteration 19762, loss = 1.24155560\n",
      "Iteration 19763, loss = 1.37635357\n",
      "Iteration 19764, loss = 1.35624547\n",
      "Iteration 19765, loss = 1.22544066\n",
      "Iteration 19766, loss = 1.22505925\n",
      "Iteration 19767, loss = 1.25886288\n",
      "Iteration 19768, loss = 1.60497124\n",
      "Iteration 19769, loss = 1.74626246\n",
      "Iteration 19770, loss = 1.84154707\n",
      "Iteration 19771, loss = 2.35267877\n",
      "Iteration 19772, loss = 2.93231593\n",
      "Iteration 19773, loss = 1.58941100\n",
      "Iteration 19774, loss = 1.61359895\n",
      "Iteration 19775, loss = 1.33003407\n",
      "Iteration 19776, loss = 1.38385307\n",
      "Iteration 19777, loss = 1.37457171\n",
      "Iteration 19778, loss = 1.75759598\n",
      "Iteration 19779, loss = 1.74006280\n",
      "Iteration 19780, loss = 2.21605347\n",
      "Iteration 19781, loss = 1.48708939\n",
      "Iteration 19782, loss = 1.39526810\n",
      "Iteration 19783, loss = 1.71050855\n",
      "Iteration 19784, loss = 2.09423115\n",
      "Iteration 19785, loss = 1.96776220\n",
      "Iteration 19786, loss = 1.35509129\n",
      "Iteration 19787, loss = 1.27593946\n",
      "Iteration 19788, loss = 1.30830719\n",
      "Iteration 19789, loss = 1.31631783\n",
      "Iteration 19790, loss = 1.59243586\n",
      "Iteration 19791, loss = 1.46071043\n",
      "Iteration 19792, loss = 1.98880091\n",
      "Iteration 19793, loss = 1.54320741\n",
      "Iteration 19794, loss = 1.60571920\n",
      "Iteration 19795, loss = 1.84419454\n",
      "Iteration 19796, loss = 1.50217846\n",
      "Iteration 19797, loss = 1.43632916\n",
      "Iteration 19798, loss = 1.22144574\n",
      "Iteration 19799, loss = 1.30099246\n",
      "Iteration 19800, loss = 1.23031408\n",
      "Iteration 19801, loss = 1.33893969\n",
      "Iteration 19802, loss = 1.28233651\n",
      "Iteration 19803, loss = 1.76184920\n",
      "Iteration 19804, loss = 1.90595625\n",
      "Iteration 19805, loss = 1.49283001\n",
      "Iteration 19806, loss = 1.63139263\n",
      "Iteration 19807, loss = 2.10160595\n",
      "Iteration 19808, loss = 1.73456777\n",
      "Iteration 19809, loss = 1.74955089\n",
      "Iteration 19810, loss = 1.55028774\n",
      "Iteration 19811, loss = 1.33581586\n",
      "Iteration 19812, loss = 1.24416656\n",
      "Iteration 19813, loss = 1.31981512\n",
      "Iteration 19814, loss = 1.33945897\n",
      "Iteration 19815, loss = 1.97878228\n",
      "Iteration 19816, loss = 2.75471292\n",
      "Iteration 19817, loss = 1.81723571\n",
      "Iteration 19818, loss = 1.87876702\n",
      "Iteration 19819, loss = 1.46787652\n",
      "Iteration 19820, loss = 1.50078120\n",
      "Iteration 19821, loss = 1.31089764\n",
      "Iteration 19822, loss = 2.26357458\n",
      "Iteration 19823, loss = 2.36627400\n",
      "Iteration 19824, loss = 1.50857747\n",
      "Iteration 19825, loss = 1.62062350\n",
      "Iteration 19826, loss = 1.79173834\n",
      "Iteration 19827, loss = 1.42107917\n",
      "Iteration 19828, loss = 1.41504281\n",
      "Iteration 19829, loss = 1.46929601\n",
      "Iteration 19830, loss = 1.68322732\n",
      "Iteration 19831, loss = 2.40571467\n",
      "Iteration 19832, loss = 1.99345424\n",
      "Iteration 19833, loss = 2.11016568\n",
      "Iteration 19834, loss = 3.02640554\n",
      "Iteration 19835, loss = 3.39338170\n",
      "Iteration 19836, loss = 6.03541345\n",
      "Iteration 19837, loss = 4.01772023\n",
      "Iteration 19838, loss = 3.31420123\n",
      "Iteration 19839, loss = 2.39082336\n",
      "Iteration 19840, loss = 2.06866154\n",
      "Iteration 19841, loss = 1.88338553\n",
      "Iteration 19842, loss = 2.17972671\n",
      "Iteration 19843, loss = 2.43984150\n",
      "Iteration 19844, loss = 6.09796917\n",
      "Iteration 19845, loss = 8.48501622\n",
      "Iteration 19846, loss = 6.26257243\n",
      "Iteration 19847, loss = 4.72857387\n",
      "Iteration 19848, loss = 4.36574104\n",
      "Iteration 19849, loss = 2.62647192\n",
      "Iteration 19850, loss = 2.25381119\n",
      "Iteration 19851, loss = 2.05540194\n",
      "Iteration 19852, loss = 1.47511567\n",
      "Iteration 19853, loss = 1.68010830\n",
      "Iteration 19854, loss = 2.19249822\n",
      "Iteration 19855, loss = 2.84289846\n",
      "Iteration 19856, loss = 2.13059514\n",
      "Iteration 19857, loss = 1.73639646\n",
      "Iteration 19858, loss = 1.52159372\n",
      "Iteration 19859, loss = 1.52661899\n",
      "Iteration 19860, loss = 1.36809855\n",
      "Iteration 19861, loss = 1.28335332\n",
      "Iteration 19862, loss = 1.27656079\n",
      "Iteration 19863, loss = 1.31541066\n",
      "Iteration 19864, loss = 1.29341809\n",
      "Iteration 19865, loss = 1.24693070\n",
      "Iteration 19866, loss = 1.30727614\n",
      "Iteration 19867, loss = 1.19718275\n",
      "Iteration 19868, loss = 1.23487086\n",
      "Iteration 19869, loss = 1.23869871\n",
      "Iteration 19870, loss = 1.23200215\n",
      "Iteration 19871, loss = 1.24756388\n",
      "Iteration 19872, loss = 1.59668038\n",
      "Iteration 19873, loss = 1.42315760\n",
      "Iteration 19874, loss = 1.39481550\n",
      "Iteration 19875, loss = 1.21651777\n",
      "Iteration 19876, loss = 1.22864454\n",
      "Iteration 19877, loss = 1.23647393\n",
      "Iteration 19878, loss = 1.22729032\n",
      "Iteration 19879, loss = 1.33721467\n",
      "Iteration 19880, loss = 1.30388721\n",
      "Iteration 19881, loss = 1.24090038\n",
      "Iteration 19882, loss = 1.19068222\n",
      "Iteration 19883, loss = 1.28323528\n",
      "Iteration 19884, loss = 1.84603657\n",
      "Iteration 19885, loss = 2.06348191\n",
      "Iteration 19886, loss = 2.05270508\n",
      "Iteration 19887, loss = 2.12654033\n",
      "Iteration 19888, loss = 2.06755422\n",
      "Iteration 19889, loss = 1.67573950\n",
      "Iteration 19890, loss = 1.43488664\n",
      "Iteration 19891, loss = 1.75424707\n",
      "Iteration 19892, loss = 1.70240681\n",
      "Iteration 19893, loss = 1.82566120\n",
      "Iteration 19894, loss = 1.66571756\n",
      "Iteration 19895, loss = 1.56414486\n",
      "Iteration 19896, loss = 1.52366137\n",
      "Iteration 19897, loss = 1.29480077\n",
      "Iteration 19898, loss = 1.37446444\n",
      "Iteration 19899, loss = 1.44655676\n",
      "Iteration 19900, loss = 1.47470116\n",
      "Iteration 19901, loss = 1.79774992\n",
      "Iteration 19902, loss = 1.41751325\n",
      "Iteration 19903, loss = 1.56202164\n",
      "Iteration 19904, loss = 1.25373940\n",
      "Iteration 19905, loss = 1.22049576\n",
      "Iteration 19906, loss = 1.20142393\n",
      "Iteration 19907, loss = 1.19620136\n",
      "Iteration 19908, loss = 1.20595727\n",
      "Iteration 19909, loss = 1.30071650\n",
      "Iteration 19910, loss = 1.35359955\n",
      "Iteration 19911, loss = 1.31756880\n",
      "Iteration 19912, loss = 1.50093847\n",
      "Iteration 19913, loss = 2.14369052\n",
      "Iteration 19914, loss = 1.56741714\n",
      "Iteration 19915, loss = 1.78044334\n",
      "Iteration 19916, loss = 1.39336756\n",
      "Iteration 19917, loss = 1.24601871\n",
      "Iteration 19918, loss = 1.60352875\n",
      "Iteration 19919, loss = 1.76910565\n",
      "Iteration 19920, loss = 1.88258779\n",
      "Iteration 19921, loss = 1.40526157\n",
      "Iteration 19922, loss = 1.39769314\n",
      "Iteration 19923, loss = 1.25183154\n",
      "Iteration 19924, loss = 1.32680562\n",
      "Iteration 19925, loss = 1.81462081\n",
      "Iteration 19926, loss = 1.30195567\n",
      "Iteration 19927, loss = 1.37913285\n",
      "Iteration 19928, loss = 1.35336225\n",
      "Iteration 19929, loss = 1.30778301\n",
      "Iteration 19930, loss = 1.28306207\n",
      "Iteration 19931, loss = 1.40499415\n",
      "Iteration 19932, loss = 1.66717506\n",
      "Iteration 19933, loss = 3.83557651\n",
      "Iteration 19934, loss = 2.26323914\n",
      "Iteration 19935, loss = 2.40644337\n",
      "Iteration 19936, loss = 1.41930541\n",
      "Iteration 19937, loss = 1.68223734\n",
      "Iteration 19938, loss = 1.56858429\n",
      "Iteration 19939, loss = 1.60208328\n",
      "Iteration 19940, loss = 1.43244630\n",
      "Iteration 19941, loss = 1.35000036\n",
      "Iteration 19942, loss = 1.31302746\n",
      "Iteration 19943, loss = 1.24568019\n",
      "Iteration 19944, loss = 1.31987090\n",
      "Iteration 19945, loss = 1.29215524\n",
      "Iteration 19946, loss = 1.26315538\n",
      "Iteration 19947, loss = 1.19131948\n",
      "Iteration 19948, loss = 1.20968417\n",
      "Iteration 19949, loss = 1.48682357\n",
      "Iteration 19950, loss = 1.48336789\n",
      "Iteration 19951, loss = 1.81058478\n",
      "Iteration 19952, loss = 1.79395977\n",
      "Iteration 19953, loss = 2.35739530\n",
      "Iteration 19954, loss = 2.06967446\n",
      "Iteration 19955, loss = 1.91849759\n",
      "Iteration 19956, loss = 1.47685796\n",
      "Iteration 19957, loss = 1.53370620\n",
      "Iteration 19958, loss = 1.34609838\n",
      "Iteration 19959, loss = 1.32352632\n",
      "Iteration 19960, loss = 1.41370226\n",
      "Iteration 19961, loss = 1.43333153\n",
      "Iteration 19962, loss = 1.38217351\n",
      "Iteration 19963, loss = 1.28113906\n",
      "Iteration 19964, loss = 1.39537849\n",
      "Iteration 19965, loss = 1.36366361\n",
      "Iteration 19966, loss = 1.53466009\n",
      "Iteration 19967, loss = 1.20546621\n",
      "Iteration 19968, loss = 2.02208196\n",
      "Iteration 19969, loss = 2.09373769\n",
      "Iteration 19970, loss = 1.86115109\n",
      "Iteration 19971, loss = 1.46711590\n",
      "Iteration 19972, loss = 1.53321207\n",
      "Iteration 19973, loss = 1.33612078\n",
      "Iteration 19974, loss = 1.47741734\n",
      "Iteration 19975, loss = 1.65929519\n",
      "Iteration 19976, loss = 1.75066132\n",
      "Iteration 19977, loss = 1.75073837\n",
      "Iteration 19978, loss = 1.71201647\n",
      "Iteration 19979, loss = 1.60843121\n",
      "Iteration 19980, loss = 1.44366152\n",
      "Iteration 19981, loss = 1.20006960\n",
      "Iteration 19982, loss = 1.19515408\n",
      "Iteration 19983, loss = 1.32421236\n",
      "Iteration 19984, loss = 1.18608316\n",
      "Iteration 19985, loss = 1.22532811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19986, loss = 1.18634646\n",
      "Iteration 19987, loss = 1.21807840\n",
      "Iteration 19988, loss = 1.33904802\n",
      "Iteration 19989, loss = 1.18550746\n",
      "Iteration 19990, loss = 1.42674813\n",
      "Iteration 19991, loss = 1.26451032\n",
      "Iteration 19992, loss = 1.34930693\n",
      "Iteration 19993, loss = 1.25360400\n",
      "Iteration 19994, loss = 1.38147447\n",
      "Iteration 19995, loss = 2.17746237\n",
      "Iteration 19996, loss = 2.41967628\n",
      "Iteration 19997, loss = 2.58791576\n",
      "Iteration 19998, loss = 2.15074637\n",
      "Iteration 19999, loss = 1.72590089\n",
      "Iteration 20000, loss = 1.67393125\n",
      "Iteration 20001, loss = 1.45741135\n",
      "Iteration 20002, loss = 1.22716191\n",
      "Iteration 20003, loss = 1.18611123\n",
      "Iteration 20004, loss = 1.33057156\n",
      "Iteration 20005, loss = 1.35895525\n",
      "Iteration 20006, loss = 1.92887915\n",
      "Iteration 20007, loss = 1.31909773\n",
      "Iteration 20008, loss = 1.46928072\n",
      "Iteration 20009, loss = 1.69434743\n",
      "Iteration 20010, loss = 1.28014969\n",
      "Iteration 20011, loss = 1.69140699\n",
      "Iteration 20012, loss = 1.67495159\n",
      "Iteration 20013, loss = 1.50978391\n",
      "Iteration 20014, loss = 1.43618265\n",
      "Iteration 20015, loss = 1.27819834\n",
      "Iteration 20016, loss = 1.34458499\n",
      "Iteration 20017, loss = 1.24385194\n",
      "Iteration 20018, loss = 1.22346980\n",
      "Iteration 20019, loss = 1.35595012\n",
      "Iteration 20020, loss = 1.39941770\n",
      "Iteration 20021, loss = 1.89357910\n",
      "Iteration 20022, loss = 4.24924064\n",
      "Iteration 20023, loss = 3.05411210\n",
      "Iteration 20024, loss = 3.97341708\n",
      "Iteration 20025, loss = 3.26396848\n",
      "Iteration 20026, loss = 2.83695995\n",
      "Iteration 20027, loss = 1.99675660\n",
      "Iteration 20028, loss = 2.88886798\n",
      "Iteration 20029, loss = 1.57269475\n",
      "Iteration 20030, loss = 1.44714998\n",
      "Iteration 20031, loss = 1.77419014\n",
      "Iteration 20032, loss = 1.34496689\n",
      "Iteration 20033, loss = 1.34357858\n",
      "Iteration 20034, loss = 1.40063802\n",
      "Iteration 20035, loss = 1.30481112\n",
      "Iteration 20036, loss = 1.33025689\n",
      "Iteration 20037, loss = 1.43160564\n",
      "Iteration 20038, loss = 1.27527808\n",
      "Iteration 20039, loss = 1.16215858\n",
      "Iteration 20040, loss = 1.24127160\n",
      "Iteration 20041, loss = 1.25016471\n",
      "Iteration 20042, loss = 1.18736853\n",
      "Iteration 20043, loss = 1.17173496\n",
      "Iteration 20044, loss = 1.31896025\n",
      "Iteration 20045, loss = 1.92240882\n",
      "Iteration 20046, loss = 1.37861669\n",
      "Iteration 20047, loss = 1.29711682\n",
      "Iteration 20048, loss = 1.26280492\n",
      "Iteration 20049, loss = 1.18667017\n",
      "Iteration 20050, loss = 1.23115954\n",
      "Iteration 20051, loss = 1.19539922\n",
      "Iteration 20052, loss = 1.40724810\n",
      "Iteration 20053, loss = 1.27405215\n",
      "Iteration 20054, loss = 1.50855664\n",
      "Iteration 20055, loss = 1.85008350\n",
      "Iteration 20056, loss = 2.54250889\n",
      "Iteration 20057, loss = 2.36768891\n",
      "Iteration 20058, loss = 1.66792736\n",
      "Iteration 20059, loss = 1.68117504\n",
      "Iteration 20060, loss = 2.12729508\n",
      "Iteration 20061, loss = 2.03831112\n",
      "Iteration 20062, loss = 3.12659141\n",
      "Iteration 20063, loss = 2.88682590\n",
      "Iteration 20064, loss = 1.64901414\n",
      "Iteration 20065, loss = 1.66178708\n",
      "Iteration 20066, loss = 1.38006107\n",
      "Iteration 20067, loss = 1.50646889\n",
      "Iteration 20068, loss = 1.29836024\n",
      "Iteration 20069, loss = 1.21803696\n",
      "Iteration 20070, loss = 1.29963049\n",
      "Iteration 20071, loss = 1.46628772\n",
      "Iteration 20072, loss = 1.34464033\n",
      "Iteration 20073, loss = 1.42838370\n",
      "Iteration 20074, loss = 1.26219729\n",
      "Iteration 20075, loss = 1.23442130\n",
      "Iteration 20076, loss = 1.17917586\n",
      "Iteration 20077, loss = 1.15249712\n",
      "Iteration 20078, loss = 1.20937598\n",
      "Iteration 20079, loss = 1.22701345\n",
      "Iteration 20080, loss = 1.21574015\n",
      "Iteration 20081, loss = 1.19120468\n",
      "Iteration 20082, loss = 1.18533571\n",
      "Iteration 20083, loss = 1.23627220\n",
      "Iteration 20084, loss = 1.25177730\n",
      "Iteration 20085, loss = 1.17808558\n",
      "Iteration 20086, loss = 1.21566606\n",
      "Iteration 20087, loss = 1.22550362\n",
      "Iteration 20088, loss = 1.45340431\n",
      "Iteration 20089, loss = 1.38089857\n",
      "Iteration 20090, loss = 1.26490744\n",
      "Iteration 20091, loss = 1.21345586\n",
      "Iteration 20092, loss = 1.19113491\n",
      "Iteration 20093, loss = 1.42253542\n",
      "Iteration 20094, loss = 1.73649065\n",
      "Iteration 20095, loss = 1.42860108\n",
      "Iteration 20096, loss = 1.40171704\n",
      "Iteration 20097, loss = 1.29751376\n",
      "Iteration 20098, loss = 1.56858062\n",
      "Iteration 20099, loss = 1.35540758\n",
      "Iteration 20100, loss = 1.37372660\n",
      "Iteration 20101, loss = 1.31657065\n",
      "Iteration 20102, loss = 1.34304115\n",
      "Iteration 20103, loss = 1.18132136\n",
      "Iteration 20104, loss = 1.77044152\n",
      "Iteration 20105, loss = 2.11075689\n",
      "Iteration 20106, loss = 4.93721778\n",
      "Iteration 20107, loss = 3.54020379\n",
      "Iteration 20108, loss = 4.62196279\n",
      "Iteration 20109, loss = 3.66956294\n",
      "Iteration 20110, loss = 4.01637088\n",
      "Iteration 20111, loss = 2.74276603\n",
      "Iteration 20112, loss = 3.62349150\n",
      "Iteration 20113, loss = 2.26665146\n",
      "Iteration 20114, loss = 5.83818116\n",
      "Iteration 20115, loss = 3.26501147\n",
      "Iteration 20116, loss = 2.46867206\n",
      "Iteration 20117, loss = 1.67823794\n",
      "Iteration 20118, loss = 2.01181344\n",
      "Iteration 20119, loss = 1.75877729\n",
      "Iteration 20120, loss = 2.26157286\n",
      "Iteration 20121, loss = 2.28825672\n",
      "Iteration 20122, loss = 2.58195801\n",
      "Iteration 20123, loss = 2.01013744\n",
      "Iteration 20124, loss = 3.40398059\n",
      "Iteration 20125, loss = 1.84109750\n",
      "Iteration 20126, loss = 1.64677527\n",
      "Iteration 20127, loss = 1.41019741\n",
      "Iteration 20128, loss = 1.97453104\n",
      "Iteration 20129, loss = 1.44446111\n",
      "Iteration 20130, loss = 1.25477544\n",
      "Iteration 20131, loss = 1.38497102\n",
      "Iteration 20132, loss = 1.59959610\n",
      "Iteration 20133, loss = 1.28808668\n",
      "Iteration 20134, loss = 2.02659540\n",
      "Iteration 20135, loss = 1.39752020\n",
      "Iteration 20136, loss = 1.60607088\n",
      "Iteration 20137, loss = 1.49429438\n",
      "Iteration 20138, loss = 1.32333185\n",
      "Iteration 20139, loss = 1.27606507\n",
      "Iteration 20140, loss = 1.19147993\n",
      "Iteration 20141, loss = 1.21134559\n",
      "Iteration 20142, loss = 1.44343873\n",
      "Iteration 20143, loss = 1.60748448\n",
      "Iteration 20144, loss = 1.34288525\n",
      "Iteration 20145, loss = 1.25449050\n",
      "Iteration 20146, loss = 1.24946796\n",
      "Iteration 20147, loss = 1.25050475\n",
      "Iteration 20148, loss = 1.15499984\n",
      "Iteration 20149, loss = 1.15820671\n",
      "Iteration 20150, loss = 1.18901655\n",
      "Iteration 20151, loss = 1.42389956\n",
      "Iteration 20152, loss = 1.19928143\n",
      "Iteration 20153, loss = 1.54542348\n",
      "Iteration 20154, loss = 1.36290186\n",
      "Iteration 20155, loss = 1.18908206\n",
      "Iteration 20156, loss = 1.25258597\n",
      "Iteration 20157, loss = 1.24170825\n",
      "Iteration 20158, loss = 1.17601904\n",
      "Iteration 20159, loss = 1.20319643\n",
      "Iteration 20160, loss = 1.22930052\n",
      "Iteration 20161, loss = 1.23256148\n",
      "Iteration 20162, loss = 1.19390569\n",
      "Iteration 20163, loss = 1.29844572\n",
      "Iteration 20164, loss = 1.25451341\n",
      "Iteration 20165, loss = 1.41700488\n",
      "Iteration 20166, loss = 1.38480987\n",
      "Iteration 20167, loss = 1.36368896\n",
      "Iteration 20168, loss = 1.26357919\n",
      "Iteration 20169, loss = 1.73711651\n",
      "Iteration 20170, loss = 1.43100467\n",
      "Iteration 20171, loss = 1.36942971\n",
      "Iteration 20172, loss = 1.19981402\n",
      "Iteration 20173, loss = 1.30460902\n",
      "Iteration 20174, loss = 1.30191136\n",
      "Iteration 20175, loss = 2.00162715\n",
      "Iteration 20176, loss = 2.33491033\n",
      "Iteration 20177, loss = 3.42495123\n",
      "Iteration 20178, loss = 2.87657386\n",
      "Iteration 20179, loss = 2.94987402\n",
      "Iteration 20180, loss = 2.20599948\n",
      "Iteration 20181, loss = 2.22699324\n",
      "Iteration 20182, loss = 3.11606032\n",
      "Iteration 20183, loss = 3.53025019\n",
      "Iteration 20184, loss = 2.06492145\n",
      "Iteration 20185, loss = 1.88666025\n",
      "Iteration 20186, loss = 1.42391933\n",
      "Iteration 20187, loss = 1.77062268\n",
      "Iteration 20188, loss = 2.20795913\n",
      "Iteration 20189, loss = 3.41349261\n",
      "Iteration 20190, loss = 3.03674515\n",
      "Iteration 20191, loss = 3.15748078\n",
      "Iteration 20192, loss = 3.30828018\n",
      "Iteration 20193, loss = 3.91217887\n",
      "Iteration 20194, loss = 2.33253518\n",
      "Iteration 20195, loss = 1.99875699\n",
      "Iteration 20196, loss = 1.64254845\n",
      "Iteration 20197, loss = 1.58044831\n",
      "Iteration 20198, loss = 1.41339887\n",
      "Iteration 20199, loss = 1.27619372\n",
      "Iteration 20200, loss = 1.48012550\n",
      "Iteration 20201, loss = 1.20171457\n",
      "Iteration 20202, loss = 1.23784884\n",
      "Iteration 20203, loss = 1.50358123\n",
      "Iteration 20204, loss = 1.41167297\n",
      "Iteration 20205, loss = 1.26159316\n",
      "Iteration 20206, loss = 1.23117599\n",
      "Iteration 20207, loss = 1.27984094\n",
      "Iteration 20208, loss = 1.23620923\n",
      "Iteration 20209, loss = 1.19411551\n",
      "Iteration 20210, loss = 1.12603254\n",
      "Iteration 20211, loss = 1.12848240\n",
      "Iteration 20212, loss = 1.26937329\n",
      "Iteration 20213, loss = 1.19758991\n",
      "Iteration 20214, loss = 1.15114333\n",
      "Iteration 20215, loss = 1.21822358\n",
      "Iteration 20216, loss = 1.18681132\n",
      "Iteration 20217, loss = 1.17787305\n",
      "Iteration 20218, loss = 1.52192732\n",
      "Iteration 20219, loss = 1.25195438\n",
      "Iteration 20220, loss = 1.27190294\n",
      "Iteration 20221, loss = 1.57166630\n",
      "Iteration 20222, loss = 1.27316804\n",
      "Iteration 20223, loss = 1.28850194\n",
      "Iteration 20224, loss = 1.24912929\n",
      "Iteration 20225, loss = 1.31914423\n",
      "Iteration 20226, loss = 1.27193818\n",
      "Iteration 20227, loss = 1.34449242\n",
      "Iteration 20228, loss = 1.28168354\n",
      "Iteration 20229, loss = 2.06780931\n",
      "Iteration 20230, loss = 1.84424357\n",
      "Iteration 20231, loss = 1.38395393\n",
      "Iteration 20232, loss = 1.17945205\n",
      "Iteration 20233, loss = 1.14252725\n",
      "Iteration 20234, loss = 1.18178699\n",
      "Iteration 20235, loss = 1.17751174\n",
      "Iteration 20236, loss = 1.22481395\n",
      "Iteration 20237, loss = 1.46071281\n",
      "Iteration 20238, loss = 1.42378487\n",
      "Iteration 20239, loss = 1.23801536\n",
      "Iteration 20240, loss = 1.39006834\n",
      "Iteration 20241, loss = 2.28298685\n",
      "Iteration 20242, loss = 2.22012745\n",
      "Iteration 20243, loss = 2.23225526\n",
      "Iteration 20244, loss = 1.48931475\n",
      "Iteration 20245, loss = 1.37012786\n",
      "Iteration 20246, loss = 1.20944232\n",
      "Iteration 20247, loss = 1.55479384\n",
      "Iteration 20248, loss = 1.42807709\n",
      "Iteration 20249, loss = 1.26173512\n",
      "Iteration 20250, loss = 1.20728694\n",
      "Iteration 20251, loss = 1.18503726\n",
      "Iteration 20252, loss = 1.18215943\n",
      "Iteration 20253, loss = 1.20283116\n",
      "Iteration 20254, loss = 1.18110760\n",
      "Iteration 20255, loss = 1.15062520\n",
      "Iteration 20256, loss = 1.14959169\n",
      "Iteration 20257, loss = 1.20233214\n",
      "Iteration 20258, loss = 1.22545457\n",
      "Iteration 20259, loss = 1.21039693\n",
      "Iteration 20260, loss = 1.30614026\n",
      "Iteration 20261, loss = 1.17906263\n",
      "Iteration 20262, loss = 1.24077518\n",
      "Iteration 20263, loss = 1.32947597\n",
      "Iteration 20264, loss = 1.22463103\n",
      "Iteration 20265, loss = 1.22109801\n",
      "Iteration 20266, loss = 1.21864318\n",
      "Iteration 20267, loss = 1.25851480\n",
      "Iteration 20268, loss = 1.16559046\n",
      "Iteration 20269, loss = 1.21978892\n",
      "Iteration 20270, loss = 1.54179602\n",
      "Iteration 20271, loss = 1.53553088\n",
      "Iteration 20272, loss = 1.23814997\n",
      "Iteration 20273, loss = 1.40384379\n",
      "Iteration 20274, loss = 1.36215300\n",
      "Iteration 20275, loss = 1.29543722\n",
      "Iteration 20276, loss = 1.14744190\n",
      "Iteration 20277, loss = 1.31408770\n",
      "Iteration 20278, loss = 1.20650523\n",
      "Iteration 20279, loss = 1.11428420\n",
      "Iteration 20280, loss = 1.11522868\n",
      "Iteration 20281, loss = 1.17970069\n",
      "Iteration 20282, loss = 1.41268611\n",
      "Iteration 20283, loss = 1.80246124\n",
      "Iteration 20284, loss = 1.42119782\n",
      "Iteration 20285, loss = 1.18506359\n",
      "Iteration 20286, loss = 1.40768012\n",
      "Iteration 20287, loss = 1.50623348\n",
      "Iteration 20288, loss = 1.79877517\n",
      "Iteration 20289, loss = 1.51930152\n",
      "Iteration 20290, loss = 1.83655261\n",
      "Iteration 20291, loss = 1.36327817\n",
      "Iteration 20292, loss = 1.29855136\n",
      "Iteration 20293, loss = 1.19508917\n",
      "Iteration 20294, loss = 1.24509056\n",
      "Iteration 20295, loss = 1.45935310\n",
      "Iteration 20296, loss = 1.25945541\n",
      "Iteration 20297, loss = 1.99640450\n",
      "Iteration 20298, loss = 1.73357901\n",
      "Iteration 20299, loss = 1.51571408\n",
      "Iteration 20300, loss = 1.34705801\n",
      "Iteration 20301, loss = 1.17229818\n",
      "Iteration 20302, loss = 1.25281502\n",
      "Iteration 20303, loss = 1.26969677\n",
      "Iteration 20304, loss = 1.15595067\n",
      "Iteration 20305, loss = 1.20764992\n",
      "Iteration 20306, loss = 1.26281729\n",
      "Iteration 20307, loss = 1.25477227\n",
      "Iteration 20308, loss = 1.19677929\n",
      "Iteration 20309, loss = 1.30535155\n",
      "Iteration 20310, loss = 1.23983056\n",
      "Iteration 20311, loss = 1.17338618\n",
      "Iteration 20312, loss = 1.37274470\n",
      "Iteration 20313, loss = 2.75885767\n",
      "Iteration 20314, loss = 5.17488872\n",
      "Iteration 20315, loss = 2.27979244\n",
      "Iteration 20316, loss = 1.57424646\n",
      "Iteration 20317, loss = 1.39983078\n",
      "Iteration 20318, loss = 1.73950977\n",
      "Iteration 20319, loss = 2.00098509\n",
      "Iteration 20320, loss = 4.38354642\n",
      "Iteration 20321, loss = 3.28658576\n",
      "Iteration 20322, loss = 4.27855616\n",
      "Iteration 20323, loss = 4.15565384\n",
      "Iteration 20324, loss = 5.97207441\n",
      "Iteration 20325, loss = 4.21773121\n",
      "Iteration 20326, loss = 2.56738142\n",
      "Iteration 20327, loss = 1.95256745\n",
      "Iteration 20328, loss = 1.68940035\n",
      "Iteration 20329, loss = 1.49194568\n",
      "Iteration 20330, loss = 1.33804161\n",
      "Iteration 20331, loss = 1.28102593\n",
      "Iteration 20332, loss = 1.25771963\n",
      "Iteration 20333, loss = 1.30888473\n",
      "Iteration 20334, loss = 1.47927693\n",
      "Iteration 20335, loss = 1.28186985\n",
      "Iteration 20336, loss = 1.25178278\n",
      "Iteration 20337, loss = 1.29928469\n",
      "Iteration 20338, loss = 1.49198592\n",
      "Iteration 20339, loss = 1.59410353\n",
      "Iteration 20340, loss = 1.65294977\n",
      "Iteration 20341, loss = 2.20365881\n",
      "Iteration 20342, loss = 3.93598176\n",
      "Iteration 20343, loss = 2.15449716\n",
      "Iteration 20344, loss = 1.88399220\n",
      "Iteration 20345, loss = 1.66406114\n",
      "Iteration 20346, loss = 1.73673841\n",
      "Iteration 20347, loss = 2.13299019\n",
      "Iteration 20348, loss = 1.49359171\n",
      "Iteration 20349, loss = 1.29213780\n",
      "Iteration 20350, loss = 1.19054594\n",
      "Iteration 20351, loss = 1.15626109\n",
      "Iteration 20352, loss = 1.22337312\n",
      "Iteration 20353, loss = 1.16769307\n",
      "Iteration 20354, loss = 1.18502169\n",
      "Iteration 20355, loss = 1.24467514\n",
      "Iteration 20356, loss = 1.29205961\n",
      "Iteration 20357, loss = 1.45575025\n",
      "Iteration 20358, loss = 1.24913831\n",
      "Iteration 20359, loss = 1.29196623\n",
      "Iteration 20360, loss = 1.38212979\n",
      "Iteration 20361, loss = 1.46568696\n",
      "Iteration 20362, loss = 1.55909151\n",
      "Iteration 20363, loss = 1.77132802\n",
      "Iteration 20364, loss = 1.44212052\n",
      "Iteration 20365, loss = 1.44980213\n",
      "Iteration 20366, loss = 2.99259531\n",
      "Iteration 20367, loss = 2.09220139\n",
      "Iteration 20368, loss = 1.64492926\n",
      "Iteration 20369, loss = 1.52955879\n",
      "Iteration 20370, loss = 1.34279355\n",
      "Iteration 20371, loss = 1.28976872\n",
      "Iteration 20372, loss = 1.28641207\n",
      "Iteration 20373, loss = 1.89992878\n",
      "Iteration 20374, loss = 1.42910804\n",
      "Iteration 20375, loss = 1.16610397\n",
      "Iteration 20376, loss = 1.19010248\n",
      "Iteration 20377, loss = 1.20373979\n",
      "Iteration 20378, loss = 1.38102228\n",
      "Iteration 20379, loss = 1.25458625\n",
      "Iteration 20380, loss = 1.21033009\n",
      "Iteration 20381, loss = 1.39336464\n",
      "Iteration 20382, loss = 1.71486234\n",
      "Iteration 20383, loss = 1.28442001\n",
      "Iteration 20384, loss = 1.20982967\n",
      "Iteration 20385, loss = 1.36196035\n",
      "Iteration 20386, loss = 1.31046512\n",
      "Iteration 20387, loss = 1.26577051\n",
      "Iteration 20388, loss = 1.50631538\n",
      "Iteration 20389, loss = 1.49843985\n",
      "Iteration 20390, loss = 1.28027843\n",
      "Iteration 20391, loss = 1.47644937\n",
      "Iteration 20392, loss = 1.90334512\n",
      "Iteration 20393, loss = 1.71436275\n",
      "Iteration 20394, loss = 1.34682982\n",
      "Iteration 20395, loss = 1.60098530\n",
      "Iteration 20396, loss = 1.29055118\n",
      "Iteration 20397, loss = 1.27713662\n",
      "Iteration 20398, loss = 1.44479990\n",
      "Iteration 20399, loss = 1.85596988\n",
      "Iteration 20400, loss = 1.78822557\n",
      "Iteration 20401, loss = 2.65083346\n",
      "Iteration 20402, loss = 6.20084657\n",
      "Iteration 20403, loss = 6.18928561\n",
      "Iteration 20404, loss = 3.72320235\n",
      "Iteration 20405, loss = 2.52282065\n",
      "Iteration 20406, loss = 1.80121170\n",
      "Iteration 20407, loss = 2.22344010\n",
      "Iteration 20408, loss = 1.49849965\n",
      "Iteration 20409, loss = 1.39962812\n",
      "Iteration 20410, loss = 1.43235233\n",
      "Iteration 20411, loss = 1.23160507\n",
      "Iteration 20412, loss = 1.29382623\n",
      "Iteration 20413, loss = 1.25945469\n",
      "Iteration 20414, loss = 1.19619189\n",
      "Iteration 20415, loss = 1.17966456\n",
      "Iteration 20416, loss = 1.16272425\n",
      "Iteration 20417, loss = 1.22181776\n",
      "Iteration 20418, loss = 1.13190156\n",
      "Iteration 20419, loss = 1.20710389\n",
      "Iteration 20420, loss = 1.22471047\n",
      "Iteration 20421, loss = 1.28808683\n",
      "Iteration 20422, loss = 1.16393849\n",
      "Iteration 20423, loss = 1.21995284\n",
      "Iteration 20424, loss = 1.45682177\n",
      "Iteration 20425, loss = 1.21580397\n",
      "Iteration 20426, loss = 1.13134969\n",
      "Iteration 20427, loss = 1.18027443\n",
      "Iteration 20428, loss = 1.28397980\n",
      "Iteration 20429, loss = 1.18386293\n",
      "Iteration 20430, loss = 1.18954343\n",
      "Iteration 20431, loss = 1.23708852\n",
      "Iteration 20432, loss = 1.22744957\n",
      "Iteration 20433, loss = 1.95652226\n",
      "Iteration 20434, loss = 2.01464522\n",
      "Iteration 20435, loss = 1.34312374\n",
      "Iteration 20436, loss = 1.30392448\n",
      "Iteration 20437, loss = 1.25919277\n",
      "Iteration 20438, loss = 1.55023726\n",
      "Iteration 20439, loss = 1.52171225\n",
      "Iteration 20440, loss = 1.81724867\n",
      "Iteration 20441, loss = 1.41308093\n",
      "Iteration 20442, loss = 1.49717391\n",
      "Iteration 20443, loss = 1.68181603\n",
      "Iteration 20444, loss = 2.87815214\n",
      "Iteration 20445, loss = 4.26696129\n",
      "Iteration 20446, loss = 6.12995605\n",
      "Iteration 20447, loss = 4.49962075\n",
      "Iteration 20448, loss = 3.44500783\n",
      "Iteration 20449, loss = 1.91955251\n",
      "Iteration 20450, loss = 2.05797866\n",
      "Iteration 20451, loss = 1.93012360\n",
      "Iteration 20452, loss = 2.28067662\n",
      "Iteration 20453, loss = 1.92293445\n",
      "Iteration 20454, loss = 2.14357361\n",
      "Iteration 20455, loss = 1.62257884\n",
      "Iteration 20456, loss = 2.08664260\n",
      "Iteration 20457, loss = 1.90155960\n",
      "Iteration 20458, loss = 1.38681798\n",
      "Iteration 20459, loss = 1.25178824\n",
      "Iteration 20460, loss = 1.22437599\n",
      "Iteration 20461, loss = 1.41178300\n",
      "Iteration 20462, loss = 1.16135164\n",
      "Iteration 20463, loss = 1.21260212\n",
      "Iteration 20464, loss = 1.57870534\n",
      "Iteration 20465, loss = 1.51274807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20466, loss = 1.41118980\n",
      "Iteration 20467, loss = 1.38117334\n",
      "Iteration 20468, loss = 2.28937368\n",
      "Iteration 20469, loss = 2.13773021\n",
      "Iteration 20470, loss = 1.53431331\n",
      "Iteration 20471, loss = 1.55732872\n",
      "Iteration 20472, loss = 1.43493597\n",
      "Iteration 20473, loss = 1.18792850\n",
      "Iteration 20474, loss = 1.10366437\n",
      "Iteration 20475, loss = 1.60817085\n",
      "Iteration 20476, loss = 1.55143203\n",
      "Iteration 20477, loss = 1.91095173\n",
      "Iteration 20478, loss = 3.43259440\n",
      "Iteration 20479, loss = 2.54843482\n",
      "Iteration 20480, loss = 2.98940476\n",
      "Iteration 20481, loss = 2.06136135\n",
      "Iteration 20482, loss = 2.59425183\n",
      "Iteration 20483, loss = 2.64279494\n",
      "Iteration 20484, loss = 4.62650312\n",
      "Iteration 20485, loss = 3.10928431\n",
      "Iteration 20486, loss = 2.28488500\n",
      "Iteration 20487, loss = 1.44047150\n",
      "Iteration 20488, loss = 1.44362008\n",
      "Iteration 20489, loss = 1.34123025\n",
      "Iteration 20490, loss = 1.25844649\n",
      "Iteration 20491, loss = 1.35819558\n",
      "Iteration 20492, loss = 2.78482637\n",
      "Iteration 20493, loss = 2.29126024\n",
      "Iteration 20494, loss = 1.86625303\n",
      "Iteration 20495, loss = 1.80472875\n",
      "Iteration 20496, loss = 1.40576599\n",
      "Iteration 20497, loss = 1.79898229\n",
      "Iteration 20498, loss = 1.40859150\n",
      "Iteration 20499, loss = 1.46691020\n",
      "Iteration 20500, loss = 1.25367674\n",
      "Iteration 20501, loss = 1.27870395\n",
      "Iteration 20502, loss = 1.21289613\n",
      "Iteration 20503, loss = 1.24128100\n",
      "Iteration 20504, loss = 1.21553091\n",
      "Iteration 20505, loss = 1.21201642\n",
      "Iteration 20506, loss = 1.21658914\n",
      "Iteration 20507, loss = 1.14792024\n",
      "Iteration 20508, loss = 1.11924664\n",
      "Iteration 20509, loss = 1.15878559\n",
      "Iteration 20510, loss = 1.19746089\n",
      "Iteration 20511, loss = 1.22215221\n",
      "Iteration 20512, loss = 1.86072387\n",
      "Iteration 20513, loss = 1.80661658\n",
      "Iteration 20514, loss = 1.48112275\n",
      "Iteration 20515, loss = 1.29763305\n",
      "Iteration 20516, loss = 1.18778662\n",
      "Iteration 20517, loss = 1.33924036\n",
      "Iteration 20518, loss = 1.50952367\n",
      "Iteration 20519, loss = 1.40889168\n",
      "Iteration 20520, loss = 1.41047632\n",
      "Iteration 20521, loss = 1.50039160\n",
      "Iteration 20522, loss = 1.40087730\n",
      "Iteration 20523, loss = 1.26049414\n",
      "Iteration 20524, loss = 1.20611502\n",
      "Iteration 20525, loss = 1.30757641\n",
      "Iteration 20526, loss = 1.20001870\n",
      "Iteration 20527, loss = 1.26526780\n",
      "Iteration 20528, loss = 1.20577702\n",
      "Iteration 20529, loss = 1.29789593\n",
      "Iteration 20530, loss = 1.15274732\n",
      "Iteration 20531, loss = 1.18558140\n",
      "Iteration 20532, loss = 1.14000191\n",
      "Iteration 20533, loss = 1.19296008\n",
      "Iteration 20534, loss = 1.26771788\n",
      "Iteration 20535, loss = 1.27786699\n",
      "Iteration 20536, loss = 1.31510107\n",
      "Iteration 20537, loss = 1.14980150\n",
      "Iteration 20538, loss = 1.15511059\n",
      "Iteration 20539, loss = 1.20035955\n",
      "Iteration 20540, loss = 1.13667989\n",
      "Iteration 20541, loss = 1.14200433\n",
      "Iteration 20542, loss = 1.14318091\n",
      "Iteration 20543, loss = 1.33522354\n",
      "Iteration 20544, loss = 1.20103194\n",
      "Iteration 20545, loss = 1.13904611\n",
      "Iteration 20546, loss = 1.32550440\n",
      "Iteration 20547, loss = 1.26319118\n",
      "Iteration 20548, loss = 1.23799300\n",
      "Iteration 20549, loss = 1.23438081\n",
      "Iteration 20550, loss = 1.36901395\n",
      "Iteration 20551, loss = 1.18941677\n",
      "Iteration 20552, loss = 1.23428126\n",
      "Iteration 20553, loss = 1.51144178\n",
      "Iteration 20554, loss = 1.57489718\n",
      "Iteration 20555, loss = 1.32198118\n",
      "Iteration 20556, loss = 1.25127259\n",
      "Iteration 20557, loss = 1.39932289\n",
      "Iteration 20558, loss = 1.32256167\n",
      "Iteration 20559, loss = 1.22581101\n",
      "Iteration 20560, loss = 1.34078171\n",
      "Iteration 20561, loss = 1.33268050\n",
      "Iteration 20562, loss = 1.27533007\n",
      "Iteration 20563, loss = 1.17241198\n",
      "Iteration 20564, loss = 1.21142912\n",
      "Iteration 20565, loss = 1.25139218\n",
      "Iteration 20566, loss = 1.77941695\n",
      "Iteration 20567, loss = 1.52433704\n",
      "Iteration 20568, loss = 1.25802970\n",
      "Iteration 20569, loss = 1.28594720\n",
      "Iteration 20570, loss = 1.82941283\n",
      "Iteration 20571, loss = 1.18946148\n",
      "Iteration 20572, loss = 1.26092565\n",
      "Iteration 20573, loss = 1.20292019\n",
      "Iteration 20574, loss = 1.16886790\n",
      "Iteration 20575, loss = 1.14811599\n",
      "Iteration 20576, loss = 1.13554390\n",
      "Iteration 20577, loss = 1.27505153\n",
      "Iteration 20578, loss = 1.19689680\n",
      "Iteration 20579, loss = 1.12807979\n",
      "Iteration 20580, loss = 1.12593289\n",
      "Iteration 20581, loss = 1.13153170\n",
      "Iteration 20582, loss = 1.19779648\n",
      "Iteration 20583, loss = 1.23650427\n",
      "Iteration 20584, loss = 1.23944217\n",
      "Iteration 20585, loss = 1.11707287\n",
      "Iteration 20586, loss = 1.26701712\n",
      "Iteration 20587, loss = 1.29681043\n",
      "Iteration 20588, loss = 1.49874517\n",
      "Iteration 20589, loss = 1.45046235\n",
      "Iteration 20590, loss = 1.80880419\n",
      "Iteration 20591, loss = 1.84323341\n",
      "Iteration 20592, loss = 1.46561383\n",
      "Iteration 20593, loss = 1.50134231\n",
      "Iteration 20594, loss = 2.79670397\n",
      "Iteration 20595, loss = 3.09527116\n",
      "Iteration 20596, loss = 2.38958783\n",
      "Iteration 20597, loss = 1.42400445\n",
      "Iteration 20598, loss = 1.39955540\n",
      "Iteration 20599, loss = 1.62236986\n",
      "Iteration 20600, loss = 1.59335559\n",
      "Iteration 20601, loss = 1.49209310\n",
      "Iteration 20602, loss = 1.51911605\n",
      "Iteration 20603, loss = 2.14611399\n",
      "Iteration 20604, loss = 1.86259517\n",
      "Iteration 20605, loss = 3.67445821\n",
      "Iteration 20606, loss = 2.41439880\n",
      "Iteration 20607, loss = 2.10502477\n",
      "Iteration 20608, loss = 1.98802351\n",
      "Iteration 20609, loss = 1.75490861\n",
      "Iteration 20610, loss = 1.28147195\n",
      "Iteration 20611, loss = 1.40657836\n",
      "Iteration 20612, loss = 1.41270028\n",
      "Iteration 20613, loss = 1.24600020\n",
      "Iteration 20614, loss = 1.14842840\n",
      "Iteration 20615, loss = 1.19191273\n",
      "Iteration 20616, loss = 1.32371331\n",
      "Iteration 20617, loss = 1.18710391\n",
      "Iteration 20618, loss = 1.12497746\n",
      "Iteration 20619, loss = 1.10571708\n",
      "Iteration 20620, loss = 1.16120452\n",
      "Iteration 20621, loss = 1.12178753\n",
      "Iteration 20622, loss = 1.16290179\n",
      "Iteration 20623, loss = 1.20636862\n",
      "Iteration 20624, loss = 1.19200477\n",
      "Iteration 20625, loss = 1.23200152\n",
      "Iteration 20626, loss = 2.11508514\n",
      "Iteration 20627, loss = 1.44907241\n",
      "Iteration 20628, loss = 1.88011602\n",
      "Iteration 20629, loss = 1.52507654\n",
      "Iteration 20630, loss = 1.56344665\n",
      "Iteration 20631, loss = 1.58511900\n",
      "Iteration 20632, loss = 1.26199144\n",
      "Iteration 20633, loss = 1.42864236\n",
      "Iteration 20634, loss = 1.48979546\n",
      "Iteration 20635, loss = 1.96734751\n",
      "Iteration 20636, loss = 2.74098087\n",
      "Iteration 20637, loss = 1.85844491\n",
      "Iteration 20638, loss = 1.58930648\n",
      "Iteration 20639, loss = 1.27996475\n",
      "Iteration 20640, loss = 1.22928975\n",
      "Iteration 20641, loss = 1.16883673\n",
      "Iteration 20642, loss = 1.12903093\n",
      "Iteration 20643, loss = 1.17640141\n",
      "Iteration 20644, loss = 1.50295421\n",
      "Iteration 20645, loss = 1.34635255\n",
      "Iteration 20646, loss = 1.44542359\n",
      "Iteration 20647, loss = 1.35565609\n",
      "Iteration 20648, loss = 1.22446560\n",
      "Iteration 20649, loss = 1.71440927\n",
      "Iteration 20650, loss = 1.62863045\n",
      "Iteration 20651, loss = 1.41583000\n",
      "Iteration 20652, loss = 3.09505591\n",
      "Iteration 20653, loss = 3.26323310\n",
      "Iteration 20654, loss = 2.68451979\n",
      "Iteration 20655, loss = 1.94254416\n",
      "Iteration 20656, loss = 1.99170385\n",
      "Iteration 20657, loss = 1.99067640\n",
      "Iteration 20658, loss = 2.16030482\n",
      "Iteration 20659, loss = 5.32328761\n",
      "Iteration 20660, loss = 3.74902326\n",
      "Iteration 20661, loss = 3.94414500\n",
      "Iteration 20662, loss = 2.12677840\n",
      "Iteration 20663, loss = 3.12775355\n",
      "Iteration 20664, loss = 2.70284836\n",
      "Iteration 20665, loss = 2.32835172\n",
      "Iteration 20666, loss = 3.15030701\n",
      "Iteration 20667, loss = 3.46350022\n",
      "Iteration 20668, loss = 3.41324521\n",
      "Iteration 20669, loss = 2.08314781\n",
      "Iteration 20670, loss = 1.77666156\n",
      "Iteration 20671, loss = 1.81719641\n",
      "Iteration 20672, loss = 1.80449404\n",
      "Iteration 20673, loss = 2.74115323\n",
      "Iteration 20674, loss = 2.01612517\n",
      "Iteration 20675, loss = 2.32009452\n",
      "Iteration 20676, loss = 1.65473832\n",
      "Iteration 20677, loss = 1.60704132\n",
      "Iteration 20678, loss = 1.21675802\n",
      "Iteration 20679, loss = 1.45698804\n",
      "Iteration 20680, loss = 1.30984211\n",
      "Iteration 20681, loss = 1.44288768\n",
      "Iteration 20682, loss = 1.81583183\n",
      "Iteration 20683, loss = 1.22899270\n",
      "Iteration 20684, loss = 1.77609694\n",
      "Iteration 20685, loss = 2.50122514\n",
      "Iteration 20686, loss = 1.18515519\n",
      "Iteration 20687, loss = 1.35359514\n",
      "Iteration 20688, loss = 1.57601014\n",
      "Iteration 20689, loss = 1.21505281\n",
      "Iteration 20690, loss = 1.59376922\n",
      "Iteration 20691, loss = 1.37573360\n",
      "Iteration 20692, loss = 1.74789507\n",
      "Iteration 20693, loss = 1.37810629\n",
      "Iteration 20694, loss = 1.39426258\n",
      "Iteration 20695, loss = 1.45923663\n",
      "Iteration 20696, loss = 1.27120014\n",
      "Iteration 20697, loss = 1.22434505\n",
      "Iteration 20698, loss = 1.19641836\n",
      "Iteration 20699, loss = 1.16978404\n",
      "Iteration 20700, loss = 1.15439959\n",
      "Iteration 20701, loss = 1.22214299\n",
      "Iteration 20702, loss = 1.24170402\n",
      "Iteration 20703, loss = 1.28652284\n",
      "Iteration 20704, loss = 1.90234804\n",
      "Iteration 20705, loss = 2.14583193\n",
      "Iteration 20706, loss = 1.76228592\n",
      "Iteration 20707, loss = 1.41515422\n",
      "Iteration 20708, loss = 1.35276308\n",
      "Iteration 20709, loss = 1.19994492\n",
      "Iteration 20710, loss = 1.27740640\n",
      "Iteration 20711, loss = 1.33774761\n",
      "Iteration 20712, loss = 1.26798748\n",
      "Iteration 20713, loss = 1.22131250\n",
      "Iteration 20714, loss = 1.49466757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20715, loss = 1.50643586\n",
      "Iteration 20716, loss = 1.39945558\n",
      "Iteration 20717, loss = 1.74774178\n",
      "Iteration 20718, loss = 1.66920067\n",
      "Iteration 20719, loss = 1.27503376\n",
      "Iteration 20720, loss = 1.57318299\n",
      "Iteration 20721, loss = 1.74808319\n",
      "Iteration 20722, loss = 2.34041178\n",
      "Iteration 20723, loss = 1.17077385\n",
      "Iteration 20724, loss = 1.34037728\n",
      "Iteration 20725, loss = 1.76087005\n",
      "Iteration 20726, loss = 1.30939094\n",
      "Iteration 20727, loss = 1.35019591\n",
      "Iteration 20728, loss = 1.15461738\n",
      "Iteration 20729, loss = 1.20336195\n",
      "Iteration 20730, loss = 1.27829954\n",
      "Iteration 20731, loss = 1.30686967\n",
      "Iteration 20732, loss = 1.47776969\n",
      "Iteration 20733, loss = 1.14870578\n",
      "Iteration 20734, loss = 1.23133069\n",
      "Iteration 20735, loss = 1.26439867\n",
      "Iteration 20736, loss = 1.13287113\n",
      "Iteration 20737, loss = 1.21945495\n",
      "Iteration 20738, loss = 1.23052562\n",
      "Iteration 20739, loss = 1.14911834\n",
      "Iteration 20740, loss = 1.30223300\n",
      "Iteration 20741, loss = 1.16530783\n",
      "Iteration 20742, loss = 1.11430987\n",
      "Iteration 20743, loss = 1.19549536\n",
      "Iteration 20744, loss = 2.77980593\n",
      "Iteration 20745, loss = 9.41075152\n",
      "Iteration 20746, loss = 13.09011426\n",
      "Iteration 20747, loss = 12.52139107\n",
      "Iteration 20748, loss = 10.87835697\n",
      "Iteration 20749, loss = 9.17673857\n",
      "Iteration 20750, loss = 8.48128917\n",
      "Iteration 20751, loss = 8.25400794\n",
      "Iteration 20752, loss = 8.45912393\n",
      "Iteration 20753, loss = 6.12822405\n",
      "Iteration 20754, loss = 4.40365642\n",
      "Iteration 20755, loss = 3.80693495\n",
      "Iteration 20756, loss = 3.52602266\n",
      "Iteration 20757, loss = 3.18403353\n",
      "Iteration 20758, loss = 3.77834100\n",
      "Iteration 20759, loss = 3.24324799\n",
      "Iteration 20760, loss = 3.30528884\n",
      "Iteration 20761, loss = 3.57398654\n",
      "Iteration 20762, loss = 2.99400443\n",
      "Iteration 20763, loss = 3.11085705\n",
      "Iteration 20764, loss = 3.02615561\n",
      "Iteration 20765, loss = 2.30187377\n",
      "Iteration 20766, loss = 1.94152191\n",
      "Iteration 20767, loss = 1.45719941\n",
      "Iteration 20768, loss = 1.41858708\n",
      "Iteration 20769, loss = 1.47346313\n",
      "Iteration 20770, loss = 1.73368723\n",
      "Iteration 20771, loss = 1.92276935\n",
      "Iteration 20772, loss = 1.62103866\n",
      "Iteration 20773, loss = 1.66630070\n",
      "Iteration 20774, loss = 1.47146230\n",
      "Iteration 20775, loss = 1.44066379\n",
      "Iteration 20776, loss = 2.05562762\n",
      "Iteration 20777, loss = 3.67833723\n",
      "Iteration 20778, loss = 2.33102320\n",
      "Iteration 20779, loss = 1.77562245\n",
      "Iteration 20780, loss = 1.53728214\n",
      "Iteration 20781, loss = 1.28242098\n",
      "Iteration 20782, loss = 1.29698171\n",
      "Iteration 20783, loss = 1.18894389\n",
      "Iteration 20784, loss = 1.20093226\n",
      "Iteration 20785, loss = 1.18299777\n",
      "Iteration 20786, loss = 1.24627274\n",
      "Iteration 20787, loss = 1.16126335\n",
      "Iteration 20788, loss = 1.13128769\n",
      "Iteration 20789, loss = 1.14716695\n",
      "Iteration 20790, loss = 1.16718526\n",
      "Iteration 20791, loss = 1.37467138\n",
      "Iteration 20792, loss = 1.22839897\n",
      "Iteration 20793, loss = 1.14005871\n",
      "Iteration 20794, loss = 1.27994156\n",
      "Iteration 20795, loss = 1.18504714\n",
      "Iteration 20796, loss = 1.32762003\n",
      "Iteration 20797, loss = 1.29456254\n",
      "Iteration 20798, loss = 1.14617815\n",
      "Iteration 20799, loss = 1.14310749\n",
      "Iteration 20800, loss = 1.17075797\n",
      "Iteration 20801, loss = 1.11953073\n",
      "Iteration 20802, loss = 1.10758855\n",
      "Iteration 20803, loss = 1.10762123\n",
      "Iteration 20804, loss = 1.13132682\n",
      "Iteration 20805, loss = 1.13607490\n",
      "Iteration 20806, loss = 1.16692355\n",
      "Iteration 20807, loss = 1.39044629\n",
      "Iteration 20808, loss = 1.42019013\n",
      "Iteration 20809, loss = 1.31715904\n",
      "Iteration 20810, loss = 1.28400875\n",
      "Iteration 20811, loss = 1.18440319\n",
      "Iteration 20812, loss = 1.14568766\n",
      "Iteration 20813, loss = 1.41285776\n",
      "Iteration 20814, loss = 1.27642101\n",
      "Iteration 20815, loss = 1.34776023\n",
      "Iteration 20816, loss = 1.34781072\n",
      "Iteration 20817, loss = 1.31926623\n",
      "Iteration 20818, loss = 1.14779190\n",
      "Iteration 20819, loss = 1.16259320\n",
      "Iteration 20820, loss = 1.10466005\n",
      "Iteration 20821, loss = 1.14017608\n",
      "Iteration 20822, loss = 1.28622665\n",
      "Iteration 20823, loss = 1.49674567\n",
      "Iteration 20824, loss = 1.55799546\n",
      "Iteration 20825, loss = 1.13195664\n",
      "Iteration 20826, loss = 1.16332635\n",
      "Iteration 20827, loss = 1.31869618\n",
      "Iteration 20828, loss = 1.23150958\n",
      "Iteration 20829, loss = 1.18631562\n",
      "Iteration 20830, loss = 1.49974305\n",
      "Iteration 20831, loss = 1.97385303\n",
      "Iteration 20832, loss = 1.54542245\n",
      "Iteration 20833, loss = 1.52604039\n",
      "Iteration 20834, loss = 1.22101611\n",
      "Iteration 20835, loss = 1.15313989\n",
      "Iteration 20836, loss = 1.07469666\n",
      "Iteration 20837, loss = 1.16734306\n",
      "Iteration 20838, loss = 1.19280148\n",
      "Iteration 20839, loss = 1.32234437\n",
      "Iteration 20840, loss = 1.15111425\n",
      "Iteration 20841, loss = 1.12873289\n",
      "Iteration 20842, loss = 1.12294261\n",
      "Iteration 20843, loss = 1.11030290\n",
      "Iteration 20844, loss = 1.12398450\n",
      "Iteration 20845, loss = 1.10385909\n",
      "Iteration 20846, loss = 1.22300683\n",
      "Iteration 20847, loss = 1.28112078\n",
      "Iteration 20848, loss = 1.52635271\n",
      "Iteration 20849, loss = 1.41012395\n",
      "Iteration 20850, loss = 1.50132615\n",
      "Iteration 20851, loss = 1.25822134\n",
      "Iteration 20852, loss = 1.18752913\n",
      "Iteration 20853, loss = 1.18595267\n",
      "Iteration 20854, loss = 1.21651359\n",
      "Iteration 20855, loss = 1.13047994\n",
      "Iteration 20856, loss = 1.29453115\n",
      "Iteration 20857, loss = 1.13938834\n",
      "Iteration 20858, loss = 1.11106020\n",
      "Iteration 20859, loss = 1.09938816\n",
      "Iteration 20860, loss = 1.11421738\n",
      "Iteration 20861, loss = 1.47476295\n",
      "Iteration 20862, loss = 1.65925834\n",
      "Iteration 20863, loss = 1.38572503\n",
      "Iteration 20864, loss = 1.36383412\n",
      "Iteration 20865, loss = 1.16306739\n",
      "Iteration 20866, loss = 1.16535659\n",
      "Iteration 20867, loss = 1.32723301\n",
      "Iteration 20868, loss = 1.26191851\n",
      "Iteration 20869, loss = 1.23910405\n",
      "Iteration 20870, loss = 1.21058940\n",
      "Iteration 20871, loss = 1.17253858\n",
      "Iteration 20872, loss = 1.39766040\n",
      "Iteration 20873, loss = 1.22218444\n",
      "Iteration 20874, loss = 1.33098687\n",
      "Iteration 20875, loss = 1.45853847\n",
      "Iteration 20876, loss = 1.33281826\n",
      "Iteration 20877, loss = 1.26283020\n",
      "Iteration 20878, loss = 1.19805317\n",
      "Iteration 20879, loss = 1.46976446\n",
      "Iteration 20880, loss = 2.79881666\n",
      "Iteration 20881, loss = 3.78998679\n",
      "Iteration 20882, loss = 2.06399714\n",
      "Iteration 20883, loss = 1.60761074\n",
      "Iteration 20884, loss = 1.28268680\n",
      "Iteration 20885, loss = 1.93556090\n",
      "Iteration 20886, loss = 2.28949781\n",
      "Iteration 20887, loss = 2.14818692\n",
      "Iteration 20888, loss = 3.96958972\n",
      "Iteration 20889, loss = 3.14853420\n",
      "Iteration 20890, loss = 4.42706464\n",
      "Iteration 20891, loss = 5.89337001\n",
      "Iteration 20892, loss = 6.82651028\n",
      "Iteration 20893, loss = 4.97635124\n",
      "Iteration 20894, loss = 4.37379434\n",
      "Iteration 20895, loss = 3.19186877\n",
      "Iteration 20896, loss = 3.42199705\n",
      "Iteration 20897, loss = 1.69856335\n",
      "Iteration 20898, loss = 1.60964510\n",
      "Iteration 20899, loss = 1.71493243\n",
      "Iteration 20900, loss = 1.54507670\n",
      "Iteration 20901, loss = 1.54214300\n",
      "Iteration 20902, loss = 1.76333816\n",
      "Iteration 20903, loss = 1.46146835\n",
      "Iteration 20904, loss = 1.24817506\n",
      "Iteration 20905, loss = 1.13673509\n",
      "Iteration 20906, loss = 1.12505702\n",
      "Iteration 20907, loss = 1.33553913\n",
      "Iteration 20908, loss = 1.78452481\n",
      "Iteration 20909, loss = 1.75558867\n",
      "Iteration 20910, loss = 3.02468315\n",
      "Iteration 20911, loss = 3.26044547\n",
      "Iteration 20912, loss = 2.45307913\n",
      "Iteration 20913, loss = 1.51689066\n",
      "Iteration 20914, loss = 1.56858683\n",
      "Iteration 20915, loss = 1.65008505\n",
      "Iteration 20916, loss = 2.27691888\n",
      "Iteration 20917, loss = 2.12707533\n",
      "Iteration 20918, loss = 1.57533706\n",
      "Iteration 20919, loss = 1.60059795\n",
      "Iteration 20920, loss = 1.33873908\n",
      "Iteration 20921, loss = 1.34813843\n",
      "Iteration 20922, loss = 1.33578044\n",
      "Iteration 20923, loss = 1.30764974\n",
      "Iteration 20924, loss = 1.28618274\n",
      "Iteration 20925, loss = 1.38009650\n",
      "Iteration 20926, loss = 1.25612643\n",
      "Iteration 20927, loss = 1.40389355\n",
      "Iteration 20928, loss = 1.20865991\n",
      "Iteration 20929, loss = 1.28027519\n",
      "Iteration 20930, loss = 1.16644530\n",
      "Iteration 20931, loss = 1.21294078\n",
      "Iteration 20932, loss = 1.14763503\n",
      "Iteration 20933, loss = 1.12252651\n",
      "Iteration 20934, loss = 1.10203976\n",
      "Iteration 20935, loss = 1.09176945\n",
      "Iteration 20936, loss = 1.14769251\n",
      "Iteration 20937, loss = 1.13248308\n",
      "Iteration 20938, loss = 1.20804890\n",
      "Iteration 20939, loss = 1.31044600\n",
      "Iteration 20940, loss = 1.32708305\n",
      "Iteration 20941, loss = 1.40400107\n",
      "Iteration 20942, loss = 1.49730760\n",
      "Iteration 20943, loss = 1.65615467\n",
      "Iteration 20944, loss = 1.36244373\n",
      "Iteration 20945, loss = 1.34968238\n",
      "Iteration 20946, loss = 1.13492976\n",
      "Iteration 20947, loss = 1.08296031\n",
      "Iteration 20948, loss = 1.15398099\n",
      "Iteration 20949, loss = 1.16465394\n",
      "Iteration 20950, loss = 1.12098447\n",
      "Iteration 20951, loss = 1.13003030\n",
      "Iteration 20952, loss = 1.13844446\n",
      "Iteration 20953, loss = 1.12908961\n",
      "Iteration 20954, loss = 1.12696733\n",
      "Iteration 20955, loss = 1.07343898\n",
      "Iteration 20956, loss = 1.10174663\n",
      "Iteration 20957, loss = 1.10372740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20958, loss = 1.11454413\n",
      "Iteration 20959, loss = 1.11004430\n",
      "Iteration 20960, loss = 1.12853264\n",
      "Iteration 20961, loss = 1.19900668\n",
      "Iteration 20962, loss = 1.23535649\n",
      "Iteration 20963, loss = 1.16752165\n",
      "Iteration 20964, loss = 1.22684717\n",
      "Iteration 20965, loss = 1.48159311\n",
      "Iteration 20966, loss = 1.43077509\n",
      "Iteration 20967, loss = 1.45934694\n",
      "Iteration 20968, loss = 1.56151359\n",
      "Iteration 20969, loss = 1.44758406\n",
      "Iteration 20970, loss = 1.32889374\n",
      "Iteration 20971, loss = 1.10182662\n",
      "Iteration 20972, loss = 1.17546536\n",
      "Iteration 20973, loss = 1.11069746\n",
      "Iteration 20974, loss = 1.10958046\n",
      "Iteration 20975, loss = 1.25042000\n",
      "Iteration 20976, loss = 1.14171214\n",
      "Iteration 20977, loss = 1.11729824\n",
      "Iteration 20978, loss = 1.14407514\n",
      "Iteration 20979, loss = 1.12122548\n",
      "Iteration 20980, loss = 1.11446999\n",
      "Iteration 20981, loss = 1.20234853\n",
      "Iteration 20982, loss = 1.08673671\n",
      "Iteration 20983, loss = 1.11583799\n",
      "Iteration 20984, loss = 1.11335904\n",
      "Iteration 20985, loss = 1.17522441\n",
      "Iteration 20986, loss = 1.20734570\n",
      "Iteration 20987, loss = 1.21241546\n",
      "Iteration 20988, loss = 1.13390733\n",
      "Iteration 20989, loss = 1.34812471\n",
      "Iteration 20990, loss = 1.74625699\n",
      "Iteration 20991, loss = 2.87830046\n",
      "Iteration 20992, loss = 9.03455071\n",
      "Iteration 20993, loss = 14.22711236\n",
      "Iteration 20994, loss = 12.50450076\n",
      "Iteration 20995, loss = 10.98039605\n",
      "Iteration 20996, loss = 10.70511950\n",
      "Iteration 20997, loss = 9.63282928\n",
      "Iteration 20998, loss = 9.05751152\n",
      "Iteration 20999, loss = 7.81488358\n",
      "Iteration 21000, loss = 6.32446303\n",
      "Iteration 21001, loss = 5.27737698\n",
      "Iteration 21002, loss = 4.27002050\n",
      "Iteration 21003, loss = 3.64346606\n",
      "Iteration 21004, loss = 3.92342230\n",
      "Iteration 21005, loss = 5.71413421\n",
      "Iteration 21006, loss = 3.63140331\n",
      "Iteration 21007, loss = 3.31633500\n",
      "Iteration 21008, loss = 2.94585154\n",
      "Iteration 21009, loss = 3.43562900\n",
      "Iteration 21010, loss = 2.74703890\n",
      "Iteration 21011, loss = 2.93961051\n",
      "Iteration 21012, loss = 3.42917231\n",
      "Iteration 21013, loss = 2.70940446\n",
      "Iteration 21014, loss = 2.44218601\n",
      "Iteration 21015, loss = 2.74060980\n",
      "Iteration 21016, loss = 2.70734725\n",
      "Iteration 21017, loss = 2.67489116\n",
      "Iteration 21018, loss = 2.83332790\n",
      "Iteration 21019, loss = 3.01552240\n",
      "Iteration 21020, loss = 3.06507384\n",
      "Iteration 21021, loss = 3.17207155\n",
      "Iteration 21022, loss = 2.66005968\n",
      "Iteration 21023, loss = 2.44995915\n",
      "Iteration 21024, loss = 2.34277617\n",
      "Iteration 21025, loss = 2.36690280\n",
      "Iteration 21026, loss = 2.31087838\n",
      "Iteration 21027, loss = 2.34716302\n",
      "Iteration 21028, loss = 2.25217282\n",
      "Iteration 21029, loss = 2.33507253\n",
      "Iteration 21030, loss = 2.60256292\n",
      "Iteration 21031, loss = 2.22717055\n",
      "Iteration 21032, loss = 2.32801040\n",
      "Iteration 21033, loss = 2.36598176\n",
      "Iteration 21034, loss = 2.58035913\n",
      "Iteration 21035, loss = 2.56719224\n",
      "Iteration 21036, loss = 1.95572597\n",
      "Iteration 21037, loss = 1.97306577\n",
      "Iteration 21038, loss = 1.38698101\n",
      "Iteration 21039, loss = 1.37737488\n",
      "Iteration 21040, loss = 1.67799148\n",
      "Iteration 21041, loss = 1.31220534\n",
      "Iteration 21042, loss = 1.24215911\n",
      "Iteration 21043, loss = 1.29977930\n",
      "Iteration 21044, loss = 1.15346022\n",
      "Iteration 21045, loss = 1.41108252\n",
      "Iteration 21046, loss = 1.26268332\n",
      "Iteration 21047, loss = 1.31948210\n",
      "Iteration 21048, loss = 2.14697562\n",
      "Iteration 21049, loss = 2.24433704\n",
      "Iteration 21050, loss = 1.73256095\n",
      "Iteration 21051, loss = 1.63292714\n",
      "Iteration 21052, loss = 1.53343321\n",
      "Iteration 21053, loss = 1.35137600\n",
      "Iteration 21054, loss = 1.26779098\n",
      "Iteration 21055, loss = 1.51479208\n",
      "Iteration 21056, loss = 1.62134359\n",
      "Iteration 21057, loss = 1.82164595\n",
      "Iteration 21058, loss = 2.46957271\n",
      "Iteration 21059, loss = 1.69577692\n",
      "Iteration 21060, loss = 1.71940047\n",
      "Iteration 21061, loss = 1.39568904\n",
      "Iteration 21062, loss = 1.46672197\n",
      "Iteration 21063, loss = 1.35544217\n",
      "Iteration 21064, loss = 1.15252435\n",
      "Iteration 21065, loss = 1.42711785\n",
      "Iteration 21066, loss = 1.34351471\n",
      "Iteration 21067, loss = 1.28031532\n",
      "Iteration 21068, loss = 1.35555332\n",
      "Iteration 21069, loss = 1.18260241\n",
      "Iteration 21070, loss = 1.21764285\n",
      "Iteration 21071, loss = 1.14885325\n",
      "Iteration 21072, loss = 1.17229941\n",
      "Iteration 21073, loss = 1.21403423\n",
      "Iteration 21074, loss = 1.13708828\n",
      "Iteration 21075, loss = 1.10911907\n",
      "Iteration 21076, loss = 1.10442702\n",
      "Iteration 21077, loss = 1.14171674\n",
      "Iteration 21078, loss = 1.22133671\n",
      "Iteration 21079, loss = 1.46639489\n",
      "Iteration 21080, loss = 1.29843030\n",
      "Iteration 21081, loss = 1.38257200\n",
      "Iteration 21082, loss = 1.40877904\n",
      "Iteration 21083, loss = 1.23459418\n",
      "Iteration 21084, loss = 1.30594524\n",
      "Iteration 21085, loss = 1.12911518\n",
      "Iteration 21086, loss = 1.16710389\n",
      "Iteration 21087, loss = 1.12054493\n",
      "Iteration 21088, loss = 1.16558552\n",
      "Iteration 21089, loss = 1.16021143\n",
      "Iteration 21090, loss = 1.11985527\n",
      "Iteration 21091, loss = 1.14925305\n",
      "Iteration 21092, loss = 1.13467460\n",
      "Iteration 21093, loss = 1.47324217\n",
      "Iteration 21094, loss = 2.19914294\n",
      "Iteration 21095, loss = 2.13007120\n",
      "Iteration 21096, loss = 2.19561970\n",
      "Iteration 21097, loss = 2.19005921\n",
      "Iteration 21098, loss = 2.85467712\n",
      "Iteration 21099, loss = 2.01447765\n",
      "Iteration 21100, loss = 2.15293975\n",
      "Iteration 21101, loss = 1.91442619\n",
      "Iteration 21102, loss = 1.90482118\n",
      "Iteration 21103, loss = 1.81061455\n",
      "Iteration 21104, loss = 4.27372007\n",
      "Iteration 21105, loss = 3.00931865\n",
      "Iteration 21106, loss = 2.67316013\n",
      "Iteration 21107, loss = 1.71366906\n",
      "Iteration 21108, loss = 2.13407166\n",
      "Iteration 21109, loss = 2.13040753\n",
      "Iteration 21110, loss = 2.25242438\n",
      "Iteration 21111, loss = 1.62558557\n",
      "Iteration 21112, loss = 1.78644364\n",
      "Iteration 21113, loss = 2.12943988\n",
      "Iteration 21114, loss = 2.72582356\n",
      "Iteration 21115, loss = 1.54832729\n",
      "Iteration 21116, loss = 1.27949419\n",
      "Iteration 21117, loss = 1.17675009\n",
      "Iteration 21118, loss = 1.16798928\n",
      "Iteration 21119, loss = 1.12419585\n",
      "Iteration 21120, loss = 1.22396854\n",
      "Iteration 21121, loss = 1.24453000\n",
      "Iteration 21122, loss = 1.20457246\n",
      "Iteration 21123, loss = 1.20334746\n",
      "Iteration 21124, loss = 1.12390185\n",
      "Iteration 21125, loss = 1.11954190\n",
      "Iteration 21126, loss = 1.12883148\n",
      "Iteration 21127, loss = 1.21198607\n",
      "Iteration 21128, loss = 1.20824353\n",
      "Iteration 21129, loss = 1.20232551\n",
      "Iteration 21130, loss = 1.16414724\n",
      "Iteration 21131, loss = 1.17017524\n",
      "Iteration 21132, loss = 1.21407905\n",
      "Iteration 21133, loss = 1.31655052\n",
      "Iteration 21134, loss = 1.21190917\n",
      "Iteration 21135, loss = 1.28855719\n",
      "Iteration 21136, loss = 1.16049368\n",
      "Iteration 21137, loss = 1.13638059\n",
      "Iteration 21138, loss = 1.14197767\n",
      "Iteration 21139, loss = 1.13210957\n",
      "Iteration 21140, loss = 1.10180554\n",
      "Iteration 21141, loss = 1.09275859\n",
      "Iteration 21142, loss = 1.11084232\n",
      "Iteration 21143, loss = 1.12701933\n",
      "Iteration 21144, loss = 1.10531006\n",
      "Iteration 21145, loss = 1.30315567\n",
      "Iteration 21146, loss = 1.20073915\n",
      "Iteration 21147, loss = 1.11250231\n",
      "Iteration 21148, loss = 1.07830134\n",
      "Iteration 21149, loss = 1.08642171\n",
      "Iteration 21150, loss = 1.10596610\n",
      "Iteration 21151, loss = 1.08424859\n",
      "Iteration 21152, loss = 1.14512272\n",
      "Iteration 21153, loss = 1.13428204\n",
      "Iteration 21154, loss = 1.25595815\n",
      "Iteration 21155, loss = 1.18278276\n",
      "Iteration 21156, loss = 1.22105140\n",
      "Iteration 21157, loss = 1.14207828\n",
      "Iteration 21158, loss = 1.11723887\n",
      "Iteration 21159, loss = 1.21362781\n",
      "Iteration 21160, loss = 1.19427794\n",
      "Iteration 21161, loss = 1.14884130\n",
      "Iteration 21162, loss = 1.14035986\n",
      "Iteration 21163, loss = 1.16898536\n",
      "Iteration 21164, loss = 1.11930637\n",
      "Iteration 21165, loss = 1.10818439\n",
      "Iteration 21166, loss = 1.09092198\n",
      "Iteration 21167, loss = 1.21699954\n",
      "Iteration 21168, loss = 1.17014341\n",
      "Iteration 21169, loss = 1.10380067\n",
      "Iteration 21170, loss = 1.15034391\n",
      "Iteration 21171, loss = 1.12002386\n",
      "Iteration 21172, loss = 1.11774154\n",
      "Iteration 21173, loss = 1.09569736\n",
      "Iteration 21174, loss = 1.09577697\n",
      "Iteration 21175, loss = 1.08658732\n",
      "Iteration 21176, loss = 1.08562971\n",
      "Iteration 21177, loss = 1.09861320\n",
      "Iteration 21178, loss = 1.14779064\n",
      "Iteration 21179, loss = 1.12982057\n",
      "Iteration 21180, loss = 1.23361539\n",
      "Iteration 21181, loss = 1.27178338\n",
      "Iteration 21182, loss = 1.26134683\n",
      "Iteration 21183, loss = 1.80724384\n",
      "Iteration 21184, loss = 1.44992819\n",
      "Iteration 21185, loss = 1.48820424\n",
      "Iteration 21186, loss = 1.40701187\n",
      "Iteration 21187, loss = 1.44775144\n",
      "Iteration 21188, loss = 1.29032772\n",
      "Iteration 21189, loss = 1.35263868\n",
      "Iteration 21190, loss = 1.18478778\n",
      "Iteration 21191, loss = 1.12669673\n",
      "Iteration 21192, loss = 1.14378290\n",
      "Iteration 21193, loss = 1.23544506\n",
      "Iteration 21194, loss = 1.21817610\n",
      "Iteration 21195, loss = 1.18829393\n",
      "Iteration 21196, loss = 1.46297367\n",
      "Iteration 21197, loss = 1.41387759\n",
      "Iteration 21198, loss = 1.27913993\n",
      "Iteration 21199, loss = 1.10335659\n",
      "Iteration 21200, loss = 1.04629527\n",
      "Iteration 21201, loss = 1.16794155\n",
      "Iteration 21202, loss = 1.34947251\n",
      "Iteration 21203, loss = 1.29379864\n",
      "Iteration 21204, loss = 1.24256347\n",
      "Iteration 21205, loss = 1.21321465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21206, loss = 1.11415443\n",
      "Iteration 21207, loss = 1.52999799\n",
      "Iteration 21208, loss = 1.35033389\n",
      "Iteration 21209, loss = 1.47564640\n",
      "Iteration 21210, loss = 1.24094210\n",
      "Iteration 21211, loss = 1.21781371\n",
      "Iteration 21212, loss = 1.36337916\n",
      "Iteration 21213, loss = 1.44226745\n",
      "Iteration 21214, loss = 1.20107619\n",
      "Iteration 21215, loss = 1.42808133\n",
      "Iteration 21216, loss = 1.44579903\n",
      "Iteration 21217, loss = 1.47829677\n",
      "Iteration 21218, loss = 6.02278697\n",
      "Iteration 21219, loss = 5.18236793\n",
      "Iteration 21220, loss = 4.93172844\n",
      "Iteration 21221, loss = 1.96790242\n",
      "Iteration 21222, loss = 3.02488614\n",
      "Iteration 21223, loss = 3.75355866\n",
      "Iteration 21224, loss = 2.38784256\n",
      "Iteration 21225, loss = 2.19323229\n",
      "Iteration 21226, loss = 1.37919140\n",
      "Iteration 21227, loss = 1.68066138\n",
      "Iteration 21228, loss = 1.35348569\n",
      "Iteration 21229, loss = 1.21890605\n",
      "Iteration 21230, loss = 1.17194773\n",
      "Iteration 21231, loss = 1.29407604\n",
      "Iteration 21232, loss = 1.32498993\n",
      "Iteration 21233, loss = 1.27934114\n",
      "Iteration 21234, loss = 1.26676882\n",
      "Iteration 21235, loss = 1.31304024\n",
      "Iteration 21236, loss = 1.21431632\n",
      "Iteration 21237, loss = 1.14680277\n",
      "Iteration 21238, loss = 1.11471458\n",
      "Iteration 21239, loss = 1.13442743\n",
      "Iteration 21240, loss = 1.18418644\n",
      "Iteration 21241, loss = 1.22643151\n",
      "Iteration 21242, loss = 1.34380212\n",
      "Iteration 21243, loss = 1.13444342\n",
      "Iteration 21244, loss = 1.19484743\n",
      "Iteration 21245, loss = 1.14503496\n",
      "Iteration 21246, loss = 1.11545597\n",
      "Iteration 21247, loss = 1.42436788\n",
      "Iteration 21248, loss = 1.36071365\n",
      "Iteration 21249, loss = 1.63381748\n",
      "Iteration 21250, loss = 1.63313243\n",
      "Iteration 21251, loss = 1.72796011\n",
      "Iteration 21252, loss = 1.41948657\n",
      "Iteration 21253, loss = 1.19007210\n",
      "Iteration 21254, loss = 1.49913613\n",
      "Iteration 21255, loss = 2.13585212\n",
      "Iteration 21256, loss = 1.62172511\n",
      "Iteration 21257, loss = 1.37666867\n",
      "Iteration 21258, loss = 1.35111648\n",
      "Iteration 21259, loss = 1.32435980\n",
      "Iteration 21260, loss = 1.23865977\n",
      "Iteration 21261, loss = 1.10950498\n",
      "Iteration 21262, loss = 1.28738223\n",
      "Iteration 21263, loss = 1.22859745\n",
      "Iteration 21264, loss = 1.18889413\n",
      "Iteration 21265, loss = 1.20166474\n",
      "Iteration 21266, loss = 1.35277307\n",
      "Iteration 21267, loss = 1.19951298\n",
      "Iteration 21268, loss = 1.33671792\n",
      "Iteration 21269, loss = 1.15264529\n",
      "Iteration 21270, loss = 1.27635777\n",
      "Iteration 21271, loss = 1.23692506\n",
      "Iteration 21272, loss = 2.15088598\n",
      "Iteration 21273, loss = 1.55410268\n",
      "Iteration 21274, loss = 1.32247872\n",
      "Iteration 21275, loss = 1.33236590\n",
      "Iteration 21276, loss = 1.36432214\n",
      "Iteration 21277, loss = 1.68597462\n",
      "Iteration 21278, loss = 1.94265159\n",
      "Iteration 21279, loss = 1.40081644\n",
      "Iteration 21280, loss = 1.25198779\n",
      "Iteration 21281, loss = 1.29212666\n",
      "Iteration 21282, loss = 1.48028065\n",
      "Iteration 21283, loss = 1.37176169\n",
      "Iteration 21284, loss = 1.45591037\n",
      "Iteration 21285, loss = 1.48905880\n",
      "Iteration 21286, loss = 1.37063072\n",
      "Iteration 21287, loss = 1.32209963\n",
      "Iteration 21288, loss = 1.21351529\n",
      "Iteration 21289, loss = 1.20479157\n",
      "Iteration 21290, loss = 1.35453097\n",
      "Iteration 21291, loss = 1.56948950\n",
      "Iteration 21292, loss = 2.28489684\n",
      "Iteration 21293, loss = 1.48917816\n",
      "Iteration 21294, loss = 1.30846823\n",
      "Iteration 21295, loss = 1.36872750\n",
      "Iteration 21296, loss = 1.31507662\n",
      "Iteration 21297, loss = 1.53052733\n",
      "Iteration 21298, loss = 1.68658487\n",
      "Iteration 21299, loss = 1.81407553\n",
      "Iteration 21300, loss = 1.78499414\n",
      "Iteration 21301, loss = 1.73406970\n",
      "Iteration 21302, loss = 1.56761925\n",
      "Iteration 21303, loss = 1.67618011\n",
      "Iteration 21304, loss = 1.56957604\n",
      "Iteration 21305, loss = 1.52335524\n",
      "Iteration 21306, loss = 1.80956556\n",
      "Iteration 21307, loss = 1.36523283\n",
      "Iteration 21308, loss = 1.63972203\n",
      "Iteration 21309, loss = 1.62010582\n",
      "Iteration 21310, loss = 1.63828460\n",
      "Iteration 21311, loss = 1.29661849\n",
      "Iteration 21312, loss = 1.32969310\n",
      "Iteration 21313, loss = 1.16039209\n",
      "Iteration 21314, loss = 1.11960302\n",
      "Iteration 21315, loss = 1.19016702\n",
      "Iteration 21316, loss = 1.13128282\n",
      "Iteration 21317, loss = 1.09704223\n",
      "Iteration 21318, loss = 1.16243530\n",
      "Iteration 21319, loss = 1.14404694\n",
      "Iteration 21320, loss = 1.11513510\n",
      "Iteration 21321, loss = 1.09357920\n",
      "Iteration 21322, loss = 1.08319340\n",
      "Iteration 21323, loss = 1.15669681\n",
      "Iteration 21324, loss = 1.12424894\n",
      "Iteration 21325, loss = 1.28289757\n",
      "Iteration 21326, loss = 1.38018844\n",
      "Iteration 21327, loss = 1.36696225\n",
      "Iteration 21328, loss = 1.25620157\n",
      "Iteration 21329, loss = 1.27406716\n",
      "Iteration 21330, loss = 1.62611021\n",
      "Iteration 21331, loss = 4.43081208\n",
      "Iteration 21332, loss = 12.93216137\n",
      "Iteration 21333, loss = 11.82011496\n",
      "Iteration 21334, loss = 9.29211870\n",
      "Iteration 21335, loss = 8.85868507\n",
      "Iteration 21336, loss = 6.43431478\n",
      "Iteration 21337, loss = 4.57279029\n",
      "Iteration 21338, loss = 3.48533827\n",
      "Iteration 21339, loss = 5.57408038\n",
      "Iteration 21340, loss = 4.29241219\n",
      "Iteration 21341, loss = 3.54290043\n",
      "Iteration 21342, loss = 2.31758712\n",
      "Iteration 21343, loss = 2.34360165\n",
      "Iteration 21344, loss = 1.80063887\n",
      "Iteration 21345, loss = 1.48980477\n",
      "Iteration 21346, loss = 1.31535896\n",
      "Iteration 21347, loss = 1.60270227\n",
      "Iteration 21348, loss = 1.79948184\n",
      "Iteration 21349, loss = 2.86699081\n",
      "Iteration 21350, loss = 25.58001370\n",
      "Iteration 21351, loss = 22.09213110\n",
      "Iteration 21352, loss = 22.65384338\n",
      "Iteration 21353, loss = 16.75996400\n",
      "Iteration 21354, loss = 12.56707163\n",
      "Iteration 21355, loss = 11.01890511\n",
      "Iteration 21356, loss = 10.24434100\n",
      "Iteration 21357, loss = 11.05553187\n",
      "Iteration 21358, loss = 10.61836395\n",
      "Iteration 21359, loss = 9.42509651\n",
      "Iteration 21360, loss = 10.81597909\n",
      "Iteration 21361, loss = 8.88152440\n",
      "Iteration 21362, loss = 8.71126978\n",
      "Iteration 21363, loss = 8.49794427\n",
      "Iteration 21364, loss = 8.42173521\n",
      "Iteration 21365, loss = 8.26595439\n",
      "Iteration 21366, loss = 8.58291089\n",
      "Iteration 21367, loss = 8.05747403\n",
      "Iteration 21368, loss = 8.51417251\n",
      "Iteration 21369, loss = 8.54687377\n",
      "Iteration 21370, loss = 7.88914467\n",
      "Iteration 21371, loss = 8.02281947\n",
      "Iteration 21372, loss = 9.60434945\n",
      "Iteration 21373, loss = 8.35019338\n",
      "Iteration 21374, loss = 8.46536481\n",
      "Iteration 21375, loss = 7.77021363\n",
      "Iteration 21376, loss = 8.26644045\n",
      "Iteration 21377, loss = 7.95230389\n",
      "Iteration 21378, loss = 7.43258193\n",
      "Iteration 21379, loss = 7.68754096\n",
      "Iteration 21380, loss = 8.99338590\n",
      "Iteration 21381, loss = 7.81773353\n",
      "Iteration 21382, loss = 8.54127663\n",
      "Iteration 21383, loss = 7.46712881\n",
      "Iteration 21384, loss = 7.87610195\n",
      "Iteration 21385, loss = 9.29590228\n",
      "Iteration 21386, loss = 9.35983297\n",
      "Iteration 21387, loss = 7.87241545\n",
      "Iteration 21388, loss = 8.15800111\n",
      "Iteration 21389, loss = 8.87296461\n",
      "Iteration 21390, loss = 7.90326030\n",
      "Iteration 21391, loss = 8.77907573\n",
      "Iteration 21392, loss = 7.74213925\n",
      "Iteration 21393, loss = 7.20368806\n",
      "Iteration 21394, loss = 6.94121474\n",
      "Iteration 21395, loss = 7.37234355\n",
      "Iteration 21396, loss = 8.14708586\n",
      "Iteration 21397, loss = 7.21319238\n",
      "Iteration 21398, loss = 7.46029165\n",
      "Iteration 21399, loss = 8.11017016\n",
      "Iteration 21400, loss = 7.11074429\n",
      "Iteration 21401, loss = 6.86999701\n",
      "Iteration 21402, loss = 6.97399715\n",
      "Iteration 21403, loss = 7.30256038\n",
      "Iteration 21404, loss = 8.44251435\n",
      "Iteration 21405, loss = 10.02261267\n",
      "Iteration 21406, loss = 11.67147058\n",
      "Iteration 21407, loss = 7.38928864\n",
      "Iteration 21408, loss = 8.44481594\n",
      "Iteration 21409, loss = 7.58338590\n",
      "Iteration 21410, loss = 9.13989078\n",
      "Iteration 21411, loss = 7.88291378\n",
      "Iteration 21412, loss = 7.19379583\n",
      "Iteration 21413, loss = 7.99096091\n",
      "Iteration 21414, loss = 6.90784305\n",
      "Iteration 21415, loss = 6.83486948\n",
      "Iteration 21416, loss = 7.23288096\n",
      "Iteration 21417, loss = 6.90303716\n",
      "Iteration 21418, loss = 6.49315127\n",
      "Iteration 21419, loss = 6.56669114\n",
      "Iteration 21420, loss = 6.92581794\n",
      "Iteration 21421, loss = 7.01769501\n",
      "Iteration 21422, loss = 7.15205234\n",
      "Iteration 21423, loss = 8.11592033\n",
      "Iteration 21424, loss = 6.77590448\n",
      "Iteration 21425, loss = 6.95683494\n",
      "Iteration 21426, loss = 6.76178061\n",
      "Iteration 21427, loss = 6.41990497\n",
      "Iteration 21428, loss = 6.44167885\n",
      "Iteration 21429, loss = 6.38687156\n",
      "Iteration 21430, loss = 6.46884657\n",
      "Iteration 21431, loss = 6.49615268\n",
      "Iteration 21432, loss = 6.37294859\n",
      "Iteration 21433, loss = 6.81938150\n",
      "Iteration 21434, loss = 6.70351065\n",
      "Iteration 21435, loss = 6.49517395\n",
      "Iteration 21436, loss = 6.54312711\n",
      "Iteration 21437, loss = 6.36530867\n",
      "Iteration 21438, loss = 6.22086318\n",
      "Iteration 21439, loss = 6.84995301\n",
      "Iteration 21440, loss = 6.34985979\n",
      "Iteration 21441, loss = 6.38940233\n",
      "Iteration 21442, loss = 9.92925191\n",
      "Iteration 21443, loss = 10.88054491\n",
      "Iteration 21444, loss = 9.78116601\n",
      "Iteration 21445, loss = 9.13758038\n",
      "Iteration 21446, loss = 6.76853361\n",
      "Iteration 21447, loss = 6.73845348\n",
      "Iteration 21448, loss = 7.88808242\n",
      "Iteration 21449, loss = 7.02670623\n",
      "Iteration 21450, loss = 6.36757444\n",
      "Iteration 21451, loss = 6.25731781\n",
      "Iteration 21452, loss = 6.88612271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21453, loss = 6.38488024\n",
      "Iteration 21454, loss = 6.76749311\n",
      "Iteration 21455, loss = 6.02363439\n",
      "Iteration 21456, loss = 6.34956603\n",
      "Iteration 21457, loss = 6.49208908\n",
      "Iteration 21458, loss = 6.41789212\n",
      "Iteration 21459, loss = 6.47224187\n",
      "Iteration 21460, loss = 8.15926873\n",
      "Iteration 21461, loss = 7.88875878\n",
      "Iteration 21462, loss = 7.70127848\n",
      "Iteration 21463, loss = 7.73567712\n",
      "Iteration 21464, loss = 8.88824099\n",
      "Iteration 21465, loss = 6.39036622\n",
      "Iteration 21466, loss = 7.92960135\n",
      "Iteration 21467, loss = 7.31738222\n",
      "Iteration 21468, loss = 8.12587274\n",
      "Iteration 21469, loss = 7.21835043\n",
      "Iteration 21470, loss = 6.58956962\n",
      "Iteration 21471, loss = 6.69150653\n",
      "Iteration 21472, loss = 6.09432001\n",
      "Iteration 21473, loss = 6.81762357\n",
      "Iteration 21474, loss = 6.41113346\n",
      "Iteration 21475, loss = 5.92571973\n",
      "Iteration 21476, loss = 5.84183106\n",
      "Iteration 21477, loss = 5.93079133\n",
      "Iteration 21478, loss = 5.92446452\n",
      "Iteration 21479, loss = 5.93028128\n",
      "Iteration 21480, loss = 5.77867223\n",
      "Iteration 21481, loss = 6.36388118\n",
      "Iteration 21482, loss = 6.30421918\n",
      "Iteration 21483, loss = 5.77817274\n",
      "Iteration 21484, loss = 5.98451096\n",
      "Iteration 21485, loss = 5.72478725\n",
      "Iteration 21486, loss = 5.68182605\n",
      "Iteration 21487, loss = 5.77389967\n",
      "Iteration 21488, loss = 5.81194249\n",
      "Iteration 21489, loss = 6.33701613\n",
      "Iteration 21490, loss = 5.61972153\n",
      "Iteration 21491, loss = 5.93473959\n",
      "Iteration 21492, loss = 6.41373007\n",
      "Iteration 21493, loss = 6.65947151\n",
      "Iteration 21494, loss = 5.97727325\n",
      "Iteration 21495, loss = 6.26163141\n",
      "Iteration 21496, loss = 6.29065554\n",
      "Iteration 21497, loss = 6.21424144\n",
      "Iteration 21498, loss = 5.64697246\n",
      "Iteration 21499, loss = 5.94364477\n",
      "Iteration 21500, loss = 5.97634808\n",
      "Iteration 21501, loss = 6.82805029\n",
      "Iteration 21502, loss = 5.69563650\n",
      "Iteration 21503, loss = 5.69486750\n",
      "Iteration 21504, loss = 5.87004219\n",
      "Iteration 21505, loss = 5.72663278\n",
      "Iteration 21506, loss = 5.67267892\n",
      "Iteration 21507, loss = 6.46101537\n",
      "Iteration 21508, loss = 5.90617944\n",
      "Iteration 21509, loss = 5.66045166\n",
      "Iteration 21510, loss = 5.64363660\n",
      "Iteration 21511, loss = 5.55564262\n",
      "Iteration 21512, loss = 5.57559138\n",
      "Iteration 21513, loss = 5.83309643\n",
      "Iteration 21514, loss = 5.72715089\n",
      "Iteration 21515, loss = 5.80067592\n",
      "Iteration 21516, loss = 6.37234259\n",
      "Iteration 21517, loss = 6.14253221\n",
      "Iteration 21518, loss = 5.51595961\n",
      "Iteration 21519, loss = 5.51174576\n",
      "Iteration 21520, loss = 5.46654169\n",
      "Iteration 21521, loss = 5.50394564\n",
      "Iteration 21522, loss = 6.19046617\n",
      "Iteration 21523, loss = 5.46311251\n",
      "Iteration 21524, loss = 5.54978547\n",
      "Iteration 21525, loss = 5.78988223\n",
      "Iteration 21526, loss = 5.52307488\n",
      "Iteration 21527, loss = 5.94613215\n",
      "Iteration 21528, loss = 5.48774014\n",
      "Iteration 21529, loss = 6.25696086\n",
      "Iteration 21530, loss = 5.75640130\n",
      "Iteration 21531, loss = 5.33618026\n",
      "Iteration 21532, loss = 5.62669219\n",
      "Iteration 21533, loss = 5.78026554\n",
      "Iteration 21534, loss = 5.81071200\n",
      "Iteration 21535, loss = 6.71133915\n",
      "Iteration 21536, loss = 5.42866195\n",
      "Iteration 21537, loss = 5.73125212\n",
      "Iteration 21538, loss = 6.29030124\n",
      "Iteration 21539, loss = 6.03931100\n",
      "Iteration 21540, loss = 6.61455949\n",
      "Iteration 21541, loss = 8.99620651\n",
      "Iteration 21542, loss = 10.04095186\n",
      "Iteration 21543, loss = 7.16933705\n",
      "Iteration 21544, loss = 8.44134275\n",
      "Iteration 21545, loss = 6.36624149\n",
      "Training loss did not improve more than tol=0.000010 for 10000 consecutive epochs. Stopping.\n",
      "R^2 Training Score: 0.836 \n",
      "R^2 Testing Score: 0.837\n",
      "RMSE Training Score: 3.717 \n",
      "RMSE Testing Score: 1.443\n",
      "MAE Training Score: 1.810 \n",
      "MAE Testing Score: 1.059\n"
     ]
    }
   ],
   "source": [
    "reg_opt = MLPRegressor(\n",
    "    activation=clf.best_params_['activation'], \n",
    "\n",
    "    hidden_layer_sizes=clf.best_params_['hidden_layer_sizes'],\n",
    "    learning_rate=clf.best_params_['learning_rate'],\n",
    "    max_iter=clf.best_params_['max_iter'],\n",
    "    solver=clf.best_params_['solver'],\n",
    "    batch_size='auto',\n",
    "    verbose=True, warm_start=False, \n",
    "    early_stopping=False,\n",
    "    alpha=0.0001,\n",
    "    learning_rate_init=0.001, shuffle=True,\n",
    "    random_state=1, tol=0.00001,\n",
    "    beta_1=0.9, beta_2=0.999, epsilon=1e-08,n_iter_no_change=10000)\n",
    "\n",
    "reg_opt.fit(X_train, y_train)\n",
    "\n",
    "print('R^2 Training Score: {:.3f} \\nR^2 Testing Score: {:.3f}'.format(reg_opt.score(X_train, y_train),reg_opt.score(X_test, y_test)))\n",
    "print('RMSE Training Score: {:.3f} \\nRMSE Testing Score: {:.3f}'.format(np.sqrt(mean_squared_error(y_train,reg_opt.predict(X_train))),np.sqrt(mean_squared_error(y_test,reg_opt.predict(X_test)))))\n",
    "print('MAE Training Score: {:.3f} \\nMAE Testing Score: {:.3f}'.format(mean_absolute_error(y_train,reg_opt.predict(X_train)),mean_absolute_error(y_test,reg_opt.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = joblib.dump(reg_opt,\"ANN\"+\"_Xe_M.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.70788955315929\n",
      "33.558540096802275\n",
      "65.98904442110157\n",
      "67.84049298007551\n",
      "56.07324021087762\n",
      "52.19357655109875\n",
      "36.81351802692757\n",
      "89.58406352260363\n",
      "53.685683093704235\n",
      "60.75767608302398\n",
      "Best score=33.5585\n",
      "Best parameters:\n",
      "    - C=90797\n",
      "    - gamma=0.000028\n",
      "    - epsilon=0.196569\n",
      "    - kernel=rbf\n",
      "R^2 Training Score: 0.749 \n",
      "R^2 Testing Score: 0.838\n",
      "RMSE Training Score: 4.594 \n",
      "RMSE Testing Score: 1.436\n",
      "MAE Training Score: 1.327 \n",
      "MAE Testing Score: 0.653\n"
     ]
    }
   ],
   "source": [
    "reg = SVR()\n",
    "space  = [Integer(100,100000, name='C'),\n",
    "            Real(0.000001,1,\"log-uniform\",name='gamma'),\n",
    "            Real(0.1,1,name='epsilon'),\n",
    "            Categorical(('linear','rbf','sigmoid'), name='kernel')\n",
    "         ]\n",
    "@use_named_args(space)\n",
    "\n",
    "def objective(**params):\n",
    "    reg.set_params(**params)\n",
    "    result=-np.mean(cross_val_score(reg, X_train, y_train.ravel(), cv=5, n_jobs=4,\n",
    "                                        scoring=\"neg_mean_squared_error\"))\n",
    "    \n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "res_gp = gp_minimize(objective, space, n_calls=10)\n",
    "\n",
    "print(\"Best score=%.4f\" % res_gp.fun)\n",
    "print(\"\"\"Best parameters:\n",
    "    - C=%d\n",
    "    - gamma=%f\n",
    "    - epsilon=%f\n",
    "    - kernel=%s\"\"\" % (res_gp.x[0],res_gp.x[1],\n",
    "                        res_gp.x[2],res_gp.x[3]))\n",
    "reg_opt = SVR(C=res_gp.x[0],\n",
    "                gamma=res_gp.x[1],\n",
    "                epsilon=res_gp.x[2],\n",
    "                kernel=res_gp.x[3],\n",
    "                tol=0.001,\n",
    "                verbose=False)\n",
    "\n",
    "reg_opt.fit(X_train, y_train.ravel())\n",
    "\n",
    "print('R^2 Training Score: {:.3f} \\nR^2 Testing Score: {:.3f}'.format(reg_opt.score(X_train, y_train),reg_opt.score(X_test, y_test)))\n",
    "print('RMSE Training Score: {:.3f} \\nRMSE Testing Score: {:.3f}'.format(np.sqrt(mean_squared_error(y_train,reg_opt.predict(X_train))),np.sqrt(mean_squared_error(y_test,reg_opt.predict(X_test)))))\n",
    "print('MAE Training Score: {:.3f} \\nMAE Testing Score: {:.3f}'.format(mean_absolute_error(y_train,reg_opt.predict(X_train)),mean_absolute_error(y_test,reg_opt.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = joblib.dump(reg_opt,\"SVM\"+\"_Xe_M.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 ABR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.210533810766801\n",
      "21.53401234958552\n",
      "17.47894190714461\n",
      "31.178695061220274\n",
      "12.54782869789895\n",
      "35.056681367758735\n",
      "17.0949557995577\n",
      "17.815070074404353\n",
      "12.107954774913456\n",
      "13.037402271746249\n",
      "Best score=12.1080\n",
      "Best parameters:\n",
      "    - learning_rate=0.024780\n",
      "    - n_estimators=233\n",
      "    - loss=linear\n",
      "R^2 Training Score: 0.948 \n",
      "R^2 Testing Score: 0.850\n",
      "RMSE Training Score: 2.099 \n",
      "RMSE Testing Score: 1.382\n",
      "MAE Training Score: 1.513 \n",
      "MAE Testing Score: 1.122\n"
     ]
    }
   ],
   "source": [
    "reg = AdaBoostRegressor()\n",
    "space  = [Real(0.01,1,\"log-uniform\",name='learning_rate'),\n",
    "            Integer(1,1000,name='n_estimators'),\n",
    "            Categorical(('linear','square','exponential'), name='loss')]\n",
    "@use_named_args(space)\n",
    "\n",
    "def objective(**params):\n",
    "    reg.set_params(**params)\n",
    "    result=-np.mean(cross_val_score(reg, X_train, y_train.ravel(), cv=5, n_jobs=4,scoring=\"neg_mean_squared_error\"))\n",
    "    \n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "res_gp = gp_minimize(objective, space, n_calls=10)\n",
    "\n",
    "print(\"Best score=%.4f\" % res_gp.fun)\n",
    "print(\"\"\"Best parameters:\n",
    "    - learning_rate=%f\n",
    "    - n_estimators=%d\n",
    "    - loss=%s\"\"\" % (res_gp.x[0],res_gp.x[1],res_gp.x[2]))\n",
    "\n",
    "reg_opt = AdaBoostRegressor(learning_rate=res_gp.x[0],n_estimators=res_gp.x[1],loss=res_gp.x[2])\n",
    "reg_opt.fit(X_train, y_train.ravel())\n",
    "\n",
    "print('R^2 Training Score: {:.3f} \\nR^2 Testing Score: {:.3f}'.format(reg_opt.score(X_train, y_train),reg_opt.score(X_test, y_test)))\n",
    "print('RMSE Training Score: {:.3f} \\nRMSE Testing Score: {:.3f}'.format(np.sqrt(mean_squared_error(y_train,reg_opt.predict(X_train))),np.sqrt(mean_squared_error(y_test,reg_opt.predict(X_test)))))\n",
    "print('MAE Training Score: {:.3f} \\nMAE Testing Score: {:.3f}'.format(mean_absolute_error(y_train,reg_opt.predict(X_train)),mean_absolute_error(y_test,reg_opt.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = joblib.dump(reg_opt,\"ABR\"+\"_Xe_M.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.34599106167255\n",
      "67.85856917352497\n",
      "70.09284714949499\n",
      "73.58976475721302\n",
      "70.6373215553111\n",
      "69.37251221463922\n",
      "71.87312298754962\n",
      "68.7606449748813\n",
      "69.33185549124694\n",
      "73.18159956485701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.85856917352497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.85856917352497\n",
      "67.44411111377946\n",
      "78.92163477875172\n",
      "67.44411111377946\n",
      "67.44411111377946\n",
      "69.92319676301615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.44411111377946\n",
      "67.44411111377946\n",
      "63.28646899022492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.28646899022492\n",
      "66.38554028814522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.28646899022492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.28646899022492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.28646899022492\n",
      "65.23487546160422\n",
      "67.44411111377946\n",
      "65.00385837902711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.28646899022492\n",
      "63.28646899022492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.28646899022492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.28646899022492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.28646899022492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.28646899022492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.28646899022492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.28646899022492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.28646899022492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.28646899022492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.28646899022492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.28646899022492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.28646899022492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.28646899022492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.28646899022492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.28646899022492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.28646899022492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.28646899022492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.28646899022492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.28646899022492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.28646899022492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.28646899022492\n",
      "Best score=63.2865\n",
      "Best parameters:\n",
      "    - weights=distance\n",
      "    - n_neighbors=3\n",
      "    - algorithm=ball_tree\n",
      "    - leaf_size=7\n",
      "    - p=1\n",
      "R^2 Training Score: 1.000 \n",
      "R^2 Testing Score: 0.468\n",
      "RMSE Training Score: 0.000 \n",
      "RMSE Testing Score: 2.605\n",
      "MAE Training Score: 0.000 \n",
      "MAE Testing Score: 1.193\n"
     ]
    }
   ],
   "source": [
    "reg = KNeighborsRegressor()\n",
    "space  = [Categorical(('uniform','distance'),name='weights'),\n",
    "            Integer(1,30,name='n_neighbors'),\n",
    "            Categorical(('auto','ball_tree','kd_tree','brute'), name='algorithm'),\n",
    "            Integer(1,10,name='leaf_size'),\n",
    "            Integer(1,10,name='p')\n",
    "         ]\n",
    "@use_named_args(space)\n",
    "\n",
    "def objective(**params):\n",
    "    reg.set_params(**params)\n",
    "    result=-np.mean(cross_val_score(reg, X_train, y_train, cv=5, n_jobs=4,\n",
    "                                        scoring=\"neg_mean_squared_error\"))\n",
    "    \n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "res_gp = gp_minimize(objective, space, n_calls=50)\n",
    "\n",
    "print(\"Best score=%.4f\" % res_gp.fun)\n",
    "print(\"\"\"Best parameters:\n",
    "    - weights=%s\n",
    "    - n_neighbors=%d\n",
    "    - algorithm=%s\n",
    "    - leaf_size=%d\n",
    "    - p=%d\"\"\" % (res_gp.x[0],res_gp.x[1],res_gp.x[2],res_gp.x[3],res_gp.x[4]))\n",
    "reg_opt = KNeighborsRegressor(weights=res_gp.x[0],n_neighbors=res_gp.x[1],algorithm=res_gp.x[2],leaf_size=res_gp.x[3],p=res_gp.x[4])\n",
    "\n",
    "reg_opt.fit(X_train, y_train)\n",
    "\n",
    "print('R^2 Training Score: {:.3f} \\nR^2 Testing Score: {:.3f}'.format(reg_opt.score(X_train, y_train),reg_opt.score(X_test, y_test)))\n",
    "print('RMSE Training Score: {:.3f} \\nRMSE Testing Score: {:.3f}'.format(np.sqrt(mean_squared_error(y_train,reg_opt.predict(X_train))),np.sqrt(mean_squared_error(y_test,reg_opt.predict(X_test)))))\n",
    "print('MAE Training Score: {:.3f} \\nMAE Testing Score: {:.3f}'.format(mean_absolute_error(y_train,reg_opt.predict(X_train)),mean_absolute_error(y_test,reg_opt.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = joblib.dump(reg_opt,\"KNN\"+\"_Xe_M.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.17768170915075\n",
      "48.96461169036316\n",
      "44.16872959182156\n",
      "49.95854470395119\n",
      "49.90813974221262\n",
      "49.00323155574214\n",
      "47.80586675828269\n",
      "50.15171653838441\n",
      "47.349960338946524\n",
      "49.17760546247512\n",
      "38.26009517828126\n",
      "38.26009517828126\n",
      "38.26009517828126\n",
      "38.26009517828126\n",
      "38.26009517828126"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.26009517828126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.26009517828126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.26009517828126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.26009517828126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.26009517828126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.26009517828126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.26009517828126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.26009517828126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.26009517828126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.26009517828126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.26009517828126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.26009517828126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.26009517828126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.26009517828126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.26009517828126\n",
      "38.26009517828126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.26009517828126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.26009517828126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.26009517828126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.26009517828126\n",
      "38.26009517828126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.26009517828126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.26009517828126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.26009517828126\n",
      "38.26009517828126\n",
      "38.27559527828767\n",
      "38.26009517828126\n",
      "38.26009517828126\n",
      "38.26009517828126\n",
      "38.44368348023017\n",
      "38.26016921331916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.26009517828126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.26009517828126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.26009517828126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.26009517828126\n",
      "Best score=38.2601\n",
      "Best parameters:\n",
      "    - random_state=0\n",
      "    - alphas=0.000000\n",
      "    - tol=0.010000\n",
      "R^2 Training Score: 0.569 \n",
      "R^2 Testing Score: 0.556\n",
      "RMSE Training Score: 6.026 \n",
      "RMSE Testing Score: 2.380\n",
      "MAE Training Score: 3.097 \n",
      "MAE Testing Score: 1.722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-f0beb3db35d0>:25: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  reg_opt.fit(X_train, y_train)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 15684.692607052246, tolerance: 727.0886187297994\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "reg = Lasso(max_iter=10000,fit_intercept=True)\n",
    "space  = [Integer(0,1000,name='random_state'),\n",
    "            Real(0,1,name='alpha'),\n",
    "            Real(0.0001,0.01,\"log-uniform\",name='tol')\n",
    "         ]\n",
    "@use_named_args(space)\n",
    "\n",
    "def objective(**params):\n",
    "    reg.set_params(**params)\n",
    "    result=-np.mean(cross_val_score(reg, X_train, y_train, cv=5, n_jobs=4,scoring=\"neg_mean_squared_error\"))\n",
    "    \n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "res_gp = gp_minimize(objective, space, n_calls=50)\n",
    "\n",
    "print(\"Best score=%.4f\" % res_gp.fun)\n",
    "print(\"\"\"Best parameters:\n",
    "    - random_state=%d\n",
    "    - alphas=%4f\n",
    "    - tol=%4f\"\"\" % (res_gp.x[0],res_gp.x[1],res_gp.x[2]))\n",
    "\n",
    "reg_opt = Lasso(random_state=res_gp.x[0],alpha=res_gp.x[1],tol=res_gp.x[2],max_iter=10000,fit_intercept=True)\n",
    "\n",
    "reg_opt.fit(X_train, y_train)\n",
    "\n",
    "print('R^2 Training Score: {:.3f} \\nR^2 Testing Score: {:.3f}'.format(reg_opt.score(X_train, y_train),reg_opt.score(X_test, y_test)))\n",
    "print('RMSE Training Score: {:.3f} \\nRMSE Testing Score: {:.3f}'.format(np.sqrt(mean_squared_error(y_train,reg_opt.predict(X_train))),np.sqrt(mean_squared_error(y_test,reg_opt.predict(X_test)))))\n",
    "print('MAE Training Score: {:.3f} \\nMAE Testing Score: {:.3f}'.format(mean_absolute_error(y_train,reg_opt.predict(X_train)),mean_absolute_error(y_test,reg_opt.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = joblib.dump(reg_opt,\"LASSO\"+\"_Xe_M.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load(\"./models/RF_Xe_M.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=22, max_features='sqrt', min_samples_leaf=7,\n",
       "                      min_samples_split=5, n_estimators=24, random_state=445)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
